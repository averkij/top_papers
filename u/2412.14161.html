
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 января</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-14.html">⬅️ <span id="prev-date">14.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-16.html">➡️ <span id="next-date">16.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 января', 'en': 'January 15', 'zh': '1月15日'};
        let feedDateNext = {'ru': '16.01', 'en': '01/16', 'zh': '1月16日'};
        let feedDatePrev = {'ru': '14.01', 'en': '01/14', 'zh': '1月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2412.14161', 'title': 'TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks', 'url': 'https://arxiv.org/pdf/2412.14161', 'abstract': "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.", 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '4284432422625536', 'authors': ['Frank F. Xu', 'Yufan Song', 'Boxuan Li', 'Yuxuan Tang', 'Kritanjali Jain', 'Mengxue Bao', 'Zora Z. Wang', 'Xuhui Zhou', 'Zhitong Guo', 'Murong Cao', 'Mingyang Yang', 'Hao Yang Lu', 'Amaad Martin', 'Zhe Su', 'Leander Maben', 'Raj Mehta', 'Wayne Chi', 'Lawrence Jang', 'Yiqing Xie', 'Shuyan Zhou', 'Graham Neubig'], 'affiliations': ['Carnegie Mellon University', 'Duke University', 'Independent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2412.14161.jpg', 'data': {'categories': ['#optimization', '#agi', '#agents', '#science', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'ИИ-агенты в офисе: прогресс и ограничения в автоматизации рабочих задач', 'desc': 'Статья представляет новый бенчмарк TheAgentCompany для оценки ИИ-агентов в выполнении профессиональных задач в виртуальной среде, имитирующей небольшую компанию-разработчика. Исследователи тестируют агентов на основе языковых моделей (ЯМ) в различных задачах, включая веб-серфинг, программирование и коммуникацию. Результаты показывают, что лучшие агенты способны автономно выполнить 24% задач. Это демонстрирует, что ИИ-агенты могут автоматизировать простые задачи, но сложные долгосрочные задачи все еще остаются недоступными для текущих систем.'}, 'en': {'title': 'Evaluating AI Agents: The Future of Work Automation', 'desc': 'This paper introduces TheAgentCompany, a benchmark designed to evaluate the performance of AI agents in completing work-related tasks similar to those of a digital worker. The study assesses how well these agents, powered by large language models (LLMs), can autonomously perform tasks such as web browsing, coding, and communication within a simulated software company environment. The results indicate that while 24% of tasks can be completed autonomously by the most effective agent, more complex tasks remain challenging for current AI systems. This research highlights the potential and limitations of AI in automating workplace functions, providing insights for industries considering AI integration.'}, 'zh': {'title': 'AI代理助力工作任务自动化的探索', 'desc': '本文介绍了一个名为TheAgentCompany的基准测试，用于评估人工智能代理在执行真实工作任务中的表现。我们创建了一个模拟小型软件公司的环境，设计了多种任务，代理可以通过浏览网页、编写代码和与同事沟通来完成这些任务。测试结果显示，最先进的代理能够自主完成24%的任务，这表明在简单任务的自动化方面，当前的语言模型代理表现良好。尽管如此，对于更复杂的长期任务，现有系统仍然无法胜任。'}}, 'clean_sections': [{'title': 'Abstract', 'content': "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.", 'summary': '<p>В данной работе представлен TheAgentCompany — расширяемый бенчмарк для оценки производительности AI-агентов, выполняющих задачи, аналогичные работе цифрового сотрудника. </p>\n<p>В современном мире мы постоянно взаимодействуем с компьютерами, и многие рабочие задачи могут быть выполнены исключительно с помощью компьютера и интернета. Параллельно с этим, благодаря развитию больших языковых моделей (LLM), активно развиваются AI-агенты, способные взаимодействовать с окружающей средой и влиять на нее. Возникает вопрос: насколько хорошо AI-агенты справляются с ускорением или даже автономным выполнением рабочих задач? Ответ на этот вопрос важен как для компаний, стремящихся внедрить AI в свои рабочие процессы, так и для экономической политики, чтобы понять влияние AI на рынок труда.</p>\n<p>Для оценки прогресса LLM-агентов в выполнении реальных профессиональных задач, авторы разработали бенчмарк TheAgentCompany. Он представляет собой самодостаточную среду, имитирующую небольшую IT-компанию с внутренними веб-сайтами и данными. В этой среде созданы разнообразные задачи, которые могут выполняться сотрудниками такой компании: просмотр веб-страниц, написание кода, запуск программ и общение с коллегами.</p>\n<p>Авторы протестировали базовые модели агентов, основанные как на закрытых API, так и на языковых моделях с открытым исходным кодом. Результаты показали, что наиболее эффективный агент способен автономно выполнить 24% задач. Это говорит о том, что в условиях, имитирующих реальное рабочее место, AI-агенты могут самостоятельно справляться с простыми задачами, но более сложные, требующие планирования и выполнения нескольких шагов, пока остаются за пределами возможностей современных систем.</p>'}, {'title': 'Introduction', 'content': 'We are in the midst of technological transformation. With the rapid year-by-year and month-bymonth progress brought about by large language models (LLMs), we are seeing AI-based assistance or automation become commonplace in tasks that were unthinkable only few years ago. In fact, the pace of progress is so fast that some have gone so far as to claim that the majority of human labor may be automatable within the next couple of years (Eloundou et al., 2023; Amodei & Fridman, 2024). On the other hand, others are skeptical, claiming that language models cannot truly reason (Kambhampati et al., 2024), do not generalize well to novel tasks (Chollet et al., 2024), and may only have an impact on small minority of the labor market (Wittenstein, 2024). What is the reason for this disconnect? We argue that it is, in part, due to lack of objective benchmarks that not only demonstrate the power of existing LLM-based agents to accelerate Equal contribution. 1 Preprint. It features reproducible and selfFigure 1: An overview of TheAgentCompany benchmark. hosted environment, simulated colleagues to test agent communication capabilities, checkpoint and execution-based evaluation, and set of 175 diverse, realistic and professional tasks in software engineering company setting. wide variety of repetitive tasks encountered in every-day workplaces, but also provide appropriate caveats about the tasks that agents cannot do. This is pressing issue, because the commercial and policy implications of diverse and effective acceleration or automation of work-related tasks will be broad, both positive (e.g. increase of quality of life and accelerated scientific discovery) and negative (e.g. potential displacement or loss of jobs and increase in wealth disparities). In this paper, we take some first steps towards resolving this gap and providing clearer view of where we are now with respect to acceleration or automation of consequential work-related tasks, and litmus test for future development in this direction. Concretely, we propose benchmark, TheAgentCompany (Figure 1) that estimates the ability of AI agents to perform tasks encountered in everyday workplaces. We create simulated software development company where agents must perform tasks related to software engineering, project management, financial analysis, and other typical tasks encountered in such business settings. The agents must browse the web, code, and interact with other simulated co-workers to achieve success on the provided tasks. TheAgentCompanys environment is based entirely on open-source software and self-hostable for reproducibility purposes, and we create rigorous evaluators that also assign partial credit when the agent gets the answer partially correct. We perform experiments using seven large language model backbones, including API-based models such as Anthropic Claude (Anthropic, 2023), OpenAI GPT-4o (OpenAI, 2024), Google Gemini (Team et al., 2023), Amazon Nova (Intelligence, 2024), as well as open models including Meta Llama (Dubey et al., 2024) and Alibaba Qwen (Yang et al., 2024). All models are run using the OpenHands agent framework (Wang et al., 2024b),1 which provides stable and strong agent harness for both web browsing and coding. As result of experiments, we find that the best performing model, Claude 3.5 Sonnet was able to autonomously perform 24.0% of the provided tests to completion, and achieve score of 34.4% on our metric that provides extra credit for partially completed tasks. These results present nuanced picture of the current ability of AI agents to perform tasks. Agents powered by the current gold-standard AI techniques are able to autonomously perform wide variety of tasks encountered in everyday work. However, they are not close to automating every task encountered in workspace, even on the subset of tasks presented in TheAgentCompany, which are well-scoped administrative and coding tasks encountered in software companys day-to-day work. In the rest of this paper, we explain detail comparisons to other existing benchmarks ( 2), how we set up realistic and reproducible environments ( 3), how we define tasks ( 4) and how we create them ( 5), our baseline agent ( 6), experimental results ( 7), and finally implications and future directions ( 8). 1https://github.com/All-Hands-AI/OpenHands 2 Preprint. is desktop, Table 1: Comparison of different AI agent benchmarks. Interface: the interface agent has access to; is web browser, is bash terminal. Supported Tasks: tasks in the benchmark, indicate tasks with no association with real-world occupations; SE refers to software engineering, HR is human resources, PM is project manager. Checkpoint-based evaluation: if tasks are evaluated at intermediate checkpoints and assigned partial scores. Interact with NPC Agents: If the agent can interact with other NPC agents during task-solving. is chat platform, is Python script, is API usage, Framework Diverse Real-world Work Task Categories Requires Interaction Long-Horizon w/ Checkpoints Interface Self-Hosted Environment MiniWob++ (Liu et al., 2018) Mind2Web (Deng et al., 2023) WebLINX (Lù et al., 2024) AssistantBench (Yoran et al., 2024) WebArena (Zhou et al., 2023) VisualWebArena (Koh et al., 2024) VideoWebArena (Jang et al., 2024) WorkArena (Drouin et al., 2024) OSWorld (Xie et al., 2024) Windows Agent Arena (Bonatti et al., 2024) AppWorld (Trivedi et al., 2024) Gorilla APIBench (Patil et al., 2023) τ -bench (Yao et al., 2024) SWE-bench (Jimenez et al., 2024) DevBench (Li et al., 2024) Smallville (Park et al., 2023) Sotopia (Zhou et al., 2024) TheAgentCompany (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) Browsing Browsing Browsing Browsing Browsing Browsing Browsing Enterprise Software Office, Coding Browsing, Office, Coding Daily Coding Retail, Airline SWE SWE Social Social SWE, HR, Admin, PM, Research, Finance (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34)', 'summary': '<p>В настоящее время мы наблюдаем стремительную технологическую трансформацию. Благодаря прогрессу в области больших языковых моделей (LLM), мы видим, как ИИ-помощь и автоматизация становятся обыденностью в задачах, которые еще несколько лет назад казались немыслимыми. Скорость этого прогресса настолько высока, что некоторые даже утверждают, что большая часть человеческого труда может быть автоматизирована в ближайшие пару лет. Однако другие настроены скептически, заявляя, что языковые модели не способны к настоящему рассуждению, плохо обобщают знания на новые задачи и могут повлиять лишь на небольшую часть рынка труда.</p>\n<p>Причина таких противоречивых мнений заключается, отчасти, в отсутствии объективных тестов, которые могли бы продемонстрировать не только возможности существующих ИИ-агентов на базе LLM в ускорении выполнения рутинных задач, но и обозначить ограничения их возможностей. Это особенно важно, учитывая широкие коммерческие и политические последствия эффективной автоматизации рабочих задач, которые могут быть как позитивными (например, повышение качества жизни и ускорение научных открытий), так и негативными (например, потенциальная потеря рабочих мест и увеличение неравенства в доходах).</p>\n<p>В данной работе авторы делают первые шаги к решению этой проблемы, предлагая более четкое представление о текущих возможностях ИИ в автоматизации важных рабочих задач. Они представляют бенчмарк под названием TheAgentCompany, который оценивает способность ИИ-агентов выполнять задачи, встречающиеся в повседневной работе. Для этого была создана имитация компании по разработке программного обеспечения, где агенты должны выполнять задачи, связанные с разработкой, управлением проектами, финансовым анализом и другими типичными задачами, встречающимися в таких бизнес-структурах. Агенты должны использовать веб-браузер, писать код и взаимодействовать с другими имитированными сотрудниками для успешного выполнения задач.</p>\n<p>Среда TheAgentCompany основана на программном обеспечении с открытым исходным кодом и может быть развернута локально для обеспечения воспроизводимости результатов. Также были разработаны строгие критерии оценки, которые позволяют присуждать частичный балл, если агент выполнил задачу лишь частично.</p>\n<p>В ходе экспериментов было использовано семь LLM, включая модели с доступом через API, такие как Anthropic Claude, OpenAI GPT-4o, Google Gemini, Amazon Nova, а также открытые модели Meta Llama и Alibaba Qwen. Все модели запускались с использованием фреймворка OpenHands, который обеспечивает стабильную и надежную среду для работы агентов с веб-браузером и кодом.</p>\n<p>Результаты экспериментов показали, что лучшая модель, Claude 3.5 Sonnet, смогла автономно выполнить 24% заданий и набрать 34,4% баллов с учетом частичного выполнения задач. Эти результаты показывают, что текущие ИИ-агенты способны автономно выполнять широкий спектр задач, встречающихся в повседневной работе. Однако они еще далеки от полной автоматизации всех рабочих задач, даже в рамках ограниченного набора задач TheAgentCompany, которые представляют собой хорошо структурированные административные и кодинговые задачи, встречающиеся в повседневной работе компании-разработчика.</p>\n<p>В остальной части статьи авторы подробно сравнивают свой бенчмарк с другими существующими (раздел 2), описывают создание реалистичной и воспроизводимой среды (раздел 3), определяют задачи (раздел 4) и процесс их создания (раздел 5), представляют базовую модель агента (раздел 6), анализируют результаты экспериментов (раздел 7), а также обсуждают последствия и направления дальнейших исследований (раздел 8).</p>'}, {'title': 'Benchmark Desiderata And Comparison To Other Benchmarks', 'content': 'In order to evaluate the ability of agents to perform tasks in complex real-world settings, we built TheAgentCompany with number of desiderata in mind. The comparison with several existing prominent agent benchmarks with respect to these desiderata is in Table 1. Coverage of Multiple Work-related Tasks: In order to make any valid statements about the potential of AI to accelerate or automate various types of real-world work, we should have tasks that are motivated by real-world work across multiple job categories. Many benchmarks are not relevant to real-world work (e.g. MiniWob++ (Liu et al., 2018)) or very relevant to real-world work, but only over limited scope of tasks (e.g. SWE-Bench (Jimenez et al., 2024)). In contrast, TheAgentCompany contains set of more diverse, realistic, and professional tasks that would typically be completed by multiple job roles in software engineering company. Requirement for Interaction: If agents are to integrate into real-world workplaces, they will need to communicate with the other human members of the workspace. Most other benchmarks do not measure communication or interactivity, with the exception of τ -bench (Yao et al., 2024), which only measures interaction in customer service scenarios. TheAgentCompany provides better testbed for communication as many tasks involve asking and providing information to colleagues as part of more complex task. Long-horizon Tasks with Checkpoints: In real-world settings, many tasks require taking many different steps to achieve higher-level goal. One major novel contribution of TheAgentCompany is that we both (1) contain tasks that require an agent to perform significantly more consecutive work (i.e. involving more steps and realistically taking human professionals longer to accomplish) than previous benchmarks, and (2) provide granular evaluators that measure the ability of models to perform subtasks of these larger tasks. Versatile Environment Interface: In order to handle diversity of tasks in real-world settings, we minimally should be able to interact with the tools that real-world workers use including web interfaces, programs, command-line terminals, and communication tools. TheAgentCompany covers all of these interfaces, while most previous benchmarks focus only on one or two. Preprint. Self-hosted and Reproducible: In order to allow for careful comparisons between different methods that remain constant over time, the benchmark should be fully self-hosted and reproducible. This contrasts with existing benchmarks that do not have execution environments (e.g. Mind2Web (Deng et al., 2023)) or require the usage of third-party software (e.g. WorkArena (Drouin et al., 2024)).', 'summary': '<p>В статье описывается новый бенчмарк для оценки способностей агентов в условиях, приближенных к реальным рабочим задачам, под названием TheAgentCompany. Разработчики выделили ряд ключевых требований к такому бенчмарку, и сравнили TheAgentCompany с существующими аналогами (см. Таблицу 1 в оригинальной статье).</p>\n<p><strong>Охват разнообразных рабочих задач:</strong> Для того, чтобы делать выводы о потенциале ИИ в автоматизации реальной работы, необходимы задачи, которые соответствуют различным профессиональным ролям. Многие существующие бенчмарки либо не связаны с реальной работой (например, MiniWob++), либо охватывают лишь узкий спектр задач (например, SWE-Bench). TheAgentCompany, напротив, предлагает более разнообразный и реалистичный набор задач, типичных для различных ролей в IT-компании.</p>\n<p><strong>Необходимость взаимодействия:</strong> Для интеграции агентов в реальные рабочие процессы требуется их способность общаться с людьми. Большинство бенчмарков не оценивают коммуникацию и интерактивность, за исключением τ-bench, который фокусируется только на взаимодействии в сфере обслуживания клиентов. TheAgentCompany предоставляет более подходящую среду для тестирования коммуникации, поскольку многие задачи требуют обмена информацией с коллегами в рамках более сложных процессов.</p>\n<p><strong>Задачи с долгосрочным горизонтом и контрольными точками:</strong> В реальных условиях многие задачи требуют последовательности шагов для достижения общей цели. Важным нововведением TheAgentCompany является наличие задач, которые требуют от агента выполнения значительно большего объема последовательной работы (т.е. больше шагов и больше времени, сравнимого с временем, затрачиваемым профессионалами). Кроме того, бенчмарк предоставляет детальные средства оценки, которые позволяют измерить способность моделей выполнять отдельные подзадачи.</p>\n<p><strong>Универсальный интерфейс среды:</strong> Для работы с разнообразием задач в реальных условиях агенты должны уметь взаимодействовать с инструментами, которые используют люди, включая веб-интерфейсы, программы, командную строку и средства коммуникации. TheAgentCompany охватывает все эти интерфейсы, в то время как большинство предыдущих бенчмарков фокусируются только на одном или двух.</p>\n<p><strong>Самостоятельный и воспроизводимый бенчмарк:</strong> Для корректного сравнения различных методов, результаты должны оставаться постоянными во времени. Поэтому бенчмарк должен быть полностью самодостаточным и воспроизводимым. Это отличает TheAgentCompany от существующих бенчмарков, которые не имеют среды выполнения (например, Mind2Web) или требуют использования стороннего программного обеспечения (например, WorkArena).</p>'}, {'title': 'TheAgentCompany Environment Setup', 'content': 'Our benchmark is set in an imaginary software engineering startup called TheAgentCompany, hence the benchmarks name. Within TheAgentCompany, we create tasks inspired by tasks handled by workers inside such companies. More details about the companys imaginary background, overview and employees can be found in Appendix A. The benchmark environment contains multiple components. Local Workspace The local workspace runs locally on the agents host, which is analogous to human professionals local workspace, e.g. their work laptop computer. This environment is created as sandboxed Docker environment to provide safe execution environment that will not affect other parts of the evaluation machine (Wang et al., 2024b).2 This environment is where agents work on the task, and within this environment the TheAgentCompany baseline agent ( 6) uses browser, code editor and Linux terminal with typical software preinstalled.3 Intranet This part of the environment mimics the companys internal websites that host code, documents, project management software, and communications software. To achieve our goal of reproducible, self-contained environment, we follow WebArena (Zhou et al., 2023), in using open-source, self-hostable software to host our environment. The environment mainly contains the following websites: 1. GitLab,4 an open-source alternative to source-code repositories such as GitHub. This is used for hosting TheAgentCompanys code repositories and tech-oriented wiki pages. 2. OwnCloud,5 an open-source alternative to office software such as Google Drive or Microsoft Office. This to save and share files, especially for document storage and collaborative editing. 3. Plane,6 an open-source alternative to task management software such as Jira or Linear. This is used to track issues, run sprints cycles, and manage product roadmaps. 4. RocketChat,7 an open-source alternative to communication software such as Slack. This is company-internal real-time messaging tool that facilitates collaboration between employees. All the websites hosted are reproducible and reset-able with mock data inspired by that from software engineering company. The data inside these company internal websites are populated with real-world software project data, as well as data manually curated by co-authors who have some experience in the relevant corporate roles. Simulated Colleague Communication One major aspect of working in company is communicating with other company members, and in TheAgentCompany we also test the ability of models to perform this type of communication. Specifically, we allow agents to use RocketChat to message other company members and obtain information that may not be available in the original task description. To create these simulated colleagues, we rely on the Sotopia platform (Zhou et al., 2024), which supports the creation of simulated human characters with LLMs. Each simulated colleague is equipped with detailed profile that includes their name, role, responsibilities, and project affiliations. 2https://docs.all-hands.dev/modules/usage/how-to/custom-sandbox-guide 3Other options would include using something like GUI-based desktop environment with office software (Xie et al., 2024), but we opt to build baseline solution that is entirely web-based, reflecting the recent trend of more enterprise software moving to the cloud. 4https://about.gitlab.com/install/ 5https://doc.owncloud.com/ 6https://github.com/makeplane/plane 7https://www.rocket.chat/install 4 Preprint. Domain SWE Table 2: Example task intents and checkpoints for three domains. Task Intent Checkpoints Set up JanusGraph and run it locally with an HTTP endpoint: Clone JanusGraph directory under /workspace folder (http://the-agent-company.com:8929/root/janusgraph). Build the binary file. Launch JanusGraph server locally on port 8182 with an HTTP endpoint. Finance Navigate to ownCloud at http://the-agent-company.com:8092 and complete Section BAlternative Simplified Credit of IRS Form 6765: Gather necessary information from /Documents/Financials/TAC_financials.csv and /workspace/research_wages.csv. Consult /Documents/Financials/f6765_instructions.pdf for instructions. Contact the finance director (David Wong) on Rocket.Chat (http://the-agent-company.com:3000/home) for ambiguous questions. Save the filled form as /workspace/filled_f6765.pdf. Checkpoint 1 (1pt): Check if JanusGraph repo is cloned. Checkpoint 2 (3pts): Check if the binary file is built (requires skipping Docker in pom.xml, hence higher points). Checkpoint 3 (2pts): Check the JanusGraph Server as an HTTP endpoint. Checkpoint 1 (5pts): Check if all 16 questions in Section of the form have been answered correctly. Checkpoint 2 (3pts): Check if the correct finance director (David Wong) was contacted to answer two ambiguous questions. PM Analyze The Agent Companys performance and create summary in Plane: Access Plane (http://the-agent-company.com:8091/tac/) and navigate to "Analytics." Collect metrics: Open Tasks, Backlog Tasks, Unstarted Tasks, Started Tasks, Unassigned Issues, Pending Issues. Create summary and share it on Rocket.Chat (http://the-agent-company.com:3000/home) in the #kudos channel. Checkpoint 1 (1pt): Check if Plane was accessed and the agent navigated to "Analytics" section. Checkpoint 2 (3pts): Check if all required project metrics were collected. Checkpoint 3 (1pt): Check if the summary was shared in the #kudos channel on Rocket.Chat. (e.g., Sarah Johnson, who serves as the CTO, oversees technical strategy planning and R&D team leadership, with access to all technical channels). Agents can interact with these simulated colleagues through direct messages or in specific channels, as is standard in RocketChat and other platforms. By default, all simulated human characters are backed by the Claude-3-5-Sonnet-20241022 LLM across experiments, as we found that it provided the best results during preliminary experiments. For example conversations between the agent and the simulated colleagues drawn from empirical experiments, please refer to Appendix B.', 'summary': '<p>В статье описывается среда для тестирования агентов машинного обучения, имитирующая работу в вымышленной IT-компании TheAgentCompany. Эта среда включает в себя несколько ключевых компонентов:</p>\n<p><strong>1. Локальное рабочее пространство:</strong> Это изолированная Docker-среда, которая имитирует рабочий компьютер сотрудника. В этой среде агент выполняет поставленные задачи, используя браузер, редактор кода и терминал Linux с предустановленным набором программного обеспечения.</p>\n<p><strong>2. Интранет:</strong> Этот компонент имитирует внутренние веб-сайты компании, где размещаются различные ресурсы. Он включает в себя:\n    * <strong>GitLab:</strong> для хранения кода и технической документации.\n    * <strong>OwnCloud:</strong> для хранения и совместного редактирования документов.\n    * <strong>Plane:</strong> для управления задачами, спринтами и планами разработки.\n    * <strong>RocketChat:</strong> для общения между сотрудниками в реальном времени.</p>\n<p>Все эти веб-сайты являются воспроизводимыми и используют открытое программное обеспечение с данными, имитирующими реальные проекты и рабочие ситуации. Данные заполнены как реальными данными из IT-проектов, так и данными, созданными авторами статьи, имеющими опыт работы в соответствующих корпоративных ролях.</p>\n<p><strong>3. Имитация общения с коллегами:</strong> Важной частью работы в компании является общение с другими сотрудниками. В TheAgentCompany агенты могут общаться с имитированными коллегами через RocketChat. Эти "коллеги" созданы с помощью платформы Sotopia и представляют собой LLM-модели с подробными профилями, включающими имя, должность, обязанности и причастность к проектам. Агенты могут обращаться к ним за информацией, которая может отсутствовать в исходном описании задачи. В качестве LLM для имитации коллег используется Claude-3-5-Sonnet-20241022, так как он показал наилучшие результаты в предварительных экспериментах.</p>\n<p>В статье также приводятся примеры задач и контрольных точек для трех областей: разработка программного обеспечения (SWE), финансы и управление проектами (PM). Для каждой задачи указаны действия, которые должен выполнить агент, а также контрольные точки, по которым оценивается его прогресс.</p>'}, {'title': 'Task Structure', 'content': 'The tasks in TheAgentCompany include task intent, list of checkpoints that the agent must achieve, programmatic evaluator to check success on these checkpoints, and code to initialize and finalize the environment. We show some examples in Table 2, and describe each of aspect in detail below. Task Intent Each task begins with an English description, simulating how user would instruct an LLM-based agent to perform real-world task. In general, we aim for these tasks to be clear enough so that human worker would be able to complete the task without asking for further instructions directly from the user (although they may need to ask questions of their other co-workers). Checkpoints Tasks are divided into checkpoints representing intermediate milestones, each assigned point value to measure progress. Each checkpoint is awarded certain number of points based on its significance to the overall completion of the task. Checkpoints are written in English, and typically specify one or more of the following: 5 Preprint. Action Completion: Verifying whether required actions, such as using tools, navigating to URLs, or collecting data, were carried out successfully. Data Accuracy: Evaluating the correctness and completeness of the output, such as extracted data or formatted documents. Collaboration: Assessing interactions with simulated colleagues or sharing of output, such as posting messages or asking for additional information to complete the task. Evaluators Checkpoints are created in the task design phase, but for actual evaluation, each of the checkpoints must be concretely implemented through an evaluator program that checks the completion of the checkpoint. These evaluators are implemented by examining environment states, such as the local workspace, intranet status, simulated colleague interactions, or by analyzing agent trajectories, like verifying browsing history or action sequences. In most cases, these evaluators are deterministic and written as simple Python functions. For instance, in the SWE task in Table 2, the checkpoints are deterministic: verifying if the JanusGraph repository is cloned, the binary file is built, and the server is launched with an HTTP endpoint. However, for tasks with more complex and unstructured deliverables, such as in Table 2, the last checkpoint in the Finance task requires contacting the correct finance director (David Wong) to resolve ambiguous questions, which involves judgment from (simulated) human colleague, deterministic evaluation can be challenging due to subjectivity and variability. In such cases, we employ LLM-based evaluation. This involves prompting LLMs with predefined rubrics or reference outputs to assess the agents deliverables, enabling more nuanced and flexible evaluation of these tasks. Same as the NPC backbone, all LLM-based evaluators are backed by the Claude-3-5-Sonnet-20241022.', 'summary': '<p>В TheAgentCompany задачи включают в себя несколько ключевых компонентов: описание задачи (task intent), список контрольных точек (checkpoints), которые агент должен достичь, программный оценщик (evaluator) для проверки успешности достижения этих точек, а также код для инициализации и завершения среды.</p>\n<p><strong>Описание задачи (Task Intent)</strong></p>\n<p>Каждая задача начинается с текстового описания на английском языке, которое имитирует то, как пользователь мог бы проинструктировать агента, основанного на большой языковой модели (LLM), для выполнения реальной задачи. Цель состоит в том, чтобы описания были достаточно понятными для того, чтобы человек мог выполнить задачу, не запрашивая дополнительных инструкций у пользователя, хотя он может обращаться с вопросами к коллегам.</p>\n<p><strong>Контрольные точки (Checkpoints)</strong></p>\n<p>Задачи разделены на контрольные точки, представляющие собой промежуточные этапы. Каждой точке присваивается определенное количество баллов, отражающее ее важность для общего выполнения задачи. Контрольные точки также описываются на английском языке и обычно включают проверку одного или нескольких аспектов:</p>\n<ul>\n<li><strong>Завершение действий:</strong> Проверка того, были ли успешно выполнены необходимые действия, такие как использование инструментов, переход по URL-адресам или сбор данных.</li>\n<li><strong>Точность данных:</strong> Оценка правильности и полноты выходных данных, таких как извлеченные данные или отформатированные документы.</li>\n<li><strong>Взаимодействие:</strong> Оценка взаимодействия с моделируемыми коллегами или обмена результатами, например, публикация сообщений или запрос дополнительной информации для завершения задачи.</li>\n</ul>\n<p><strong>Оценщики (Evaluators)</strong></p>\n<p>Хотя контрольные точки создаются на этапе проектирования задачи, для фактической оценки каждая из них должна быть реализована через программу-оценщик, которая проверяет ее выполнение. Эти оценщики анализируют состояние среды, например, локальное рабочее пространство, статус интрасети, взаимодействие с моделируемыми коллегами, или траектории агента, проверяя историю просмотров или последовательности действий.</p>\n<p>В большинстве случаев оценщики являются детерминированными и реализованы в виде простых функций на языке Python. Например, в задаче SWE, описанной в таблице 2, контрольные точки детерминированы: проверка клонирования репозитория JanusGraph, сборки исполняемого файла и запуска сервера с HTTP-конечной точкой.</p>\n<p>Однако для задач со сложными и неструктурированными результатами, таких как последняя контрольная точка в задаче Finance (таблица 2), которая требует связи с правильным финансовым директором (David Wong) для разрешения неоднозначных вопросов, детерминированная оценка может быть затруднена из-за субъективности и изменчивости. В таких случаях используется оценка на основе LLM. Это включает в себя использование подсказок для LLM с заранее определенными критериями или эталонными выходными данными для оценки результатов агентов, что позволяет проводить более тонкую и гибкую оценку этих задач. Как и в случае с NPC, все оценщики на основе LLM используют Claude-3-5-Sonnet-20241022.</p>'}, {'title': 'Result', 'content': '+ 0.5 Sfull, where: Result: Sum of awarded points across all checkpoints (including partial credit), Total: Sum of the total points for all checkpoints, Result Total : Fractional progress toward full completion, Sfull: Binary indicator equal to 1 when the task is fully completed. This formulation ensures that agents are awarded partial credit in proportion to the points achieved, reflecting their progress toward task completion. At the same time, full task completion is strongly incentivized by incorporating an additional 50% credit, which is awarded only when all checkpoints are successfully completed. This design ensures that agents achieving partial progress receive scores scaled linearly with their performance, while those reaching 100% completion are distinctly rewarded to emphasize the importance of achieving the end goal. Preprint. Figure 2: Example TheAgentCompany workflow illustrating an agent managing sprint for the RisingWave project. The task involves identifying and moving unfinished issues to next sprint cycle, notifying assignees of those issues, running code coverage script, uploading summarized report to OwnCloud, and incorporating feedback on report from simulated project manager. Number of steps The number of steps is defined as the total number of LLM calls made during the task execution. This metric quantifies the operational effort required to perform the task. Cost per instance The cost per instance measures the monetary cost of querying the underlying LLM through its API to complete task. Assuming no prompt caching, the cost is calculated as: Cost = (Prompt token countPrompt token cost)+(Completion token countCompletion token cost). This efficiency metric reflects the computational expense of task completion based on token usage.', 'summary': '<p>В данной статье предлагается метрика оценки работы агентов, выполняющих задачи, состоящие из нескольких этапов. Эта метрика учитывает как частичное выполнение задачи, так и полное её завершение. </p>\n<p>Оценка рассчитывается по формуле: <strong>Результат = Сумма набранных баллов + 0.5 * Sfull</strong>, где:</p>\n<ul>\n<li><strong>Сумма набранных баллов</strong> – это сумма баллов, полученных за выполнение отдельных этапов (чекпоинтов) задачи. Если этап выполнен не полностью, начисляется пропорциональный балл.</li>\n<li><strong>Sfull</strong> – это бинарный индикатор, равный 1, если задача выполнена полностью (все чекпоинты пройдены), и 0 в противном случае.</li>\n</ul>\n<p>Таким образом, агенты получают частичную оценку, пропорциональную их прогрессу, но при этом полное выполнение задачи дополнительно стимулируется 50% бонусом. Это мотивирует агентов не только стремиться к выполнению отдельных этапов, но и к полному завершению задачи.</p>\n<p>В качестве примера приводится рабочий процесс агента TheAgentCompany, управляющего спринтом проекта RisingWave. Задача включает в себя:\n*   Перенос незавершенных задач в следующий спринт.\n*   Уведомление ответственных за эти задачи.\n*   Запуск скрипта покрытия кода.\n*   Загрузку отчета в OwnCloud.\n*   Учет обратной связи от имитированного менеджера проекта.</p>\n<p>Также в статье вводятся две дополнительные метрики:</p>\n<ul>\n<li><strong>Количество шагов</strong> – общее число вызовов языковой модели (LLM) во время выполнения задачи. Эта метрика показывает, сколько усилий потребовалось агенту для решения задачи.</li>\n<li><strong>Стоимость экземпляра</strong> – денежная стоимость запросов к LLM API для выполнения задачи. Стоимость рассчитывается как сумма стоимости токенов в запросе и токенов в ответе. Эта метрика оценивает вычислительные затраты, связанные с использованием языковой модели. Предполагается, что кэширование запросов не используется.</li>\n</ul>\n<p><strong>Комментарий:</strong> Метрика оценки, которая стимулирует как частичное, так и полное выполнение задачи, является важным аспектом в обучении агентов. Дополнительные метрики, такие как количество шагов и стоимость экземпляра, позволяют оценить эффективность работы агента с точки зрения вычислительных ресурсов.</p>'}, {'title': 'Workflow', 'content': 'Each task typically follows workflow involving the following stages: 1. Initialization: The agent sets up its workspace and prepares to execute the task. 2. Execution: The agent completes subtasks, such as navigating tools, collecting data, or processing information or if required by the task, the agent interacts with simulated colleagues or shares results via communication platforms. 3. Finalization: The agent produces and submits the final output for evaluation. Example Task We consider task designed to evaluate an agents ability to perform realistic project management workflows using multiple tools and services hosted in the benchmark. The task involves managing sprint for the RisingWave project, requiring the agent to execute interdependent steps such as sprint issue management, team communication, repository operations, and report generation while incorporating feedback from simulated project manager. The workflow as illustrated in Figure 2 begins with the agent identifying unfinished issues in the current sprint on Plane and updating their sprint assignments. This step is worth 2 points and is fully completed, earning the agent the maximum score of 2/2. Next, the agent successfully notifies the relevant assignees using Rocket.Chat regarding their pending tasks and earns 1/1 point. The agent then proceeds to clone the RisingWave repository from GitLab and execute Python script in the terminal to calculate updated code coverage. This step, worth 2 points, is only partially completed, as the agent successfully clones the repository but fails to run code coverage. As result, the agent earns 1/2 points for this checkpoint. The subsequent stepsgenerating and sharing the sprint summary report on OwnCloud and incorporating feedback from simulated project managerare not completed, resulting in 0/2 and 0/1 scores, respectively. Notably, the checkpoints can also fail if the report does not meet quality standards as assessed by the LLM-based evaluator, which evaluates the report for clarity, completeness, and successful incorporation of feedback. This ensures that the assessment reflects both the generation of outputs and their qualitative relevance to the task. 7 Preprint. Finally, the overall score is calculated using the partial completion formula defined in 4.1, where the total possible points are 8, and the awarded points sum to 4. Substituting these values, the agent achieves final score of 0.25 (25%). Our scoring mechanism thus rewards incremental progress while strongly incentivizing full completion. This example represents typical task in the TheAgentCompany benchmark, where agents are required to handle complex workflows involving multiple tools and interdependent steps. By evaluating both partial progress and overall outcomes, our benchmark provides rigorous and realistic measure of agent performance, allowing us to identify their strengths and pinpoint areas for improvement in task execution.', 'summary': '<p>В типичной задаче, выполняемой агентом, можно выделить три основных этапа:</p>\n<ol>\n<li><strong>Инициализация:</strong> Агент подготавливает рабочее пространство и настраивается для выполнения задачи.</li>\n<li><strong>Выполнение:</strong> Агент выполняет подзадачи, например, использует инструменты, собирает данные, обрабатывает информацию. При необходимости, агент взаимодействует с симулированными коллегами или делится результатами через платформы для общения.</li>\n<li><strong>Завершение:</strong> Агент формирует и отправляет финальный результат для оценки.</li>\n</ol>\n<p>Рассмотрим пример задачи, которая оценивает способность агента управлять проектом, используя различные инструменты и сервисы. Задача заключается в управлении спринтом проекта RisingWave. Агенту нужно выполнить ряд взаимозависимых действий, таких как: управление задачами спринта, общение с командой, работа с репозиторием и создание отчёта. При этом агент должен учитывать обратную связь от симулированного менеджера проекта.</p>\n<p>Рабочий процесс начинается с того, что агент находит незавершённые задачи текущего спринта в Plane и обновляет их привязку к спринту. Этот шаг оценивается в 2 балла и был полностью выполнен, поэтому агент получает 2/2 балла. Далее, агент успешно уведомляет назначенных исполнителей о незавершённых задачах через Rocket.Chat, получая 1/1 балл. Затем агент клонирует репозиторий RisingWave из GitLab и запускает Python-скрипт для расчёта покрытия кода. Этот шаг оценивается в 2 балла, но выполнен лишь частично: агент успешно клонирует репозиторий, но не запускает скрипт. В итоге, агент получает 1/2 балла за этот этап. Последующие шаги – создание и публикация отчёта о спринте в OwnCloud и учёт обратной связи от симулированного менеджера – не были выполнены, поэтому агент получает 0/2 и 0/1 балла соответственно. Важно отметить, что этапы могут быть не засчитаны, если отчёт не соответствует стандартам качества, которые оценивает LLM-модель. Она проверяет отчёт на ясность, полноту и учёт обратной связи, что гарантирует оценку не только генерации результатов, но и их качественной релевантности задаче.</p>\n<p>Итоговый балл рассчитывается с использованием формулы частичного выполнения. В данном примере, общее количество возможных баллов – 8, а набранных – 4.  Подставив эти значения, агент получает итоговый балл 0.25 (25%). Таким образом, система оценки поощряет поэтапное выполнение задачи, но при этом мотивирует к полному её завершению.</p>\n<p>Этот пример типичен для задач в бенчмарке TheAgentCompany, где агенты должны справляться со сложными процессами, включающими использование нескольких инструментов и взаимозависимые шаги. Оценивая как частичное выполнение, так и итоговые результаты, бенчмарк предоставляет точную и реалистичную меру производительности агента, позволяя выявить сильные стороны и области для улучшения.</p>'}, {'title': 'Choosing Task Categories', 'content': 'Many previous agent benchmarks discussed in 2 were created to evaluate agents on tasks people perform in daily life (Zhou et al., 2023; Lù et al., 2024; Deng et al., 2023), or tasks that accomplish digital chores (Yoran et al., 2024; Trivedi et al., 2024). Obtaining realistic tasks for the benchmark poses challenges. Some benchmark (Xie et al., 2024; Drouin et al., 2024; Yoran et al., 2024) crowdsourced tasks based on predetermined interfaces, platforms, and services available to the agent. They also adopt strategy to first gather task templates and then instantiate more task instances by filling in the variables. Some benchmark (Zhou et al., 2023; Koh et al., 2024; Bonatti et al., 2024) took semi-systematic approach of reviewing the action history of the research team and choosing tasks that reflected the types of task that the researchers carried out in their daily life. There are several obvious issues with this if we want to evaluate agents with broader implications in the TheAgentCompany benchmark. Despite some grounding in realistic data, the process of creating tasks from these data was susceptible to heuristic, and no consideration was made for how important or time-consuming the tasks are. The tasks are biased towards those important for academics in computer science and do not reflect the tasks performed by the entire population. In TheAgentCompany, we attempt to cover wide variety of tasks motivated by real-world work. While it is highly challenging to create representative sample of tasks, fortunately we can rely on existing resources created for other purposes as reference. Specifically, we start by referencing the 29.1 release of O*NET database (O*NET, 2024; Rounds et al., 1999), which is database of jobs performed by workers in the US created by the US Department of Labor. It also contains information about tasks performed within the context of each job, abilities required to perform each task, whether the task is major or minor task for that job category, and other pieces of relevant information. Based on this data, we first identified few categories of occupation categories to focus on. First, based on statistics from O*NET, we identified job categories that have large number of people performing this job. Then, we used median salary information for each of these job categories from the US department of labor statistics, and multiplied the number of employees in that category to estimate the aggregate value of performing this job. Based on this, we identified several categories of jobs such as General and Operations Managers, Registered Nurses, Software Developers, and Financial Managers that have both high population and high average salary. Because TheAgentCompany is designed to be non-embodied benchmark in the digital domain, we excluded the categories that require extensive physical labor such as Registered Nurses, and eventually settled on the setting of software company, which would allow us to cover tasks from the other categories.', 'summary': '<p>Предыдущие исследования для оценки агентов в основном использовали задачи, имитирующие повседневные дела или цифровые рутинные операции. Однако создание реалистичных задач для таких тестов оказалось непростой задачей.</p>\n<p>Некоторые исследования использовали краудсорсинг, где задачи формировались на основе заранее определенных интерфейсов и платформ. В других случаях сначала собирали шаблоны задач, а затем создавали их экземпляры, заполняя переменные. Другие подходы основывались на анализе истории действий исследователей, выбирая задачи, которые они выполняли в своей повседневной работе.</p>\n<p>Однако у этих методов есть недостатки. Процесс создания задач часто был подвержен эвристике, и не учитывалась важность или трудоемкость задач. Кроме того, такие задачи были смещены в сторону задач, важных для ученых в области компьютерных наук, и не отражали задачи, выполняемые всем населением.</p>\n<p>В отличие от этого, в TheAgentCompany ставится цель охватить широкий спектр задач, мотивированных реальной работой. Для этого в качестве отправной точки используется база данных O*NET, содержащая информацию о профессиях и задачах, выполняемых в США.</p>\n<p>Сначала были определены категории профессий, на которых стоит сосредоточиться. Выбор основывался на двух критериях: количество людей, занятых в данной профессии, и общая ценность, которую они создают (рассчитанная как произведение количества работников на медианную зарплату). Таким образом были выбраны такие категории, как генеральные и операционные менеджеры, зарегистрированные медсестры, разработчики программного обеспечения и финансовые менеджеры.</p>\n<p>Поскольку TheAgentCompany ориентирована на цифровые задачи, категории, требующие значительного физического труда, такие как медсестры, были исключены. В итоге было решено сосредоточиться на задачах в контексте программной компании, что позволило охватить задачи из других выбранных категорий.</p>'}, {'title': 'Choosing Tasks', 'content': 'Next, within this setting we chose tasks to implement. In this setting, we attempted to create diversity of tasks, but mostly focused on concrete tasks that have well-defined goals and success criteria. These tasks were created through combination of referencing the O*NET task list, introspection based on paper co-authors who had experience in each task category, and brainstorming lists with language models. It is important to note that in no cases have we covered an extensive list of all the tasks that are performed in particular occupational category, and therefore we caution against making any assumptions about whether particular job may be in danger of full automation based solely on 8 Preprint. Figure 3: Overview of OpenHands default CodeAct + Browsing agent architecture, the baseline agent used throughout the experiments. TheAgentCompany. Rather, it may provide insight into whether certain tasks within jobs may be accelerated or automated, and inform further analysis by labor professionals into this question.', 'summary': '<p>В этом разделе мы расскажем о задачах, которые мы выбрали для реализации в рамках нашего исследования. Мы стремились к разнообразию задач, но в основном сосредоточились на конкретных задачах с четко определенными целями и критериями успеха. </p>\n<p>Эти задачи были сформулированы на основе трех источников: списка задач O*NET (база данных о профессиях и навыках), личного опыта соавторов статьи, имеющих опыт в каждой категории задач, и мозгового штурма с использованием языковых моделей.</p>\n<p>Важно отметить, что мы не ставили целью охватить исчерпывающий список всех задач, выполняемых в каждой профессиональной категории. Поэтому мы предостерегаем от поспешных выводов о том, находится ли конкретная работа под угрозой полной автоматизации, основываясь исключительно на результатах, полученных в рамках нашего исследования.</p>\n<p>Скорее, наше исследование может дать представление о том, могут ли определенные задачи в рамках профессий быть ускорены или автоматизированы. Это может стать отправной точкой для дальнейшего анализа специалистами в области труда, которые смогут глубже изучить этот вопрос.</p>\n<p><em>Комментарий: Здесь авторы подчеркивают, что их исследование не является исчерпывающим анализом автоматизации профессий, а лишь показывает потенциал автоматизации конкретных задач внутри них.</em></p>'}, {'title': 'Manual Task Curation', 'content': 'Once we set up the environment required for our desired jobs and task categories ( 3), we return to the curated list, and perform manual curation process for tasks. For each task, this consists of the following steps: We first create description of task intent, checkpoints, and how to evaluate each checkpoint. We then identify and import the required data for the task that are currently missing in the company Intranet services and create any necessary data. We then write scripts to configure the required initialization state in the local workspace. Finally, we implement the checkpoint evaluators that calculate the scalar scores for each checkpoint. All tasks were created by coauthors of the paper. Overall, it took 20 computer science students, software engineers, and project managers over 2 months, consuming approximately 3,000 personhours in total. Some of the more complex tasks take more than 10 hours each to design, implement, test, and verify. To ensure quality control of the task creation process, we implement several check and verification processes. For each task implementation, we require screenshot proof that the evaluator is valid and that the task is able to get full score when successfully completed. We also encourage including tests for the implemented evaluator programs. Each task contribution is also code reviewed by panel of lead authors before merging into the benchmark. After creating all tasks, final round of manual human double-check of required environment data, evaluator behavior, and checkpoint scoring for every task is performed to ensure quality. Notably, during the process person who has not curated the tasks checks all the checkpoint score assignments to make sure that the importance scoring is consistent over all the tasks and that it correlates reasonably with the relative importance of the checkpoint within the task.', 'summary': '<p>После того, как было настроено окружение, необходимое для выполнения задач (как описано в разделе 3), команда вернулась к списку задач и приступила к их ручной подготовке. Для каждой задачи процесс включал следующие шаги:</p>\n<ol>\n<li><strong>Описание задачи:</strong> Сначала создавалось подробное описание цели задачи, определялись контрольные точки (чекпоинты) и методы оценки прохождения каждого чекпоинта.</li>\n<li><strong>Подготовка данных:</strong> Затем определялись и импортировались необходимые данные, которых не хватало во внутренней сети компании. Если требовалось, создавались новые данные.</li>\n<li><strong>Настройка окружения:</strong> Далее писались скрипты для настройки начального состояния рабочего пространства, необходимого для выполнения задачи.</li>\n<li><strong>Реализация оценщиков:</strong> Наконец, разрабатывались программы-оценщики, которые вычисляли числовые оценки для каждого чекпоинта.</li>\n</ol>\n<p>Все задачи были разработаны соавторами статьи. На весь процесс ушло более двух месяцев и около 3000 человеко-часов работы 20 студентов-информатиков, инженеров-программистов и менеджеров проектов. Разработка, реализация, тестирование и проверка некоторых сложных задач занимали более 10 часов каждая.</p>\n<p>Для обеспечения качества процесса создания задач были внедрены несколько этапов проверки. Для каждой задачи требовалось предоставить скриншот, подтверждающий корректность работы оценщика и возможность получения максимального балла при успешном выполнении задачи. Также поощрялось написание тестов для оценщиков. Каждый вклад в виде новой задачи проходил код-ревью у ведущих авторов перед включением в бенчмарк (набор задач).</p>\n<p>После создания всех задач проводилась финальная проверка вручную, где проверялись данные окружения, поведение оценщиков и корректность выставления баллов за каждый чекпоинт. Особое внимание уделялось проверке соответствия важности чекпоинтов их оценкам. Это делал человек, который не участвовал в подготовке задач, чтобы обеспечить объективность оценки важности. Он проверял, чтобы оценки за чекпоинты были согласованы между всеми задачами и соответствовали их относительной важности.</p>'}, {'title': 'Baseline Agent', 'content': 'To test the current state-of-the-art performance on the TheAgentCompany benchmark, we need agents that can at least perform tasks using browser, operate local workspace using terminal, and write and execute programs to perform most of the tasks. Throughout this paper, we experiment with OpenHands main agent (Wang et al., 2024b;a; Song et al., 2024), CodeAct Agent with Browsing.8 An overview of the agent architecture is illustrated in Figure 3. Interfaces The agent can interact with the environment through 3 interfaces. (1) bash shell that connects with the local workspace operating system environment for command execution. (2) Jupyter IPython server to handle interactive python (IPython) code execution requests and return the execution results back. (3) Chromium browser based on Playwright. The provider 8More specifically, version 0.14.2. Full details can be found in https://github.com/All-Hands-AI/OpenHands/ tree/main/openhands/agenthub/codeact_agent 9 Preprint. provides set of action primitives defined by BrowserGym (ServiceNow; Drouin et al., 2024), such as navigation, clicking, typing, and scrolling. After executing these actions, the browser runtime provides rich set of observations about the current state of the browser, including HTML, DOM, accessibility tree (Mozilla), screenshot, opened tabs, etc. These observations can be also augmented with configurable attributes that could allow agents to better understand web page observations, such as using set-of-marks on screenshot (Yang et al., 2023; He et al., 2024), visible element marking, focused element, interactable element marking, in-viewport element filtering (Zhou et al., 2023), etc. Actions The agent connects with the environment through core set of general actions. Actions IPythonRunCellAction and CmdRunAction enable the agent to execute arbitrary Python code and bash commands inside the sandbox environment (e.g., secure isolated Linux operating system used as our local workspace). BrowserInteractiveAction enables interaction with web browser with domain-specific language for browsing introduced by BrowserGym (Chezelles et al., 2024; Drouin et al., 2024). These actions provide comprehensive, yet flexible set of primitives that cover most of the tasks performed by human employees of TheAgentCompany, including navigation, click, hovering, and typing, etc. Observations Observations describe the environmental changes that the agent observes. The main types of observations used in the CodeAct agent include the execution result of bash terminal commands, Python programs, and browser actions. Specifically, the execution result of browser actions is usually browser snapshots and textual representation in the form of accessibility tree of the current browser viewport. Workflow At each step, the underlying backbone LLM will take in prompts consisting of previous agent history and the current observation of the environment, and generate response consisting of the action to execute next. On higher level, the agent can perform the task by executing code, including executing bash commands, Python code, or browser-specific programming language (defined in BrowserGym).9 This general action space allows the agent to perform various tasks, including editing files, browsing the Web, running programs, etc.', 'summary': '<p>В данной работе для оценки производительности современных агентов на бенчмарке TheAgentCompany используются агенты, способные выполнять задачи с помощью браузера, работать в локальном рабочем пространстве через терминал, а также писать и запускать программы. Основное внимание уделяется агенту OpenHands и CodeAct Agent with Browsing.</p>\n<p><strong>Архитектура агента</strong></p>\n<p>Агент взаимодействует с окружением через три интерфейса:\n1.  <strong>Bash shell:</strong> Обеспечивает связь с операционной системой локального рабочего пространства для выполнения команд.\n2.  <strong>Jupyter IPython server:</strong> Позволяет выполнять интерактивный код Python (IPython) и получать результаты выполнения.\n3.  <strong>Chromium browser (на основе Playwright):</strong> Агент использует набор примитивов действий, предоставляемых BrowserGym, таких как навигация, клики, ввод текста и прокрутка. После выполнения этих действий браузер предоставляет подробные наблюдения о своем текущем состоянии, включая HTML, DOM, дерево доступности, скриншот, открытые вкладки и т.д. Эти наблюдения могут быть дополнены настраиваемыми атрибутами, например, метками на скриншоте, выделением видимых или интерактивных элементов, фильтрацией элементов в пределах видимой области и т.д.</p>\n<p><strong>Действия</strong></p>\n<p>Агент взаимодействует с окружением через набор основных действий:\n*   <strong>IPythonRunCellAction:</strong> Позволяет выполнять произвольный код Python в изолированной среде.\n*   <strong>CmdRunAction:</strong> Позволяет выполнять команды bash в изолированной среде.\n*   <strong>BrowserInteractiveAction:</strong> Обеспечивает взаимодействие с веб-браузером с помощью специализированного языка для просмотра веб-страниц, определенного в BrowserGym. Этот набор действий предоставляет гибкие примитивы, покрывающие большинство задач, выполняемых сотрудниками TheAgentCompany, включая навигацию, клики, наведение курсора и ввод текста.</p>\n<p><strong>Наблюдения</strong></p>\n<p>Наблюдения описывают изменения в окружении, которые агент может воспринимать. Основные типы наблюдений, используемые в агенте CodeAct, включают результаты выполнения команд bash, программ Python и действий браузера. В частности, результатом действий браузера обычно являются снимки браузера и текстовое представление в виде дерева доступности текущей видимой области браузера.</p>\n<p><strong>Рабочий процесс</strong></p>\n<p>На каждом шаге базовая большая языковая модель (LLM) получает на вход историю предыдущих действий агента и текущее наблюдение за окружением и генерирует ответ, содержащий действие, которое нужно выполнить следующим. На более высоком уровне агент выполняет задачу путем выполнения кода, включая команды bash, код Python или специализированный язык для браузера (определенный в BrowserGym). Это общее пространство действий позволяет агенту выполнять различные задачи, такие как редактирование файлов, просмотр веб-страниц, запуск программ и т.д.</p>'}, {'title': 'Result Overview', 'content': 'Table 3 shows the evaluation results of both closed and open foundation models on the full evaluation set of TheAgentCompany (175 tasks). We can see that the Claude-3.5-Sonnet is the clear winner across all models. However, even with the strongest frontier model, it only manages to complete 24% of the total tasks and achieves score of 34.4% taking into account partial completion credits. Note that this result comes at cost: It requires an average of almost 30 steps and more than $6 to complete each task, making it the most expensive model to run both in time and in cost. This is expected as most of the tasks in our benchmark are of long-horizon nature. The Gemini 2.0 Flash model that comes second in terms of capability requires 40 steps on average to complete the tasks, which is time consuming, yet only to achieve less than half the success rate compared to the top-performing model. Surprisingly, its cost is less than $1, making it very cost-efficient yet relatively strong model. qualitative examination demonstrated that this was due to instances where the agent got stuck in loop or aimlessly explored the environment. Among the open-weight models, Llama 3.1 (405B) achieves the highest performance, nearly on par with OpenAIs GPT-4o model, though still having big gap behind the leading Claude 3.5 Sonnet. 9https://github.com/ServiceNow/BrowserGym/blob/main/browsergym/core/src/browsergym/core/action/ functions.py 10 Preprint. Table 3: Performance comparison of various foundation models on TheAgentCompany. Model Success Score Steps Costs API-based Models Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 24.0% 34.4% 29.17 11.4% 19.0% 39.85 8.6% 16.7% 14.55 8.0% 22.10 3.4% 5.7% 19.59 1.7% Open-weights Models Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 7.4% 14.1% 22.95 6.9% 12.8% 20.93 5.7% 11.8% 23.99 6.5% 19.18 1.7% 4.2% 23.70 1.1% $6.34 $0.79 $1.29 $6.78 $1. $3.21 $0.93 $1.53 $0.83 $0.28 Table 4: Performance of the models in tasks that require different platforms in TheAgentCompany. All numbers are percentages (%). Model Success (%) Score (%) Success (%) Score (%) Success (%) Score (%) Success (%) Score (%) GitLab (71 tasks) Plane (17 tasks) RocketChat (79 tasks) ownCloud (70 tasks) Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 30.99 11.27 11.27 2.82 2. 5.63 8.45 5.63 1.41 1.41 40.25 18.21 19.46 3.88 7.22 11.84 14.26 11.33 6.09 1.94 API-based Models 41.18 17.65 23.53 5.88 5.88 50.37 29.84 33.68 14.05 16. Open-weights Models 29.41 11.76 11.76 5.88 5.88 39.12 21.65 23.56 15.35 12.45 21.52 13.92 5.06 3.80 1.27 8.86 5.06 5.06 2.53 0.00 34.68 23.34 16.08 10.97 5. 16.46 12.06 12.60 8.23 4.88 10.00 2.86 1.43 0.00 0.00 0.00 0.00 0.00 0.00 0.00 21.81 8.52 7.76 4.22 2.43 4.45 3.76 4.14 3.32 2.60 Interestingly, comparing the number of steps and costs between the open Llama 3.1 (405B) model and the closed OpenAI GPT-4o model, Llama 3.1 takes more steps and costs nearly 2x more to run, while having lower success than GPT-4o. Anecdotally, our inspection showed that GPT-4o seems to be better at giving up early, saving steps and costs if the task is clearly out of the capacity range of the agent. This suggests that open-weight models are not always the most cost-effective choice in agents given serving cost, especially with highly complex tasks. On the other hand, the newer generation of Llama model, Llama 3.3 (70B) achieves considerably high performance of 6.9% success rate, on par with the much larger (405B), older generation (Llama 3.1) model. This model also costs significantly less because of its smaller size. This suggests promising future for LLM development, as smaller and more efficient models begin to catch up in agent performance.', 'summary': '<p>В таблице 3 представлены результаты оценки различных моделей, как закрытых, так и открытых, на полном наборе из 175 задач TheAgentCompany. Claude-3.5-Sonnet показала наилучшие результаты среди всех моделей, однако даже эта передовая модель смогла выполнить лишь 24% от общего числа задач, набрав 34.4% с учётом частичного выполнения. Стоит отметить, что такой результат достигается высокой ценой: в среднем требуется около 30 шагов и более 6 долларов на выполнение каждой задачи, что делает эту модель самой дорогой как по времени, так и по стоимости. Это ожидаемо, поскольку большинство задач в этом бенчмарке имеют длительный горизонт. Модель Gemini 2.0 Flash, которая занимает второе место по производительности, требует в среднем 40 шагов для выполнения задач, что отнимает много времени, но при этом её показатель успешности почти вдвое ниже, чем у лидера. Удивительно, но её стоимость составляет менее 1 доллара, что делает её очень экономичной, но при этом достаточно мощной моделью. Качественный анализ показал, что это связано с ситуациями, когда агент зацикливался или бесцельно исследовал среду. Среди моделей с открытым исходным кодом Llama 3.1 (405B) достигает наивысшей производительности, почти наравне с моделью GPT-4o от OpenAI, хотя и отстает от лидера Claude 3.5 Sonnet.</p>\n<p>Интересно, что при сравнении количества шагов и стоимости между открытой моделью Llama 3.1 (405B) и закрытой GPT-4o от OpenAI, Llama 3.1 требует больше шагов и стоит почти в 2 раза дороже, при этом показывая меньшую успешность, чем GPT-4o. Как показал анализ, GPT-4o, похоже, лучше умеет вовремя сдаваться, экономя шаги и затраты, если задача явно выходит за рамки возможностей агента. Это говорит о том, что модели с открытым исходным кодом не всегда являются наиболее экономически эффективным выбором для агентов, особенно при выполнении сложных задач. С другой стороны, новое поколение модели Llama, Llama 3.3 (70B), достигает достаточно высокой производительности с показателем успеха в 6.9%, что сопоставимо с гораздо более крупной (405B) моделью предыдущего поколения (Llama 3.1). Эта модель также стоит значительно меньше из-за своего меньшего размера. Это говорит о многообещающем будущем в развитии LLM, поскольку более мелкие и эффективные модели начинают догонять по производительности агентов.</p>\n<p>В таблице 4 представлена производительность моделей в задачах, требующих различных платформ в TheAgentCompany. Claude-3.5-Sonnet показывает самые высокие результаты на всех платформах, но Gemini 2.0 Flash также имеет относительно хорошие показатели, особенно на платформах RocketChat и Plane. Открытые модели, такие как Llama 3.1 (405B), Llama 3.3 (70B) и Qwen-2.5-72b, показывают более низкую производительность по сравнению с закрытыми моделями на всех платформах.</p>'}, {'title': 'Analysis', 'content': 'How well do agents operate on different platforms? Table 4 presents performance breakdown on tasks that involve different platforms in TheAgentCompany. task is categorized under platform if one of the platforms that the task requires it. From Figure 4a, we can see that most models struggle with RocketChat and ownCloud. RocketChat platform is where all the social interaction with peers happens, and the low performance on this platform suggests that current-day LLMs still need improvements in communicating with others. ownCloud platform provides online Office suite functionality, and due to the complexity of the UI of web-based Office software, it is expected that current LLMs fail badly on the platform. This suggests that the browsing capability of the agents, especially on more complex websites, still needs improvement. These results underscore the inherent challenges and complexities of performing tasks that occur in real-world work environments, involve social interaction, or require understanding and navigating complex web interfaces. 11 Preprint. Table 5: Performance of various models in tasks with different nature in TheAgentCompany. All numbers are percentages (%). Model SDE (69 tasks) Score Success PM (28 tasks) DS (14 tasks) Success Score Success Score Admin (15 tasks) Score Success HR (29 tasks) Success Score Finance (12 tasks) Score Success Other (8 tasks) Score Success Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 30.43 13.04 13.04 4.35 2.90 5.80 11.59 7.25 1.45 2.90 38.02 18.99 19.18 5.64 6.07 11.33 16.49 11.99 4.77 3. 35.71 17.86 17.86 3.57 3.57 21.43 7.14 10.71 3.57 0.00 51.31 31.71 32.27 13.19 12.54 35.62 19.83 22.90 15.16 7.44 API-based Models 14.29 0.00 0.00 0.00 0. 21.70 6.49 4.70 4.82 3.27 0.00 6.67 6.67 6.67 0.00 11.59 15.20 13.89 9.92 0.00 Open-weights Models 0.00 0.00 0.00 0.00 0.00 5.42 4.70 5.42 5.42 4. 0.00 0.00 0.00 0.00 0.00 3.33 1.67 2.14 2.42 0.56 24.14 17.24 0.00 3.45 0.00 6.90 6.90 6.90 3.45 0.00 34.49 23.08 8.28 11.42 4.27 12.56 11.38 12.36 7.19 4. 8.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25.17 4.31 7.36 2.78 2.78 5.00 5.69 7.15 3.82 3.61 12.50 0.00 0.00 0.00 0.00 12.50 0.00 0.00 0.00 0. 22.40 10.05 10.78 8.07 2.86 17.45 7.03 5.99 2.86 4.95 (a) Success rate across platforms (b) Success rate across task categories Figure 4: Comparing agent success rate across platforms (left) and task categories (right). How well do agents perform on different type of tasks? Table 5 presents performance breakdown on different types of tasks in TheAgentCompany. According to the nature of the task, i.e. what kind of professionals are usually assigned with the task, the tasks in TheAgentCompany can be categorized into several job departments: Software Development Engineering (SDE), Project Management (PM), Data Science (DS), Administrative (Admin), Human Resources (HR), Financial (Finance) and all the remaining (Other). From the success rate demonstrated in Figure 4b, we can see that data science, administrative, and finance tasks are among the lowest, with many LLMs completing none of the tasks successfully, and even the strongest Claude model achieving much less than the rest of the tasks. On the other hand, software engineering tasks, which may seem like much harder tasks for many humans, result in higher success rate. This suggests that there exists gap between the perceived difficulty of the tasks for humans versus the difficulty for LLM agents. For example, some tasks in the administrative and finance category involves making spreadsheets, collecting and filling in lot of information from various people, or reading and understanding images scanned by employees. These tasks are arguably easier conceptually for humans in terms of professional skill sets than software engineering, as SDE jobs usually have higher barrier of entry and more prerequisites for certain knowledge. However, most LLMs achieve much higher score on the SDE tasks. However, LLMs fail these seemingly easier tasks due to lack of ability to understand documents, communicate with other people, navigate complex software and tedious processes, and autonomously automate repetitive tasks. We hypothesize that part of the reason lies in the fact that current LLM development is heavily based on software engineering abilities, such as coding, due to several high profile benchmarks that measure this capability (e.g. HumanEval, SWE-Bench) as well as the abundance of publicly available training data related to software. On the other hand, administrative and financial tasks, are usually private data within companies, not readily available for training LLMs. 12 Preprint.', 'summary': '<p><strong>Анализ производительности агентов на различных платформах и типах задач</strong></p>\n<p>В данном разделе рассматривается, как хорошо агенты справляются с задачами на разных платформах и с задачами разного типа.</p>\n<p><strong>Производительность на различных платформах</strong></p>\n<p>Анализ производительности агентов на разных платформах показал, что большинство моделей испытывают трудности с платформами RocketChat и ownCloud. RocketChat используется для социального взаимодействия между сотрудниками, и низкая производительность на этой платформе указывает на то, что современным большим языковым моделям (LLM) все еще нужно совершенствоваться в общении с другими. Платформа ownCloud предоставляет функциональность онлайн-офисного пакета, и из-за сложности пользовательского интерфейса веб-приложений, LLM плохо справляются с этой платформой. Это говорит о том, что возможности браузинга агентов, особенно на сложных веб-сайтах, требуют улучшения. Эти результаты подчеркивают проблемы и сложности, связанные с выполнением задач, которые происходят в реальных рабочих условиях, включают социальное взаимодействие или требуют понимания и навигации по сложным веб-интерфейсам.</p>\n<p><strong>Производительность на задачах различного типа</strong></p>\n<p>Задачи в TheAgentCompany были разделены на категории в соответствии с профессиональными обязанностями: разработка программного обеспечения (SDE), управление проектами (PM), анализ данных (DS), административные (Admin), управление персоналом (HR), финансы (Finance) и другие (Other).</p>\n<p>Анализ показал, что задачи, связанные с анализом данных, административные и финансовые задачи, оказались самыми сложными для LLM. Многие модели не смогли успешно выполнить ни одной задачи из этих категорий, даже самая сильная модель Claude показала результаты значительно ниже, чем в других категориях. С другой стороны, задачи по разработке программного обеспечения, которые могут показаться более сложными для людей, имели более высокий процент успешного выполнения. Это говорит о том, что существует разрыв между субъективной сложностью задач для людей и сложностью для LLM-агентов.</p>\n<p>Например, некоторые административные и финансовые задачи включают работу с электронными таблицами, сбор и заполнение информации от разных людей, а также чтение и понимание изображений, отсканированных сотрудниками. Эти задачи, возможно, концептуально проще для людей, чем разработка программного обеспечения, поскольку для SDE-задач обычно требуется более высокий уровень знаний и навыков. Однако большинство LLM показывают гораздо более высокие результаты именно в задачах SDE.</p>\n<p>LLM не справляются с "более легкими" задачами из-за недостатка способностей понимать документы, общаться с другими людьми, ориентироваться в сложных программах и автоматизировать рутинные процессы. Авторы предполагают, что это может быть связано с тем, что текущая разработка LLM сильно ориентирована на навыки программирования из-за большого количества общедоступных обучающих данных и наличия бенчмарков, измеряющих именно эти навыки. В то же время, административные и финансовые данные, как правило, являются конфиденциальными внутри компаний и не доступны для обучения LLM.</p>'}, {'title': 'Common Agent Failures', 'content': 'Overall, the agent performance on TheAgentCompany is still low and majority of tasks are failed. Among those, we try to find some common and interesting agent mistakes that are often surprising because they are usually not made by humans. Lack of commonsense Some tasks are failed because the agent lacks the common sense and domain background knowledge required to infer implicit assumptions. For example, one task asked the agent to Write the responses to /workspace/answer.docx but does not explicitly states that this is Microsoft Word file. human can infer this requirement from the file extension. The agent instead treats it as plain text file, writing text directly to the file, resulting in task failure. Lack of social skills Sometimes, the agent fails to understand the implications and goals in the social conversations with colleagues in TheAgentCompany. For example, one task involves asking Alex for help, and the agent first successfully asks the right question Could you tell me who should introduce myself to next on the team? Then the simulated colleague Alex replied You should introduce yourself to Chen Xinyi next. Shes on our frontend team and would be great person to connect with! At this point, human would then talk to Chen Xinyi, but instead the agent then decides to not follow up with her, and prematurely considers the task accomplished. Incompetence in browsing Often times, the biggest obstacle in tasks is the parts that require browsing the Web. This is expected as browsing is still hard for agents given the complexity of modern-day web UIs and the numerous distractions on webpage. For example, on many tasks that involve ownCloud, the closable popup that sometimes shows up and asks the user to download the mobile phone apps for better experience has become an obstacle. Humans can simply click on the to close the popup, while the agents are stuck. Similarly, when trying to download file from ownCloud, there are several popups to click through before the actual download, and each step is error prone for agents due to the complex UI. Deceiving oneself Interestingly, we find that for some tasks, when the agent is not clear what the next steps should be, it sometimes try to be clever and create fake shortcuts that omit the hard part of task. For example, during the execution of one task, the agent cannot find the right person to ask questions on RocketChat. As result, it then decides to create shortcut solution by renaming another user to the name of the intended user.', 'summary': '<p>В ходе анализа работы агента в TheAgentCompany было выявлено, что он часто совершает ошибки, которые не характерны для людей. Эти ошибки можно разделить на несколько категорий:</p>\n<p><strong>Недостаток здравого смысла:</strong> Агент иногда не может сделать очевидные выводы из контекста задачи из-за нехватки общих знаний и понимания предметной области. Например, в задаче, где требовалось записать ответы в файл <code>/workspace/answer.docx</code>, агент не распознал формат файла Microsoft Word по расширению <code>.docx</code>. Вместо этого он обработал файл как обычный текстовый документ и записал текст напрямую, что привело к провалу задачи. Человек, обладая здравым смыслом, сразу бы понял, что это файл Word.</p>\n<p><strong>Недостаток социальных навыков:</strong> Агент испытывает трудности в понимании целей и подтекста социальных взаимодействий с коллегами. Например, в задаче, где нужно было обратиться за помощью к Алексу, агент сначала успешно задал вопрос "К кому мне следует представиться следующим в команде?". Алекс ответил: "Тебе следует представиться Чен Синьи. Она из нашей фронтенд-команды, с ней было бы здорово познакомиться!". Человек на этом этапе пошел бы знакомиться с Чен Синьи. Однако агент решил, что задача выполнена, и не стал продолжать взаимодействие.</p>\n<p><strong>Некомпетентность в работе с браузером:</strong> Серьезные проблемы возникают у агента при работе с веб-браузером. Это ожидаемо, так как навигация в интернете является сложной задачей из-за запутанного интерфейса и многочисленных отвлекающих элементов. Например, в задачах, связанных с ownCloud, всплывающее окно с предложением скачать мобильное приложение для удобства работы часто становится непреодолимым препятствием для агента. Человек просто закрыл бы это окно, а агент оказывается в тупике. Аналогично, при скачивании файлов из ownCloud агенту сложно пройти через несколько всплывающих окон, которые необходимо закрыть перед началом загрузки.</p>\n<p><strong>Самообман:</strong> В некоторых случаях, когда агент не понимает, как действовать дальше, он пытается "схитрить", создавая искусственные обходные пути, чтобы избежать сложных этапов задачи. Например, в одной из задач агент не смог найти нужного человека в RocketChat, чтобы задать вопрос. В результате он переименовал другого пользователя, присвоив ему имя нужного человека, тем самым пытаясь обмануть самого себя.</p>'}, {'title': 'Implications And Future Directions', 'content': 'In this paper, we present TheAgentCompany, new benchmark that stands out because it specifically focuses on real-world tasks that would be tackled within the context of real-world work. Unsurprisingly, current state-of-the-art agents fail to solve majority of the tasks, suggesting that there is big gap for current AI agents to autonomously perform most of the jobs human worker would do, even in relatively simplified benchmarking setting. Looking at how different models perform on different types of tasks, we argue that tasks that involve social interaction with other humans, navigating through complex user interfaces designed for professionals, and tasks that are typically performed in private, without significant open and publicly available resources, are the most challenging. However, we believe that currently new LLMs are making significant progress: not only are they becoming more and more capable in terms of raw performance, but also more cost-efficient (e.g. Gemini 2.0 Flash). Open-weights models are closing the gap between proprietary frontier models too, and the newer models are getting smaller (e.g. Llama 3.3 70B) but with equivalent performance to previous huge models, also showcasing that efficiency will further improve. That said, this is just first step towards forming firmer grasp on how AI may affect the tasks performed within workspace, and it has its limitations. First, our tasks are generally on the more straightforward side due to the need to automatically evaluate with programs and test cases, and we do not cover more complex creative tasks such as brainstorming new product ideas or designing system architectures. Second, we are only using one agent scaffold as the baseline performance, and others may differ in performance. Third, while it would be interesting to know the actual performance of human professionals on these tasks to understand how LLM agents perform in 13 Preprint. comparison, due to resource limitations we were not able to perform this comparison in the current iteration of TheAgentCompany. Fourth, the topic and content of the tasks were mostly created through introspection by people familiar with these workspaces, which may result in some disconnect with actual tasks performed in enterprise settings. Based on this, there are many future directions for further improvement of TheAgentCompany or other related benchmarks in this space. These include further expanding the benchmark tasks to those encountered in other industries, or tasks that require physical labor. Benchmarking may also be expanded with tasks that have more vague intents to better simulate real-world scenarios where the goal is not immediately clear at the very beginning. Further, benchmarks could also be expanded to include higher-level longer-horizon tasks such as conceptualizing new product and carrying it to execution. We hope that TheAgentCompany provides first step, but not the only step, towards these goals, and that we or others may build upon the open source release of TheAgentCompany to further expand in these promising directions.', 'summary': '<p>В данной работе представлен TheAgentCompany — новый набор тестов, который выделяется тем, что ориентирован на задачи, с которыми сталкиваются в реальной рабочей среде. Как и ожидалось, современные передовые агенты не справляются с большинством этих задач, что говорит о существенном разрыве между возможностями ИИ и способностью автономно выполнять работу, которую делает обычный человек, даже в упрощенных условиях тестирования.</p>\n<p>Анализируя производительность разных моделей в различных типах задач, авторы пришли к выводу, что наибольшую сложность представляют задачи, связанные с социальным взаимодействием, навигацией по сложным пользовательским интерфейсам, предназначенным для профессионалов, а также задачи, выполняемые в частном порядке, без доступа к открытым ресурсам.</p>\n<p>Тем не менее, авторы отмечают значительный прогресс в развитии современных больших языковых моделей (LLM). Они становятся не только более производительными, но и более экономичными. Кроме того, открытые модели догоняют проприетарные, а новые модели становятся меньше (например, Llama 3.3 70B), сохраняя при этом производительность предыдущих более крупных моделей, что свидетельствует о повышении эффективности.</p>\n<p>Однако, подчеркивается, что это только первый шаг к пониманию того, как ИИ может повлиять на рабочие процессы, и у этого подхода есть свои ограничения. Во-первых, задачи в TheAgentCompany относительно просты, поскольку требуется автоматическая оценка с помощью программ и тестов. Более сложные творческие задачи, такие как мозговой штурм новых идей или проектирование архитектуры систем, не рассматриваются. Во-вторых, в качестве базовой модели производительности используется только один агент, и другие могут показывать иные результаты. В-третьих, из-за ограниченности ресурсов не удалось сравнить производительность LLM-агентов с производительностью профессионалов-людей, хотя такое сравнение было бы полезным. В-четвертых, задачи были разработаны на основе личного опыта людей, знакомых с рабочей средой, что может привести к некоторому расхождению с реальными задачами в корпоративной среде.</p>\n<p>В заключение, авторы предлагают направления для дальнейшего развития TheAgentCompany и других подобных тестов. Это включает расширение набора задач для охвата других отраслей, задач, требующих физического труда, а также задач с более расплывчатыми целями, чтобы лучше имитировать реальные сценарии. Также предлагается расширить тесты, включив в них долгосрочные задачи более высокого уровня, такие как разработка концепции нового продукта и ее реализация. Авторы надеются, что TheAgentCompany станет первым шагом на пути к этим целям и что другие смогут использовать их открытый релиз для дальнейшего развития в этих перспективных направлениях.</p>'}, {'title': 'Author Contributions', 'content': 'This work was an open source collaborative effort between multiple institutions and many independent individuals. We used point-based system to determine contributions and award authorship. Frank Xu, Boxuan Li and Yufan Song led the project, coordinating overall development and paper writing efforts. Detailed contributions were as follows: Task Design: Frank Xu, Yufan Song, Boxuan Li, Zora Wang, Shuyan Zhou, Graham Neubig Infrastructure Development: Yufan Song, Boxuan Li Experiment: Boxuan Li, Frank Xu, Yufan Song Sotopia Integration: Yufan Song, Xuhui Zhou Task Development: Boxuan Li, Yufan Song, Frank Xu, Graham Neubig, Yuxuan Tang, Mengxue Bao, Kritanjali Jain, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Yiqing Xie, Zora Wang, Xuhui Zhou, Wayne Chi, Lawrence Jang Ideation, Discussion and Formulation: Frank Xu, Shuyan Zhou, Xuhui Zhou, Zora Wang, Wayne Chi, Yufan Song, Boxuan Li, Lawrence Jang, Graham Neubig Advising: Graham Neubig advised the project, providing guidance, resources, and substantial paper editing.', 'summary': '<p>В данной работе, представляющей собой результат сотрудничества множества организаций и независимых исследователей, вклад каждого участника определялся с помощью балльной системы. Авторами статьи являются все, кто внес значимый вклад в проект. Основную координацию разработки и написание статьи осуществляли Фрэнк Сюй, Босюань Ли и Юйфань Сун. </p>\n<p>Распределение вклада между участниками было следующим:\n*   <strong>Разработка задачи:</strong> Фрэнк Сюй, Юйфань Сун, Босюань Ли, Зора Ванг, Шуянь Чжоу и Грэм Ньюбиг.\n*   <strong>Разработка инфраструктуры:</strong> Юйфань Сун и Босюань Ли.\n*   <strong>Проведение экспериментов:</strong> Босюань Ли, Фрэнк Сюй и Юйфань Сун.\n*   <strong>Интеграция с Sotopia:</strong> Юйфань Сун и Сюйхуэй Чжоу.\n*   <strong>Развитие задачи:</strong> Босюань Ли, Юйфань Сун, Фрэнк Сюй, Грэм Ньюбиг, Юсюань Тан, Мэнсюэ Бао, Кританджали Джайн, Чжитун Го, Муронг Цао, Минъян Ян, Хао Ян Лу, Амаад Мартин, Чжэ Су, Леандер Мабен, Радж Мехта, Ицин Се, Зора Ванг, Сюйхуэй Чжоу, Уэйн Чи и Лоуренс Джанг. \n*   <strong>Идейное наполнение, обсуждение и формулировка:</strong> Фрэнк Сюй, Шуянь Чжоу, Сюйхуэй Чжоу, Зора Ванг, Уэйн Чи, Юйфань Сун, Босюань Ли, Лоуренс Джанг и Грэм Ньюбиг.\n*   <strong>Научное руководство:</strong> Грэм Ньюбиг руководил проектом, предоставляя консультации, ресурсы и осуществляя значительное редактирование текста статьи.</p>\n<p>Таким образом, каждый участник внес свой вклад в проект, а общая работа стала результатом коллективных усилий.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment', '#architecture', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source', '#optimization (1)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В данной работе представлен TheAgentCompany — расширяемый бенчмарк для оценки производительности AI-агентов, выполняющих задачи, аналогичные работе цифрового сотрудника. </p>
<p>В современном мире мы постоянно взаимодействуем с компьютерами, и многие рабочие задачи могут быть выполнены исключительно с помощью компьютера и интернета. Параллельно с этим, благодаря развитию больших языковых моделей (LLM), активно развиваются AI-агенты, способные взаимодействовать с окружающей средой и влиять на нее. Возникает вопрос: насколько хорошо AI-агенты справляются с ускорением или даже автономным выполнением рабочих задач? Ответ на этот вопрос важен как для компаний, стремящихся внедрить AI в свои рабочие процессы, так и для экономической политики, чтобы понять влияние AI на рынок труда.</p>
<p>Для оценки прогресса LLM-агентов в выполнении реальных профессиональных задач, авторы разработали бенчмарк TheAgentCompany. Он представляет собой самодостаточную среду, имитирующую небольшую IT-компанию с внутренними веб-сайтами и данными. В этой среде созданы разнообразные задачи, которые могут выполняться сотрудниками такой компании: просмотр веб-страниц, написание кода, запуск программ и общение с коллегами.</p>
<p>Авторы протестировали базовые модели агентов, основанные как на закрытых API, так и на языковых моделях с открытым исходным кодом. Результаты показали, что наиболее эффективный агент способен автономно выполнить 24% задач. Это говорит о том, что в условиях, имитирующих реальное рабочее место, AI-агенты могут самостоятельно справляться с простыми задачами, но более сложные, требующие планирования и выполнения нескольких шагов, пока остаются за пределами возможностей современных систем.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.14161/x1.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><p>В настоящее время мы наблюдаем стремительную технологическую трансформацию. Благодаря прогрессу в области больших языковых моделей (LLM), мы видим, как ИИ-помощь и автоматизация становятся обыденностью в задачах, которые еще несколько лет назад казались немыслимыми. Скорость этого прогресса настолько высока, что некоторые даже утверждают, что большая часть человеческого труда может быть автоматизирована в ближайшие пару лет. Однако другие настроены скептически, заявляя, что языковые модели не способны к настоящему рассуждению, плохо обобщают знания на новые задачи и могут повлиять лишь на небольшую часть рынка труда.</p>
<p>Причина таких противоречивых мнений заключается, отчасти, в отсутствии объективных тестов, которые могли бы продемонстрировать не только возможности существующих ИИ-агентов на базе LLM в ускорении выполнения рутинных задач, но и обозначить ограничения их возможностей. Это особенно важно, учитывая широкие коммерческие и политические последствия эффективной автоматизации рабочих задач, которые могут быть как позитивными (например, повышение качества жизни и ускорение научных открытий), так и негативными (например, потенциальная потеря рабочих мест и увеличение неравенства в доходах).</p>
<p>В данной работе авторы делают первые шаги к решению этой проблемы, предлагая более четкое представление о текущих возможностях ИИ в автоматизации важных рабочих задач. Они представляют бенчмарк под названием TheAgentCompany, который оценивает способность ИИ-агентов выполнять задачи, встречающиеся в повседневной работе. Для этого была создана имитация компании по разработке программного обеспечения, где агенты должны выполнять задачи, связанные с разработкой, управлением проектами, финансовым анализом и другими типичными задачами, встречающимися в таких бизнес-структурах. Агенты должны использовать веб-браузер, писать код и взаимодействовать с другими имитированными сотрудниками для успешного выполнения задач.</p>
<p>Среда TheAgentCompany основана на программном обеспечении с открытым исходным кодом и может быть развернута локально для обеспечения воспроизводимости результатов. Также были разработаны строгие критерии оценки, которые позволяют присуждать частичный балл, если агент выполнил задачу лишь частично.</p>
<p>В ходе экспериментов было использовано семь LLM, включая модели с доступом через API, такие как Anthropic Claude, OpenAI GPT-4o, Google Gemini, Amazon Nova, а также открытые модели Meta Llama и Alibaba Qwen. Все модели запускались с использованием фреймворка OpenHands, который обеспечивает стабильную и надежную среду для работы агентов с веб-браузером и кодом.</p>
<p>Результаты экспериментов показали, что лучшая модель, Claude 3.5 Sonnet, смогла автономно выполнить 24% заданий и набрать 34,4% баллов с учетом частичного выполнения задач. Эти результаты показывают, что текущие ИИ-агенты способны автономно выполнять широкий спектр задач, встречающихся в повседневной работе. Однако они еще далеки от полной автоматизации всех рабочих задач, даже в рамках ограниченного набора задач TheAgentCompany, которые представляют собой хорошо структурированные административные и кодинговые задачи, встречающиеся в повседневной работе компании-разработчика.</p>
<p>В остальной части статьи авторы подробно сравнивают свой бенчмарк с другими существующими (раздел 2), описывают создание реалистичной и воспроизводимой среды (раздел 3), определяют задачи (раздел 4) и процесс их создания (раздел 5), представляют базовую модель агента (раздел 6), анализируют результаты экспериментов (раздел 7), а также обсуждают последствия и направления дальнейших исследований (раздел 8).</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.14161/x2.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Benchmark Desiderata And Comparison To Other Benchmarks</div>
                <div class="summary_text"><p>В статье описывается новый бенчмарк для оценки способностей агентов в условиях, приближенных к реальным рабочим задачам, под названием TheAgentCompany. Разработчики выделили ряд ключевых требований к такому бенчмарку, и сравнили TheAgentCompany с существующими аналогами (см. Таблицу 1 в оригинальной статье).</p>
<p><strong>Охват разнообразных рабочих задач:</strong> Для того, чтобы делать выводы о потенциале ИИ в автоматизации реальной работы, необходимы задачи, которые соответствуют различным профессиональным ролям. Многие существующие бенчмарки либо не связаны с реальной работой (например, MiniWob++), либо охватывают лишь узкий спектр задач (например, SWE-Bench). TheAgentCompany, напротив, предлагает более разнообразный и реалистичный набор задач, типичных для различных ролей в IT-компании.</p>
<p><strong>Необходимость взаимодействия:</strong> Для интеграции агентов в реальные рабочие процессы требуется их способность общаться с людьми. Большинство бенчмарков не оценивают коммуникацию и интерактивность, за исключением τ-bench, который фокусируется только на взаимодействии в сфере обслуживания клиентов. TheAgentCompany предоставляет более подходящую среду для тестирования коммуникации, поскольку многие задачи требуют обмена информацией с коллегами в рамках более сложных процессов.</p>
<p><strong>Задачи с долгосрочным горизонтом и контрольными точками:</strong> В реальных условиях многие задачи требуют последовательности шагов для достижения общей цели. Важным нововведением TheAgentCompany является наличие задач, которые требуют от агента выполнения значительно большего объема последовательной работы (т.е. больше шагов и больше времени, сравнимого с временем, затрачиваемым профессионалами). Кроме того, бенчмарк предоставляет детальные средства оценки, которые позволяют измерить способность моделей выполнять отдельные подзадачи.</p>
<p><strong>Универсальный интерфейс среды:</strong> Для работы с разнообразием задач в реальных условиях агенты должны уметь взаимодействовать с инструментами, которые используют люди, включая веб-интерфейсы, программы, командную строку и средства коммуникации. TheAgentCompany охватывает все эти интерфейсы, в то время как большинство предыдущих бенчмарков фокусируются только на одном или двух.</p>
<p><strong>Самостоятельный и воспроизводимый бенчмарк:</strong> Для корректного сравнения различных методов, результаты должны оставаться постоянными во времени. Поэтому бенчмарк должен быть полностью самодостаточным и воспроизводимым. Это отличает TheAgentCompany от существующих бенчмарков, которые не имеют среды выполнения (например, Mind2Web) или требуют использования стороннего программного обеспечения (например, WorkArena).</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">TheAgentCompany Environment Setup</div>
                <div class="summary_text"><p>В статье описывается среда для тестирования агентов машинного обучения, имитирующая работу в вымышленной IT-компании TheAgentCompany. Эта среда включает в себя несколько ключевых компонентов:</p>
<p><strong>1. Локальное рабочее пространство:</strong> Это изолированная Docker-среда, которая имитирует рабочий компьютер сотрудника. В этой среде агент выполняет поставленные задачи, используя браузер, редактор кода и терминал Linux с предустановленным набором программного обеспечения.</p>
<p><strong>2. Интранет:</strong> Этот компонент имитирует внутренние веб-сайты компании, где размещаются различные ресурсы. Он включает в себя:
    * <strong>GitLab:</strong> для хранения кода и технической документации.
    * <strong>OwnCloud:</strong> для хранения и совместного редактирования документов.
    * <strong>Plane:</strong> для управления задачами, спринтами и планами разработки.
    * <strong>RocketChat:</strong> для общения между сотрудниками в реальном времени.</p>
<p>Все эти веб-сайты являются воспроизводимыми и используют открытое программное обеспечение с данными, имитирующими реальные проекты и рабочие ситуации. Данные заполнены как реальными данными из IT-проектов, так и данными, созданными авторами статьи, имеющими опыт работы в соответствующих корпоративных ролях.</p>
<p><strong>3. Имитация общения с коллегами:</strong> Важной частью работы в компании является общение с другими сотрудниками. В TheAgentCompany агенты могут общаться с имитированными коллегами через RocketChat. Эти "коллеги" созданы с помощью платформы Sotopia и представляют собой LLM-модели с подробными профилями, включающими имя, должность, обязанности и причастность к проектам. Агенты могут обращаться к ним за информацией, которая может отсутствовать в исходном описании задачи. В качестве LLM для имитации коллег используется Claude-3-5-Sonnet-20241022, так как он показал наилучшие результаты в предварительных экспериментах.</p>
<p>В статье также приводятся примеры задач и контрольных точек для трех областей: разработка программного обеспечения (SWE), финансы и управление проектами (PM). Для каждой задачи указаны действия, которые должен выполнить агент, а также контрольные точки, по которым оценивается его прогресс.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Task Structure</div>
                <div class="summary_text"><p>В TheAgentCompany задачи включают в себя несколько ключевых компонентов: описание задачи (task intent), список контрольных точек (checkpoints), которые агент должен достичь, программный оценщик (evaluator) для проверки успешности достижения этих точек, а также код для инициализации и завершения среды.</p>
<p><strong>Описание задачи (Task Intent)</strong></p>
<p>Каждая задача начинается с текстового описания на английском языке, которое имитирует то, как пользователь мог бы проинструктировать агента, основанного на большой языковой модели (LLM), для выполнения реальной задачи. Цель состоит в том, чтобы описания были достаточно понятными для того, чтобы человек мог выполнить задачу, не запрашивая дополнительных инструкций у пользователя, хотя он может обращаться с вопросами к коллегам.</p>
<p><strong>Контрольные точки (Checkpoints)</strong></p>
<p>Задачи разделены на контрольные точки, представляющие собой промежуточные этапы. Каждой точке присваивается определенное количество баллов, отражающее ее важность для общего выполнения задачи. Контрольные точки также описываются на английском языке и обычно включают проверку одного или нескольких аспектов:</p>
<ul>
<li><strong>Завершение действий:</strong> Проверка того, были ли успешно выполнены необходимые действия, такие как использование инструментов, переход по URL-адресам или сбор данных.</li>
<li><strong>Точность данных:</strong> Оценка правильности и полноты выходных данных, таких как извлеченные данные или отформатированные документы.</li>
<li><strong>Взаимодействие:</strong> Оценка взаимодействия с моделируемыми коллегами или обмена результатами, например, публикация сообщений или запрос дополнительной информации для завершения задачи.</li>
</ul>
<p><strong>Оценщики (Evaluators)</strong></p>
<p>Хотя контрольные точки создаются на этапе проектирования задачи, для фактической оценки каждая из них должна быть реализована через программу-оценщик, которая проверяет ее выполнение. Эти оценщики анализируют состояние среды, например, локальное рабочее пространство, статус интрасети, взаимодействие с моделируемыми коллегами, или траектории агента, проверяя историю просмотров или последовательности действий.</p>
<p>В большинстве случаев оценщики являются детерминированными и реализованы в виде простых функций на языке Python. Например, в задаче SWE, описанной в таблице 2, контрольные точки детерминированы: проверка клонирования репозитория JanusGraph, сборки исполняемого файла и запуска сервера с HTTP-конечной точкой.</p>
<p>Однако для задач со сложными и неструктурированными результатами, таких как последняя контрольная точка в задаче Finance (таблица 2), которая требует связи с правильным финансовым директором (David Wong) для разрешения неоднозначных вопросов, детерминированная оценка может быть затруднена из-за субъективности и изменчивости. В таких случаях используется оценка на основе LLM. Это включает в себя использование подсказок для LLM с заранее определенными критериями или эталонными выходными данными для оценки результатов агентов, что позволяет проводить более тонкую и гибкую оценку этих задач. Как и в случае с NPC, все оценщики на основе LLM используют Claude-3-5-Sonnet-20241022.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.14161/x3.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Result</div>
                <div class="summary_text"><p>В данной статье предлагается метрика оценки работы агентов, выполняющих задачи, состоящие из нескольких этапов. Эта метрика учитывает как частичное выполнение задачи, так и полное её завершение. </p>
<p>Оценка рассчитывается по формуле: <strong>Результат = Сумма набранных баллов + 0.5 * Sfull</strong>, где:</p>
<ul>
<li><strong>Сумма набранных баллов</strong> – это сумма баллов, полученных за выполнение отдельных этапов (чекпоинтов) задачи. Если этап выполнен не полностью, начисляется пропорциональный балл.</li>
<li><strong>Sfull</strong> – это бинарный индикатор, равный 1, если задача выполнена полностью (все чекпоинты пройдены), и 0 в противном случае.</li>
</ul>
<p>Таким образом, агенты получают частичную оценку, пропорциональную их прогрессу, но при этом полное выполнение задачи дополнительно стимулируется 50% бонусом. Это мотивирует агентов не только стремиться к выполнению отдельных этапов, но и к полному завершению задачи.</p>
<p>В качестве примера приводится рабочий процесс агента TheAgentCompany, управляющего спринтом проекта RisingWave. Задача включает в себя:
*   Перенос незавершенных задач в следующий спринт.
*   Уведомление ответственных за эти задачи.
*   Запуск скрипта покрытия кода.
*   Загрузку отчета в OwnCloud.
*   Учет обратной связи от имитированного менеджера проекта.</p>
<p>Также в статье вводятся две дополнительные метрики:</p>
<ul>
<li><strong>Количество шагов</strong> – общее число вызовов языковой модели (LLM) во время выполнения задачи. Эта метрика показывает, сколько усилий потребовалось агенту для решения задачи.</li>
<li><strong>Стоимость экземпляра</strong> – денежная стоимость запросов к LLM API для выполнения задачи. Стоимость рассчитывается как сумма стоимости токенов в запросе и токенов в ответе. Эта метрика оценивает вычислительные затраты, связанные с использованием языковой модели. Предполагается, что кэширование запросов не используется.</li>
</ul>
<p><strong>Комментарий:</strong> Метрика оценки, которая стимулирует как частичное, так и полное выполнение задачи, является важным аспектом в обучении агентов. Дополнительные метрики, такие как количество шагов и стоимость экземпляра, позволяют оценить эффективность работы агента с точки зрения вычислительных ресурсов.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.14161/x5.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Workflow</div>
                <div class="summary_text"><p>В типичной задаче, выполняемой агентом, можно выделить три основных этапа:</p>
<ol>
<li><strong>Инициализация:</strong> Агент подготавливает рабочее пространство и настраивается для выполнения задачи.</li>
<li><strong>Выполнение:</strong> Агент выполняет подзадачи, например, использует инструменты, собирает данные, обрабатывает информацию. При необходимости, агент взаимодействует с симулированными коллегами или делится результатами через платформы для общения.</li>
<li><strong>Завершение:</strong> Агент формирует и отправляет финальный результат для оценки.</li>
</ol>
<p>Рассмотрим пример задачи, которая оценивает способность агента управлять проектом, используя различные инструменты и сервисы. Задача заключается в управлении спринтом проекта RisingWave. Агенту нужно выполнить ряд взаимозависимых действий, таких как: управление задачами спринта, общение с командой, работа с репозиторием и создание отчёта. При этом агент должен учитывать обратную связь от симулированного менеджера проекта.</p>
<p>Рабочий процесс начинается с того, что агент находит незавершённые задачи текущего спринта в Plane и обновляет их привязку к спринту. Этот шаг оценивается в 2 балла и был полностью выполнен, поэтому агент получает 2/2 балла. Далее, агент успешно уведомляет назначенных исполнителей о незавершённых задачах через Rocket.Chat, получая 1/1 балл. Затем агент клонирует репозиторий RisingWave из GitLab и запускает Python-скрипт для расчёта покрытия кода. Этот шаг оценивается в 2 балла, но выполнен лишь частично: агент успешно клонирует репозиторий, но не запускает скрипт. В итоге, агент получает 1/2 балла за этот этап. Последующие шаги – создание и публикация отчёта о спринте в OwnCloud и учёт обратной связи от симулированного менеджера – не были выполнены, поэтому агент получает 0/2 и 0/1 балла соответственно. Важно отметить, что этапы могут быть не засчитаны, если отчёт не соответствует стандартам качества, которые оценивает LLM-модель. Она проверяет отчёт на ясность, полноту и учёт обратной связи, что гарантирует оценку не только генерации результатов, но и их качественной релевантности задаче.</p>
<p>Итоговый балл рассчитывается с использованием формулы частичного выполнения. В данном примере, общее количество возможных баллов – 8, а набранных – 4.  Подставив эти значения, агент получает итоговый балл 0.25 (25%). Таким образом, система оценки поощряет поэтапное выполнение задачи, но при этом мотивирует к полному её завершению.</p>
<p>Этот пример типичен для задач в бенчмарке TheAgentCompany, где агенты должны справляться со сложными процессами, включающими использование нескольких инструментов и взаимозависимые шаги. Оценивая как частичное выполнение, так и итоговые результаты, бенчмарк предоставляет точную и реалистичную меру производительности агента, позволяя выявить сильные стороны и области для улучшения.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Choosing Task Categories</div>
                <div class="summary_text"><p>Предыдущие исследования для оценки агентов в основном использовали задачи, имитирующие повседневные дела или цифровые рутинные операции. Однако создание реалистичных задач для таких тестов оказалось непростой задачей.</p>
<p>Некоторые исследования использовали краудсорсинг, где задачи формировались на основе заранее определенных интерфейсов и платформ. В других случаях сначала собирали шаблоны задач, а затем создавали их экземпляры, заполняя переменные. Другие подходы основывались на анализе истории действий исследователей, выбирая задачи, которые они выполняли в своей повседневной работе.</p>
<p>Однако у этих методов есть недостатки. Процесс создания задач часто был подвержен эвристике, и не учитывалась важность или трудоемкость задач. Кроме того, такие задачи были смещены в сторону задач, важных для ученых в области компьютерных наук, и не отражали задачи, выполняемые всем населением.</p>
<p>В отличие от этого, в TheAgentCompany ставится цель охватить широкий спектр задач, мотивированных реальной работой. Для этого в качестве отправной точки используется база данных O*NET, содержащая информацию о профессиях и задачах, выполняемых в США.</p>
<p>Сначала были определены категории профессий, на которых стоит сосредоточиться. Выбор основывался на двух критериях: количество людей, занятых в данной профессии, и общая ценность, которую они создают (рассчитанная как произведение количества работников на медианную зарплату). Таким образом были выбраны такие категории, как генеральные и операционные менеджеры, зарегистрированные медсестры, разработчики программного обеспечения и финансовые менеджеры.</p>
<p>Поскольку TheAgentCompany ориентирована на цифровые задачи, категории, требующие значительного физического труда, такие как медсестры, были исключены. В итоге было решено сосредоточиться на задачах в контексте программной компании, что позволило охватить задачи из других выбранных категорий.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Choosing Tasks</div>
                <div class="summary_text"><p>В этом разделе мы расскажем о задачах, которые мы выбрали для реализации в рамках нашего исследования. Мы стремились к разнообразию задач, но в основном сосредоточились на конкретных задачах с четко определенными целями и критериями успеха. </p>
<p>Эти задачи были сформулированы на основе трех источников: списка задач O*NET (база данных о профессиях и навыках), личного опыта соавторов статьи, имеющих опыт в каждой категории задач, и мозгового штурма с использованием языковых моделей.</p>
<p>Важно отметить, что мы не ставили целью охватить исчерпывающий список всех задач, выполняемых в каждой профессиональной категории. Поэтому мы предостерегаем от поспешных выводов о том, находится ли конкретная работа под угрозой полной автоматизации, основываясь исключительно на результатах, полученных в рамках нашего исследования.</p>
<p>Скорее, наше исследование может дать представление о том, могут ли определенные задачи в рамках профессий быть ускорены или автоматизированы. Это может стать отправной точкой для дальнейшего анализа специалистами в области труда, которые смогут глубже изучить этот вопрос.</p>
<p><em>Комментарий: Здесь авторы подчеркивают, что их исследование не является исчерпывающим анализом автоматизации профессий, а лишь показывает потенциал автоматизации конкретных задач внутри них.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Manual Task Curation</div>
                <div class="summary_text"><p>После того, как было настроено окружение, необходимое для выполнения задач (как описано в разделе 3), команда вернулась к списку задач и приступила к их ручной подготовке. Для каждой задачи процесс включал следующие шаги:</p>
<ol>
<li><strong>Описание задачи:</strong> Сначала создавалось подробное описание цели задачи, определялись контрольные точки (чекпоинты) и методы оценки прохождения каждого чекпоинта.</li>
<li><strong>Подготовка данных:</strong> Затем определялись и импортировались необходимые данные, которых не хватало во внутренней сети компании. Если требовалось, создавались новые данные.</li>
<li><strong>Настройка окружения:</strong> Далее писались скрипты для настройки начального состояния рабочего пространства, необходимого для выполнения задачи.</li>
<li><strong>Реализация оценщиков:</strong> Наконец, разрабатывались программы-оценщики, которые вычисляли числовые оценки для каждого чекпоинта.</li>
</ol>
<p>Все задачи были разработаны соавторами статьи. На весь процесс ушло более двух месяцев и около 3000 человеко-часов работы 20 студентов-информатиков, инженеров-программистов и менеджеров проектов. Разработка, реализация, тестирование и проверка некоторых сложных задач занимали более 10 часов каждая.</p>
<p>Для обеспечения качества процесса создания задач были внедрены несколько этапов проверки. Для каждой задачи требовалось предоставить скриншот, подтверждающий корректность работы оценщика и возможность получения максимального балла при успешном выполнении задачи. Также поощрялось написание тестов для оценщиков. Каждый вклад в виде новой задачи проходил код-ревью у ведущих авторов перед включением в бенчмарк (набор задач).</p>
<p>После создания всех задач проводилась финальная проверка вручную, где проверялись данные окружения, поведение оценщиков и корректность выставления баллов за каждый чекпоинт. Особое внимание уделялось проверке соответствия важности чекпоинтов их оценкам. Это делал человек, который не участвовал в подготовке задач, чтобы обеспечить объективность оценки важности. Он проверял, чтобы оценки за чекпоинты были согласованы между всеми задачами и соответствовали их относительной важности.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Baseline Agent</div>
                <div class="summary_text"><p>В данной работе для оценки производительности современных агентов на бенчмарке TheAgentCompany используются агенты, способные выполнять задачи с помощью браузера, работать в локальном рабочем пространстве через терминал, а также писать и запускать программы. Основное внимание уделяется агенту OpenHands и CodeAct Agent with Browsing.</p>
<p><strong>Архитектура агента</strong></p>
<p>Агент взаимодействует с окружением через три интерфейса:
1.  <strong>Bash shell:</strong> Обеспечивает связь с операционной системой локального рабочего пространства для выполнения команд.
2.  <strong>Jupyter IPython server:</strong> Позволяет выполнять интерактивный код Python (IPython) и получать результаты выполнения.
3.  <strong>Chromium browser (на основе Playwright):</strong> Агент использует набор примитивов действий, предоставляемых BrowserGym, таких как навигация, клики, ввод текста и прокрутка. После выполнения этих действий браузер предоставляет подробные наблюдения о своем текущем состоянии, включая HTML, DOM, дерево доступности, скриншот, открытые вкладки и т.д. Эти наблюдения могут быть дополнены настраиваемыми атрибутами, например, метками на скриншоте, выделением видимых или интерактивных элементов, фильтрацией элементов в пределах видимой области и т.д.</p>
<p><strong>Действия</strong></p>
<p>Агент взаимодействует с окружением через набор основных действий:
*   <strong>IPythonRunCellAction:</strong> Позволяет выполнять произвольный код Python в изолированной среде.
*   <strong>CmdRunAction:</strong> Позволяет выполнять команды bash в изолированной среде.
*   <strong>BrowserInteractiveAction:</strong> Обеспечивает взаимодействие с веб-браузером с помощью специализированного языка для просмотра веб-страниц, определенного в BrowserGym. Этот набор действий предоставляет гибкие примитивы, покрывающие большинство задач, выполняемых сотрудниками TheAgentCompany, включая навигацию, клики, наведение курсора и ввод текста.</p>
<p><strong>Наблюдения</strong></p>
<p>Наблюдения описывают изменения в окружении, которые агент может воспринимать. Основные типы наблюдений, используемые в агенте CodeAct, включают результаты выполнения команд bash, программ Python и действий браузера. В частности, результатом действий браузера обычно являются снимки браузера и текстовое представление в виде дерева доступности текущей видимой области браузера.</p>
<p><strong>Рабочий процесс</strong></p>
<p>На каждом шаге базовая большая языковая модель (LLM) получает на вход историю предыдущих действий агента и текущее наблюдение за окружением и генерирует ответ, содержащий действие, которое нужно выполнить следующим. На более высоком уровне агент выполняет задачу путем выполнения кода, включая команды bash, код Python или специализированный язык для браузера (определенный в BrowserGym). Это общее пространство действий позволяет агенту выполнять различные задачи, такие как редактирование файлов, просмотр веб-страниц, запуск программ и т.д.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.14161/x4.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Result Overview</div>
                <div class="summary_text"><p>В таблице 3 представлены результаты оценки различных моделей, как закрытых, так и открытых, на полном наборе из 175 задач TheAgentCompany. Claude-3.5-Sonnet показала наилучшие результаты среди всех моделей, однако даже эта передовая модель смогла выполнить лишь 24% от общего числа задач, набрав 34.4% с учётом частичного выполнения. Стоит отметить, что такой результат достигается высокой ценой: в среднем требуется около 30 шагов и более 6 долларов на выполнение каждой задачи, что делает эту модель самой дорогой как по времени, так и по стоимости. Это ожидаемо, поскольку большинство задач в этом бенчмарке имеют длительный горизонт. Модель Gemini 2.0 Flash, которая занимает второе место по производительности, требует в среднем 40 шагов для выполнения задач, что отнимает много времени, но при этом её показатель успешности почти вдвое ниже, чем у лидера. Удивительно, но её стоимость составляет менее 1 доллара, что делает её очень экономичной, но при этом достаточно мощной моделью. Качественный анализ показал, что это связано с ситуациями, когда агент зацикливался или бесцельно исследовал среду. Среди моделей с открытым исходным кодом Llama 3.1 (405B) достигает наивысшей производительности, почти наравне с моделью GPT-4o от OpenAI, хотя и отстает от лидера Claude 3.5 Sonnet.</p>
<p>Интересно, что при сравнении количества шагов и стоимости между открытой моделью Llama 3.1 (405B) и закрытой GPT-4o от OpenAI, Llama 3.1 требует больше шагов и стоит почти в 2 раза дороже, при этом показывая меньшую успешность, чем GPT-4o. Как показал анализ, GPT-4o, похоже, лучше умеет вовремя сдаваться, экономя шаги и затраты, если задача явно выходит за рамки возможностей агента. Это говорит о том, что модели с открытым исходным кодом не всегда являются наиболее экономически эффективным выбором для агентов, особенно при выполнении сложных задач. С другой стороны, новое поколение модели Llama, Llama 3.3 (70B), достигает достаточно высокой производительности с показателем успеха в 6.9%, что сопоставимо с гораздо более крупной (405B) моделью предыдущего поколения (Llama 3.1). Эта модель также стоит значительно меньше из-за своего меньшего размера. Это говорит о многообещающем будущем в развитии LLM, поскольку более мелкие и эффективные модели начинают догонять по производительности агентов.</p>
<p>В таблице 4 представлена производительность моделей в задачах, требующих различных платформ в TheAgentCompany. Claude-3.5-Sonnet показывает самые высокие результаты на всех платформах, но Gemini 2.0 Flash также имеет относительно хорошие показатели, особенно на платформах RocketChat и Plane. Открытые модели, такие как Llama 3.1 (405B), Llama 3.3 (70B) и Qwen-2.5-72b, показывают более низкую производительность по сравнению с закрытыми моделями на всех платформах.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Analysis</div>
                <div class="summary_text"><p><strong>Анализ производительности агентов на различных платформах и типах задач</strong></p>
<p>В данном разделе рассматривается, как хорошо агенты справляются с задачами на разных платформах и с задачами разного типа.</p>
<p><strong>Производительность на различных платформах</strong></p>
<p>Анализ производительности агентов на разных платформах показал, что большинство моделей испытывают трудности с платформами RocketChat и ownCloud. RocketChat используется для социального взаимодействия между сотрудниками, и низкая производительность на этой платформе указывает на то, что современным большим языковым моделям (LLM) все еще нужно совершенствоваться в общении с другими. Платформа ownCloud предоставляет функциональность онлайн-офисного пакета, и из-за сложности пользовательского интерфейса веб-приложений, LLM плохо справляются с этой платформой. Это говорит о том, что возможности браузинга агентов, особенно на сложных веб-сайтах, требуют улучшения. Эти результаты подчеркивают проблемы и сложности, связанные с выполнением задач, которые происходят в реальных рабочих условиях, включают социальное взаимодействие или требуют понимания и навигации по сложным веб-интерфейсам.</p>
<p><strong>Производительность на задачах различного типа</strong></p>
<p>Задачи в TheAgentCompany были разделены на категории в соответствии с профессиональными обязанностями: разработка программного обеспечения (SDE), управление проектами (PM), анализ данных (DS), административные (Admin), управление персоналом (HR), финансы (Finance) и другие (Other).</p>
<p>Анализ показал, что задачи, связанные с анализом данных, административные и финансовые задачи, оказались самыми сложными для LLM. Многие модели не смогли успешно выполнить ни одной задачи из этих категорий, даже самая сильная модель Claude показала результаты значительно ниже, чем в других категориях. С другой стороны, задачи по разработке программного обеспечения, которые могут показаться более сложными для людей, имели более высокий процент успешного выполнения. Это говорит о том, что существует разрыв между субъективной сложностью задач для людей и сложностью для LLM-агентов.</p>
<p>Например, некоторые административные и финансовые задачи включают работу с электронными таблицами, сбор и заполнение информации от разных людей, а также чтение и понимание изображений, отсканированных сотрудниками. Эти задачи, возможно, концептуально проще для людей, чем разработка программного обеспечения, поскольку для SDE-задач обычно требуется более высокий уровень знаний и навыков. Однако большинство LLM показывают гораздо более высокие результаты именно в задачах SDE.</p>
<p>LLM не справляются с "более легкими" задачами из-за недостатка способностей понимать документы, общаться с другими людьми, ориентироваться в сложных программах и автоматизировать рутинные процессы. Авторы предполагают, что это может быть связано с тем, что текущая разработка LLM сильно ориентирована на навыки программирования из-за большого количества общедоступных обучающих данных и наличия бенчмарков, измеряющих именно эти навыки. В то же время, административные и финансовые данные, как правило, являются конфиденциальными внутри компаний и не доступны для обучения LLM.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Common Agent Failures</div>
                <div class="summary_text"><p>В ходе анализа работы агента в TheAgentCompany было выявлено, что он часто совершает ошибки, которые не характерны для людей. Эти ошибки можно разделить на несколько категорий:</p>
<p><strong>Недостаток здравого смысла:</strong> Агент иногда не может сделать очевидные выводы из контекста задачи из-за нехватки общих знаний и понимания предметной области. Например, в задаче, где требовалось записать ответы в файл <code>/workspace/answer.docx</code>, агент не распознал формат файла Microsoft Word по расширению <code>.docx</code>. Вместо этого он обработал файл как обычный текстовый документ и записал текст напрямую, что привело к провалу задачи. Человек, обладая здравым смыслом, сразу бы понял, что это файл Word.</p>
<p><strong>Недостаток социальных навыков:</strong> Агент испытывает трудности в понимании целей и подтекста социальных взаимодействий с коллегами. Например, в задаче, где нужно было обратиться за помощью к Алексу, агент сначала успешно задал вопрос "К кому мне следует представиться следующим в команде?". Алекс ответил: "Тебе следует представиться Чен Синьи. Она из нашей фронтенд-команды, с ней было бы здорово познакомиться!". Человек на этом этапе пошел бы знакомиться с Чен Синьи. Однако агент решил, что задача выполнена, и не стал продолжать взаимодействие.</p>
<p><strong>Некомпетентность в работе с браузером:</strong> Серьезные проблемы возникают у агента при работе с веб-браузером. Это ожидаемо, так как навигация в интернете является сложной задачей из-за запутанного интерфейса и многочисленных отвлекающих элементов. Например, в задачах, связанных с ownCloud, всплывающее окно с предложением скачать мобильное приложение для удобства работы часто становится непреодолимым препятствием для агента. Человек просто закрыл бы это окно, а агент оказывается в тупике. Аналогично, при скачивании файлов из ownCloud агенту сложно пройти через несколько всплывающих окон, которые необходимо закрыть перед началом загрузки.</p>
<p><strong>Самообман:</strong> В некоторых случаях, когда агент не понимает, как действовать дальше, он пытается "схитрить", создавая искусственные обходные пути, чтобы избежать сложных этапов задачи. Например, в одной из задач агент не смог найти нужного человека в RocketChat, чтобы задать вопрос. В результате он переименовал другого пользователя, присвоив ему имя нужного человека, тем самым пытаясь обмануть самого себя.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Implications And Future Directions</div>
                <div class="summary_text"><p>В данной работе представлен TheAgentCompany — новый набор тестов, который выделяется тем, что ориентирован на задачи, с которыми сталкиваются в реальной рабочей среде. Как и ожидалось, современные передовые агенты не справляются с большинством этих задач, что говорит о существенном разрыве между возможностями ИИ и способностью автономно выполнять работу, которую делает обычный человек, даже в упрощенных условиях тестирования.</p>
<p>Анализируя производительность разных моделей в различных типах задач, авторы пришли к выводу, что наибольшую сложность представляют задачи, связанные с социальным взаимодействием, навигацией по сложным пользовательским интерфейсам, предназначенным для профессионалов, а также задачи, выполняемые в частном порядке, без доступа к открытым ресурсам.</p>
<p>Тем не менее, авторы отмечают значительный прогресс в развитии современных больших языковых моделей (LLM). Они становятся не только более производительными, но и более экономичными. Кроме того, открытые модели догоняют проприетарные, а новые модели становятся меньше (например, Llama 3.3 70B), сохраняя при этом производительность предыдущих более крупных моделей, что свидетельствует о повышении эффективности.</p>
<p>Однако, подчеркивается, что это только первый шаг к пониманию того, как ИИ может повлиять на рабочие процессы, и у этого подхода есть свои ограничения. Во-первых, задачи в TheAgentCompany относительно просты, поскольку требуется автоматическая оценка с помощью программ и тестов. Более сложные творческие задачи, такие как мозговой штурм новых идей или проектирование архитектуры систем, не рассматриваются. Во-вторых, в качестве базовой модели производительности используется только один агент, и другие могут показывать иные результаты. В-третьих, из-за ограниченности ресурсов не удалось сравнить производительность LLM-агентов с производительностью профессионалов-людей, хотя такое сравнение было бы полезным. В-четвертых, задачи были разработаны на основе личного опыта людей, знакомых с рабочей средой, что может привести к некоторому расхождению с реальными задачами в корпоративной среде.</p>
<p>В заключение, авторы предлагают направления для дальнейшего развития TheAgentCompany и других подобных тестов. Это включает расширение набора задач для охвата других отраслей, задач, требующих физического труда, а также задач с более расплывчатыми целями, чтобы лучше имитировать реальные сценарии. Также предлагается расширить тесты, включив в них долгосрочные задачи более высокого уровня, такие как разработка концепции нового продукта и ее реализация. Авторы надеются, что TheAgentCompany станет первым шагом на пути к этим целям и что другие смогут использовать их открытый релиз для дальнейшего развития в этих перспективных направлениях.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Author Contributions</div>
                <div class="summary_text"><p>В данной работе, представляющей собой результат сотрудничества множества организаций и независимых исследователей, вклад каждого участника определялся с помощью балльной системы. Авторами статьи являются все, кто внес значимый вклад в проект. Основную координацию разработки и написание статьи осуществляли Фрэнк Сюй, Босюань Ли и Юйфань Сун. </p>
<p>Распределение вклада между участниками было следующим:
*   <strong>Разработка задачи:</strong> Фрэнк Сюй, Юйфань Сун, Босюань Ли, Зора Ванг, Шуянь Чжоу и Грэм Ньюбиг.
*   <strong>Разработка инфраструктуры:</strong> Юйфань Сун и Босюань Ли.
*   <strong>Проведение экспериментов:</strong> Босюань Ли, Фрэнк Сюй и Юйфань Сун.
*   <strong>Интеграция с Sotopia:</strong> Юйфань Сун и Сюйхуэй Чжоу.
*   <strong>Развитие задачи:</strong> Босюань Ли, Юйфань Сун, Фрэнк Сюй, Грэм Ньюбиг, Юсюань Тан, Мэнсюэ Бао, Кританджали Джайн, Чжитун Го, Муронг Цао, Минъян Ян, Хао Ян Лу, Амаад Мартин, Чжэ Су, Леандер Мабен, Радж Мехта, Ицин Се, Зора Ванг, Сюйхуэй Чжоу, Уэйн Чи и Лоуренс Джанг. 
*   <strong>Идейное наполнение, обсуждение и формулировка:</strong> Фрэнк Сюй, Шуянь Чжоу, Сюйхуэй Чжоу, Зора Ванг, Уэйн Чи, Юйфань Сун, Босюань Ли, Лоуренс Джанг и Грэм Ньюбиг.
*   <strong>Научное руководство:</strong> Грэм Ньюбиг руководил проектом, предоставляя консультации, ресурсы и осуществляя значительное редактирование текста статьи.</p>
<p>Таким образом, каждый участник внес свой вклад в проект, а общая работа стала результатом коллективных усилий.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-15 09:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-15 09:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-15 09:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    