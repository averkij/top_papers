
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. December 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 декабря</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-12.html">⬅️ <span id="prev-date">12.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-16.html">➡️ <span id="next-date">16.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 декабря', 'en': 'December 13', 'zh': '12月13日'};
        let feedDateNext = {'ru': '16.12', 'en': '12/16', 'zh': '12月16日'};
        let feedDatePrev = {'ru': '12.12', 'en': '12/12', 'zh': '12月12日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2412.05210', 'title': 'Evaluating and Aligning CodeLLMs on Human Preference', 'url': 'https://huggingface.co/papers/2412.05210', 'abstract': 'Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}', 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-06', 'pub_date_card': {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'}, 'hash': '0232aabe01d37826', 'authors': ['Jian Yang', 'Jiaxi Yang', 'Ke Jin', 'Yibo Miao', 'Lei Zhang', 'Liqun Yang', 'Zeyu Cui', 'Yichang Zhang', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'Shanghai Jiao Tong University', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2412.05210.jpg', 'data': {'categories': ['#plp', '#alignment', '#benchmark', '#open_source', '#synthetic', '#training'], 'emoji': '🏆', 'ru': {'title': 'CodeArena: новый стандарт оценки ИИ-помощников программиста', 'desc': 'В статье представлен новый бенчмарк CodeArena для оценки языковых моделей кода с учетом предпочтений пользователей. Авторы создали набор из 397 высококачественных примеров задач по программированию на 44 языках. Также был разработан синтетический набор инструкций SynCode-Instruct объемом около 20 миллиардов токенов для дообучения моделей. Эксперименты показали значительный разрыв в производительности между открытыми и проприетарными моделями кода, подчеркивая важность согласования с предпочтениями человека.'}, 'en': {'title': 'Bridging Code Generation and Human Preferences with CodeArena', 'desc': 'This paper discusses the advancements in code large language models (codeLLMs) for code generation tasks. It highlights the limitations of existing benchmarks that primarily focus on code correctness without considering human preferences in real-world applications. To address this, the authors introduce CodeArena, a human-curated benchmark that includes diverse coding tasks across multiple programming languages. Additionally, they present SynCode-Instruct, a large synthetic instruction dataset that enhances the training of codeLLMs, revealing significant performance differences between various models when evaluated against human-aligned benchmarks.'}, 'zh': {'title': '提升代码生成模型与人类偏好的对齐', 'desc': '本文介绍了一种新的基准测试工具CodeArena，用于评估代码生成大语言模型（code LLMs）的性能。现有的基准测试主要关注代码片段的正确性，而忽视了与人类偏好的对齐。CodeArena通过提供397个高质量样本，涵盖40个类别和44种编程语言，模拟真实编码任务的复杂性和多样性。研究还提出了一个多样化的合成指令语料库SynCode-Instruct，以验证大规模合成指令微调的有效性，结果显示开源代码LLMs与专有LLMs之间存在显著的性能差距。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}', 'summary': '<p>В последнее время большие языковые модели (БЯМ), специализирующиеся на коде (codeLLM), достигли значительных успехов в генерации кода. Для оценки их производительности обычно используются стандартные наборы данных, включающие различные упражнения по программированию и соответствующие тесты. Однако, современные codeLLM в основном сосредоточены на создании корректного кода, не учитывая соответствие человеческим предпочтениям. В реальных сценариях запросы должны отражать практические задачи, а ответы моделей должны соответствовать ожиданиям людей.</p>\n<p>Чтобы устранить этот разрыв, авторы статьи представили новый, тщательно отобранный людьми набор данных под названием CodeArena. Он имитирует сложность и разнообразие реальных задач программирования и включает 397 высококачественных примеров, охватывающих 40 категорий и 44 языка программирования. Эти примеры были отобраны из реальных пользовательских запросов.</p>\n<p>Кроме того, авторы предлагают использовать разнообразный синтетический корпус инструкций SynCode-Instruct, содержащий почти 20 миллиардов токенов. Он был создан путем масштабирования инструкций с веб-сайта, чтобы проверить эффективность обучения codeLLM на большом количестве синтетических данных. Модель Qwen2.5-SynCoder, обученная исключительно на этих синтетических данных, показала лучшие результаты среди открытых codeLLM.</p>\n<p>Результаты исследования выявили различия в оценке производительности моделей при использовании стандартных наборов данных, основанных на выполнении кода, и CodeArena. Систематические эксперименты с CodeArena, проведенные на более чем 40 языковых моделях, показали значительный разрыв в производительности между открытыми codeLLM (например, Qwen2.5-Coder) и проприетарными моделями (например, OpenAI o1). Это подчеркивает важность учета человеческих предпочтений при обучении и оценке codeLLM.</p>'}, {'title': 'Introduction', 'content': 'Advanced large language models (LLMs)(OpenAI, 2023; Anthropic, 2023) have demonstrated impressive performance across wide range of tasks, particularly excelling in code completion and generation. Code capabilities have established LLMs as 1https://codearenaeval.github.io/ Figure 1: comparison between the GPT4o with better human preference and Qwen2.5-Coder-7B-Instruct. Qwen2.5-Coder-7B-Instruct solves the user question by simply replying with the code snippet without details. essential productivity tools in software engineering. Recently, open code-specific LLMs, such as StarCoder(Li et al., 2023), DeepSeekCoder (Guo et al., 2024a), and QwenCoder (Hui et al., 2024), have made significant progress, achieving performance on fundamental code generation tasks (Austin et al., 2021; Cassano et al., 2023) that approaches the level of top-tier proprietary models. Moreover, their open and transparent model weights address developers concerns about privacy, enabling the deployment of localized code assistants. With the advancing code capabilities of LLMs, effectively evaluating performance on code-related tasks has emerged as challenge. Popular code-related benchmarks typically focus on self-contained function snippets, relying on limited number of test cases to verify code correctness, such as HumanEval (Chen et al., 2021a), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2024). While recent efforts have expanded the scope of test cases (Liu et al., 2023), tasks (Lai et al., 2022) and programming languages (Chai et al., 2024; Kwiatkowski et al., 2019), these benchmarks remain constrained to validating the correctness of generated code snippets. However, ChatBot Arena (Chiang et al., 2024) has demonstrated that alignment between modelgenerated responses and user preferences is also critical evaluation criterion. As shown in Figure 1, Qwen2.5-Coder primarily generates alone code snippets, while Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference. Therefore, there is an urgent need to establish human preference benchmark specifically for code-related tasks, enabling the community to evaluate and track the alignment between human preferences and modelgenerated responses in real-world scenarios. Furthermore, effective data for improving the human preference alignment of codeLLMs remains scarce. Achieving robust alignment across diverse coding tasks poses significant challenges, particularly in terms of the quantity and quality of data required during the supervised fine-tuning (SFT) stage. To this end, we first introduce comprehensive human-curated benchmark, CodeArena, comprising 397 high-quality samples across 40 categories derived from real-world user queries. Additionally, we develop diverse synthetic instruction corpus, SynCode-Instruct, containing nearly 20 billion tokens, by scaling instructions from web sources. Our extensive evaluation of over nearly 40 large language models (LLMs) using CodeArena reveals significant performance differences between code-execution-based benchmarks and our humancurated benchmark. Notably, we observe substantial performance gap between open-source code LLMs (such as Qwen-Coder) and closed-source LLMs (like the o1 and Claude series), emphasizing the critical role of aligning AI models with human preferences in coding tasks. The contributions are summarized as follows: (1) We propose CodeArena comprised of 397 manually annotated samples, comprehensive code evaluation benchmark for evaluating the alignment between the model-generated response and human preference, encompassing 7 major categories and 40 subcategories. (2) We introduce SynCodeInstruct, the large-scale synthetic code instruction corpora from the website. Based on SynCodeInstruct, an effective coder Qwen2.5-SynCoder is used as strong baseline for CodeArena. (3) We systematically evaluate 40+ LLMs on CodeArena and create leaderboard to dynamically update the results. Notably, extensive experiments suggest that CodeArena can effectively measure the alignment between the model-generated response and human preference.', 'summary': '<p>Современные большие языковые модели (LLM), такие как GPT-4 и Claude, демонстрируют впечатляющие результаты в различных задачах, особенно в генерации и дополнении кода. Это сделало LLM важными инструментами для повышения производительности в разработке программного обеспечения. В последнее время открытые модели, ориентированные на код, такие как StarCoder, DeepSeekCoder и QwenCoder, достигли значительного прогресса, приближаясь по производительности к топовым проприетарным моделям в базовых задачах генерации кода. Кроме того, их открытость и прозрачность снимают опасения разработчиков по поводу конфиденциальности, позволяя развертывать локальные ассистенты для работы с кодом.</p>\n<p>С ростом возможностей LLM в области кода, эффективная оценка их производительности в соответствующих задачах стала важной проблемой. Популярные бенчмарки для оценки кода обычно фокусируются на изолированных фрагментах функций, используя ограниченное количество тестовых случаев для проверки корректности кода. Примеры таких бенчмарков: HumanEval, MBPP и BigCodeBench. Хотя в последнее время предпринимаются попытки расширить охват тестовых случаев, задач и языков программирования, эти бенчмарки по-прежнему ограничены проверкой корректности сгенерированных фрагментов кода.</p>\n<p>Однако, как показал ChatBot Arena, соответствие ответов модели предпочтениям пользователей также является важным критерием оценки. Например, модель Qwen2.5-Coder генерирует в основном фрагменты кода, в то время как Claude3.5 предоставляет ответы с подробными объяснениями, хорошо структурированным форматированием и комментариями к коду, что делает ее более предпочтительной с точки зрения пользователя. Таким образом, существует острая необходимость в создании бенчмарка, ориентированного на человеческие предпочтения, специально для задач, связанных с кодом. Это позволит сообществу оценивать и отслеживать соответствие ответов моделей предпочтениям пользователей в реальных сценариях.</p>\n<p>Кроме того, эффективные данные для улучшения соответствия LLM для работы с кодом предпочтениям пользователей по-прежнему в дефиците. Достижение надежного соответствия в различных задачах кодирования представляет значительные трудности, особенно с точки зрения количества и качества данных, необходимых на этапе обучения с учителем (SFT).</p>\n<p>В связи с этим, авторы статьи представляют <strong>CodeArena</strong> – тщательно отобранный людьми бенчмарк, включающий 397 высококачественных примеров из 40 категорий, основанных на реальных запросах пользователей. Также авторы разработали <strong>SynCode-Instruct</strong> – обширный корпус синтетических инструкций, содержащий почти 20 миллиардов токенов, путем масштабирования инструкций из веб-источников.</p>\n<p>Обширная оценка более чем 40 больших языковых моделей с использованием CodeArena выявила значительные различия в производительности между бенчмарками, основанными на выполнении кода, и бенчмарком, созданном людьми. В частности, наблюдается существенный разрыв в производительности между открытыми моделями (такими как Qwen-Coder) и закрытыми моделями (такими как модели серии o1 и Claude), что подчеркивает важную роль соответствия моделей ИИ предпочтениям пользователей в задачах кодирования.</p>\n<p>Основные вклады данной работы:</p>\n<ol>\n<li>Представлен <strong>CodeArena</strong>, включающий 397 вручную аннотированных примеров, – всеобъемлющий бенчмарк для оценки соответствия ответов модели предпочтениям пользователей, охватывающий 7 основных категорий и 40 подкатегорий.</li>\n<li>Представлен <strong>SynCodeInstruct</strong> – крупномасштабный корпус синтетических инструкций для работы с кодом, полученный из веб-источников. На основе SynCodeInstruct создана эффективная модель Qwen2.5-SynCoder, используемая в качестве сильной базовой модели для CodeArena.</li>\n<li>Проведена систематическая оценка более 40 LLM на CodeArena и создан динамически обновляемый лидерборд. Результаты экспериментов показывают, что CodeArena эффективно измеряет соответствие ответов модели предпочтениям пользователей.</li>\n</ol>'}, {'title': 'Length question', 'content': '- maximum length - minimum length - avg length Baseline Answer - maximum length - minimum length - avg length 397 45 131 91 39 62 22 7 97/173/132 6736 tokens 5 tokens 291 tokens 5913 tokens 7 tokens 4517 tokens Table 1: CodeArena dataset statistics. Multiple Programming Languages Figure 3 plots the distribution of programming languages, where we strive to cover common programming languages in CodeArena. Unlike previous studies (Cassano et al., 2023), our benchmarks emphasize diverse range of programming languages that are commonly used in everyday programming tasks. For instance, we have incorporated languages like Google Apps Script (GAS) and PowerShell in CodeArena to better address the needs of practical Q&A scenarios. Evaluation Inspired by the previous work (Chiang et al., 2024), we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games compare and and compare and (avoid the relative position of and affecting the results) to calculate the win rate of compared to the baseline B. Decontainmation. To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPLE (Cassano et al., 2023), MBPP (Austin et al., 2021), McEval (Chen et al., 2021a), and NaturalCodeBench (Zhang et al., 2024). Comparison with other benchmarks We compare CodeArena with other code benchmarks. Our benchmark provides valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios. CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.', 'summary': '<p><strong>Описание набора данных CodeArena и методология оценки</strong></p>\n<p>В статье представлен набор данных CodeArena, предназначенный для оценки моделей машинного обучения в задачах, связанных с программированием. В таблице 1 приведены основные характеристики набора данных: максимальная, минимальная и средняя длина кода. Видно, что длина кода варьируется от очень коротких фрагментов до достаточно длинных.</p>\n<p>На рисунке 3 показано распределение языков программирования в CodeArena.  В отличие от предыдущих исследований,  в этом наборе данных сделан акцент на разнообразие языков, которые часто используются в повседневной практике.  Например,  были добавлены такие языки, как Google Apps Script (GAS) и PowerShell, чтобы лучше соответствовать реальным сценариям вопросов и ответов.</p>\n<p>Для оценки производительности моделей используется GPT-4o-2024-08-06 в качестве арбитра.  Методика оценки заключается в сравнении результатов работы модели с базовым решением. Для этого используются два варианта сравнения, чтобы исключить влияние порядка следования решений на результат.  В итоге вычисляется процент побед сравниваемой модели над базовой.</p>\n<p>Чтобы избежать утечки данных, в CodeArena была проведена деконтаминация.  Это означает, что из набора данных были удалены все точные совпадения (на основе 10-граммового перекрытия слов) с другими известными наборами данных, такими как MultiPLE, MBPP, McEval и NaturalCodeBench. Это гарантирует уникальность задач в CodeArena.</p>\n<p>Также проводится сравнение CodeArena с другими наборами данных для оценки кода.  CodeArena выделяется тем, что предоставляет комплексный набор задач, включающий 40 подзадач и 44 языка программирования. Это позволяет проводить оценку моделей в условиях, более приближенных к реальным.  Многие задачи в CodeArena не подходят для проверки с помощью модульного тестирования, что делает этот набор данных более подходящим для оценки моделей в реалистичных сценариях.</p>'}, {'title': 'SynCode-Instruct', 'content': 'Recall from Common Crawl. trained fasttext is used to distinguish the code-related text and other common raw text, which is used to recall and clean potential code data and filter out low-quality content using weak model-based classifiers and scorers. Our approach encompasses both file-level and repository-level pertaining to ensure comprehensive coverage. Code Classification for Code Snippet. We extract the first layer of CodeBERT (Feng et al., 2020) and fine-tune the tiny classifier on nearly 100 programming languages to build language identification model. We keep the main language data (e.g. C, Python, and Java) and downsample highresource language data (e.g. HTML and Java) to keep the balance. Besides, we also remove the samples with no code snippets. Scaling Code Instruction Initially, we adopt rule-based filtering to clean pre-extracted content from recalled documents by removing site information, advertisements, and HTML tags, thereby significantly reducing document length for further processing. Different from the previous work (Yue et al., 2024), we utilize Qwen2.5-72B to create new questions instead of extracting question and Figure 2: Task types of CodeArena. Difficulty levels of CodeArena Figure 4 illustrates the difficulty levels of CodeArena, where all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting significant challenge to LLMs. Human Annotation & Quality Control To make CodeArena comprehensive evaluation benchmark, we implement rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check. All annotators participate in annotation tutorial and learn the annotation guidelines. The annotation process involved creating new question based on the given question, checking the difficulty level (easy/medium/hard) based on the complexity of the prompt, and annotating the corresponding programming languages. Following the classification in Figure 2, we uniformly sample 2K samples and assign them to annotators. The annotators select 822 suitable original samples to create queries. The process includes regular quality checks and feedback sessions to maintain high standards throughout the annotation phase, which results in diverse and well-curated dataset spanning multiple programming languages and tasks, suitable for evaluating and improving alignment between the human preference and model-generated response. The other four senior programming developers vote on the same issue to determine whether it is valid and can be resolved. Finally, 397 samples are kept (at least 3 checkers reach consensus) to from CodeArena, considering the cost of the LLM-as-a-judge. Figure 3: Statistics of programming languages in CodeArena. Figure 4: Number of samples of different difficulties (Easy/Medium/Hard) across categories in CodeArena.', 'summary': '<h2>Извлечение и очистка данных, классификация и создание инструкций</h2>\n<p>Для сбора данных использовался Common Crawl. Сначала применялась модель fasttext, обученная отличать текст, связанный с кодом, от обычного текста. Это помогало отбирать потенциальные данные с кодом и отфильтровывать низкокачественный контент с помощью слабых классификаторов и скореров. Такой подход применялся как к файлам, так и к репозиториям, обеспечивая более полное покрытие.</p>\n<p>Далее, для классификации фрагментов кода, извлекался первый слой модели CodeBERT и на его основе дообучался небольшой классификатор для определения языка программирования. Модель обучалась на данных почти 100 языков. Основные языки (например, C, Python и Java) были сохранены, а данные по языкам с большим объемом (например, HTML и Java) были сокращены для баланса. Также были удалены образцы без фрагментов кода.</p>\n<p>Для масштабирования инструкций по коду, на начальном этапе применялась фильтрация на основе правил для очистки предварительно извлеченного контента. Удалялась информация о сайтах, реклама и HTML-теги, что значительно сокращало длину документов для дальнейшей обработки. В отличие от предыдущих работ, для создания новых вопросов использовалась модель Qwen2.5-72B, а не извлечение вопросов из существующих данных.</p>\n<h2>Разметка данных и контроль качества</h2>\n<p>Для создания всестороннего бенчмарка CodeArena, был реализован строгий процесс разметки данных людьми. В нем участвовали 4 штатных сотрудника, владеющих различными языками программирования, и 4 опытных разработчика для проверки качества. Все разметчики прошли обучение и ознакомились с инструкциями.</p>\n<p>Процесс разметки включал создание новых вопросов на основе имеющихся, определение уровня сложности (легкий, средний, сложный) в зависимости от сложности запроса и аннотирование соответствующих языков программирования. Было выбрано 2000 образцов, которые были распределены между разметчиками. Разметчики выбрали 822 подходящих исходных образца для создания запросов. Процесс включал регулярные проверки качества и обратную связь.</p>\n<p>Затем другие четыре опытных разработчика голосовали за каждый вопрос, чтобы определить, является ли он корректным и может ли быть решен. В итоге, для CodeArena было отобрано 397 образцов (при условии согласия не менее 3 проверяющих). Это было сделано с учетом стоимости использования LLM в качестве судьи.</p>\n<p>В результате был получен разнообразный и хорошо отобранный набор данных, охватывающий множество языков программирования и задач, подходящий для оценки и улучшения соответствия между предпочтениями человека и ответами, сгенерированными моделью.</p>'}, {'title': 'Human preference', 'content': 'Table 2: Comparison between CodeArena and other benchmarks. CodeArena provides comprehensive view by creating diverse user prompts to evaluation alignment between the model-generated response and human preference. answer pairs. As shown in Figure 6. We use the Qwen2.5-Coder to generate multiple responses by sampling for the same document. For the algorithmic generated question and answer, we first adopt fine-tuned generator to generate the test cases and adopt the multilingual sandbox to verify the correctness of the generated code snippet. As shown in Figure 5, for the non-algorithmic query, we first randomly generate four candidates (Best-of-N) and use the LLM to score the candidates (LLM scorer), where the candidates are fed into the LLM to select the best response with the reason. For the algorithmic queries, the generated test cases by LLM are used to verify the correctness of the responses (Executor). Finally, we select the response with the best score as the response to create SynCodeInstruct. The synthetic instruction corpora generated by Qwen2.5 is used for the first stage and the high-quality data from GPT-4o is used for the second stage.', 'summary': '<p>В статье описывается методика создания набора данных SynCodeInstruct для обучения моделей программирования. Этот набор данных уникален тем, что он нацелен на оценку соответствия между ответами, сгенерированными моделью, и предпочтениями человека. </p>\n<p>Для создания этого набора данных использовались следующие подходы:</p>\n<p><strong>1. Генерация ответов:</strong> \n   - Для одного и того же запроса к модели Qwen2.5-Coder генерировалось несколько вариантов ответа. Это делалось путем "семплирования", то есть случайного выбора из вероятностного распределения возможных ответов.</p>\n<p><strong>2. Оценка ответов:</strong>\n   - <strong>Алгоритмические запросы:</strong> Для запросов, требующих написания кода, сначала генерировались тестовые случаи с помощью специально обученной модели. Затем сгенерированный код проверялся на корректность в многоязычной "песочнице" (sandbox).\n   - <strong>Неалгоритмические запросы:</strong> Для запросов, не связанных с программированием, генерировалось четыре случайных варианта ответа. Затем большая языковая модель (LLM) оценивала эти варианты, выбирая лучший и объясняя свой выбор. Это называется "Best-of-N" подходом с использованием LLM-скорера.</p>\n<p><strong>3. Выбор лучшего ответа:</strong>\n   - Для алгоритмических запросов выбирался ответ, успешно прошедший проверку тестовыми случаями (Executor).\n   - Для неалгоритмических запросов выбирался ответ, получивший наивысшую оценку от LLM-скорера.</p>\n<p><strong>4. Создание SynCodeInstruct:</strong>\n   - Выбранные лучшие ответы вместе с исходными запросами формировали набор данных SynCodeInstruct.</p>\n<p><strong>5. Двухэтапный подход:</strong>\n   - На первом этапе для генерации данных использовалась модель Qwen2.5.\n   - На втором этапе использовались более качественные данные, сгенерированные моделью GPT-4o.</p>\n<p><strong>Ключевые моменты:</strong>\n- <strong>Разнообразие запросов:</strong> CodeArena создает разнообразные запросы, чтобы оценить соответствие сгенерированных ответов человеческим предпочтениям.\n- <strong>Автоматизированная оценка:</strong> Использование LLM для оценки неалгоритмических запросов и автоматическая проверка кода для алгоритмических.\n- <strong>Двухэтапное обучение:</strong> Использование сначала менее мощной, а затем более мощной модели для генерации данных.</p>\n<p>Таким образом, SynCodeInstruct - это набор данных, созданный с использованием автоматизированных методов, которые позволяют оценить качество ответов модели с точки зрения соответствия человеческим предпочтениям и корректности кода.</p>'}, {'title': 'Instruction dataset', 'content': 'CodeLLMs We evaluate 40+ models with sizes ranging from 0.5B to 200B parameters, including general/code LLMs and open/closed-source models. For general models, we evaluate GPTs (Brown et al., 2020; OpenAI, 2023) (GPT-3.5-Turbo, GPT4-o), Qwen series (Qwen2.5 and QwenMax) (Bai et al., 2023), Claude series (Anthropic, 2023), Llama3/3.1 (Dubey et al., 2024), Yi (Young et al., 2024), and o1 series. For code models, Figure 5: Overview of the CodeArena creation benchmark. We first collect the online code Q&A and code-related raw text from the website. We cluster the code-related data and classify them into different categories using LLM. We uniformly sample the samples from different subtasks as the seed data for manual annotation. et al., 2021) to test the code generation capabilities. The benchmark reports the scores of HumanEval (HE)/MBPP with base test cases and HumanEval+ (HE+)/MBPP+ with plus test cases. MultiPL-E The MultiPL-E test set (Cassano et al., 2023) contains the HumanEval (Python) and translated test set of other programming languages, i.e., Java, C++, Javascript, and Typescript. CodeArena Different from the EvalPlus and MultiPL-E, CodeArena consists of many nonalgorihtmic, which is not suitable for codeexecution-based evaluation. Each question is scored twice to calculate the win rate and tie rate by GPT-4o using different input order A, and B, A, where is the baseline from gpt-4-turbo-2024-04-09 and is the model-generated response.', 'summary': '<p>В данном исследовании проводится оценка более 40 моделей машинного обучения с размерами от 0.5 миллиарда до 200 миллиардов параметров. В выборку входят как общие языковые модели (LLM), так и специализированные модели для работы с кодом, а также модели с открытым и закрытым исходным кодом.</p>\n<p>Среди общих моделей рассматриваются:</p>\n<ul>\n<li>Модели GPT (включая GPT-3.5-Turbo и GPT4-o)</li>\n<li>Серия Qwen (Qwen2.5 и QwenMax)</li>\n<li>Серия Claude</li>\n<li>Llama3/3.1</li>\n<li>Yi</li>\n<li>Серия o1</li>\n</ul>\n<p>Среди моделей для работы с кодом для оценки возможностей генерации кода используются следующие наборы данных:</p>\n<ul>\n<li><strong>HumanEval (HE) и MBPP:</strong> Эти наборы данных используются с базовыми тестовыми случаями, а также с расширенными тестовыми случаями (HE+ и MBPP+). Это позволяет оценить, насколько хорошо модели справляются с генерацией кода на основе заданных спецификаций.</li>\n<li><strong>MultiPL-E:</strong> Этот набор данных расширяет HumanEval, добавляя переводы тестовых случаев на другие языки программирования, такие как Java, C++, Javascript и Typescript. Это позволяет оценить способность моделей генерировать код на нескольких языках.</li>\n</ul>\n<p>Также вводится новый набор данных <strong>CodeArena</strong>, который отличается от EvalPlus и MultiPL-E тем, что состоит из множества неалгоритмических задач. Это означает, что для оценки не подходит стандартный подход, основанный на выполнении кода. Вместо этого, для каждой задачи используется подход сравнения ответов, с помощью модели GPT-4o. Ответы сравниваются в двух вариантах: A против B и B против A, где A - это ответ модели gpt-4-turbo-2024-04-09, а B - ответ тестируемой модели. На основе этих сравнений рассчитывается процент побед и ничьих. Это позволяет оценить качество ответов модели в задачах, где нет однозначного правильного решения.</p>'}, {'title': 'Evaluation benchmark', 'content': 'EvalPlus and MultiPL-E. The EvalPlus (Liu et al., 2023) is an upgraded version of the HumanEval (Chen et al., 2021a) and MBPP (Austin LLM as judgement Due to the high cost of collecting human preferences (Zheng et al., 2023a), we use pairwise comparison for judgment, where an LLM judger is fed with question and two answers and determines which one is better or Model Size UI&UX Development& Programming Specialized Computing Tools, Environs, & Practices Emerging Techs &Apps Miscellaneous & General Inquiry Databases& Data Handling Avg. Claude-3.5-Sonnet-20240620 Claude-3.5-Sonnet-20241022 GPT-3.5-turbo-0125 GPT-4o-mini-2024-07-18 GPT-4o-2024-08-06 o1-mini o1-preview Yi-lightning Doubao-Pro Qwen-Max Proprietary LLMs and 200B+ LLMs (cid:181) 88.9/2.2 (cid:181) 82.2/6.7 (cid:181) 17.8/24.4 (cid:181) 71.1/13.3 (cid:181) 66.7/17.8 (cid:181) 93.3/4.4 (cid:181) 93.3/2.2 (cid:181) 62.2/15.6 (cid:181) 51.1/20.0 (cid:181) 75.6/17.8 77.3/13.6 75.8/12.9 11.4/20.5 62.1/17.4 72.7/19.7 94.7/2.6 81.8/7.6 60.0/11.5 40.8/18.5 74.2/13.6 74.2/18.0 76.4/16.9 4.5/19.1 50.0/13.6 62.9/19.1 84.1/7.6 85.4/7.9 57.9/5.3 55.3/26.3 59.6/24.7 81.4/11.9 84.7/10.2 11.9/18.6 65.2/14.6 69.5/15.3 91.0/5.6 78.0/6.8 49.4/16.9 38.2/19.1 78.0/6.8 0.5B+ Open-source LLMs Qwen2.5-0.5B-Instruct Qwen2.5-Coder-0.5B-Instruct 0.5B 0.5B 2.2/4.4 2.2/2.2 4.6/4.6 4.6/6.9 5.3/10.5 2.6/5.3 2.2/4.5 4.5/2. DS-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat Qwen2.5-Coder-1.5B-Instruct OpenCoder-1.5B-Instruct 1.3B 1.5B 1.5B 1.5B 66.7/2.2 11.1/2.2 11.1/4.4 11.1/4.4 2.3/5.4 5.1/3.4 15.9/9.1 3.8/5.4 1B+ Open-source LLMs 2.6/10.5 5.4/4.6 9.0/16.9 0.0/5. 1.7/6.8 2.6/5.3 13.6/11.9 2.2/4.5 3B+ Open-source LLMs 78.9/10.5 84.2/13.2 10.5/21.1 72.9/13.6 76.3/13.2 88.1/3.4 92.1/2.6 71.2/11.9 47.5/22.0 68.4/23.7 3.4/5.1 3.4/5.1 0.0/9.1 2.2/5.6 13.2/5.3 3.4/8.5 71.4/28.6 57.1/28.6 13.6/9.1 71.1/18.4 85.7/14.3 95.5/0.0 77.3/4.5 54.5/13.6 36.4/31.8 100.0/0. 63.6/4.5 68.2/22.7 0.0/14.3 71.4/14.3 59.1/22.7 100.0/0.0 71.4/28.6 85.7/0.0 42.9/57.1 81.8/4.5 77.8/12.5 78.1/13.5 10.5/19.6 65.8/15.6 69.1/18.1 89.3/5.1 83.9/6.6 59.5/12.6 43.6/21.5 71.9/15.8 4.5/9.1 4.5/0.0 0.0/14.3 28.6/14.3 3.6/5.6 4.4/4.6 2.2/3.4 4.5/4.5 14.3/42.9 4.5/9. 0.0/14.3 14.3/14.3 18.2/4.5 0.0/0.0 2.6/5.6 7.4/5.1 13.2/10.7 6.7/3.8 Qwen2.5-Coder-3B-Instruct 3B 35.6/11.1 29.5/10.6 27.0/15. 20.3/18.6 28.9/10.5 42.9/14.3 27.3/13.6 28.3/13.3 6B+ Open-source Models CodeLlama-7B-Instruct Llama3-8B-Instruct Llama3.1-8B-Instruct DS-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat DS-Coder-V2-Lite-Instruct Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct 7B 33.3/8.9 7B 20.0/17.8 7B 2.2/8.9 6.7B 11.1/17.8 7B 17.8/15.6 9B 15.6/17.8 2.4/16B 42.2/20.0 7B 40.0/22.2 24.4/8.9 8B 28.8/18.6 14.6/11.5 4.5/10.1 13.1/13.8 13.8/12.3 15.4/9.2 33.3/17.4 46.2/19.7 14.6/8.5 23.8/13.8 15.8/2.6 3.8/6.2 13.6/8.5 15.8/0.0 15.8/7.9 31.5/16.9 43.8/15.7 10.5/7. 18.2/9.1 13.5/9.0 3.4/6.8 13.2/7.9 15.7/9.0 13.5/13.5 35.6/20.3 40.7/20.3 9.0/4.5 13B+ Models CodeLlama-13B-Instruct Starcoder2-15B-Instruct-v0.1 Qwen2.5-Coder-14B-Instruct 13.3/4.4 6.7/6.7 13B 15B 14B 51.1/24.4 7.9/6.7 6.8/12.9 53.0/17. 6.8/8.5 4.5/15.7 52.8/16.9 7.7/6.2 6.8/6.8 50.8/18.6 20B+ Models CodeLlama-34B-Instruct CodeStral-22B-v0.1 DS-Coder-33B-Instruct CodeLlama-70B-Instruct DS-Coder-V2-Instruct DS-V2.5 Llama3-70B-Instruct Llama3.1-70B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-32B-Instruct QwQ-32B-Preview Qwen2.5-72B-Instruct Qwen2.5-SynCoder 34B 11.1/6.7 22B 17.8/22.2 33B 13.3/11.1 70B 11.1/22.2 21/236B 55.6/11.1 21/236B 77.8/11.1 7B 35.6/20.0 7B 48.9/24.4 32B 71.1/13.3 32B 62.2/15.6 32B 53.3/15.6 72B 82.2/6.7 32B 55.6/26.7 2.6/2.6 27.3/13.6 22.0/9.8 9.2/10.0 62.1/18.2 72.0/12.9 26.2/26.2 43.8/20.0 66.7/15.9 52.3/15.4 56.8/16.7 71.5/14.6 49.2/20. 6.9/2.3 14.6/14.6 12.4/12.4 10.5/5.3 60.7/14.6 71.9/13.5 25.4/22.0 34.2/26.3 67.4/16.9 57.9/18.4 50.6/16.9 76.3/13.2 36.8/36.8 8.5/6.8 25.4/10.2 13.6/6.8 9.0/6.7 50.8/18.6 71.2/8.5 34.2/15.8 40.4/22.5 74.6/13.6 50.6/23.6 64.4/5.1 75.3/15.7 50.6/20.2 31.6/5.3 16.9/11.9 5.3/2.6 9.0/7.9 15.3/15.3 10.2/20.3 39.5/21.1 34.2/15.8 13.6/6.8 4.5/4.5 5.3/13.2 57.9/7.9 7.9/10.1 18.4/10.5 13.2/18.4 16.9/8.5 52.6/21.1 73.7/10.5 23.6/14.6 54.2/20.3 65.8/18.4 54.2/13.6 52.6/21.1 71.2/18.6 52.5/20.3 29.2/14.6 22.7/0.0 9.1/9.1 13.6/4.5 18.2/13.6 18.2/13.6 71.4/14.3 71.4/0.0 18.2/9. 5.3/5.3 13.6/13.6 28.6/28.6 9.1/9.1 14.3/42.9 28.6/42.9 9.1/13.6 71.4/14.3 100.0/0.0 36.4/4.5 45.5/9.1 100.0/0.0 50.0/13.6 85.7/0.0 63.6/13.6 40.9/18.2 71.4/0.0 57.1/14.3 14.3/0.0 28.6/0.0 14.3/42.9 28.6/28.6 31.8/22.7 40.9/22.7 14.3/0.0 28.2/12.8 16.7/10.3 7.9/4.4 12.3/10.8 15.4/11.8 14.6/13.3 35.5/18.6 43.1/18.6 14.1/7.1 14.3/14.3 0.0/14.3 36.4/27.3 11.2/7.9 6.4/12.0 60.6/51. 14.3/0.0 22.7/22.7 22.7/18.2 0.0/0.0 40.9/31.8 68.2/13.6 14.3/57.1 71.4/14.3 63.6/18.2 71.4/14.3 63.6/9.1 85.7/14.3 57.1/0.0 7.7/5.6 21.7/15.8 16.8/12.0 15.5/10.5 57.4/17.6 73.0/11.7 27.7/20.5 44.9/21.0 68.9/15.6 54.1/17.1 56.6/14.5 73.8/14.4 49.2/22.3 Table 3: The win/tie rate of different instruction LLMs on CodeArena. The underlined numbers represent the best scores within the same model size range. declares tie2. We report win rate/tie rate for CodeArena. 4.', 'summary': '<p>В статье рассматриваются результаты тестирования различных языковых моделей (LLM) на наборе задач CodeArena, который оценивает их способности в программировании. Для оценки моделей используется методика парного сравнения, где LLM-судья определяет, какой из двух предложенных ответов на задачу является лучшим или объявляет ничью.</p>\n<p>Оценка проводится с помощью набора данных EvalPlus, который является улучшенной версией HumanEval и MBPP. EvalPlus содержит более сложные задачи, требующие от моделей не только генерации кода, но и его понимания и отладки.</p>\n<p>В таблице 3 представлены результаты сравнения различных моделей, сгруппированных по размеру (количество параметров). Для каждой модели указан процент побед и ничьих (win rate/tie rate) на задачах CodeArena. Подчеркнуты лучшие результаты в каждой группе моделей одного размера.</p>\n<p>В таблице представлены результаты как проприетарных моделей (например, Claude, GPT), так и открытых моделей различных размеров (от 0.5B до 70B+ параметров). Это позволяет сравнивать производительность моделей различных архитектур и масштабов.</p>\n<p>В целом, таблица демонстрирует, что более крупные модели, как правило, показывают лучшие результаты, но также есть модели с меньшим количеством параметров, которые достигают сравнимых результатов в определенных категориях задач. Это говорит о том, что размер модели не единственный фактор, влияющий на ее производительность.</p>'}, {'title': 'Main results', 'content': 'CodeArena. Table 3 shows that the win rate/tie rate of different instruction LLM on CodeArena. The closed-source LLMs such as Claude and o1 series still get dominant advantage compared to Qwen2.5-Coder and DeepseekCoder. There still exists notable performance gap between open codeLLMs (e.g. Qwen-Coder) and closed-source LLMs (e.g., o1 and Claude series), emphasizing the Model Size HE HE+ MBPP MBPP+ Python Java C++ C# TS JS PHP Bash Avg. Closed-APIs Claude-3.5-Sonnet-20240620 Claude-3.5-Sonnet-20241022 GPT-4o-mini-2024-07-18 GPT-4o-2024-08-06 o1-mini o1-preview (cid:181) 89.0 (cid:181) 92.1 (cid:181) 87.8 (cid:181) 92.1 (cid:181) 97.6 (cid:181) 95. 81.1 86.0 84.8 86.0 90.2 88.4 87.6 91.0 86.0 86.8 93.9 93.4 72.0 74.6 72.2 72.5 78.3 77.8 89.6 93.9 87.2 90.9 95.7 96.3 86.1 86.7 75.9 83.5 90.5 88.0 82.6 88.2 77.6 76.4 93.8 91. 85.4 87.3 79.7 81.0 77.2 84.2 84.3 88.1 79.2 83.6 91.2 90.6 84.5 91.3 81.4 90.1 92.5 93.8 80.7 82.6 75.2 78.9 84.5 90.1 48.1 52.5 43.7 48.1 55.1 47.5 80.2 83.8 75.0 79.1 85.1 85. 0.5B+ Models Qwen2.5-Coder-0.5B-Instruct 0.5B 61.6 57.3 52.4 43. 61.6 57.3 52.4 43.7 50.3 50. 52.8 27.8 49.6 1B+ Models DS-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat Qwen2.5-Coder-1.5B-Instruct 1.3B 65.9 1.5B 69.5 1.5B 70. 60.4 64.0 66.5 65.3 65.9 69.2 54.8 57.7 59.4 65.2 67.7 71.2 51.9 51.9 55.7 45.3 49.1 50. 55.1 57.6 64.6 59.7 57.9 61.0 52.2 59.6 62.1 45.3 52.2 59.0 12.7 19.0 29.1 48.4 51.9 56. 3B+ Models Qwen2.5-Coder-3B-Instruct 3B 84.1 80.5 73.6 62. 83.5 74.7 68.3 78.5 79.9 75. 73.3 43.0 72.1 CodeLlama-7B-Instruct DS-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat DS-Coder-V2-Lite-Instruct Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct 7B 40.9 6.7B 74.4 7B 83.5 9B 82.3 2.4/16B 81.1 7B 88.4 8B 83.5 33.5 71.3 78.7 74.4 75.6 84.1 78. CodeLlama-13B-Instruct Starcoder2-15B-Instruct-v0.1 Qwen2.5-Coder-14B-Instruct 13B 40.2 15B 67.7 14B 89.6 32.3 60.4 87.2 CodeLlama-34B-Instruct CodeStral-22B-v0.1 DS-Coder-33B-Instruct CodeLlama-70B-Instruct DS-Coder-V2-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-SynCoder 34B 48.2 22B 81.1 33B 81.1 70B 72.0 21/236B 85.4 32B 92.7 32B 87.8 32B 85.4 32B 92.7 40.2 73.2 75.0 65.9 82.3 87.2 82.9 79.3 87. 6B+ Models 44.4 65.6 67.2 69.0 70.4 71.7 69.0 13B+ Models 51.1 65.1 72.8 20B+ Models 50.5 62.2 70.1 64.6 75.1 75.1 70.9 77.0 74. 54.0 74.9 77.7 82.0 82.8 83.5 79.1 60.3 78.0 86.2 61.1 78.2 80.4 77.8 89.4 90.2 86.8 90.5 86.2 34.8 78.6 84.1 85.4 81.1 87.8 83.5 42.7 68.9 89.0 41.5 81.1 79.3 67.8 90.2 92.7 88.4 82.9 92. 30.4 68.4 73.4 76.0 76.6 76.5 72.2 31.1 63.4 74.5 67.7 75.8 75.6 61.5 21.6 72.8 77.8 76.6 76.6 80.3 75.9 32.7 67.2 71.7 72.3 80.5 81.8 78.0 - 72.7 75.2 78.9 77.6 83.2 79.5 28.6 68.9 70.8 72.1 74.5 78.3 73. 10.1 36.7 39.2 45.6 43.0 48.7 44.3 40.5 53.8 79.7 42.2 50.9 85.1 24.0 62.7 84.2 39.0 57.9 86.8 - 59.6 84. 32.3 53.4 80.1 13.9 24.7 47.5 43.7 63.3 73.4 58.2 82.3 80.4 80.4 81.0 80.4 45.3 65.2 68.9 53.4 84.8 79.5 81.0 80.7 80.7 31.0 43.7 74.1 36.7 82.3 82.9 74.5 81.6 81.6 40.3 68.6 67.9 39.0 83.0 86.8 83.5 81.1 83. - - 73.9 - 84.5 85.7 82.4 82.0 85.7 36.6 68.9 72.7 58.4 79.5 78.9 78.3 77.0 77.6 19.6 42.4 43.0 29.7 52.5 48.1 46.8 48.7 49.4 - 66.1 70.8 71.8 73.2 76.5 71.0 - 54.0 79.6 - - 69.2 - 79.9 79.4 76.9 75.1 78. Table 4: The performance of different instruction LLMs on EvalPlus and MultiPL-E. HE denotes the HumanEval, HE+ denotes the plus version with more test cases, and MBPP+ denotes the plus version with more test cases. importance of alignment between model-generated response human preference. Qwen2.5-SynCoder totally trained on the large-scale synthetic instruction corpus SynCode-Instruct can still get strong performance on CodeArena, which verifies the correctness of the route of taking large-scale synthetic data to improve model performance. EvalPlus and MultiPL-E. Table 4 shows that Qwen2.5-SynCoder significantly beats previous strong open-source baselines using large-scale synthetic instruction, closing the gap with GPT-4o and Claude, which verifies that the large-scale synthetic data can bring more significant improvement for the base model in the code-execution-based benchmark (code generation) compared to CodeArena.', 'summary': '<p>В таблице 3 показаны результаты соревнования моделей, обученных для генерации кода, на платформе CodeArena. В частности, сравниваются показатели побед и ничьих (win rate/tie rate) различных моделей.</p>\n<p>Из таблицы видно, что закрытые модели, такие как Claude и o1, по-прежнему демонстрируют значительное преимущество по сравнению с открытыми моделями Qwen2.5-Coder и DeepseekCoder. Подчеркивается, что существует заметный разрыв в производительности между открытыми (например, Qwen-Coder) и закрытыми моделями (например, o1 и Claude), что говорит о важности размера модели и качества обучения.</p>\n<p>Далее в таблице 4 представлены результаты моделей на наборах данных EvalPlus и MultiPL-E.  Qwen2.5-SynCoder, модель, обученная на большом наборе синтетических данных SynCode-Instruct, показывает хорошие результаты на CodeArena, что подтверждает эффективность использования синтетических данных для улучшения производительности модели.</p>\n<p>Также отмечается, что Qwen2.5-SynCoder значительно превосходит предыдущие сильные открытые модели на EvalPlus и MultiPL-E, приближаясь по производительности к GPT-4o и Claude. Это подтверждает, что использование больших объемов синтетических данных может привести к более существенному улучшению базовой модели в задачах, связанных с выполнением кода (генерация кода), по сравнению с CodeArena.</p>'}, {'title': 'Discussion', 'content': 'Examples of CodeArena. Figure 7 lists six examples from the different subtasks, covering Python, HTML, CSS, and Java. Different from the previous benchmarks (Cassano et al., 2023; Jain et al., 2024) comprised of algorithmic questions in fixed format, the queries of CodeArena are more consistent with the distribution of user questions in real Q&A scenarios. For example, the query huggingface dataset move all the columns to metadata, except two, problem and solution is closer to the question style of real users. For the baseline response and model-generated response B, the GPT4o thinks beats based on the judgment provides correct and relevant solution using the appropriate library for Hugging Face datasets, which select responses that are more aligned with human preferences. Difference between CodeArena and Executionbased Benchmark. Compared to the benchmark MultiPL-E evaluated by code execution, CodeArena is created from real-world Q&A and evaluated by LLM-as-a-judge to evaluate the alignment between the model-generated response and human preference. For example, the LLMs tend to only generate the code without any natural descripFigure 7: Examples of CodeArena. The LLM judger decides which response is better. Figure 8: Comparison between MultiPL-E and CodeArena. LLMs in the blue circle present relatively mismatched performances on two benchmarks. Figure 9: Results of CodeArena with different data size on MultiPL-E and CodeArena. tion (even the code is correct) will bring an unsatisfactory experience to users, which will also lead to poor performance in CodeArena. In Figure 8, we can observe that the state-of-the-art closed-source LLMs (e.g. o1 and Claude series) get balanced performance between the code execution benchmark and CodeArena. The open-source models (e.g. DeepseekCoder and Qwen-Coder) are likely to bring bad experience to users, where the generated response lacks more detailed explanation or more complete details compared to closed-source LLMs. Scaling Synthetic Instruction Corpora. We would like to further analyze the performance of Qwen2.5-SynCoder in MultiPl-E and CodeArena given different sizes of instruction corpora. Therefore, we select the full instruction (19B synthetic data is at the front of the data and 1B high-quality data is at the end) set SynCode-Instruct and extract the first billion tokens as the fine-tuned data. We set = {2, 4, . . . , 20}. We randomly extract specific data from the whole sentence pairs. Figure 9 shows the performance on CodeArena. With the increase of instruction data, Qwen2.5-SynCoder still can get significant improvement, which emphasizes the importance of the scaling instruction corpora. Besides, the two-stage SFT gets better performance compared to the one-stage training (red line), where the high-quality data brings huge improvement at last. Distribution of different benchmarks. We visualize the queries of CodeArena and MultiPL-E (Python, Java, and CPP) by extracting the encoder representations of the last layer for t-SNE (Van der Maaten and Hinton, 2008). The average of all hidden states of the last encoder layer is regarded as the query representation. In Figure 10, the representations of CodeArena are distributed in the LLMs, numerous benchmarks have been proposed, including code translation (Jiao et al., 2023; Yan et al., 2023; Zhu et al., 2022), code retrieval (Huang et al., 2021; Husain et al., 2019; Lu et al., 2021), code completion (Bavarian et al., 2022; Liu et al., 2024a; Zhang et al., 2023), code debugging (Huq et al., 2022; Tian et al., 2024; Liu et al., 2024b), and structured data understanding (Wu et al., 2024; Su et al., 2024). Recent initiatives such as McEval (Chai et al., 2024) have expanded the evaluative scope to 40 programming languages for multilingual scenarios, while MdEval (Liu et al., 2024b) has developed multilingual code debugging benchmark encompassing nearly 20 programming languages. Nonetheless, many of these studies concentrate on assessing only single aspect of LLM capabilities, often overlooking the evaluation of LLMs as comprehensive program developers across variety of real-world coding scenarios. In this work, we propose FullStack Bench to evaluate the capabilities of LLMs across multiple practical code development contexts.', 'summary': '<p>В статье представлен новый бенчмарк CodeArena, предназначенный для оценки способности больших языковых моделей (LLM) отвечать на вопросы о коде в стиле, приближенном к реальным пользовательским запросам.</p>\n<p><strong>Отличия CodeArena от других бенчмарков:</strong></p>\n<ul>\n<li><strong>Реалистичные запросы:</strong> В отличие от предыдущих бенчмарков, которые в основном содержали алгоритмические задачи в фиксированном формате, CodeArena использует запросы, более похожие на те, которые пользователи задают в реальных ситуациях. Например, запрос "huggingface dataset move all the columns to metadata, except two, problem and solution" (переместить все столбцы в метаданные, кроме двух, problem и solution, в наборе данных huggingface) отражает типичный вопрос пользователя.</li>\n<li><strong>Оценка на основе предпочтений человека:</strong> Вместо оценки на основе выполнения кода, как в бенчмарке MultiPL-E, CodeArena оценивает ответы LLM с помощью другой LLM, выступающей в роли судьи. Эта модель-судья определяет, какой ответ лучше соответствует человеческим предпочтениям, учитывая не только корректность кода, но и его релевантность и наличие пояснений.</li>\n<li><strong>Акцент на пользовательском опыте:</strong> CodeArena учитывает, что LLM, генерирующие только код без каких-либо пояснений (даже если код правильный), могут не удовлетворить пользователя. Это приводит к более низким оценкам в CodeArena по сравнению с бенчмарками, ориентированными только на исполнение кода.</li>\n</ul>\n<p><strong>Сравнение с MultiPL-E:</strong></p>\n<ul>\n<li>Бенчмарк MultiPL-E оценивает LLM на основе корректности выполнения сгенерированного кода.</li>\n<li>CodeArena оценивает LLM на основе соответствия сгенерированного ответа человеческим предпочтениям, учитывая как корректность кода, так и его релевантность и наличие пояснений.</li>\n<li>Некоторые LLM, такие как модели DeepseekCoder и Qwen-Coder, показывают хорошие результаты в MultiPL-E, но получают более низкие оценки в CodeArena, поскольку их ответы часто не содержат подробных объяснений или полных деталей. Закрытые модели (например, GPT-4o и Claude) показывают более сбалансированные результаты на обоих бенчмарках.</li>\n</ul>\n<p><strong>Влияние размера обучающих данных:</strong></p>\n<ul>\n<li>Исследование показало, что увеличение размера обучающих данных для модели Qwen2.5-SynCoder приводит к значительному улучшению результатов в CodeArena. Это подчеркивает важность масштабирования обучающих данных для улучшения производительности LLM в задачах генерации кода.</li>\n<li>Двухэтапное обучение (сначала на большом наборе синтетических данных, затем на небольшом наборе высококачественных данных) дает лучшие результаты, чем одноэтапное обучение, что говорит о важности высококачественных данных для улучшения производительности.</li>\n</ul>\n<p><strong>Распределение запросов:</strong></p>\n<ul>\n<li>Визуализация запросов CodeArena и MultiPL-E с использованием t-SNE показала, что запросы CodeArena распределены более равномерно, в то время как запросы MultiPL-E образуют более выраженные кластеры. Это говорит о том, что CodeArena охватывает более широкий спектр реальных пользовательских запросов.</li>\n</ul>\n<p><strong>Общая идея:</strong></p>\n<p>CodeArena представляет собой более реалистичный и всесторонний способ оценки LLM в задачах генерации кода, учитывая как корректность кода, так и его соответствие человеческим предпочтениям и пользовательскому опыту. Это позволяет выявить сильные и слабые стороны различных моделей и стимулирует разработку LLM, которые не только генерируют правильный код, но и предоставляют полезные и понятные ответы для пользователей.</p>'}, {'title': 'Conclusion', 'content': 'In this work, We introduce CodeArena, meticulously human-curated benchmark composed of 397 high-quality samples spanning 40 categories, derived from real-world user queries, to address discrepancies between model-generated responses and human preferences in coding tasks. Additionally, we create SynCode-Instruct, diverse synthetic instruction corpus containing nearly 20 billion tokens, by scaling web-sourced instructions. Our evaluation of over 20 large language models (LLMs) using CodeArena highlights significant performance discrepancies between code-executionbased benchmarks and our human-curated benchmark. Notably, there is marked performance gap between open-source code LLMs (such as DeepSeek-Coder) and closed-source LLMs (such as the o1 and Claude series), underscoring the importance of aligning AI models with human preferences in coding tasks.', 'summary': '<p>В данной работе представлен CodeArena, тщательно разработанный людьми эталонный набор данных, состоящий из 397 высококачественных примеров, охватывающих 40 категорий. Эти примеры основаны на реальных запросах пользователей и предназначены для устранения расхождений между ответами, сгенерированными моделями, и предпочтениями людей в задачах кодирования. Кроме того, создан SynCode-Instruct, разнообразный корпус синтетических инструкций, содержащий около 20 миллиардов токенов. Этот корпус был получен путем масштабирования инструкций из веб-источников.</p>\n<p>Оценка более 20 больших языковых моделей (LLM) с использованием CodeArena выявила значительные различия в производительности между эталонами, основанными на выполнении кода, и нашим эталоном, разработанным людьми. Примечательно, что существует заметный разрыв в производительности между LLM с открытым исходным кодом (такими как DeepSeek-Coder) и LLM с закрытым исходным кодом (такими как серии o1 и Claude). Это подчеркивает важность согласования моделей искусственного интеллекта с предпочтениями людей в задачах кодирования.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (1)', '#architecture', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source (1)', '#optimization', '#plp (1)', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В последнее время большие языковые модели (БЯМ), специализирующиеся на коде (codeLLM), достигли значительных успехов в генерации кода. Для оценки их производительности обычно используются стандартные наборы данных, включающие различные упражнения по программированию и соответствующие тесты. Однако, современные codeLLM в основном сосредоточены на создании корректного кода, не учитывая соответствие человеческим предпочтениям. В реальных сценариях запросы должны отражать практические задачи, а ответы моделей должны соответствовать ожиданиям людей.</p>
<p>Чтобы устранить этот разрыв, авторы статьи представили новый, тщательно отобранный людьми набор данных под названием CodeArena. Он имитирует сложность и разнообразие реальных задач программирования и включает 397 высококачественных примеров, охватывающих 40 категорий и 44 языка программирования. Эти примеры были отобраны из реальных пользовательских запросов.</p>
<p>Кроме того, авторы предлагают использовать разнообразный синтетический корпус инструкций SynCode-Instruct, содержащий почти 20 миллиардов токенов. Он был создан путем масштабирования инструкций с веб-сайта, чтобы проверить эффективность обучения codeLLM на большом количестве синтетических данных. Модель Qwen2.5-SynCoder, обученная исключительно на этих синтетических данных, показала лучшие результаты среди открытых codeLLM.</p>
<p>Результаты исследования выявили различия в оценке производительности моделей при использовании стандартных наборов данных, основанных на выполнении кода, и CodeArena. Систематические эксперименты с CodeArena, проведенные на более чем 40 языковых моделях, показали значительный разрыв в производительности между открытыми codeLLM (например, Qwen2.5-Coder) и проприетарными моделями (например, OpenAI o1). Это подчеркивает важность учета человеческих предпочтений при обучении и оценке codeLLM.</p></div>
                
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><p>Современные большие языковые модели (LLM), такие как GPT-4 и Claude, демонстрируют впечатляющие результаты в различных задачах, особенно в генерации и дополнении кода. Это сделало LLM важными инструментами для повышения производительности в разработке программного обеспечения. В последнее время открытые модели, ориентированные на код, такие как StarCoder, DeepSeekCoder и QwenCoder, достигли значительного прогресса, приближаясь по производительности к топовым проприетарным моделям в базовых задачах генерации кода. Кроме того, их открытость и прозрачность снимают опасения разработчиков по поводу конфиденциальности, позволяя развертывать локальные ассистенты для работы с кодом.</p>
<p>С ростом возможностей LLM в области кода, эффективная оценка их производительности в соответствующих задачах стала важной проблемой. Популярные бенчмарки для оценки кода обычно фокусируются на изолированных фрагментах функций, используя ограниченное количество тестовых случаев для проверки корректности кода. Примеры таких бенчмарков: HumanEval, MBPP и BigCodeBench. Хотя в последнее время предпринимаются попытки расширить охват тестовых случаев, задач и языков программирования, эти бенчмарки по-прежнему ограничены проверкой корректности сгенерированных фрагментов кода.</p>
<p>Однако, как показал ChatBot Arena, соответствие ответов модели предпочтениям пользователей также является важным критерием оценки. Например, модель Qwen2.5-Coder генерирует в основном фрагменты кода, в то время как Claude3.5 предоставляет ответы с подробными объяснениями, хорошо структурированным форматированием и комментариями к коду, что делает ее более предпочтительной с точки зрения пользователя. Таким образом, существует острая необходимость в создании бенчмарка, ориентированного на человеческие предпочтения, специально для задач, связанных с кодом. Это позволит сообществу оценивать и отслеживать соответствие ответов моделей предпочтениям пользователей в реальных сценариях.</p>
<p>Кроме того, эффективные данные для улучшения соответствия LLM для работы с кодом предпочтениям пользователей по-прежнему в дефиците. Достижение надежного соответствия в различных задачах кодирования представляет значительные трудности, особенно с точки зрения количества и качества данных, необходимых на этапе обучения с учителем (SFT).</p>
<p>В связи с этим, авторы статьи представляют <strong>CodeArena</strong> – тщательно отобранный людьми бенчмарк, включающий 397 высококачественных примеров из 40 категорий, основанных на реальных запросах пользователей. Также авторы разработали <strong>SynCode-Instruct</strong> – обширный корпус синтетических инструкций, содержащий почти 20 миллиардов токенов, путем масштабирования инструкций из веб-источников.</p>
<p>Обширная оценка более чем 40 больших языковых моделей с использованием CodeArena выявила значительные различия в производительности между бенчмарками, основанными на выполнении кода, и бенчмарком, созданном людьми. В частности, наблюдается существенный разрыв в производительности между открытыми моделями (такими как Qwen-Coder) и закрытыми моделями (такими как модели серии o1 и Claude), что подчеркивает важную роль соответствия моделей ИИ предпочтениям пользователей в задачах кодирования.</p>
<p>Основные вклады данной работы:</p>
<ol>
<li>Представлен <strong>CodeArena</strong>, включающий 397 вручную аннотированных примеров, – всеобъемлющий бенчмарк для оценки соответствия ответов модели предпочтениям пользователей, охватывающий 7 основных категорий и 40 подкатегорий.</li>
<li>Представлен <strong>SynCodeInstruct</strong> – крупномасштабный корпус синтетических инструкций для работы с кодом, полученный из веб-источников. На основе SynCodeInstruct создана эффективная модель Qwen2.5-SynCoder, используемая в качестве сильной базовой модели для CodeArena.</li>
<li>Проведена систематическая оценка более 40 LLM на CodeArena и создан динамически обновляемый лидерборд. Результаты экспериментов показывают, что CodeArena эффективно измеряет соответствие ответов модели предпочтениям пользователей.</li>
</ol></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.05210/x1.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Length question</div>
                <div class="summary_text"><p><strong>Описание набора данных CodeArena и методология оценки</strong></p>
<p>В статье представлен набор данных CodeArena, предназначенный для оценки моделей машинного обучения в задачах, связанных с программированием. В таблице 1 приведены основные характеристики набора данных: максимальная, минимальная и средняя длина кода. Видно, что длина кода варьируется от очень коротких фрагментов до достаточно длинных.</p>
<p>На рисунке 3 показано распределение языков программирования в CodeArena.  В отличие от предыдущих исследований,  в этом наборе данных сделан акцент на разнообразие языков, которые часто используются в повседневной практике.  Например,  были добавлены такие языки, как Google Apps Script (GAS) и PowerShell, чтобы лучше соответствовать реальным сценариям вопросов и ответов.</p>
<p>Для оценки производительности моделей используется GPT-4o-2024-08-06 в качестве арбитра.  Методика оценки заключается в сравнении результатов работы модели с базовым решением. Для этого используются два варианта сравнения, чтобы исключить влияние порядка следования решений на результат.  В итоге вычисляется процент побед сравниваемой модели над базовой.</p>
<p>Чтобы избежать утечки данных, в CodeArena была проведена деконтаминация.  Это означает, что из набора данных были удалены все точные совпадения (на основе 10-граммового перекрытия слов) с другими известными наборами данных, такими как MultiPLE, MBPP, McEval и NaturalCodeBench. Это гарантирует уникальность задач в CodeArena.</p>
<p>Также проводится сравнение CodeArena с другими наборами данных для оценки кода.  CodeArena выделяется тем, что предоставляет комплексный набор задач, включающий 40 подзадач и 44 языка программирования. Это позволяет проводить оценку моделей в условиях, более приближенных к реальным.  Многие задачи в CodeArena не подходят для проверки с помощью модульного тестирования, что делает этот набор данных более подходящим для оценки моделей в реалистичных сценариях.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">SynCode-Instruct</div>
                <div class="summary_text"><h2>Извлечение и очистка данных, классификация и создание инструкций</h2>
<p>Для сбора данных использовался Common Crawl. Сначала применялась модель fasttext, обученная отличать текст, связанный с кодом, от обычного текста. Это помогало отбирать потенциальные данные с кодом и отфильтровывать низкокачественный контент с помощью слабых классификаторов и скореров. Такой подход применялся как к файлам, так и к репозиториям, обеспечивая более полное покрытие.</p>
<p>Далее, для классификации фрагментов кода, извлекался первый слой модели CodeBERT и на его основе дообучался небольшой классификатор для определения языка программирования. Модель обучалась на данных почти 100 языков. Основные языки (например, C, Python и Java) были сохранены, а данные по языкам с большим объемом (например, HTML и Java) были сокращены для баланса. Также были удалены образцы без фрагментов кода.</p>
<p>Для масштабирования инструкций по коду, на начальном этапе применялась фильтрация на основе правил для очистки предварительно извлеченного контента. Удалялась информация о сайтах, реклама и HTML-теги, что значительно сокращало длину документов для дальнейшей обработки. В отличие от предыдущих работ, для создания новых вопросов использовалась модель Qwen2.5-72B, а не извлечение вопросов из существующих данных.</p>
<h2>Разметка данных и контроль качества</h2>
<p>Для создания всестороннего бенчмарка CodeArena, был реализован строгий процесс разметки данных людьми. В нем участвовали 4 штатных сотрудника, владеющих различными языками программирования, и 4 опытных разработчика для проверки качества. Все разметчики прошли обучение и ознакомились с инструкциями.</p>
<p>Процесс разметки включал создание новых вопросов на основе имеющихся, определение уровня сложности (легкий, средний, сложный) в зависимости от сложности запроса и аннотирование соответствующих языков программирования. Было выбрано 2000 образцов, которые были распределены между разметчиками. Разметчики выбрали 822 подходящих исходных образца для создания запросов. Процесс включал регулярные проверки качества и обратную связь.</p>
<p>Затем другие четыре опытных разработчика голосовали за каждый вопрос, чтобы определить, является ли он корректным и может ли быть решен. В итоге, для CodeArena было отобрано 397 образцов (при условии согласия не менее 3 проверяющих). Это было сделано с учетом стоимости использования LLM в качестве судьи.</p>
<p>В результате был получен разнообразный и хорошо отобранный набор данных, охватывающий множество языков программирования и задач, подходящий для оценки и улучшения соответствия между предпочтениями человека и ответами, сгенерированными моделью.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.05210/x6.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Human preference</div>
                <div class="summary_text"><p>В статье описывается методика создания набора данных SynCodeInstruct для обучения моделей программирования. Этот набор данных уникален тем, что он нацелен на оценку соответствия между ответами, сгенерированными моделью, и предпочтениями человека. </p>
<p>Для создания этого набора данных использовались следующие подходы:</p>
<p><strong>1. Генерация ответов:</strong> 
   - Для одного и того же запроса к модели Qwen2.5-Coder генерировалось несколько вариантов ответа. Это делалось путем "семплирования", то есть случайного выбора из вероятностного распределения возможных ответов.</p>
<p><strong>2. Оценка ответов:</strong>
   - <strong>Алгоритмические запросы:</strong> Для запросов, требующих написания кода, сначала генерировались тестовые случаи с помощью специально обученной модели. Затем сгенерированный код проверялся на корректность в многоязычной "песочнице" (sandbox).
   - <strong>Неалгоритмические запросы:</strong> Для запросов, не связанных с программированием, генерировалось четыре случайных варианта ответа. Затем большая языковая модель (LLM) оценивала эти варианты, выбирая лучший и объясняя свой выбор. Это называется "Best-of-N" подходом с использованием LLM-скорера.</p>
<p><strong>3. Выбор лучшего ответа:</strong>
   - Для алгоритмических запросов выбирался ответ, успешно прошедший проверку тестовыми случаями (Executor).
   - Для неалгоритмических запросов выбирался ответ, получивший наивысшую оценку от LLM-скорера.</p>
<p><strong>4. Создание SynCodeInstruct:</strong>
   - Выбранные лучшие ответы вместе с исходными запросами формировали набор данных SynCodeInstruct.</p>
<p><strong>5. Двухэтапный подход:</strong>
   - На первом этапе для генерации данных использовалась модель Qwen2.5.
   - На втором этапе использовались более качественные данные, сгенерированные моделью GPT-4o.</p>
<p><strong>Ключевые моменты:</strong>
- <strong>Разнообразие запросов:</strong> CodeArena создает разнообразные запросы, чтобы оценить соответствие сгенерированных ответов человеческим предпочтениям.
- <strong>Автоматизированная оценка:</strong> Использование LLM для оценки неалгоритмических запросов и автоматическая проверка кода для алгоритмических.
- <strong>Двухэтапное обучение:</strong> Использование сначала менее мощной, а затем более мощной модели для генерации данных.</p>
<p>Таким образом, SynCodeInstruct - это набор данных, созданный с использованием автоматизированных методов, которые позволяют оценить качество ответов модели с точки зрения соответствия человеческим предпочтениям и корректности кода.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Instruction dataset</div>
                <div class="summary_text"><p>В данном исследовании проводится оценка более 40 моделей машинного обучения с размерами от 0.5 миллиарда до 200 миллиардов параметров. В выборку входят как общие языковые модели (LLM), так и специализированные модели для работы с кодом, а также модели с открытым и закрытым исходным кодом.</p>
<p>Среди общих моделей рассматриваются:</p>
<ul>
<li>Модели GPT (включая GPT-3.5-Turbo и GPT4-o)</li>
<li>Серия Qwen (Qwen2.5 и QwenMax)</li>
<li>Серия Claude</li>
<li>Llama3/3.1</li>
<li>Yi</li>
<li>Серия o1</li>
</ul>
<p>Среди моделей для работы с кодом для оценки возможностей генерации кода используются следующие наборы данных:</p>
<ul>
<li><strong>HumanEval (HE) и MBPP:</strong> Эти наборы данных используются с базовыми тестовыми случаями, а также с расширенными тестовыми случаями (HE+ и MBPP+). Это позволяет оценить, насколько хорошо модели справляются с генерацией кода на основе заданных спецификаций.</li>
<li><strong>MultiPL-E:</strong> Этот набор данных расширяет HumanEval, добавляя переводы тестовых случаев на другие языки программирования, такие как Java, C++, Javascript и Typescript. Это позволяет оценить способность моделей генерировать код на нескольких языках.</li>
</ul>
<p>Также вводится новый набор данных <strong>CodeArena</strong>, который отличается от EvalPlus и MultiPL-E тем, что состоит из множества неалгоритмических задач. Это означает, что для оценки не подходит стандартный подход, основанный на выполнении кода. Вместо этого, для каждой задачи используется подход сравнения ответов, с помощью модели GPT-4o. Ответы сравниваются в двух вариантах: A против B и B против A, где A - это ответ модели gpt-4-turbo-2024-04-09, а B - ответ тестируемой модели. На основе этих сравнений рассчитывается процент побед и ничьих. Это позволяет оценить качество ответов модели в задачах, где нет однозначного правильного решения.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluation benchmark</div>
                <div class="summary_text"><p>В статье рассматриваются результаты тестирования различных языковых моделей (LLM) на наборе задач CodeArena, который оценивает их способности в программировании. Для оценки моделей используется методика парного сравнения, где LLM-судья определяет, какой из двух предложенных ответов на задачу является лучшим или объявляет ничью.</p>
<p>Оценка проводится с помощью набора данных EvalPlus, который является улучшенной версией HumanEval и MBPP. EvalPlus содержит более сложные задачи, требующие от моделей не только генерации кода, но и его понимания и отладки.</p>
<p>В таблице 3 представлены результаты сравнения различных моделей, сгруппированных по размеру (количество параметров). Для каждой модели указан процент побед и ничьих (win rate/tie rate) на задачах CodeArena. Подчеркнуты лучшие результаты в каждой группе моделей одного размера.</p>
<p>В таблице представлены результаты как проприетарных моделей (например, Claude, GPT), так и открытых моделей различных размеров (от 0.5B до 70B+ параметров). Это позволяет сравнивать производительность моделей различных архитектур и масштабов.</p>
<p>В целом, таблица демонстрирует, что более крупные модели, как правило, показывают лучшие результаты, но также есть модели с меньшим количеством параметров, которые достигают сравнимых результатов в определенных категориях задач. Это говорит о том, что размер модели не единственный фактор, влияющий на ее производительность.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Main results</div>
                <div class="summary_text"><p>В таблице 3 показаны результаты соревнования моделей, обученных для генерации кода, на платформе CodeArena. В частности, сравниваются показатели побед и ничьих (win rate/tie rate) различных моделей.</p>
<p>Из таблицы видно, что закрытые модели, такие как Claude и o1, по-прежнему демонстрируют значительное преимущество по сравнению с открытыми моделями Qwen2.5-Coder и DeepseekCoder. Подчеркивается, что существует заметный разрыв в производительности между открытыми (например, Qwen-Coder) и закрытыми моделями (например, o1 и Claude), что говорит о важности размера модели и качества обучения.</p>
<p>Далее в таблице 4 представлены результаты моделей на наборах данных EvalPlus и MultiPL-E.  Qwen2.5-SynCoder, модель, обученная на большом наборе синтетических данных SynCode-Instruct, показывает хорошие результаты на CodeArena, что подтверждает эффективность использования синтетических данных для улучшения производительности модели.</p>
<p>Также отмечается, что Qwen2.5-SynCoder значительно превосходит предыдущие сильные открытые модели на EvalPlus и MultiPL-E, приближаясь по производительности к GPT-4o и Claude. Это подтверждает, что использование больших объемов синтетических данных может привести к более существенному улучшению базовой модели в задачах, связанных с выполнением кода (генерация кода), по сравнению с CodeArena.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Discussion</div>
                <div class="summary_text"><p>В статье представлен новый бенчмарк CodeArena, предназначенный для оценки способности больших языковых моделей (LLM) отвечать на вопросы о коде в стиле, приближенном к реальным пользовательским запросам.</p>
<p><strong>Отличия CodeArena от других бенчмарков:</strong></p>
<ul>
<li><strong>Реалистичные запросы:</strong> В отличие от предыдущих бенчмарков, которые в основном содержали алгоритмические задачи в фиксированном формате, CodeArena использует запросы, более похожие на те, которые пользователи задают в реальных ситуациях. Например, запрос "huggingface dataset move all the columns to metadata, except two, problem and solution" (переместить все столбцы в метаданные, кроме двух, problem и solution, в наборе данных huggingface) отражает типичный вопрос пользователя.</li>
<li><strong>Оценка на основе предпочтений человека:</strong> Вместо оценки на основе выполнения кода, как в бенчмарке MultiPL-E, CodeArena оценивает ответы LLM с помощью другой LLM, выступающей в роли судьи. Эта модель-судья определяет, какой ответ лучше соответствует человеческим предпочтениям, учитывая не только корректность кода, но и его релевантность и наличие пояснений.</li>
<li><strong>Акцент на пользовательском опыте:</strong> CodeArena учитывает, что LLM, генерирующие только код без каких-либо пояснений (даже если код правильный), могут не удовлетворить пользователя. Это приводит к более низким оценкам в CodeArena по сравнению с бенчмарками, ориентированными только на исполнение кода.</li>
</ul>
<p><strong>Сравнение с MultiPL-E:</strong></p>
<ul>
<li>Бенчмарк MultiPL-E оценивает LLM на основе корректности выполнения сгенерированного кода.</li>
<li>CodeArena оценивает LLM на основе соответствия сгенерированного ответа человеческим предпочтениям, учитывая как корректность кода, так и его релевантность и наличие пояснений.</li>
<li>Некоторые LLM, такие как модели DeepseekCoder и Qwen-Coder, показывают хорошие результаты в MultiPL-E, но получают более низкие оценки в CodeArena, поскольку их ответы часто не содержат подробных объяснений или полных деталей. Закрытые модели (например, GPT-4o и Claude) показывают более сбалансированные результаты на обоих бенчмарках.</li>
</ul>
<p><strong>Влияние размера обучающих данных:</strong></p>
<ul>
<li>Исследование показало, что увеличение размера обучающих данных для модели Qwen2.5-SynCoder приводит к значительному улучшению результатов в CodeArena. Это подчеркивает важность масштабирования обучающих данных для улучшения производительности LLM в задачах генерации кода.</li>
<li>Двухэтапное обучение (сначала на большом наборе синтетических данных, затем на небольшом наборе высококачественных данных) дает лучшие результаты, чем одноэтапное обучение, что говорит о важности высококачественных данных для улучшения производительности.</li>
</ul>
<p><strong>Распределение запросов:</strong></p>
<ul>
<li>Визуализация запросов CodeArena и MultiPL-E с использованием t-SNE показала, что запросы CodeArena распределены более равномерно, в то время как запросы MultiPL-E образуют более выраженные кластеры. Это говорит о том, что CodeArena охватывает более широкий спектр реальных пользовательских запросов.</li>
</ul>
<p><strong>Общая идея:</strong></p>
<p>CodeArena представляет собой более реалистичный и всесторонний способ оценки LLM в задачах генерации кода, учитывая как корректность кода, так и его соответствие человеческим предпочтениям и пользовательскому опыту. Это позволяет выявить сильные и слабые стороны различных моделей и стимулирует разработку LLM, которые не только генерируют правильный код, но и предоставляют полезные и понятные ответы для пользователей.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.05210/x7.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Conclusion</div>
                <div class="summary_text"><p>В данной работе представлен CodeArena, тщательно разработанный людьми эталонный набор данных, состоящий из 397 высококачественных примеров, охватывающих 40 категорий. Эти примеры основаны на реальных запросах пользователей и предназначены для устранения расхождений между ответами, сгенерированными моделями, и предпочтениями людей в задачах кодирования. Кроме того, создан SynCode-Instruct, разнообразный корпус синтетических инструкций, содержащий около 20 миллиардов токенов. Этот корпус был получен путем масштабирования инструкций из веб-источников.</p>
<p>Оценка более 20 больших языковых моделей (LLM) с использованием CodeArena выявила значительные различия в производительности между эталонами, основанными на выполнении кода, и нашим эталоном, разработанным людьми. Примечательно, что существует заметный разрыв в производительности между LLM с открытым исходным кодом (такими как DeepSeek-Coder) и LLM с закрытым исходным кодом (такими как серии o1 и Claude). Это подчеркивает важность согласования моделей искусственного интеллекта с предпочтениями людей в задачах кодирования.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-13 11:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-13 11:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-13 11:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    