
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. December 6.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">6 декабря</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-05.html">⬅️ <span id="prev-date">05.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-09.html">➡️ <span id="next-date">09.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '6 декабря', 'en': 'December 6', 'zh': '12月6日'};
        let feedDateNext = {'ru': '09.12', 'en': '12/09', 'zh': '12月9日'};
        let feedDatePrev = {'ru': '05.12', 'en': '12/05', 'zh': '12月5日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2412.04003', 'title': 'Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement', 'url': 'https://huggingface.co/papers/2412.04003', 'abstract': 'Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 декабря', 'en': 'December 5', 'zh': '12月5日'}, 'hash': '435ec5749dcb2e12', 'authors': ['Lingfeng Ming', 'Bo Zeng', 'Chenyang Lyu', 'Tianqi Shi', 'Yu Zhao', 'Xue Yang', 'Yefeng Liu', 'Yiyu Wang', 'Linlong Xu', 'Yangyang Liu', 'Xiaohu Zhao', 'Hao Wang', 'Heng Liu', 'Hao Zhou', 'Huifeng Yin', 'Zifu Shang', 'Haijun Li', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets\\pdf\\title_img\\2412.04003.jpg', 'data': {'categories': ['#training', '#machine_translation', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Marco-LLM: Преодоление языкового барьера в мире ИИ', 'desc': 'Marco-LLM - это новая многоязычная модель больших языковых моделей (LLM), разработанная для улучшения производительности в задачах с низкоресурсными языками. Модель была обучена на большом объеме многоязычных данных с использованием архитектуры Qwen2. Marco-LLM показала значительные улучшения по сравнению с современными LLM на различных многоязычных тестах, включая MMMLU, AGIEval, Belebele и другие. Кроме того, модель продемонстрировала существенный прогресс в задачах машинного перевода между любыми языками.'}, 'en': {'title': 'Bridging Language Gaps with Marco-LLM', 'desc': 'This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds.'}, 'zh': {'title': 'Marco-LLM：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在近年来取得了显著进展，但其优秀表现主要集中在主要世界语言上，尤其是英语。为了改善低资源语言的多语言任务表现，我们提出了Marco-LLM，这是一个针对跨语言增强的大规模多语言训练模型。我们收集了大量低资源语言的多语言数据，并使用Qwen2模型进行了广泛的持续预训练。Marco-LLM在多项多语言基准测试中表现出显著的改进，尤其在任意对任意的机器翻译任务中，展示了其多语言模型的有效性。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.', 'summary': '<p>В последние годы большие языковые модели (LLM) достигли значительных успехов, однако их высокая производительность в основном ограничена основными мировыми языками, прежде всего английским. Многие LLM по-прежнему сталкиваются с трудностями при выполнении мультилингвальных задач, особенно когда речь идет о языках с ограниченными ресурсами. Чтобы решить эту проблему, мы представили Marco-LLM: Массовое многоязычное обучение для кросс-лингвистического улучшения LLM. Мы собрали значительное количество мультилингвальных данных для нескольких языков с ограниченными ресурсами и провели обширную непрерывную предварительную подготовку с использованием моделей Qwen2. В результате этих усилий была создана мультилингвальная LLM под названием Marco-LLM. Благодаря всесторонним оценкам на различных мультилингвальных тестах, включая MMMLU, AGIEval, Belebele, Flores-200, XCOPA и многие другие, Marco-LLM продемонстрировала значительные улучшения по сравнению с современными LLM. Кроме того, Marco-LLM достигла существенных улучшений в любых переводческих задачах "любой-к-любому", что демонстрирует эффективность нашей мультилингвальной LLM. Marco-LLM — это новаторская мультилингвальная LLM, предназначенная не только для превосходной работы в мультилингвальных задачах, включая языки с ограниченными ресурсами, но также для поддержания высокой производительности на английском и других основных языках, тем самым сокращая разрыв в возможностях между языками с высокими и низкими ресурсами. Объединяя различные языки, эта работа демонстрирует нашу приверженность обеспечению точности работы LLM на разных языках.</p>'}, {'title': 'Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training', 'content': '. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [Üstün et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, Çağatay Yıldız et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [Üstün et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1𝑒 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6𝑒 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (𝛽1 = 0.9, 𝛽2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1𝑒 5 6𝑒 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/', 'summary': '<p>Введение в крупные языковые модели</p>\n<p>Крупномасштабные языковые модели (LLM), такие как GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024] и LLaMA [Touvron et al., 2023b], значительно изменили область обработки естественного языка (NLP), демонстрируя выдающиеся результаты по различным задачам, таким как понимание языка, генерация текста и перевод. Эти достижения были достигнуты главным образом за счет увеличения размера моделей и объема обучающих данных. Однако большинство этих достижений сосредоточены на языках с высокими ресурсами, особенно на английском языке. Это привело к разрыву в производительности при применении этих моделей для мультилингвальных задач, особенно связанных с языками с ограниченными ресурсами.</p>\n<p>Мультилингвистическая обработка естественного языка сталкивается со специфическими трудностями из-за разнообразия и дисбаланса лингвистических ресурсов. Языки с ограниченными ресурсами часто страдают от недостатка обширных текстовых данных, необходимых для обучения больших моделей, что затрудняет разработку эффективных технологий обработки языка для таких языков. В результате говорящие на языках с ограниченными ресурсами оказываются недостаточно представленными в выгодах, приносимых последними достижениями в области NLP.</p>\n<p>Для устранения этого неравенства была разработана модель Marco-LLM — мультилингвистическая языковая модель, ориентированная на языки с ограниченными ресурсами. Marco-LLM использует значительные объемы мультилингвистичных данных, включая серии мало представленных языков, и проходит массовое непрерывное предварительное обучение и пост-обучение (включая мультилингвистическую дообучающую настройку и выравнивание предпочтений) на основе модели Qwen2 [Bai et al., 2023].</p>\n<p>Основные вклады включают:</p>\n<ol>\n<li>Сбор и курирование большого набора мультилингвистичных данных, предназначенных для языков с ограниченными ресурсами.</li>\n<li>Массовое мультилингвистическое непрерывное предобучение и пост-тренинг на модели Qwen2 для разработки Marco-LLM.</li>\n<li>Проведение всесторонних оценок на различных бенчмарках, показывающих превосходство Marco-LLM над существующими моделями в мультилингвистической среде.</li>\n</ol>\n<p>Остальная часть работы структурирована следующим образом:</p>\n<p>Раздел 2 представляет обзор литературы о развитии LLM, особенно мультилингвистических LLM и непрерывной предварительной тренировки для LLM.\nРаздел 3 описывает детали экспериментов по непрерывному предобучению, включая сбор и обработку моно- и мультилингвистичных данных, настройки обучения и результаты оценки.\nРаздел 4 демонстрирует проведение пост-тренинга (включая контролируемую дообучающую настройку и выравнивание предпочтений) для предварительно обученной модели Marco-LLM, описанной в разделе 3.</p>'}, {'title': 'Evaluation Benchmarks For Marco-LLM', 'content': 'alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: "A person is cooking onions on stove. They begin to cry because..." Options: 1. "the onions release gas that irritates the eyes." (Correct) 2. "they are listening to sad music." 3. "the stove is very hot." 4. "they forgot to buy garlic." MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: "The ground is wet." Options: 1. "It rained last night." (Cause) 2. "The sun is shining." X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. "Maria woke up early on Saturday." 2. "She was excited about the trip." 3. "She packed her bags quickly." 4. "She grabbed her keys and left the house." Endings: A. "She arrived at the airport just in time for her flight." (Correct) B. "She decided to go back to sleep because it was raining." 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1𝑒5, 2𝑒5, 3𝑒5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3𝑒5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1𝑒5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35', 'summary': '<p>В статье представлена комплексная оценка различных наборов данных, используемых в экспериментах с моделями искусственного интеллекта. Эти наборы данных охватывают четыре основные категории задач: общие знания, мультилингвистическое понимание, ответы на вопросы и машинный перевод. Для каждой задачи используются различные метрики оценки, такие как точность (Accuracy), показатель F1 или BLEU. Оценка проводится для разных языков, от одноязычных до многоязычных сценариев.</p>\n<p>В разделе о наборах данных для общих знаний упоминаются следующие:</p>\n<ol>\n<li><strong>CEVAL</strong>: Набор данных, включающий более 13000 вопросов из китайских национальных экзаменов по различным предметам, таких как математика, физика, право, медицина и языковые искусства.</li>\n<li><strong>AGIEval</strong>: Вопросы из реальных мировых стандартных тестов, таких как GRE, GMAT, LSAT и другие профессиональные сертификационные экзамены.</li>\n<li><strong>ARC</strong> (AI2 Reasoning Challenge): Набор данных, состоящий из почти 7800 вопросов, предназначенных для оценки способности модели отвечать на научные вопросы уровня средней школы. Вопросы разделены на два набора: простой и сложный, требующий рассуждений и фоновых знаний.</li>\n</ol>\n<p>Для мультилингвистического понимания рассматриваются данные:</p>\n<ol>\n<li><strong>X-MMLU</strong>: Многоязычный набор данных для тестирования моделей на понимание текста на разных языках.</li>\n<li><strong>XCOPA</strong>: Множество вариантов выбора ответов на вопросы на нескольких языках.</li>\n<li><strong>XStoryCloze</strong>: Тестирование способности модели к завершению историй на разных языках.</li>\n</ol>\n<p>Наборы данных для ответа на вопросы включают:</p>\n<ol>\n<li><strong>TyDiQA</strong>: Многоязычные вопросно-ответные пары на основе текстов из Википедии на 11 разных языках. Задача состоит в том, чтобы модель могла понимать и генерировать ответы без перевода на английский.</li>\n</ol>\n<p>Наконец, для машинного перевода используется:</p>\n<ol>\n<li><strong>Flores</strong>: Параллельные предложения на более чем 100 языках, включая мало представленные и низко ресурсные языки.</li>\n</ol>\n<p>Эти наборы данных позволяют всесторонне оценивать способности моделей ИИ в различных языковых и когнитивных задачах.</p>'}, {'title': 'Performance Evaluation Of Multilingual Language Models', 'content': 'B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.', 'summary': '<p>В таблице 8 представлены результаты сравнения производительности языковых моделей (LLM) различных размеров по нескольким основным бенчмаркам: MMMLU, TydiQA, AGIEval, CEval и Belebele. Эти бенчмарки оценивают модели на разнообразных задачах и языках, подчеркивая их способность к многоязычному пониманию и рассуждению.</p>\n<p>Модель Marco-Chat-7B демонстрирует наилучшие показатели среди моделей с параметрами размером в 7 миллиардов на всех тестах. В частности, она значительно превосходит базовые модели на тестах CEval и Belebele, которые сосредоточены на китайских образовательных предметах и разнообразии африканских языков соответственно. На CEval модель Marco-Chat-7B достигает результата 86.4, что существенно выше следующего лучшего результата — Qwen2-7B (81.8). Это указывает на сильную способность нашей модели понимать и обрабатывать китайский контент. Аналогично, на Belebele, где оценивается владение мало представленными африканскими языками, Marco-Chat-7B показывает результат 79.3, превосходя другие модели почти на 9 пунктов.</p>\n<p>Среди моделей с параметром 70 миллиардов, модель Marco-72B также лидирует во всех тестах. Она получает оценку 76.1 на MMMLU, которая оценивает понимание языка при выполнении множества задач на разных языках. Этот результат на 4.4 пункта лучше, чем у следующей лучшей модели, Llama3.1-70B. На TydiQA, тесте для вопросов и ответов на различных типологически разных языках, Marco-72B набирает 61.0, что на 7.9 пунктов больше, чем у второй лучшей модели. Это говорит о сильной способности модели выполнять различные задачи на широком спектре языков с различными грамматическими структурами и письменностями. Кроме того, Marco-72B достигает впечатляющего результата 94.5 на CEval, показывая отличную компетентность в китайском языке по различным академическим дисциплинам. На Belebele модель достигает оценки 89.6, демонстрируя высокую производительность на языках Африки.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture', '#audio', '#benchmark', '#cv', '#data', '#dataset (1)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal', '#open_source', '#optimization', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В последние годы большие языковые модели (LLM) достигли значительных успехов, однако их высокая производительность в основном ограничена основными мировыми языками, прежде всего английским. Многие LLM по-прежнему сталкиваются с трудностями при выполнении мультилингвальных задач, особенно когда речь идет о языках с ограниченными ресурсами. Чтобы решить эту проблему, мы представили Marco-LLM: Массовое многоязычное обучение для кросс-лингвистического улучшения LLM. Мы собрали значительное количество мультилингвальных данных для нескольких языков с ограниченными ресурсами и провели обширную непрерывную предварительную подготовку с использованием моделей Qwen2. В результате этих усилий была создана мультилингвальная LLM под названием Marco-LLM. Благодаря всесторонним оценкам на различных мультилингвальных тестах, включая MMMLU, AGIEval, Belebele, Flores-200, XCOPA и многие другие, Marco-LLM продемонстрировала значительные улучшения по сравнению с современными LLM. Кроме того, Marco-LLM достигла существенных улучшений в любых переводческих задачах "любой-к-любому", что демонстрирует эффективность нашей мультилингвальной LLM. Marco-LLM — это новаторская мультилингвальная LLM, предназначенная не только для превосходной работы в мультилингвальных задачах, включая языки с ограниченными ресурсами, но также для поддержания высокой производительности на английском и других основных языках, тем самым сокращая разрыв в возможностях между языками с высокими и низкими ресурсами. Объединяя различные языки, эта работа демонстрирует нашу приверженность обеспечению точности работы LLM на разных языках.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2412.04003/extracted/6047129/Figures/marco_fig_init.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training</div>
                <div class="summary_text"><p>Введение в крупные языковые модели</p>
<p>Крупномасштабные языковые модели (LLM), такие как GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024] и LLaMA [Touvron et al., 2023b], значительно изменили область обработки естественного языка (NLP), демонстрируя выдающиеся результаты по различным задачам, таким как понимание языка, генерация текста и перевод. Эти достижения были достигнуты главным образом за счет увеличения размера моделей и объема обучающих данных. Однако большинство этих достижений сосредоточены на языках с высокими ресурсами, особенно на английском языке. Это привело к разрыву в производительности при применении этих моделей для мультилингвальных задач, особенно связанных с языками с ограниченными ресурсами.</p>
<p>Мультилингвистическая обработка естественного языка сталкивается со специфическими трудностями из-за разнообразия и дисбаланса лингвистических ресурсов. Языки с ограниченными ресурсами часто страдают от недостатка обширных текстовых данных, необходимых для обучения больших моделей, что затрудняет разработку эффективных технологий обработки языка для таких языков. В результате говорящие на языках с ограниченными ресурсами оказываются недостаточно представленными в выгодах, приносимых последними достижениями в области NLP.</p>
<p>Для устранения этого неравенства была разработана модель Marco-LLM — мультилингвистическая языковая модель, ориентированная на языки с ограниченными ресурсами. Marco-LLM использует значительные объемы мультилингвистичных данных, включая серии мало представленных языков, и проходит массовое непрерывное предварительное обучение и пост-обучение (включая мультилингвистическую дообучающую настройку и выравнивание предпочтений) на основе модели Qwen2 [Bai et al., 2023].</p>
<p>Основные вклады включают:</p>
<ol>
<li>Сбор и курирование большого набора мультилингвистичных данных, предназначенных для языков с ограниченными ресурсами.</li>
<li>Массовое мультилингвистическое непрерывное предобучение и пост-тренинг на модели Qwen2 для разработки Marco-LLM.</li>
<li>Проведение всесторонних оценок на различных бенчмарках, показывающих превосходство Marco-LLM над существующими моделями в мультилингвистической среде.</li>
</ol>
<p>Остальная часть работы структурирована следующим образом:</p>
<p>Раздел 2 представляет обзор литературы о развитии LLM, особенно мультилингвистических LLM и непрерывной предварительной тренировки для LLM.
Раздел 3 описывает детали экспериментов по непрерывному предобучению, включая сбор и обработку моно- и мультилингвистичных данных, настройки обучения и результаты оценки.
Раздел 4 демонстрирует проведение пост-тренинга (включая контролируемую дообучающую настройку и выравнивание предпочтений) для предварительно обученной модели Marco-LLM, описанной в разделе 3.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluation Benchmarks For Marco-LLM</div>
                <div class="summary_text"><p>В статье представлена комплексная оценка различных наборов данных, используемых в экспериментах с моделями искусственного интеллекта. Эти наборы данных охватывают четыре основные категории задач: общие знания, мультилингвистическое понимание, ответы на вопросы и машинный перевод. Для каждой задачи используются различные метрики оценки, такие как точность (Accuracy), показатель F1 или BLEU. Оценка проводится для разных языков, от одноязычных до многоязычных сценариев.</p>
<p>В разделе о наборах данных для общих знаний упоминаются следующие:</p>
<ol>
<li><strong>CEVAL</strong>: Набор данных, включающий более 13000 вопросов из китайских национальных экзаменов по различным предметам, таких как математика, физика, право, медицина и языковые искусства.</li>
<li><strong>AGIEval</strong>: Вопросы из реальных мировых стандартных тестов, таких как GRE, GMAT, LSAT и другие профессиональные сертификационные экзамены.</li>
<li><strong>ARC</strong> (AI2 Reasoning Challenge): Набор данных, состоящий из почти 7800 вопросов, предназначенных для оценки способности модели отвечать на научные вопросы уровня средней школы. Вопросы разделены на два набора: простой и сложный, требующий рассуждений и фоновых знаний.</li>
</ol>
<p>Для мультилингвистического понимания рассматриваются данные:</p>
<ol>
<li><strong>X-MMLU</strong>: Многоязычный набор данных для тестирования моделей на понимание текста на разных языках.</li>
<li><strong>XCOPA</strong>: Множество вариантов выбора ответов на вопросы на нескольких языках.</li>
<li><strong>XStoryCloze</strong>: Тестирование способности модели к завершению историй на разных языках.</li>
</ol>
<p>Наборы данных для ответа на вопросы включают:</p>
<ol>
<li><strong>TyDiQA</strong>: Многоязычные вопросно-ответные пары на основе текстов из Википедии на 11 разных языках. Задача состоит в том, чтобы модель могла понимать и генерировать ответы без перевода на английский.</li>
</ol>
<p>Наконец, для машинного перевода используется:</p>
<ol>
<li><strong>Flores</strong>: Параллельные предложения на более чем 100 языках, включая мало представленные и низко ресурсные языки.</li>
</ol>
<p>Эти наборы данных позволяют всесторонне оценивать способности моделей ИИ в различных языковых и когнитивных задачах.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Performance Evaluation Of Multilingual Language Models</div>
                <div class="summary_text"><p>В таблице 8 представлены результаты сравнения производительности языковых моделей (LLM) различных размеров по нескольким основным бенчмаркам: MMMLU, TydiQA, AGIEval, CEval и Belebele. Эти бенчмарки оценивают модели на разнообразных задачах и языках, подчеркивая их способность к многоязычному пониманию и рассуждению.</p>
<p>Модель Marco-Chat-7B демонстрирует наилучшие показатели среди моделей с параметрами размером в 7 миллиардов на всех тестах. В частности, она значительно превосходит базовые модели на тестах CEval и Belebele, которые сосредоточены на китайских образовательных предметах и разнообразии африканских языков соответственно. На CEval модель Marco-Chat-7B достигает результата 86.4, что существенно выше следующего лучшего результата — Qwen2-7B (81.8). Это указывает на сильную способность нашей модели понимать и обрабатывать китайский контент. Аналогично, на Belebele, где оценивается владение мало представленными африканскими языками, Marco-Chat-7B показывает результат 79.3, превосходя другие модели почти на 9 пунктов.</p>
<p>Среди моделей с параметром 70 миллиардов, модель Marco-72B также лидирует во всех тестах. Она получает оценку 76.1 на MMMLU, которая оценивает понимание языка при выполнении множества задач на разных языках. Этот результат на 4.4 пункта лучше, чем у следующей лучшей модели, Llama3.1-70B. На TydiQA, тесте для вопросов и ответов на различных типологически разных языках, Marco-72B набирает 61.0, что на 7.9 пунктов больше, чем у второй лучшей модели. Это говорит о сильной способности модели выполнять различные задачи на широком спектре языков с различными грамматическими структурами и письменностями. Кроме того, Marco-72B достигает впечатляющего результата 94.5 на CEval, показывая отличную компетентность в китайском языке по различным академическим дисциплинам. На Belebele модель достигает оценки 89.6, демонстрируя высокую производительность на языках Африки.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-06 15:40',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-06 15:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-06 15:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    