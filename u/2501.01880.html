
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 января</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-10.html">⬅️ <span id="prev-date">10.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-14.html">➡️ <span id="next-date">14.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 января', 'en': 'January 13', 'zh': '1月13日'};
        let feedDateNext = {'ru': '14.01', 'en': '01/14', 'zh': '1月14日'};
        let feedDatePrev = {'ru': '10.01', 'en': '01/10', 'zh': '1月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2501.01880', 'title': 'Long Context vs. RAG for LLMs: An Evaluation and Revisits', 'url': 'https://arxiv.org/pdf/2501.01880', 'abstract': 'Extending context windows (i.e., Long Context, LC) and using retrievers to\nselectively access relevant information (i.e., Retrieval-Augmented Generation,\nRAG) are the two main strategies to enable LLMs to incorporate extremely long\nexternal contexts. This paper revisits recent studies on this topic,\nhighlighting their key insights and discrepancies. We then provide a more\ncomprehensive evaluation by filtering out questions answerable without external\ncontext, identifying the most effective retrieval methods, and expanding the\ndatasets. We show that LC generally outperforms RAG in question-answering\nbenchmarks, especially for Wikipedia-based questions. Summarization-based\nretrieval performs comparably to LC, while chunk-based retrieval lags behind.\nHowever, RAG has advantages in dialogue-based and general question queries.\nThese insights underscore the trade-offs between RAG and LC strategies,\noffering guidance for future optimization of LLMs with external knowledge\nsources. We also provide an in-depth discussion on this topic, highlighting the\noverlooked importance of context relevance in existing studies.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': 'b641dbe6c9dd585f', 'authors': ['Xinze Li', 'Yixin Cao', 'Yubo Ma', 'Aixin Sun'], 'affiliations': ['S-Lab, Nanyang Technological University', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2501.01880.jpg', 'data': {'categories': ['#rag', '#optimization', '#long_context', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Битва гигантов: LC vs RAG в расширении возможностей языковых моделей', 'desc': 'Статья исследует две основные стратегии для расширения возможностей языковых моделей (LLM) работать с длинным контекстом: увеличение контекстного окна (LC) и использование ретриверов для выборочного доступа к релевантной информации (RAG). Авторы проводят комплексную оценку этих подходов на различных датасетах, фильтруя вопросы, на которые можно ответить без внешнего контекста. Результаты показывают, что LC в целом превосходит RAG в задачах вопросно-ответного анализа, особенно для вопросов на основе Википедии. Однако RAG имеет преимущества в диалоговых запросах и общих вопросах.'}, 'en': {'title': 'Balancing Long Context and Retrieval for Optimal LLM Performance', 'desc': 'This paper explores two main strategies for enhancing large language models (LLMs) with long external contexts: Long Context (LC) and Retrieval-Augmented Generation (RAG). It reviews recent research, clarifying key findings and differences between these approaches. The authors conduct a thorough evaluation, revealing that LC generally performs better than RAG in answering questions, particularly those based on Wikipedia. However, RAG shows strengths in dialogue and general queries, highlighting the need to balance these strategies for optimal performance in LLMs.'}, 'zh': {'title': '长上下文与检索增强生成的权衡分析', 'desc': '本文探讨了扩展上下文窗口（长上下文，LC）和使用检索器（检索增强生成，RAG）来选择性访问相关信息的两种主要策略，以帮助大型语言模型（LLMs）处理极长的外部上下文。研究表明，在问答基准测试中，长上下文通常优于检索增强生成，尤其是在基于维基百科的问题上。总结性检索与长上下文的表现相当，而基于块的检索则相对较弱。然而，检索增强生成在对话和一般问题查询中具有优势，这突显了两种策略之间的权衡，为未来优化大型语言模型提供了指导。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.', 'summary': '<p>В данной работе рассматриваются два основных подхода, позволяющих большим языковым моделям (LLM) работать с очень длинным внешним контекстом: увеличение размера контекстного окна (Long Context, LC) и использование механизмов извлечения релевантной информации (Retrieval-Augmented Generation, RAG).</p>\n<p><strong>Увеличение контекстного окна (LC)</strong> позволяет модели обрабатывать больше текста за один раз, что потенциально дает ей доступ ко всей необходимой информации. <strong>Метод RAG</strong>, напротив, полагается на извлечение только той части внешнего контекста, которая наиболее релевантна текущему запросу.</p>\n<p>В статье проводится анализ существующих исследований по этим двум подходам, выделяются их основные идеи и противоречия. Затем авторы проводят более тщательную оценку, отфильтровывая вопросы, на которые можно ответить без внешнего контекста, определяя наиболее эффективные методы извлечения и расширяя наборы данных.</p>\n<p>Результаты показывают, что <strong>в задачах ответов на вопросы LC обычно превосходит RAG</strong>, особенно когда дело касается вопросов, основанных на Wikipedia. <strong>Извлечение на основе суммаризации</strong> (summarization-based retrieval) показывает результаты, сопоставимые с LC, в то время как <strong>извлечение на основе фрагментов</strong> (chunk-based retrieval) отстает. Тем не менее, у <strong>RAG есть преимущества в диалоговых задачах и общих запросах</strong>.</p>\n<p>Эти результаты подчеркивают компромиссы между стратегиями RAG и LC и дают рекомендации для будущей оптимизации LLM с использованием внешних источников знаний. В заключение авторы также обсуждают важность <strong>релевантности контекста</strong>, которая часто упускается из виду в существующих исследованиях.</p>'}, {'title': 'Introduction', 'content': 'Large Language Models (LLMs) (Brown et al., 2020) have demonstrated strong zero/few-shot capabilities in open-ended question answering (Yang et al., 2019). However, they face challenges such as hallucinations (Shuster et al., 2021; Ji et al., 2023), lacking real-time information and domain-specific knowledge (Su et al., 2024; Zhang et al., 2024), among others. common solution is to enhance LLMs with external memory to provide reliable and up-to-date data sources. Yet, incorporating additional content is constrained by the limited context window of LLMs. To address this, two main approaches are adopted: (i) building models with long context windows to read in more information (LC) (Fei et al., 2024; Chen et al., 2023; Wang et al., 2024c), and (ii) employing retrievers to include text segments relevant to the query (RAG) (Jiang et al., 2023; Asai et al., 2024; Gao et al., 2023). As shown by the timeline in Figure 1a, there is clear trend toward developing models that handle longer context windows and combining LC with RAG methods. The chronological overview of related studies highlights an increasing focus on both LC and RAG since mid-2023, as evidenced by growing number of publications aimed at optimizing the efficient retrieval, and utilization of long contexts. The development of models supporting longer context windows underscores the growing importance of handling extensive inputs effectively. Despite the broad consensus regarding the importance of LC and RAG, there remain disagreements and contradictory insights from different studies, summarized in Table 1. For example, while several studies agree on the effectiveness of combining LC and RAG (Xu et al., 2024b; Jiang et al., 2024b), others suggest that combining may not be beneficial (Bai et al., 2024a; Jin et al., 2024). Moreover, conflicting conclusions are reported regarding the benefits of RAG versus LC. Some papers find RAG advantageous in certain contexts (Xu et al., 2024a; Yu et al., 2024), while others highlight superior results from LC (Li et al., 2024; Xu et al., 2024b). These divergent insights showcase the complexity and ongoing debates in the field, suggesting that optimal strategies may vary depending on specific model architectures and benchmark conditions. To explore the underlying reasons, we conduct an in-depth investigation into the conditions that lead to disagreements among existing studies. During this process, we also identify key aspects that may have been overlooked in earlier research. Specifically, we revisit the evaluation process and implement the following changes. First, we fil- (a) Related work on LC and RAG, each paper is labeled by char and one color. For instance, green and "L" represent "LongRAG". (b) Chronological progress of key LLMs from 2023 to 2024. We focus on the models that publications in 1a use. We underline the models that support context window length of 32K. (c) History of frequently used retrievers from the 1980s until 2024. We bold the retrievers that no existing publications in 1a uses. Figure 1: Chronological overview of the development of RAG and LC. The Sub-graphs respectively illustrate the timelines for (a) publications related to LC and RAG, (b) long-context models, and (c) retrievers. We label before each model and retriever with the char and color block representing the publication that uses it. ter out questions from existing datasets that can be correctly answered without external context, removing biases from the parametric knowledge of LLMs and focusing on questions requiring external knowledge. Second, we evaluate retrieval methods and baselines on smaller filtered dataset (1,000+ questions) from 12 QA datasets to identify the best retriever. Third, we expand the dataset size by approximately 10 times by collecting additional data from the original sources of the 12 datasets1. Lastly, we compare the answers produced by the two settings, i.e., LC and RAG, and conduct an in-depth analysis. Our results are based on the expanded dataset using the long-context setting and the best retrieval method identified earlier. Our key contributions in this paper are as follows: (i) Providing comprehensive survey of existing studies on LC and RAG, analyzing their implementations and key insights. (ii) Proposing fair and systematic evaluation framework, and performing detailed analyses to understand the strengths and limitations of LC and RAG. (iii) Discussing chal1The experiment code and expanded datasets are available at https://github.com/lixinze777/LC_VS_RAG lenges for comparing and combining LC and RAG, reflecting on the key points that researchers tend to overlook in this field. Evaluation results indicate that LC models generally outperform RAG when processing self-contained information like stories, while RAG excels at handling fragmented information, particularly in dialogue-based contexts. These experiments deepen our understanding of the strengths and limitations of LC and RAG, offering valuable insights into optimizing retrieval strategies and effectively integrating these approaches to enhance performance in open-domain question answering. These findings also based on systematic survey of existing studies on this topic (see 2). Additionally, we discuss key aspects of comparing LC and RAG in 6, highlighting areas that have been underexplored in prior research.', 'summary': '<p>Большие языковые модели (LLM) демонстрируют хорошие результаты при ответах на вопросы в открытом формате, но сталкиваются с проблемами, такими как галлюцинации, отсутствие актуальной информации и узкоспециализированных знаний. Распространенным решением является использование внешней памяти для предоставления LLM надежных и обновленных данных. Однако, добавление внешнего контента ограничено размером контекстного окна LLM. Для решения этой проблемы используются два основных подхода: (i) создание моделей с длинным контекстным окном для обработки большего объема информации (LC) и (ii) использование механизмов поиска (retrievers) для включения текстовых фрагментов, релевантных запросу (RAG). Наблюдается явная тенденция к разработке моделей, способных обрабатывать более длинные контекстные окна, а также к комбинированию LC и RAG.</p>\n<p>Хронологический обзор исследований показывает, что с середины 2023 года наблюдается растущий интерес к LC и RAG, о чем свидетельствует увеличение количества публикаций, направленных на оптимизацию эффективного поиска и использования длинных контекстов. Разработка моделей, поддерживающих более длинные контекстные окна, подчеркивает растущую важность эффективной обработки больших объемов входных данных.</p>\n<p>Несмотря на общее признание важности LC и RAG, существуют разногласия и противоречивые выводы в различных исследованиях. Например, некоторые исследования подтверждают эффективность комбинации LC и RAG, в то время как другие предполагают, что это может быть нецелесообразно. Также противоречивы выводы относительно преимуществ RAG по сравнению с LC. Некоторые работы показывают преимущества RAG в определенных контекстах, в то время как другие подчеркивают превосходство LC. Эти противоречивые выводы демонстрируют сложность и продолжающиеся дебаты в этой области, предполагая, что оптимальные стратегии могут варьироваться в зависимости от конкретных архитектур моделей и условий тестирования.</p>\n<p>Для изучения причин этих разногласий, авторы статьи проводят углубленное исследование условий, приводящих к противоречиям между существующими работами. В процессе исследования также выявляются ключевые аспекты, которые могли быть упущены в предыдущих исследованиях. В частности, авторы пересматривают процесс оценки и вносят следующие изменения:\n1.  Фильтрация вопросов из существующих наборов данных, на которые можно правильно ответить без внешнего контекста, чтобы устранить смещения, вызванные параметрическими знаниями LLM, и сосредоточиться на вопросах, требующих внешних знаний.\n2.  Оценка методов поиска и базовых моделей на уменьшенном отфильтрованном наборе данных (1000+ вопросов) из 12 наборов данных QA для определения лучшего метода поиска.\n3.  Увеличение размера набора данных примерно в 10 раз путем сбора дополнительных данных из исходных источников 12 наборов данных.\n4.  Сравнение ответов, полученных в двух настройках, т.е. LC и RAG, и проведение углубленного анализа.</p>\n<p>Результаты основаны на расширенном наборе данных с использованием настройки длинного контекста и лучшего метода поиска, определенного ранее.</p>\n<p>Основные вклады данной работы:\n1.  Предоставление всестороннего обзора существующих исследований по LC и RAG, анализ их реализаций и ключевых выводов.\n2.  Предложение справедливой и систематической структуры оценки и проведение подробного анализа для понимания сильных и слабых сторон LC и RAG.\n3.  Обсуждение проблем сравнения и объединения LC и RAG, с акцентом на ключевые моменты, которые исследователи часто упускают из виду в этой области.</p>\n<p>Результаты оценки показывают, что модели LC обычно превосходят RAG при обработке самодостаточной информации, такой как истории, в то время как RAG превосходит в обработке фрагментированной информации, особенно в контексте диалогов. Эти эксперименты углубляют понимание сильных и слабых сторон LC и RAG, предлагая ценные сведения для оптимизации стратегий поиска и эффективной интеграции этих подходов для повышения производительности при ответах на вопросы в открытом доступе. Эти выводы также основаны на систематическом обзоре существующих исследований по этой теме. Кроме того, обсуждаются ключевые аспекты сравнения LC и RAG, выделяя области, которые недостаточно изучены в предыдущих исследованиях.</p>'}, {'title': 'Related work', 'content': 'Our primary focus is to evaluate and compare LC and RAG. To this end, we review papers with similar focus, and provide detailed analysis of the retrievers and long-context settings they employ. 2.1 Retrievers Retrievers, as fundamental components of RAG pipelines, focus on identifying and extracting contextually relevant segments of documents. We categorize retrieval strategies into three main approaches: chunk-based retrieval, which splits documents into smaller segments and then retrieves those most relevant to query; index-based retrieval, which builds specialized index structures to guide efficient and context-rich lookups; and summarization-based retrieval, which leverages hierarchical summaries to capture documents key information at various levels of abstraction. Chunk-based Retrieval can be broadly categorized into sparse retrievers and dense retrievers. Sparse retrievers, such as the classic BM25 (Robertson and Zaragoza, 2009), operate on term frequency-based representations of text and rank chunks based on similarity function, leveraging exact matches and term weighting. With the advent of word embeddings, dense retrievers have gained prominence. These models encode both queries and document chunks into dense vector representations and calculate relevance using similarity metrics, such as cosine similarity. Since text similarity is often defined by measuring the distance between embeddings, the quality of these embeddings is particularly important. Contriever (Izacard et al., 2022) leverages contrastive learning for training without supervision. By generating synthetic queries and pre-training on unlabeled data, Contriever provides robust retrieval capabilities especially in cross-lingual applications. On larger scale, BGE-Large (Xiao et al., 2023) employs diverse datasets and sophisticated training methods to outperform previous models on comprehensive benchmarks such as C-MTEB. E5Mistral7b (Wang et al., 2024b) combines open-source, decoder-only LLMs with synthetic data generation pipelines. With minimal human annotations, the fine-tuning achieves SOTA performance on BEIR and MTEB. Dragon (Lin et al., 2023) also employs data augmentation, including cropping and generative queries, and integrates labels from multiple retrieval sources. This strategy ensures its effectiveness without increasing model complexity. Another method of learning high-quality embeddings is through strong generalization ability from LLMs. For instance, OpenAI embeddings draw upon the GPT-3.5/4 family while Zhipu-embedding-3 leverages the GLM family (Zeng et al., 2024). Index-based Retrieval requires pre-processing on the documents with more complicated data structures (Gupta et al., 2018). With the development of LLM, Llama-Index (Liu, 2022) was proposed to facilitate interaction between the model and documents more conveniently. The index provides flexible interface to construct various data structures, known as indices that store, organize, and facilitate quick retrieval of context. Once created, these indices can be efficiently queried, guiding the LLM to the most relevant information, improving the accuracy of responses. Some classic indexing methods include tree index which constructs hierarchical tree from nodes, and knowledge graph index, which builds knowledge graph with labeled nodes and relationships. Summarization-based Retrieval is built on top of chunkand index-based approaches. It provides comprehensive summaries for key points in document. These summaries available for retrieval. RAPTOR (Sarthi et al., 2024) improves retrieval by generating recursive summaries of text chunks organized in tree structure. Instead of retrieving short, contiguous text snippets, RAPTOR clusters text segments, summarizes them at various levels, and forms hierarchical tree that represents the documents content at different levels of abstraction. This allows retrieval models to extract context at varying levels of detail, improving the ability to handle complex questions that require synthesizing information from multiple parts of the document. Such summarization-based retrieval method enhances retrieval accuracy for tasks requiring longrange or multi-step reasoning. 2.2 Long-Context LLMs Many research efforts focus on extending input and output windows to accommodate more context (see Figure 1b), enabling applications such as extended dialogues, large document processing, and complex multimodal tasks. Thus, our analysis focuses on two dimensions: the model capabilities and the context length they can reach. Model Ability. While most of the models discussed here excel at understanding long documents, many emphasize specialized capabilities. ChatGLM2-6B-32K (Zeng et al., 2024) employs Multi-Query Attention to achieve high reasoning efficiency with low memory usage, making it suitable for tasks requiring deep reasoning. XGen-7B-8K (Nijkamp et al., 2023) enhances long-context conversational understanding and text summarization, enabling coherent and contextually rich dialogues. InternLM-7B-8k (Cai et al., 2024) is optimized for knowledge understanding, reading comprehension, and multilingual translation, supporting diverse linguistic applications. Models like DeepSeek-V2-Chat (DeepSeekAI et al., 2024), Qwen2-72B-Instruct (Yang et al., 2024), Qwen2.5-72B-Instruct (Qwen et al., 2024), Mixtral-7x8b (Jiang et al., 2024a), and DBRXInstruct excel in mathematical computations, logical reasoning, and coding, demonstrating strong performance in technical and analytical tasks. Additionally, Claude-3-Opus, Sonnet, Haiku, Gemini-1.5-flash, and Gemini-1.5-pro (Reid et al., 2024) incorporate multi-modal capabilities, effectively handling both textual and visual information. GLM-4-9B-Chat (Zeng et al., 2024), Mistral12b-Instruct, and Llama-3.1-Instruct (Dubey et al., 2024) offer robust multilingual abilities, strong instruction-following and multi-turn dialogue capabilities, increasing their utility in wide range of conversational scenarios. Finally, Claude-2 is notable for low hallucination rate when processing extra-long documents, ensuring high accuracy and reliability in information retrieval and synthesis. Context Length. As shown in Figure 1b, there is clear trend of increasing context length in newly released models. Following the categorization approach proposed by ChatQA2 (Xu et al., 2024a), we classify these models into three categories based on their supported context windows: short (up to 4K), long (up to 32K), and ultra-long (more than 32K) context models. Short context models, such as Llama2-70B and llama2-7B-chat-4k (Touvron et al., 2023), support up to 4K tokens and are typically employed as baselines for retrieval and standard conversational tasks. Long context models, including XGen-7B8K(Nijkamp et al., 2023), InternLM-7B-8k(Cai et al., 2024), Mixtral-7x8b (Jiang et al., 2024a), DBRX-Instruct and Gemma2-9B (Mesnard et al., 2024), offer context windows ranging from 8K to 32K tokens. These are ideal for extended conversations, comprehensive text analysis, and detailed summarization tasks. Ultra-long context models extend beyond 32K tokens. For example, Claude-2 provides 100K token window, while Claude-3-Opus, Sonnet, and Haiku handle up to 200K tokens. GPT-4-Turbo(OpenAI et al., 2023), GPT-4o, and GPT-o1 all support 128K tokens, as do DeepSeek-V2-Chat(DeepSeek-AI et al., 2024), Qwen2-72B-Instruct(Yang et al., 2024), Qwen2.572B-Instruct (Qwen et al., 2024), GLM-4-9BChat (Zeng et al., 2024), GLM-4-Plus, Mistral-12bInstruct, and Llama-3.1-Instruct. Notably, Gemini1.5-flahs and Gemini-1.5-pro(Reid et al., 2024) both support up to an unprecedented 10M tokens. These ultra long-context models enable the processing of exceptionally large documents, complex multimodal tasks, and extensive multi-turn dialogues. 2.3 Comparing & Combining LC and RAG Since the increase in LLMs context window lengths, some models can contain the entire document, reducing the need to retrieve on documents. Hence, more studies have begun comparing the performance of long-context LLMs and RAG, as well as investigating ways to combine them. LongBench (Bai et al., 2024a) conducts early comparison experiments on 4K model with RAG and 32K model. Xu et al. (2024b) systematically compare LC LLMs and RAG, and proposes their combination. LongRAG (Jiang et al., 2024b) introduces long retrievers and long readers, successful application of long retrieval units to RAG. ChatQA2 (Xu et al., 2024a) instruction-tunes long-context LLMs to 128K context window and tests their ability with long-context retrievers. Self-ROUTE (Li et al., 2024) enables the model to select either RAG or LC based on self-reflection to reduce costs. OPRAG (Yu et al., 2024) preserves the original order of retrieved chunks, and LC LLM meets RAG (Jin et al., 2024) investigates long-context LLMs in RAG systems, proposing retrieval reordering methods. LC RAG Performance of LLM (Leng et al., 2024) evaluates the effectiveness of RAG on longcontext LLMs across context lengths from 2K to 2M tokens. Very recently, LongBench is updated to LongBench V2 (Bai et al., 2024b), which tests LLMs on long context comprehension and reasoning with more realistic and challenging setting. We summarize the key insights from these papers into three categories: (1) general insights such as chunking strategies, (2) combining the two strategies, and (3) comparing the performance between LC and RAG (see Table 1). Some papers reach consensus on chunking strategy that, retrieval units should be longer (Jiang et al., 2024b) and the number of chunks should be kept low (Yu et al., 2024). According to (Xu et al., 2024b), selecting the top 5 to 10 chunks typically yields strong performance, while retrieving Paper Type Findings LongBench (B) (Bai et al., 2024a) Ret-LC LLM (R) (Xu et al., 2024b) LongRAG (L) (Jiang et al., 2024b) ChatQA2 (C) (Xu et al., 2024a) Self-ROUTE (S) (Li et al., 2024) OP-RAG (O) (Yu et al., 2024) LC LLM-RAG (M) (Jin et al., 2024) LC RAG Performance (P) (Leng et al., 2024) LongBench v2 (V) (Bai et al., 2024b) + + + + + + Retrieval helps 4k model, but not 16k/32k models. Models benefit from continuous training on long contexts. Splitting context into shorter and more chunks is better. LC is better for multi-hop benchmarks than 4k RAG. RAG improves on 70B/43B models on all context lengths. For LC model, best results are obtained from top-5 or top-10. Retrieval benefits from long retrieval units. For sequence lengths up to 32K, RAG outperforms LC. From 3K to 24K, greater context window benefits RAG. LC consistently outperforms RAG, but RAG has lower cost. Efficient retrieval can outperform brute-force LC. Too many chunks in RAG harms performance. Preserving the original order is better than ordering by score. Retrieve more passages first improves performance then drops. Ordering higher score information to front and back helps. Most close models RAG improves up to 100k tokens. Most open models RAG peak at 16k-32k then performance drops. GPT-4o performs better at 128k without RAG. GPT-4o performance keeps increasing to 128k RAG context. Qwen2.5 & GLM-4-Plus drop with >32k RAG contexts. Table 1: Important findings from existing studies that compare or combine LC with RAG (label in brackets). We group the insights into three categories: 1) General strategies that improve performance marked by +. 2) Combining LC and RAG, where indicates combining is good, and for combining is not helpful, and 3) Comparing LC and RAG, where indicates RAG outperforms LC, and for LC outperforms RAG. more than 20 chunks leads to diminished results. LongBench (Bai et al., 2024a) presents different finding, suggesting that splitting long context into shorter and more numerous chunks is better. However, at the time of its publication, LLMs generally exhibited weaker long-context capabilities, and the study did not incorporate very long retrieval units (>1000 tokens). Consequently, LongBenchs findings are not at odds with the broader consensus. Nonetheless, these papers present disagreement regarding performance of retrieval on long-context LLMs. For instance, LongBench (Bai et al., 2024a) finds that retrieval helps short-context models but not 7B long-context models. In contrast, Xu et al. (2024b) suggest that RAG improves 70B models across all context lengths, attributing the discrepancy to the difference between model sizes. Similarly, ChatQA2 (Xu et al., 2024a) observes that increasing the context window from 3K to 24K tokens consistently benefits RAG. Notably, LongBench V2 (Bai et al., 2024b) shows that GPT-4o continues to improve in RAG performance even at 128K input, whereas Qwen2.5 and GLM-4-Plus show performance deterioration beyond 32K input. The observations align with findings from (Leng et al., 2024) that RAG for close-source models can improve up to 100K input, whereas performance for some open-source models peaks around 16K tokens. Hence, the varying behaviors might be due to different model size and architecture. There are even greater discrepancies in the direct comparisons between the two methods. Xu et al. (2024b) claims that long-context models outperform retrieval with short-context models in multihop benchmarks. In contrast, ChatQA2 (Xu et al., 2024a) finds that RAG can outperform LC if sufficient number of top-k chunks are used. SelfROUTE (Li et al., 2024) fully supports LC, arguing that it outperforms RAG in all benchmarks. Meanwhile, OP-RAG (Yu et al., 2024) defends RAG, demonstrating that efficient retrieval strategies can outperform brute-force approach of processing extremely long contexts. The reasons for the differences among these studies are manifold. For instance, There are three categories of retrieval methods (i.e., chunk-based, index-based, and summarization-based retrieval), but current studies rely predominantly on chunkbased retrieval, leaving room for further optimization. Additionally, evaluation scores often represent weighted averages across different datasets. Because each dataset has distinct characteristics, placing more emphasis on one dataset and less on another can alter the final results. Finally, most existing studies use only few datasets with around 200 questions each. This small sample size creates greater room for variability and reduces the general reliability of these findings.', 'summary': '<p>В статье рассматриваются методы извлечения информации (ретриверы) и модели с длинным контекстом (LC LLM), а также их сравнение и комбинация.</p>\n<p><strong>Ретриверы</strong> — это ключевые компоненты RAG-пайплайнов, которые отвечают за поиск и извлечение релевантных фрагментов документов. Их можно разделить на три основных типа:</p>\n<ol>\n<li><strong>Чанк-ориентированные ретриверы:</strong><ul>\n<li><strong>Разреженные ретриверы</strong> (например, BM25) работают на основе частоты терминов и ранжируют фрагменты по схожести, используя точные совпадения и взвешивание терминов.</li>\n<li><strong>Плотные ретриверы</strong> кодируют запросы и фрагменты документов в плотные векторные представления и измеряют релевантность с помощью метрик схожести, таких как косинусное расстояние. Качество этих векторных представлений имеет решающее значение. Примерами являются Contriever, BGE-Large, E5Mistral7b и Dragon. Некоторые модели, такие как OpenAI embeddings и Zhipu-embedding-3, используют возможности LLM для создания качественных эмбеддингов.</li>\n</ul>\n</li>\n<li><strong>Индекс-ориентированные ретриверы:</strong> требуют предварительной обработки документов с использованием сложных структур данных. Llama-Index упрощает взаимодействие между моделью и документами, предоставляя гибкий интерфейс для построения различных индексов. Индексы организуют данные для быстрого извлечения контекста. Примеры включают древовидные индексы и индексы на основе графов знаний.</li>\n<li><strong>Ретриверы на основе суммаризации:</strong> строятся на основе чанк- и индекс-ориентированных подходов. Они предоставляют краткие изложения основных моментов документа. RAPTOR, например, создает иерархические изложения текстовых фрагментов, что позволяет извлекать контекст на разных уровнях детализации, улучшая способность обрабатывать сложные запросы.</li>\n</ol>\n<p><strong>Модели с длинным контекстом (LC LLM)</strong>: исследования направлены на увеличение длины входных и выходных окон, что позволяет обрабатывать большие объемы текста. Анализ проводится по двум направлениям:</p>\n<ol>\n<li><strong>Возможности модели:</strong> Многие модели, помимо понимания длинных документов, обладают специализированными возможностями. ChatGLM2-6B-32K обеспечивает высокую эффективность рассуждений при низком потреблении памяти, XGen-7B-8K улучшает понимание длинного контекста и суммирование текста, InternLM-7B-8k оптимизирован для понимания знаний и перевода. Модели DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, Mixtral-7x8b и DBRX-Instruct демонстрируют хорошие результаты в математических вычислениях, логических рассуждениях и кодировании. Claude-3-Opus, Sonnet, Haiku, Gemini-1.5-flash и Gemini-1.5-pro поддерживают мультимодальность. GLM-4-9B-Chat, Mistral-12b-Instruct и Llama-3.1-Instruct обладают надежными многоязычными возможностями. Claude-2 отличается низким уровнем галлюцинаций при обработке очень длинных документов.</li>\n<li><strong>Длина контекста:</strong> Модели делятся на три категории:<ul>\n<li><strong>Короткий контекст</strong> (до 4K токенов), например, Llama2-70B и llama2-7B-chat-4k.</li>\n<li><strong>Длинный контекст</strong> (до 32K токенов), например, XGen-7B8K, InternLM-7B-8k, Mixtral-7x8b, DBRX-Instruct и Gemma2-9B.</li>\n<li><strong>Ультра-длинный контекст</strong> (более 32K токенов), например, Claude-2 (100K), Claude-3-Opus, Sonnet и Haiku (200K), GPT-4-Turbo, GPT-4o, GPT-o1, DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, GLM-4-9BChat, GLM-4-Plus, Mistral-12b-Instruct, Llama-3.1-Instruct (128K), Gemini-1.5-flahs и Gemini-1.5-pro (10M).</li>\n</ul>\n</li>\n</ol>\n<p><strong>Сравнение и комбинация LC и RAG:</strong> С увеличением длины контекстного окна в LLM, некоторые модели могут вместить весь документ, что снижает необходимость извлечения. Поэтому исследования начали сравнивать производительность LC LLM и RAG, а также изучать способы их объединения.</p>\n<ul>\n<li>LongBench проводит сравнение моделей с контекстом 4K и 32K с RAG.</li>\n<li>Xu et al. (2024b) систематически сравнивают LC LLM и RAG и предлагают их комбинацию.</li>\n<li>LongRAG вводит длинные ретриверы и ридеры.</li>\n<li>ChatQA2 обучает LC LLM до 128K контекстного окна и тестирует их с длинными ретриверами.</li>\n<li>Self-ROUTE позволяет модели выбирать RAG или LC на основе самоанализа для снижения затрат.</li>\n<li>OPRAG сохраняет исходный порядок извлеченных фрагментов.</li>\n<li>LC LLM meets RAG исследует LC LLM в RAG-системах и предлагает методы переупорядочивания извлечения.</li>\n<li>LC RAG Performance of LLM оценивает эффективность RAG на LC LLM с длиной контекста от 2K до 2M токенов.</li>\n<li>LongBench V2 тестирует LLM на понимание длинного контекста и рассуждения в более реалистичных условиях.</li>\n</ul>\n<p>Основные выводы:</p>\n<ol>\n<li><strong>Общие выводы:</strong><ul>\n<li>Фрагменты для извлечения должны быть длиннее.</li>\n<li>Количество фрагментов должно быть небольшим.</li>\n<li>Обычно достаточно выбирать 5-10 лучших фрагментов.</li>\n</ul>\n</li>\n<li><strong>Комбинирование стратегий:</strong><ul>\n<li>Изучаются способы объединения LC и RAG для достижения лучшей производительности.</li>\n</ul>\n</li>\n<li><strong>Сравнение LC и RAG:</strong><ul>\n<li>Проводится сравнение производительности LC LLM и RAG в различных задачах.</li>\n</ul>\n</li>\n</ol>'}, {'title': 'Question filtering and expansion', 'content': 'To ensure fair and comprehensive comparison, we curate our evaluation dataset based on existing datasets, and apply necessary filtering ( 3.1) and augmentation ( 3.2). We select 12 long-context QA datasets frequently used in studies comparing LC and RAG: Natural Questions (Kwiatkowski et al., 2019), 2WikiMultihopQA (Ho et al., 2020), HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), MultiFieldQA (Bai et al., 2024a), NarrativeQA (Koˇciský et al., 2018), QASPER (Dasigi et al., 2021), QuALTY (Pang et al., 2022), Coursera, TOEFL-QA, and MultiDoc2Dial (An et al., 2024). We also include the NovelQA (Wang et al., 2024a) dataset, high-quality, human-annotated resource derived from long-form novels. We present an overview of these datasets in Table 2, including their type, context type (single-doc or multi-doc), context source, average context length, and representative studies that have utilized each dataset. 3.1 Question Filtering Given the strong capabilities of modern LLMs, many questions can be directly answered based on knowledge encoded in their parameters (Basmova et al., 2024), reducing the need for external context in some cases. However, certain queries, such as those related to private conversations, will always require additional context. To determine which approach more effectively enhances an LLMs performance with long documents, we filter the datasets to include only questions that the LLM cannot answer correctly without external context. This ensures that any correct answers obtained subsequently must rely on external knowledge rather than the models built-in knowledge. For our implementation, we use GPT-4o for question filtering due to its strong capabilities. We employ strict exact-match scoring metric to ensure that the model not only provides the correct answer but also demonstrates complete understanding of the required information. 3.2 Question (and Context) Expansion RAG and LC produce identical answers for about 60% of the questions in existing evaluations (Li et al., 2024), leaving relatively few questions to help us understand the differences between the two. To ensure robust statistical significance, we expand the dataset size to approximately 20,000 questions by collecting additional samples. To maintain similar distribution as the original datasets, we follow two principles during data collection. First, we collect questions only from the original source of each dataset, avoiding artificially generated or LLM-augmented questions. Second, we add distracting passages to the original context for each question to extend the context length, following the implementation described in LongBench. For NovelQA, we use all its available questions. For Coursera, MultiFieldQA, and MultiDoc2Dial datasets, we do not further enlarge their sizes to avoid introducing artificial data. Hereafter, we refer to the expanded dataset as the full question set and the original, pre-expansion dataset as the sample question set. 3.3 Dataset Statistics After expansion, we obtain 19,188 questions, of which 13,651 require context to be answered using the filtering method from 3.1, as listed in Table 3. Notably, questions grounded in factual knowledge, such as those from Coursera, show high removal rate. Similarly, questions drawn from well-known books or requiring multi-hop reasoning often exhibit higher likelihood of being directly answered by LLMs without context. Comparing the 12 individual datasets, we observe similar filtering rate between the sample and the full question sets (see Tables 2 and 3), indicating that both sets follow similar distribution.', 'summary': '<p>Для обеспечения честного и всестороннего сравнения методов, используемых в машинном обучении, авторы статьи создали свой собственный набор данных для оценки, опираясь на существующие наборы данных и применяя к ним необходимую фильтрацию и расширение.</p>\n<p><strong>3.1 Фильтрация вопросов</strong></p>\n<p>Они выбрали 12 наборов данных, часто используемых для сравнения методов обработки длинного контекста (LC) и извлечения с генерацией (RAG): Natural Questions, 2WikiMultihopQA, HotpotQA, MuSiQue, MultiFieldQA, NarrativeQA, QASPER, QuALTY, Coursera, TOEFL-QA и MultiDoc2Dial, а также добавили NovelQA, который является высококачественным набором данных, полученным из художественных романов. В таблице 2 представлен обзор этих наборов данных, включая их тип, тип контекста (однодокументный или многодокументный), источник контекста, среднюю длину контекста и исследования, в которых они использовались.</p>\n<p>Так как современные большие языковые модели (LLM) обладают значительными возможностями, многие вопросы могут быть отвечены непосредственно на основе знаний, закодированных в их параметрах, что снижает необходимость во внешнем контексте. Однако некоторые запросы, например, связанные с личными разговорами, всегда требуют дополнительного контекста. Чтобы определить, какой подход более эффективно повышает производительность LLM при работе с длинными документами, авторы отфильтровали наборы данных, включив только те вопросы, на которые LLM не может ответить правильно без внешнего контекста. Это гарантирует, что любые правильные ответы, полученные впоследствии, будут зависеть от внешних знаний, а не от встроенных знаний модели. Для фильтрации вопросов использовалась модель GPT-4o из-за ее высокой производительности, при этом применялся строгий критерий точного соответствия, чтобы убедиться, что модель не только дает правильный ответ, но и демонстрирует полное понимание требуемой информации.</p>\n<p><strong>3.2 Расширение вопросов (и контекста)</strong></p>\n<p>Методы RAG и LC дают идентичные ответы примерно на 60% вопросов в существующих оценках, оставляя относительно мало вопросов для понимания различий между этими двумя подходами. Чтобы обеспечить надежную статистическую значимость, авторы расширили размер набора данных примерно до 20 000 вопросов, собрав дополнительные образцы. Для сохранения исходного распределения данных при сборе новых вопросов соблюдались два принципа. Во-первых, вопросы собирались только из исходного источника каждого набора данных, избегая искусственно сгенерированных или дополненных LLM вопросов. Во-вторых, к исходному контексту каждого вопроса добавлялись отвлекающие фрагменты, чтобы увеличить длину контекста, как это описано в работе LongBench. Для NovelQA использовались все доступные вопросы. Для наборов данных Coursera, MultiFieldQA и MultiDoc2Dial их размеры не были увеличены, чтобы избежать внесения искусственных данных. Расширенный набор данных далее называется полным набором вопросов, а исходный, до расширения, набор данных - выборочным набором вопросов.</p>\n<p><strong>3.3 Статистика набора данных</strong></p>\n<p>После расширения было получено 19 188 вопросов, из которых 13 651 требовали контекста для ответа, согласно методу фильтрации из раздела 3.1, как указано в таблице 3. Вопросы, основанные на фактических знаниях, например, из Coursera, показали высокую скорость удаления. Аналогично, вопросы, взятые из известных книг или требующие многошаговых рассуждений, часто с большей вероятностью могли быть отвечены LLM без контекста. При сравнении 12 отдельных наборов данных наблюдалась схожая скорость фильтрации между выборочным и полным наборами вопросов (см. таблицы 2 и 3), что указывает на то, что оба набора имеют схожее распределение.</p>'}, {'title': 'Evaluation methodology', 'content': '4.1 Evaluation Framework Our evaluation of RAG and LC is conducted in the following three phases. Phase 1: Empirical Study on Retrievers. We evaluate five retrievers: BM25, Contriever, OpenAI Embeddings, Llama-Index, and RAPTOR, on the sample question set. The retriever yielding the best performance is then selected for subsequent comparisons with LC on the full question set. Phase 2: Comparing RAG and LC. Using the best retriever, RAG is compared with LC by anDataset Doc Source Avg Len Used by Papers # # Kept % Kept Mode NQ Coursera NovelQA 2WikiMHQA HotpotQA MuSiQue MultiFieldQA NarrativeQA QASPER QuALTY TOEFL-QA MultiDoc2Dial multi multi single multi multi multi single single single single single multi Wikipedia Coursera books Wikipedia Wikipedia Wikipedia papers, reports books, films papers stories exams dialogue 18,164.7 M, 7,934.3 NIL (L-eval) 67,000.0 NIL (NovelQA) 7,191.3 10,602.7 12,974.3 5,706.1 25,274.2 5,350.3 5,089.2 109 172 210 B, S, 300 B, R, L, C, S, 200 200 B, R, C, 150 B, R, L, C, 200 B, R, 224 B, R, 202 R, 121 729.1 NIL (L-eval) 158 3,076.9 NIL (L-eval) 22 54 109 152 93 140 121 171 221 202 121 158 20 Open 32 MCQ 52 MCQ Open 51 Open 47 Open 70 Open 81 Open 86 99 Open 100 MCQ 100 MCQ Open 100 Table 2: Overview of the original datasets (i.e., the pre-expanded sample question set) and their characteristics. The column represents dataset type with values for Knowledge, for reasoning, and for reading comprehension. For each dataset, we report the existing papers (with the label) about LC & RAG that use it. If no paper has used it, we report its source like L-eval (An et al., 2024). We also report number of questions in each set (# Q), number and percentage of questions retained after filtering (# Kept and % Kept) out questions needing no context, and mode of question. Dataset # Questions # Kept % Kept 4.2 Retriever Selection Coursera NQ NovelQA 2WikiMHQA HotpotQA MuSiQue MultiFieldQA NarrativeQA QASPER QuALTY TOEFL-QA MultiDoc2Dial 172 1,109 2,283 2,300 2,200 2,200 150 2,211 2,718 2,725 962 54 373 869 1,036 1,113 1,663 121 1,880 2,674 2,725 962 158 32 34 38 45 51 78 81 85 98 100 100 100 Total 71 19,188 Table 3: Statistics of the full question set, ordered by increasing percentage of questions kept after filtering out questions needing no context. 13,628 For Figure 1 shows that existing studies primarily select one or more chunk-based retrieval methods, while indexand summarization-based retrievers are less frequently evaluated. In our study, we evaluate various retrieval methods to ensure that RAG is supported by the most effective retrievers. we use BM25 (Robertson and Zaragoza, 2009), Contriever (Izacard et al., 2022), and OpenAIs text-embedding-3-Small. BM25 serves as classic baseline, while Contriever and textembedding-3-Small represent embeddings from well-performing closed-source and open-source models, respectively. chunk-based retrieval, swering questions on the full question set. Both methods use the same underlying LLM for question answering. For RAG, relevant documents or chunks are fetched from the available context and provided to the LLM as input to generate answers. In contrast, for LC, the entire context available to the question is given to the LLM, with truncation from the back of the context applied if the context exceeds the models context window. The evaluation metrics are explained in 4.3. Phase 3: In-depth Analysis. We focus on 4 specific subsets of questions: 1) those answered correctly only by RAG, 2) those answered correctly only by LC, 3) those RAG gives better answers, and 4) those LC gives better answers. These subsets are analyzed to understand the types of questions each method excels at, providing insights into the strengths and limitations of both approaches in different scenarios. For index-based retrieval, we employ Llamaindex and leverage two indexing methods that suit long documents. Specifically, tree-index organizes documents into hierarchical tree structure, enabling efficient retrieval of context. The root node contains high-level summary, while subsequent child nodes store progressively finer-grained representations. When queried, the retrieval process navigates through this hierarchy, starting from the toplevel summary and moving down to more specific nodes as needed. Sentence Window Retriever focuses on local, sentence-level context rather than entire documents or large text chunks. It creates smaller windows of few sentences each. When query arrives, the retriever searches these windows to identify segments most semantically similar to the query. By working at finer granularity, the sentence window retriever provides more targeted and contextually accurate snippets of text, Match (EM) score strictly to all questions to determine the correctness of the answers. Excluding the overlap, the top right block indicates the questions that only LC answers correctly, and similarly, the bottom left block indicates the questions that only RAG answers correctly. The remaining gray block represents the questions that both RAG and LC answer incorrectly, as judged by Exact Match. Since many questions involve long open-ended responses, we calculate the 1 scores of the answers provided by both methods against the ground truth. If RAG achieves higher 1 score than LC, we consider RAG to have answered the question better, and vice versa for LC. detailed explanation of 1 score calculation is provided in appendix The loose evaluation setting considers all cases in which one method outperforms the other, including 1) when one method obtains the correct answer and the other is wrong under EM, and 2) when one method achieves higher 1 score. We adopt this loose evaluation because references for some datasets are long, open-ended answers, making it very unlikely to match them exactly under EM. In addition, some short answers (about 56 words) may differ slightly from the reference while still conveying the correct idea. Although these answers would be marked incorrect by EM, they might attain high 1 score. Hence, comparing 1 scores helps compensate for the strictness of EM.', 'summary': '<p>В разделе 4.1 описывается методология оценки RAG (Retrieval-Augmented Generation) и LC (Long Context). Оценка проводится в три этапа:</p>\n<p><strong>Этап 1: Эмпирическое исследование ретриверов.</strong> На первом этапе сравниваются пять различных методов извлечения информации (ретриверов) на небольшом наборе вопросов: BM25, Contriever, OpenAI Embeddings, Llama-Index и RAPTOR.  Выбирается ретривер, показавший наилучшие результаты, который затем используется для сравнения RAG с LC на полном наборе вопросов. <em>Комментарий: Этот этап важен для выбора оптимального метода извлечения информации для RAG, поскольку качество извлеченной информации напрямую влияет на качество ответов RAG.</em></p>\n<p><strong>Этап 2: Сравнение RAG и LC.</strong> На втором этапе RAG сравнивается с LC на полном наборе вопросов с использованием лучшего ретривера, выбранного на первом этапе. Оба метода используют одну и ту же большую языковую модель (LLM) для ответов на вопросы. В RAG релевантные документы или фрагменты извлекаются из контекста и предоставляются LLM в качестве входных данных для генерации ответов. В LC весь доступный контекст (с возможным усечением с конца, если он превышает контекстное окно модели) передается LLM. <em>Комментарий: Сравниваются подходы, где RAG сначала извлекает релевантную информацию, а LC использует весь контекст.</em></p>\n<p><strong>Этап 3: Углубленный анализ.</strong> На третьем этапе проводится углубленный анализ четырех подмножеств вопросов: 1) на которые правильно отвечает только RAG, 2) на которые правильно отвечает только LC, 3) на которые RAG дает лучшие ответы, и 4) на которые LC дает лучшие ответы. Этот анализ позволяет выявить сильные и слабые стороны каждого подхода в разных сценариях. <em>Комментарий: Этот этап позволяет понять, в каких ситуациях каждый метод работает лучше, что важно для понимания их различий.</em></p>\n<p>Также в разделе описываются используемые ретриверы. BM25 используется как классический базовый метод. Contriever и text-embedding-3-Small представляют эмбеддинги из закрытой и открытой моделей соответственно. Для индексного извлечения используется Llama-Index с двумя методами индексации: tree-index (организует документы в иерархическую структуру) и Sentence Window Retriever (фокусируется на локальном контексте на уровне предложений).  <em>Комментарий: Описываются конкретные методы, которые используются для извлечения информации.</em></p>\n<p>В разделе также поясняются используемые метрики оценки. Для определения правильности ответов используется метрика Exact Match (EM), которая требует точного совпадения ответа с эталонным.  Для сравнения качества ответов используется метрика F1 score, которая позволяет оценить степень совпадения ответов с эталонными, даже если они не совпадают точно.  Для более объективной оценки используется "свободная" оценка, которая учитывает случаи, когда один метод превосходит другой, включая случаи, когда один метод дает правильный ответ по EM, а другой нет, а также случаи, когда один метод имеет более высокий F1 score. <em>Комментарий: Объясняется, почему используется "свободная" оценка, и как она помогает компенсировать строгость EM.</em></p>'}, {'title': 'Experiments', 'content': 'To obtain answers, we use the same prompt From the context: [context], answer the questions briefly with no explanation. for both retrieval and long context settings. For MCQ questions, we add one sentence Answer the question with the letters of the correct options (e.g. A, BC, C, ACD, etc.) without including text. These prompts ensure LLMs to directly answer the questions, which makes evaluation more convenient. Figure 2: Evaluation Matrix for In-depth Analysis. improving the models ability to answer specific questions. For summarization-based retrieval, we use RAPTOR (Sarthi et al., 2024). It constructs hierarchical tree by recursively clustering text chunks based on semantic similarity, summarizing each cluster into parent node, and continuing this process until no further clustering is possible. After constructing the tree, we apply the collapsed tree traversal approach, as previous work has demonstrated its superior performance. This approach flattens the hierarchical structure into single layer and compares the query against all nodes across every level simultaneously. The top-k most relevant nodes are then selected based on predefined token limit, ensuring that the retrieved information maintains the appropriate level of granularity. Although RAPTORs implementation appears similar to the Llama Tree Index, they differ in both construction and navigation. First, Llama Tree Index groups consecutive nodes, while RAPTOR freely clusters nodes from far positions, and even allows single node to appear in multiple clusters. Second, Llama Tree Index navigates down the hierarchy to retrieve only leaf nodes, while RAPTOR evaluates all nodes from all layers simultaneously. Hence, RAPTOR can retrieve not only original texts but also generated summaries. 4.3 Evaluation Metric 5.1 Phase 1: Retrievers We use win-lose rate system to compare LC and RAG, as illustrated in Figure 2. The horizontal yellow block represents the questions that the LLM answers correctly using LC, while the vertical blue block represents the questions that the LLM answers correctly using RAG. Their overlap in the top-left corner represents the questions that both methods answer correctly. We apply an Exact Evaluated on the sample question set, Table 5 reports the results of chunk-, index-, and summarization-based retrievers. Among them, RAPTOR performs the best with correct answer rate of 38.5%, while Index-based retrievers outperform chunk-based retrievers. Within index-based retrievers, the RAG Only score for Tree Index is much lower than that for Window Parsing (82 Dataset # Questions LC Correct RAG Correct LC Only RAG Only LC Better RAG Better Coursera 2WikiMHQA HotpotQA MultiFieldQA NQ NarrativeQA QASPER QuALITY TOEFL-QA MuiQue MultiDoc2Dial NovelQA 54 1,036 1,113 121 373 1,880 2,674 2,725 962 1,663 158 869 26 594 876 63 189 558 884 2,290 895 821 14 466 20 431 723 60 138 405 863 2,050 884 663 38 10 242 212 14 75 276 517 402 26 344 5 164 4 79 59 11 24 123 496 162 15 186 29 106 10 265 231 44 104 685 1,011 402 26 426 65 164 4 107 67 21 35 281 762 162 15 225 58 106 13,628 Overall 6,683 Table 4: Performance of LC and RAG across different datasets. We report the number of questions answered correctly by each method, as well as the breakdown of questions where: only LC answers correctly (LC Only), only RAG answers correctly (RAG Only), LC outperforms RAG (LC Better), and RAG outperforms LC (RAG Better). 1,294 2,287 3,433 1,843 Type Retriever Correct (%) RAG Only RAG Better Chunk Index BM25 Contriever Text-emb-3-small Tree Index Window Parsing 319 (20.4) 315 (20.1) 338 (21.6) 470 (30.1) 555 (35.5) 50 43 47 82 91 141 143 234 237 Summarization RAPTOR Table 5: Comparison of different retrieval methods 602 (38.5) 258 97 vs. 91), and their RAG Better scores are nearly identical (234 vs. 237). This discrepancy suggests that Tree Index may be undervalued in the RAG Only metric but still contributes in open question scenarios that require long answers. We further observe the questions and contexts that each retriever exclusively answers correctly. RAPTOR shows stronger ability than other retrievers, especially in scenarios that require an entire understanding of the document, like research papers. Chunk-based methods struggle when required information is spread across multiple chunks. Indexbased retrievers are not as strong in overall understanding as RAPTOR, but they show good ability in interpreting dialogues. Therefore, we select RAPTOR as the primary retriever for evaluation on the full question set. 5.2 Phase 2: Comparing LC and RAG We compare LC and RAG on the filtered, full question set. The results across 12 datasets are summarized in Table 4. Overall, LC correctly answers 56.3% of the questions, while RAG provides correct answers to 49.0%. LC correctly answers more than 2,000 questions that RAG misses, while RAG exclusively answers almost 1,300 questions. When looking at the loose evaluation setting, LC answers 3,433 questions better than RAG, and RAG anLooking at swers 1,843 questions better than LC. The gap further widens compared to strict setting, indicating long-context LLMs ability to answer questions with open long answers is also strong. individual datasets, in MultiDoc2Dial, RAG exhibits better performance than LC in strict evaluation (5 vs 29), but is surpassed by LC in loose evaluation (65 vs 58). In contrast, on datasets like NarrativeQA and QuaLITY, LC shows strong lead not just in overall correctness but also in the number of questions that are answered better. Collectively, the results show that both methods have unique strengths and limitations. Although LC shows better overall results than RAG, out of the 13,628 questions, almost 10% can be only answered correctly by RAG, which is not small ratio. This shows that retrievers cannot be simply replaced by long-context LLM in searching. This also motivates us to further examine what kind of questions (and context) can be only answered correctly by RAG (or LC). 5.3 Phase 3: In-Depth Analysis The overall results are influenced by the combined effects of different scenarios, so we need to separately analyze each scenario to see if more detailed results can be obtained. We analyze the performance of LC and RAG across different knowledge sources (Figure 3) and question types (Figures 4). Here, we use EM Scores only, for strict evaluation standard. We also report the results for loose evaluation standard (i.e., EM Scores and 1 Scores) in appendix B, which shows similar trends. From Figure 3, it is evident that LC excels with knowledge sources such as Wikipedia and stories. However, the Wikipedia context is collected Figure 3: Performance breakdown by knowledge source for LC Only and RAG Only. by adding extensive noise to create long context, which generally makes the context less relevant to the question, with only small portion being useful. This synthetic context formation partially simulates the RAG process and may introduce an In addiunfair bias against the RAG pipeline. tion, summarization-based retrieval methods may split Wikipedia articles unnaturally, generating less meaningful summaries. LCs strong performance demonstrates that long-context LLMs are robust to noise in such forms of context. In contrast, RAG performs better with dialoguerelated sources and achieves comparable performance with papers or reports. The information in these sources is naturally segmented, conversations have turns, and papers and reports have clearly defined sections or subsections, making the retrieval of key segments easier. Figure 4 shows that LC performs better for factbased questions such as Who, Where, and Which. These questions often benefit from having all the relevant context available in dense region close to the answer. RAG, however, is largely comparable to LC for more open-ended questions such as How, which often require synthesizing information from multiple sources and therefore benefit from retrieval-based approaches. Furthermore, RAG outperforms LC in the Other questions, which consist mainly of general questions that can be answered with Yes or No. We hypothesize that the reason could be due to the training data. Long-context LLMs are more familiar with phrasing of common type questions than general questions. Words like Who or Where act as keywords for long-context LLMs to search, while retrievers use these keywords not so well. 5.4 Word Frequency Visualization To better understand the scenarios that LC and RAG each excels at, we visualize the word frequencies by their TF-IDF scores, plotted in Figure 5. The TF-IDF scores were calculated from Figure 4: Performance breakdown by question type for LC Only and RAG Only. Figure 5: Top 15 Words based on TF-IDF Score for LC Only vs. RAG Only. questions in the datasets where either LC or RAG produced correct answers exclusively. Specifically, all questions from each dataset are concatenated and treated as single document for this analysis, meaning that the TF-IDF scores primarily reflect the term frequency within each dataset. Stopwords are removed and not shown in the plot. Figure 5 presents the top 15 words that appear most frequently combined in both LC only and RAG only questions. Words such as song, film, and novel have higher TF-IDF scores for LC, suggesting that LC performs better with narrative topics. Conversely, words like country, dataset, and model have higher scores for RAG, indicating its strength in retrieving information on technical or data-oriented topics. This analysis underscores the complementary strengths and limitations of LC and RAG in handling different types of questions. 5.5 Impact of Generation Model in RAG We now evaluate the impact of different generation models on RAGs performance. Table 6 shows the results of using GPT-4o and GPT-4-Turbo as the generator with three retrievers (BM25, Tree Index, RAPTOR), each of which represents one retriever type. The results indicate that the performance of different generation models remains largely conRetriever Model Correct (%) RAG Only RAG Better BM25 Tree-Index GPT-4o GPT-4-Turbo GPT-4o GPT-4-Turbo 319 (20.4) 310 (19.8) 470 (30.1) 458 (29.3) 50 51 82 81 141 152 234 RAPTOR 602 (38.5) 589 (37.7) Table 6: Results of using different generation models GPT-4o GPT-4-Turbo 258 295 97 99 sistent regardless of the retriever used. RAPTOR performs the best across both generation models, though there is slight decrease in performance when using GPT-4-Turbo compared to GPT-4o. While GPT-4o slightly outperforms GPT-4Turbo across all retrievers, the differences are marginal. This implies that both generation models are capable of generating high-quality responses, and the choice between them may depend more on other factors such as efficiency or resource availability. The consistency across retrievers also demonstrates that the retrieval method plays larger role in determining overall performance than the specific generation model used. We will report the results from other models and the experiment is in progress. 5.6 Case Study For deeper understanding of the difference between LC and RAG, we conduct case study to analyze the frequent errors from each method, and present them in Tables 7 and 8. We manually examine the questions that only RAG made mistakes, and those only LC made mistakes. The most frequent mistake made by RAG is its failure to retrieve the relevant context, leading to its refusal to answer the question. As shown in Table 7, the model correctly identifies that Anthony Upko was formerly involved in the government of Nigeria but fails to retrieve the debt-to-GDP ratio as part of the context. This retrieval failure can arise due to two possible reasons: the retriever might fail to locate the relevant sentences from documents, or the sentences may be split across two chunks, with the debt-to-GDP ratio lacking clear subject. Interestingly, when provided with the same prompt, LC rarely reports lack of context, suggesting its robustness in handling such cases. Another error made by RAG is misinterpreting partial context. In the second example, where RAG incorrectly answered the birthday, the model retrieved May 8, 1940, instead of the correct date, January 8, 1935. This occurred because the sen1. 11 percent Nigeria is the The context does not Question: What is the debt-to-GDP ratio of the country where Anthony Upko was formerly involved in the government? Wrong Answer: provide the debt-to-GDP ratio for Nigeria. Gold: Relevant Sents: worlds 20th largest economy ... the debt-to-GDP ratio is only 11 percent. Anthony Ukpo was Minister of 2. Information and Culture, and then Governor of Rivers State, Nigeria. Question: song Swing Down Sweet Chariot birthday? Wrong Answer: Gold: Relevant Sents: Chariot is traditional song ... recorded by Elvis Presley. 2. Elvis Aaron Presley (January 8, 1935 - August 16, 1977), also known as ... Table 7: Examples cases where RAG made mistakes When is the performer of Swing Down Sweet January 8, 1935 May 8, 1940 1. This helped us narrow Yes, the tweets come Do the tweets come from Question: specific region? Wrong Answer: from 16 different countries. Gold: No Relevant Sents: down our query space to 16 countries. Question: his wealth? Wrong Answer: Gold: Relevant Sents: aunts estate, Emily learns that Valancourt has gone to Paris and lost his wealth. Where did Valancourt lose Returning to her In Gambling. Paris Table 8: Examples representing common cases where only RAG answers correctly tence Swing Down Sweet Chariot is traditional song ... recorded by Elvis Presley spans too long, creating ambiguity in linking the birthday to the correct person. This type of retrieval failure highlights core limitation: RAG relies heavily on retrieving continuous text spans, and any fragmentation or overly long context can lead to an incomplete understanding. In contrast, LC tends to provide more holistic answers when processing longer contexts directly, as it bypasses the dependency on retrieval module. Wrong answers by LC are often caused by question misinterpretation. For instance, as shown in Table 8, when asked whether the tweets come from specific region, LC answers yes, referencing that the tweets originate from 16 countries. It fails to interpret the relationship between specific region and 16 different countries. In another example, when asked where Valancourt lost his wealth, the model identifies the correct sentence but answers how instead of where. These examples highlight that LC sometimes struggles to align its semantic understanding with the required level of specificity or perspective, resulting in answers that are related but not addressing the questions intent. In both cases, the LLMs are able to locate the related texts from the documents, but the reasoning ability might be affected by the noise.', 'summary': '<p>В этом разделе статьи рассматривается сравнение двух подходов к ответу на вопросы: использование длинного контекста (LC) и извлечение информации с последующей генерацией ответа (RAG). </p>\n<p><strong>Методология оценки:</strong></p>\n<p>Для оценки использовалась система "выиграл-проиграл". Вопросы, на которые правильно ответила модель, использующая длинный контекст, отмечены как "LC Correct", а те, на которые правильно ответила модель RAG, как "RAG Correct". Пересечение этих категорий - вопросы, на которые обе модели ответили верно. "LC Only" - вопросы, на которые правильно ответила только модель LC, "RAG Only" - только RAG. "LC Better" и "RAG Better" обозначают, какая модель дала лучший ответ (не обязательно точный).</p>\n<p><strong>Используемые модели извлечения (Retrievers):</strong></p>\n<p>Для RAG использовались различные методы извлечения информации:\n* <strong>Chunk-based:</strong> разбиение текста на фрагменты.\n* <strong>Index-based:</strong> индексирование текста для быстрого поиска. Включает методы BM25, Contriever, Text-emb-3-small, Tree Index и Window Parsing.\n* <strong>Summarization-based:</strong> извлечение информации с помощью суммаризации. Использовался метод RAPTOR.</p>\n<p><strong>RAPTOR:</strong>\nRAPTOR строит иерархическое дерево, рекурсивно кластеризуя текстовые фрагменты на основе семантической схожести. Каждый кластер суммируется в родительский узел. Затем дерево "схлопывается" в один слой, и запрос сравнивается со всеми узлами на всех уровнях одновременно. Выбираются наиболее релевантные узлы в пределах заданного лимита токенов.\n*   <strong>Отличие от Llama Tree Index:</strong> RAPTOR может объединять фрагменты из разных частей текста и даже включать один и тот же фрагмент в несколько кластеров. Также RAPTOR оценивает все узлы на всех уровнях, а не только листовые, что позволяет извлекать не только исходные тексты, но и сгенерированные резюме.</p>\n<p><strong>Результаты:</strong></p>\n<ul>\n<li><strong>Сравнение методов извлечения:</strong> RAPTOR показал наилучший результат (38.5% правильных ответов), опередив другие методы. Индексные методы показали себя лучше, чем методы, основанные на фрагментации.</li>\n<li><strong>Сравнение LC и RAG:</strong><ul>\n<li>В целом, LC показал лучшие результаты (56.3% правильных ответов), чем RAG (49.0%). </li>\n<li>Однако, RAG правильно ответил на почти 10% вопросов, на которые LC не ответил.</li>\n<li>В "свободной" оценке (когда сравнивается качество ответа, а не только точность), LC также превзошёл RAG, что говорит о силе LC в ответах на открытые вопросы.</li>\n<li>На некоторых наборах данных (например, MultiDoc2Dial) RAG показал лучшие результаты, в то время как на других (NarrativeQA и QuaLITY) LC был значительно лучше. Это говорит о том, что у каждого метода есть свои сильные и слабые стороны.</li>\n</ul>\n</li>\n<li><strong>Анализ по источникам знаний:</strong><ul>\n<li>LC лучше работает с такими источниками, как Wikipedia и истории. При этом, контекст Wikipedia был специально "зашумлен", что, возможно, повлияло на результат RAG.</li>\n<li>RAG лучше справляется с диалоговыми источниками и работает на уровне LC с научными статьями и отчетами.</li>\n</ul>\n</li>\n<li><strong>Анализ по типам вопросов:</strong><ul>\n<li>LC лучше отвечает на фактические вопросы (например, "Кто", "Где", "Какой").</li>\n<li>RAG сравним с LC по открытым вопросам (например, "Как").</li>\n<li>RAG превосходит LC в ответах на общие вопросы (например, "Да/Нет").</li>\n</ul>\n</li>\n<li><strong>Анализ частоты слов:</strong><ul>\n<li>Слова, связанные с повествованием (песня, фильм, роман), чаще встречались в вопросах, на которые правильно отвечал LC.</li>\n<li>Слова, связанные с техническими аспектами (страна, набор данных, модель), чаще встречались в вопросах, на которые правильно отвечал RAG.</li>\n</ul>\n</li>\n<li><strong>Влияние модели генерации в RAG:</strong><ul>\n<li>Влияние разных моделей генерации на RAG также оценивалось, но результаты описаны в таблице.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Выводы:</strong></p>\n<p>Оба метода имеют свои преимущества и недостатки. LC хорошо работает с длинным, даже зашумленным контекстом, и лучше отвечает на фактические вопросы. RAG, в свою очередь, хорошо справляется с диалогами, научными текстами и открытыми вопросами, где требуется синтез информации из разных источников. Это подчеркивает, что извлечение информации не может быть просто заменено длинным контекстом, и что оба подхода могут дополнять друг друга.</p>'}, {'title': 'Discussion', 'content': '6.1 What is Long Context? Although we have reviewed 9 studies that either directly or implicitly compare or integrate RAG and Long Context, very few studies clearly define what Long Context is. To this end, we separately interpret the two words long and context. Long. Out of the 9 studies reviewed earlier, only 2 studies, ChatQA2 and LongBench v2 explicitly define Long Context as greater than 32k and greater than 8k tokens respectively. For other studies, we can only infer their definitions of long based on the models and datasets they use. It seems that three studies consider 8k as minimum requirement for long context, and another three studies set this requirement at 16k. Lastly, OP-RAG regards 128k as long context. In short, each work defines Long Context based on its own criteria due to the lack of clear standard. Moreover, as the context windows of language models continue to expand, the terms long and short are relative. For example, 4k tokens are not considered long context in any of the reviewed studies but are extremely long for BERTbase models, which support only 512 tokens. As result, the definition of long remains ambiguous, leading to inconsistent use of this concept among researchers. In practice, the definition of long is complicated, depending on the context length of latest LLMs, and the length of the documents in targeted domain. Context In the English dictionary, context is defined as the situation within which something happens, and that can help explain it. By this definition, the context of question is expected to help explain it, implying that the context should have strong relevance to the question. However, long-context datasets are not always constructed with this principle in mind. The construction of long-context datasets can generally be categorized into two types: Realistic Long Texts: These datasets originate from sources such as novels, research papers, or other lengthy narratives, exemplified by datasets like NovelQA. Such datasets typically pose challenges that involve reading comprehension and require models to process and synthesize dense information spread across cohesive, extended text. Synthetic Long Texts: These datasets are often created by concatenating smaller, query-relevant segments of text, such as Wikipedia-sourced datasets in LongBench. This construction process may involve stitching together Wikipedia excerpts, injecting noise, or combining unrelated passages to simulate long document. critical observation is that realistic long contexts align more closely with reading comprehension tasks, where models primarily absorb and reason over information. Such datasets have high contextual relevance, since the questions are normally based on the documents that users provided. In contrast, synthetic long contexts often resemble factual reasoning tasks, where models retrieve and verify knowledge. Such datasets inherently incorporate pre-processing step like RAG pipeline. They can assess the impact of information placement on model performance, such as the lost-in-the-middle phenomenon. On the other hand, realistic and synthetic long texts can only serve as proxies to reflect context relevance to some extent. The scope of the context is question-dependent and difficult to define clearly. 6.2 How to Compare or Combine LC & RAG? The lack of clear definition for long context also indicates the absence of coherent framework for comparing or combining LC and RAG. We propose such framework by examining three key perspectives: context length, context relevance, and experiment design. Context Length. From the models perspective, context length refers to the maximum number of tokens model can process. From the datasets perspective, it denotes the amount of text provided In synthetic datasets, context with question. length is flexible, but this introduces trade-off between length and relevance. Adding irrelevant information as context may help to test models robustness to noise, but such testing may not represent real-world use cases. Therefore, any framework for comparing LC and RAG should clearly define what is considered long, while indicating whether this length criterion originates from the models capabilities, the datasets design, or both. Context Relevance. An evaluation framework must also address the relevance of the text provided as input to the model. It is crucial to distinguish between realistic long contexts and synthetic long contexts. When benchmarks include both types, separate evaluations are necessary, as synthetic contexts often have low relevance and may not accurately reflect real-world scenarios. Interestingly, the construction of synthetic long contexts often mirrors RAG pipelines. Providing an entire curated text to an LLM as context essentially represents long context RAG approach, given that such text is assembled during dataset creation. Further chunking can introduce biases against RAG by disrupting the continuity of information within each piece. Additionally, many benchmarks categorize tasks as single-doc or multi-doc based on whether the text originates from single source or multiple documents. While convenient, this categorization does not perfectly align with realistic or synthetic contexts. single document may sometimes be artificially composed of smaller fragments, while multi-sourced document might involve highly relevant sources, such as group of research papers discussing the same problem. The key issue remains determining to what extent the context provided as input to LLMs contains sufficient and relevant content to answer the question, without introducing unnecessary or unrelated information. Experiment Settings. When investigating LC and RAG, the experimental objectives can be broadly grouped into two categories: comparison and combination. Short RAG v.s. Long Single Input: one might compare short-context RAG pipeline against long-context single-input setup, analyzing both performance and computational cost. This provides insights into the trade-off between running an extra retrieval pipeline for shorter contexts versus allowing the model to process larger uninterrupted text. Long RAG v.s. Long Single Input: One may also compare long-context RAG pipeline with longcontext single-input approach. Here, the goal is to see whether chunking or filtering more relevant content through retrieval can outperform or complement fully integrated long-context approach by truncating exceptionally long documents. In the first setting, the retrieval pipeline naturally reduces the number of tokens. In the second setting, the context length remains the same for both methods, with the only difference being how the text is processed. RAG over Increasing Context: Another possible goal is understanding how RAG performance changes with increasing context lengths. In this scenario, the LC refers specifically to how many tokens model can handle. This line of work can reveal how well RAG pipelines scale when models absorb increasingly larger inputs. On the other hand, findings from evaluations often serve as guidelines for settings that address real-world problems. In this sense, RAG and LC may complement each other in real-world settings, depending on the characteristics of the data source and the types of questions to be answered. 6.3 Revisiting All Studies Based on the earlier discussion, the exploration of LC and RAG methods in LLMs highlights some critical challenges that researchers often overlook. Trade-off between Context Length and Relevance. Many studies hesitate between using flexible synthetic context with noisy concatenated contexts, or realistic context with dense information but less availability. Among the 9 studies, 6 select synthetic context as part of the datasets. Our own evaluation has also selected synthetic context datasets, but we consider the influence of synthetic long context and separately evaluate their results by context source; e.g. Wikipedia source with manually added noises represents low context relevance. Several studies have attempted to address this challenge. LongBench recently updated v2 which collects only realistic data. Despite smaller scale, LongBench v2 shows substantial improvement in context relevance compared to its first version. LongRAG retrieves from massive corpus for all questions, instead of assigning one context to each question. This method avoids retrieving from synthetic long context and is hence recommendable. Diversity in Retrieval Mechanisms. In the comparison of RAG and LC, RAG is often underrepresented due to an over-reliance on traditional retrieval strategies. Among the 9 studies, 5 experiment with different retrievers, only 2 try different chunking sizes, and none consider any retrieval method beyond chunk-based retrievers. Although we experiment with index-based and summarization-based retrievers, we cannot promise that our selected method outperforms all retrieval strategies. For investigating RAG performance over increasing context, some studies propose their own strategies for chunking and placing RAG. OP-RAG proposes preserving the original order of chunks from the context, while LC LLM-RAG proposes placing higher-scored chunks at the front and back. In addition to more advanced retrievers, certain in- (Manning et al., 2008) formation retrieval (IR) techniques like relevance feedback (Harman, 1992) or query expansion (Carpineto and Romano, 2012) might further enhance RAG performance, yet these have been overlooked in existing frameworks. Computational Cost. Most existing studies test on 6 to 8 datasets, and it becomes increasingly expensive to conduct experiments on too many models. This is especially the case when new longcontext LLMs are being released at very fast pace. Hence, any work might be questioned because the experiment results are only applicable to one or few models. Among all works, LC RAG Performance includes the largest number of models (20). While their efforts are remarkable, they only experiment on 3 datasets. FinanceBench (Islam et al., 2023) looks at finance domain, Databricks DocsQA is based on Databricks platform, and NQ as shown table 2 as very low rate of requiring external knowledge. This is not meant as criticism but rather to show the trade-off between testing many models and having comprehensive benchmark.', 'summary': '<h2>Изложение раздела 6 "Длинный контекст и RAG"</h2>\n<p><strong>6.1 Что такое "длинный контекст"?</strong></p>\n<p>Несмотря на наличие исследований, сравнивающих или объединяющих RAG (Retrieval-Augmented Generation) и длинный контекст (Long Context, LC), чёткого определения последнего практически нет. Поэтому авторы статьи рассматривают понятия "длинный" и "контекст" по отдельности.</p>\n<ul>\n<li>\n<p><strong>Длинный:</strong> Из рассмотренных исследований только два (ChatQA2 и LongBench v2) явно определяют длинный контекст как более 32 тысяч и 8 тысяч токенов соответственно. В остальных работах определение длинного контекста можно вывести только из используемых моделей и наборов данных. Три исследования считают минимальным требованием 8 тысяч токенов, ещё три — 16 тысяч. OP-RAG рассматривает 128 тысяч токенов как длинный контекст. Таким образом, каждое исследование определяет длинный контекст по собственным критериям из-за отсутствия единого стандарта. Более того, по мере расширения контекстного окна языковых моделей понятия "длинный" и "короткий" становятся относительными. Например, 4 тысячи токенов не считаются длинным контекстом ни в одном из рассмотренных исследований, но являются очень длинными для моделей BERTbase, поддерживающих только 512 токенов. В результате, определение "длинного" остаётся неоднозначным, что приводит к непоследовательному использованию этого понятия. На практике определение "длинного" зависит от длины контекста последних LLM и длины документов в целевой области.</p>\n</li>\n<li>\n<p><strong>Контекст:</strong> В словаре контекст определяется как ситуация, в которой что-то происходит и которая помогает это объяснить. Следовательно, контекст вопроса должен помогать его объяснить, то есть иметь сильную связь с вопросом. Однако наборы данных для длинного контекста не всегда создаются с учётом этого принципа. Их можно разделить на два типа:</p>\n<ul>\n<li><strong>Реалистичные длинные тексты:</strong> Это данные из романов, научных статей и других длинных повествований (например, NovelQA). Эти наборы данных требуют от моделей понимания прочитанного и синтеза информации, распределённой по связному тексту.</li>\n<li><strong>Синтетические длинные тексты:</strong> Создаются путём объединения небольших, релевантных запросу фрагментов текста (например, наборы данных из Википедии в LongBench). Это может включать объединение отрывков из Википедии, добавление шума или объединение несвязанных фрагментов.</li>\n</ul>\n<p>Реалистичные длинные контексты больше соответствуют задачам понимания прочитанного, где модели в основном поглощают и анализируют информацию. Такие наборы данных имеют высокую контекстную релевантность, поскольку вопросы обычно основаны на документах, предоставленных пользователями. Напротив, синтетические длинные контексты часто напоминают задачи фактического рассуждения, где модели извлекают и проверяют знания. Такие наборы данных включают этап предварительной обработки, аналогичный конвейеру RAG. Они могут оценивать влияние расположения информации на производительность модели, например, эффект "потери в середине". С другой стороны, как реалистичные, так и синтетические длинные тексты могут лишь в некоторой степени отражать релевантность контекста. Объем контекста зависит от вопроса и его трудно определить однозначно.</p>\n</li>\n</ul>\n<p><strong>6.2 Как сравнивать или объединять LC и RAG?</strong></p>\n<p>Отсутствие чёткого определения длинного контекста также указывает на отсутствие согласованной системы для сравнения или объединения LC и RAG. Авторы предлагают такую систему, рассматривая три ключевых аспекта: длина контекста, релевантность контекста и дизайн эксперимента.</p>\n<ul>\n<li>\n<p><strong>Длина контекста:</strong> С точки зрения моделей, это максимальное количество токенов, которое модель может обработать. С точки зрения наборов данных, это количество текста, предоставленного в контексте с вопросом. В синтетических наборах данных длина контекста является гибкой, но это создаёт компромисс между длиной и релевантностью. Добавление нерелевантной информации в качестве контекста может помочь проверить устойчивость моделей к шуму, но такое тестирование может не соответствовать реальным сценариям использования. Поэтому любая система для сравнения LC и RAG должна чётко определять, что считается длинным, и указывать, исходит ли этот критерий длины из возможностей моделей, структуры набора данных или и того, и другого.</p>\n</li>\n<li>\n<p><strong>Релевантность контекста:</strong> Система оценки должна также учитывать релевантность текста, предоставляемого в качестве входных данных для модели. Крайне важно различать реалистичные длинные контексты и синтетические длинные контексты. Если тесты включают оба типа, необходимы отдельные оценки, поскольку синтетические контексты часто имеют низкую релевантность и могут неточно отражать реальные сценарии. Интересно, что построение синтетических длинных контекстов часто имитирует конвейеры RAG. Предоставление LLM целого кураторского текста в качестве контекста по сути представляет собой подход RAG с длинным контекстом, учитывая, что такой текст собирается во время создания набора данных. Дальнейшее разделение на части может создать предвзятость против RAG, нарушая непрерывность информации внутри каждой части. Кроме того, многие тесты классифицируют задачи как однодокументные или многодокументные в зависимости от того, исходит ли текст из одного источника или из нескольких документов. Хотя это удобно, эта классификация не полностью соответствует реалистичным или синтетическим контекстам. Один документ иногда может быть искусственно составлен из более мелких фрагментов, в то время как многоисточниковый документ может включать в себя очень релевантные источники, такие как группа научных работ, обсуждающих одну и ту же проблему. Ключевым вопросом остается определение того, в какой степени контекст, предоставляемый в качестве входных данных для LLM, содержит достаточное и релевантное содержание для ответа на вопрос, не вводя ненужной или несвязанной информации.</p>\n</li>\n<li>\n<p><strong>Настройки эксперимента:</strong> При исследовании LC и RAG экспериментальные цели можно разделить на две категории: сравнение и объединение.</p>\n<ul>\n<li><strong>Короткий RAG против длинного единичного ввода:</strong> Можно сравнить конвейер RAG с коротким контекстом с настройкой длинного контекста с единичным вводом, анализируя как производительность, так и вычислительные затраты. Это даёт понимание компромисса между запуском дополнительного конвейера извлечения для более коротких контекстов и предоставлением модели возможности обрабатывать более длинный непрерывный текст.</li>\n<li><strong>Длинный RAG против длинного единичного ввода:</strong> Можно также сравнить конвейер RAG с длинным контекстом с подходом с длинным контекстом с единичным вводом. Здесь цель состоит в том, чтобы увидеть, может ли разделение на части или фильтрация более релевантного контента с помощью извлечения превзойти или дополнить полностью интегрированный подход с длинным контекстом путём усечения исключительно длинных документов. В первом случае конвейер извлечения естественным образом уменьшает количество токенов. Во втором случае длина контекста остается одинаковой для обоих методов, и единственным отличием является способ обработки текста.</li>\n<li><strong>RAG при увеличении контекста:</strong> Другая возможная цель — понять, как производительность RAG изменяется с увеличением длины контекста. В этом сценарии LC относится конкретно к тому, сколько токенов может обработать модель. Эта работа может показать, насколько хорошо масштабируются конвейеры RAG, когда модели поглощают всё более крупные входные данные.</li>\n</ul>\n</li>\n</ul>\n<p>С другой стороны, результаты оценок часто служат ориентиром для настроек, решающих реальные проблемы. В этом смысле RAG и LC могут дополнять друг друга в реальных условиях, в зависимости от характеристик источника данных и типов вопросов, на которые нужно ответить.</p>\n<p><strong>6.3 Пересмотр всех исследований</strong></p>\n<p>Основываясь на предыдущем обсуждении, изучение методов LC и RAG в LLM выявляет некоторые важные проблемы, которые исследователи часто упускают из виду.</p>\n<ul>\n<li>\n<p><strong>Компромисс между длиной контекста и релевантностью:</strong> Многие исследования колеблются между использованием гибкого синтетического контекста с зашумленными объединенными контекстами или реалистичного контекста с плотной информацией, но меньшей доступностью. Из 9 исследований 6 выбирают синтетический контекст в качестве части наборов данных. Собственная оценка также выбрала наборы данных с синтетическим контекстом, но авторы учитывают влияние синтетического длинного контекста и отдельно оценивают их результаты по источнику контекста; например, источник Википедии с добавленным вручную шумом представляет низкую релевантность контекста. Несколько исследований пытались решить эту проблему. LongBench недавно обновил v2, который собирает только реалистичные данные. Несмотря на меньший масштаб, LongBench v2 показывает значительное улучшение релевантности контекста по сравнению с первой версией. LongRAG извлекает данные из массивного корпуса для всех вопросов, а не назначает один контекст каждому вопросу. Этот метод позволяет избежать извлечения из синтетического длинного контекста и поэтому рекомендуется.</p>\n</li>\n<li>\n<p><strong>Разнообразие механизмов извлечения:</strong> При сравнении RAG и LC, RAG часто недопредставлен из-за чрезмерной зависимости от традиционных стратегий извлечения. Из 9 исследований 5 экспериментируют с различными извлекателями, только 2 пробуют разные размеры фрагментов, и ни одно не рассматривает какой-либо метод извлечения, выходящий за рамки извлекателей на основе фрагментов. Хотя авторы экспериментируют с извлекателями на основе индексов и на основе суммирования, они не могут обещать, что выбранный ими метод превосходит все стратегии извлечения. Для исследования производительности RAG при увеличении контекста некоторые исследования предлагают свои собственные стратегии разделения на части и размещения RAG. OP-RAG предлагает сохранять исходный порядок фрагментов из контекста, в то время как LC LLM-RAG предлагает размещать фрагменты с более высокими оценками в начале и конце. В дополнение к более продвинутым извлекателям, некоторые методы извлечения информации (IR), такие как обратная связь по релевантности или расширение запросов, могут ещё больше повысить производительность RAG, но они были упущены из виду в существующих системах.</p>\n</li>\n<li>\n<p><strong>Вычислительные затраты:</strong> Большинство существующих исследований проводят тесты на 6-8 наборах данных, и становится всё более дорогостоящим проводить эксперименты на слишком большом количестве моделей. Это особенно актуально, когда новые LLM с длинным контекстом выпускаются очень быстро. Следовательно, любая работа может быть поставлена под сомнение, поскольку результаты эксперимента применимы только к одной или нескольким моделям. Среди всех работ LC RAG Performance включает наибольшее количество моделей (20). Хотя их усилия замечательны, они... (далее в тексте).</p>\n</li>\n</ul>'}, {'title': 'Conclusion', 'content': 'In this paper, we survey existing studies comparing or combining LC and RAG, analyzing why different implementations may result in some conflicts among their insights. Therefore, we present thorough comparison of LC and RAG approaches by leveraging diverse set of long context QA datasets. We filtered out questions that could be answered from parametric knowledge, ensuring fair comparison by focusing on questions that required external context. Along these lines, we have developed systematic filtering and evaluation process, identified the best retrieval method, and expanded the dataset to provide statistically significant basis for analysis. The results indicate that LC generally outperforms RAG for tasks involving wellstructured, dense contextssuch as Wikipedia articles and booksand is better at answering questions requiring specific information. By contrast, RAG demonstrates advantages in handling fragmented information, particularly in dialogue-based scenarios and for more general questions. Beyond merely presenting the experimental results and findings, we delve deeper into the concept of long context and examine how LC and RAG should be compared. Our discussion aims to ensure that the insights gained are more impactful and applicable to real-world scenarios.', 'summary': '<p>В этой статье мы проводим обзор исследований, сравнивающих или объединяющих методы Long Context (LC) и Retrieval-Augmented Generation (RAG). Мы анализируем причины, по которым разные реализации этих методов могут приводить к противоречивым выводам. Для этого мы проводим тщательное сравнение LC и RAG на разнообразных наборах данных для QA (вопросно-ответных систем) с длинным контекстом. </p>\n<p>Чтобы обеспечить честное сравнение, мы отфильтровали вопросы, на которые можно ответить, используя только "параметрические знания" (то есть знания, уже содержащиеся в модели). Мы сосредоточились на вопросах, требующих обращения к внешнему контексту. Для этого мы разработали систематический процесс фильтрации и оценки, определили лучший метод извлечения информации и расширили набор данных, чтобы обеспечить статистически значимую основу для анализа.</p>\n<p>Результаты показывают, что LC в целом превосходит RAG в задачах, где контекст хорошо структурирован и плотный (например, статьи из Википедии и книги). LC лучше справляется с ответами на вопросы, требующие конкретной информации. Напротив, RAG демонстрирует преимущества при работе с фрагментированной информацией, особенно в диалоговых сценариях и для более общих вопросов.</p>\n<p>Помимо представления экспериментальных результатов, мы углубляемся в концепцию длинного контекста и рассматриваем, как следует сравнивать LC и RAG. Цель нашего обсуждения – сделать полученные выводы более значимыми и применимыми к реальным сценариям.</p>\n<p><strong>Комментарий:</strong>\n*   <strong>Long Context (LC)</strong> относится к моделям, способным обрабатывать очень длинные последовательности текста.\n*   <strong>Retrieval-Augmented Generation (RAG)</strong> – это подход, при котором модель сначала извлекает релевантную информацию из внешнего источника, а затем использует эту информацию для генерации ответа.\n*   <strong>Параметрические знания</strong> – это знания, которые модель "выучила" в процессе обучения и хранит в своих параметрах.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source', '#optimization (1)', '#plp', '#rag (1)', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В данной работе рассматриваются два основных подхода, позволяющих большим языковым моделям (LLM) работать с очень длинным внешним контекстом: увеличение размера контекстного окна (Long Context, LC) и использование механизмов извлечения релевантной информации (Retrieval-Augmented Generation, RAG).</p>
<p><strong>Увеличение контекстного окна (LC)</strong> позволяет модели обрабатывать больше текста за один раз, что потенциально дает ей доступ ко всей необходимой информации. <strong>Метод RAG</strong>, напротив, полагается на извлечение только той части внешнего контекста, которая наиболее релевантна текущему запросу.</p>
<p>В статье проводится анализ существующих исследований по этим двум подходам, выделяются их основные идеи и противоречия. Затем авторы проводят более тщательную оценку, отфильтровывая вопросы, на которые можно ответить без внешнего контекста, определяя наиболее эффективные методы извлечения и расширяя наборы данных.</p>
<p>Результаты показывают, что <strong>в задачах ответов на вопросы LC обычно превосходит RAG</strong>, особенно когда дело касается вопросов, основанных на Wikipedia. <strong>Извлечение на основе суммаризации</strong> (summarization-based retrieval) показывает результаты, сопоставимые с LC, в то время как <strong>извлечение на основе фрагментов</strong> (chunk-based retrieval) отстает. Тем не менее, у <strong>RAG есть преимущества в диалоговых задачах и общих запросах</strong>.</p>
<p>Эти результаты подчеркивают компромиссы между стратегиями RAG и LC и дают рекомендации для будущей оптимизации LLM с использованием внешних источников знаний. В заключение авторы также обсуждают важность <strong>релевантности контекста</strong>, которая часто упускается из виду в существующих исследованиях.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.01880/x1.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><p>Большие языковые модели (LLM) демонстрируют хорошие результаты при ответах на вопросы в открытом формате, но сталкиваются с проблемами, такими как галлюцинации, отсутствие актуальной информации и узкоспециализированных знаний. Распространенным решением является использование внешней памяти для предоставления LLM надежных и обновленных данных. Однако, добавление внешнего контента ограничено размером контекстного окна LLM. Для решения этой проблемы используются два основных подхода: (i) создание моделей с длинным контекстным окном для обработки большего объема информации (LC) и (ii) использование механизмов поиска (retrievers) для включения текстовых фрагментов, релевантных запросу (RAG). Наблюдается явная тенденция к разработке моделей, способных обрабатывать более длинные контекстные окна, а также к комбинированию LC и RAG.</p>
<p>Хронологический обзор исследований показывает, что с середины 2023 года наблюдается растущий интерес к LC и RAG, о чем свидетельствует увеличение количества публикаций, направленных на оптимизацию эффективного поиска и использования длинных контекстов. Разработка моделей, поддерживающих более длинные контекстные окна, подчеркивает растущую важность эффективной обработки больших объемов входных данных.</p>
<p>Несмотря на общее признание важности LC и RAG, существуют разногласия и противоречивые выводы в различных исследованиях. Например, некоторые исследования подтверждают эффективность комбинации LC и RAG, в то время как другие предполагают, что это может быть нецелесообразно. Также противоречивы выводы относительно преимуществ RAG по сравнению с LC. Некоторые работы показывают преимущества RAG в определенных контекстах, в то время как другие подчеркивают превосходство LC. Эти противоречивые выводы демонстрируют сложность и продолжающиеся дебаты в этой области, предполагая, что оптимальные стратегии могут варьироваться в зависимости от конкретных архитектур моделей и условий тестирования.</p>
<p>Для изучения причин этих разногласий, авторы статьи проводят углубленное исследование условий, приводящих к противоречиям между существующими работами. В процессе исследования также выявляются ключевые аспекты, которые могли быть упущены в предыдущих исследованиях. В частности, авторы пересматривают процесс оценки и вносят следующие изменения:
1.  Фильтрация вопросов из существующих наборов данных, на которые можно правильно ответить без внешнего контекста, чтобы устранить смещения, вызванные параметрическими знаниями LLM, и сосредоточиться на вопросах, требующих внешних знаний.
2.  Оценка методов поиска и базовых моделей на уменьшенном отфильтрованном наборе данных (1000+ вопросов) из 12 наборов данных QA для определения лучшего метода поиска.
3.  Увеличение размера набора данных примерно в 10 раз путем сбора дополнительных данных из исходных источников 12 наборов данных.
4.  Сравнение ответов, полученных в двух настройках, т.е. LC и RAG, и проведение углубленного анализа.</p>
<p>Результаты основаны на расширенном наборе данных с использованием настройки длинного контекста и лучшего метода поиска, определенного ранее.</p>
<p>Основные вклады данной работы:
1.  Предоставление всестороннего обзора существующих исследований по LC и RAG, анализ их реализаций и ключевых выводов.
2.  Предложение справедливой и систематической структуры оценки и проведение подробного анализа для понимания сильных и слабых сторон LC и RAG.
3.  Обсуждение проблем сравнения и объединения LC и RAG, с акцентом на ключевые моменты, которые исследователи часто упускают из виду в этой области.</p>
<p>Результаты оценки показывают, что модели LC обычно превосходят RAG при обработке самодостаточной информации, такой как истории, в то время как RAG превосходит в обработке фрагментированной информации, особенно в контексте диалогов. Эти эксперименты углубляют понимание сильных и слабых сторон LC и RAG, предлагая ценные сведения для оптимизации стратегий поиска и эффективной интеграции этих подходов для повышения производительности при ответах на вопросы в открытом доступе. Эти выводы также основаны на систематическом обзоре существующих исследований по этой теме. Кроме того, обсуждаются ключевые аспекты сравнения LC и RAG, выделяя области, которые недостаточно изучены в предыдущих исследованиях.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Related work</div>
                <div class="summary_text"><p>В статье рассматриваются методы извлечения информации (ретриверы) и модели с длинным контекстом (LC LLM), а также их сравнение и комбинация.</p>
<p><strong>Ретриверы</strong> — это ключевые компоненты RAG-пайплайнов, которые отвечают за поиск и извлечение релевантных фрагментов документов. Их можно разделить на три основных типа:</p>
<ol>
<li><strong>Чанк-ориентированные ретриверы:</strong><ul>
<li><strong>Разреженные ретриверы</strong> (например, BM25) работают на основе частоты терминов и ранжируют фрагменты по схожести, используя точные совпадения и взвешивание терминов.</li>
<li><strong>Плотные ретриверы</strong> кодируют запросы и фрагменты документов в плотные векторные представления и измеряют релевантность с помощью метрик схожести, таких как косинусное расстояние. Качество этих векторных представлений имеет решающее значение. Примерами являются Contriever, BGE-Large, E5Mistral7b и Dragon. Некоторые модели, такие как OpenAI embeddings и Zhipu-embedding-3, используют возможности LLM для создания качественных эмбеддингов.</li>
</ul>
</li>
<li><strong>Индекс-ориентированные ретриверы:</strong> требуют предварительной обработки документов с использованием сложных структур данных. Llama-Index упрощает взаимодействие между моделью и документами, предоставляя гибкий интерфейс для построения различных индексов. Индексы организуют данные для быстрого извлечения контекста. Примеры включают древовидные индексы и индексы на основе графов знаний.</li>
<li><strong>Ретриверы на основе суммаризации:</strong> строятся на основе чанк- и индекс-ориентированных подходов. Они предоставляют краткие изложения основных моментов документа. RAPTOR, например, создает иерархические изложения текстовых фрагментов, что позволяет извлекать контекст на разных уровнях детализации, улучшая способность обрабатывать сложные запросы.</li>
</ol>
<p><strong>Модели с длинным контекстом (LC LLM)</strong>: исследования направлены на увеличение длины входных и выходных окон, что позволяет обрабатывать большие объемы текста. Анализ проводится по двум направлениям:</p>
<ol>
<li><strong>Возможности модели:</strong> Многие модели, помимо понимания длинных документов, обладают специализированными возможностями. ChatGLM2-6B-32K обеспечивает высокую эффективность рассуждений при низком потреблении памяти, XGen-7B-8K улучшает понимание длинного контекста и суммирование текста, InternLM-7B-8k оптимизирован для понимания знаний и перевода. Модели DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, Mixtral-7x8b и DBRX-Instruct демонстрируют хорошие результаты в математических вычислениях, логических рассуждениях и кодировании. Claude-3-Opus, Sonnet, Haiku, Gemini-1.5-flash и Gemini-1.5-pro поддерживают мультимодальность. GLM-4-9B-Chat, Mistral-12b-Instruct и Llama-3.1-Instruct обладают надежными многоязычными возможностями. Claude-2 отличается низким уровнем галлюцинаций при обработке очень длинных документов.</li>
<li><strong>Длина контекста:</strong> Модели делятся на три категории:<ul>
<li><strong>Короткий контекст</strong> (до 4K токенов), например, Llama2-70B и llama2-7B-chat-4k.</li>
<li><strong>Длинный контекст</strong> (до 32K токенов), например, XGen-7B8K, InternLM-7B-8k, Mixtral-7x8b, DBRX-Instruct и Gemma2-9B.</li>
<li><strong>Ультра-длинный контекст</strong> (более 32K токенов), например, Claude-2 (100K), Claude-3-Opus, Sonnet и Haiku (200K), GPT-4-Turbo, GPT-4o, GPT-o1, DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, GLM-4-9BChat, GLM-4-Plus, Mistral-12b-Instruct, Llama-3.1-Instruct (128K), Gemini-1.5-flahs и Gemini-1.5-pro (10M).</li>
</ul>
</li>
</ol>
<p><strong>Сравнение и комбинация LC и RAG:</strong> С увеличением длины контекстного окна в LLM, некоторые модели могут вместить весь документ, что снижает необходимость извлечения. Поэтому исследования начали сравнивать производительность LC LLM и RAG, а также изучать способы их объединения.</p>
<ul>
<li>LongBench проводит сравнение моделей с контекстом 4K и 32K с RAG.</li>
<li>Xu et al. (2024b) систематически сравнивают LC LLM и RAG и предлагают их комбинацию.</li>
<li>LongRAG вводит длинные ретриверы и ридеры.</li>
<li>ChatQA2 обучает LC LLM до 128K контекстного окна и тестирует их с длинными ретриверами.</li>
<li>Self-ROUTE позволяет модели выбирать RAG или LC на основе самоанализа для снижения затрат.</li>
<li>OPRAG сохраняет исходный порядок извлеченных фрагментов.</li>
<li>LC LLM meets RAG исследует LC LLM в RAG-системах и предлагает методы переупорядочивания извлечения.</li>
<li>LC RAG Performance of LLM оценивает эффективность RAG на LC LLM с длиной контекста от 2K до 2M токенов.</li>
<li>LongBench V2 тестирует LLM на понимание длинного контекста и рассуждения в более реалистичных условиях.</li>
</ul>
<p>Основные выводы:</p>
<ol>
<li><strong>Общие выводы:</strong><ul>
<li>Фрагменты для извлечения должны быть длиннее.</li>
<li>Количество фрагментов должно быть небольшим.</li>
<li>Обычно достаточно выбирать 5-10 лучших фрагментов.</li>
</ul>
</li>
<li><strong>Комбинирование стратегий:</strong><ul>
<li>Изучаются способы объединения LC и RAG для достижения лучшей производительности.</li>
</ul>
</li>
<li><strong>Сравнение LC и RAG:</strong><ul>
<li>Проводится сравнение производительности LC LLM и RAG в различных задачах.</li>
</ul>
</li>
</ol></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Question filtering and expansion</div>
                <div class="summary_text"><p>Для обеспечения честного и всестороннего сравнения методов, используемых в машинном обучении, авторы статьи создали свой собственный набор данных для оценки, опираясь на существующие наборы данных и применяя к ним необходимую фильтрацию и расширение.</p>
<p><strong>3.1 Фильтрация вопросов</strong></p>
<p>Они выбрали 12 наборов данных, часто используемых для сравнения методов обработки длинного контекста (LC) и извлечения с генерацией (RAG): Natural Questions, 2WikiMultihopQA, HotpotQA, MuSiQue, MultiFieldQA, NarrativeQA, QASPER, QuALTY, Coursera, TOEFL-QA и MultiDoc2Dial, а также добавили NovelQA, который является высококачественным набором данных, полученным из художественных романов. В таблице 2 представлен обзор этих наборов данных, включая их тип, тип контекста (однодокументный или многодокументный), источник контекста, среднюю длину контекста и исследования, в которых они использовались.</p>
<p>Так как современные большие языковые модели (LLM) обладают значительными возможностями, многие вопросы могут быть отвечены непосредственно на основе знаний, закодированных в их параметрах, что снижает необходимость во внешнем контексте. Однако некоторые запросы, например, связанные с личными разговорами, всегда требуют дополнительного контекста. Чтобы определить, какой подход более эффективно повышает производительность LLM при работе с длинными документами, авторы отфильтровали наборы данных, включив только те вопросы, на которые LLM не может ответить правильно без внешнего контекста. Это гарантирует, что любые правильные ответы, полученные впоследствии, будут зависеть от внешних знаний, а не от встроенных знаний модели. Для фильтрации вопросов использовалась модель GPT-4o из-за ее высокой производительности, при этом применялся строгий критерий точного соответствия, чтобы убедиться, что модель не только дает правильный ответ, но и демонстрирует полное понимание требуемой информации.</p>
<p><strong>3.2 Расширение вопросов (и контекста)</strong></p>
<p>Методы RAG и LC дают идентичные ответы примерно на 60% вопросов в существующих оценках, оставляя относительно мало вопросов для понимания различий между этими двумя подходами. Чтобы обеспечить надежную статистическую значимость, авторы расширили размер набора данных примерно до 20 000 вопросов, собрав дополнительные образцы. Для сохранения исходного распределения данных при сборе новых вопросов соблюдались два принципа. Во-первых, вопросы собирались только из исходного источника каждого набора данных, избегая искусственно сгенерированных или дополненных LLM вопросов. Во-вторых, к исходному контексту каждого вопроса добавлялись отвлекающие фрагменты, чтобы увеличить длину контекста, как это описано в работе LongBench. Для NovelQA использовались все доступные вопросы. Для наборов данных Coursera, MultiFieldQA и MultiDoc2Dial их размеры не были увеличены, чтобы избежать внесения искусственных данных. Расширенный набор данных далее называется полным набором вопросов, а исходный, до расширения, набор данных - выборочным набором вопросов.</p>
<p><strong>3.3 Статистика набора данных</strong></p>
<p>После расширения было получено 19 188 вопросов, из которых 13 651 требовали контекста для ответа, согласно методу фильтрации из раздела 3.1, как указано в таблице 3. Вопросы, основанные на фактических знаниях, например, из Coursera, показали высокую скорость удаления. Аналогично, вопросы, взятые из известных книг или требующие многошаговых рассуждений, часто с большей вероятностью могли быть отвечены LLM без контекста. При сравнении 12 отдельных наборов данных наблюдалась схожая скорость фильтрации между выборочным и полным наборами вопросов (см. таблицы 2 и 3), что указывает на то, что оба набора имеют схожее распределение.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluation methodology</div>
                <div class="summary_text"><p>В разделе 4.1 описывается методология оценки RAG (Retrieval-Augmented Generation) и LC (Long Context). Оценка проводится в три этапа:</p>
<p><strong>Этап 1: Эмпирическое исследование ретриверов.</strong> На первом этапе сравниваются пять различных методов извлечения информации (ретриверов) на небольшом наборе вопросов: BM25, Contriever, OpenAI Embeddings, Llama-Index и RAPTOR.  Выбирается ретривер, показавший наилучшие результаты, который затем используется для сравнения RAG с LC на полном наборе вопросов. <em>Комментарий: Этот этап важен для выбора оптимального метода извлечения информации для RAG, поскольку качество извлеченной информации напрямую влияет на качество ответов RAG.</em></p>
<p><strong>Этап 2: Сравнение RAG и LC.</strong> На втором этапе RAG сравнивается с LC на полном наборе вопросов с использованием лучшего ретривера, выбранного на первом этапе. Оба метода используют одну и ту же большую языковую модель (LLM) для ответов на вопросы. В RAG релевантные документы или фрагменты извлекаются из контекста и предоставляются LLM в качестве входных данных для генерации ответов. В LC весь доступный контекст (с возможным усечением с конца, если он превышает контекстное окно модели) передается LLM. <em>Комментарий: Сравниваются подходы, где RAG сначала извлекает релевантную информацию, а LC использует весь контекст.</em></p>
<p><strong>Этап 3: Углубленный анализ.</strong> На третьем этапе проводится углубленный анализ четырех подмножеств вопросов: 1) на которые правильно отвечает только RAG, 2) на которые правильно отвечает только LC, 3) на которые RAG дает лучшие ответы, и 4) на которые LC дает лучшие ответы. Этот анализ позволяет выявить сильные и слабые стороны каждого подхода в разных сценариях. <em>Комментарий: Этот этап позволяет понять, в каких ситуациях каждый метод работает лучше, что важно для понимания их различий.</em></p>
<p>Также в разделе описываются используемые ретриверы. BM25 используется как классический базовый метод. Contriever и text-embedding-3-Small представляют эмбеддинги из закрытой и открытой моделей соответственно. Для индексного извлечения используется Llama-Index с двумя методами индексации: tree-index (организует документы в иерархическую структуру) и Sentence Window Retriever (фокусируется на локальном контексте на уровне предложений).  <em>Комментарий: Описываются конкретные методы, которые используются для извлечения информации.</em></p>
<p>В разделе также поясняются используемые метрики оценки. Для определения правильности ответов используется метрика Exact Match (EM), которая требует точного совпадения ответа с эталонным.  Для сравнения качества ответов используется метрика F1 score, которая позволяет оценить степень совпадения ответов с эталонными, даже если они не совпадают точно.  Для более объективной оценки используется "свободная" оценка, которая учитывает случаи, когда один метод превосходит другой, включая случаи, когда один метод дает правильный ответ по EM, а другой нет, а также случаи, когда один метод имеет более высокий F1 score. <em>Комментарий: Объясняется, почему используется "свободная" оценка, и как она помогает компенсировать строгость EM.</em></p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.01880/x4.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Experiments</div>
                <div class="summary_text"><p>В этом разделе статьи рассматривается сравнение двух подходов к ответу на вопросы: использование длинного контекста (LC) и извлечение информации с последующей генерацией ответа (RAG). </p>
<p><strong>Методология оценки:</strong></p>
<p>Для оценки использовалась система "выиграл-проиграл". Вопросы, на которые правильно ответила модель, использующая длинный контекст, отмечены как "LC Correct", а те, на которые правильно ответила модель RAG, как "RAG Correct". Пересечение этих категорий - вопросы, на которые обе модели ответили верно. "LC Only" - вопросы, на которые правильно ответила только модель LC, "RAG Only" - только RAG. "LC Better" и "RAG Better" обозначают, какая модель дала лучший ответ (не обязательно точный).</p>
<p><strong>Используемые модели извлечения (Retrievers):</strong></p>
<p>Для RAG использовались различные методы извлечения информации:
* <strong>Chunk-based:</strong> разбиение текста на фрагменты.
* <strong>Index-based:</strong> индексирование текста для быстрого поиска. Включает методы BM25, Contriever, Text-emb-3-small, Tree Index и Window Parsing.
* <strong>Summarization-based:</strong> извлечение информации с помощью суммаризации. Использовался метод RAPTOR.</p>
<p><strong>RAPTOR:</strong>
RAPTOR строит иерархическое дерево, рекурсивно кластеризуя текстовые фрагменты на основе семантической схожести. Каждый кластер суммируется в родительский узел. Затем дерево "схлопывается" в один слой, и запрос сравнивается со всеми узлами на всех уровнях одновременно. Выбираются наиболее релевантные узлы в пределах заданного лимита токенов.
*   <strong>Отличие от Llama Tree Index:</strong> RAPTOR может объединять фрагменты из разных частей текста и даже включать один и тот же фрагмент в несколько кластеров. Также RAPTOR оценивает все узлы на всех уровнях, а не только листовые, что позволяет извлекать не только исходные тексты, но и сгенерированные резюме.</p>
<p><strong>Результаты:</strong></p>
<ul>
<li><strong>Сравнение методов извлечения:</strong> RAPTOR показал наилучший результат (38.5% правильных ответов), опередив другие методы. Индексные методы показали себя лучше, чем методы, основанные на фрагментации.</li>
<li><strong>Сравнение LC и RAG:</strong><ul>
<li>В целом, LC показал лучшие результаты (56.3% правильных ответов), чем RAG (49.0%). </li>
<li>Однако, RAG правильно ответил на почти 10% вопросов, на которые LC не ответил.</li>
<li>В "свободной" оценке (когда сравнивается качество ответа, а не только точность), LC также превзошёл RAG, что говорит о силе LC в ответах на открытые вопросы.</li>
<li>На некоторых наборах данных (например, MultiDoc2Dial) RAG показал лучшие результаты, в то время как на других (NarrativeQA и QuaLITY) LC был значительно лучше. Это говорит о том, что у каждого метода есть свои сильные и слабые стороны.</li>
</ul>
</li>
<li><strong>Анализ по источникам знаний:</strong><ul>
<li>LC лучше работает с такими источниками, как Wikipedia и истории. При этом, контекст Wikipedia был специально "зашумлен", что, возможно, повлияло на результат RAG.</li>
<li>RAG лучше справляется с диалоговыми источниками и работает на уровне LC с научными статьями и отчетами.</li>
</ul>
</li>
<li><strong>Анализ по типам вопросов:</strong><ul>
<li>LC лучше отвечает на фактические вопросы (например, "Кто", "Где", "Какой").</li>
<li>RAG сравним с LC по открытым вопросам (например, "Как").</li>
<li>RAG превосходит LC в ответах на общие вопросы (например, "Да/Нет").</li>
</ul>
</li>
<li><strong>Анализ частоты слов:</strong><ul>
<li>Слова, связанные с повествованием (песня, фильм, роман), чаще встречались в вопросах, на которые правильно отвечал LC.</li>
<li>Слова, связанные с техническими аспектами (страна, набор данных, модель), чаще встречались в вопросах, на которые правильно отвечал RAG.</li>
</ul>
</li>
<li><strong>Влияние модели генерации в RAG:</strong><ul>
<li>Влияние разных моделей генерации на RAG также оценивалось, но результаты описаны в таблице.</li>
</ul>
</li>
</ul>
<p><strong>Выводы:</strong></p>
<p>Оба метода имеют свои преимущества и недостатки. LC хорошо работает с длинным, даже зашумленным контекстом, и лучше отвечает на фактические вопросы. RAG, в свою очередь, хорошо справляется с диалогами, научными текстами и открытыми вопросами, где требуется синтез информации из разных источников. Это подчеркивает, что извлечение информации не может быть просто заменено длинным контекстом, и что оба подхода могут дополнять друг друга.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.01880/x5.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Discussion</div>
                <div class="summary_text"><h2>Изложение раздела 6 "Длинный контекст и RAG"</h2>
<p><strong>6.1 Что такое "длинный контекст"?</strong></p>
<p>Несмотря на наличие исследований, сравнивающих или объединяющих RAG (Retrieval-Augmented Generation) и длинный контекст (Long Context, LC), чёткого определения последнего практически нет. Поэтому авторы статьи рассматривают понятия "длинный" и "контекст" по отдельности.</p>
<ul>
<li>
<p><strong>Длинный:</strong> Из рассмотренных исследований только два (ChatQA2 и LongBench v2) явно определяют длинный контекст как более 32 тысяч и 8 тысяч токенов соответственно. В остальных работах определение длинного контекста можно вывести только из используемых моделей и наборов данных. Три исследования считают минимальным требованием 8 тысяч токенов, ещё три — 16 тысяч. OP-RAG рассматривает 128 тысяч токенов как длинный контекст. Таким образом, каждое исследование определяет длинный контекст по собственным критериям из-за отсутствия единого стандарта. Более того, по мере расширения контекстного окна языковых моделей понятия "длинный" и "короткий" становятся относительными. Например, 4 тысячи токенов не считаются длинным контекстом ни в одном из рассмотренных исследований, но являются очень длинными для моделей BERTbase, поддерживающих только 512 токенов. В результате, определение "длинного" остаётся неоднозначным, что приводит к непоследовательному использованию этого понятия. На практике определение "длинного" зависит от длины контекста последних LLM и длины документов в целевой области.</p>
</li>
<li>
<p><strong>Контекст:</strong> В словаре контекст определяется как ситуация, в которой что-то происходит и которая помогает это объяснить. Следовательно, контекст вопроса должен помогать его объяснить, то есть иметь сильную связь с вопросом. Однако наборы данных для длинного контекста не всегда создаются с учётом этого принципа. Их можно разделить на два типа:</p>
<ul>
<li><strong>Реалистичные длинные тексты:</strong> Это данные из романов, научных статей и других длинных повествований (например, NovelQA). Эти наборы данных требуют от моделей понимания прочитанного и синтеза информации, распределённой по связному тексту.</li>
<li><strong>Синтетические длинные тексты:</strong> Создаются путём объединения небольших, релевантных запросу фрагментов текста (например, наборы данных из Википедии в LongBench). Это может включать объединение отрывков из Википедии, добавление шума или объединение несвязанных фрагментов.</li>
</ul>
<p>Реалистичные длинные контексты больше соответствуют задачам понимания прочитанного, где модели в основном поглощают и анализируют информацию. Такие наборы данных имеют высокую контекстную релевантность, поскольку вопросы обычно основаны на документах, предоставленных пользователями. Напротив, синтетические длинные контексты часто напоминают задачи фактического рассуждения, где модели извлекают и проверяют знания. Такие наборы данных включают этап предварительной обработки, аналогичный конвейеру RAG. Они могут оценивать влияние расположения информации на производительность модели, например, эффект "потери в середине". С другой стороны, как реалистичные, так и синтетические длинные тексты могут лишь в некоторой степени отражать релевантность контекста. Объем контекста зависит от вопроса и его трудно определить однозначно.</p>
</li>
</ul>
<p><strong>6.2 Как сравнивать или объединять LC и RAG?</strong></p>
<p>Отсутствие чёткого определения длинного контекста также указывает на отсутствие согласованной системы для сравнения или объединения LC и RAG. Авторы предлагают такую систему, рассматривая три ключевых аспекта: длина контекста, релевантность контекста и дизайн эксперимента.</p>
<ul>
<li>
<p><strong>Длина контекста:</strong> С точки зрения моделей, это максимальное количество токенов, которое модель может обработать. С точки зрения наборов данных, это количество текста, предоставленного в контексте с вопросом. В синтетических наборах данных длина контекста является гибкой, но это создаёт компромисс между длиной и релевантностью. Добавление нерелевантной информации в качестве контекста может помочь проверить устойчивость моделей к шуму, но такое тестирование может не соответствовать реальным сценариям использования. Поэтому любая система для сравнения LC и RAG должна чётко определять, что считается длинным, и указывать, исходит ли этот критерий длины из возможностей моделей, структуры набора данных или и того, и другого.</p>
</li>
<li>
<p><strong>Релевантность контекста:</strong> Система оценки должна также учитывать релевантность текста, предоставляемого в качестве входных данных для модели. Крайне важно различать реалистичные длинные контексты и синтетические длинные контексты. Если тесты включают оба типа, необходимы отдельные оценки, поскольку синтетические контексты часто имеют низкую релевантность и могут неточно отражать реальные сценарии. Интересно, что построение синтетических длинных контекстов часто имитирует конвейеры RAG. Предоставление LLM целого кураторского текста в качестве контекста по сути представляет собой подход RAG с длинным контекстом, учитывая, что такой текст собирается во время создания набора данных. Дальнейшее разделение на части может создать предвзятость против RAG, нарушая непрерывность информации внутри каждой части. Кроме того, многие тесты классифицируют задачи как однодокументные или многодокументные в зависимости от того, исходит ли текст из одного источника или из нескольких документов. Хотя это удобно, эта классификация не полностью соответствует реалистичным или синтетическим контекстам. Один документ иногда может быть искусственно составлен из более мелких фрагментов, в то время как многоисточниковый документ может включать в себя очень релевантные источники, такие как группа научных работ, обсуждающих одну и ту же проблему. Ключевым вопросом остается определение того, в какой степени контекст, предоставляемый в качестве входных данных для LLM, содержит достаточное и релевантное содержание для ответа на вопрос, не вводя ненужной или несвязанной информации.</p>
</li>
<li>
<p><strong>Настройки эксперимента:</strong> При исследовании LC и RAG экспериментальные цели можно разделить на две категории: сравнение и объединение.</p>
<ul>
<li><strong>Короткий RAG против длинного единичного ввода:</strong> Можно сравнить конвейер RAG с коротким контекстом с настройкой длинного контекста с единичным вводом, анализируя как производительность, так и вычислительные затраты. Это даёт понимание компромисса между запуском дополнительного конвейера извлечения для более коротких контекстов и предоставлением модели возможности обрабатывать более длинный непрерывный текст.</li>
<li><strong>Длинный RAG против длинного единичного ввода:</strong> Можно также сравнить конвейер RAG с длинным контекстом с подходом с длинным контекстом с единичным вводом. Здесь цель состоит в том, чтобы увидеть, может ли разделение на части или фильтрация более релевантного контента с помощью извлечения превзойти или дополнить полностью интегрированный подход с длинным контекстом путём усечения исключительно длинных документов. В первом случае конвейер извлечения естественным образом уменьшает количество токенов. Во втором случае длина контекста остается одинаковой для обоих методов, и единственным отличием является способ обработки текста.</li>
<li><strong>RAG при увеличении контекста:</strong> Другая возможная цель — понять, как производительность RAG изменяется с увеличением длины контекста. В этом сценарии LC относится конкретно к тому, сколько токенов может обработать модель. Эта работа может показать, насколько хорошо масштабируются конвейеры RAG, когда модели поглощают всё более крупные входные данные.</li>
</ul>
</li>
</ul>
<p>С другой стороны, результаты оценок часто служат ориентиром для настроек, решающих реальные проблемы. В этом смысле RAG и LC могут дополнять друг друга в реальных условиях, в зависимости от характеристик источника данных и типов вопросов, на которые нужно ответить.</p>
<p><strong>6.3 Пересмотр всех исследований</strong></p>
<p>Основываясь на предыдущем обсуждении, изучение методов LC и RAG в LLM выявляет некоторые важные проблемы, которые исследователи часто упускают из виду.</p>
<ul>
<li>
<p><strong>Компромисс между длиной контекста и релевантностью:</strong> Многие исследования колеблются между использованием гибкого синтетического контекста с зашумленными объединенными контекстами или реалистичного контекста с плотной информацией, но меньшей доступностью. Из 9 исследований 6 выбирают синтетический контекст в качестве части наборов данных. Собственная оценка также выбрала наборы данных с синтетическим контекстом, но авторы учитывают влияние синтетического длинного контекста и отдельно оценивают их результаты по источнику контекста; например, источник Википедии с добавленным вручную шумом представляет низкую релевантность контекста. Несколько исследований пытались решить эту проблему. LongBench недавно обновил v2, который собирает только реалистичные данные. Несмотря на меньший масштаб, LongBench v2 показывает значительное улучшение релевантности контекста по сравнению с первой версией. LongRAG извлекает данные из массивного корпуса для всех вопросов, а не назначает один контекст каждому вопросу. Этот метод позволяет избежать извлечения из синтетического длинного контекста и поэтому рекомендуется.</p>
</li>
<li>
<p><strong>Разнообразие механизмов извлечения:</strong> При сравнении RAG и LC, RAG часто недопредставлен из-за чрезмерной зависимости от традиционных стратегий извлечения. Из 9 исследований 5 экспериментируют с различными извлекателями, только 2 пробуют разные размеры фрагментов, и ни одно не рассматривает какой-либо метод извлечения, выходящий за рамки извлекателей на основе фрагментов. Хотя авторы экспериментируют с извлекателями на основе индексов и на основе суммирования, они не могут обещать, что выбранный ими метод превосходит все стратегии извлечения. Для исследования производительности RAG при увеличении контекста некоторые исследования предлагают свои собственные стратегии разделения на части и размещения RAG. OP-RAG предлагает сохранять исходный порядок фрагментов из контекста, в то время как LC LLM-RAG предлагает размещать фрагменты с более высокими оценками в начале и конце. В дополнение к более продвинутым извлекателям, некоторые методы извлечения информации (IR), такие как обратная связь по релевантности или расширение запросов, могут ещё больше повысить производительность RAG, но они были упущены из виду в существующих системах.</p>
</li>
<li>
<p><strong>Вычислительные затраты:</strong> Большинство существующих исследований проводят тесты на 6-8 наборах данных, и становится всё более дорогостоящим проводить эксперименты на слишком большом количестве моделей. Это особенно актуально, когда новые LLM с длинным контекстом выпускаются очень быстро. Следовательно, любая работа может быть поставлена под сомнение, поскольку результаты эксперимента применимы только к одной или нескольким моделям. Среди всех работ LC RAG Performance включает наибольшее количество моделей (20). Хотя их усилия замечательны, они... (далее в тексте).</p>
</li>
</ul></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Conclusion</div>
                <div class="summary_text"><p>В этой статье мы проводим обзор исследований, сравнивающих или объединяющих методы Long Context (LC) и Retrieval-Augmented Generation (RAG). Мы анализируем причины, по которым разные реализации этих методов могут приводить к противоречивым выводам. Для этого мы проводим тщательное сравнение LC и RAG на разнообразных наборах данных для QA (вопросно-ответных систем) с длинным контекстом. </p>
<p>Чтобы обеспечить честное сравнение, мы отфильтровали вопросы, на которые можно ответить, используя только "параметрические знания" (то есть знания, уже содержащиеся в модели). Мы сосредоточились на вопросах, требующих обращения к внешнему контексту. Для этого мы разработали систематический процесс фильтрации и оценки, определили лучший метод извлечения информации и расширили набор данных, чтобы обеспечить статистически значимую основу для анализа.</p>
<p>Результаты показывают, что LC в целом превосходит RAG в задачах, где контекст хорошо структурирован и плотный (например, статьи из Википедии и книги). LC лучше справляется с ответами на вопросы, требующие конкретной информации. Напротив, RAG демонстрирует преимущества при работе с фрагментированной информацией, особенно в диалоговых сценариях и для более общих вопросов.</p>
<p>Помимо представления экспериментальных результатов, мы углубляемся в концепцию длинного контекста и рассматриваем, как следует сравнивать LC и RAG. Цель нашего обсуждения – сделать полученные выводы более значимыми и применимыми к реальным сценариям.</p>
<p><strong>Комментарий:</strong>
*   <strong>Long Context (LC)</strong> относится к моделям, способным обрабатывать очень длинные последовательности текста.
*   <strong>Retrieval-Augmented Generation (RAG)</strong> – это подход, при котором модель сначала извлекает релевантную информацию из внешнего источника, а затем использует эту информацию для генерации ответа.
*   <strong>Параметрические знания</strong> – это знания, которые модель "выучила" в процессе обучения и хранит в своих параметрах.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-13 12:48',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-13 12:48')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-13 12:48')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    