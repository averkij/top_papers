
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. December 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 –¥–µ–∫–∞–±—Ä—è</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-20.html">‚¨ÖÔ∏è <span id="prev-date">20.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-24.html">‚û°Ô∏è <span id="next-date">24.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 23', 'zh': '12Êúà23Êó•'};
        let feedDateNext = {'ru': '24.12', 'en': '12/24', 'zh': '12Êúà24Êó•'};
        let feedDatePrev = {'ru': '20.12', 'en': '12/20', 'zh': '12Êúà20Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2412.15115', 'title': 'Qwen2.5 Technical Report', 'url': 'https://arxiv.org/abs/2412.15115', 'abstract': 'In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-19', 'pub_date_card': {'ru': '19 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 19', 'zh': '12Êúà19Êó•'}, 'hash': 'f656de775add8c33', 'authors': ['Qwen', ':', 'An Yang', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chengyuan Li', 'Dayiheng Liu', 'Fei Huang', 'Haoran Wei', 'Huan Lin', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keming Lu', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Mei Li', 'Mingfeng Xue', 'Pei Zhang', 'Qin Zhu', 'Rui Men', 'Runji Lin', 'Tianhao Li', 'Tingyu Xia', 'Xingzhang Ren', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zeyu Cui', 'Zhenru Zhang', 'Zihan Qiu'], 'affiliations': ['Alibaba Cloud Model Studio', 'Hugging Face Hub', 'Kaggle', 'ModelScope'], 'pdf_title_img': 'assets\\pdf\\title_img\\2412.15115.jpg', 'data': {'categories': ['#benchmark', '#training', '#reasoning', '#alignment', '#multimodal', '#architecture', '#agi', '#dataset', '#optimization', '#open_source'], 'emoji': 'üß†', 'ru': {'title': 'Qwen2.5: –ù–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª–∏ –ø—Ä–æ—à–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω—ã —Ç–µ—Ö–Ω–∏–∫–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. Qwen2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–º–∏ –µ–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É.'}, 'en': {'title': 'Qwen2.5: Elevating Language Models with Unmatched Scale and Precision', 'desc': "Qwen2.5 is a new series of large language models (LLMs) that have been enhanced through extensive pre-training and post-training processes. The pre-training phase utilized a massive dataset of 18 trillion tokens, significantly improving the model's common sense and reasoning abilities. In the post-training phase, advanced techniques like supervised finetuning and reinforcement learning were applied to refine the model's performance on tasks such as long text generation and instruction following. Qwen2.5 models are available in various sizes and configurations, demonstrating top performance across multiple benchmarks and applications, including specialized models for math and coding."}, 'zh': {'title': 'Qwen2.5ÔºöÊª°Ë∂≥Â§öÊ†∑ÂåñÈúÄÊ±ÇÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜQwen2.5ÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁ≥ªÂàóÔºåÊó®Âú®Êª°Ë∂≥Â§öÊ†∑ÂåñÁöÑÈúÄÊ±Ç„ÄÇ‰∏é‰πãÂâçÁöÑÁâàÊú¨Áõ∏ÊØîÔºåQwen2.5Âú®È¢ÑËÆ≠ÁªÉÂíåÂêéËÆ≠ÁªÉÈò∂ÊÆµÈÉΩÊúâÊòæËëóÊîπËøõÔºåÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰ªé7‰∏á‰∫ø‰∏™Ê†áËÆ∞Êâ©Â±ïÂà∞18‰∏á‰∫ø‰∏™Ê†áËÆ∞Ôºå‰∏∫Â∏∏ËØÜ„ÄÅ‰∏ìÂÆ∂Áü•ËØÜÂíåÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇÂêéËÆ≠ÁªÉÊñπÈù¢ÔºåÈááÁî®‰∫ÜË∂ÖËøá100‰∏áÊ†∑Êú¨ÁöÑÂ§çÊùÇÁõëÁù£ÂæÆË∞ÉÂíåÂ§öÈò∂ÊÆµÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæËëóÊèêÂçá‰∫Ü‰∫∫Á±ªÂÅèÂ•ΩÂíåÈïøÊñáÊú¨ÁîüÊàêËÉΩÂäõ„ÄÇQwen2.5Âú®ËØ≠Ë®ÄÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅÊï∞Â≠¶„ÄÅÁºñÁ†ÅÁ≠âÂ§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂ÊóóËà∞Ê®°ÂûãQwen2.5-72B-InstructÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜËÆ∏Â§öÂºÄÊîæÂíå‰∏ìÊúâÊ®°Âûã„ÄÇ'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.', 'summary': '<p>–í —ç—Ç–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Qwen2.5, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è —Å–æ–±–æ–π —Å–µ—Ä–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, Qwen 2.5 –±—ã–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–∞ –∫–∞–∫ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏.</p>\n<p>–í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ–±—ä–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —É–≤–µ–ª–∏—á–µ–Ω —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—á–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>\n<p>–ù–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–µ –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–ª—É—á—à–∞—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∑–∞–º–µ—Ç–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–µ—Ä–∏—è LLM Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—é—Ç –±–∞–∑–æ–≤—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã, –∞ —Ç–∞–∫–∂–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE): Qwen2.5-Turbo –∏ Qwen2.5-Plus, –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤ Alibaba Cloud Model Studio.</p>\n<p>Qwen2.5 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —à–∏—Ä–æ–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ç–µ—Å—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ —Ç.–¥. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Qwen2.5-72B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä—è–¥ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo –∏ Qwen2.5-Plus –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–∏ —ç—Ç–æ–º –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å –º–æ–¥–µ–ª—è–º–∏ GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª–∏ Qwen2.5 —Å–ª—É–∂–∞—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Qwen2.5-Math, Qwen2.5-Coder, QwQ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>'}, {'title': 'Introduction', 'content': 'The sparks of artificial general intelligence (AGI) are increasingly visible through the fast development of large foundation models, notably large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; 2024a; Gemini Team, 2024; Anthropic, 2023a;b; 2024; Bai et al., 2023; Yang et al., 2024a; Touvron et al., 2023a;b; Dubey et al., 2024). The continuous advancement in model and data scaling, combined with the paradigm of large-scale pre-training followed by high-quality supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), has enabled large language models (LLMs) to develop emergent capabilities in language understanding, generation, and reasoning. Building on this foundation, recent breakthroughs in inference time scaling, particularly demonstrated by o1 (OpenAI, 2024b), have enhanced LLMs capacity for deep thinking through step-by-step reasoning and reflection. These developments have elevated the potential of language models, suggesting they may achieve significant breakthroughs in scientific exploration as they continue to demonstrate emergent capabilities indicative of more general artificial intelligence. Besides the fast development of model capabilities, the recent two years have witnessed burst of open (open-weight) large language models in the LLM community, for example, the Llama series (Touvron et al., 2023a;b; Dubey et al., 2024), Mistral series (Jiang et al., 2023a; 2024), and our Qwen series (Bai et al., 2023; Yang et al., 2024a; Qwen Team, 2024a; Hui et al., 2024; Qwen Team, 2024c; Yang et al., 2024b). The open-weight models have democratized the access of large language models to common users and developers, enabling broader research participation, fostering innovation through community collaboration, and accelerating the development of AI applications across diverse domains. Recently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the openweight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized models in different precisions. Specifically, the flagship model Qwen2.5-72B-Instruct demonstrates competitive performance against the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Additionally, we also release the proprietary models of Mixture-of-Experts (MoE, Lepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022), namely Qwen2.5-Turbo and Qwen2.5-Plus1, which performs competitively against GPT-4o-mini and GPT-4o respectively. In this technical report, we introduce Qwen2.5, the result of our continuous endeavor to create better LLMs. Below, we show the key features of the latest version of Qwen: Better in Size: Compared with Qwen2, in addition to 0.5B, 1.5B, 7B, and 72B models, Qwen2.5 brings back the 3B, 14B, and 32B models, which are more cost-effective for resource-limited scenarios and are under-represented in the current field of open foundation models. Qwen2.5Turbo and Qwen2.5-Plus offer great balance among accuracy, latency, and cost. Better in Data: The pre-training and post-training data have been improved significantly. The pre-training data increased from 7 trillion tokens to 18 trillion tokens, with focus on knowledge, coding, and mathematics. The pre-training is staged to allow transitions among different mixtures. The post-training data amounts to 1 million examples, across the stage of supervised finetuning (SFT, Ouyang et al., 2022), direct preference optimization (DPO, Rafailov et al., 2023), and group relative policy optimization (GRPO, Shao et al., 2024). Better in Use: Several key limitations of Qwen2 in use have been eliminated, including larger generation length (from 2K tokens to 8K tokens), better support for structured input and output, (e.g., tables and JSON), and easier tool use. In addition, Qwen2.5-Turbo supports context length of up to 1 million tokens.', 'summary': '<p>–†–∞–∑–≤–∏—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –Ω–∞—Å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (AGI). –£—Å–ø–µ—Ö–∏ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π (RLHF) –ø–æ–∑–≤–æ–ª–∏–ª–∏ LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ù–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ o1, —É–ª—É—á—à–∏–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å LLM –∫ –≥–ª—É–±–æ–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é. –≠—Ç–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–æ–ª–µ–µ –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.</p>\n<p>–ü–æ–º–∏–º–æ —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ –≥–æ–¥–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ—Å—Ç —á–∏—Å–ª–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö (open-weight) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama, Mistral –∏ Qwen. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ —Å–¥–µ–ª–∞–ª–∏ LLM –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ –∫—Ä—É–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º—É —É—á–∞—Å—Ç–∏—é –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, —Å—Ç–∏–º—É–ª–∏—Ä—É—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∑–∞ —Å—á–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏ —É—Å–∫–æ—Ä—è—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p>–ù–µ–¥–∞–≤–Ω–æ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–∏–∏ Qwen ‚Äî Qwen2.5. –í —Ä–∞–º–∫–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ 7 —Ä–∞–∑–º–µ—Ä–æ–≤ (0.5B, 1.5B, 3B, 7B, 14B, 32B –∏ 72B), –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ bfloat16, —Ç–∞–∫ –∏ –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö. –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ Mixture-of-Experts (MoE) ‚Äî Qwen2.5-Turbo –∏ Qwen2.5-Plus1, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç —Å GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>\n<p>–í —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Qwen2.5, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —É—Å–∏–ª–∏–π –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É–ª—É—á—à–µ–Ω–Ω—ã—Ö LLM. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ Qwen:</p>\n<ul>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö:</strong> –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –≤ Qwen2.5, –ø–æ–º–∏–º–æ –º–æ–¥–µ–ª–µ–π 0.5B, 1.5B, 7B –∏ 72B, –≤–æ–∑–≤—Ä–∞—â–µ–Ω—ã –º–æ–¥–µ–ª–∏ 3B, 14B –∏ 32B, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. Qwen2.5-Turbo –∏ Qwen2.5-Plus –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é, –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å—é.</li>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö:</strong> –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É–≤–µ–ª–∏—á–∏–ª—Å—è —Å 7 –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∑–Ω–∞–Ω–∏—è, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ, —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –º–∏–ª–ª–∏–æ–Ω –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —ç—Ç–∞–ø—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º (SFT), –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (DPO) –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO).</li>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏:</strong> –£—Å—Ç—Ä–∞–Ω–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Qwen2, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–Ω—É—é –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å 2K –¥–æ 8K —Ç–æ–∫–µ–Ω–æ–≤), —É–ª—É—á—à–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞–±–ª–∏—Ü –∏ JSON) –∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Turbo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –¥–ª–∏–Ω—É –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤.</li>\n</ul>'}, {'title': 'Architecture & Tokenizer', 'content': 'Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus. Below, we provide details about the architecture of models. For dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford et al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components: Grouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation function (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su 1Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx (to be released) in the API. 2 Table 1: Model architecture and license of Qwen2.5 open-weight models. Models Layers Heads (Q / KV) Tie Embedding Context / Generation Length License 0.5B 1.5B 3B 7B 14B 32B 72B 24 28 36 28 48 64 14 / 2 12 / 2 16 / 2 28 / 4 40 / 8 40 / 8 64 / 8 Yes Yes Yes No No No No 32K / 8K 32K / 8K 32K / 8K 128K / 8K 128K / 8K 128K / 8K 128K / 8K Apache 2.0 Apache 2.0 Qwen Research Apache 2.0 Apache 2.0 Apache 2.0 Qwen et al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and RMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training. Building upon the dense model architectures, we extend it to MoE model architectures. This is achieved by replacing standard feed-forward network (FFN) layers with specialized MoE layers, where each layer comprises multiple FFN experts and routing mechanism that dispatches tokens to the top-K experts. Following the approaches demonstrated in Qwen1.5-MoE (Yang et al., 2024a), we implement fine-grained expert segmentation (Dai et al., 2024) and shared experts routing (Rajbhandari et al., 2022; Dai et al., 2024). These architectural innovations have yielded substantial improvements in model performance across downstream tasks. For tokenization, we utilize Qwens tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with vocabulary of 151,643 regular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen versions, adding two new tokens for tool functionality and allocating the remainder for other model capabilities. This expansion establishes unified vocabulary across all Qwen2.5 models, enhancing consistency and reducing potential compatibility issues.', 'summary': '<p>–°–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen2.5 –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º (Qwen2.5-0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B) –∏ –º–æ–¥–µ–ª–∏ MoE (Mixture of Experts) –¥–ª—è API-—Å–µ—Ä–≤–∏—Å–æ–≤ (Qwen2.5-Turbo –∏ Qwen2.5-Plus).</p>\n<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π:</strong></p>\n<ul>\n<li><strong>–ü–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏:</strong><ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–µ–∫–æ–¥–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer (–∫–∞–∫ –∏ –≤ Qwen2).</li>\n<li>–í–∫–ª—é—á–∞—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:<ul>\n<li><strong>Grouped Query Attention (GQA):</strong> –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV-–∫—ç—à–∞ (—Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π).</li>\n<li><strong>–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ SwiGLU:</strong> –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.</li>\n<li><strong>Rotary Positional Embeddings (RoPE):</strong> –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>\n<li><strong>QKV bias:</strong> —Å–º–µ—â–µ–Ω–∏–µ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è.</li>\n<li><strong>RMSNorm —Å –ø—Ä–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π:</strong> –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>–ú–æ–¥–µ–ª–∏ MoE:</strong><ul>\n<li>–ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</li>\n<li>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–ª–æ–∏ FFN (feed-forward network) –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–∏ MoE.</li>\n<li>–°–ª–æ–π MoE —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö FFN-—ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –∫ top-K —ç–∫—Å–ø–µ—Ä—Ç–∞–º.</li>\n<li>–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ç–æ—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –æ–±—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (–∫–∞–∫ –≤ Qwen1.5-MoE).</li>\n</ul>\n</li>\n</ul>\n<p><strong>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Qwen, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç byte-level byte-pair encoding (BBPE) —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º –∏–∑ 151 643 –æ–±—ã—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.</li>\n<li>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 22 (–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Qwen).<ul>\n<li>–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.</li>\n<li>–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤—ã–¥–µ–ª–µ–Ω—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏.</li>\n</ul>\n</li>\n<li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5 –ø–æ–≤—ã—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.</li>\n</ul>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>\n* GQA –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–Ω–∏–º–∞–Ω–∏—è.\n* MoE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –±—ã—Ç—å –±–æ–ª–µ–µ –µ–º–∫–∏–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ.\n* –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏.</p>'}, {'title': 'Pre-training Data', 'content': 'Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor Qwen2. These improvements stem from several key aspects: (1) Better data filtering. High-quality pre-training data is crucial for model performance, making data quality assessment and filtering critical component of our pipeline. We leverage Qwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional analysis to evaluate and score training samples. The filtering method represents significant advancement over our previous approach used for Qwen2, as it benefits from Qwen2s expanded pre-training on larger multilingual corpus. The enhanced capabilities enable more nuanced quality assessment, resulting in both improved retention of high-quality training data and more effective filtering of low-quality samples across multiple languages. (2) Better math and code data. During the pre-training phase of Qwen2.5, we incorporate training data from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024). This data integration strategy proves highly effective, as these specialized datasets are instrumental in achieving state-of-the-art performance on mathematical and coding tasks. By leveraging these high-quality domain-specific datasets during pre-training, Qwen2.5 inherits strong capabilities in both mathematical reasoning and code generation. (3) Better synthetic data. To generate high-quality synthetic data, particularly in mathematics, code, and knowledge domains, we leverage both Qwen2-72B-Instruct (Yang et al., 2024a) and Qwen2Math-72B-Instruct (Qwen Team, 2024c). The quality of this synthesized data is further enhanced through rigorous filtering using our proprietary general reward model and the specialized Qwen2-Math-RM-72B (Qwen Team, 2024c) model. 3 (4) Better data mixture. To optimize the pre-training data distribution, we employ Qwen2-Instruct models to classify and balance content across different domains. Our analysis revealed that domains like e-commerce, social media, and entertainment are significantly overrepresented in web-scale data, often containing repetitive, template-based, or machine-generated content. Conversely, domains such as technology, science, and academic research, while containing higherquality information, are traditionally underrepresented. Through strategic down-sampling of overrepresented domains and up-sampling of high-value domains, we ensure more balanced and information-rich training dataset that better serves our models learning objectives. Building on these techniques, we have developed larger and higher-quality pre-training dataset, expanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens.', 'summary': '<p>–í Qwen2.5 –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∑–∞ —Å—á–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤:</p>\n<ol>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:</strong> –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í Qwen2.5 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-Instruct. –û–Ω–∏ –ø—Ä–æ–≤–æ–¥—è—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç –∏–º –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —à–∞–≥–æ–º –≤–ø–µ—Ä–µ–¥ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–¥—Ö–æ–¥–æ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–º—Å—è –≤ Qwen2, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Qwen2 –Ω–∞ –±–æ–ª—å—à–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –ª—É—á—à–µ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –ø—Ä–∏—á–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –∫–æ–¥–∞:</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ Qwen2.5-Math –∏ Qwen2.5-Coder. –≠—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–æ—Å—Ç–∏—á—å –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–∏—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, Qwen2.5 –ø–æ–ª—É—á–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:</strong> –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç—è—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –∫–æ–¥–∞ –∏ –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-72B-Instruct –∏ Qwen2Math-72B-Instruct. –ö–∞—á–µ—Å—Ç–≤–æ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen2-Math-RM-72B.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:</strong> –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª–∏ Qwen2-Instruct –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Ç–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏, –∫–∞–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è, —á—Ä–µ–∑–º–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–µ–±-–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è, —à–∞–±–ª–æ–Ω–Ω—ã–π –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—à–∏–Ω–∞–º–∏ –∫–æ–Ω—Ç–µ–Ω—Ç. –ù–∞–ø—Ä–æ—Ç–∏–≤, –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –Ω–∞—É–∫–∞ –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã. –ó–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–µ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏–∑ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, —Å–æ–∑–¥–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª—è–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>\n</li>\n</ol>\n<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π, –±—ã–ª —Å–æ–∑–¥–∞–Ω –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Ä–æ—Å —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ Qwen2, –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.</p>'}, {'title': 'Scaling Law for Hyper-parameters', 'content': 'We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al., 2022; Kaplan et al., 2020). While previous studies (Dubey et al., 2024; Almazrouei et al., 2023; Hoffmann et al., 2022) primarily used scaling laws to determine optimal model sizes given compute budgets, we leverage them to identify optimal hyperparameters across model architectures. Specifically, our scaling laws help determine key training parameters like batch size and learning rate ¬µ for both dense models and MoE models of varying sizes. Through extensive experimentation, we systematically study the relationship between model architecture and optimal training hyper-parameters. Specifically, we analyze how the optimal learning rate ¬µopt and batch size Bopt vary with model size and pre-training data size D. Our experiments cover comprehensive range of architectures, including dense models with 44M to 14B parameters and MoE models with 44M to 1B activated parameters, trained on datasets ranging from 0.8B to 600B tokens. Using these optimal hyper-parameter predictions, we then model the final loss as function of model architecture and training data scale. Additionally, we leverage scaling laws to predict and compare the performance of MoE models with varying parameter counts against their dense counterparts. This analysis guides our hyper-parameter configuration for MoE models, enabling us to achieve performance parity with specific dense model variants (such as Qwen2.5-72B and Qwen2.5-14B) through careful tuning of both activated and total parameters.', 'summary': '<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö, –∑–¥–µ—Å—å –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π.</p>\n<p>–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–º–æ–≥–∞—é—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch size) –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) ¬µ, –∫–∞–∫ –¥–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫ –∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE (Mixture of Experts) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. –ü—É—Ç–µ–º –æ–±—à–∏—Ä–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞–µ—Ç—Å—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è ¬µopt –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ Bopt –º–µ–Ω—è—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è D.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∫–ª—é—á–∞—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–æ–¥–µ–ª–∏ MoE —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 1 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç 0,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 600 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –º–∞—Å—à—Ç–∞–±–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π MoE —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Ö –ø–ª–æ—Ç–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ø–∞—Ä–∏—Ç–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ Qwen2.5-72B –∏ Qwen2.5-14B) –∑–∞ —Å—á–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∫ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –æ–±—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>'}, {'title': 'Long-context Pre-training', 'content': 'For optimal training efficiency, Qwen2.5 employs two-phase pre-training approach: an initial phase with 4,096-token context length, followed by an extension phase for longer sequences. Following the strategy used in Qwen2, we extend the context length from 4,096 to 32,768 tokens during the final pre-training stage for all model variants except Qwen2.5-Turbo. Concurrently, we increase the base frequency of RoPEfrom 10,000 to 1,000,000 using the ABF technique (Xiong et al., 2023). For Qwen2.5-Turbo, we implement progressive context length expansion strategy during training, advancing through four stages: 32,768 tokens, 65,536 tokens, 131,072 tokens, and ultimately 262,144 tokens, with RoPE base frequency of 10,000,000. At each stage, we carefully curate the training data to include 40% sequences at the current maximum length and 60% shorter sequences. This progressive training methodology enables smooth adaptation to increasing context lengths while maintaining the models ability to effectively process and generalize across sequences of varying lengths. To enhance our models ability to process longer sequences during inference, we implement two key strategies: YARN (Peng et al., 2023) and Dual Chunk Attention (DCA, An et al., 2024). Through these innovations, we achieve four-fold increase in sequence length capacity, enabling Qwen2.5-Turbo to handle up to 1 million tokens and other models to process up to 131,072 tokens. Notably, these approaches not only improve the modeling of long sequences by reducing perplexity but also maintain the models strong performance on shorter sequences, ensuring consistent quality across varying input lengths.', 'summary': '<p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 4096 —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º, –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è.</p>\n<p>–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –≤ Qwen2, –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏, –∫—Ä–æ–º–µ Qwen2.5-Turbo, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 4096 –¥–æ 32768 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º, –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 10 000 –¥–æ 1 000 000 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ ABF.</p>\n<p>–î–ª—è Qwen2.5-Turbo –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞: 32 768 —Ç–æ–∫–µ–Ω–æ–≤, 65 536 —Ç–æ–∫–µ–Ω–æ–≤, 131 072 —Ç–æ–∫–µ–Ω–æ–≤ –∏, –Ω–∞–∫–æ–Ω–µ—Ü, 262 144 —Ç–æ–∫–µ–Ω–æ–≤. –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 10 000 000. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è, –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã 40% –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–º–µ–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞, –∞ 60% –±—ã–ª–∏ –∫–æ—Ä–æ—á–µ. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø–ª–∞–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –æ–±–æ–±—â–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã.</p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: YARN –∏ Dual Chunk Attention (DCA). –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è–º, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å, —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞–∑–∞. Qwen2.5-Turbo —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∞ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ ‚Äì –¥–æ 131 072 —Ç–æ–∫–µ–Ω–æ–≤. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–∞—é—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ —Å—á–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏, –Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—Ö–æ–¥–∞—Ö —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã.</p>'}, {'title': 'Post-training', 'content': 'Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages massive dataset comprising millions of high-quality examples. This expansion specifically addresses key areas where the previous model showed limitations, such as long-sequence 4 generation, mathematical problem-solving, coding, instruction-following, structured data understanding, logical reasoning, cross-lingual transfer, and robust system instruction. (2) Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL. Offline RL: This stage focuses on developing capabilities that are challenging for the reward model to evaluate, such as reasoning, factuality, and instruction-following. Through meticulous construction and validation of training data, we ensure that the Offline RL signals are both learnable and reliable (Xiang et al., 2024), enabling the model to acquire those complex skills effectively. Online RL: The Online RL phase leverages the reward models ability to detect nuances in output quality, including truthfulness, helpfulness, conciseness, relevance, harmlessness and debiasing. It enables the model to generate responses that are precise, coherent, and well-structured while maintaining safety and readability. As result, the models outputs consistently meet human quality standards and expectations.', 'summary': '<p>–í Qwen 2.5 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen 2 –≤–Ω–µ—Å–µ–Ω—ã –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏:</p>\n<ol>\n<li>\n<p><strong>–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-tuning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –º–∏–ª–ª–∏–æ–Ω—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –ø–µ—Ä–µ–Ω–æ—Å –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–∏—Å—Ç–µ–º–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –¢–æ –µ—Å—Ç—å, –º–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º —Å–ø–µ–∫—Ç—Ä–æ–º –∑–∞–¥–∞—á –∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ —Å–ª—É—á–∞—è–º–∏.</p>\n</li>\n<li>\n<p><strong>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ Qwen 2.5 —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞: –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Offline RL) –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Online RL).</p>\n<ul>\n<li><strong>–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —Å–∏–≥–Ω–∞–ª—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –æ–±—É—á–∞–µ–º—ã–º–∏, —Ç–∞–∫ –∏ –Ω–∞–¥–µ–∂–Ω—ã–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—Ç—å —ç—Ç–∏ —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–≤—ã–∫–∏.<ul>\n<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: —Ç—É—Ç –≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞—Ä–∞–Ω–µ–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</em></li>\n</ul>\n</li>\n<li><strong>–û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –Ω—é–∞–Ω—Å—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—ã–≤–æ–¥–∞, –≤–∫–ª—é—á–∞—è –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –∫—Ä–∞—Ç–∫–æ—Å—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏, —Å–≤—è–∑–Ω—ã–º–∏ –∏ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –≤—ã–≤–æ–¥—ã –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞.<ul>\n<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ª–µ—Ç—É.</em></li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Qwen 2.5 —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –Ω–∞–¥–µ–∂–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º.</p>'}, {'title': 'Supervised Fine-tuning', 'content': 'In this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on several critical areas: (1) Long-sequence Generation: Qwen2.5 is capable of generating high-quality content with an output context length of up to 8,192 tokens, significant advancement over the typical posttraining response length, which often remains under 2,000 tokens. To address this gap, we develop long-response datasets (Quan et al., 2024). We employ back-translation techniques to generate queries for long-text data from pre-training corpora, impose output length constraints, and use Qwen2 to filter out low-quality paired data. (2) Mathematics: We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b), which encompasses diverse range of query sources, including public datasets, K-12 problem collections, and synthetic problems. To ensure high-quality reasoning, we employ rejection sampling (Yuan et al., 2023) along with reward modeling and annotated answers for guidance, producing step-by-step reasoning process. (3) Coding: To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5Coder (Hui et al., 2024). We use multiple language-specific agents into collaborative framework, generating diverse and high-quality instruction pairs across nearly 40 programming languages. We expand our instruction dataset by synthesizing new examples from code-related Q&A websites and gathering algorithmic code snippets from GitHub. comprehensive multilingual sandbox is used to perform static code checking and validate code snippets through automated unit testing, ensuring code quality and correctness (Dou et al., 2024; Yang et al., 2024c). (4) Instruction-following: To ensure high-quality instruction-following data, we implement rigorous code-based validation framework. In this approach, LLMs generate both instructions and corresponding verification code, along with comprehensive unit tests for cross-validation. Through execution feedback-based rejection sampling, we carefully curate the training data used for Supervised Fine-Tuning, thereby guaranteeing the models faithful adherence to intended instructions (Dong et al., 2024). (5) Structured Data Understanding: We develop comprehensive structured understanding dataset that encompasses both traditional tasks, such as tabular question-answering, fact verification, error correction, and structural understanding, as well as complex tasks involving structured and semi-structured data. By incorporating reasoning chains into the models responses, we significantly enhance its ability to infer information from structured data, thereby improving its performance across these diverse tasks. This approach not only broadens the scope of the dataset but also deepens the models capacity to reason and derive meaningful insights from complex data structures. (6) Logical Reasoning: To enhance the models logical reasoning capabilities, we introduce diverse set of 70,000 new queries spanning various domains. These queries encompass multiple-choice questions, true / false questions, and open-ended questions. The model is trained to approach problems systematically, employing range of reasoning methods such as deductive reasoning, inductive generalization, analogical reasoning, causal reasoning, and statistical reasoning. Through iterative refinement, we systematically filter out data containing incorrect answers or flawed reasoning processes. This process progressively strengthens the models ability to reason logically and accurately, ensuring robust performance across different types of reasoning tasks. 5 (7) Cross-Lingual Transfer: To facilitate the transfer of the models general capabilities across languages, we employ translation model to convert instructions from high-resource languages into various low-resource languages, thereby generating corresponding response candidates. To ensure the accuracy and consistency of these responses, we evaluate the semantic alignment between each multilingual response and its original counterpart. This process preserves the logical structure and stylistic nuances of the original responses, thereby maintaining their integrity and coherence across different languages. (8) Robust System Instruction: We construct hundreds of general system prompts to improve the diversity of system prompts in post-training, ensuring consistency between system prompts and conversations. Evaluations with different system prompts show that the model maintains good performance (Lu et al., 2024b) and reduced variance, indicating improved robustness. (9) Response Filtering: To evaluate the quality of responses, we employ multiple automatic annotation methods, including dedicated critic model and multi-agent collaborative scoring system. Responses are subjected to rigorous assessment, and only those deem flawless by all scoring systems are retained. This comprehensive approach ensures that our outputs maintain the highest quality standards. Ultimately, we construct dataset of over 1 million SFT examples. The model is fine-tuned for two epochs with sequence length of 32,768 tokens. To optimize learning, the learning rate is gradually decreased from 7 106 to 7 107. To address overfitting, we apply weight decay of 0.1, and gradient norms are clipped at maximum value of 1.0.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è, –≤–Ω–µ—Å–µ–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å Qwen2.5 –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—Å–∞—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π:</p>\n<ol>\n<li>\n<p><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:</strong> Qwen2.5 —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–∏–ø–∏—á–Ω—É—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –º–µ–Ω–µ–µ 2000 —Ç–æ–∫–µ–Ω–æ–≤). –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤, –Ω–∞–∫–ª–∞–¥—ã–≤–∞–ª–∏—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Ñ–∏–ª—å—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen2 –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –¥–∞–Ω–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> –í –º–æ–¥–µ–ª—å –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Å—Ç–∏–ª–µ "—Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π" (chain-of-thought). –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—É–±–ª–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∑–∞–¥–∞—á–∏ –∏–∑ —à–∫–æ–ª—å–Ω—ã—Ö —É—á–µ–±–Ω–∏–∫–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (rejection sampling) –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p>\n</li>\n<li>\n<p><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏–∑ Qwen2.5Coder. –ë—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–∫–æ–¥ –ø–æ—á—Ç–∏ –¥–ª—è 40 —è–∑—ã–∫–æ–≤. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Ä–∞—Å—à–∏—Ä–µ–Ω –∑–∞ —Å—á–µ—Ç —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Å–∞–π—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å–±–æ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ —Å GitHub. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞, –≤—ã–ø–æ–ª–Ω—è—é—â–∞—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥—É–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>\n</li>\n<li>\n<p><strong>–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º:</strong> –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ç—Ä–æ–≥–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞. –ú–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π –∫–æ–¥, –∞ —Ç–∞–∫–∂–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏. –ü—É—Ç–µ–º –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞, –±—ã–ª–∏ –æ—Ç–æ–±—Ä–∞–Ω—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–ª–æ —Ç–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n</li>\n<li>\n<p><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:</strong> –ë—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –∫–∞–∫ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫), —Ç–∞–∫ –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–≤—ã—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>\n</li>\n<li>\n<p><strong>–õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –∏–∑ 70 000 –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏. –≠—Ç–∏ –∑–∞–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞–ª–∏ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –≤–æ–ø—Ä–æ—Å—ã "–≤–µ—Ä–Ω–æ/–Ω–µ–≤–µ—Ä–Ω–æ" –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –ø–æ–¥—Ö–æ–¥–∏—Ç—å –∫ –ø—Ä–æ–±–ª–µ–º–∞–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∞–Ω–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ, –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–ª–∏ –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏.</p>\n</li>\n<li>\n<p><strong>–ö—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å:</strong> –î–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ —è–∑—ã–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —è–∑—ã–∫–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–ª–æ—Å—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∫–∞–∂–¥—ã–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ –µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º.</p>\n</li>\n<li>\n<p><strong>–ù–∞–¥–µ–∂–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:</strong> –ë—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã —Å–æ—Ç–Ω–∏ –æ–±—â–∏—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏. –û—Ü–µ–Ω–∫–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å.</p>\n</li>\n<li>\n<p><strong>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤:</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫–∞ –∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏. –û—Ç–≤–µ—Ç—ã –ø–æ–¥–≤–µ—Ä–≥–∞–ª–∏—Å—å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ, –∏ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏–∑–Ω–∞–Ω—ã –±–µ–∑—É–ø—Ä–µ—á–Ω—ã–º–∏ –≤—Å–µ–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –æ—Ü–µ–Ω–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å.</p>\n</li>\n</ol>\n<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–≤—É—Ö —ç–ø–æ—Ö —Å –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–ª–∞—Å—å, –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –±—ã–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.</p>'}, {'title': 'Offline Reinforcement Learning', 'content': 'Compared to Online Reinforcement Learning (RL), Offline RL enables the pre-preparation of training signals, which is particularly advantageous for tasks where standard answers exist but are challenging to evaluate using reward models. In this study, we focus on objective query domains such as mathematics, coding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex. In the previous phase, we extensively employ strategies like execution feedback and answer matching to ensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model to resample responses for new set of queries. Responses that pass our quality checks are used as positive examples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO) training (Rafailov et al., 2023). To further enhance the reliability and accuracy of the training signals, we make use of both human and automated review processes (Cao et al., 2024). This dual approach ensures that the training data is not only learnable but also aligned with human expectations. Ultimately, we construct dataset consisting of approximately 150,000 training pairs. The model is then trained for one epoch using the Online Merging Optimizer (Lu et al., 2024a), with learning rate of 7 107.', 'summary': '<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ –æ—Ç–≤–µ—Ç—ã –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í —ç—Ç–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω—ã–º.</p>\n<p>–ù–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —ç—Ç–∞–ø–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç–∞–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ —ç—Ç–æ—Ç –∂–µ –∫–æ–Ω–≤–µ–π–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ: –º–æ–¥–µ–ª—å SFT (Supervised Fine-Tuning) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—Ç—ã, –ø—Ä–æ—à–µ–¥—à–∏–µ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –∞ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É, ‚Äî –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO). </p>\n<p>–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ —Ä—É—á–Ω—ã–µ, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ø—Ä–æ–≤–µ—Ä–∫–∏. –¢–∞–∫–æ–π –¥–≤–æ–π–Ω–æ–π –ø–æ–¥—Ö–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≥–æ–¥–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞. –í –∏—Ç–æ–≥–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –ø—Ä–∏–º–µ—Ä–Ω–æ –∏–∑ 150 000 –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä.</p>\n<p>–ó–∞—Ç–µ–º –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Online Merging Optimizer —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è 7 * 10^-7.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>\n*   SFT (Supervised Fine-Tuning) - —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n*   DPO (Direct Preference Optimization) - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–µ–∂–¥—É –¥–≤—É–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–∞.\n*   Online Merging Optimizer - —ç—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.</p>'}, {'title': 'Online Reinforcement Learning', 'content': 'To develop robust reward model for online RL, we adhere to set of carefully defined labeling criteria. Those criteria ensure that the responses generated by the model are not only high-quality but also aligned with ethical and user-centric standards (Wang et al., 2024a). The specific guidelines for data labeling are as follows: Truthfulness: Responses must be grounded in factual accuracy, faithfully reflecting the provided context and instructions. The model should avoid generating information that is false or unsupported by the given data. Helpfulness: The models output should be genuinely useful, addressing the users query effectively while providing content that is positive, engaging, educational, and relevant. It should follow the given instructions precisely and offer value to the user. Conciseness: Responses should be succinct and to the point, avoiding unnecessary verbosity. The goal is to convey information clearly and efficiently without overwhelming the user with excessive detail. Relevance: All parts of the response should be directly related to the users query, dialogue history, and the assistants context. The model should tailor its output to ensure it is perfectly aligned with the users needs and expectations. Harmlessness: The model must prioritize user safety by avoiding any content that could lead to illegal, immoral, or harmful behavior. It should promote ethical conduct and responsible communication at all times. 6 Debiasing: The model should produce responses that are free from bias, including but not limited to gender, race, nationality, and politics. It should treat all topics equally and fairly, adhering to widely accepted moral and ethical standards. The queries utilized to train the reward model are drawn from two distinct datasets: publicly available open-source data and proprietary query set characterized by higher complexity. Responses are generated from checkpoints of the Qwen models, which have been fine-tuned using different methodsSFT, DPO, and RLat various stages of training. To introduce diversity, those responses are sampled at different temperature settings. Preference pairs are created through both human and automated labeling processes, and the training data for DPO is also integrated into this dataset. In our online reinforcement learning (RL) framework, we employ Group Relative Policy Optimization (GRPO, Shao et al., 2024). The query set utilized for training the reward model is identical to the one used in the RL training phase. The sequence in which queries are processed during training is determined by the variance of their response scores, as evaluated by the reward model. Specifically, queries with higher variance in response scores are prioritized to ensure more effective learning. We sample 8 responses for each query. All models are trained with 2048 global batch size and 2048 samples in each episode, considering pair of queries and responses as sample.', 'summary': '<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model) –¥–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç—ã, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é, –±—É–¥—É—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —ç—Ç–∏—á–µ—Å–∫–∏–º –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.</p>\n<p>–í–æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö:</p>\n<ul>\n<li><strong>–ü—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö –∏ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∏–∑–±–µ–≥–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω–æ–π –∏–ª–∏ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>\n<li><strong>–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å:</strong> –í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –ø–æ–ª–µ–∑–Ω—ã–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç–≤–µ—á–∞—è –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –û–Ω –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ø—Ä–∏–Ω–æ—Å–∏—Ç—å –ø–æ–ª—å–∑—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.</li>\n<li><strong>–ö—Ä–∞—Ç–∫–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º–∏ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, –∏–∑–±–µ–≥–∞—è –∏–∑–ª–∏—à–Ω–µ–π –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç–∏. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ç–∫–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏.</li>\n<li><strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:</strong> –í—Å–µ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω—ã —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –≤—ã–≤–æ–¥, —á—Ç–æ–±—ã –æ–Ω —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</li>\n<li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏–∑–±–µ–≥–∞—è –ª—é–±–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–º—É, –∞–º–æ—Ä–∞–ª—å–Ω–æ–º—É –∏–ª–∏ –≤—Ä–µ–¥–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–¥–≤–∏–≥–∞—Ç—å —ç—Ç–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö.</li>\n<li><strong>–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –≤—ã–¥–∞–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, —Å–≤–æ–±–æ–¥–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è—Å—å, –≥–µ–Ω–¥–µ—Ä–Ω—É—é, —Ä–∞—Å–æ–≤—É—é, –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫—É—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫–æ –≤—Å–µ–º —Ç–µ–º–∞–º —Ä–∞–≤–Ω–æ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—è—Å—å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã—Ö –º–æ—Ä–∞–ª—å–Ω—ã—Ö –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.</li>\n</ul>\n<p>–ó–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –≤–∑—è—Ç—ã –∏–∑ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö: –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–µ–≥–æ—Å—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é. –û—Ç–≤–µ—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –º–æ–¥–µ–ª–µ–π Qwen, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ (SFT, DPO –∏ RL) –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –≤–Ω–µ—Å–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã. –ü–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Å–æ–∑–¥–∞—é—Ç—Å—è –∫–∞–∫ —Å –ø–æ–º–æ—â—å—é —Ä—É—á–Ω–æ–π, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, –∞ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è DPO —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç—Å—è –≤ —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–í —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Group Relative Policy Optimization (GRPO). –ù–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏–¥–µ–Ω—Ç–∏—á–µ–Ω —Ç–æ–º—É, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è RL. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–ø—Ä–æ—Å—ã —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è 8 –æ—Ç–≤–µ—Ç–æ–≤. –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–∫–µ—Ç–∞ 2048 –∏ 2048 –æ–±—Ä–∞–∑—Ü–∞–º–∏ –≤ –∫–∞–∂–¥–æ–º —ç–ø–∏–∑–æ–¥–µ, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø–∞—Ä—É –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü.</p>'}, {'title': 'Long Context Fine-tuning', 'content': 'To further extend the context length of Qwen2.5-Turbo, we introduce longer SFT examples during post-training, enabling it to better align with human preference in long queries. In the SFT phase, we employ two-stage approach. In the first stage, the model is fine-tuned exclusively using short instructions, each containing up to 32,768 tokens. This stage uses the same data and training steps as those employed for the other Qwen2.5 models, ensuring strong performance on short tasks. In the second stage, the fine-tuning process combines both short instructions (up to 32,768 tokens) and long instructions (up to 262,144 tokens). This hybrid approach effectively enhances the models instruction-following ability in long context tasks while maintaining its performance on short tasks. During the RL stage, we use training strategy similar to that used for the other Qwen2.5 models, focusing solely on short instructions. This design choice is driven by two primary considerations: first, RL training is computationally expensive for long context tasks; second, there is currently scarcity of reward models that provide suitable reward signals for long context tasks. Additionally, we find that adopting RL on short instructions alone can still significantly enhance the models alignment with human preferences in long context tasks.', 'summary': '<p>–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å Qwen2.5-Turbo, –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.</p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è (SFT) —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –∏ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤), —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 262 144 —Ç–æ–∫–µ–Ω–æ–≤). –¢–∞–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.</p>\n<p>–ù–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Ç–æ–π, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏ –æ–±—É—á–µ–Ω–∏–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –≠—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ –¥–≤—É–º—è –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏—á–∏–Ω–∞–º–∏: –≤–æ-–ø–µ—Ä–≤—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º; –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –≤—Å–µ —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>'}, {'title': 'Base Models', 'content': 'We conduct comprehensive evaluations of the base language models of the Qwen2.5 series. The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities. The evaluation datasets include: General Tasks MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024b) (5-shot), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot), ARC-C (Clark et al., 2018) (25-shot), TruthfulQA (Lin et al., 2022a) (0-shot), Winogrande (Sakaguchi et al., 2021) (5-shot), HellaSwag (Zellers et al., 2019) (10-shot). 7 Table 2: Performance of the 70B+ base models and Qwen2.5-Plus. Datasets Llama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus MMLU MMLU-Pro MMLU-redux BBH ARC-C TruthfulQA WindoGrande HellaSwag GPQA TheoremQA MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 79.5 52.8 75.0 81.0 68.8 45.6 85.3 88. 36.3 32.3 42.5 73.7 77.6 48.2 42.1 70.4 58.4 46.3 70.0 79.9 67.1 38.0 General Tasks 85.2 61.6 - 85.9 - - 86.7 - 77.8 51.6 72.9 78.9 70.7 51.0 85.0 88. Mathematics & Science Tasks 34.3 35.9 41.7 71.7 83.7 46.3 40.2 71.7 58.1 46.7 63.5 77.7 62.9 23.3 - - 53.8 - 89.0 Coding Tasks 61.0 - 73.0 - - Multilingual Tasks - - - - 84.2 55.7 80.5 82.4 68.9 54.8 85.1 87.3 37.4 42.8 50.9 79.6 89.0 64.6 56.1 76.9 63.9 59. 76.6 80.7 76.0 37.8 86.1 58.1 83.9 86.3 72.4 60.4 83.9 87.6 45.9 42.4 62.1 82.7 91.5 59.1 51.2 84.7 69.2 60.5 78.7 89.6 76.7 39.0 85.4 64.0 82.8 85.8 70.9 55.3 85.5 89. 43.9 48.5 64.4 81.2 93.0 59.1 52.4 79.7 66.9 61.0 78.5 89.2 82.4 40.4 Mathematics & Science Tasks GPQA (Rein et al., 2023) (5-shot), Theorem QA (Chen et al., 2023a) (5-shot), GSM8K (Cobbe et al., 2021) (4-shot), MATH (Hendrycks et al., 2021b) (4-shot). Coding Tasks HumanEval (Chen et al., 2021) (0-shot), HumanEval+ (Liu et al., 2023)(0-shot), MBPP (Austin et al., 2021) (0-shot), MBPP+ (Liu et al., 2023) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript). Multilingual Tasks We group them into four categories: (a) Exam: M3Exam (5-shot, we only choose examples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova et al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French, Portuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar et al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023) (5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c) Mathematics: MGSM (Goyal et al., 2022) (8-shot CoT); and (d) Translation: Flores-101 (Goyal et al., 2022) (5-shot). For base models, we compare Qwen2.5 models with Qwen2 models and other leading open-weight models in terms of scales of parameters. Qwen2.5-72B & Qwen2.5-Plus We compare the base models of Qwen2.5-72B and Qwen2.5-Plus to other leading open-weight base models: Llama3-70B (Dubey et al., 2024), Llama3-405B (Dubey et al., 2024), Mixtrail-8x22B (Jiang et al., 2024), and our previous 72B version, the Qwen2-72B (Yang et al., 2024a). The Qwen2.5-72B base model significantly outperforms its peers in the same category across wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges. With significantly lower training and inference costs, Qwen2.5-Plus achieves very competitive performance results compared to Qwen2.5-72B and Llama3-405B, outperforming other baseline models on the Hellaswag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics, and Multi-Translation. Moreover, Qwen2.5-Plus achieves 64.0 on MMLU-Pro, which is 5.9 points higher than Qwen2.5-72B. Qwen2.5-14B/32B & Qwen2.5-Turbo The evaluation of the Qwen2.5-Turbo, Qwen2.5-14B, and 32B models is compared against baselines of similar sizes. These baselines include Yi-1.5-34B (Young et al., 8 Table 3: Performance of the 14B-30B+ base models and Qwen2.5-Turbo. Datasets Qwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande Hellaswag GPQA Theoremqa MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 74.3 44.1 69.0 66.8 63.6 57.4 81.5 85. 30.8 28.8 36.1 66.5 78.5 43.3 40.2 64.2 53.9 38.5 61.6 76.5 56.1 33.5 General Tasks 77.2 48.3 74.1 76.4 65.6 53.9 84.9 85.9 75.2 49.1 - 74.9 71.4 40.1 59.7 86. Mathematics & Science Tasks 34.9 35.8 42.7 71.0 81.1 54.9 46.3 75.7 60.2 48.0 65.8 82.2 61.6 38.7 37.4 40.0 41.7 72.6 81.7 Coding Tasks 46.3 40.2 65.5 55.4 39.5 Multilingual Tasks 58.3 73.9 49.3 30.0 79.5 55.6 77.1 76.1 67.8 56.3 81.1 85.0 41.4 42.1 55.6 77.0 88.3 57.3 51.2 76.2 63.0 53. 70.3 85.3 71.3 36.8 79.7 51.2 76.6 78.2 67.3 58.4 81.0 84.3 32.8 43.0 55.6 76.4 90.2 56.7 51.2 76.7 63.2 53.5 70.6 85.9 68.5 36.2 83.3 55.1 82.0 84.5 70.4 57.8 82.0 85. 48.0 44.1 57.7 80.9 92.9 58.5 52.4 84.5 67.2 59.4 75.4 88.4 73.7 37.3 Table 4: Performance of the 7B+ base models. Datasets Mistral-7B Llama3-8B Gemma2-9B Qwen2-7B Qwen2.5-7B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande HellaSwag GPQA TheoremQA MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 64.2 30.9 58.1 56.1 60.0 42.2 78.4 83.3 24.7 19.2 10.2 50.1 36. 29.3 24.4 51.1 40.9 29.4 47.1 63.3 26.3 23.3 General Tasks 66.6 35.4 61.6 57.7 59.3 44.0 77.4 82.1 71.3 44.7 67.9 68.2 68.2 45.3 79.5 81.9 Mathematics & Science Tasks 32.8 28.9 37.7 65.1 70.7 37.8 30.5 62.2 50.6 34.9 61.2 78.3 53.0 36.5 25.8 22.1 20.5 55.3 55.3 Coding Tasks 33.5 29.3 53.9 44.4 22. Multilingual Tasks 52.3 68.6 36.3 31.9 9 70.3 40.1 68.1 62.3 60.6 54.2 77.0 80.7 30.8 29.6 43.5 64.2 80.2 51.2 43.3 64.2 51.9 41. 59.2 72.0 57.5 31.5 74.2 45.0 71.1 70.4 63.7 56.4 75.9 80.2 36.4 36.0 49.8 72.3 85.4 57.9 50.6 74.9 62.9 50.3 59.4 79.3 57.8 32.4 Table 5: Performance of the smaller base models. Datasets Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande Hellaswag GPQA TheoremQA MATH MMLU-STEM GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 44.3 14.7 40.7 18.2 31.0 39.7 56.9 49.1 29.8 9.6 11.2 27.5 36.4 22.6 18.9 33.1 27.6 16.3 29.4 40.4 7.8 14.1 General Tasks 55.9 21.6 51.8 36.5 43.7 45.9 65.0 67. 47.5 15.7 45.1 20.3 35.6 40.2 56.3 52.1 Mathematics & Science Tasks 24.8 16.0 19.5 39.8 41.6 30.5 26.8 39.3 33.8 18.9 30.8 41.0 13.5 15.3 20.7 14.8 21.6 42.7 46. Coding Tasks 34.8 29.9 46.9 37.6 27.9 Multilingual Tasks 43.1 50.7 21.3 23.8 60.9 28.5 58.5 45.1 54.7 46.6 65.0 67.9 24.2 22.1 35.0 54.8 68. 37.2 32.9 60.2 49.6 33.1 47.9 65.1 37.5 25.0 52.2 23.0 50.9 41.9 55.7 36.2 71.5 74.6 25.3 15.9 18.3 45.8 30.3 19.5 15.9 42.1 33.6 17.6 38.1 46.8 18.2 26. 65.6 34.6 63.7 56.3 56.5 48.9 71.1 74.6 26.3 27.4 42.6 62.5 79.1 42.1 36.0 57.1 49.4 41.2 54.6 76.6 48.9 29.3 2024), Gemma2-27B (Gemma Team et al., 2024), and Qwen1.5-32B (Qwen Team, 2024b). The results are shown in Table 3. The Qwen2.5-14B model demonstrates solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP. For Qwen2.5-Turbo, although its training cost and inference cost are significantly smaller than those of Qwen2.5-14B, it achieves comparable results, where its MMLU-Pro score is even better than that of Qwen2.5-32B. Qwen2.5-7B For 7B-level models, we focus on comparing Qwen2.5-7B with other leading 7B+ models, including Mistral-7B (Jiang et al., 2023a), Llama3-8B (Dubey et al., 2024), Gemma2-9B (Gemma Team et al., 2024), and our predecessor, Qwen2-7B (Yang et al., 2024a). The results can be found in Table 4. Note that the non-embedding parameters of Qwen2-7B and Qwen2.5-7B are only 6.5B, while that of Gemma2-9B is 8.2B. The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU (Hendrycks et al., 2021a), 49.8 on math challenges such as MATH (Hendrycks et al., 2021b), and 57.9 on coding tasks like HumanEval (Chen et al., 2021). Qwen2.5-0.5B/1.5B/3B For edge-side models, we compare Qwen2.5-0.5B, 1.5B, and 3B against established baselines: Qwen2-0.5B/1.5B (Yang et al., 2024a) and Gemma2-2.6B (Gemma Team et al., 2024). The results are given in Table 5. Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ–¥—Ä–æ–±–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen2.5. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —É–¥–µ–ª—è–µ—Ç—Å—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –æ–±—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –Ω–∞—É—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:</p>\n<ul>\n<li><strong>–û–±—â–∏–µ –∑–∞–¥–∞—á–∏:</strong> MMLU, MMLU-Pro, MMLU-redux, BBH, ARC-C, TruthfulQA, Winogrande, HellaSwag.</li>\n<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –Ω–∞—É–∫–∞:</strong> GPQA, TheoremQA, GSM8K, MATH.</li>\n<li><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> HumanEval, HumanEval+, MBPP, MBPP+, MultiPL-E (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript).</li>\n<li><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏:</strong><ul>\n<li><strong>–≠–∫–∑–∞–º–µ–Ω:</strong> M3Exam, IndoMMLU, ruMMLU –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π MMLU (–∞—Ä–∞–±—Å–∫–∏–π, –∏—Å–ø–∞–Ω—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –Ω–µ–º–µ—Ü–∫–∏–π, –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –∫–æ—Ä–µ–π—Å–∫–∏–π).</li>\n<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ:</strong> BELEBELE, XCOPA, XWinograd, XStoryCloze –∏ PAWS-X.</li>\n<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> MGSM.</li>\n<li><strong>–ü–µ—Ä–µ–≤–æ–¥:</strong> Flores-101.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-72B –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Qwen2.5-72B –∏ Qwen2.5-Plus —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –≤–µ–¥—É—â–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏: Llama3-70B, Llama3-405B, Mixtral-8x22B –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π Qwen2-72B.</p>\n<p>Qwen2.5-72B –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Å Llama-3-405B, –ø—Ä–∏ —ç—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å –ø—è—Ç—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B, Qwen2.5-72B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>\n<p>Qwen2.5-Plus, –∏–º–µ—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—á–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2.5-72B –∏ Llama3-405B, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HellaSwag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics –∏ Multi-Translation. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 64.0 –Ω–∞ MMLU-Pro, —á—Ç–æ –Ω–∞ 5.9 –ø—É–Ω–∫—Ç–æ–≤ –≤—ã—à–µ, —á–µ–º —É Qwen2.5-72B.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-14B/32B –∏ Qwen2.5-Turbo —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo, Qwen2.5-14B –∏ 32B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Yi-1.5-34B, Gemma2-27B –∏ Qwen1.5-32B.</p>\n<p>Qwen2.5-14B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU –∏ BBH, –≥–¥–µ –æ–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. Qwen2.5-32B –æ—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è, —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –û–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen1.5-32B, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>\n<p>Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∞ –ø–æ MMLU-Pro –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-32B.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5-7B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å Mistral-7B, Llama3-8B, Gemma2-9B –∏ Qwen2-7B. Qwen2.5-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–æ –º–Ω–æ–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—é. –û–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 74.2 –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU, 49.8 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MATH, –∏ 57.9 –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ HumanEval.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-0.5B/1.5B/3B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-0.5B, 1.5B –∏ 3B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å Qwen2-0.5B/1.5B –∏ Gemma2-2.6B. Qwen2.5-0.5B, 1.5B –∏ 3B –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, Qwen2.5-0.5B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Gemma2-2.6B –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>'}, {'title': 'Instruction-tuned Model', 'content': 'To critically evaluate instruction-tuned models, we adopt multifaceted approach. Foundational skills and human preferences are assessed using open datasets and benchmarks. Additionally, our detailed in-house evaluations delve deeper into the modelscompetencies in key areas and multilingualism. particular focus is placed on assessing long-context capability. The subsequent sections outline the evaluation methods and present the results. 10 Table 6: Performance of the 70B+ Instruct models and Qwen2.5-Plus. Datasets Llama-3.1-70B Llama-3.1-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus MMLU-Pro MMLU-redux LiveBench GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 66.4 83.0 46.6 46.7 68.0 95.1 80.5 84.2 68.2 32. 83.6 55.7 8.79 General Tasks 73.3 86.2 53.2 64.4 81.6 41.5 Mathematics & Science Tasks 51.1 73.8 96. Coding Tasks 89.0 84.5 73.5 41.6 42.4 69.0 93.2 86.0 80.2 69.2 32.2 Alignment Tasks 86.0 69.3 9. 77.6 48.1 9.12 71.1 86.8 52.3 49.0 83.1 95.8 86.6 88.2 75.1 55.5 84.1 81.2 9.35 72.5 86.3 54. 49.7 84.7 96.0 87.8 85.5 77.0 51.4 86.3 81.4 9.30 Table 7: Performance of the 14B-30B+ instruction-tuned models and Qwen2.5-Turbo. Datasets Qwen2-57BA14B Gemma2-27B GPT4o-mini Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B MMLU-Pro MMLU-redux LiveBench 0831 GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 52.8 72.6 31.1 34.3 49.1 85. 79.9 70.9 66.4 22.5 59.9 17.8 8.55 General Tasks 55.5 75.7 39.6 63.1 81.5 43.3 Mathematics & Science Tasks 38.4 54.4 90.4 78.7 81.0 67.4 - 77.1 57.5 9.10 40.2 70.2 93.2 Coding Tasks 88.4 85.7 75.0 40. Alignment Tasks 80.4 74.9 - 64.5 81.7 42.3 42.3 81.1 93.8 86.6 82.8 73.7 37.8 76.3 67.1 8. 63.7 80.0 44.4 45.5 80.0 94.8 83.5 82.0 72.8 42.6 81.0 68.3 8.88 69.0 83.9 50.7 49.5 83.1 95. 88.4 84.0 75.4 51.2 79.5 74.5 9.', 'summary': '<p>–î–ª—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥. –ë–∞–∑–æ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–µ—Ç–∞–ª—å–Ω–µ–µ –∏–∑—É—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∞ —Ç–∞–∫–∂–µ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 6 –∏ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–¢–∞–±–ª–∏—Ü–∞ 6</strong> –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 70 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –±–æ–ª–µ–µ, –∞ —Ç–∞–∫–∂–µ Qwen2.5-Plus. –í —Ç–∞–±–ª–∏—Ü–µ —É–∫–∞–∑–∞–Ω—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö: MMLU-Pro, MMLU-redux, LiveBench, GPQA, MATH, GSM8K, HumanEval, MBPP, MultiPL-E, LiveCodeBench, IFEval, Arena-Hard –∏ MTbench. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–∞–¥–∞—á–∏ (General Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ (Mathematics &amp; Science Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é (Coding Tasks) –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º (Alignment Tasks).</p>\n<p><strong>–¢–∞–±–ª–∏—Ü–∞ 7</strong> –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 14-30 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ Qwen2.5-Turbo. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ –∂–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á, —á—Ç–æ –∏ –≤ —Ç–∞–±–ª–∏—Ü–µ 6.</p>\n<p>–ò–∑ —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ–±—â–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.</p>'}, {'title': 'Open Benchmark Evaluation', 'content': 'To comprehensively evaluate the quality of instruction-tuned models, we compile automatic and human evaluation to assess the capabilities and human preference. For the evaluation of basic capabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural language understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU-Pro, MMLU-redux and LiveBench 0831 (White et al., 2024) for general evaluation, GPQA, GSM8K and MATH for science and mathematics, HumanEval, MBPP, MultiPL-E and LiveCodeBench 2305-2409 (Jain et al., 2024) for coding, IFEval (Zhou et al., 2023)2 for instruction following. Additionally, we assess the performance of human preference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng et al., 2023) and Arena-Hard (Li et al., 2024). Qwen2.5-72B-Instruct & Qwen2.5-Plus As shown in Table 6, we compare Qwen2.5-72B-Instruct and Qwen2.5-Plus to other leading open-weight instrution-tuned models: Llama3.1-70B-Instruct (Dubey 2For simplicity, we report the results of the subset strict-prompt. 11 Table 8: Performance of the 7B+ instruction-tuned models. Datasets Gemma2-9B Llama3.1-8B Qwen2-7B Qwen2.5-7B General Tasks MMLU-Pro MMLU-redux LiveBench 0831 52.1 72.8 30.6 48.3 67.2 26.7 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 32.8 44.3 76.7 68.9 74.9 53.4 18.9 70.1 41.6 8. 32.8 51.9 84.5 Coding Tasks 72.6 69.6 50.7 8.3 Alignment Tasks 75.9 27.8 8.23 44.1 67.3 29. 34.3 52.9 85.7 79.9 67.2 59.1 23.9 54.7 25.0 8.26 56.3 75.4 35.9 36.4 75.5 91.6 84.8 79.2 70.4 28. 71.2 52.0 8.75 Table 9: Performance comparison of 2B-4B instruction-tuned models. Datasets Gemma2-2B Phi3.5-Mini MiniCPM3-4B Qwen2.5-3B Non-Emb Params 2.0B 3.6B 4.0B 2.8B General Tasks MMLU-Pro MMLU-redux LiveBench 0831 26.7 51.9 20. 47.5 67.7 27.4 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval 29.3 26.6 63. 68.9 74.9 30.5 5.8 51.0 27.2 48.5 86.2 Coding Tasks 72.6 63.2 47.2 15.8 Alignment Tasks 52.1 43.0 59.9 27.6 31.3 46.6 81.1 74.4 72.5 49.1 23.8 68.4 43.7 64.4 26. 30.3 65.9 86.7 74.4 72.7 60.2 19.9 58.2 et al., 2024), Llama3.1-405B-Instruct (Dubey et al., 2024), and our previous 72B version, Qwen2-72BInstruct (Yang et al., 2024a). The Qwen2.5-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B-Instruct in several critical benchmarks including MMLU-redux, MATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard and MTBench. Moreover, Qwen2.5-Plus outperforms Qwen2.5-72B-Instruct on 9 out of 13 benchmarks. Qwen2.5-14B/32B-Instruct & Qwen2.5-Turbo The performance of the Qwen2.5-Turbo, Qwen2.5-14BInstruct, and Qwen2.5-32B-Instruct models is evaluated and compared against baselines of similar sizes. The baselines include GPT4o-mini, Gemma2-27B-IT (Gemma Team et al., 2024), and Qwen2-57BA14BInstruct (Yang et al., 2024a). The results are summarized in Table 7. The Qwen2.5-32B-Instruct model exhibits superior performance across most tasks when compared to other models of similar size. Notably, our open-weight Qwen2.5-14B-Instruct model delivers competitive results across all benchmarks, rivaling those of GPT-4o-mini. Despite its significantly lower training and inference costs, the Qwen2.5-Turbo model outperforms Qwen2.5-14B-Instruct on eight out of ten benchmarks. This demonstrates that Qwen2.5-Turbo achieves remarkable efficiency and effectiveness, making it compelling choice for resource-constrained environments. Table 10: Performance comparison of 0.5B-1.5B instruction-tuned models. Datasets Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B General Tasks MMLU-Pro MMLU-redux LiveBench 14.4 12.9 7. 15.0 24.1 12.6 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval 23.7 13.9 40. 31.1 39.7 20.8 1.6 14.6 29.8 34.4 49.6 Coding Tasks 35.4 49.6 28.5 5.1 Alignment Tasks 27.9 22.9 41.2 12.4 21.2 25.3 61.6 42.1 44.2 38.5 4.5 29.0 32.4 50.7 18. 29.8 55.2 73.2 61.6 63.2 50.4 14.8 42.5 Table 11: Performance Comparison on our in-house English automatic evaluation benchmark. Models IF Knowledge Comprehension Coding Math Reasoning Proprietary LLMs GPT-4o-2024-08-06 GPT-4o-2024-11-20 Claude3.5-sonnet-2024-10-22 83.28 80.06 84.22 68.08 65.25 74.61 Qwen2-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-0.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-Turbo Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Plus 18.33 29.42 50.47 76.08 81.33 83.33 33.35 40.25 60.60 70.01 74.17 72.76 76.79 82.65 83.18 Qwen2 Series 18.59 29.23 44.79 59.49 Llama-3.1 Series 63.42 67.10 Qwen2.5 Series 30.29 41.19 46.11 52.74 59.78 58.56 64.08 66.09 68.41 76.51 79.07 79. 30.64 45.81 58.04 72.19 69.29 75.55 29.78 47.69 57.98 62.69 69.11 68.70 71.28 74.43 79.35 58.05 60.19 67.17 5.42 17.02 43.04 48.95 52.36 49.74 48. 13.16 20.34 38.31 48.07 55.96 58.14 48.00 47.09 15.41 26.19 41.43 48.41 52.68 54.48 58.90 60.41 59.58 26.29 40.99 49.38 56.93 59.68 57.77 60.97 59.73 62.52 66.45 67.07 70. 32.03 38.86 50.25 60.33 63.18 64.74 36.13 42.23 49.80 54.69 62.51 61.06 65.49 65.90 66.92 Other Instruction-tuned Models As illustrated in Table 8, the Qwen2.5-7B-Instruct model significantly outperforms its competitors, Gemma2-9B-IT and Llama3.1-8B-Instruct, across all tasks except IFEval. Notably, Qwen2.5-7B-Instruct exhibits clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8). For the edge-side instruction models, the Qwen2.5-3B-Instruct model, despite having fewer parameters than both the Phi3.5-mini-instruct (Abdin et al., 2024) and MiniCPM3-4B-Instruct (Hu et al., 2024) models, surpasses them in mathematics and coding tasks, as shown in Table 9. Additionally, it delivers competitive results in language understanding. The Qwen2.5-1.5B-Instruct and Qwen2.5-0.5BInstruct models have also seen substantial performance improvements over their previous versions, as detailed in Table 10. These enhancements make them particularly well-suited for edge-side applications in highly resource-constrained environments. 13 Table 12: Performance Comparison on our in-house Chinese automatic evaluation benchmark. Models IF Knowledge Comprehension Coding Math Reasoning Proprietary LLMs GPT-4o-2024-08-06 GPT-4o-2024-11-20 Claude3.5-sonnet-2024-10-22 42.50 42.71 49. 68.55 71.29 72.09 Qwen2-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-0.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-Turbo Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Plus 4.69 6.81 16.83 31.98 28.96 30. 6.12 7.38 16.50 26.64 26.87 32.94 32.64 37.22 46.15 Qwen2 Series 40.43 51.54 65.95 74.96 Llama-3.1 Series 57.41 63.79 Qwen2.5 Series 39.13 48.68 57.18 65.77 70.28 72.93 74.70 75.86 72.07 80.11 83.04 82.16 39.13 46.89 60.30 75.49 67.24 72.27 42.97 49.69 62.55 67.55 76.96 74.37 79.46 78.85 82.64 61.53 62.39 66. 9.85 14.14 37.05 41.57 61.74 66.04 63.71 14.07 24.57 50.52 65.55 54.82 60.73 41.18 46.05 9.60 22.96 29.88 39.56 49.78 51.92 54.45 56.71 58. 24.03 37.30 51.64 61.06 67.01 66.08 67.86 68.39 69.96 56.88 62.04 66.60 32.73 35.19 44.96 58.19 52.42 55.88 33.72 39.17 39.57 49.70 56.41 53.30 60.19 63.02 62.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫ –∏ –æ—Ü–µ–Ω–∫–∞ –ª—é–¥—å–º–∏, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —É–º–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è MMLU-Pro, MMLU-redux –∏ LiveBench 0831. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è GPQA, GSM8K –∏ MATH. –ù–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é HumanEval, MBPP, MultiPL-E –∏ LiveCodeBench 2305-2409. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è IFEval.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –¥–ª—è —á–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è MT-Bench –∏ Arena-Hard.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2.5-72B-Instruct –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Llama3.1-70B-Instruct, Llama3.1-405B-Instruct –∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è Qwen2-72B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—É—é –º–æ–¥–µ–ª—å Llama-3.1-405B-Instruct –ø–æ —Ä—è–¥—É –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è MMLU-redux, MATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard –∏ MTBench. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-72B-Instruct –ø–æ 9 –∏–∑ 13 –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.</p>\n<p>–¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π Qwen2.5-Turbo, Qwen2.5-14B-Instruct –∏ Qwen2.5-32B-Instruct –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT4o-mini, Gemma2-27B-IT –∏ Qwen2-14B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-32B-Instruct –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å Qwen2.5-14B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å GPT-4o-mini. –ú–æ–¥–µ–ª—å Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-14B-Instruct –ø–æ –≤–æ—Å—å–º–∏ –∏–∑ –¥–µ—Å—è—Ç–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è —Å—Ä–µ–¥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, Gemma2-9B-IT –∏ Llama3.1-8B-Instruct, –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º IFEval. –û—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω—ã –µ—ë –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ (MATH: 75.5) –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ (HumanEval: 84.8). –ú–æ–¥–µ–ª—å Qwen2.5-3B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ Phi3.5-mini-instruct –∏ MiniCPM3-4B-Instruct –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª–∏ Qwen2.5-1.5B-Instruct –∏ Qwen2.5-0.5B-Instruct —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –≥–¥–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∑–Ω–∞–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p>'}, {'title': 'In-house Automatic Evaluation', 'content': 'Despite the availability of several open benchmark datasets for evaluation, we believe that these are insufficient to fully capture the capabilities of LLMs. To address this, we have developed series of in-house datasets designed to assess various aspects of model performance, including knowledge understanding, text generation, coding, and more. These evaluations are conducted in both Chinese and English. In addition, we have specifically evaluated the multilingual performance of instructiontuned models. The results are summarized in Table 11 for English, Table 12 for Chinese, Table 13 for multilingualism of 70B+ Instruct models, and Table 14 for 7B-14B models, respectively. English & Chinese Evaluation We compare the performance of Qwen2.5-Instruct models against several leading language models, including GPT-4, Claude3.5-sonnet, Qwen2, and Llama-3.1, across both English and Chinese languages. Our analysis focuses on model size and its impact on performance, as well as how our latest Qwen2.5 series compares to previous iterations and competing models. For smaller models, we observe that the Qwen2.5-0.5B model achieves performance that is on par with or even surpasses the Qwen2-1.5B model. This indicates that the Qwen2.5 series has optimized parameter usage, enabling mid-sized models to achieve similar performance levels to larger models from the previous generation. The Qwen2.5-3B model demonstrates performance that is comparable to the Qwen2-7B model. Notably, the Qwen2.5-32B model exhibits remarkable improvement over the Qwen2-72B model. Our flagship model, Qwen2.5-72B, further narrows the gap between Qwen and state-of-the-art models like GPT-4 and Claude3.5-sonnet. In particular, Qwen2.5-72B matches or exceeds the performance of Llama-3.1-405B in all metrics except for instruction following. This achievement underscores the competitiveness of Qwen2.5-72B in wide range of language processing tasks, while also identifying areas for future improvement. Qwen2.5-Plus addresses the previous shortcomings in Chinese instruction following and further enhances its advantages in other areas. Multilingual Evaluation To comprehensively evaluate the multilingual capabilities of instruction-tuned models, we followed P-MMEval (Zhang et al., 2024) and extended several benchmarks as follows: (1) IFEval (Multilingual): We expanded the IFEval benchmark, originally in English, to include multilingual examples. To ensure language neutrality, we removed instances that contained language-specific content (e.g., start with letter A). (2) Knowledge Utilization: to assess the knowledge utilization abilities of the Qwen2.5 series models across multiple languages, we employed five MMLU-like benchmarks (multiple-choice format). These benchmarks include: AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Additionally, we evaluated the models performance on the translated version of the MMLU benchmark (okapi MMLU), which has been adapted 14 Table 13: Performance of the 70B+ Instruct models on Multilingual Tasks. Datasets Qwen2-72B Llama3.1-70B Qwen2.5-32B Mistral-Large GPT4o-mini Qwen2.5-72B IFEval (multilingual) 79.69 80.47 82.68 82. 85.03 86.98 Instruction Following AMMLU (Arabic) JMMLU (Japanese) KMMLU (Korean) IndoMMLU (Indonesian) TurkishMMLU (Turkish) okapi MMLU (translated) 68.85 77.37 57.04 66.31 69.22 77.84 70.08 73.89 53.23 67.50 66.89 76. Knowledge 70.44 76.55 60.75 66.42 72.41 77.16 Math Reasoning 69.24 75.77 56.42 63.21 64.78 78.37 69.73 73.74 56.77 67.75 71.19 73.44 72.44 80.56 61.96 69.25 76.12 79. MGSM8K (extended) 82.72 73.31 87.15 89.01 87. 88.16 Cultural Nuances BLEnD 25.90 30.49 27. 33.47 35.91 32.48 Table 14: Performance of the 7B-14B Instruct models on Multilingual Tasks. Datasets Qwen2-7B Llama3.1-8B Qwen2.5-7B Gemma2-9B Qwen2.5-14B IFEval (multilingual) 51.43 60.68 74.87 77.47 77. Instruction Following AMMLU (Arabic) JMMLU (Japanese) KMMLU (Korean) IndoMMLU (Indonesian) TurkishMMLU (Turkish) okapi MMLU (translated) 54.87 57.71 43.96 54.05 49.27 60.47 Knowledge 54.28 53.26 42.28 53.92 45.61 55.18 59.78 61.88 46.59 56.42 54.28 66. Math Reasoning 60.26 64.59 46.24 61.73 55.44 46.72 66.81 72.78 59.71 65.09 66.85 72.12 MGSM8K (extended) 56.13 66. 66.11 78.37 82.27 Cultural Nuances BLEnD 22. 19.47 23.66 28.31 26.99 into multiple languages from its original English form. (3) MGSM8K (Extended): Building upon the original MGSM8K benchmark, we extended the language support to include Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). (4) Cultural Nuances: To evaluate the models ability to capture cultural nuances, we utilized the BLEnD benchmark (Myung et al., 2024). This benchmark is specifically designed to test LLMs on their understanding of cultural subtleties. Qwen2.5 exhibits competitive performance in instruction following, multilingual knowledge, and mathematical reasoning, aligning well with models of comparable size. Although it shows notable improvements in capturing cultural nuances relative to its predecessor, Qwen2, there remains potential for further refinement in this domain.', 'summary': '<p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT-4, Claude3.5-sonnet –∏ Llama-3.1. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ. –ò—Å–ø—ã—Ç–∞–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –∫–∞–∫ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö</strong></p>\n<p>–°—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å –º–æ–¥–µ–ª–∏ Qwen2.5 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —Å –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\n*   –ú–æ–¥–µ–ª—å Qwen2.5-0.5B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π Qwen2-1.5B.\n*   Qwen2.5-3B —Å—Ä–∞–≤–Ω–∏–º–∞ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å Qwen2-7B.\n*   Qwen2.5-32B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B.\n*   –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ —Å GPT-4 –∏ Claude3.5-sonnet, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Llama-3.1-405B –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –∫—Ä–æ–º–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-Plus —É–ª—É—á—à–∏–ª–∞ —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±—ã–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã:\n1.  <strong>IFEval (Multilingual):</strong> –†–∞—Å—à–∏—Ä–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç IFEval –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –≤ —Å–µ–±—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –ü—Ä–∏ —ç—Ç–æ–º –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —è–∑—ã–∫–æ-–∑–∞–≤–∏—Å–∏–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏.\n2.  <strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π:</strong> –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Å –ø–æ–º–æ—â—å—é –ø—è—Ç–∏ —Ç–µ—Å—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö MMLU, –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º (AMMLU), —è–ø–æ–Ω—Å–∫–æ–º (JMMLU), –∫–æ—Ä–µ–π—Å–∫–æ–º (KMMLU), –∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–æ–º (IndoMMLU) –∏ —Ç—É—Ä–µ—Ü–∫–æ–º (TurkishMMLU) —è–∑—ã–∫–∞—Ö. –¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ MMLU (okapi MMLU).\n3.  <strong>MGSM8K (Extended):</strong> –Ø–∑—ã–∫–æ–≤–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ MGSM8K –±—ã–ª–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –∑–∞ —Å—á—ë—Ç –∞—Ä–∞–±—Å–∫–æ–≥–æ, –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ –∏ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤.\n4.  <strong>–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –Ω—é–∞–Ω—Å—ã:</strong> –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ç–µ—Å—Ç BLEnD –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Qwen2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –Ω—é–∞–Ω—Å–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –Ω–æ —ç—Ç–∞ –æ–±–ª–∞—Å—Ç—å –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏.</p>'}, {'title': 'Reward Model', 'content': 'The reward model serves as the cornerstone for guiding RL processes, and thus we conduct separate evaluation of the reward model used in the Qwen2.5 series. Our assessment benchmarks encompass Reward Bench (Lambert et al., 2024), RMB (Zhou et al., 2024), PPE (Frick et al., 2024b), and an internally collected out-of-domain Chinese human preference benchmark (Human-Preference-Chinese) to provide comprehensive analysis. For comparison, we included baseline models such as Nemotron-4-340BReward (Adler et al., 2024), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024c), and Athene-RM70B (Frick et al., 2024a). The results are shown in Table 15. Overall, our findings indicate that Llama-3.1Nemotron-70B-Reward excels on the Reward Bench, while Athene-RM-70B performs best on the RMB benchmark. The Qwen2.5-RM-72B, leads in both the PPE and Human-Preference-Chinese evaluations, ranking second only to Athene-RM-70B on the RMB and achieving performance level comparable to 15 Table 15: Performance comparison across multiple RM benchmarks. Metric Chat Chat Hard Safety Reasoning Score Helpfulness (BoN) Helpfulness (Pairwise) Harmlessness (BoN) Harmlessness (Pairwise) Overall Human Preference IFEval GPQA MATH MBPP-Plus MMLU-Pro Objective-Avg Nemotron-4-340BReward Llama-3.1-Nemotron70B-Reward Athene-RM -70B Qwen2.5-RM -72B Reward Bench 95.80 87.10 91.50 93.60 92.00 48.85 68.70 50.92 70.84 59.83 59.28 62.66 56.56 65.12 49.15 69.69 60.64 RMB PPE 97.50 85.70 95.10 98.10 94. 61.02 75.28 52.00 69.96 64.57 64.32 63.40 59.14 69.73 55.62 70.20 63.62 98.32 70.61 92.10 92.19 88.32 67.24 80.82 67.02 80.83 73.98 66.48 62.15 59.26 79.14 67.97 76.95 69.09 97.21 78.73 92.71 97.65 91. 65.72 78.83 56.35 73.94 68.71 64.80 67.97 59.80 81.48 64.34 75.66 69.85 Accuracy 50.46 59.95 61. 61.27 Human-Preference-Chinese Nemotron-4-340B-Reward on the Reward Bench, albeit slightly behind Llama-3.1-Nemotron-70B-Reward. Due to the lack of evaluation methods for reward models, current reward models are typically evaluated using Reward Bench. However, our evaluation results from multiple RM benchmarks suggest that overoptimization on specific benchmark may trigger Goodharts law (Hoskin, 1996), resulting in degraded performance on other benchmarks and potentially impacting downstream alignment performance. This highlights the need for comprehensive evaluation of reward models across diverse benchmarks rather than relying solely on single benchmark. More importantly, through iterative experimentation, we have also come to recognize critical limitation: current reward model evaluation benchmarks do not accurately predict the performance of the RL models trained under their guidance. In other words, higher score on RM benchmarks does not necessarily correlate with superior performance of the resulting RL model. This insight underscores the need for further research into more predictive evaluation methods for reward models.', 'summary': '<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model, RM), –∫–æ—Ç–æ—Ä–∞—è –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –æ—Ü–µ–Ω–∫—É RM, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –≤ —Å–µ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: Reward Bench, RMB, PPE –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ (Human-Preference-Chinese). –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –º–æ–¥–µ–ª–∏ Nemotron-4-340BReward, Llama-3.1-Nemotron-70B-Reward –∏ Athene-RM70B.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Llama-3.1-Nemotron-70B-Reward –ª–∏–¥–∏—Ä—É–µ—Ç –Ω–∞ Reward Bench, –∞ Athene-RM-70B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ RMB. –ú–æ–¥–µ–ª—å Qwen2.5-RM-72B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ PPE –∏ Human-Preference-Chinese, –∞ –Ω–∞ RMB –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –ø–æ—Å–ª–µ Athene-RM-70B. –ù–∞ Reward Bench –æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å Nemotron-4-340B-Reward, —Ö–æ—Ç—è –∏ –Ω–µ–º–Ω–æ–≥–æ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç Llama-3.1-Nemotron-70B-Reward.</p>\n<p>–ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–∞—Å—Ç–æ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ Reward Bench. –û–¥–Ω–∞–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —É—Ö—É–¥—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö. –≠—Ç–æ—Ç —ç—Ñ—Ñ–µ–∫—Ç, –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–∞–∫ –∑–∞–∫–æ–Ω –ì—É–¥—Ö–∞—Ä—Ç–∞, –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ RM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.</p>\n<p>–ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RM –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ RL-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ–±—É—á–µ–Ω—ã —Å –∏—Ö –ø–æ–º–æ—â—å—é. –¢–æ –µ—Å—Ç—å, –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö RM –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å RL-–º–æ–¥–µ–ª–µ–π. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>'}, {'title': 'Long Context Capabilities', 'content': 'We utilize three benchmarks to evaluate long context capabilities of Qwen2.5 models: RULER (Hsieh et al., 2024), LV-Eval (Yuan et al., 2024), and Longbench-Chat (Bai et al., 2024). In LV-Eval, we adopt keyword recall as the reported score to mitigate the high rate of false negatives present in the original metrics. The results are shown in Table 16 and Table 17. We can observe that the Qwen2.5 models, after equipping length extrapolation techniques (i.e., DCA + YARN), have demonstrated strong long context processing capabilities on the three datasets. Among them, Qwen2.5-72B-Instruct has shown the strongest performance across all context lengths, significantly outperforming existing open-weight long-context models as well as the proprietary models like GPT-4o-mini and GPT-4. Furthermore, as shown in Figure 2, Qwen2.5-Turbo achieves 100% accuracy in the 1M-token passkey retrieval task, demonstrating its exceptional ability to capture detailed information from ultra-long contexts. We introduce sparse attention mechanism to significantly enhance inference speed, which is critical for user experience when processing long contexts. For sequences of 1M tokens, this approach reduces the computational load of the attention mechanism by 12.5 times. Figure 3 illustrates the time to first token (TTFT) of Qwen2.5-Turbo across various hardware configurations, where our method achieves 3.2 to 4.3 times speedup. 16 Table 16: Performance of Qwen2.5 Models on RULER. YARN+DCA does not change the model behavior within 32K tokens. Model GLM4-9b-Chat-1M Llama-3-8B-Instruct-Gradient-1048k Llama-3.1-70B-Instruct GPT-4o-mini GPT-4 Qwen2.5-7B-Instruct w/o DCA + YARN Qwen2.5-14B-Instruct w/o DCA + YARN Qwen2.5-32B-Instruct w/o DCA + YARN Qwen2.5-72B-Instruct w/o DCA + YARN Qwen2.5-Turbo Claimed Length Avg. 4K 1M 89.9 1M 88.3 128K 89.6 128K 87.3 128K 91.6 128K 85.4 80.1 128K 91.4 86.5 128K 92.9 88.0 128K 95.1 90.8 1M 93.1 94.7 95.5 96.5 95.0 96.6 96.7 96.7 97.7 97.7 96.9 96.9 97.7 97.7 97. RULER 8K 92.8 93.8 95.8 92.9 96.3 95.1 95.1 96.8 96.8 97.1 97.1 97.2 97.2 95.7 16K 32K 64K 128K 92.1 91.6 95.4 92.7 95.2 93.7 93.7 95.9 95.9 95.5 95.5 97.7 97.7 95.5 89.9 87.4 94.8 90.2 93.2 89.4 89.4 93.4 93.4 95.5 95.5 96.5 96.5 94. 86.7 84.7 88.4 87.6 87.0 82.3 74.5 86.7 82.3 90.3 85.3 93.0 88.5 90.8 83.1 77.0 66.6 65.8 81.2 55.1 31.4 78.1 53.0 82.0 57.7 88.4 67.0 84. Table 17: Performance of Qwen2.5 Models on LV-Eval and LongBench-Chat. YARN+DCA does not change the model behavior within 32k tokens. Model GLM4-9B-Chat-1M Llama-3-8B-Instruct-Gradient-1048k Llama-3.1-70B-Instruct GPT-4o-mini Qwen2.5-7B-Instruct w/o DCA + YARN Qwen2.5-14B-Instruct w/o DCA + YARN Qwen2.5-32B-Instruct w/o DCA + YARN Qwen2.5-72B-Instruct w/o DCA + YARN Qwen2.5-Turbo Claimed Length 16k 1M 46.4 1M 31.7 48.6 52.9 128k 128k 128k 128k 128k 128k 55.9 55.9 53.0 53.0 56.0 56.0 60.4 60.4 1M 53.4 32k 43.2 31.8 47.4 48.1 49.7 49.7 50.8 50.8 53.6 53.6 57.5 57. 50.0 LV-Eval 64k 128k 256k LongBenchChat 42.9 28.8 42.9 46.0 48.0 33.1 46.8 37.0 48.8 40.1 53.9 47.4 45.4 37.0 40.4 26.3 21.1 26.2 N/A 40.7 N/A 41.1 13.6 43.6 18.4 45.3 20.5 50.9 27.0 43. 36.9 0.5 39.4 0.8 41.0 0.7 45.2 2.4 38.0 7.82 6.20 6.80 8.48 7.42 - 8.04 - 8.70 - 8.72 - 8.34 Figure 2: Performance of Qwen2.5-Turbo on Passkey Retrieval Task with 1M Token Lengths. 17 Figure 3: TTFT (Time To First Token) of Qwen2.5-Turbo and Qwen2.5-7B with Full Attention and Our Method.', 'summary': '<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5 –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç—Ä–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: RULER, LV-Eval –∏ Longbench-Chat. –í LV-Eval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å "keyword recall" (–ø–æ–ª–Ω–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤), —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 16 –∏ 17.</p>\n<p>–ò–∑ —ç—Ç–∏—Ö —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5, –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –¥–ª–∏–Ω—ã (DCA + YARN), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4o-mini –∏ GPT-4.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 2, –º–æ–¥–µ–ª—å Qwen2.5-Turbo –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è passkey (–∫–æ–¥–æ–≤–∞—è —Ñ—Ä–∞–∑–∞) –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ—ë –≤—ã—Å–æ–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.</p>\n<p>–î–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, –±—ã–ª –≤–Ω–µ–¥—Ä–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention). –î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ 12.5 —Ä–∞–∑. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT) –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen2.5-Turbo –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ 3.2-4.3 —Ä–∞–∑–∞.</p>'}, {'title': 'Conclusion', 'content': 'Qwen2.5 represents significant advancement in large language models (LLMs), with enhanced pretraining on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning. These improvements boost human preference alignment, long text generation, and structural data analysis, making Qwen2.5 highly effective for instruction-following tasks. Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters and proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus. Empirical evaluations show that Qwen2.5-72B-Instruct matches the performance of the state-of-the-art Llama-3-405B-Instruct, despite being six times smaller. Qwen2.5 also serves as foundation for specialized models, demonstrating its versatility for domain-specific applications. We believe that Qwen2.5s robust performance, flexible architecture, and broad availability make it valuable resource for both academic research and industrial applications, positioning it as key player of future innovations. In the future, we will focus on advancing robust foundational models. First, we will iteratively refine both base and instruction-tuned large language models (LLMs) by incorporating broader, more diverse, higherquality data. Second, we will also continue to develop multimodal models. Our goal is to integrate various modalities into unified framework. This will facilitate seamless, end-to-end information processing across textual, visual, and auditory domains. Third, we are committed to enhancing the reasoning capabilities of our models. This will be achieved through strategic scaling of inference compute resources. These efforts aim to push the boundaries of current technological limitations and contribute to the broader field of artificial intelligence.', 'summary': '<p>–ú–æ–¥–µ–ª—å Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –±—ã–ª–∞ —É–ª—É—á—à–µ–Ω–∞ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–≤—ã—à–∞—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏, –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –¥–µ–ª–∞–µ—Ç Qwen2.5 –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>Qwen2.5 –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –æ—Ç 0,5 –¥–æ 72 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã MoE (Mixture of Experts), —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen2.5-Turbo –∏ Qwen2.5-Plus. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Qwen2.5-72B-Instruct –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Llama-3-405B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–Ω–∞ –≤ —à–µ—Å—Ç—å —Ä–∞–∑ –º–µ–Ω—å—à–µ. Qwen2.5 —Ç–∞–∫–∂–µ —Å–ª—É–∂–∏—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–≤–æ—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p>–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≥–∏–±–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —à–∏—Ä–æ–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Qwen2.5 –¥–µ–ª–∞—é—Ç –µ–µ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –∫–∞–∫ –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ç–∞–∫ –∏ –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π, –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É—è –µ–µ –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–≥–æ –∏–≥—Ä–æ–∫–∞ –≤ –±—É–¥—É—â–∏—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö.</p>\n<p>–í –±—É–¥—É—â–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –Ω–∞–¥–µ–∂–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∏ –±—É–¥—É—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ, —Ç–∞–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–Ω–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å—Ç—Ä–µ–º—è—Å—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ—Å—à–æ–≤–Ω—É—é —Å–∫–≤–æ–∑–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–π, –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Å–ª—É—Ö–æ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö. –í-—Ç—Ä–µ—Ç—å–∏—Ö, –æ–Ω–∏ –Ω–∞–º–µ—Ä–µ–Ω—ã –ø–æ–≤—ã—Å–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–≤–æ–∏—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞. –≠—Ç–∏ —É—Å–∏–ª–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ç–µ–∫—É—â–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≤–∫–ª–∞–¥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ü–µ–ª–æ–º.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi (1)', '#alignment (1)', '#architecture (1)', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset (1)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (1)', '#open_source (1)', '#optimization (1)', '#plp', '#rag', '#reasoning (1)', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>–í —ç—Ç–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Qwen2.5, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è —Å–æ–±–æ–π —Å–µ—Ä–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, Qwen 2.5 –±—ã–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–∞ –∫–∞–∫ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏.</p>
<p>–í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ–±—ä–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —É–≤–µ–ª–∏—á–µ–Ω —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—á–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>
<p>–ù–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–µ –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–ª—É—á—à–∞—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∑–∞–º–µ—Ç–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>
<p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–µ—Ä–∏—è LLM Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—é—Ç –±–∞–∑–æ–≤—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã, –∞ —Ç–∞–∫–∂–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE): Qwen2.5-Turbo –∏ Qwen2.5-Plus, –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤ Alibaba Cloud Model Studio.</p>
<p>Qwen2.5 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —à–∏—Ä–æ–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ç–µ—Å—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ —Ç.–¥. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Qwen2.5-72B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä—è–¥ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo –∏ Qwen2.5-Plus –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–∏ —ç—Ç–æ–º –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å –º–æ–¥–µ–ª—è–º–∏ GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>
<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª–∏ Qwen2.5 —Å–ª—É–∂–∞—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Qwen2.5-Math, Qwen2.5-Coder, QwQ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><p>–†–∞–∑–≤–∏—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –Ω–∞—Å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (AGI). –£—Å–ø–µ—Ö–∏ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π (RLHF) –ø–æ–∑–≤–æ–ª–∏–ª–∏ LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ù–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ o1, —É–ª—É—á—à–∏–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å LLM –∫ –≥–ª—É–±–æ–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é. –≠—Ç–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–æ–ª–µ–µ –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.</p>
<p>–ü–æ–º–∏–º–æ —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ –≥–æ–¥–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ—Å—Ç —á–∏—Å–ª–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö (open-weight) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama, Mistral –∏ Qwen. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ —Å–¥–µ–ª–∞–ª–∏ LLM –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ –∫—Ä—É–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º—É —É—á–∞—Å—Ç–∏—é –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, —Å—Ç–∏–º—É–ª–∏—Ä—É—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∑–∞ —Å—á–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏ —É—Å–∫–æ—Ä—è—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>
<p>–ù–µ–¥–∞–≤–Ω–æ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–∏–∏ Qwen ‚Äî Qwen2.5. –í —Ä–∞–º–∫–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ 7 —Ä–∞–∑–º–µ—Ä–æ–≤ (0.5B, 1.5B, 3B, 7B, 14B, 32B –∏ 72B), –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ bfloat16, —Ç–∞–∫ –∏ –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö. –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ Mixture-of-Experts (MoE) ‚Äî Qwen2.5-Turbo –∏ Qwen2.5-Plus1, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç —Å GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>
<p>–í —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Qwen2.5, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —É—Å–∏–ª–∏–π –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É–ª—É—á—à–µ–Ω–Ω—ã—Ö LLM. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ Qwen:</p>
<ul>
<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö:</strong> –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –≤ Qwen2.5, –ø–æ–º–∏–º–æ –º–æ–¥–µ–ª–µ–π 0.5B, 1.5B, 7B –∏ 72B, –≤–æ–∑–≤—Ä–∞—â–µ–Ω—ã –º–æ–¥–µ–ª–∏ 3B, 14B –∏ 32B, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. Qwen2.5-Turbo –∏ Qwen2.5-Plus –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é, –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å—é.</li>
<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö:</strong> –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É–≤–µ–ª–∏—á–∏–ª—Å—è —Å 7 –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∑–Ω–∞–Ω–∏—è, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ, —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –º–∏–ª–ª–∏–æ–Ω –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —ç—Ç–∞–ø—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º (SFT), –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (DPO) –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO).</li>
<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏:</strong> –£—Å—Ç—Ä–∞–Ω–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Qwen2, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–Ω—É—é –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å 2K –¥–æ 8K —Ç–æ–∫–µ–Ω–æ–≤), —É–ª—É—á—à–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞–±–ª–∏—Ü –∏ JSON) –∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Turbo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –¥–ª–∏–Ω—É –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤.</li>
</ul></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Architecture & Tokenizer</div>
                <div class="summary_text"><p>–°–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen2.5 –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º (Qwen2.5-0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B) –∏ –º–æ–¥–µ–ª–∏ MoE (Mixture of Experts) –¥–ª—è API-—Å–µ—Ä–≤–∏—Å–æ–≤ (Qwen2.5-Turbo –∏ Qwen2.5-Plus).</p>
<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π:</strong></p>
<ul>
<li><strong>–ü–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏:</strong><ul>
<li>–ò—Å–ø–æ–ª—å–∑—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–µ–∫–æ–¥–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer (–∫–∞–∫ –∏ –≤ Qwen2).</li>
<li>–í–∫–ª—é—á–∞—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:<ul>
<li><strong>Grouped Query Attention (GQA):</strong> –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV-–∫—ç—à–∞ (—Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π).</li>
<li><strong>–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ SwiGLU:</strong> –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.</li>
<li><strong>Rotary Positional Embeddings (RoPE):</strong> –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>
<li><strong>QKV bias:</strong> —Å–º–µ—â–µ–Ω–∏–µ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è.</li>
<li><strong>RMSNorm —Å –ø—Ä–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π:</strong> –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.</li>
</ul>
</li>
</ul>
</li>
<li><strong>–ú–æ–¥–µ–ª–∏ MoE:</strong><ul>
<li>–ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</li>
<li>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–ª–æ–∏ FFN (feed-forward network) –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–∏ MoE.</li>
<li>–°–ª–æ–π MoE —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö FFN-—ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –∫ top-K —ç–∫—Å–ø–µ—Ä—Ç–∞–º.</li>
<li>–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ç–æ—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –æ–±—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (–∫–∞–∫ –≤ Qwen1.5-MoE).</li>
</ul>
</li>
</ul>
<p><strong>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:</strong></p>
<ul>
<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Qwen, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç byte-level byte-pair encoding (BBPE) —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º –∏–∑ 151 643 –æ–±—ã—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.</li>
<li>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 22 (–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Qwen).<ul>
<li>–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.</li>
<li>–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤—ã–¥–µ–ª–µ–Ω—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏.</li>
</ul>
</li>
<li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5 –ø–æ–≤—ã—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.</li>
</ul>
<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>
* GQA –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–Ω–∏–º–∞–Ω–∏—è.
* MoE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –±—ã—Ç—å –±–æ–ª–µ–µ –µ–º–∫–∏–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ.
* –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Pre-training Data</div>
                <div class="summary_text"><p>–í Qwen2.5 –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∑–∞ —Å—á–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤:</p>
<ol>
<li>
<p><strong>–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:</strong> –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í Qwen2.5 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-Instruct. –û–Ω–∏ –ø—Ä–æ–≤–æ–¥—è—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç –∏–º –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —à–∞–≥–æ–º –≤–ø–µ—Ä–µ–¥ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–¥—Ö–æ–¥–æ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–º—Å—è –≤ Qwen2, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Qwen2 –Ω–∞ –±–æ–ª—å—à–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –ª—É—á—à–µ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –ø—Ä–∏—á–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤.</p>
</li>
<li>
<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –∫–æ–¥–∞:</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ Qwen2.5-Math –∏ Qwen2.5-Coder. –≠—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–æ—Å—Ç–∏—á—å –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–∏—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, Qwen2.5 –ø–æ–ª—É—á–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.</p>
</li>
<li>
<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:</strong> –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç—è—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –∫–æ–¥–∞ –∏ –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-72B-Instruct –∏ Qwen2Math-72B-Instruct. –ö–∞—á–µ—Å—Ç–≤–æ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen2-Math-RM-72B.</p>
</li>
<li>
<p><strong>–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:</strong> –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª–∏ Qwen2-Instruct –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Ç–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏, –∫–∞–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è, —á—Ä–µ–∑–º–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–µ–±-–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è, —à–∞–±–ª–æ–Ω–Ω—ã–π –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—à–∏–Ω–∞–º–∏ –∫–æ–Ω—Ç–µ–Ω—Ç. –ù–∞–ø—Ä–æ—Ç–∏–≤, –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –Ω–∞—É–∫–∞ –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã. –ó–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–µ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏–∑ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, —Å–æ–∑–¥–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª—è–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>
</li>
</ol>
<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π, –±—ã–ª —Å–æ–∑–¥–∞–Ω –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Ä–æ—Å —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ Qwen2, –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Scaling Law for Hyper-parameters</div>
                <div class="summary_text"><p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö, –∑–¥–µ—Å—å –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π.</p>
<p>–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–º–æ–≥–∞—é—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch size) –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) ¬µ, –∫–∞–∫ –¥–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫ –∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE (Mixture of Experts) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. –ü—É—Ç–µ–º –æ–±—à–∏—Ä–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞–µ—Ç—Å—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è ¬µopt –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ Bopt –º–µ–Ω—è—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è D.</p>
<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∫–ª—é—á–∞—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–æ–¥–µ–ª–∏ MoE —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 1 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç 0,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 600 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –º–∞—Å—à—Ç–∞–±–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π MoE —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Ö –ø–ª–æ—Ç–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ø–∞—Ä–∏—Ç–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ Qwen2.5-72B –∏ Qwen2.5-14B) –∑–∞ —Å—á–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∫ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –æ–±—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Long-context Pre-training</div>
                <div class="summary_text"><p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 4096 —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º, –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è.</p>
<p>–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –≤ Qwen2, –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏, –∫—Ä–æ–º–µ Qwen2.5-Turbo, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 4096 –¥–æ 32768 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º, –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 10 000 –¥–æ 1 000 000 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ ABF.</p>
<p>–î–ª—è Qwen2.5-Turbo –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞: 32 768 —Ç–æ–∫–µ–Ω–æ–≤, 65 536 —Ç–æ–∫–µ–Ω–æ–≤, 131 072 —Ç–æ–∫–µ–Ω–æ–≤ –∏, –Ω–∞–∫–æ–Ω–µ—Ü, 262 144 —Ç–æ–∫–µ–Ω–æ–≤. –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 10 000 000. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è, –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã 40% –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–º–µ–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞, –∞ 60% –±—ã–ª–∏ –∫–æ—Ä–æ—á–µ. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø–ª–∞–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –æ–±–æ–±—â–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã.</p>
<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: YARN –∏ Dual Chunk Attention (DCA). –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è–º, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å, —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞–∑–∞. Qwen2.5-Turbo —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∞ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ ‚Äì –¥–æ 131 072 —Ç–æ–∫–µ–Ω–æ–≤. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–∞—é—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ —Å—á–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏, –Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—Ö–æ–¥–∞—Ö —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Post-training</div>
                <div class="summary_text"><p>–í Qwen 2.5 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen 2 –≤–Ω–µ—Å–µ–Ω—ã –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏:</p>
<ol>
<li>
<p><strong>–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-tuning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –º–∏–ª–ª–∏–æ–Ω—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –ø–µ—Ä–µ–Ω–æ—Å –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–∏—Å—Ç–µ–º–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –¢–æ –µ—Å—Ç—å, –º–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º —Å–ø–µ–∫—Ç—Ä–æ–º –∑–∞–¥–∞—á –∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ —Å–ª—É—á–∞—è–º–∏.</p>
</li>
<li>
<p><strong>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ Qwen 2.5 —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞: –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Offline RL) –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Online RL).</p>
<ul>
<li><strong>–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —Å–∏–≥–Ω–∞–ª—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –æ–±—É—á–∞–µ–º—ã–º–∏, —Ç–∞–∫ –∏ –Ω–∞–¥–µ–∂–Ω—ã–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—Ç—å —ç—Ç–∏ —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–≤—ã–∫–∏.<ul>
<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: —Ç—É—Ç –≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞—Ä–∞–Ω–µ–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</em></li>
</ul>
</li>
<li><strong>–û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –Ω—é–∞–Ω—Å—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—ã–≤–æ–¥–∞, –≤–∫–ª—é—á–∞—è –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –∫—Ä–∞—Ç–∫–æ—Å—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏, —Å–≤—è–∑–Ω—ã–º–∏ –∏ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –≤—ã–≤–æ–¥—ã –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞.<ul>
<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ª–µ—Ç—É.</em></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Qwen 2.5 —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –Ω–∞–¥–µ–∂–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Supervised Fine-tuning</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è, –≤–Ω–µ—Å–µ–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å Qwen2.5 –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—Å–∞—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π:</p>
<ol>
<li>
<p><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:</strong> Qwen2.5 —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–∏–ø–∏—á–Ω—É—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –º–µ–Ω–µ–µ 2000 —Ç–æ–∫–µ–Ω–æ–≤). –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤, –Ω–∞–∫–ª–∞–¥—ã–≤–∞–ª–∏—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Ñ–∏–ª—å—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen2 –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –¥–∞–Ω–Ω—ã—Ö.</p>
</li>
<li>
<p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> –í –º–æ–¥–µ–ª—å –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Å—Ç–∏–ª–µ "—Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π" (chain-of-thought). –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—É–±–ª–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∑–∞–¥–∞—á–∏ –∏–∑ —à–∫–æ–ª—å–Ω—ã—Ö —É—á–µ–±–Ω–∏–∫–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (rejection sampling) –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p>
</li>
<li>
<p><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏–∑ Qwen2.5Coder. –ë—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–∫–æ–¥ –ø–æ—á—Ç–∏ –¥–ª—è 40 —è–∑—ã–∫–æ–≤. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Ä–∞—Å—à–∏—Ä–µ–Ω –∑–∞ —Å—á–µ—Ç —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Å–∞–π—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å–±–æ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ —Å GitHub. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞, –≤—ã–ø–æ–ª–Ω—è—é—â–∞—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥—É–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>
</li>
<li>
<p><strong>–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º:</strong> –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ç—Ä–æ–≥–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞. –ú–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π –∫–æ–¥, –∞ —Ç–∞–∫–∂–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏. –ü—É—Ç–µ–º –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞, –±—ã–ª–∏ –æ—Ç–æ–±—Ä–∞–Ω—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–ª–æ —Ç–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>
</li>
<li>
<p><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:</strong> –ë—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –∫–∞–∫ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫), —Ç–∞–∫ –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–≤—ã—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>
</li>
<li>
<p><strong>–õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –∏–∑ 70 000 –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏. –≠—Ç–∏ –∑–∞–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞–ª–∏ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –≤–æ–ø—Ä–æ—Å—ã "–≤–µ—Ä–Ω–æ/–Ω–µ–≤–µ—Ä–Ω–æ" –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –ø–æ–¥—Ö–æ–¥–∏—Ç—å –∫ –ø—Ä–æ–±–ª–µ–º–∞–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∞–Ω–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ, –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–ª–∏ –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏.</p>
</li>
<li>
<p><strong>–ö—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å:</strong> –î–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ —è–∑—ã–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —è–∑—ã–∫–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–ª–æ—Å—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∫–∞–∂–¥—ã–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ –µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º.</p>
</li>
<li>
<p><strong>–ù–∞–¥–µ–∂–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:</strong> –ë—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã —Å–æ—Ç–Ω–∏ –æ–±—â–∏—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏. –û—Ü–µ–Ω–∫–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å.</p>
</li>
<li>
<p><strong>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤:</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫–∞ –∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏. –û—Ç–≤–µ—Ç—ã –ø–æ–¥–≤–µ—Ä–≥–∞–ª–∏—Å—å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ, –∏ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏–∑–Ω–∞–Ω—ã –±–µ–∑—É–ø—Ä–µ—á–Ω—ã–º–∏ –≤—Å–µ–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –æ—Ü–µ–Ω–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å.</p>
</li>
</ol>
<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–≤—É—Ö —ç–ø–æ—Ö —Å –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–ª–∞—Å—å, –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –±—ã–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Offline Reinforcement Learning</div>
                <div class="summary_text"><p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ –æ—Ç–≤–µ—Ç—ã –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í —ç—Ç–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω—ã–º.</p>
<p>–ù–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —ç—Ç–∞–ø–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç–∞–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ —ç—Ç–æ—Ç –∂–µ –∫–æ–Ω–≤–µ–π–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ: –º–æ–¥–µ–ª—å SFT (Supervised Fine-Tuning) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—Ç—ã, –ø—Ä–æ—à–µ–¥—à–∏–µ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –∞ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É, ‚Äî –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO). </p>
<p>–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ —Ä—É—á–Ω—ã–µ, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ø—Ä–æ–≤–µ—Ä–∫–∏. –¢–∞–∫–æ–π –¥–≤–æ–π–Ω–æ–π –ø–æ–¥—Ö–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≥–æ–¥–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞. –í –∏—Ç–æ–≥–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –ø—Ä–∏–º–µ—Ä–Ω–æ –∏–∑ 150 000 –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä.</p>
<p>–ó–∞—Ç–µ–º –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Online Merging Optimizer —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è 7 * 10^-7.</p>
<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>
*   SFT (Supervised Fine-Tuning) - —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
*   DPO (Direct Preference Optimization) - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–µ–∂–¥—É –¥–≤—É–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–∞.
*   Online Merging Optimizer - —ç—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Online Reinforcement Learning</div>
                <div class="summary_text"><p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model) –¥–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç—ã, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é, –±—É–¥—É—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —ç—Ç–∏—á–µ—Å–∫–∏–º –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.</p>
<p>–í–æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö:</p>
<ul>
<li><strong>–ü—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö –∏ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∏–∑–±–µ–≥–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω–æ–π –∏–ª–∏ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>
<li><strong>–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å:</strong> –í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –ø–æ–ª–µ–∑–Ω—ã–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç–≤–µ—á–∞—è –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –û–Ω –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ø—Ä–∏–Ω–æ—Å–∏—Ç—å –ø–æ–ª—å–∑—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.</li>
<li><strong>–ö—Ä–∞—Ç–∫–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º–∏ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, –∏–∑–±–µ–≥–∞—è –∏–∑–ª–∏—à–Ω–µ–π –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç–∏. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ç–∫–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏.</li>
<li><strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:</strong> –í—Å–µ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω—ã —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –≤—ã–≤–æ–¥, —á—Ç–æ–±—ã –æ–Ω —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</li>
<li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏–∑–±–µ–≥–∞—è –ª—é–±–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–º—É, –∞–º–æ—Ä–∞–ª—å–Ω–æ–º—É –∏–ª–∏ –≤—Ä–µ–¥–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–¥–≤–∏–≥–∞—Ç—å —ç—Ç–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö.</li>
<li><strong>–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –≤—ã–¥–∞–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, —Å–≤–æ–±–æ–¥–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è—Å—å, –≥–µ–Ω–¥–µ—Ä–Ω—É—é, —Ä–∞—Å–æ–≤—É—é, –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫—É—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫–æ –≤—Å–µ–º —Ç–µ–º–∞–º —Ä–∞–≤–Ω–æ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—è—Å—å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã—Ö –º–æ—Ä–∞–ª—å–Ω—ã—Ö –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.</li>
</ul>
<p>–ó–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –≤–∑—è—Ç—ã –∏–∑ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö: –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–µ–≥–æ—Å—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é. –û—Ç–≤–µ—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –º–æ–¥–µ–ª–µ–π Qwen, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ (SFT, DPO –∏ RL) –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –≤–Ω–µ—Å–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã. –ü–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Å–æ–∑–¥–∞—é—Ç—Å—è –∫–∞–∫ —Å –ø–æ–º–æ—â—å—é —Ä—É—á–Ω–æ–π, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, –∞ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è DPO —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç—Å—è –≤ —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–í —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Group Relative Policy Optimization (GRPO). –ù–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏–¥–µ–Ω—Ç–∏—á–µ–Ω —Ç–æ–º—É, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è RL. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–ø—Ä–æ—Å—ã —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è 8 –æ—Ç–≤–µ—Ç–æ–≤. –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–∫–µ—Ç–∞ 2048 –∏ 2048 –æ–±—Ä–∞–∑—Ü–∞–º–∏ –≤ –∫–∞–∂–¥–æ–º —ç–ø–∏–∑–æ–¥–µ, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø–∞—Ä—É –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Long Context Fine-tuning</div>
                <div class="summary_text"><p>–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å Qwen2.5-Turbo, –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.</p>
<p>–ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è (SFT) —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –∏ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤), —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 262 144 —Ç–æ–∫–µ–Ω–æ–≤). –¢–∞–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.</p>
<p>–ù–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Ç–æ–π, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏ –æ–±—É—á–µ–Ω–∏–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –≠—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ –¥–≤—É–º—è –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏—á–∏–Ω–∞–º–∏: –≤–æ-–ø–µ—Ä–≤—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º; –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –≤—Å–µ —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Base Models</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ–¥—Ä–æ–±–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen2.5. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —É–¥–µ–ª—è–µ—Ç—Å—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –æ–±—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –Ω–∞—É—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.</p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:</p>
<ul>
<li><strong>–û–±—â–∏–µ –∑–∞–¥–∞—á–∏:</strong> MMLU, MMLU-Pro, MMLU-redux, BBH, ARC-C, TruthfulQA, Winogrande, HellaSwag.</li>
<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –Ω–∞—É–∫–∞:</strong> GPQA, TheoremQA, GSM8K, MATH.</li>
<li><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> HumanEval, HumanEval+, MBPP, MBPP+, MultiPL-E (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript).</li>
<li><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏:</strong><ul>
<li><strong>–≠–∫–∑–∞–º–µ–Ω:</strong> M3Exam, IndoMMLU, ruMMLU –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π MMLU (–∞—Ä–∞–±—Å–∫–∏–π, –∏—Å–ø–∞–Ω—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –Ω–µ–º–µ—Ü–∫–∏–π, –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –∫–æ—Ä–µ–π—Å–∫–∏–π).</li>
<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ:</strong> BELEBELE, XCOPA, XWinograd, XStoryCloze –∏ PAWS-X.</li>
<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> MGSM.</li>
<li><strong>–ü–µ—Ä–µ–≤–æ–¥:</strong> Flores-101.</li>
</ul>
</li>
</ul>
<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-72B –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>
<p>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Qwen2.5-72B –∏ Qwen2.5-Plus —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –≤–µ–¥—É—â–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏: Llama3-70B, Llama3-405B, Mixtral-8x22B –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π Qwen2-72B.</p>
<p>Qwen2.5-72B –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Å Llama-3-405B, –ø—Ä–∏ —ç—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å –ø—è—Ç—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B, Qwen2.5-72B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>
<p>Qwen2.5-Plus, –∏–º–µ—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—á–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2.5-72B –∏ Llama3-405B, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HellaSwag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics –∏ Multi-Translation. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 64.0 –Ω–∞ MMLU-Pro, —á—Ç–æ –Ω–∞ 5.9 –ø—É–Ω–∫—Ç–æ–≤ –≤—ã—à–µ, —á–µ–º —É Qwen2.5-72B.</p>
<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-14B/32B –∏ Qwen2.5-Turbo —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>
<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo, Qwen2.5-14B –∏ 32B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Yi-1.5-34B, Gemma2-27B –∏ Qwen1.5-32B.</p>
<p>Qwen2.5-14B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU –∏ BBH, –≥–¥–µ –æ–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. Qwen2.5-32B –æ—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è, —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –û–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen1.5-32B, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>
<p>Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∞ –ø–æ MMLU-Pro –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-32B.</p>
<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5-7B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>
<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å Mistral-7B, Llama3-8B, Gemma2-9B –∏ Qwen2-7B. Qwen2.5-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–æ –º–Ω–æ–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—é. –û–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 74.2 –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU, 49.8 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MATH, –∏ 57.9 –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ HumanEval.</p>
<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-0.5B/1.5B/3B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>
<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-0.5B, 1.5B –∏ 3B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å Qwen2-0.5B/1.5B –∏ Gemma2-2.6B. Qwen2.5-0.5B, 1.5B –∏ 3B –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, Qwen2.5-0.5B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Gemma2-2.6B –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Instruction-tuned Model</div>
                <div class="summary_text"><p>–î–ª—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥. –ë–∞–∑–æ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–µ—Ç–∞–ª—å–Ω–µ–µ –∏–∑—É—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∞ —Ç–∞–∫–∂–µ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.</p>
<p>–î–∞–ª–µ–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 6 –∏ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
<p><strong>–¢–∞–±–ª–∏—Ü–∞ 6</strong> –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 70 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –±–æ–ª–µ–µ, –∞ —Ç–∞–∫–∂–µ Qwen2.5-Plus. –í —Ç–∞–±–ª–∏—Ü–µ —É–∫–∞–∑–∞–Ω—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö: MMLU-Pro, MMLU-redux, LiveBench, GPQA, MATH, GSM8K, HumanEval, MBPP, MultiPL-E, LiveCodeBench, IFEval, Arena-Hard –∏ MTbench. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–∞–¥–∞—á–∏ (General Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ (Mathematics &amp; Science Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é (Coding Tasks) –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º (Alignment Tasks).</p>
<p><strong>–¢–∞–±–ª–∏—Ü–∞ 7</strong> –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 14-30 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ Qwen2.5-Turbo. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ –∂–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á, —á—Ç–æ –∏ –≤ —Ç–∞–±–ª–∏—Ü–µ 6.</p>
<p>–ò–∑ —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ–±—â–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Open Benchmark Evaluation</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫ –∏ –æ—Ü–µ–Ω–∫–∞ –ª—é–¥—å–º–∏, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.</p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —É–º–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è MMLU-Pro, MMLU-redux –∏ LiveBench 0831. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è GPQA, GSM8K –∏ MATH. –ù–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é HumanEval, MBPP, MultiPL-E –∏ LiveCodeBench 2305-2409. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è IFEval.</p>
<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –¥–ª—è —á–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è MT-Bench –∏ Arena-Hard.</p>
<p>–í —Å—Ç–∞—Ç—å–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2.5-72B-Instruct –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Llama3.1-70B-Instruct, Llama3.1-405B-Instruct –∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è Qwen2-72B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—É—é –º–æ–¥–µ–ª—å Llama-3.1-405B-Instruct –ø–æ —Ä—è–¥—É –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è MMLU-redux, MATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard –∏ MTBench. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-72B-Instruct –ø–æ 9 –∏–∑ 13 –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.</p>
<p>–¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π Qwen2.5-Turbo, Qwen2.5-14B-Instruct –∏ Qwen2.5-32B-Instruct –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT4o-mini, Gemma2-27B-IT –∏ Qwen2-14B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-32B-Instruct –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å Qwen2.5-14B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å GPT-4o-mini. –ú–æ–¥–µ–ª—å Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-14B-Instruct –ø–æ –≤–æ—Å—å–º–∏ –∏–∑ –¥–µ—Å—è—Ç–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è —Å—Ä–µ–¥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>
<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, Gemma2-9B-IT –∏ Llama3.1-8B-Instruct, –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º IFEval. –û—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω—ã –µ—ë –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ (MATH: 75.5) –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ (HumanEval: 84.8). –ú–æ–¥–µ–ª—å Qwen2.5-3B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ Phi3.5-mini-instruct –∏ MiniCPM3-4B-Instruct –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª–∏ Qwen2.5-1.5B-Instruct –∏ Qwen2.5-0.5B-Instruct —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.</p>
<p>–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –≥–¥–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∑–Ω–∞–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">In-house Automatic Evaluation</div>
                <div class="summary_text"><p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT-4, Claude3.5-sonnet –∏ Llama-3.1. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ. –ò—Å–ø—ã—Ç–∞–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –∫–∞–∫ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>
<p><strong>–û—Ü–µ–Ω–∫–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö</strong></p>
<p>–°—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å –º–æ–¥–µ–ª–∏ Qwen2.5 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —Å –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
*   –ú–æ–¥–µ–ª—å Qwen2.5-0.5B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π Qwen2-1.5B.
*   Qwen2.5-3B —Å—Ä–∞–≤–Ω–∏–º–∞ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å Qwen2-7B.
*   Qwen2.5-32B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B.
*   –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ —Å GPT-4 –∏ Claude3.5-sonnet, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Llama-3.1-405B –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –∫—Ä–æ–º–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>
<p>–ú–æ–¥–µ–ª—å Qwen2.5-Plus —É–ª—É—á—à–∏–ª–∞ —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>
<p><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</strong></p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±—ã–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã:
1.  <strong>IFEval (Multilingual):</strong> –†–∞—Å—à–∏—Ä–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç IFEval –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –≤ —Å–µ–±—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –ü—Ä–∏ —ç—Ç–æ–º –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —è–∑—ã–∫–æ-–∑–∞–≤–∏—Å–∏–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏.
2.  <strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π:</strong> –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Å –ø–æ–º–æ—â—å—é –ø—è—Ç–∏ —Ç–µ—Å—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö MMLU, –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º (AMMLU), —è–ø–æ–Ω—Å–∫–æ–º (JMMLU), –∫–æ—Ä–µ–π—Å–∫–æ–º (KMMLU), –∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–æ–º (IndoMMLU) –∏ —Ç—É—Ä–µ—Ü–∫–æ–º (TurkishMMLU) —è–∑—ã–∫–∞—Ö. –¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ MMLU (okapi MMLU).
3.  <strong>MGSM8K (Extended):</strong> –Ø–∑—ã–∫–æ–≤–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ MGSM8K –±—ã–ª–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –∑–∞ —Å—á—ë—Ç –∞—Ä–∞–±—Å–∫–æ–≥–æ, –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ –∏ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤.
4.  <strong>–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –Ω—é–∞–Ω—Å—ã:</strong> –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ç–µ—Å—Ç BLEnD –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π.</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Qwen2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –Ω—é–∞–Ω—Å–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –Ω–æ —ç—Ç–∞ –æ–±–ª–∞—Å—Ç—å –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Reward Model</div>
                <div class="summary_text"><p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model, RM), –∫–æ—Ç–æ—Ä–∞—è –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –æ—Ü–µ–Ω–∫—É RM, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –≤ —Å–µ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: Reward Bench, RMB, PPE –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ (Human-Preference-Chinese). –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –º–æ–¥–µ–ª–∏ Nemotron-4-340BReward, Llama-3.1-Nemotron-70B-Reward –∏ Athene-RM70B.</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Llama-3.1-Nemotron-70B-Reward –ª–∏–¥–∏—Ä—É–µ—Ç –Ω–∞ Reward Bench, –∞ Athene-RM-70B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ RMB. –ú–æ–¥–µ–ª—å Qwen2.5-RM-72B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ PPE –∏ Human-Preference-Chinese, –∞ –Ω–∞ RMB –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –ø–æ—Å–ª–µ Athene-RM-70B. –ù–∞ Reward Bench –æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å Nemotron-4-340B-Reward, —Ö–æ—Ç—è –∏ –Ω–µ–º–Ω–æ–≥–æ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç Llama-3.1-Nemotron-70B-Reward.</p>
<p>–ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–∞—Å—Ç–æ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ Reward Bench. –û–¥–Ω–∞–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —É—Ö—É–¥—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö. –≠—Ç–æ—Ç —ç—Ñ—Ñ–µ–∫—Ç, –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–∞–∫ –∑–∞–∫–æ–Ω –ì—É–¥—Ö–∞—Ä—Ç–∞, –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ RM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.</p>
<p>–ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RM –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ RL-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ–±—É—á–µ–Ω—ã —Å –∏—Ö –ø–æ–º–æ—â—å—é. –¢–æ –µ—Å—Ç—å, –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö RM –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å RL-–º–æ–¥–µ–ª–µ–π. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Long Context Capabilities</div>
                <div class="summary_text"><p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5 –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç—Ä–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: RULER, LV-Eval –∏ Longbench-Chat. –í LV-Eval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å "keyword recall" (–ø–æ–ª–Ω–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤), —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 16 –∏ 17.</p>
<p>–ò–∑ —ç—Ç–∏—Ö —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5, –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –¥–ª–∏–Ω—ã (DCA + YARN), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4o-mini –∏ GPT-4.</p>
<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 2, –º–æ–¥–µ–ª—å Qwen2.5-Turbo –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è passkey (–∫–æ–¥–æ–≤–∞—è —Ñ—Ä–∞–∑–∞) –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ—ë –≤—ã—Å–æ–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.</p>
<p>–î–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, –±—ã–ª –≤–Ω–µ–¥—Ä–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention). –î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ 12.5 —Ä–∞–∑. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT) –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen2.5-Turbo –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ 3.2-4.3 —Ä–∞–∑–∞.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Conclusion</div>
                <div class="summary_text"><p>–ú–æ–¥–µ–ª—å Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –±—ã–ª–∞ —É–ª—É—á—à–µ–Ω–∞ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–≤—ã—à–∞—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏, –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –¥–µ–ª–∞–µ—Ç Qwen2.5 –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>
<p>Qwen2.5 –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –æ—Ç 0,5 –¥–æ 72 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã MoE (Mixture of Experts), —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen2.5-Turbo –∏ Qwen2.5-Plus. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Qwen2.5-72B-Instruct –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Llama-3-405B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–Ω–∞ –≤ —à–µ—Å—Ç—å —Ä–∞–∑ –º–µ–Ω—å—à–µ. Qwen2.5 —Ç–∞–∫–∂–µ —Å–ª—É–∂–∏—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–≤–æ—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>
<p>–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≥–∏–±–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —à–∏—Ä–æ–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Qwen2.5 –¥–µ–ª–∞—é—Ç –µ–µ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –∫–∞–∫ –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ç–∞–∫ –∏ –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π, –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É—è –µ–µ –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–≥–æ –∏–≥—Ä–æ–∫–∞ –≤ –±—É–¥—É—â–∏—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö.</p>
<p>–í –±—É–¥—É—â–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –Ω–∞–¥–µ–∂–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∏ –±—É–¥—É—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ, —Ç–∞–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–Ω–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å—Ç—Ä–µ–º—è—Å—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ—Å—à–æ–≤–Ω—É—é —Å–∫–≤–æ–∑–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–π, –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Å–ª—É—Ö–æ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö. –í-—Ç—Ä–µ—Ç—å–∏—Ö, –æ–Ω–∏ –Ω–∞–º–µ—Ä–µ–Ω—ã –ø–æ–≤—ã—Å–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–≤–æ–∏—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞. –≠—Ç–∏ —É—Å–∏–ª–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ç–µ–∫—É—â–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≤–∫–ª–∞–¥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ü–µ–ª–æ–º.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-12-23 04:38',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-23 04:38')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-23 04:38')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    