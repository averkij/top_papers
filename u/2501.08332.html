
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 января</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-14.html">⬅️ <span id="prev-date">14.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-16.html">➡️ <span id="next-date">16.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 января', 'en': 'January 15', 'zh': '1月15日'};
        let feedDateNext = {'ru': '16.01', 'en': '01/16', 'zh': '1月16日'};
        let feedDatePrev = {'ru': '14.01', 'en': '01/14', 'zh': '1月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2501.08332', 'title': 'MangaNinja: Line Art Colorization with Precise Reference Following', 'url': 'https://huggingface.co/papers/2501.08332', 'abstract': 'Derived from diffusion models, MangaNinjia specializes in the task of\nreference-guided line art colorization. We incorporate two thoughtful designs\nto ensure precise character detail transcription, including a patch shuffling\nmodule to facilitate correspondence learning between the reference color image\nand the target line art, and a point-driven control scheme to enable\nfine-grained color matching. Experiments on a self-collected benchmark\ndemonstrate the superiority of our model over current solutions in terms of\nprecise colorization. We further showcase the potential of the proposed\ninteractive point control in handling challenging cases, cross-character\ncolorization, multi-reference harmonization, beyond the reach of existing\nalgorithms.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-01-14', 'pub_date_card': {'ru': '14 января', 'en': 'January 14', 'zh': '1月14日'}, 'hash': '20ea6b75639e2ced', 'authors': ['Zhiheng Liu', 'Ka Leong Cheng', 'Xi Chen', 'Jie Xiao', 'Hao Ouyang', 'Kai Zhu', 'Yu Liu', 'Yujun Shen', 'Qifeng Chen', 'Ping Luo'], 'affiliations': ['Ant Group', 'HKU', 'HKUST', 'Tongyi Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2501.08332.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Прецизионное раскрашивание манги с помощью ИИ', 'desc': 'MangaNinjia - это модель для раскрашивания линейных рисунков манги, основанная на диффузионных моделях. Она использует модуль перемешивания патчей для обучения соответствиям между цветным изображением-образцом и целевым линейным рисунком. Модель также включает схему точечного контроля для точного подбора цветов. Эксперименты показывают превосходство MangaNinjia над существующими решениями в точности раскрашивания.'}, 'en': {'title': 'MangaNinjia: Mastering Line Art Colorization with Precision', 'desc': 'MangaNinjia is a model designed for coloring line art by using reference images. It employs a patch shuffling module to help the model learn how to match colors from the reference image to the target line art accurately. Additionally, it features a point-driven control scheme that allows for detailed color adjustments, ensuring that colors are applied precisely. Our experiments show that MangaNinjia outperforms existing methods in colorization tasks, especially in complex scenarios involving multiple references and different characters.'}, 'zh': {'title': 'MangaNinjia：精准上色的新方法', 'desc': 'MangaNinjia 是一种基于扩散模型的参考引导线条艺术上色技术。我们设计了两个模块来确保角色细节的准确转录，包括补丁洗牌模块和点驱动控制方案，以实现精细的颜色匹配。实验结果表明，我们的模型在精确上色方面优于现有解决方案。我们还展示了所提议的交互式点控制在处理复杂案例和多参考协调方面的潜力，超越了现有算法的能力。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.', 'summary': '<p>В этой работе представлен метод MangaNinjia, который основан на диффузионных моделях и специализируется на раскрашивании контурных рисунков (лайн-арта) с использованием цветовой палитры из другого, эталонного изображения. </p>\n<p>Для точного переноса деталей персонажа из эталонного изображения в раскрашиваемый контур, авторы используют два ключевых подхода:</p>\n<ol>\n<li><strong>Модуль перемешивания патчей (patch shuffling module)</strong>: Этот модуль помогает установить соответствие между цветовыми областями эталонного изображения и областями контурного рисунка. Проще говоря, он находит, какие части эталонного изображения соответствуют каким частям контура, чтобы правильно перенести цвет.</li>\n<li><strong>Управление цветом на основе точек (point-driven control scheme)</strong>: Этот механизм позволяет более точно контролировать процесс раскрашивания. Пользователь может указать конкретные точки на контурном рисунке и задать, какой цвет должен быть в этих точках, что дает возможность точной настройки цветовой гаммы.</li>\n</ol>\n<p>Результаты экспериментов на специально собранном наборе данных показывают, что MangaNinjia превосходит существующие методы в точности раскрашивания. Более того, авторы демонстрируют, как интерактивное управление цветом с помощью точек позволяет справляться со сложными случаями, такими как раскрашивание персонажей из разных источников, гармонизация нескольких эталонных изображений, что недоступно для других алгоритмов.</p>\n<p><strong>Комментарий:</strong> <em>Диффузионные модели, на которых основан MangaNinjia, являются мощным инструментом для генерации изображений. Использование эталонного изображения позволяет контролировать цветовое решение, а добавленные модули делают процесс более точным и управляемым.</em></p>'}, {'title': 'Introduction', 'content': 'ation, etc., beyond the reach of existing algorithms. 1. Introduction Reference-based line art colorization aims to transform line art image into color image, maintaining consistency with the reference image [8, 9, 32, 41, 82]. This technique is in high demand for comics, animation, and various other content creation applications [10, 23, 33, 81, 83]. Unlike methods that rely solely on strokes, palettes, or text conditions [25, 62, 75], reference-based line art colorization excels in preserving both identity and semantic meaning as shown in Fig. 1, which is crucial for comics and manga. Existing approaches [8, 32] have explored referencebased colorization with fused attention mechanisms. However, these methods exhibit two main limitations. First, substantial variations between the line art and reference image often lead to semantic mismatches or confusion of colorization. Hence, these approaches typically demand high standard for the reference image, requiring it to closely resemble the line art, which is impractical for real-world applications. Second, existing methods lack precise control, resulting in the loss of crucial details from the reference image during the colorization process. In this paper, we introduce MangaNinja, consisting of dual-branch structure for correspondences finding between the reference and line art images by leveraging the rich diffusion priors through cross attention. Observing that the basic dual-branch design tends to transfer global style rather than matching local semantics, we propose patch shuffling module, which divides the reference image into patches to encourage local matching capabilities of the model. The patch shuffling pushes our model out of its comfort zone during optimization, facilitating it to learn an implicit matching capability that effectively handles disparities between the input line art and reference image. However, such semantic correspondence can still suffer from ambiguity, especially when color images include details that are hard to capture in line art (e.g., nose shading in Fig. 2a), when some elements in the line art occupy only small area of the reference image (e.g., shoulder garment pattern in Fig. 2a), or when significant variations and complex compositions create semantic confusion (e.g., multiple characters in Fig. 2b). To further support finergrained coloring matching, we introduce point-driven control scheme powered by PointNet, which offers detailed control using user-defined cues in an interactive manner. During experiments, we find that point control only works when the model is aware of local semantics, highlighting the importance and effectiveness of patch shuffling. We take advantage of the inherently natural semantic correspondences and visual variances presented in anime videos to construct training data pairs. Specifically, we randomly select two frames from video: one serves as the reference for the Reference U-Net, while the other, along with its line art version, acts as the target and input for the Denoising U-Net. As for the explicit correspondence, we employ an off-the-shelf model to label matching points in the training image pairs, encode these points with PointNet, and integrate them into the main branch via attention. With our carefully designed patch shuffling strategy and point-driven control scheme, MangaNinja effectively manages challenging scenarios, such as varying poses or details missing between reference and line art, multi-reference inputs, and colorization with discrepant references, as shown in Sec. 4.3. It excels in complex colorization tasks, producing high-quality results from line art while accurately preserving character identity, as demonstrated in Fig. 1. For fair and systematic evaluation, we construct comprehensive benchmark for line art colorization. Our extensive quantitative and qualitative experiments demonstrate that our approach outperforms existing baselines, achieving state-of-the-art results in visual fidelity and identity preservation, making it beneficial for comics, animation, and various content creation applications. 2. Related Work 2.1. Line Art Colorization Line art colorization aims to fill the blank regions of line art with appropriate colors. Currently, several user-guided colorization techniques exist, including text prompts [7, 27, 80], scribble [7, 14, 37, 53, 78, 79], and reference image [13, 31, 32, 66, 67, 81]. However, text-based and scribble methods have limitations in achieving precise color filling for the overall line art. Existing reference-based colorization approaches often have limited performance due to inaccurate structural and semantic matching, particularly when there are substantial differences between the reference image and the line art. Moreover, in practical applications, more complex scenarios arise, such as requiring multiple reference images to handle the colorization of various elements in the line art. Consequently, it is challenging to seamlessly integrate the existing line art colorization methods into the animation industry workflow. Our approach leverages priors from pretrained diffusion models and enhances the models matching capabilities by learning from video data, allowing users to accomplish complex colorization tasks with simple point guidance. 2.2. Visual Correspondence In computer vision, correspondence [76] involves identifying and matching related features or points across different images, often used for tasks such as stereo vision [1, 45, Figure 2. Visualization of point guidance. By introducing points as guidance, MangaNinja can tackle many challenging tasks, such as when there are significant variations between reference images and line art while preserving details. See more in Sec. 4.3. 54, 55], motion tracking [16, 71]. Traditional methods use hand-crafted features [4, 40] to find correspondences, whereas recent deep learning approaches [12, 22, 28, 30] leverage supervised learning with labeled data to learn matching capabilities. However, due to the requirement for precise pixel-level annotations, these methods struggle to scale up, as such detailed labeling is challenging and expensive. Later, researchers begin exploring the establishment of weakly supervised [63] or self-supervised [24, 64] visual correspondence models. Recent studies [19, 48, 59] show that the rich priors inherent in the latent representations of generative pretrained models like GAN [17] and Diffusion [57] models can be utilized to identify visual correspondence. Leveraging the inherent rich priors of correspondences in pre-trained diffusion models, our method achieves reference-based colorization by learning to match between line art and reference images. 2.3. Diffusion-based Consistent Generation Consistent generation based on pretrained diffusion models can be categorized into three main directions. The first direction leverages training-free or rapid fine-tuning strategy for image editing [3, 5, 6, 20, 26, 34, 36, 42, 43, 56, 61], where they conduct global or local editing by modifying text prompts or introducing new guidance to adjust the attention layers. However, they generally struggle with robustness in challenging scenarios and rely heavily on the input guidance signals. The second direction is customized generation [2, 15, 18, 29, 38, 39, 51, 52, 60], which generally involves fine-tuning on 3 to 5 example images per concept, where some methods may take about half an hour for single concept. The third direction involves further training the pretrained diffusion model with extensive domain-specific data, learning to incorporate encoded image features into the main denoising network [46, 68, 74, 77]. For instance, Paint-by-Example [72] and ObjectStitch [58] utilize CLIP [49] to encode images for extracting object representations, while AnyDoor [11] collects training samples from videos and employs the DINOv2 [44] as the image encoder. However, these methods primarily focus on general objects in images, lacking fine-grained matching capabilities. 3. Method 3.1. Overall Pipeline The overall framework of MangaNinja is presented in Fig. 3. Our goal is to match and colorize, producing vibrant anime image Itarget from line art Iline and reference image Iref of the same character. Additionally, users can pre-define specific points Pref on the reference image and their corresponding points Pline on the line art. Guided by the matching points, the model ensures color consistency during the colorization process, thereby achieving fine-grained control and excellent performance even in challenging scenarios. Anime video sequences inherently present identity consistency across frames while simultaneously exhibiting various spatial and temporal transformations. These transformations include, but are not limited to, scale variations (e.g., zooming effects), changes in object orientation, and alterations in pose. Thanks to such property, we construct training image pairs by randomly sampling two distinct frames from video clip. The first frame serves as the reference, and we employ an off-the-shelf line art extraction model [80] to derive the line art from the second frame, which serves as the target image. During training, we use LightGlue [35], state-of-the-art point-matching algorithm, to extract corresponding point pairs between two frames. 3.2. Architecture Design Reference U-Net. Given the stringent detail requirements in line art colorization, the main challenge is how to effectively encode the reference image for finer-grained Figure 3. The training process of MangaNinja. We randomly select two frames from video data, using one frame as reference image and extracti', 'summary': '<h2>Изложение раздела "Введение" статьи о MangaNinja</h2>\n<p>Статья посвящена задаче колоризации контурных рисунков на основе эталонного изображения. Этот метод очень востребован в создании комиксов, анимации и других видов контента. В отличие от методов, которые полагаются только на штрихи, палитры или текстовые подсказки, колоризация по эталону позволяет сохранить как идентичность персонажа, так и его семантическое значение, что особенно важно для комиксов и манги.</p>\n<p>Существующие подходы к колоризации по эталону используют механизмы внимания, но имеют два основных недостатка. Во-первых, значительные различия между контурным рисунком и эталонным изображением часто приводят к семантическим несоответствиям или путанице в цветах. Поэтому эти методы требуют, чтобы эталонное изображение было очень похоже на контурный рисунок, что непрактично в реальных условиях. Во-вторых, существующие методы не обеспечивают точного контроля, что приводит к потере важных деталей из эталонного изображения в процессе колоризации.</p>\n<p>В данной работе представлен метод MangaNinja, который использует двухканальную структуру для нахождения соответствий между эталонным и контурным рисунками, опираясь на богатые диффузионные априорные знания через кросс-внимание.  Для решения проблемы переноса глобального стиля вместо сопоставления локальной семантики, авторы предлагают модуль перемешивания патчей. Этот модуль разделяет эталонное изображение на патчи, чтобы усилить способность модели к локальному сопоставлению. Перемешивание патчей выводит модель из зоны комфорта во время обучения, помогая ей научиться неявному сопоставлению, которое эффективно справляется с различиями между контурным рисунком и эталонным изображением.</p>\n<p>Однако, семантическое соответствие все еще может быть неоднозначным, особенно когда цветные изображения содержат детали, которые трудно передать в контурном рисунке, или когда некоторые элементы на контурном рисунке занимают небольшую площадь на эталонном изображении, а также при значительных различиях и сложных композициях. Для более точного сопоставления цветов авторы вводят схему управления точками, основанную на PointNet. Этот метод позволяет пользователям интерактивно задавать точки-подсказки для более детального контроля. Эксперименты показали, что управление точками работает только тогда, когда модель учитывает локальную семантику, что подчеркивает важность и эффективность перемешивания патчей.</p>\n<p>Для обучения модели авторы используют видео аниме, так как они содержат естественные семантические соответствия и визуальные различия. Из видео случайным образом выбираются два кадра: один используется в качестве эталона, а другой, вместе со своей версией в виде контурного рисунка, служит целевым изображением и входом для модели. Для определения точного соответствия используется готовая модель, которая размечает соответствующие точки на обучающих парах изображений. Эти точки кодируются с помощью PointNet и интегрируются в основную ветвь через механизм внимания.</p>\n<p>Благодаря разработанной стратегии перемешивания патчей и схеме управления точками, MangaNinja эффективно справляется со сложными сценариями, такими как различия в позах или отсутствие деталей между эталонным и контурным рисунками, использование нескольких эталонов и колоризация с несоответствующими эталонами. Модель отлично справляется со сложными задачами колоризации, создавая высококачественные результаты из контурных рисунков, точно сохраняя идентичность персонажей. Для объективной оценки был создан всесторонний бенчмарк для колоризации контурных рисунков. Результаты экспериментов показали, что предложенный подход превосходит существующие методы, достигая наилучших результатов в плане визуальной точности и сохранения идентичности, что делает его полезным для создания комиксов, анимации и другого контента.</p>'}, {'title': 'Enhancing image colorization with reference u-net and pointnet', 'content': 'ng the line art from the other. Both frames are input into the Reference U-Net and the Denoising U-Net, respectively. To enhance the models automatic matching and fine-grained control capabilities, we propose series of training strategies, including progressive patch shuffling. Additionally, we employ an off-the-shelf model to extract matching points from the two frames, and these point maps are fed into the main branch through PointNet. feature extraction. Recent studies [21, 70] demonstrate the effectiveness of leveraging an additional U-Net architecture to address this issue, and we are inspired to introduce Reference U-Net using similar design. After encoding the reference image into 4-channel latent representation using VAE, it is fed into the Reference U-Net to extract multi-level features for fusion with the main Denoising U-Net. Specifically, we concatenate the key and value from the self-attention layers of both the reference and denoising branches, as described in Eq. (1), injecting the multi-level reference features into the corresponding layers of the Denoising U-Net. Attn = softmax( Qtar [Ktar, Kref ] )[Vtar, Vref ]. (1) Denoising U-Net. The main branch utilizes the Reference U-Net and PointNet as conditions for image colorization. We extract the line art from the images using LineartAnimeDetector [80], then replicate the single-channel line art three times to input into the variational autoencoder (VAE) for compression into the latent space. Next, we concatenate this with the noisy image latent, resulting in total of 8 channels. Additionally, we experiment with sending the line art through ControlNet [80] and find that both approaches yield comparable performance. For resource efficiency, we opt for the first method. Furthermore, we replace the original text embeddings with image embeddings extracted from the CLIP encoder. Progressive patch shuffle for local matching. Although we inject the reference image features layer by layer into the Denoising U-Net, we observe that the strong structural cues provided by the line art enable easy coarse global matching, which hinders the learning of detailed matching ability. To address this, we propose progressive patch shuffle strategy. Specifically, we divide the reference image into multiple small patches and randomly shuffle them to disrupt the overall structural coherence, as shown in Fig. 3. The idea behind this technique is to encourage the model to focus more on smaller patches (even at the pixel level) within the reference image to achieve finer-grained, local matching abilities rather than global ones. Moreover, we adopt coarse-to-fine learning scheme by progressively increasing the number of randomly shuffled patches from 22 to 3232. Apart from the shuffling technique, we also employ some common data augmentation techniques, such as random flipping and rotation, to increase the variation between the reference and target image. 3.3. Fine-grained Point Control However, such semantic correspondence can still suffer from ambiguity, especially when color images contain details that are difficult to capture in line art. Moreover, users often require simple interactive method to handle complex tasks. To address this, we design point-based fine-grained control mechanism and propose series of strategies to enhance the effectiveness of point control. Point embedding injection. We represent user-specified matching point pairs using two point maps, each being 4 single-channel matrix matching the input images resolution. For each matching point pair, we assign the same unique integer values to their respective coordinates on both point maps, with all other positions set to 0. During training, we randomly select up to 24 matching point pairs, with the option to select zero points as well. Hence, users can opt not to indicate matching points for control during inference, instead fully relying on the autonomous matching capability of the model. We propose PointNet composed of multiple convolutional layers and SiLU activation functions to encode the point maps as multi-scale embeddings. Similarly, the point embeddings Etar and Eref are integrated into the main branch via cross-attention mechanism by adding them to the query and key, as described in Eq. (2): Attn = softmax( tar, tar[K ref ] )[Vtar, Vref ], (2) tar = Qtar+Etar, tar = Ktar+Etar, and where Kref + Eref . Multi classifier-free guidance. To individually control the guiding strength of the reference image and the points during the generation inference process, we employ multiple classifier-free guidance: ref = ϵθ(zt, cref , cpoints) = ϵθ(zt, , ) + ωref + ωpoints (cid:0)ϵθ(zt, cref , ) ϵθ(zt, , )(cid:1) (3) (cid:0)ϵθ(zt, cref , cpoints) ϵθ(zt, cref , )(cid:1), where cref denotes the condition input from the reference image via the Reference U-Net, while cpoints denotes the condition input from the user-specified points through the PointNet. Increasing ωref makes the model rely more on its automatic matching capabilities. However, when we want to use points as guidance to accomplish more complex tasks (see Sec. 4.3), we should increase wpoints to amplify the influence of the points. Condition dropping. To enhance the models reliance on sparse point-based control signals, we randomly drop the line art condition during training. Without the structural guidance of the line art, we prompt the model to reconstruct the target image Itarget from the reference image Iref , relying solely on the sparse yet precise matching indicated by the point pairs Pref and Pline. This helps compel our model to learn the precise point-based control more effectively. Two-stage training. To further amplify the effects of precise point-based control, we design two-stage training strategy. In the first stage, we adopt condition dropping for both the reference image and point signals for unconditional generation, where the model concurrently learns the abilities to extract corresponding reference features and leverage the specified point correspondences for colorization. In the second stage, we only train the PointNet module, thereby enhancing the ability of PointNet to encode point maps, leading to stronger point control. 3.4. Evaluation Benchmark Existing works such as BasicPBC [13] and Animediffusion [8] design test sets that focus only on specific domains, involve minimal discrepancy between the reference and images, and evaluate using inconsistent metrics. target Therefore, we see the crucial need to establish comprehensive and consistent evaluation benchmark. We construct benchmark to systematically evaluate the performance of line art colorization. Specifically, we collect 200 image pairs of the same character from various anime, encompassing both human and non-human characters with diverse facial expressions and appearances, including attire. Each evaluation sample consists of target image with its line art estimated using an off-the-shell LineartAnimeDetector model [80] and reference image as colorization guidance. In the colorization process, the focus is typically on the foreground character portions, so we segment all images to extract the foreground subjects. Moreover, we follow the methodology outlined in DreamBooth [51] to calculate the CLIP [49] and DINO [44] semantic image similarities between the generated images and the ground truth. Furthermore, to assess the quality of the generated images, we compute the Peak Signal-to-Noise Ratio (PSNR) and the Multi-Scale Structural Similarity Index (MS-SSIM) [65]. Meanwhile, to evaluate coloring accuracy in complex taskssuch as those involving multiple references or colorization with differing reference points mentioned in Sec. 4.3we require more granular evaluation at the pixel level. Specifically, we annotate 50 predefined pairs of matching points for each image pair; for evaluation we employ the mean squared error (MSE) for the 3 3 patches centered around each pair of matching points. 4. Experiments 4.1. Implementation Details Training details. For training MangaNinja , we utilize dataset, sakuga-42m [47], which comprises 42 million keyframes covering wide range of artistic styles, geographical regions, and historical periods. We eliminate excessively similar duplicate frames by calculating the Structural Similarity Index (SSIM). Furthermore, we set the frame interval between the reference and target frames to 36, excluding videos that are too short. Ultimately, we retain 300,000 video clips. We initialize both the Reference and Denoising U-Net with pre-trained weights sourced from Stable Diffusion 1.5 [50] The training process spans 200k steps (with the first stage lasting 180k steps and the second stage 20k steps), starting with an initial learning rate Figure 4. Qualitative comparisons. We compare our method with the state-of-the-art non-generative colorization method BasicPBC, the consistency generation method IP-Adapter, and AnyDoor. The results demonstrate that our method significantly outperforms them in terms of colorization accuracy and generated image quality. Notably, our method does not use points for guidance in the generated results. of 103, which decays every 30k steps. The entire training process is completed within one day using eight A100-80G GPUs. 4.2. Comparisons In this section, we compare with the current state-of-the-art line art colorization method, BasicPBC [13]. Additionally, we also conduct comparisons with several generative methods that can achieve similar functions. These include IPAdapter [73], which serves as an adapter to enhance the image prompting capabilities of pretrained text-to-image diffusion models, and Anydoor [11], zero-shot object', 'summary': '<h2>Изложение раздела статьи о методе MangaNinja</h2>\n<p>В статье представлен метод MangaNinja для колоризации изображений на основе контуров (лайн-арт), использующий в качестве референса другое изображение. Модель состоит из двух основных U-Net архитектур: Reference U-Net и Denoising U-Net.</p>\n<p><strong>Reference U-Net</strong> обрабатывает референсное изображение, кодируя его в 4-канальное латентное представление с помощью вариационного автоэнкодера (VAE). Затем эта информация используется для извлечения многоуровневых признаков, которые в дальнейшем объединяются с Denoising U-Net. Для этого используются механизмы внимания: ключи и значения из слоев самовнимания обеих сетей конкатенируются, что позволяет передавать контекст референсного изображения в соответствующие слои Denoising U-Net.</p>\n<p><strong>Denoising U-Net</strong> является основной ветвью модели и отвечает за непосредственно колоризацию. На вход подается латентное представление контура, полученное из исходного изображения, а также латентное представление зашумленного изображения. Контур извлекается с помощью LineartAnimeDetector, а затем реплицируется в три канала для подачи в VAE. В качестве альтернативы, контур можно пропустить через ControlNet, но для экономии ресурсов используется первый подход. Текстовые эмбеддинги заменены на эмбеддинги изображений, извлеченные из CLIP-энкодера.</p>\n<p>Для улучшения способности модели к сопоставлению деталей, используется <strong>прогрессивное перемешивание патчей</strong>. Референсное изображение делится на небольшие патчи, которые затем случайным образом перемешиваются. Это нарушает общую структурную целостность изображения и заставляет модель обращать больше внимания на локальные соответствия, а не на глобальные. Количество перемешиваемых патчей постепенно увеличивается в процессе обучения, что соответствует стратегии обучения от общего к частному. В дополнение к этому, применяются стандартные методы аугментации данных, такие как случайные повороты и отражения.</p>\n<p>Для более точного управления процессом колоризации, вводится <strong>механизм точечного контроля</strong>. Пользователь задает пары соответствующих точек на референсном и целевом изображениях. Эти точки кодируются в виде двух карт точек, где каждой точке присваивается уникальное целочисленное значение. Карты точек обрабатываются с помощью <strong>PointNet</strong>, который извлекает многомасштабные эмбеддинги. Эти эмбеддинги интегрируются в Denoising U-Net через механизмы кросс-внимания.</p>\n<p>Для контроля влияния референсного изображения и точек на процесс генерации используется <strong>множественное управление без классификатора</strong>. Это позволяет регулировать, насколько модель полагается на автоматическое сопоставление и на заданные пользователем точки. Для усиления влияния точечного контроля, во время обучения случайно отбрасывается условие контура, что заставляет модель полагаться только на точечные соответствия.</p>\n<p>Обучение состоит из <strong>двух этапов</strong>. На первом этапе отбрасываются условия как референсного изображения, так и точечные сигналы, что позволяет модели учиться извлекать признаки и использовать точечные соответствия для колоризации. На втором этапе обучается только модуль PointNet, что усиливает его способность кодировать карты точек и, соответственно, улучшает точечный контроль.</p>\n<p>Для оценки качества работы модели, был создан <strong>бенчмарк</strong>, включающий 200 пар изображений персонажей из аниме. Для каждой пары есть целевое изображение, его контур и референсное изображение. Оценка качества включает в себя расчет семантического сходства между сгенерированными и оригинальными изображениями с использованием CLIP и DINO, а также метрики PSNR и MS-SSIM. Кроме того, для оценки точности колоризации на уровне пикселей, используются 50 аннотированных пар точек соответствия, для которых вычисляется среднеквадратичная ошибка (MSE) в окрестности 3x3 пикселя.</p>\n<p>Эксперименты проводились на датасете sakuga-42m, содержащем 42 миллиона кадров из аниме. Для обучения использовались 300 000 видеоклипов. Модели Reference U-Net и Denoising U-Net инициализированы весами из Stable Diffusion 1.5. Обучение проводилось в течение 200 000 шагов с начальной скоростью обучения 10^-3.</p>\n<p>В разделе сравнения метод MangaNinja сравнивается с BasicPBC, IPAdapter и Anydoor. Результаты показывают, что MangaNinja превосходит их по точности колоризации и качеству сгенерированных изображений.</p>'}, {'title': 'Comparative analysis of image colorization methods', 'content': 'level image customization method. In addition, we will discuss the cartoon interpolation method ToonCrafter [69] in the supplementary materials, as the official repository has not yet released its colorization function and exhibits poor performance when there are significant discrepancies. Qualitative comparison. We visualize the comparison results in Fig. 4. BasicPBC samples colors in the vicinity of the corresponding area in the line art; hence, the generated results can be unsatisfactory when there is large discrepancy between the reference and the line art. Moreover, as the model itself does not have generative capability, it does not perform well in handling light and shadow. For generative methods, we introduce controlnet for the IPAdapter and AnyDoor, and carefully annotate the masks of the reference region, then feed them to AnyDoor. Leveraging the strong prior knowledge of pre-trained models, the generated results become more natural. Compared to the IP-Adapter, AnyDoor better retains the color details of the reference image. However, neither method possesses finegrained matching capability and can only achieve coarse colorization results, leading to serious color confusion. Notably, our method does not use points for guidance in the generated results. This is because, during the training process, our method learns from image pairs in video data and gradually shuffles the reference image at the patch level from simple to complex, which endows the model with excellent matching capability. Benefiting from the design of the point, as shown in Sec. 4.3, our method also excels in some more complex scenarios. Quantitative comparison. We conduct quantitative comparison using our constructed benchmark. It is worth noting that this benchmark contains 200 pairs of images, which means we perform total of 400 inferences (interchanging the reference image and ground truth). The results are 6 Table 1. Quantitative comparison. MangaNinja demonstrates superior performance across both objective and perceptual metrics. AnyDoor: without mask; AnyDoor*: with mask. Ours: no point guidance; Ours (full): with point guidance. Method DINO CLIP PSNR MS-SSIM LPIPS BasicPBC [13] 42.64 IP-Adapter [73] 55.42 51.36 Anydoor [11] 63.79 AnyDoor* [11] Ours Ours (full) 68.23 69.91 79.64 82.39 80.73 83.91 88.34 90.02 17.58 16.19 15.12 16.24 20.37 21.34 0.894 0.845 0.827 0. 0.962 0.972 0.33 0.30 0.32 0.27 0.22 0.21 presented in Tab. 1. The results indicate that the BasicPBC outperforms generative methods in pixel-level evaluation metrics. However, it is noteworthy that BasicPBC performs weaker in terms of image feature similarity metrics compared to these methods. Additionally, Anydoor requires manual labeling of masks in reference images to achieve better performance. In contrast, our approach demonstrates significant advantage over previous methods in both pixellevel and image feature similarity metrics. 4.3. Challenging Cases with Point Guidance Varying poses or missing details. As shown in Fig. 5, we present some more challenging examples of line art colorization. As demonstrated in the first row, even with substantial variations between the line art and the reference image, excellent colorization can be achieved with points serving as guidance. Furthermore, the reference image sometimes lacks certain elements present in the line art, as exemplified in the first column of the second row. The line art includes the complete garment, but the reference image only provides the upper half. With MangaNinja , users can color the lower half of the clothes guided by points, utilizing the upper half from the reference image. Finally, as shown in the second column of the second row, there may be multiple objects in the line art that interact with each other. Segmenting the line art and coloring each part separately can sometimes result in inaccuracies and additional costs. However, with the guidance of points, MangaNinja can achieve one-time colorization of multiple objects with good performance. Multi-ref colorization. As demonstrated in Fig. 6, in practical applications, single reference image may not always encompass all the elements in line art that require colorization. Benefiting from the point-guided design, our method allows for the simultaneous use of multiple reference images for colorization. Specifically, Users can combine multiple images and input them into Reference UNet, which then employs points to match different regions from the reference images with corresponding elements in the line art. This approach facilitates many-to-one colorizaFigure 5. Visualization of varying poses or missing details. With point guidance, MangaNinja can tackle many challenging cases. For instance, in the first two rows, there are significant variations between the reference image and line art. Furthermore, users can employ point guidance to colorize regions or elements with no matches in the reference; for example, the lower parts of the clothing are missing in the reference image of the third sample. When dealing with multiple objects, point guidance effectively prevents color confusion, as demonstrated in the last row. 6. Visualization of multi-ref colorization. Figure MangaNinja enables users to select specific areas from multiple reference images through points, providing guidance for all elements in the line art. Additionally, it effectively resolves conflicts between similar visual elements across the reference images. tion and effectively resolves content conflicts among the various reference images. 7 Table 2. Ablation study on the effect of various training strategies. The first five evaluation metrics assess the overall quality of the coloring results, while the MSE metric evaluates the coloring accuracy at the specified matching pixels. The base model indicates training solely with video data, without employing any strategies; the full model incorporates all training strategies. Gray numbers in parentheses represent statistics assessed without point guidance. DINO Sim CLIP Sim PSNR MS-SSIM LPIPS MSE I. base model II. base model + condition dropping III. base model + progressive patch shuffle IV. base model + multi cfg V. base model + two-stage training 64.13 (63.91) 64.92 (64.79) 67.78 (67.12) 64.63 64.32 85.05 (84.75) 85.44 (85.22) 87.42 (86.93) 86.02 86.34 18.12 (18.02) 19.02 (18.61) 20.18 (19.72) 18.74 19.36 0.914 (0.912) 0.941 (0.929) 0.956 (0.952) 0.943 0. 0.26 (0.27) 0.25 (0.25) 0.23 (0.23) 0.24 0.24 0.0151 0.0125 0.0091 0.0133 0.0113 VI. full model 69.91 (68.23) 90.02 (88.34) 21.34 (20.37) 0.972 (0.962) 0.21 (0.22) 0.0072 Colorization with references of different characters. MangaNinja is trained on large number of image pairs from video data, which provides it with semantic matching capability and excellent generalization properties. Moreover, by utilizing point guidance, we can achieve precise colorization. Consequently, even when the reference image and the line art are different characters, the model can still perform colorization effectively. As illustrated in Fig. 7, users can take advantage of this capability and engage in an interactive process to explore and find inspiration for colorization. 4.4. Ablation Studies Ablation of training strategies. We conduct series of ablation studies in Tab. 2 to investigate how different training strategies impact the colorization performance and matching capability of our model. The first five metrics assess the overall quality of the colorization, while the MSE measures the accuracy of the color predictions at the pixel locations of the guiding points. The ablation performance of our model with point guidance is shown in black; to further demonstrate the models ability for automatic color matching, we present the ablation results as gray numbers in parentheses, representing evaluations done without point guidance. The ablation experiments demonstrate that all strategies contribute to improving point-guided generation, enabling our method to address broader range of complex tasks. Notably, even without using points as guidance, both condition dropping and progressive patch shuffle enhance the models automatic matching capability, with the latter yielding the most notable improvement. Specifically, it disrupts the reference images inherent structural patterns during training, enabling the model to learn local matching capabilities. Only after learning this local matching ability does the effect of point guidance become clearly evident. Meanwhile, we provide further analysis of the progressive patch shuffle in the supplementary materials. 8 Figure 7. Visualization of colorization with discrepant reference. Our method uses points as guidance to achieve semantic color matching with fine control. We believe this interactive colorization with discrepant references can effectively assist users in their colorization attempts and inspire new ideas. 5. Conclusion In this work, we present MangaNinja , novel reference-guided line art colorization method. Through series of training strategies, our method utilizes dual-branch structure and PointNet to achieve precise automatic matching while also allowing users to exert points. fine-grained MangaNinja exhibits impressive performance in complex reference colorization, scenarios, including discrepant defining matching control by significant variations between reference images and line art, and multi-subject colorization. Additionally, we propose benchmark for the standardized evaluation of reference-based colorization. Our work serves as practical tool to accelerate the coloring process in the anime industry while inspiring future research in colorization.', 'summary': '<p><strong>Сравнение с другими методами и оценка качества</strong></p>\n<p>В статье рассматривается метод MangaNinja для раскрашивания контурных рисунков с использованием референсных изображений. Сначала проводится сравнение с другими подходами, такими как BasicPBC, IP-Adapter и AnyDoor. BasicPBC, как метод, основанный на выборке цвета из окрестности, показывает неудовлетворительные результаты при больших различиях между референсом и контуром, а также плохо справляется со светом и тенью. Генеративные методы, такие как IP-Adapter и AnyDoor (с использованием ControlNet), демонстрируют более естественные результаты, но AnyDoor лучше сохраняет детали цвета. Однако ни один из этих методов не обладает точным соответствием и может приводить к путанице цветов.</p>\n<p>В отличие от них, метод MangaNinja не использует точки для наведения при генерации. Он обучается на парах изображений из видео, постепенно перемешивая референсное изображение на уровне патчей от простого к сложному, что позволяет модели достичь отличного соответствия. Благодаря этому подходу MangaNinja превосходит другие методы даже в сложных сценариях, как будет показано далее.</p>\n<p>Для количественной оценки был создан бенчмарк из 200 пар изображений (всего 400 выводов, меняя местами референс и целевое изображение). Результаты показывают, что BasicPBC превосходит генеративные методы по метрикам на уровне пикселей, но уступает по метрикам схожести признаков. AnyDoor требует ручной разметки масок для достижения лучшей производительности, в то время как MangaNinja значительно превосходит все предыдущие методы по обеим категориям метрик.</p>\n<p><strong>Сложные случаи и использование точек-подсказок</strong></p>\n<p>Метод MangaNinja особенно хорошо проявляет себя в сложных случаях. Например, даже при значительных различиях между контуром и референсом, точное раскрашивание может быть достигнуто с помощью точек-подсказок. Кроме того, если референсное изображение не содержит всех элементов, присутствующих на контуре, пользователи могут раскрасить недостающие части, используя точки для наведения (например, раскрасить нижнюю часть одежды, отсутствующую на референсе, ориентируясь на верхнюю). Также MangaNinja эффективно справляется с одновременным раскрашиванием нескольких объектов, избегая путаницы цветов.</p>\n<p><strong>Раскрашивание с использованием нескольких референсов</strong></p>\n<p>В реальных условиях одного референсного изображения может быть недостаточно. Метод MangaNinja позволяет использовать несколько референсов одновременно. Пользователи могут комбинировать изображения и вводить их в Reference UNet, который затем сопоставляет различные области референсов с соответствующими элементами на контуре, используя точки-подсказки. Это позволяет эффективно раскрашивать объекты и избегать конфликтов между визуально похожими элементами из разных референсов.</p>\n<p><strong>Раскрашивание с референсами разных персонажей</strong></p>\n<p>MangaNinja обладает семантическим соответствием и хорошей обобщающей способностью, так как обучался на большом количестве пар изображений из видео. Благодаря этому модель может эффективно раскрашивать контуры, даже если референс и контур изображают разных персонажей. Это позволяет пользователям экспериментировать и находить новые решения для раскрашивания.</p>\n<p><strong>Исследование влияния различных стратегий обучения</strong></p>\n<p>В статье также проводится исследование влияния различных стратегий обучения на производительность модели. Результаты показывают, что все стратегии способствуют улучшению качества раскрашивания с использованием точек-подсказок. Даже без использования точек-подсказок, такие методы, как "condition dropping" и "progressive patch shuffle", улучшают способность модели к автоматическому сопоставлению. Особенно заметно влияние "progressive patch shuffle", который нарушает структурные паттерны референсных изображений во время обучения, позволяя модели изучать локальное соответствие. Эффект от точек-подсказок становится более явным только после обучения локальному сопоставлению.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture', '#audio', '#benchmark (1)', '#cv (1)', '#data', '#dataset', '#diffusion (1)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source', '#optimization', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В этой работе представлен метод MangaNinjia, который основан на диффузионных моделях и специализируется на раскрашивании контурных рисунков (лайн-арта) с использованием цветовой палитры из другого, эталонного изображения. </p>
<p>Для точного переноса деталей персонажа из эталонного изображения в раскрашиваемый контур, авторы используют два ключевых подхода:</p>
<ol>
<li><strong>Модуль перемешивания патчей (patch shuffling module)</strong>: Этот модуль помогает установить соответствие между цветовыми областями эталонного изображения и областями контурного рисунка. Проще говоря, он находит, какие части эталонного изображения соответствуют каким частям контура, чтобы правильно перенести цвет.</li>
<li><strong>Управление цветом на основе точек (point-driven control scheme)</strong>: Этот механизм позволяет более точно контролировать процесс раскрашивания. Пользователь может указать конкретные точки на контурном рисунке и задать, какой цвет должен быть в этих точках, что дает возможность точной настройки цветовой гаммы.</li>
</ol>
<p>Результаты экспериментов на специально собранном наборе данных показывают, что MangaNinjia превосходит существующие методы в точности раскрашивания. Более того, авторы демонстрируют, как интерактивное управление цветом с помощью точек позволяет справляться со сложными случаями, такими как раскрашивание персонажей из разных источников, гармонизация нескольких эталонных изображений, что недоступно для других алгоритмов.</p>
<p><strong>Комментарий:</strong> <em>Диффузионные модели, на которых основан MangaNinjia, являются мощным инструментом для генерации изображений. Использование эталонного изображения позволяет контролировать цветовое решение, а добавленные модули делают процесс более точным и управляемым.</em></p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.08332/x1.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><h2>Изложение раздела "Введение" статьи о MangaNinja</h2>
<p>Статья посвящена задаче колоризации контурных рисунков на основе эталонного изображения. Этот метод очень востребован в создании комиксов, анимации и других видов контента. В отличие от методов, которые полагаются только на штрихи, палитры или текстовые подсказки, колоризация по эталону позволяет сохранить как идентичность персонажа, так и его семантическое значение, что особенно важно для комиксов и манги.</p>
<p>Существующие подходы к колоризации по эталону используют механизмы внимания, но имеют два основных недостатка. Во-первых, значительные различия между контурным рисунком и эталонным изображением часто приводят к семантическим несоответствиям или путанице в цветах. Поэтому эти методы требуют, чтобы эталонное изображение было очень похоже на контурный рисунок, что непрактично в реальных условиях. Во-вторых, существующие методы не обеспечивают точного контроля, что приводит к потере важных деталей из эталонного изображения в процессе колоризации.</p>
<p>В данной работе представлен метод MangaNinja, который использует двухканальную структуру для нахождения соответствий между эталонным и контурным рисунками, опираясь на богатые диффузионные априорные знания через кросс-внимание.  Для решения проблемы переноса глобального стиля вместо сопоставления локальной семантики, авторы предлагают модуль перемешивания патчей. Этот модуль разделяет эталонное изображение на патчи, чтобы усилить способность модели к локальному сопоставлению. Перемешивание патчей выводит модель из зоны комфорта во время обучения, помогая ей научиться неявному сопоставлению, которое эффективно справляется с различиями между контурным рисунком и эталонным изображением.</p>
<p>Однако, семантическое соответствие все еще может быть неоднозначным, особенно когда цветные изображения содержат детали, которые трудно передать в контурном рисунке, или когда некоторые элементы на контурном рисунке занимают небольшую площадь на эталонном изображении, а также при значительных различиях и сложных композициях. Для более точного сопоставления цветов авторы вводят схему управления точками, основанную на PointNet. Этот метод позволяет пользователям интерактивно задавать точки-подсказки для более детального контроля. Эксперименты показали, что управление точками работает только тогда, когда модель учитывает локальную семантику, что подчеркивает важность и эффективность перемешивания патчей.</p>
<p>Для обучения модели авторы используют видео аниме, так как они содержат естественные семантические соответствия и визуальные различия. Из видео случайным образом выбираются два кадра: один используется в качестве эталона, а другой, вместе со своей версией в виде контурного рисунка, служит целевым изображением и входом для модели. Для определения точного соответствия используется готовая модель, которая размечает соответствующие точки на обучающих парах изображений. Эти точки кодируются с помощью PointNet и интегрируются в основную ветвь через механизм внимания.</p>
<p>Благодаря разработанной стратегии перемешивания патчей и схеме управления точками, MangaNinja эффективно справляется со сложными сценариями, такими как различия в позах или отсутствие деталей между эталонным и контурным рисунками, использование нескольких эталонов и колоризация с несоответствующими эталонами. Модель отлично справляется со сложными задачами колоризации, создавая высококачественные результаты из контурных рисунков, точно сохраняя идентичность персонажей. Для объективной оценки был создан всесторонний бенчмарк для колоризации контурных рисунков. Результаты экспериментов показали, что предложенный подход превосходит существующие методы, достигая наилучших результатов в плане визуальной точности и сохранения идентичности, что делает его полезным для создания комиксов, анимации и другого контента.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.08332/x2.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Enhancing image colorization with reference u-net and pointnet</div>
                <div class="summary_text"><h2>Изложение раздела статьи о методе MangaNinja</h2>
<p>В статье представлен метод MangaNinja для колоризации изображений на основе контуров (лайн-арт), использующий в качестве референса другое изображение. Модель состоит из двух основных U-Net архитектур: Reference U-Net и Denoising U-Net.</p>
<p><strong>Reference U-Net</strong> обрабатывает референсное изображение, кодируя его в 4-канальное латентное представление с помощью вариационного автоэнкодера (VAE). Затем эта информация используется для извлечения многоуровневых признаков, которые в дальнейшем объединяются с Denoising U-Net. Для этого используются механизмы внимания: ключи и значения из слоев самовнимания обеих сетей конкатенируются, что позволяет передавать контекст референсного изображения в соответствующие слои Denoising U-Net.</p>
<p><strong>Denoising U-Net</strong> является основной ветвью модели и отвечает за непосредственно колоризацию. На вход подается латентное представление контура, полученное из исходного изображения, а также латентное представление зашумленного изображения. Контур извлекается с помощью LineartAnimeDetector, а затем реплицируется в три канала для подачи в VAE. В качестве альтернативы, контур можно пропустить через ControlNet, но для экономии ресурсов используется первый подход. Текстовые эмбеддинги заменены на эмбеддинги изображений, извлеченные из CLIP-энкодера.</p>
<p>Для улучшения способности модели к сопоставлению деталей, используется <strong>прогрессивное перемешивание патчей</strong>. Референсное изображение делится на небольшие патчи, которые затем случайным образом перемешиваются. Это нарушает общую структурную целостность изображения и заставляет модель обращать больше внимания на локальные соответствия, а не на глобальные. Количество перемешиваемых патчей постепенно увеличивается в процессе обучения, что соответствует стратегии обучения от общего к частному. В дополнение к этому, применяются стандартные методы аугментации данных, такие как случайные повороты и отражения.</p>
<p>Для более точного управления процессом колоризации, вводится <strong>механизм точечного контроля</strong>. Пользователь задает пары соответствующих точек на референсном и целевом изображениях. Эти точки кодируются в виде двух карт точек, где каждой точке присваивается уникальное целочисленное значение. Карты точек обрабатываются с помощью <strong>PointNet</strong>, который извлекает многомасштабные эмбеддинги. Эти эмбеддинги интегрируются в Denoising U-Net через механизмы кросс-внимания.</p>
<p>Для контроля влияния референсного изображения и точек на процесс генерации используется <strong>множественное управление без классификатора</strong>. Это позволяет регулировать, насколько модель полагается на автоматическое сопоставление и на заданные пользователем точки. Для усиления влияния точечного контроля, во время обучения случайно отбрасывается условие контура, что заставляет модель полагаться только на точечные соответствия.</p>
<p>Обучение состоит из <strong>двух этапов</strong>. На первом этапе отбрасываются условия как референсного изображения, так и точечные сигналы, что позволяет модели учиться извлекать признаки и использовать точечные соответствия для колоризации. На втором этапе обучается только модуль PointNet, что усиливает его способность кодировать карты точек и, соответственно, улучшает точечный контроль.</p>
<p>Для оценки качества работы модели, был создан <strong>бенчмарк</strong>, включающий 200 пар изображений персонажей из аниме. Для каждой пары есть целевое изображение, его контур и референсное изображение. Оценка качества включает в себя расчет семантического сходства между сгенерированными и оригинальными изображениями с использованием CLIP и DINO, а также метрики PSNR и MS-SSIM. Кроме того, для оценки точности колоризации на уровне пикселей, используются 50 аннотированных пар точек соответствия, для которых вычисляется среднеквадратичная ошибка (MSE) в окрестности 3x3 пикселя.</p>
<p>Эксперименты проводились на датасете sakuga-42m, содержащем 42 миллиона кадров из аниме. Для обучения использовались 300 000 видеоклипов. Модели Reference U-Net и Denoising U-Net инициализированы весами из Stable Diffusion 1.5. Обучение проводилось в течение 200 000 шагов с начальной скоростью обучения 10^-3.</p>
<p>В разделе сравнения метод MangaNinja сравнивается с BasicPBC, IPAdapter и Anydoor. Результаты показывают, что MangaNinja превосходит их по точности колоризации и качеству сгенерированных изображений.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Comparative analysis of image colorization methods</div>
                <div class="summary_text"><p><strong>Сравнение с другими методами и оценка качества</strong></p>
<p>В статье рассматривается метод MangaNinja для раскрашивания контурных рисунков с использованием референсных изображений. Сначала проводится сравнение с другими подходами, такими как BasicPBC, IP-Adapter и AnyDoor. BasicPBC, как метод, основанный на выборке цвета из окрестности, показывает неудовлетворительные результаты при больших различиях между референсом и контуром, а также плохо справляется со светом и тенью. Генеративные методы, такие как IP-Adapter и AnyDoor (с использованием ControlNet), демонстрируют более естественные результаты, но AnyDoor лучше сохраняет детали цвета. Однако ни один из этих методов не обладает точным соответствием и может приводить к путанице цветов.</p>
<p>В отличие от них, метод MangaNinja не использует точки для наведения при генерации. Он обучается на парах изображений из видео, постепенно перемешивая референсное изображение на уровне патчей от простого к сложному, что позволяет модели достичь отличного соответствия. Благодаря этому подходу MangaNinja превосходит другие методы даже в сложных сценариях, как будет показано далее.</p>
<p>Для количественной оценки был создан бенчмарк из 200 пар изображений (всего 400 выводов, меняя местами референс и целевое изображение). Результаты показывают, что BasicPBC превосходит генеративные методы по метрикам на уровне пикселей, но уступает по метрикам схожести признаков. AnyDoor требует ручной разметки масок для достижения лучшей производительности, в то время как MangaNinja значительно превосходит все предыдущие методы по обеим категориям метрик.</p>
<p><strong>Сложные случаи и использование точек-подсказок</strong></p>
<p>Метод MangaNinja особенно хорошо проявляет себя в сложных случаях. Например, даже при значительных различиях между контуром и референсом, точное раскрашивание может быть достигнуто с помощью точек-подсказок. Кроме того, если референсное изображение не содержит всех элементов, присутствующих на контуре, пользователи могут раскрасить недостающие части, используя точки для наведения (например, раскрасить нижнюю часть одежды, отсутствующую на референсе, ориентируясь на верхнюю). Также MangaNinja эффективно справляется с одновременным раскрашиванием нескольких объектов, избегая путаницы цветов.</p>
<p><strong>Раскрашивание с использованием нескольких референсов</strong></p>
<p>В реальных условиях одного референсного изображения может быть недостаточно. Метод MangaNinja позволяет использовать несколько референсов одновременно. Пользователи могут комбинировать изображения и вводить их в Reference UNet, который затем сопоставляет различные области референсов с соответствующими элементами на контуре, используя точки-подсказки. Это позволяет эффективно раскрашивать объекты и избегать конфликтов между визуально похожими элементами из разных референсов.</p>
<p><strong>Раскрашивание с референсами разных персонажей</strong></p>
<p>MangaNinja обладает семантическим соответствием и хорошей обобщающей способностью, так как обучался на большом количестве пар изображений из видео. Благодаря этому модель может эффективно раскрашивать контуры, даже если референс и контур изображают разных персонажей. Это позволяет пользователям экспериментировать и находить новые решения для раскрашивания.</p>
<p><strong>Исследование влияния различных стратегий обучения</strong></p>
<p>В статье также проводится исследование влияния различных стратегий обучения на производительность модели. Результаты показывают, что все стратегии способствуют улучшению качества раскрашивания с использованием точек-подсказок. Даже без использования точек-подсказок, такие методы, как "condition dropping" и "progressive patch shuffle", улучшают способность модели к автоматическому сопоставлению. Особенно заметно влияние "progressive patch shuffle", который нарушает структурные паттерны референсных изображений во время обучения, позволяя модели изучать локальное соответствие. Эффект от точек-подсказок становится более явным только после обучения локальному сопоставлению.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-15 09:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    