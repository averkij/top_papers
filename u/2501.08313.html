
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 января</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-14.html">⬅️ <span id="prev-date">14.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-16.html">➡️ <span id="next-date">16.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 января', 'en': 'January 15', 'zh': '1月15日'};
        let feedDateNext = {'ru': '16.01', 'en': '01/16', 'zh': '1月16日'};
        let feedDatePrev = {'ru': '14.01', 'en': '01/14', 'zh': '1月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2501.08313', 'title': 'MiniMax-01: Scaling Foundation Models with Lightning Attention', 'url': 'https://huggingface.co/papers/2501.08313', 'abstract': 'We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-01-14', 'pub_date_card': {'ru': '14 января', 'en': 'January 14', 'zh': '1月14日'}, 'hash': 'a57d7b1914e7383a', 'authors': ['MiniMax', 'Aonian Li', 'Bangwei Gong', 'Bo Yang', 'Boji Shan', 'Chang Liu', 'Cheng Zhu', 'Chunhao Zhang', 'Congchao Guo', 'Da Chen', 'Dong Li', 'Enwei Jiao', 'Gengxin Li', 'Guojun Zhang', 'Haohai Sun', 'Houze Dong', 'Jiadai Zhu', 'Jiaqi Zhuang', 'Jiayuan Song', 'Jin Zhu', 'Jingtao Han', 'Jingyang Li', 'Junbin Xie', 'Junhao Xu', 'Junjie Yan', 'Kaishun Zhang', 'Kecheng Xiao', 'Kexi Kang', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Zheng', 'Linbo Chai', 'Long Xing', 'Meizhi Ju', 'Mingyuan Chi', 'Mozhi Zhang', 'Peikai Huang', 'Pengcheng Niu', 'Pengfei Li', 'Pengyu Zhao', 'Qi Yang', 'Qidi Xu', 'Qiexiang Wang', 'Qin Wang', 'Qiuhui Li', 'Ruitao Leng', 'Shengmin Shi', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tao Huang', 'Tianrun Liang', 'Weigao Sun', 'Weixuan Sun', 'Weiyu Cheng', 'Wenkai Li', 'Xiangjun Song', 'Xiao Su', 'Xiaodong Han', 'Xinjie Zhang', 'Xinzhu Hou', 'Xu Min', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yingjie Zhu', 'Yipeng Zhou', 'Yiran Zhong', 'Yongyi Hu', 'Yuanxiang Fan', 'Yue Yu', 'Yufeng Yang', 'Yuhao Li', 'Yunan Huang', 'Yunji Li', 'Yunpeng Huang', 'Yunzhi Xu', 'Yuxin Mao', 'Zehan Li', 'Zekang Li', 'Zewei Tao', 'Zewen Ying', 'Zhaoyang Cong', 'Zhen Qin', 'Zhenhua Fan', 'Zhihang Yu', 'Zhuo Jiang', 'Zijia Wu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2501.08313.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#benchmark', '#long_context', '#training'], 'emoji': '🚀', 'ru': {'title': 'MiniMax-01: Революция в обработке длинных контекстов', 'desc': 'Исследователи представили серию моделей MiniMax-01, включая MiniMax-Text-01 и MiniMax-VL-01, которые сравнимы с лучшими моделями, но обладают улучшенными возможностями обработки длинных контекстов. В основе лежит технология lightning attention и ее эффективное масштабирование, интегрированные с Mixture of Experts (MoE). Модель имеет 32 эксперта и 456 миллиардов параметров, из которых 45,9 миллиардов активируются для каждого токена. Контекстное окно MiniMax-Text-01 может достигать 1 миллиона токенов при обучении и экстраполироваться до 4 миллионов токенов при инференсе.'}, 'en': {'title': 'Unleashing Long Contexts with MiniMax-01 Models', 'desc': 'The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities.'}, 'zh': {'title': 'MiniMax-01：超长上下文处理的新纪元', 'desc': '我们介绍了MiniMax-01系列，包括MiniMax-Text-01和MiniMax-VL-01，这些模型在处理更长的上下文时具有优越的能力。核心技术是闪电注意力和高效的扩展能力。为了最大化计算能力，我们将其与专家混合模型（MoE）结合，创建了一个拥有32个专家和4560亿参数的模型。我们的实验表明，这些模型在标准和内部基准测试中表现出色，能够与最先进的模型相媲美，同时提供20到32倍更长的上下文窗口。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.', 'summary': '<p>В статье представлена серия моделей MiniMax-01, включающая в себя MiniMax-Text-01 (текстовую модель) и MiniMax-VL-01 (модель для обработки текста и изображений). Эти модели сравнимы по производительности с лучшими моделями, но при этом превосходят их по возможностям обработки длинных контекстов.</p>\n<p>Ключевым элементом является "молниеносное внимание" (lightning attention) и его эффективное масштабирование. Для максимального использования вычислительных ресурсов, эта технология интегрирована с архитектурой Mixture of Experts (MoE), создавая модель с 32 экспертами и общим количеством параметров в 456 миллиардов. При этом для каждого токена активируется только 45.9 миллиарда параметров.</p>\n<p>Разработана оптимизированная стратегия параллельной обработки и эффективные техники перекрытия вычислений и коммуникаций для MoE и "молниеносного внимания". Это позволяет эффективно обучать и использовать модели с сотнями миллиардов параметров на контекстах длиной в миллионы токенов.</p>\n<p>Контекстное окно MiniMax-Text-01 может достигать 1 миллиона токенов во время обучения и до 4 миллионов токенов во время использования (inference), при этом сохраняется доступная стоимость. Модель MiniMax-VL-01, обрабатывающая текст и изображения, была создана путем дообучения на 512 миллиардах токенов, содержащих и текст, и изображения.</p>\n<p>Эксперименты на стандартных и внутренних наборах данных показали, что модели MiniMax-01 соответствуют производительности передовых моделей, таких как GPT-4o и Claude-3.5-Sonnet, при этом предлагая в 20-32 раза большее контекстное окно.</p>\n<p>Модели MiniMax-01 опубликованы в открытом доступе на GitHub.</p>'}, {'title': 'Experimental setup', 'content': 'We conducted training on softmax (equipped with FlashAttention-2 (Dao, 2024)), lightning attention, and hybrid-lightning attention models across various scales: 70 million, 160 million, 410 million, 1 billion, 3 billion, and 7 billion parameters. Each model was trained on dataset consisting of up to 300 billion tokens, with context length of 8192. Our training methodology follows the approach proposed by Chinchilla (Hoffmann et al., 2022), where the training loss serves as direct indicator of test performance. For each model architecture and training sequence length, we maintained 8 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 2 Summary of Scaling Laws: It shows the relationships between loss (𝐿), optimal model size (𝑁𝑜𝑝𝑡), and optimal dataset size (𝐷𝑜𝑝𝑡) as functions of computational budget (𝐶). It reveals that, given the same budget, the hybrid model uses more parameters and tokens but achieves lower loss. Arch Softmax Attention Lightning Attention Hybrid-lightning 𝐿(𝐶) 3.7087𝐶 0.0798 3.5391𝐶 0.0768 3.4797𝐶 0.0763 𝑁𝑜𝑝𝑡 (𝐶) (1.82 108)𝐶0.7118 (2.74 108)𝐶0.6470 (2.57 108)𝐶0.6670 𝐷𝑜𝑝𝑡 (𝐶) (2.56 1010)𝐶0.5102 (4.43 1010)𝐶0.4684 (3.70 1010)𝐶0.4707 Figure 6 Summary of Scaling Laws. Training curves (left) span models from 70M to 7B parameters. Optimal model size (center) and training tokens (right) are derived based on specified compute budget estimation. uniform global batch size of 4 million tokens. The Adam optimizer was employed, configured with learning rate of 3e-4 and weight decay of 0.1. fixed learning rate scheduler was applied across all experiments due to constrained computational resources. We employ diverse set of evaluation benchmarks, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (both easy and challenge variants) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), Needle in Haystack (NIAH) (Shen et al., 2024), and SCROLLS (Shaham et al., 2022). Each benchmark assesses distinct capabilities of the models.', 'summary': '<p>В данном исследовании проводилось обучение моделей с использованием различных механизмов внимания: softmax с FlashAttention-2, lightning attention и гибридного lightning attention. Модели обучались в разных масштабах, от 70 миллионов до 7 миллиардов параметров. Каждая модель обучалась на наборе данных, содержащем до 300 миллиардов токенов, с длиной контекста 8192. Методология обучения следовала подходу, предложенному в работе Chinchilla, где потери при обучении являются прямым индикатором производительности модели на тестовых данных.</p>\n<p>Для каждой архитектуры модели и длины обучающей последовательности поддерживался единый глобальный размер пакета в 4 миллиона токенов. Использовался оптимизатор Adam с начальной скоростью обучения 3e-4 и весовым коэффициентом 0.1. Из-за ограниченных вычислительных ресурсов применялся фиксированный планировщик скорости обучения для всех экспериментов.</p>\n<p>Для оценки моделей использовался разнообразный набор бенчмарков, включая BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (легкий и сложный варианты), OpenBookQA, Needle in Haystack (NIAH) и SCROLLS. Каждый бенчмарк оценивает различные способности моделей.</p>\n<p><em>Комментарий: Таким образом, в исследовании проводилось систематическое сравнение разных архитектур внимания при обучении моделей разных размеров, с использованием фиксированных гиперпараметров и единого подхода к обучению. Это позволяет оценить влияние архитектуры внимания на производительность модели.</em></p>'}, {'title': 'Scaling laws', 'content': 'We fit the scaling curves based on our experiments over the above mentioned settings, where we alter the model size (𝑁) and dataset size (𝐷) for different computational budget (𝐶) and observe the corresponding training loss (𝐿) that serving as an estimator of test loss. We begin by establishing power-law relationships between 𝐿 and 𝐶, following Chinchillas methodology (Hoffmann et al., 2022). Using the fitted curve, we derive coefficients for optimal model size 𝑁𝑜𝑝𝑡 𝐶𝑎 and optimal dataset size 𝐷𝑜𝑝𝑡 𝐶𝑏. The original scaling laws (Kaplan et al., 2020) use 𝐿(𝑋) = (𝑋0/𝑋)𝛼𝑋 , while subsequent studies (Clark et al., 2022; Gao et al., 2024; Henighan et al., 2020; Hoffmann et al., 2022) employ 𝐿(𝑋) = 𝜖 + (𝑋0/𝑋)𝛼𝑋 for better fitting, where 𝜖 denotes the irreducible loss. For simplicity, we unify these forms into 𝐿(𝑋) = 𝛽𝑋 𝑋 𝛼𝑋 , facilitating direct comparison of scaling capabilities based on 𝛼𝑋 and 𝛽𝑋 . The summary of scaling laws is shown in Table 2 and Figure 6. It can be intuitively understood that given the same computational budget, models with lightning attention tend to utilize more parameters and tokens, yet they achieve lower loss compared to models with pure softmax attention. 9 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 7 Larger models and hybrid-lightning attention achieve the best performance across benchmarks. Performance is evaluated on CSR (Common Sense Reasoning), NIAH (Needle in Haystack), and SCROLLS benchmarks using three attention mechanism models from 410M to 7B parameters.', 'summary': '<p>В данном исследовании изучается, как размер модели (N) и размер обучающего набора данных (D) влияют на ошибку обучения (L) при различных вычислительных ресурсах (C). Цель – понять, как масштабирование этих параметров влияет на производительность модели.</p>\n<p>Для начала устанавливаются степенные зависимости между L и C, следуя методологии Chinchilla. Это позволяет вывести коэффициенты для оптимального размера модели (N_opt * C^a) и оптимального размера набора данных (D_opt * C^b).</p>\n<p>В качестве основы для анализа используется функция, связывающая ошибку обучения с размером модели/данных. Изначально применялась формула L(X) = (X0/X)^αX, но последующие исследования показали, что формула L(X) = ε + (X0/X)^αX лучше подходит для аппроксимации, где ε – это неустранимая ошибка. Для упрощения, все эти формы объединены в L(X) = βX * X^(-αX), что позволяет напрямую сравнивать масштабируемость моделей на основе параметров αX и βX.</p>\n<p>Результаты этих зависимостей представлены в таблице и на графике. Из них следует, что при одинаковых вычислительных ресурсах модели с механизмом "lightning attention" (облегченным вниманием) склонны использовать больше параметров и токенов, но при этом достигают меньшей ошибки, чем модели с обычным softmax вниманием.</p>\n<p>Также показано, что более крупные модели и модели с гибридным "lightning attention" достигают наилучших результатов на различных бенчмарках (CSR, NIAH и SCROLLS). Производительность оценивалась на моделях с тремя механизмами внимания, с количеством параметров от 410 миллионов до 7 миллиардов.</p>'}, {'title': 'Benchmarking training speed and performance of attention mechanisms', 'content': 'We assess the end-to-end training speed of softmax attention, lightning attention, and hybridlightning models with 3 billion parameters by measuring the tokens processed per GPU per second (TGS). For completeness, we also included popular linear models such as HGRN2 and Mamba2 in our evaluation. For the speed benchmark, the training context length was gradually increased until reaching the out-ofmemory limit on single-node H800 GPUs. As illustrated in Fig. 8, lightning attention achieves constant training speed irrespective of the sequence length and is the sole linear model that outperforms FlashAttention2. Figure 8 The training speed of various attention mechanisms, including softmax, lightning, hybridlightning, HGRN2, and Mamba2, was benchmarked across sequence lengths ranging from 1,024 to 65,536. Performance was measured in terms of training speed, reported as tokens processed per GPU per second (TGS). 10 MiniMax-01: Scaling Foundation Models with Lightning Attention 2.2.3. Hybrid Architecture Our preliminary experiments with the hybrid architecture have yielded promising results, motivating us to delve deeper into its potential through two variants: hybrid-cosformer2 and hybrid-hgrn2. In the hybrid-cosformer2 model, we replace the linear attention layers in the cosformer2 architecture with softmax attention layers at intervals of every eight layers. This substitution strategy is similarly applied in the hybrid-hgrn2 model. We conduct experiments using consistent setups to evaluate the downstream performance of these alternatives. Our findings, as summarized in Table 3, indicate that the hybrid-lightning model achieves the best performance. Table 3 Benchmarking various hybrid-linear models with 1 Billion Parameters. We present the average CSR score, weighted average accuracy for NIAH, and the average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: TGS (token per gpu per second), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, and SCR (SCROLLS). Hybrid-linear Arch. TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR Hybrid-cosformer2 Hybrid-hgrn2 Hybrid-lightning 23.3K 70.29 45.63 51.46 29.5K 70.89 51.23 56.51 33.4K 70.73 50.41 55.80 55.77 59.68 59.93 26.11 28.50 27. 30.60 46.64 32.40 49.87 32.80 49.55 43.6 91.8 95.7 10.9 10.8 13.3 In addition to linear models, sliding window attention can also achieve linear computational complexity by appropriately adjusting the window size. As it is grounded in softmax attention, it serves as robust baseline for evaluating linear architectures. Therefore, we incorporated the hybrid-window approach by replacing the sliding window attention with full softmax attention every eight layers. We evaluated various window sizes of SWA ranging from 256 to 1024. Our results indicate that larger window sizes lead to slower training speeds compared to the hybrid-lightning model. To compare these models under equivalent speed conditions, we did not consider window sizes larger than 1024. As shown in Table 4, the hybrid-lightning model outperforms all other models across all metrics, particularly excelling in the NIAH benchmark. Table 4 Benchmark comparison of hybrid-lightning and hybrid-window Models. Metrics include average CSR score, weighted NIAH accuracy, and average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: PS (parameter size, billion), W.S. (window size of SWA), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, SCR (SCROLLS), TGS (token per gpu per second). P.S 1B 3B W.S. Arch. 256 512 1024 TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR 35.6K 70.29 48.68 53.35 Hybrid35.1K 70.95 48.19 52.33 window 33.6K 69.75 47.80 53.12 Hybrid-lightning 33.4K 70.73 50.41 55.80 16.1K 73.83 59.70 59.59 Hybrid15.8K 73.29 60.00 59.04 window 15.4K 74.27 59.02 57.85 Hybrid-lightning 15.1K 74.21 61.06 59. 48.61 32.60 47.70 30.00 31.60 48.02 32.80 49.55 54.31 35.00 36.00 53.97 33.00 53.44 35.80 55.16 28.75 27.22 28.33 27.65 33.62 32.51 31.91 34.90 57.95 57.53 57.53 59.93 64.10 62.96 64.56 65.49 10.6 11.9 10.6 13.3 14.2 14.2 13.3 14.7 46.8 25.7 53.9 95.7 40.9 57.9 41.6 98.0 256 512 2.2.4. Discussion Based on our analysis of scaling law experiment, downstream performance and speed comparison, we conclude that while pure linear attention models are computationally efficient, they are not suitable 11 MiniMax-01: Scaling Foundation Models with Lightning Attention for LLMs. This is due to their inherent inability to perform retrieval, capability that is essential for in-context learning. In contrast, our hybrid model not only matches but also surpasses softmax attention in both retrieval and extrapolation tasks. This outcome is somewhat counterintuitive. To understand this phenomenon, consider the following explanation of softmax attention: = Softmax(QK/ 𝑑)V. (10) It can be rewritten into linear recurrent form as: 𝑠0 𝑡 = 0, 𝑡 = 𝑠 𝑗1 𝑠 𝑗 𝑡 + exp(q𝑡k𝑇 𝑗 / 𝑑), 𝑡 = (𝑠 𝑗1 𝑗 𝑡 /𝑠 𝑗 𝑡 )o 𝑗1 𝑡 + (1 𝑠 𝑗1 𝑡 /𝑠 𝑗 𝑡 )v 𝑗, o𝑡 = o𝑡 𝑡, 𝑗 = 1, . . . , 𝑡. (11) Note that the linear recurrence form of lightning attention is as follows: kv0 = 0, kv 𝑗 = kv 𝑗1 + 𝑗v 𝑗 𝑗 = kv 𝑗 𝑗, 𝑗 = 1, . . . , 𝑡. (12) The softmax attention mechanism can be interpreted as linear RNN (Qin et al., 2024a). At each time step 𝑡, the hidden state is recalculated starting from the initial time 𝑡0 = 1, process often described as "Going Through Book." This method enables the model to accurately retain input information by systematically revisiting previous data. In contrast, linear models lack this recomputation process, which hinders their ability to effectively retain input data. Let us define the capacity of an RNN as the size of its recurrent state. Upon closer examination of Eq. 11, we can deduce that the capacity of softmax attention is 𝑂(𝑑). In contrast, as illustrated in Eq. 12, the capacity of lightning attention is 𝑂(𝑑2/ℎ). Given that 𝑑 > ℎ, it follows that lightning attention possesses larger capacity than softmax attention. Consequently, the hybrid-lightning model exhibits superior retrieval and extrapolation capabilities compared to models relying solely on softmax attention. 2.3. Module Ablations in MoE Based on the conclusions from previous sections, we conduct two additional sets of ablation experiments to validate module choices within the MoE architecture on larger scale: (1) Hybrid-lightning attention versus softmax attention: To verify the advantages of the hybrid lightning attention in the MoE. (2) Pre-Layer Normalization versus Post-Layer Normalization: In our hybrid architecture, the effective depth of the model plays significant role. Thus, we expect to find better normalization algorithm for the deep model. Hybrid-lightning Attention versus Softmax Attention. We perform small-scale comparative analysis between softmax attention and hybrid-lightning attention within the MoE architecture. Specifically, we use 28 billion parameter MoE with 5 billion activation parameters that utilize softmax attention as the base model. For every 8 consecutive layers in the base model, we systematically replace softmax attention with lightning attention in the first 7 layers. Both the base model and the modified model are trained on 1 trillion tokens. As shown in Table 5, the results reveal that substituting certain softmax attention layers with lightning attention improves accuracy across most benchmarks. Pre Layer Normalization versus Post Layer Normalization. Pre Layer Normalization(Baevski and Auli, 2018; Child et al., 2019; Wang et al., 2019) (PreNorm), which applies normalization layers before residual connections and attention mechanisms, has demonstrated enhanced stability and performance in LLMs. Since PreNorm allows gradients to flow more directly from the output to the input through residual connections, bypassing the sub-layers to certain extent, it reduces the effective depth of the model. In contrast, Post Layer Normalization(Wang et al., 2019) (PostNorm) applies normalization after the residual connection and attention mechanisms, thereby preserving 12 MiniMax-01: Scaling Foundation Models with Lightning Attention the models effective depth. However, PostNorm can be prone to vanishing and exploding gradients, presenting significant challenges in training LLMs. Most existing LLMs predominantly use PreNorm, as the performance differences between wider and deeper networks in the conventional Transformer architecture are often negligible, and training stability is prioritized. The experiments are performed on models with 9.3 billion activation parameters and total of 60 billion parameters, each consisting of 48 layers that employ different normalization methods. Both models are trained on 500 billion tokens. For PostNorm, we utilize DeepNorm (Wang et al., 2024a) to ensure more stable training. As illustrated in Table 5, PostNorm consistently outperforms PreNorm across all evaluated metrics. Table 5 Module Ablations. Abbreviations: BBH (BIG-Bench Hard), DROP (Discrete Reasoning Over Paragraphs), MMLU (Massive Multitask Language Understanding), CMMLU (Massive Multitask Language Understanding in Chinese), GSM8k (Grade School Math 8K), ARC-C (Arc-Challenge), WG (WinoGrande). Arch. BBH DROP MMLU CMMLU MATH Softmax Hybrid-lightning Pre Layer Norm. Post Layer Norm. 28.2 32.2 29.9 32.6 27.4 29.0 26.8 27. 49.3 49.5 43.9 50.2 47.3 46.0 41.8 49.2 4.6 6.8 4.8 5. GSM8k 18.8 18.5 12.2 16.8 ARC-C WG 46.4 47.4 43.5 46.2 65.6 67. 65.5 65.4 2.4. Model Spec Upon finalizing the architecture of the models modules, the subsequent step entails scaling up the model, which necessitates meticulous design of the models hyperparameters across various dimensions. Our primary goal is to strike balance between performance and inference efficiency. Single-device inference offers superior efficiency compared to multi-device implementations by eliminating cross-machine communication overhead. Consequently, we constrain the models total parameters to 500B, ensuring compatibility with single-node inference on an 8 80G configuration for sequences up to 1M tokens under 8-bit quantization. Given our limited training budget, we formulate the following optimization problem to determine optimal parameter allocations: min 𝑃all,𝑃act 𝐿(𝑃all, 𝑃act, 𝑇) subject to 𝐶compute(𝑃all, 𝑃act, 𝑇) < 𝐶 and 𝑃all < 500𝐵, (13) where 𝐿 denotes the loss, 𝑃all and 𝑃act represent the total and activation parameter counts respectively, 𝑇 is the number of training tokens, 𝐶compute denotes the computational costs (dependent on parameter counts and data consumption), and 𝐶 signifies the budget constraint. Through comparative experiments on small-scale models, we first establish optimal ranges for several key variables: (1) the mixing ratio between softmax and linear attention mechanisms; (2) the depth-to-width ratio of the model architecture; (3) the ratio of linear attention memory size to hidden size; (4) the ratio of activated FFN to attention; (5) the proportion of dimensions utilizing RoPE for softmax attention. Our experiments reveal that the hybrid architecture demonstrates particular sensitivity to layer depth, with deeper models consistently outperforming shallower counterparts. Notably, shallow models require substantially more softmax attention layers to achieve comparable performance, underlining the efficiency advantages of deeper architectures. We also observe that increasing linear attention memory size significantly enhances model performance, and implementing RoPE on half of the softmax attention dimensions enables length extrapolation without performance degradation. 13 MiniMax-01: Scaling Foundation Models with Lightning Attention Based on these optimized architectural variables, we employ established scaling laws (Clark et al., 2022; Hoffmann et al., 2022) to determine the optimal model size. We train models with activation parameters ranging from 44 million to 1.2 billion across 500 billion tokens, utilizing 16, 32, and 64 experts. However, we find the predictions from these methods become less reliable when extrapolating to larger model with 9.3 billion parameters. To address this limitation and achieve more accurate predictions, we propose the following formula: 𝐿(𝑃act, 𝑇 𝐸) = 𝑑 + 𝑎𝑃𝛼 act + 𝑏𝑇 𝛽 + 𝑐(𝑃act𝑇)𝛾, (14) where 𝐿(𝑃act, 𝑇 𝐸) represents the loss conditioned on the number of experts, while 𝑎, 𝑏, 𝑐, 𝑑, 𝛼, 𝛽, and 𝛾 are parameters to be fitted in relation to the number of experts. Based on the predictions of Eq. 13 and Eq. 14, we have identified candidate model with 45.9 billion activation parameters and 456 billion total parameters as the optimal configuration. 3. Computation Optimization In this section, we present our computation part, including the training and inference. In this project, we have dynamically changing GPU cluster, where the number of H800 GPUs ranges from 1500 to 2500. An efficient architecture necessitates robust implementation optimization to fully harness its computational benefits at scale. To scale our novel architecture to the requisite size, we present three key optimization strategies that primarily address the following three challenges: 1. Mitigating the all-to-all (a2a) communication overhead during the training of Mixture of Experts (MoE) architecture is persistent challenge. The configuration we choose for our experts, specifically opting for large models, imposes substantial demands on GPU memory. Therefore, the primary challenge lies in achieving an optimal equilibrium between memory utilization, computational efficiency, and the overhead associated with all-to-all communication. 2. As we endeavor to support at least 1 million token context window in both training and inference, the accurate distribution of tokens within such an extensive context window across different GPUs becomes imperative for this colossal model. This necessity, however, inevitably introduces additional communication overhead. As result, devising strategies to minimize this overhead, particularly in the context of our hybrid architecture, presents significant challenge. 3. The current implementation of the lightning attention mechanism is specifically optimized for training processes. However, in the inference scenario, the challenge arises in effectively managing real-world batched inputs, which may encompass variable sequence lengths and specific inputs that inco', 'summary': '<h2>Анализ производительности и архитектурных решений в моделях с вниманием</h2>\n<p>В данном разделе статьи рассматривается скорость обучения различных механизмов внимания и проводится анализ гибридных архитектур, а также исследуется влияние разных вариантов нормализации на производительность моделей.</p>\n<p><strong>Скорость обучения механизмов внимания</strong></p>\n<p>Сравнивается скорость обучения моделей с 3 миллиардами параметров, использующих softmax внимание, lightning внимание и гибридное lightning внимание. Скорость измеряется в токенах, обработанных на GPU в секунду (TGS). Для сравнения также включены линейные модели HGRN2 и Mamba2. </p>\n<p>В ходе тестов контекстная длина постепенно увеличивалась до достижения предела памяти на GPU H800. Результаты показывают, что lightning внимание обеспечивает постоянную скорость обучения независимо от длины последовательности и превосходит FlashAttention2.  В то время как softmax внимание замедляется при увеличении длины последовательности.</p>\n<p><strong>Гибридные архитектуры</strong></p>\n<p>Исследуются гибридные архитектуры, в которых линейное внимание (например, cosformer2 или hgrn2) комбинируется с softmax вниманием. В моделях hybrid-cosformer2 и hybrid-hgrn2 каждые восемь слоев линейного внимания заменяются на softmax внимание. Эксперименты показывают, что гибридная модель с lightning вниманием (hybrid-lightning) демонстрирует наилучшие результаты по сравнению с другими гибридными моделями.</p>\n<p><strong>Сравнение гибридных моделей с окнами внимания</strong></p>\n<p>Также сравниваются гибридные модели с lightning вниманием и модели с "окнами" внимания (sliding window attention), где softmax внимание применяется только в пределах заданного окна. Результаты показывают, что гибридная модель с lightning вниманием превосходит все модели с окнами внимания, особенно по метрике NIAH.</p>\n<p><strong>Обсуждение результатов</strong></p>\n<p>Анализ показывает, что, хотя линейные модели внимания эффективны с точки зрения вычислений, они не подходят для больших языковых моделей (LLM) из-за их неспособности к извлечению информации, что важно для обучения в контексте. В отличие от них, гибридные модели не только не уступают softmax вниманию, но и превосходят его в задачах извлечения и экстраполяции.</p>\n<p><strong>Механизм работы softmax внимания</strong></p>\n<p>Механизм softmax внимания может быть представлен в виде линейной рекуррентной нейронной сети (RNN). На каждом шаге времени t скрытое состояние пересчитывается, начиная с начального времени t0=1, что позволяет модели сохранять информацию за счет повторного просмотра предыдущих данных. Линейные модели не обладают этим механизмом пересчета, что ограничивает их способность удерживать входные данные.</p>\n<p><strong>Вместимость (Capacity) механизмов внимания</strong></p>\n<p>Вместимость RNN определяется размером ее рекуррентного состояния. Вместимость softmax внимания составляет O(d), а вместимость lightning внимания - O(d^2/h), где d - размерность модели, а h - число голов внимания. Поскольку d &gt; h, lightning внимание имеет большую вместимость, чем softmax внимание, что объясняет превосходство гибридной модели с lightning вниманием в задачах извлечения и экстраполяции.</p>\n<p><strong>Абляция модулей в MoE</strong></p>\n<p>Проводятся эксперименты по абляции модулей в архитектуре Mixture of Experts (MoE). Сравнивается гибридное lightning внимание и softmax внимание, а также нормализация до слоя (Pre-Layer Normalization) и нормализация после слоя (Post-Layer Normalization).</p>\n<p><strong>Гибридное lightning внимание vs Softmax внимание в MoE</strong></p>\n<p>Замена некоторых слоев softmax внимания на lightning внимание в архитектуре MoE с 28 миллиардами параметров улучшает точность на большинстве бенчмарков.</p>\n<p><strong>Pre-Layer Normalization vs Post-Layer Normalization</strong></p>\n<p>Эксперименты на моделях с 60 миллиардами параметров показывают, что Post-Layer Normalization с использованием DeepNorm превосходит Pre-Layer Normalization по всем метрикам. PostNorm сохраняет эффективную глубину модели, в то время как PreNorm может уменьшать ее.</p>\n<p><strong>Спецификация модели</strong></p>\n<p>После выбора архитектурных решений следующим шагом является масштабирование модели, что требует тщательной настройки гиперпараметров. Основная цель - достичь баланса между производительностью и эффективностью вывода. Для обеспечения эффективности одно-устройственного вывода, общий размер параметров модели ограничен 500 миллиардами.</p>'}, {'title': 'Optimizing MoE architecture for efficient distributed training', 'content': 'rporate prefix caching. It is noteworthy that the existing open-source frameworks in the industry currently lack the necessary mature technical support to adequately address these challenges. Thus, we independently and comprehensively reinvent our distributed training and inference framework, thereby successfully addressing these challenges with the desired level of efficiency. 3.1. MoE Optimization The primary objective in optimizing the MoE architecture is to minimize communication overhead, particularly for MoE models that utilize all-to-all (a2a) communication. To address this, We implement token-grouping-based overlap scheme, as illustrated in Figure 9. In this scheme, the a2a communication is performed within the expert parallel (EP) communication group, and it overlaps with the processing of tokens from different expert groups. To ensure the correctness of the communication 14 MiniMax-01: Scaling Foundation Models with Lightning Attention results, we restrict each ProcessGroup to execute communication operators sequentially. As result, a2a communications across different groups cannot overlap, leading to the emergence of idle time. 0 0 expert Device 1 Device 0 Idle time a2a-combine a2a-dispatch w/ EP overlap w/o EP overlap This approach leads to significant performance improvements. However, upon more detailed analysis, we identified critical tradeoff specific to the expert configuration of the MiniMax-Text-01 model. When Tensor Parallelism (TP) is employed to partition the expert parameters, the computational intensity becomes excessively low, thereby hindering the efficiency of the computation. However, opting not to use TP leads to an excessively large parameter count, which necessitates the activation of larger Pipeline Parallelism (PP) configuration. The challenge emerges because PP does not reduce the memory footprint required for storing activations. This limitation is particularly detrimental for training models with long contexts, as the increase in memory consumption does not provide proportional benefits in terms of computational efficiency or training speed. Consequently, it is imperative to develop new parameter partitioning strategy that adeptly balances memory usage and computational intensity to optimize the training process for our specific model and task. Figure 9 Expert Parallel (EP) Overlap Illustration. Chunk tokens into 2 groups thus computation can overlap with communication between different groups. Time 0 1 1 To achieve enhanced efficiency, we first introduce novel ProcessGroup, termed ETP (Expert Tensor Parallel), which is specifically designed to manage the weight partitioning of experts. Concurrently, we propose another distinct ProcessGroup, named EDP (Expert Data Parallel), to encapsulate the data parallelism of identical experts. In our system, we define the total number of GPUs involved in training as 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒. The system must satisfy two key conditions: 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒 = 𝑠𝑖𝑧𝑒𝑃𝑃 𝑠𝑖𝑧𝑒𝐷𝑃 𝑠𝑖𝑧𝑒𝐶𝑃 𝑠𝑖𝑧𝑒𝑇 𝑃 and 𝑤𝑜𝑟𝑙𝑑_𝑠𝑖𝑧𝑒 = 𝑠𝑖𝑧𝑒𝑃𝑃 𝑠𝑖𝑧𝑒𝐸𝐷𝑃 𝑠𝑖𝑧𝑒𝐸𝑇 𝑃 𝑠𝑖𝑧𝑒𝐸𝑃 (15) (16) This configuration empowers the MoE component with the flexibility to define the distribution of experts, manage the weight partitioning of experts, and independently configure the ZeRO (Zero Redundancy Optimizer) algorithm (Rajbhandari et al., 2020). Based on this implementation, we are able to completely decouple the parallel strategies of the MoE components from those of the non-MoE components. Building upon this modification, we can flexibly configure the ETP to achieve an optimal balance between memory usage and computational intensity. Furthermore, to mitigate communication overhead, we design an EP-ETP overlap strategy. This strategy aims to maximize the utilization of both network resources and computational resources, as illustrated in Figure 10 (a). Since communications within the same process group must be executed sequentially, extended periods of computation not only facilitate overlap with greater number of communications but also create additional opportunities for communications across different process groups to overlap, leading to enhanced overall performance as illustrated in Figure 10 (b). When determining the number of groups, several trade-offs must be considered. Theoretically, only by dividing the workload into sufficiently large number of groups can we achieve ample overlap between communication and computation, as illustrated in Figure 10 (c). However, in practice, an excessive number of groups can significantly increase the complexity of scheduling and introduce the 15 MiniMax-01: Scaling Foundation Models with Lightning Attention w/o EP + ETP overlap Idle time Waste time a2a-dispatch allgather expert reduce scatter a2a-combine (a) Device Device 1 Device 2 Device 3 w/ EP + ETP overlap 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 w/o EP + ETP overlap a2a-dispatch allgather expert reduce scatter a2a-combine w/ EP + ETP overlap Device 0 0 0 (b) Device Device 2 Device 3 0 1 1 1 2 2 3 3 (c) Device 0 Device 1 Time 0 0 0 0 1 1 1 2 2 3 3 3 0 1 1 1 Figure 10 EP-ETP Overlap Illustration. (a) EP-ETP overlap with the lower computation portion. (b) EP-ETP overlap with the higher computation portion. (c) EP-ETP overlap with fewer groups. Compared with (a) and (b), it shows that if the compute time cost is longer, the efficiency will be better. Comparing with (b) and (c), it shows that fewer groups will lead to insufficient overlap. risk of becoming CPU-bound. Given that the proportion of ETP (Expert Tensor Parallel) in the overall MoE (Mixture of Experts) architecture is not substantial, it is crucial to make adjustments based on the specific context and requirements. Through the aforementioned optimization strategies, we achieve balanced configuration of storage and computational intensity for the specific expert specifications in the MoE (Mixture of Experts) structure of the MiniMax-Text-01 model. Furthermore, based on these optimizations, we reduce the pure communication overhead of the MoE component by 50% compared to the preoptimization state, resulting in significant improvement in training efficiency. 3.2. Long Context Optimization significant challenge in long context training is that real training samples are difficult to standardize into uniform length. The conventional approach of using padding to make samples the same length leads to substantial computational waste. In the context of training at the 1M sequence length scale, this waste becomes particularly significant. To address this issue, we adopt data formatting technique during training where different samples are concatenated end-to-end along the sequence dimension. We refer to this technique as "data-packing". This format minimizes computational waste during the computation process, thereby conserving computational resources. 3.2.1. Varlen Ring Attention For Softmax Attention, the ring attention algorithm (Liu et al., 2024a) offers an effective method to partition data, thereby enabling unlimited scalability. However, the existing implementations MiniMax-01: Scaling Foundation Models with Lightning Attention causal compute causal varlen compute non-causal compute non-causal varlen compute (a) (b) Figure 11 Ring Attention v.s. Varlen Ring Attention. (a) No data packing in ring attention. (b) Pack 3 samples with different lengths in varlen ring attention. are not optimized to efficiently handle the ring attention mechanism for the data-packing format. In the case of FlashAttention (Dao, 2024), while it provides varlen (variable length) interface to accommodate the data-packing format, there is no corresponding ring attention implementation available. Regarding TransformerEngine (NVIDIA, 2023), the implementation incorporates Context Parallel (CP) ProcessGroup to support the ring attention algorithm. However, this approach poses risk of computational resource waste when dealing with the data-packing format. This is because the algorithm divides each sequence into 2 𝑠𝑖𝑧𝑒𝐶𝑃 segments and applies the ring attention mechanism to each segment. Consequently, this approach restricts each sequence to length that must be an integer multiple of 2 𝑠𝑖𝑧𝑒𝐶𝑃. In scenarios where the sample distribution is unknown and the CP size is set to large value, this can lead to significant padding, resulting in the waste of computational resources. Motivated by the principle of not making assumptions about the sample distribution, we redesign the algorithm and name it Varlen Ring Attention. This approach avoids the excessive padding and subsequent computational waste associated with traditional methods by applying the ring attention algorithm directly to the entire sequence after data-packing. Specifically, the implementation involves distinguishing the offset of the attention mask corresponding to each sequence within the ring attention computation. The key modification is to transform the original causal computations into varlen causal computations and similarly convert the non-causal computations into varlen non-causal computations, shown in Figure 11. 3.2.2. Improved Linear Attention Sequence Parallelism For lightning attention, the LASP (Linear Attention Sequence Parallelism) algorithm (Sun et al., 2024) leverages the communication group of CP to facilitate the expansion of long sequences. As illustrated in Figure 12 (a), the LASP algorithm mandates that all CP ranks engage in send-recv operations to exchange intermediate key-value (𝐾𝑉) block results. This requirement imposes sequential dependency among the CP ranks, thereby compelling the computation to be performed in serial manner. Consequently, this sequential dependency significantly impedes the overall efficiency of the training process, as the inherent parallelism of the system is not fully exploited. To fully harness the parallel computing capabilities of GPU devices, we propose an optimized approach that refines the computational and communication workflow to eliminate dependencies during the computation process. This optimization effectively transforms serial computation into parallelized one. The enhanced approach, termed LASP+ (Figure 12 (b)), operates as follows: 1. Local Prefix Sum Calculation: Each computing node i.e., the CP rank, initiates the process by 17 MiniMax-01: Scaling Foundation Models with Lightning Attention block size padding init 𝐾𝑉 = 𝐾0𝑉0 with zeros shape [𝑑, 𝑑] init diag with decay shape [𝑑, 𝑑] input sequence, shape [𝑠, ℎ, 𝑑] [ℎ, 𝑠, 𝑑] 𝑄,𝐾,𝑉 have the same shape, split sequence with CP 𝑄2 𝐾2 𝑉2 𝑄4 𝐾4 𝑉4 4 𝑄3 𝐾3 𝑉3 3 recv 𝑄6 𝐾6 𝑉 6 𝑄5 𝐾5 𝑉5 5 recv 𝑄7 𝐾7 𝑉7 recv 8 deprecate send sum(0-2) sum(0-3) send sum(0-4) sum(0-5) send sum(0-6) sum(0-7) deprecate output sequence, shape [ℎ, 𝑠, 𝑑] [𝑠, ℎ, 𝑑] Initialize Phase (a) 𝑄 𝐾 𝑉 𝑄1 𝐾1 𝑉1 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 = (𝑄𝑖𝐾𝑖 𝑀)𝑉𝑖 [ℎ, 𝐵, 𝑑] 𝐾𝑖𝑉𝑖 = 𝐷 𝐾𝑖 𝑉𝑖 [ℎ, 𝑑, 𝑑] 𝐾𝑉 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 = 𝐷 𝑄𝑖 𝐾𝑉 [ℎ, 𝐵, 𝑑] 𝑂𝑖 = 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 + 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 0 𝐾𝑉 += 𝐾𝑖𝑉𝑖 sum(0-1) 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 = (𝑄𝑖𝐾𝑖 𝑀)𝑉𝑖 [ℎ, 𝐵, 𝑑] 𝐾𝑖𝑉𝑖 = 𝐷 𝐾𝑖 𝑉𝑖 [ℎ, 𝑑, 𝑑] 1 2 3 4 5 7 8 deprecate local prefix sum 𝐾𝑉𝐿 sum(1-2) sum(3-4) sum(5-6) sum(7-8) (b) allgather across ranks global prefix sum 𝐾𝑉𝐺 sum(0) sum(0-1) sum(0-2) sum(0-3) sum(0-4) sum(0-5) sum(0-6) sum(0-7) 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 = 𝐷 𝑄𝑖 𝐾𝑉𝐺 [ℎ, 𝐵, 𝑑] 𝑂𝑖 = 𝑂𝑖𝑛𝑡𝑒𝑟 𝑖 + 𝑂𝑖𝑛𝑡𝑟𝑎 𝑖 output sequence, shape [ℎ, 𝑠, 𝑑] [𝑠, ℎ, 𝑑] Figure 12 Difference of LASP Algorithm and LASP+ Algorithm. (a) LASP Algorithm. 1. Initialization Phase: initializing KV to zero and the diagonal decay matrix. 2. Data Partitioning and Padding: partitioning the Q, K, and matrices along the sequence dimension into CP size (4 segments illustrated in the figure) blocks, dividing each block into smaller blocks based on the BlockSize 𝐵 and padding the remaining part (e.g. Q7, K7, V7) that cannot be divided evenly by 𝐵. 3. Intra-block Computation: performing intra-block of each CP rank computations in parallel. 4. Inter-block Computation and Communication: starting from CP rank 0, computing the inter-block portion of the current 𝑄𝑖 with all previous KV blocks and the prefix sum 𝐾𝑖𝑉𝑖. Different CP ranks communicate data through send-recv operations. (b) LASP+ Algorithm. Building upon figure (a), each CP rank computes the local prefix sum 𝐾𝑉𝐿 and performs AllGather operation to synchronize, then selects the local prefix sum 𝐾𝑉𝐿 to compute the global prefix sum 𝐾𝑉𝐺. The remaining computational components are same as (a). 18 MiniMax-01: Scaling Foundation Models with Lightning Attention independently calculating its local prefix sum, denoted as 𝐾𝑉𝐿. 2. Global Synchronization via AllGather: Following the local calculations, an AllGather operation is performed to synchronize the information from all nodes globally. This step ensures that each node has access to the necessary data from all other nodes. 3. Prefix Sum Computation: Each node selects the specific CP ranks 𝐾𝑉𝐿 on which to perform the prefix sums, decision based on its assigned computation order. By implementing these steps, the LASP+ approach effectively removes the original dependencies between the computation nodes. This elimination of dependencies facilitates fully parallelized computation process, thereby significantly enhancing the overall efficiency and throughput of the system. The transformation from serial to parallel computation not only leverages the full potential of GPU devices but also ensures that the training process can be executed more rapidly and with greater scalability. The proposed modifications, while incurring additional costs in terms of increased total communication volume and temporary memory usage, are unequivocally justified by the substantial performance benefits they confer. These enhancements significantly outweigh the associated overhead in communication and memory consumption. Through comprehensive testing and verification, it is empirically demonstrated that the computation speed in the LASP+ approach can attain up to 1/𝑁𝑝𝑐𝑛 of the original LASP algorithm, where 𝑁𝑝𝑐𝑛 denotes the number of parallel computing nodes. Furthermore, the overhead introduced by the AllGather operation is minimal, which is consistent with our anticipations and underscores the efficacy of the optimization. Building upon the LASP+ framework, we further introduce support for the varlen feature to effectively manage the data-packing data structure. This enhancement is particularly beneficial for handling batched samples that comprise inputs with unequal token lengths. The ', 'summary': '<h2>Оптимизация MoE и длинных контекстов в модели MiniMax-Text-01</h2>\n<p>В статье рассматриваются оптимизации, примененные к архитектуре MoE (Mixture of Experts) и к обучению с длинными контекстами в модели MiniMax-Text-01.</p>\n<h3>Оптимизация MoE</h3>\n<p>Основная цель оптимизации MoE — минимизировать накладные расходы на коммуникацию, особенно для моделей, использующих all-to-all (a2a) коммуникацию. Для этого вводится схема перекрытия вычислений на основе группировки токенов. В этой схеме a2a коммуникация выполняется внутри группы параллелизма экспертов (EP), и она перекрывается с обработкой токенов из разных групп экспертов. Чтобы гарантировать правильность коммуникации, каждая группа процессов выполняет коммуникационные операции последовательно. Из-за этого a2a коммуникации между разными группами не могут перекрываться, что приводит к простоям.</p>\n<p>Однако, при более детальном анализе, выявился компромисс, специфичный для конфигурации экспертов модели MiniMax-Text-01. При использовании параллелизма тензоров (TP) для разделения параметров экспертов, вычислительная интенсивность становится слишком низкой, что снижает эффективность вычислений. Но отказ от TP приводит к слишком большому количеству параметров, что требует активации параллелизма конвейера (PP). Проблема в том, что PP не уменьшает объем памяти, необходимый для хранения активаций. Это особенно плохо для обучения моделей с длинными контекстами, так как увеличение потребления памяти не дает пропорциональных преимуществ в скорости обучения.</p>\n<p>Поэтому была разработана новая стратегия разделения параметров, которая балансирует использование памяти и вычислительную интенсивность. Вводятся новые группы процессов:</p>\n<ul>\n<li><strong>ETP (Expert Tensor Parallel)</strong>: управляет разделением весов экспертов.</li>\n<li><strong>EDP (Expert Data Parallel)</strong>: инкапсулирует параллелизм данных идентичных экспертов.</li>\n</ul>\n<p>Общее количество GPU, участвующих в обучении (world_size), должно удовлетворять двум условиям, связывающим размеры групп параллелизма:</p>\n<p><code>world_size = sizePP * sizeDP * sizeCP * sizeTP</code>\n<code>world_size = sizePP * sizeEDP * sizeETP * sizeEP</code></p>\n<p>Такая конфигурация позволяет гибко распределять экспертов, управлять разделением их весов и независимо настраивать алгоритм ZeRO (Zero Redundancy Optimizer). Это позволяет полностью отделить стратегии параллелизма компонентов MoE от стратегий не-MoE компонентов. ETP можно гибко настраивать для достижения оптимального баланса между использованием памяти и вычислительной интенсивностью.</p>\n<p>Для снижения накладных расходов на коммуникацию используется стратегия перекрытия EP-ETP. Это максимизирует использование как сетевых, так и вычислительных ресурсов. Коммуникации внутри одной группы процессов выполняются последовательно, поэтому более длительное время вычислений позволяет перекрывать больше коммуникаций и создавать возможности для перекрытия коммуникаций между разными группами, что повышает производительность.</p>\n<p>Количество групп требует компромисса. Теоретически, только разделив работу на достаточно большое количество групп, можно достичь достаточного перекрытия между коммуникацией и вычислениями. Однако на практике слишком большое количество групп может усложнить планирование и привести к ограничению по CPU. Так как доля ETP в архитектуре MoE невелика, необходима настройка на основе конкретных условий.</p>\n<p>Представленные оптимизации позволили сбалансировать использование памяти и вычислительную интенсивность для экспертов в структуре MoE модели MiniMax-Text-01. Кроме того, удалось снизить накладные расходы на коммуникацию компонента MoE на 50% по сравнению с состоянием до оптимизации, что привело к значительному повышению эффективности обучения.</p>\n<h3>Оптимизация длинных контекстов</h3>\n<p>Основная проблема при обучении с длинными контекстами заключается в том, что реальные обучающие примеры трудно стандартизировать до одинаковой длины. Использование паддинга для выравнивания длины приводит к значительным вычислительным потерям, особенно при обучении на последовательностях длиной в 1 миллион токенов.</p>\n<p>Для решения этой проблемы используется техника "data-packing", когда разные примеры конкатенируются в последовательность. Это минимизирует вычислительные потери во время вычислений.</p>\n<h4>Varlen Ring Attention</h4>\n<p>Алгоритм кольцевого внимания (ring attention) эффективно разделяет данные, обеспечивая неограниченную масштабируемость. Однако существующие реализации не оптимизированы для формата data-packing. FlashAttention предоставляет интерфейс varlen (переменной длины), но не имеет реализации кольцевого внимания. TransformerEngine использует Context Parallel (CP), но это приводит к потерям при использовании data-packing, так как алгоритм делит последовательность на сегменты, длина которых должна быть кратна <code>2 * sizeCP</code>.</p>\n<p>Чтобы избежать потерь, связанных с паддингом, был разработан алгоритм Varlen Ring Attention. Он применяет кольцевое внимание непосредственно ко всей последовательности после data-packing. Ключевое изменение заключается в различении смещения маски внимания, соответствующей каждой последовательности. Таким образом, обычные вычисления внимания превращаются в вычисления внимания с переменной длиной.</p>\n<h4>Улучшенный параллелизм последовательностей линейного внимания (LASP)</h4>\n<p>Алгоритм LASP (Linear Attention Sequence Parallelism) использует группу коммуникации CP для расширения длинных последовательностей. Но LASP требует, чтобы все ранги CP участвовали в операциях send-recv для обмена промежуточными результатами key-value (KV). Это создает последовательную зависимость между рангами CP, что препятствует параллелизму вычислений.</p>\n<p>Для полного использования параллельных возможностей GPU предложен оптимизированный подход, который устраняет зависимости во время вычислений. Это позволяет полностью раскрыть параллелизм системы и повысить эффективность обучения.</p>'}, {'title': 'Optimizing inference in lightning attention mechanism', 'content': 'process involves the following steps: 1). Padding to Block Size: Each input within the batch is padded to ensure that its length is multiple of the predefined block size, which is set to 256. This padding step is crucial for aligning the data structure with the computational requirements of the kernel. 2). Sequential Concatenation: After padding, the inputs are sequentially concatenated. This concatenation facilitates the use of single kernel to perform parallel computations across multiple batches. By organizing the data in this manner, we can efficiently leverage the parallel processing capabilities of the GPU, thereby optimizing computational performance. The integration of the varlen feature with the LASP+ framework ensures that the system can handle diverse input lengths without compromising on efficiency. This approach not only simplifies the computational workflow but also maximizes resource utilization by enabling the processing of multiple batches concurrently. 3.3. Lightning Attention Inference Optimization The initial implementation of the lightning attention mechanism is primarily research-oriented and not yet suitable for practical applications, especially for inference. However, the optimization of inference processes is of paramount importance in real-world scenarios, as the long-term cost of deploying trained model is predominantly determined by the efficiency of its inference. To this end, we implement four optimization strategies for lightning attention: batched kernel fusion, separated prefill and decoding execution, multi-level padding, and strided batched matmul extension. 19 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.1. Batched Kernel Fusion We fuse multiple memory-bound kernels and extend support to accommodate all batch inputs. In the prefill phase, we perform kernel fusion for processing the 𝑄, 𝐾, and 𝑉 tensors, including padding in the sequence dimension, partitioning into blocks, adjusting the internal layout, and computing the decay values. In the decoding phase, we perform kernel fusion for the computation of 𝐾𝑉 and the updating of the prefix 𝐾𝑉 cache. These kernel fusions reduce intermediate result storage and memory access operations, thereby significantly improving memory access efficiency and reducing end-to-end latency by 10% in the decoding phase and short-text input scenarios. By the way, these optimizations can bring very noticeable benefits on H20 compared to H800. 3.3.2. Separated Prefill and Decoding Execution The implementation of the lightning attention mechanism for long sequence computations primarily revolves around the differentiation between intra-block and inter-block computations. However, this approach is not optimal for inference tasks, particularly in the decoding phase, where the token length is consistently equal to 1. Given that the computational kernel for tokens of length 1 is predominantly memory-bound and necessitates only limited number of GPU Streaming Multiprocessors (SMs), we propose strategy that segregates the processing of tokens with length of 1 from those with length greater than 1. This is achieved by employing two distinct kernels. Subsequently, we utilize two separate CUDA streams to schedule these kernels in parallel, thereby enhancing computational efficiency and ensuring balanced GPU utilization, especially in scenarios involving mixed inputs. For instance, in batch size of 20, where all inputs contain prefix key-value (KV) cache, and the scenario includes one or two inputs with token length of 50 while the remaining inputs have token length of 1, this approach can significantly reduce latency. Specifically, the latency can be approximately equivalent to that of processing only the longer inputs, demonstrating reduction from 100 milliseconds to 50 milliseconds. 3.3.3. Multi-level Padding By applying padding to the 𝑄, 𝐾, 𝑉 tensors along the sequence dimension, the intra-block and interblock components can be effectively decomposed into multiple identical matrix multiplications. This decomposition is particularly advantageous as it aligns seamlessly with the StrideBatchedMatmul interface, thereby facilitating the maximization of parallel processing capabilities. Initially, the block size for padding was set to 256, configuration that was consistent with the training parameters. However, upon the implementation of the prefix cache technique, it is observed that the token lengths within batch typically fall below 256. This discrepancy led to redundant computations within each matrix multiplication operation. To address this inefficiency and minimize unnecessary computations, we propose the introduction of additional segmentation options, specifically 32, 64, and 128. This multi-level padding approach enables the dynamic selection of the computational scale that incurs the minimal padding overhead, based on the current input sequence length. By adopting this approach, the utilization of computational resources is optimized, ensuring that the system operates with increased efficiency and reduced redundancy. This strategic adjustment not only conserves computational resources but also contributes to the overall performance enhancement of the system. 20 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.4. StridedBatchedMatmul Extension We utilize the optimized function cublasGemmStridedBatchedEx from the NVIDIA cuBLAS Library to manage StridedBatchedMatmul operations, thereby ensuring both high performance and versatility across diverse hardware architectures. Concurrently, we are in the process of implementing more extensive kernel fusion strategy, with the objective of substantially improving the computational efficiency of Hopper GPUs. Given that our sequence partitioning block size is configured to 256, the associated General Matrix-Matrix Multiplication (GEMM) operations, which involve matrices of dimensions 256x256, can leverage warpgroup-wide WGMMA instructions for computation. To further enhance memory access efficiency, we integrate the asynchronous operations of the Tensor Memory Accelerator (TMA) and delegate certain preprocessing and postprocessing computational tasks to be executed asynchronously on the CUDA Cores. Ultimately, our goal is to dynamically regulate the number of pipeline stages to adaptively attain optimal performance across both H20 and H800 GPU architectures. This adaptive control mechanism will ensure that the system can efficiently handle varying workloads and hardware configurations, thus maximizing overall computational throughput and resource utilization. By implementing the aforementioned optimizations, we achieve Model Flops Utilization (MFU) exceeding 75% on the H20 GPU for end-to-end inference tasks (Chowdhery et al., 2023). Specifically, in our MiniMax-Text-01 and MiniMax-VL-01 inference, when considering the latency ratio between the attention operation and the Feed-Forward Network (FFN) operation within the MoE structure, the softmax attention constitutes 95% of the latency at sequence length of 1,024,000 tokens. In contrast, the lightning attention implementation contributes to less than 12% of the latency under the same conditions. Our lightning attention implementation exhibits remarkable efficiency in managing heterogeneous batch inputs, which are characterized by diverse sequence lengths. This efficiency is particularly evident in scenarios where some inputs incorporate the prefix caching strategy while others do not. The reduction in latency not only enhances the overall speed of the inference process but also ensures that the system can handle wide range of input types with minimal performance degradation. This adaptability underscores the robustness and versatility of our lightning attention approach in real-world applications. 4. Pre-Training In this section, we provide an overview of the pre-training methodology for MiniMax-Text-01. First, we detail the meticulous construction of our pre-training corpus, with particular emphasis on data quality, standardized formatting, and mixing strategies to maximize model performance. Subsequently, we outline our innovative data experimentation framework, which enables rapid and resource-efficient evaluation of data effectiveness while minimizing computational costs. Lastly, we present an in-depth analysis of the models training hyper-parameters and present hierarchical training approach, which enables context length scaling up to 4 million tokens. 4.1. Data 4.1.1. Pre-training Corpus The pre-training corpus for MiniMax-Text-01 encompasses comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and 21 MiniMax-01: Scaling Foundation Models with Lightning Attention programming code. We enhance corpus quality through several strategic dimensions: Data Quality Enhancement. Superior data quality is fundamental for Large Language Models. We implement sophisticated filtering pipeline, combining rule-based cleaning and deduplication procedures aligned with established practices (Penedo et al., 2023, 2024; Rae et al., 2021). To assess document quality at granular level, we utilize our previous-generation model as the reward labeler (a MoE model with 5B activations and 60B total parameters). Initially, we evaluate multiple quality dimensions including coherence, conciseness, educational value, helpfulness, knowledge richness, and categorical relevance. Through comprehensive analysis, we identify significant correlations among these metrics and ultimately focus on three key dimensions: knowledge depth, practical helpfulness, and categorical distribution, while maintaining other metrics as secondary validation indicators. Data Formatting Optimization. The content from websites and books, once appropriately extracted and cleaned, can naturally be used as high-quality textbooks (Gunasekar et al., 2023) without further formatting. For dialogue and question-answering data, the sequential nature of text inherently captures conversational logic and question-answer relationships. Although humans benefit from additional formatting (e.g., Markdown) for readability and comprehension, we find that heavy formatting can actually diminish data diversity and quality by introducing fixed patterns that constrain the natural variation present in human conversations. Ultimately, to maintain format generalization capabilities and accommodate human preferences in alignment, we implement nested document format with versatile templates for dialogue and QA data, carefully balancing natural comprehension with structural consistency across various interaction patterns. Data Mixture Investigation. We develop sophisticated approach to tuning the data distribution, leveraging our three primary quality metrics. Based on the experiment paradigm detailed in the subsequent section, we discover that while high-scoring content on knowledge depth and helpfulness generally yielded superior performance in capability assessments, completely eliminating lower-scoring content can adversely affect downstream task performance. Therefore, we implement balanced sampling strategy, beginning with uniform distribution across the base corpus, and then adjusting sampling weights to favor high-quality content while maintaining sufficient representation of diverse categories. 4.1.2. Tokenization For tokenization, we employ byte-level Byte Pair Encoding (BPE) (Brown et al., 2020; Shibata et al., 1999), incorporating the pre-tokenizer methodology. We strategically up-sample multilingual content, to enhance the corresponding compression efficiency. The resulting vocabulary size is set to 200K tokens. 4.1.3. Data Experiment To systematically evaluate our design choices regarding pre-training data quality, format, and composition, we conduct extensive ablation experiments. These experiments involve training multiple small-scale MoE models using comparable token quantities but varying data characteristics. This approach enables us to isolate and measure the impact of individual data attributes while maintaining computational efficiency. 22 MiniMax-01: Scaling Foundation Models with Lightning Attention 4.1.3.1 Paradigm Formulation. We conduct Data Experiments to systematically compare the performance of different model variants. Specifically, we formulate experiments as statistical hypothesis tests that compare evaluation metric distributions between baseline model and models trained with different data configurations. When testing the effectiveness of new data corpus D, we formulate our alternative > 𝜇𝑇baseline, where 𝜇 represents the weighted average performance metric and 𝑇 hypothesis as 𝐻1 : 𝜇𝑇D denotes the distribution of evaluation values across test samples. Evaluation. We carefully design our evaluation norms to ensure meaningful insights. We look at wide range of multiple-choice benchmarks, discarding choice indices in query formulation and look at the likelihoods of completion. We observe the distributions of sample-wise log-normalized accuracy log accnorm2, defined as log accnorm2 (𝑥) = log softmax𝑝 (𝑐𝐶𝑥 ) (cid:110) ( 𝑝(𝑐)) (cid:111) , 𝑖 (𝑐) = 𝑝𝑖 (𝑐) where 𝑝 bytes(c) is the byte-normalized probability of choice 𝑐 for sample 𝑖. We choose bytewise normalization to exclude the effect of tokenizer, while alleviating the disfavor towards longer choices. We conduct extensive experiments to ensure that this metric is stable across training, while maintaining the discriminative power of the metric, which is quantified by the ratio Δobvious/𝜎seed, where Δobvious represents the obvious difference in performance between models and 𝜎seed denotes the standard deviation across different random seeds. Experiment Efficiency & Setup. With such statistical setup, we are able to conduct power analysis to decide minimal test sample size while maintaining the MDE (Minimal Detectable Effect) at similar level as our training variance, and guaranteeing 95% confidence level and 80% power for decision making. With the confidence methodologies set, we conduct simple scaling experiments on token amount and the model size, and eventually land at an experiment step of training MoEs of 1B activation and 8B total parameters with 40B tokens of data, where data mixture comprises 20B web documents and 20B data of hypothesi', 'summary': '<h2>Оптимизация вывода для механизма Lightning Attention</h2>\n<p>В данном разделе описываются оптимизации, которые были внедрены для повышения эффективности механизма Lightning Attention при выводе (inference). Изначально, реализация Lightning Attention была ориентирована на исследовательские цели и не подходила для практического применения, особенно в задачах вывода, где важна скорость работы.  Оптимизация вывода критически важна, поскольку именно от нее зависит долгосрочная стоимость использования обученной модели. Для этого были реализованы четыре ключевые стратегии оптимизации:</p>\n<p><strong>1. Слияние пакетных ядер (Batched Kernel Fusion):</strong></p>\n<p>Были объединены несколько операций, связанных с памятью, и расширена их поддержка для работы со всеми входными данными в пакете. На этапе предварительной обработки (prefill) выполняется слияние ядер для обработки тензоров Q, K и V, включая добавление паддинга (padding) по размерности последовательности, разделение на блоки, настройку внутреннего макета и вычисление значений затухания. На этапе декодирования выполняется слияние ядер для вычисления KV и обновления префиксного KV-кеша. Это слияние позволяет уменьшить объем промежуточных результатов, операций доступа к памяти, повысить эффективность доступа к памяти и уменьшить общую задержку на 10% на этапе декодирования и при работе с короткими текстовыми вводами. Эти оптимизации особенно заметны на GPU H20 по сравнению с H800.</p>\n<p><strong>2. Раздельное выполнение prefill и декодирования (Separated Prefill and Decoding Execution):</strong></p>\n<p>Механизм Lightning Attention для длинных последовательностей разделяет вычисления внутри блоков и между блоками. Однако, это не оптимально для вывода, особенно на этапе декодирования, где длина токена обычно равна 1. Поскольку вычислительное ядро для токенов длины 1 в основном ограничено скоростью доступа к памяти и требует лишь небольшого количества потоковых мультипроцессоров (SM) GPU, предлагается стратегия разделения обработки токенов длины 1 и токенов большей длины. Для этого используются два отдельных ядра CUDA, которые запускаются параллельно в двух отдельных потоках, что повышает эффективность вычислений и обеспечивает сбалансированное использование GPU, особенно при смешанных вводах. Например, в пакете из 20 элементов, где все элементы имеют префиксный KV-кеш, а 1-2 элемента имеют длину 50, а остальные 1, такой подход позволяет снизить задержку до уровня обработки только более длинных элементов (например, с 100 мс до 50 мс).</p>\n<p><strong>3. Многоуровневый паддинг (Multi-level Padding):</strong></p>\n<p>Применяя паддинг к тензорам Q, K, V по размерности последовательности, можно разбить вычисления внутри и между блоками на множество идентичных матричных умножений. Это позволяет эффективно использовать интерфейс StrideBatchedMatmul для максимизации параллельной обработки. Изначально, размер блока для паддинга был 256, что соответствовало параметрам обучения. Однако, при использовании префиксного кеша, длина последовательностей в пакете обычно меньше 256, что приводит к лишним вычислениям. Чтобы это исправить, были добавлены дополнительные варианты сегментации: 32, 64 и 128. Это позволяет динамически выбирать размер блока с минимальными накладными расходами на паддинг, в зависимости от текущей длины входной последовательности.</p>\n<p><strong>4. Расширение StridedBatchedMatmul (StridedBatchedMatmul Extension):</strong></p>\n<p>Для управления операциями StridedBatchedMatmul используется оптимизированная функция cublasGemmStridedBatchedEx из библиотеки NVIDIA cuBLAS, что обеспечивает высокую производительность и универсальность на различных аппаратных архитектурах. Также ведется работа по расширению слияния ядер для повышения эффективности вычислений на GPU Hopper. Поскольку размер блока для разделения последовательности составляет 256, операции GEMM (General Matrix-Matrix Multiplication) с матрицами 256x256 могут использовать инструкции WGMMA для вычислений. Для повышения эффективности доступа к памяти используются асинхронные операции Tensor Memory Accelerator (TMA), а некоторые задачи предварительной и постобработки выполняются асинхронно на ядрах CUDA. Цель состоит в том, чтобы динамически регулировать количество этапов конвейера для достижения оптимальной производительности на GPU H20 и H800.</p>\n<p>Благодаря этим оптимизациям, коэффициент использования вычислительных ресурсов модели (MFU) превышает 75% на GPU H20 при выводе. В частности, при сравнении задержки операций внимания и операций Feed-Forward Network (FFN) в структуре MoE, softmax внимание занимает 95% задержки при длине последовательности 1 024 000 токенов, в то время как Lightning Attention – менее 12%. Реализация Lightning Attention эффективно обрабатывает неоднородные пакетные данные с различными длинами последовательностей, особенно когда некоторые элементы используют префиксное кеширование, а другие нет.</p>'}, {'title': 'Optimizing LLM performance through repetition-aware deduplication strategies', 'content': 'The incorporation of repeated data has been empirically demonstrated to introduce several detrimental effects on the models performance and generalization capabilities (Hernandez et al., 2022). Consequently, implementing deduplication strategies is essential for optimizing LLM performance. Recent studies (Abdin et al., 2024; Penedo et al., 2024) suggest that repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times, where the repetition is measured by MinHash similarity(Broder, 1997; Lee et al., 2022). However, our empirical analysis reveals that their experimental paradigm is inadequate for assessing the impact of repetition, as data efficiency is not consistent throughout the training process. To achieve better alignment with the results of the full training, we introduce novel repetitionaware experimental framework. Specifically, we first perform global deduplication on the dataset to remove redundant entries. Then, we down-sample the documents to align the repetition frequency with the requirements of the final training schedule while adhering to the budget constraints of our ablation experiments, different from the previous experimental setups which directly adopted data distributions identical or similar to those used in the final training stage. Our findings indicate that low-quality data suffer substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs, similar to previous 23 MiniMax-01: Scaling Foundation Models with Lightning Attention observations (Muennighoff et al., 2023). Notably, the solution derived from the proposed framework yields better alignment with the results obtained using considerably more computational resources. By carefully controlling the repetition and quality of the training data, we achieve more efficient and effective data mixture, ultimately leading to better model performance. 4.2. Training Strategy Initial Pre-training. We initialize all model parameters using the Xavier initialization method (Glorot and Bengio, 2010), the scaling factors of DeepNorm (Wang et al., 2024a) are set to 𝛼 = (2𝑁)0.25 and 𝛽 = (8𝑁) 0.25, where 𝑁 denotes the number of layers. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with 𝛽1 = 0.9, 𝛽2 = 0.95, and the weight decay is set to 0.1. The training sequence length is 8192, and the batch size is progressively scaled from an initial size of 16M to 32M at 69B tokens, to 64M at 790B tokens, and finally to 128M at 4.7T tokens, where it remains until the end of training. The schedule is designed based on the correlation between training loss and the critical batch size (McCandlish et al., 2018). It is argued that training at the critical batch size yields near-optimal balance between training time and data efficiency (Kaplan et al., 2020). Following this, we fit power-law relationship between the loss and the critical batch size on data from smaller models, as shown in Figure 13. The batch size is doubled when the corresponding loss is reached. The learning rate schedule begins with linear warm-up over 500 iterations to peak value of 2 104, followed by training with constant learning rate for 7.2T tokens. In the latter stages of training, we notice anomalous gradient norm values. This issue is attributed to an excessively high learning rate and we adjusted lr to 1.3 104 for the remaining 3.2T tokens. During the fast decay phase, we train 1T tokens and exponentially decrease the learning rate to 3 105. Additionally, the MoE auxiliary loss coefficient is set to 0.01. Figure 13 The power-law fit for the training loss and the critical batch size, utilizing data from models ranging from 50M to 600M in activated parameters counts. We mark the points where the batch size is doubled with dashed gray lines. Long-Context Extension. We incrementally expand the models training context length to 1M tokens. Due to our architectures effective length extrapolation capabilities, the model successfully demonstrates its ability to process sequences up to 4M tokens in the vanilla Needle-In-AHaystack retrieval task (NIAH) test 2, despite only being trained on contexts up to 1M tokens, as illustrated in Figure 14. Specifically, we employ three-stage training procedure to systematically upsample long-context data across diverse length ranges, while preserving the distributional characteristics of critical domains to preserve short-context evaluation performances steady. The details of the training data mixture, RoPE base frequency, and training length are shown in Table 6. We also mix in 10% of high-quality long-context question-answering data with similar length distribution as long-context pre-training data during the last 20% of training cycles in each stage(Parmar et al., 2024). To mitigate potential instabilities resulting from distributional shifts, we utilize linear interpolation of source-specific weights throughout the transitional phase. This method facilitates gradual and controlled evolution of the data distribution towards the desired target distribution, thereby ensuring training stability 2Same as Gemini (Team et al., 2024a), we use Paul Graham (https://paulgraham.com/articles.html) as the haystack and The special magic {city} number is: {number} as the needle. 24 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 14 4 Million vanilla Needle-In-A-Haystack retrieval task pressure test on MiniMax-Text-01. The token interval is 32K when it is less than 1M, and the token interval is 0.5M when it is greater than 1M. and preserving convergence properties. Additionally, our findings indicate that NIAH is inadequate for effectively monitoring the models performance throughout the training process. This is primarily because NIAH metric performance reaches its peak score early on, specifically within the initial 128K training steps. To tackle this limitation, we evaluate the models intermediate checkpoints using more demanding tasks, which are designed to increase in complexity as training progresses. Notably, despite the escalating difficulty of these tasks, we consistently observe steady improvement in the models performance metrics. This sustained upward trajectory clearly demonstrates the critical importance and necessity of implementing long-context continual pretraining. More details are given in Section 5.7.2. Table 6 Long-Context Extension Recipe. For clarity, we categorize the data as follows: data with fewer than 32K tokens are labeled as Short; data ranging from 32K to 128K tokens are labeled as Medium; and data exceeding 128K tokens are categorized as Long. Training Length RoPE Frequency # Tokens Short (%) Medium (%) Long (%) 128K 512K 1M 5M 10M 10M 300B 32B 26B 30 35 30 70 35 30 0 30 40 5. Post-training In this section, we present thorough post-training framework designed to enhance the models general performance, long-context capability, and real-world applicability. Our approach begins with the creation of diverse, high-quality prompt dataset, accompanied by hierarchical reward system that evaluates responses across multiple dimensions: correctness, truthfulness, helpfulness, and harmlessness. The training process consists of Supervised Fine-Tuning (SFT), Offline and Online Reinforcement Learning (RL). Through these phases, we systematically align the model with our defined objectives. Model safety is ensured through exhaustive data mining techniques and specialized harml', 'summary': '<p>В статье рассматривается влияние повторения данных на обучение больших языковых моделей (LLM). Исследования показывают, что повторное использование одних и тех же данных может негативно сказаться на производительности и обобщающей способности модели. Поэтому для оптимизации обучения LLM необходимо применять стратегии дедупликации данных.</p>\n<p>Недавние исследования показали, что повторное обучение на высококачественных документах может улучшить производительность модели. Однако, авторы статьи считают, что предыдущие эксперименты неадекватно оценивали влияние повторения, так как эффективность данных не является постоянной на протяжении всего процесса обучения.</p>\n<p>Для более точной оценки, авторы предлагают новый экспериментальный подход, учитывающий повторение данных. Сначала проводится глобальная дедупликация данных, чтобы удалить дубликаты. Затем, оставшиеся документы подвергаются выборочному уменьшению, чтобы частота их повторения соответствовала финальному расписанию обучения, но при этом учитывались ограничения вычислительных ресурсов. Такой подход отличается от предыдущих экспериментов, где напрямую использовались распределения данных, аналогичные тем, что применялись на финальном этапе обучения.</p>\n<p>Результаты показывают, что низкокачественные данные приводят к значительному снижению производительности после двух эпох обучения. В то же время, высококачественные данные могут эффективно обучаться до четырех эпох. Предложенный подход позволяет получить результаты, более точно соответствующие результатам, полученным при использовании больших вычислительных ресурсов. Таким образом, контролируя повторение и качество обучающих данных, можно добиться более эффективного обучения моделей.</p>\n<p>Кроме того, в статье описана стратегия обучения модели, которая включает в себя:\n* <strong>Начальное предварительное обучение:</strong> используется инициализация параметров модели методом Xavier, оптимизатор AdamW, длина последовательности 8192 токена, и динамически изменяемый размер батча. Размер батча увеличивается по мере обучения, основываясь на зависимости между потерей обучения и критическим размером батча. Скорость обучения начинается с линейного разогрева, затем идет обучение с постоянной скоростью, а в конце обучения скорость обучения экспоненциально снижается.\n* <strong>Расширение контекста:</strong> контекстная длина модели постепенно увеличивается до 1 миллиона токенов. Модель демонстрирует способность обрабатывать последовательности до 4 миллионов токенов, несмотря на то, что обучалась на контекстах до 1 миллиона токенов. Используется трехступенчатая процедура обучения для увеличения количества данных с длинным контекстом, при этом сохраняя характеристики распределения важных доменов. Для стабилизации обучения применяется линейная интерполяция весов. Также отмечается, что метрика NIAH (задача поиска иглы в стоге сена) недостаточно эффективна для мониторинга производительности модели на протяжении всего обучения. Поэтому используются более сложные задачи, сложность которых возрастает по мере обучения.\n* <strong>Пост-тренировка:</strong> для улучшения общих характеристик модели, ее способности работать с длинным контекстом и применимости в реальных условиях, используется разнообразный набор высококачественных промптов, а также иерархическая система вознаграждения, оценивающая ответы по различным критериям (правильность, правдивость, полезность, безопасность). Обучение состоит из этапов контролируемой тонкой настройки (SFT), автономного и онлайн обучения с подкреплением (RL). Безопасность модели обеспечивается с помощью методов интеллектуального анализа данных и специализированного вредоносного обучения.</p>'}, {'title': 'Enhancing long-context processing in reward models', 'content': 'ess reward model. We introduce novel multi-stage training methodology that significantly enhances the models capacity to process extended contexts while maintaining optimal MiniMax-01: Scaling Foundation Models with Lightning Attention performance on shorter sequences. This approach results in robust system capable of handling complex, real-world scenarios. Extensive evaluations conducted across both academic and in-house benchmarks demonstrate that our model achieves top performance across all tasks, while establishing new standards of extremely long-context processing. 5.1. Prompt Collection Our extensive prompt collection encompasses millions of diverse, high-quality queries from various sources. We develop tagging system that categorizes each prompt based on task type, knowledge domain, and difficulty level. The collection process incorporates sophisticated filtering mechanisms to eliminate redundant prompts while maintaining an optimal difficulty distribution. The prompt set spans various domains including long-context, programming, math, logical reasoning, creative writing, function calling, general-knowledge, and safety-related scenarios. 5.2. Reward Model Our reward model framework evaluates responses across four critical dimensions to ensure alignment with our core principles: Correctness. We implement rigorous evaluation system for responses that can be strictly validated. For mathematical and reasoning tasks, we utilize early-version MiniMax-Text-01 to generate binary reward signals based on answer consistency. Programming solutions undergo comprehensive testing in secured sandbox environment, with performance metrics derived from test case success rates. Truthfulness. We employ verification pipeline to assess the factual accuracy of the response. The process involves systematic response sampling, statement decomposition and clustering, crowd-sourced verification, and automated comparison using advanced language models to generate truthfulness scores. Helpfulness. Our evaluation framework assesses compliance with user instructions through both deterministic and probabilistic approaches. We implement automated rule-based constraint verification systems complemented by human evaluation of key metrics including coherence, depth, contextual relevance, and stylistic appropriateness. The final helpfulness score combines multiple evaluation signals through weighted scoring system. Harmlessness. Building upon Constitutional AI principles (Bai et al., 2022b), we develop evaluation criteria encompassing safety protocols, content appropriateness, and legal compliance. Our assessment system leverages carefully calibrated prompts validated against human annotations, with early-version MiniMax-Text-01 providing standardized safety evaluations. 5.3. Supervised Fine-Tuning Our SFT dataset construction involves multi-stage process utilizing domain-specific expert models trained through iterative SFT and RL cycles. We implement rejection sampling (Bai et al., 2022a; Dubey et al., 2024) to generate high-quality responses by the experts, sampling multiple variations per prompt across different temperature settings to select optimal demonstrations measured by the reward hierarchy. The response selection process further incorporates both n-gram and semantic similarity filters to ensure maximum diversity and quality in the training data. 26 MiniMax-01: Scaling Foundation Models with Lightning Attention 5.4. Reinforcement Learning 5.4.1. Offline Reinforcement Learning We incorporate the offline RL phase, i.e., Direct Preference Optimization (DPO) (Rafailov et al., 2023), to optimize the models performance across diverse prompt distributions, owing to its simplicity and ease of data construction for long-context scenarios. We specifically focus on prompts that maintain distributional consistency with those utilized in the SFT stage. To evaluate the impact of prompt selection, we conduct comparative experiments using two prompt categories: SFT-trained prompts and SFT-untrained but homologous prompts. Empirical results demonstrate negligible performance variations between SFT-trained prompts and their untrained counterparts. Thus, we adopt the SFTtrained ones for the offline RL phase. The experimental protocol involves generating responses with varying temperature parameters for each prompt, followed by systematic evaluation using the reward models described in Section 5.2. We then identify the best and the worst responses to construct preference pairs for DPO training. 5.4.2. Online Reinforcement Learning Online learning demonstrates superior sample efficiency and cross-domain generalization capabilities compared to offline learning methodologies. Therefore, we implement online RL to improve model performance, particularly in mathematical reasoning tasks. Our approach emphasizes prompt diversity and prioritizes prompts with moderate success rates to maximize information gain during policy updates. Notably, we employ SFT-untrained prompts during online RL, as our empirical observations indicate that reusing prompts from previous phases resulted in model saturation, characterized by diminished response perplexity. We propose modified Group Relative Policy Optimization (GRPO) (Shao et al., 2024) approach incorporating the following key innovations: Importance Sampling Weight Clipping. The conventional PPO/GRPO implementation employs one-sided clipping (Schulman et al., 2017; Shao et al., 2024), sometimes leading to gradient instability when processing tokens with large policy ratio and negative advantage. To address this issue, we implement additional clipping that abandoned this case in the loss function, which effectively regulates the importance sampling magnitude and mitigates noise propagation. KL Divergence Optimization. Due to the similar gradient instability issue, we reformulate the KL divergence term through theoretical analysis of the variance-bias trade-off to further stabilize gradient behavior, resulting in 𝔻𝐾𝐿(𝜃) = 𝔼𝑡 [SG(𝜋𝜃(𝑎𝑡 𝑠𝑡) 𝜋ref(𝑎𝑡 𝑠𝑡)) log 𝜋𝜃(𝑎𝑡 𝑠𝑡)], where SG() denotes the stop-gradient operator. This formulation maintains policy consistency while reducing gradient variance. Balanced Advantage Estimation. We also ensure equitable reward contributions between positive and negative examples, which proves particularly effective in scenarios with skewed distributions. This approach maintains stable training dynamics by regulating the absolute magnitude of rewards across different example groups. 5.5. Safety Alignment The safety alignment of our model is meticulously addressed throughout both the SFT and RL stages. To strike an optimal balance between the models harmlessness and helpfulness, we employ an approach that encompasses the following key components. MiniMax-01: Scaling Foundation Models with Lightning Attention 5.5.1. Training Data Construction We construct high-quality alignment training data with focus on ensuring data diversity and accuracy. This involves the implementation of several data collection methodologies designed to cover broad spectrum of safety scenarios: Safety-Category Specific Prompts. Leveraging established safety classification standards and insights from safety and domain experts, we generate tailored prompts for specific safety categories. This ensures that the model is exposed to comprehensive set of safety-related scenarios. Real-World User Data Collection. We collect real-world user questions from various web documents to incorporate authentic and diverse safety-related queries into our training ', 'summary': '<h2>Подробное изложение раздела статьи о обучении модели MiniMax-01</h2>\n<p>В данной статье описывается процесс обучения модели MiniMax-01, включающий несколько этапов, направленных на создание мощной системы, способной обрабатывать как короткие, так и очень длинные контексты. Модель была протестирована на различных академических и внутренних бенчмарках, где продемонстрировала превосходные результаты.</p>\n<p><strong>5.1. Коллекция подсказок (промптов)</strong></p>\n<p>Для обучения модели была собрана обширная коллекция из миллионов разнообразных и качественных промптов из разных источников. Каждый промпт был категоризирован по типу задачи, области знаний и уровню сложности.  Процесс сбора включал механизмы фильтрации для удаления дубликатов и поддержания оптимального распределения сложности. Коллекция охватывает широкий спектр областей, включая длинный контекст, программирование, математику, логические рассуждения, творческое письмо, вызовы функций, общие знания и сценарии, связанные с безопасностью.</p>\n<p><strong>5.2. Модель вознаграждения (Reward Model)</strong></p>\n<p>Модель вознаграждения оценивает ответы по четырем ключевым параметрам, чтобы обеспечить соответствие принципам разработки модели:</p>\n<ul>\n<li><strong>Корректность:</strong> Для ответов, которые можно строго проверить, используется система оценки. В математических и логических задачах модель MiniMax-Text-01 генерирует бинарные сигналы вознаграждения на основе соответствия ответа. Решения по программированию проходят тестирование в изолированной среде, где производительность оценивается по успешности тестов.</li>\n<li><strong>Правдивость:</strong> Оценка фактической точности ответа. Процесс включает выборку ответов, декомпозицию утверждений, кластеризацию, проверку с помощью краудсорсинга и автоматическое сравнение с использованием продвинутых языковых моделей для получения оценок правдивости.</li>\n<li><strong>Полезность:</strong> Оценивается соответствие инструкциям пользователя с помощью детерминированных и вероятностных подходов. Автоматическая проверка правил дополняется оценкой людьми по критериям связности, глубины, контекстной релевантности и стилистической уместности. Итоговая оценка полезности формируется на основе взвешенной суммы нескольких оценочных сигналов.</li>\n<li><strong>Безвредность:</strong> Оценка безопасности, соответствия контента и юридических требований, основанная на принципах Constitutional AI. Используются тщательно откалиброванные промпты, проверенные с помощью разметки людьми, а MiniMax-Text-01 обеспечивает стандартизированные оценки безопасности.</li>\n</ul>\n<p><strong>5.3. Обучение с учителем (Supervised Fine-Tuning, SFT)</strong></p>\n<p>Для создания обучающего набора данных SFT используется многоступенчатый процесс с применением экспертных моделей, обученных итеративно через SFT и RL.  Метод отклоняющей выборки используется для генерации высококачественных ответов экспертами, с отбором лучших демонстраций на основе иерархии вознаграждения. Процесс отбора ответов также включает фильтры n-грамм и семантической схожести для обеспечения разнообразия и качества обучающих данных.</p>\n<p><strong>5.4. Обучение с подкреплением (Reinforcement Learning, RL)</strong></p>\n<ul>\n<li><strong>5.4.1. Офлайн обучение с подкреплением:</strong>  Применяется метод Direct Preference Optimization (DPO) для оптимизации производительности модели на различных распределениях промптов. Основное внимание уделяется промптам, которые соответствуют распределению, использованному на этапе SFT. Эксперименты показали, что нет существенной разницы между использованием промптов, обученных на SFT, и их необученными аналогами. Для DPO обучения генерируются ответы с разными параметрами температуры, которые затем оцениваются с помощью модели вознаграждения. На основе этих оценок формируются пары предпочтений для обучения DPO.</li>\n<li><strong>5.4.2. Онлайн обучение с подкреплением:</strong>  Онлайн обучение демонстрирует более высокую эффективность использования данных и обобщающую способность. Для улучшения производительности модели, особенно в задачах математического рассуждения, используется онлайн RL с акцентом на разнообразие промптов и приоритетом промптов со средним уровнем успешности. Во время онлайн RL используются промпты, не участвовавшие в SFT, так как повторное использование старых промптов приводило к насыщению модели. Применяется модифицированный метод Group Relative Policy Optimization (GRPO) со следующими ключевыми инновациями:<ul>\n<li><strong>Ограничение весов важности выборки:</strong> Для предотвращения нестабильности градиента при обработке токенов с большим отношением политик и отрицательным преимуществом, вводится дополнительное ограничение, которое исключает такие случаи из функции потерь.</li>\n<li><strong>Оптимизация дивергенции KL:</strong> Для стабилизации градиента переформулируется член дивергенции KL, что сохраняет согласованность политики при снижении дисперсии градиента.</li>\n<li><strong>Сбалансированная оценка преимущества:</strong> Обеспечивается равномерный вклад вознаграждений между положительными и отрицательными примерами, что эффективно в ситуациях со скошенными распределениями.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5.5. Обеспечение безопасности</strong></p>\n<p>Безопасность модели тщательно прорабатывается на этапах SFT и RL. Для достижения баланса между безвредностью и полезностью используется подход, включающий следующие компоненты:</p>\n<ul>\n<li><strong>5.5.1. Создание обучающих данных:</strong>  Формируются высококачественные данные для обучения с акцентом на разнообразие и точность. Это включает в себя несколько методологий сбора данных для охвата широкого спектра сценариев безопасности:<ul>\n<li><strong>Промпты, специфичные для категорий безопасности:</strong> Используются стандарты классификации безопасности и мнения экспертов для создания промптов для конкретных категорий безопасности.</li>\n<li><strong>Сбор пользовательских данных:</strong>  Собираются реальные вопросы пользователей из различных веб-документов для включения аутентичных запросов, связанных с безопасностью.</li>\n</ul>\n</li>\n</ul>'}, {'title': 'Enhancing model robustness through prompt augmentation and long-context training', 'content': 'data. Prompt Augmentation. We instruct early-version MiniMax-Text-01 to generate additional related prompts based on the collected typical red team attack prompts. This approach aims to expand the diversity of safety scenarios and enhance the robustness of the models safety mechanisms. 5.5.2. Response Generation with Harmless Reward Model To generate safe and appropriate responses, we employ harmless reward model (Bai et al., 2022b) that is developed based on set of detailed safety rules. To prevent the model from producing unreasonable refusals, we carefully integrate principles of helpfulness into the safety rules. This integration plays crucial role in achieving balanced output capability, enabling the model to provide safer responses without compromising its utility to the user. The resulting safety-aligned system demonstrates robust protection against potential misuse while maintaining high performance across intended use cases. 5.6. Training Methodology with Long-Context Adaptation We propose systematic multi-stage training methodology to enhance the models capacity for processing extended contexts, as shown in Tab. 7. This approach is methodically designed to optimize long-sequence handling while maintaining performance efficacy on conventional shorter sequences. The RoPE base frequency is maintained at 10 million throughout the post-training phase to ensure consistency in positional encoding. Stage I: Initial Short-Context Training. The first stage implements SFT with sequences constrained to 8,192 tokens. This foundational phase establishes baseline competency in processing standardlength queries and responses, which constitute the majority of practical applications. We remove the long-context prompts that are longer than 8,192 tokens in this stage. Stage II: Extended Context Training. The second stage implements significant extension of the sequence length to 1,032,192 tokens. This phase incorporates training samples across diverse sequence lengths with 50% long-context prompts, facilitating comprehensive model adaptation to extensive contextual processing. The strategic expansion of the sequence length is fundamental to achieving robust long-context capabilities. Stage III: Short-Context Preference Optimization. In this phase, we revert to 8,192 tokens for sequence length and implement Direct Preference Optimization (DPO). This calibration ensures optimal performance on conventional context sizes while maintaining the previously acquired capabilities. Stage IV: Long-Context Preference Optimization. The fourth stage focuses on reinforcing longcontext processing capabilities through DPO with sequences of 1,032,192 tokens. This phase employs 28 MiniMax-01: Scaling Foundation Models with Lightning Attention training protocols analogous to Stage III with entirely long-context data, adapted for extended sequence lengths. Stage V: Online Reinforcement Learning. The final stage implements short-context Online Reinforcement Learning with sequence length of 8,192 tokens. More details have been outlined in Section 5.4.2. Table 7 Training Recipe for Post-training Alignment. Stage Stage II Stag III Stage IV Stage Sequence Length Epoch Batch Size Max LR Min LR LR Decay 8192 2 128 1e-5 1e-6 Cosine 1032192 2 80 3e-6 3e-6 Constant 8192 1 64 5e-7 5e-8 Cosine 1032192 1 64 5e-7 5e-7 Constant 8192 1 512 1e-6 1e-7 Cosine 5.7. Academic Benchmarks We observe and report open-source shortand long-context benchmarks that highlight our models capabilities across various aspects. Along with the user-oriented evaluations we will discuss in Section 5.8, we show that MiniMax-Text-01 is leading open-source model that achieves top performance in long-context retrieval, understanding, long in-context learning and knowledge-based requests, while performing well in math, reasoning, and code tasks and demonstrating strong usefulness in real-user assistant scenarios. 5.7.1. Core Benchmarks MMLU (Hendrycks et al., 2021a) and MMLU-Pro (Wang et al., 2024b) are widely adopted datasets that assess the extent of models knowledge across broad range of domains. We further observe SimpleQA (Wei et al., 2024), factuality benchmark that challenges the models knowledge boundary, and C-SimpleQA (He et al., 2024b) which is an adapted version of SimpleQA under the Chinese culture. For the observation of reasoning capabilities, we evaluate on GPQA (Rein et al., 2024) for graduate-level knowledge reasoning, and DROP (Dua et al., 2019) for reading comprehension reasoning. We test our models performance on math problem-solving with grade-school-level task GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) that spans from AMC-8 to AIMElevel across 7 subjects. We monitor our models coding capability by observing the Pass@1 rate on HumanEval (Chen et al., 2021) and MBPP Plus (Austin et al., 2021; Liu et al., 2023) datasets. To test the models ability to interpret and execute detailed and nuanced instructions, we evaluate the IFEval (Zhou et al., 2023) benchmark. Furthermore, we observe Arena-Hard-Auto (Li et al., 2024b) that reflects the alignment to human preferences. We adopt greedy decoding and zero-shot chain-of-thought strategy (Wei et al., 2022) in evaluating our instruction-tuned model. We compare with other leading and open-source LLMs, which we evaluate under the same setting, if not reported. We present the performance of MiniMax-Text-01 in Table 8. As shown, MiniMax-Text-01 exhibits remarkable performance across most dimensions. It surpasses all models on C-SimpleQA with its more extensive knowledge boundary under Chinese culture. MiniMax-Text-01 also achieves top-3 performance across MMLU, IFEval, and Arena-Hard, showing its exceptional capability of applying its comprehensive knowledge within given constraints to well satisfy user queries and align with human preferences. Meanwhile, it achieves better MATH pass@1 rate than GPT-4o, Claude-3.5-Sonnet, and Llama-3.1-405B, and exhibits comparable 29 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 8 Performance of MiniMax-Text-01 on core academic benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2.572B-Inst. DeepSeekV3 Llama-3.1405B-Inst. MiniMaxText-01 MMLU MMLU-Pro SimpleQA C-SimpleQA IFEval (avg) Arena-Hard 85.7 74.4 39. 64.6 84.1 92.4 GPQA (diamond) DROP (F1) 46.0 89. GSM8k MATH MBPP + HumanEval 95.6 76.6 76. 90.2 88.3 78.0 28.1 56.8 90. 87.6 65.0 88.8 96.9 74.1 75. 93.7 Evaluated following 0-shot CoT setting. General 86.5 76.4 26. 63.3 88.4 72.7 Reasoning 62.1 89. 86.8 75.8 23.4 59.4 89.4 85. 59.1 89.2 Mathematics 95.2 84.6 75. 86.6 Coding 95.4 83.9 75.9 89. 86.1 71.1 10.3 52.2 87.2 81. 49.0 85.0 95.8 81.8 77.0 86. 88.5 75.9 24.9 64.8 87.3 91. 59.1 91.0 96.7 84.6 78.8 92. 88.6 73.3 23.2 54.7 86.4 63. 50.7 92.5 96.7 73.8 73.0 89. 88.5 75.7 23.7 67.4 89.1 89. 54.4 87.8 94.8 77.4 71.7 86. performance with instructed Qwen2.5-72B on HumanEval. Moreover, MiniMax-Text-01 achieves 54.4 on GPQA Diamond, which exceeds most open-source instruction-tuned LLMs and the latest version of GPT-4o. 5.7.2. Long Benchmarks As previously discussed in the long-context extension part of section 4.2, the NIAH task is kind of simplistic for our model, rendering it insufficient for observing the models optimization progress. Consequently, we shift our evaluation to more challenging tasks. Our current long-context evaluation framework focuses on three primary dimensions: (1) Long-Context Retrieval, (2) Long-Context Understanding, and (3) Long In-Context Learnin', 'summary': '<h2>Улучшение безопасности и возможностей модели MiniMax-Text-01</h2>\n<p>В данном разделе описываются методы, использованные для улучшения безопасности и производительности модели MiniMax-Text-01, особенно в контексте обработки длинных текстов.</p>\n<p><strong>5.5.1. Расширение набора данных для безопасности</strong></p>\n<p>Для улучшения безопасности модели и её устойчивости к атакам, использовался метод расширения набора данных. На основе собранных типовых запросов для атак (red team attack prompts), модель MiniMax-Text-01 ранней версии генерировала дополнительные похожие запросы. Это позволило увеличить разнообразие сценариев, связанных с безопасностью, и сделать защитные механизмы модели более надежными.</p>\n<p><strong>5.5.2. Генерация ответов с использованием модели вознаграждения за безопасность</strong></p>\n<p>Для генерации безопасных и подходящих ответов использовалась модель вознаграждения за безопасность (harmless reward model), разработанная на основе набора подробных правил безопасности. Чтобы избежать необоснованных отказов модели, в правила безопасности были включены принципы полезности. Это позволило достичь баланса между безопасностью и полезностью, обеспечивая безопасные ответы без ущерба для функциональности модели. В результате, система демонстрирует надежную защиту от злоупотреблений, сохраняя при этом высокую производительность в рамках целевого использования.</p>\n<p><strong>5.6. Методология обучения с адаптацией к длинному контексту</strong></p>\n<p>Предложена многоэтапная методология обучения для улучшения способности модели обрабатывать длинные тексты. Эта методология направлена на оптимизацию обработки длинных последовательностей, сохраняя при этом эффективность на более коротких последовательностях. Базовая частота RoPE (метод позиционного кодирования) поддерживается на уровне 10 миллионов на протяжении всего обучения, что обеспечивает согласованность в позиционном кодировании.</p>\n<ul>\n<li>\n<p><strong>Этап I: Начальное обучение на коротком контексте.</strong> На этом этапе модель обучается на последовательностях длиной до 8192 токенов с использованием метода SFT (Supervised Fine-Tuning). Это обеспечивает базовую компетентность в обработке стандартных запросов и ответов. Длинные запросы, превышающие 8192 токена, на этом этапе исключаются.</p>\n</li>\n<li>\n<p><strong>Этап II: Обучение на расширенном контексте.</strong> Длина последовательности значительно увеличивается до 1 032 192 токенов. На этом этапе используются обучающие примеры разной длины, включая 50% длинных запросов. Это позволяет модели адаптироваться к обработке больших объемов контекста.</p>\n</li>\n<li>\n<p><strong>Этап III: Оптимизация предпочтений на коротком контексте.</strong> Длина последовательности возвращается к 8192 токенам, и используется метод DPO (Direct Preference Optimization). Это обеспечивает оптимальную производительность на стандартных размерах контекста, сохраняя при этом ранее полученные навыки.</p>\n</li>\n<li>\n<p><strong>Этап IV: Оптимизация предпочтений на длинном контексте.</strong> Усиливается способность модели обрабатывать длинный контекст с помощью DPO на последовательностях длиной 1 032 192 токенов. Этот этап аналогичен этапу III, но использует только данные с длинным контекстом.</p>\n</li>\n<li>\n<p><strong>Этап V: Онлайн-обучение с подкреплением.</strong> Завершающий этап - онлайн-обучение с подкреплением на коротком контексте (8192 токена).</p>\n</li>\n</ul>\n<p><strong>5.7. Оценка на академических бенчмарках</strong></p>\n<p>Модель MiniMax-Text-01 была протестирована на различных открытых бенчмарках для оценки её способностей в обработке как короткого, так и длинного контекста. Эти тесты показывают, что модель является лидером среди открытых моделей в задачах извлечения информации из длинного контекста, понимания, обучения в контексте и запросов, основанных на знаниях. При этом модель также хорошо справляется с задачами по математике, рассуждению и кодированию, а также демонстрирует высокую полезность в реальных сценариях использования.</p>\n<p><strong>5.7.1. Основные бенчмарки</strong></p>\n<ul>\n<li><strong>MMLU и MMLU-Pro:</strong> Оценивают знания модели в различных областях.</li>\n<li><strong>SimpleQA и C-SimpleQA:</strong> Проверяют знания модели и ее способность отвечать на фактические вопросы, особенно в контексте китайской культуры.</li>\n<li><strong>GPQA и DROP:</strong> Оценивают способности модели к рассуждению и пониманию прочитанного.</li>\n<li><strong>GSM8k и MATH:</strong> Проверяют навыки решения математических задач.</li>\n<li><strong>HumanEval и MBPP Plus:</strong> Оценивают навыки программирования.</li>\n<li><strong>IFEval:</strong> Оценивает способность модели интерпретировать и выполнять сложные инструкции.</li>\n<li><strong>Arena-Hard-Auto:</strong> Отражает соответствие модели человеческим предпочтениям.</li>\n</ul>\n<p>Результаты показывают, что MiniMax-Text-01 превосходит большинство моделей по многим параметрам, включая C-SimpleQA, MMLU, IFEval и Arena-Hard. Кроме того, модель показывает хорошую производительность в математических задачах и задачах на кодирование, сравнимую с результатами других передовых моделей.</p>\n<p><strong>5.7.2. Бенчмарки для длинного контекста</strong></p>\n<p>Оценка возможностей модели по работе с длинным контекстом была переориентирована на более сложные задачи, поскольку NIAH (ранее использовавшийся тест) оказался слишком простым. Тестирование проводится по трем основным направлениям:</p>\n<ol>\n<li><strong>Извлечение информации из длинного контекста</strong></li>\n<li><strong>Понимание длинного контекста</strong></li>\n<li><strong>Обучение в длинном контексте</strong></li>\n</ol>\n<p>Таким образом, данный раздел описывает комплексный подход к обучению и оценке модели MiniMax-Text-01, направленный на повышение ее безопасности, эффективности и способности обрабатывать длинные тексты.</p>'}, {'title': 'Long-context retrieval', 'content': 'This dimension assesses the models memory capabilities, which serve as the foundation for almost all long-context tasks. In addition to vanilla k-M NIAH (Kamradt, 2023), we construct more challenging variation to assess our Long-Context Retrieval performance, namely Multi-Round NeedlesIn-A-Haystack (MR-NIAH), serving as crucial back up for retrieval tasks in long multi-turn dialogue contexts, revealing the fundamental capabilities for building lifelong companion AI assistants. Similar to Multi-round co-reference resolution (MRCR) (Vodrahalli et al., 2024) which is not open-source, we construct haystacks of MR-NIAH as history dialogues, where user queries are synthetic but explicit requests of event descriptions and creative writing. In the last round, the query requests the model to repeat the response of one of the history requests. The haystacks span from 2K to 1M tokens (up to around 2000 interactions), and each needle request is injected at 25%, 50%, and 75% of the 30 MiniMax-01: Scaling Foundation Models with Lightning Attention conversation, respectively. Each ground truth response contains three core components, and we look at an adjusted recall corr. comp. . We show case illustration in Appendix B.2. Figure 15 illustrates comparison results of MR-NIAH. Our model (MiniMax-Text-01, red line) shows strong performance across wide range of sequence lengths in both English and Chinese evaluations. Compared to competing baselines (e.g., GPT, Claude, and Gemini variants), our model also shows less performance degradation at large input lengths, underscoring its robustness for long-context retrieval tasks. 5.7.2.2 Long-Context Understanding This dimension measures the models longcontext understanding ability which contains logical reasoning skills based on long-context inputs. We utilize two comprehensive longcontext QA datasets, Ruler (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2024) to evaluate this aspect. Ruler includes 13 different tasks and notably introduces multi-hop tracing and aggregation tasks to evaluate the complex reasoning abilities of models. We test Ruler up to sequence length of 1M tokens. LongBench-V2 encompasses question-answering tasks of varying difficulty levels across multiple context types, including single and multi-document, multi-turn dialogue, code repositories, and long structured data, among others. Following LongBenchV2 (Bai et al., 2024), we consider two test modes: w/o CoT and w/ CoT, and the text lengths are categorized as follows: Short, ranging from 0 to 32K words; Medium, spanning from 32K to 128K words; and Long, covering 128K to 2M words. Figure 15 MR-NIAH in English and Chinese. As Table 9 illustrates, our model exhibits notable strengths in processing Rulers long-context reasoning tasks. While performance at the 64k input level remains competitive with leading models (including GPT-4o and Claude-3.5-Sonnet) with minimal variation, MiniMax-Text-01 establishes distinct advantage beginning at 128k, achieving impressive scores and surpassing all benchmark models. This superiority becomes particularly pronounced in ultra-long-context scenarios (such as 1M), where MiniMax-Text-01 maintains its commanding lead. Moreover, as evident in Table 103, MiniMax-Text-01 exhibits outstanding capabilities in LongBench-V2s long-context reasoning tasks. The model achieves state-of-the-art results among all evaluated systems in the w/ CoT setting, while also displaying remarkable effectiveness in scenarios w/o CoT. Overall, MiniMax-Text-01 demonstrates exceptional capability in long-context understanding especially reasoning tasks, both with and without CoT reasoning, particularly excelling in scenarios requiring complex reasoning. The exceptional robustness and stability of the model in processing long-context understanding tasks can be attributed to the hybrid architecture with half RoPE and carefully tuned training recipes for both pre-training and alignment, which enhance the models ability to handle long sequences effectively. 3We present the other models performance reported at https://longbench2.github.io/ 31 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 9 Performance comparison of MiniMax-Text-01 on Ruler. Model 4k 8k 16k 32k 64k 128k 256k 512k 1M GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) MiniMax-Text-01 0.970 0.965 0.962 0.960 0.963 0.921 0.960 0.960 0.960 0.961 0.890 0.957 0.960 0.951 0. 0.888 0.950 0.958 0.957 0.954 0.884 0.952 0.938 0.937 0.943 - 0.938 0.917 0.860 0.947 - - 0.916 0.797 0.945 - - 0.861 0.709 0.928 - - 0.850 - 0. Table 10 Performance comparison of MiniMax-Text-01 on LongBench v2. Model Human overall easy hard short medium long 53.7 100.0 25.1 47.2 59. 53.7 w/ CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 51.4 46.7 - 43.5 56.5 54.2 55.2 - 47.9 66.1 w/o CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 50.1 41.0 48.7 42.1 52.9 57.4 46.9 - 42.7 60.9 49.7 41.5 - 40.8 50.5 45.6 37.3 - 41.8 47.9 59.6 53.9 - 48.9 61. 53.3 46.1 - 45.6 58.9 48.6 41.9 - 40.9 56.7 52.4 38.6 - 38.1 52.6 43.5 44.4 - 39.8 47.2 40.2 37.0 - 44.4 43.', 'summary': '<p>В этом разделе рассматриваются возможности модели по работе с длинным контекстом, а именно, её память и способность к пониманию.</p>\n<p><strong>Память в длинном контексте</strong></p>\n<p>Для оценки способности модели извлекать информацию из длинного контекста, используется несколько тестов. Помимо стандартного теста "Иголка в стоге сена" (NIAH), предложен более сложный вариант — "Многораундовая иголка в стоге сена" (MR-NIAH). MR-NIAH имитирует диалоговые ситуации, где в истории переписки содержатся запросы, а в последнем раунде модель должна повторить ответ на один из предыдущих запросов. Стога сена варьируются по длине от 2 тысяч до 1 миллиона токенов (до 2000 взаимодействий), а "иголки" (запросы на повторение) вставляются на 25%, 50% и 75% длины диалога. Качество ответа оценивается по трем компонентам.</p>\n<p>Результаты MR-NIAH показывают, что модель MiniMax-Text-01 демонстрирует высокую производительность на разных длинах последовательностей как на английском, так и на китайском языках. По сравнению с другими моделями (GPT, Claude, Gemini), MiniMax-Text-01 меньше теряет в качестве при увеличении длины ввода, что говорит о её надежности при работе с длинным контекстом.</p>\n<p><strong>Понимание длинного контекста</strong></p>\n<p>Для оценки способности модели понимать длинный контекст и делать логические выводы на его основе, используются два набора данных: Ruler и LongBench-V2.</p>\n<ul>\n<li><strong>Ruler</strong> включает 13 различных задач, в том числе задачи на многошаговое рассуждение и агрегацию данных. Тестирование проводится на последовательностях длиной до 1 миллиона токенов.</li>\n<li><strong>LongBench-V2</strong> содержит задачи на ответы на вопросы разной сложности, охватывающие различные типы контекста: одиночные и множественные документы, диалоги, репозитории кода и структурированные данные. Тестирование проводится в двух режимах: с использованием цепочки рассуждений (CoT) и без неё. Длина текста разделена на три категории: короткий (до 32 тысяч слов), средний (32-128 тысяч слов) и длинный (128 тысяч - 2 миллиона слов).</li>\n</ul>\n<p>Результаты показывают, что MiniMax-Text-01 отлично справляется с задачами на рассуждение в длинном контексте из набора Ruler. На длине ввода 64 тысячи токенов модель показывает сравнимые результаты с лидерами (GPT-4o, Claude-3.5-Sonnet), но начиная со 128 тысяч токенов MiniMax-Text-01 значительно превосходит все остальные модели, особенно в ситуациях с ультра-длинным контекстом (1 миллион токенов).</p>\n<p>В LongBench-V2 модель также показывает отличные результаты, достигая наилучших показателей среди всех протестированных моделей в режиме с CoT и демонстрируя высокую эффективность и без CoT.</p>\n<p>В целом, MiniMax-Text-01 демонстрирует исключительные возможности в понимании длинного контекста, особенно в задачах, требующих сложных рассуждений. Устойчивость и надежность модели в работе с длинными последовательностями объясняется гибридной архитектурой с использованием RoPE и тщательно настроенными алгоритмами обучения.</p>'}, {'title': 'Evaluating long in-context learning in lifelong models', 'content': 'This dimension evaluates the models ability to learn from context, core area of research in lifelong learning. We benchmark our Long In-Context Learning capability with the MTOB (Machine Translation from One Book) (Tanzer et al., 2024) dataset. The task requires model to translate between English and Kalamang, language that is very limited in open data and thus within the training corpus, and the LLM is expected to learn the language only from parts of grammar book and 375 translation examples, all given in the context for each translation query (Appendix B.1). The context length is 81K tokens under half-book setting and 133K tokens under total-book setting. We present our results in Table 11. Figure 16 Changes of eng kalam (ChrF) during the whole long-context extension training process. We carefully examined the pre-training data and found that only very small amount of data contains Kalamang-related content. As result, the eng kalam (ChrF) score of our model is the lowest in the no-context scenario, while other models we compared with likely have had their pre-train or post-train data enhanced with relevant Kalamang data. As well as the delta half and full book metrics, our model surpasses all models in terms of the eng kalam (ChrF) metric. And our model also has comparable performance with other models on kalam eng (BLEURT) metric. 32 MiniMax-01: Scaling Foundation Models with Lightning Attention In the course of long-context extension, as described in section 4.2, we observed gradual enhancement in In-Context Learning ability, as indicated by MTOB, illustrated in Figure 16. While we have explored some remarkable works(Agarwal et al., 2024; Dong et al., 2024) specifically aimed at improving In-Context Learning capabilities, we believe that such ability should merely be one aspect of the reasoning capabilities of long-context models. Therefore, we plan to conduct in-depth research on long-context data quality and scale from more fundamental perspective to further enhance the long-context reasoning capabilities of our model. Table 11 Performance comparison of MiniMax-Text-01 on MTOB. Context Type no context half book full book Δ half book Δ full book GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 eng kalam (ChrF) 54.30 9.90 20.22 53.62 16.79 53.68 12.20 49.50 16.55 48.48 6.0 51.74 kalam eng (BLEURT) - 55.65 57.90 53.30 45.94 51.60 GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 33.20 31.42 32.02 33.80 30.13 33.65 58.30 59.70 61.52 57.50 53.14 57.10 - 62.30 63.09 57.00 32.15 58. 44.40 33.39 36.89 37.30 31.92 45.7 25.10 28.28 29.50 23.70 23.01 23.45 - 35.42 41.11 41.10 29.39 45.6 - 30.88 31.07 23.20 2.02 24.35 5.8. User-in-the-loop While achieving top performance on the core open-source benchmarks, we realize that academic evaluations lack an understanding of real-world user interactions. Hence, we also focus on monitoring and improving user experience through our Hailuo AI 4 by incorporating user-in-the-loop evaluations based on real-world cases and adapting tools for better usability and performance in practical applications. 5.8.1. In-House Evaluations We maintain series of in-house evaluations that include: (1) automatic assessments of General Assistant capabilities, Knowledge Q&A, Creative Writing, Hard Capability, Instruction Following, Coding, Safety, and Long Context, and (2) expert human evaluations. Its worth noting that since our test queries are primarily derived from Hailuo AI user interactions, significant portion of our in-house samples are in Mandarin and deeply rooted in Chinese cultural contexts. Our results indicate notable discrepancy between performance on academic benchmarks and actual user experience, where leading open-source and commercial models can underperform when used as interactive assistants. We show in Table 12 5 that, through our dedicated efforts, MiniMaxText-01 is able to handle these situations quite well. In general, our model outperforms other models 4https://www.hailuo.ai/ 5We omit scores for in-applicable models. 33 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 12 Performance comparison of MiniMax-Text-01 on in-house benchmarks. General Assistant Hard Capability Creative Writing Knowledge Q&A Instruction Following Coding Safety Long Context GPT-4o (11-20) GPT-4o (08-06) GPT-4o (05-13) Claude-3.5-Sonnet (10-22) Claude-3.5-Sonnet (06-20) Gemini-2.0-Flash (exp) Qwen2.5-72B-Inst. DeepSeek-V3 Llama-3.1-405B-Inst. MiniMax-Text70.9 63.5 67.7 66.8 60.5 70. 66.4 66.8 53.3 73.9 73.5 62. 63.3 68.3 67.4 61.8 66.1 68. - 64.8 70.3 66 58.3 54. 51.0 70.0 61.7 64.6 63.6 81. 69.2 68.0 69.6 52.0 51.8 75. 68.9 77.0 46.0 78.6 50.4 49. 49.6 61.5 64.4 39.9 34.1 51. 50.3 46.3 94.0 93.6 93.2 94. 93.6 86.5 93.9 94.0 87.6 90. 85.4 79.7 79.7 92.9 95.0 66. - 74.9 70.7 90.9 86.2 58. 77.2 47.1 47.1 81.9 81.5 77. 60.3 93.8 in common Assistant scenarios, particularly when compared to open-source counterparts. This superiority is most evident in our Creative Writing (Appendix B.5, B.7, B.6) and Knowledge Q&A collections, where it aligns more closely with user intentions than other models, delivering accurate and detailed responses to wide range of queries. In productivity scenarios that require Long Context (Appendix B.3), such as document translation, summarization, and analysis, our model demonstrates high proficiency and reliability. Moreover, we prioritize the safety of our model, as it achieves top-tier performance on our established in-house Safety benchmarks. Meanwhile, we are agile in gathering and updating complex productivity scenarios with multilevel instruction following requests at which our model fails and current LLMs cannot master, constructing our Harder Capability and Instruction Following in-house evaluations. While leading LLMs tend to underperform in these sets, these requests reflect our models limitations when given multi-level instructions, which stems primarily from insufficient training data for specific instruction types. Moving forward, we are committed to substantially expanding our training dataset with high-quality, targeted content to address these gaps and improve model capabilities. 5.8.2. Search in Hailuo AI During user interaction case studies, we find models capability to utilize search tools can compensate for the limited knowledge boundary by accessing real-time, extensive, and precise information from the web. To maximize the models benefits from search while minimizing additional performance degradation, we first carefully pre-define the scope of search scenarios, which cover approximately 30 40% of user queries, including but not limited to precision-demanding, domain-specific, and time-sensitive requests. Meanwhile, to ensure seamless conversation experience, we define the system as invoking tools directly through special tokens, which avoid the complexity of multi-step planning (Chen et al., 2024b) or chain-of-thought reasoning6 that might disrupt the natural flow of the interactions. We create SFT datasets comprising search and non-search decisions across diverse domains, while carefully controlling for other interaction features unrelated to search decisions, such as conversation length, to maintain uniform data distribution across each dimension and prevent overfitting. Importantly, we employ the corresponding reward model of each sample to ensure response quality, failing at which would introduce suboptimal samples into the training 6https://docs.anthropic.com/en/docs/build-with-claude/tool-use 34 MiniMax-01: Scaling Foundation Models with Lightning Attention data, potentially affecting the models fundamental capabilities. The search decision boundary was calibrated to align with the models knowledge boundaries, discarding samples that our model already masters from the search corpus, such as general Chinese knowledge Q&A. After careful assessments by human evaluation experts, we conclude that our models use of the search tool extensively improved user experience, landing at performance leap from 58% to 71.5% on our out-of-domain Hailuo AI end-to-end evaluation (Appendix B.9). Since we are unsure whether other LLM-based assistants include similar search tools, we refrain from making unfair performance comparisons. 6. Vision-language Model By integrating an image encoder and an image adapter into our MiniMax-Text-01 model, we develop MiniMax-VL-01, which extends the capabilities of the model to visual understanding tasks. To ensure robust visual understanding, we design proprietary dataset and implement multi-stage training strategy, where the newly introduced image encoder and adapter first undergo large-scale visual pre-training, followed by comprehensive fi', 'summary': '<h2>Оценка способности к обучению в контексте</h2>\n<p>В этом разделе оценивается способность модели обучаться на основе контекста, что является ключевой областью исследований в области непрерывного обучения. Для оценки этой способности используется набор данных MTOB (Machine Translation from One Book), предназначенный для машинного перевода с использованием одного учебника.</p>\n<p>Задача заключается в переводе между английским и языком каламанг, который имеет очень мало открытых данных и, следовательно, практически не представлен в обучающем корпусе модели. Ожидается, что модель освоит этот язык, опираясь исключительно на части грамматического пособия и 375 примеров перевода, которые предоставляются в контексте для каждого запроса перевода. Длина контекста составляет 81 тысячу токенов в режиме "половина книги" и 133 тысячи токенов в режиме "полная книга".</p>\n<p>Результаты представлены в таблице 11. На рисунке 16 показано изменение метрики ChrF (eng kalam) в процессе обучения с использованием длинного контекста. Тщательный анализ данных предварительного обучения показал, что контент, связанный с языком каламанг, представлен в очень малом объеме. В результате, показатель ChrF (eng kalam) для нашей модели в сценарии без контекста является самым низким, в то время как другие сравниваемые модели, вероятно, имели в своих данных предварительного или последующего обучения больше данных на каламанге. </p>\n<p>Однако, в сценариях с контекстом (половина и полная книга) наша модель превосходит все остальные модели по метрике ChrF (eng kalam). Также наша модель показывает сравнимые результаты с другими моделями по метрике BLEURT (kalam eng).</p>\n<p>В процессе расширения контекста, как описано в разделе 4.2, наблюдалось постепенное улучшение способности к обучению в контексте, что подтверждается данными MTOB и показано на рисунке 16. Хотя были изучены некоторые работы, направленные на улучшение способностей к обучению в контексте, авторы считают, что эта способность должна быть лишь одним из аспектов возможностей рассуждения моделей с длинным контекстом. Поэтому планируется провести углубленное исследование качества и масштаба данных с длинным контекстом, чтобы в дальнейшем улучшить возможности рассуждения модели в этом направлении.</p>\n<p><strong>Комментарий:</strong> <em>В этом разделе статьи описывается, как модель справляется с задачей перевода на редкий язык, основываясь только на предоставленном контексте. Это демонстрирует способность модели к обучению "на лету" и является важным аспектом для моделей с длинным контекстом.</em></p>'}, {'title': 'Dataset and training overview', 'content': 'ne-tuning of the entire pipeline. In the following section, we begin with comprehensive description of the dataset used for training our image encoder and vision-language model. Subsequently, we provide an in-depth overview of the model architecture, followed by an exposition of our four-stage training regimen. We conclude the section by presenting our benchmark results. 6.1. Multimodal Data 6.1.1. Caption Data To pre-train the vision encoder, we curate substantial image-caption dataset by aggregating and filtering data from internet sources. Our Vision Transformer (ViT) is trained using 694 million unique image-caption pairs. To enhance data quality, we acquire refined captions for 180 million images within these pairs. During the training process, we employ an augmentation strategy by randomly sampling raw and refined captions with equal probability (𝑝 = 0.5). 6.1.2. Description Data In existing vision-language models, the utility of descriptive imagery for model training has been well-documented (Li et al., 2024a, 2022, 2023; Schuhmann et al., 2021). To further explore this avenue, we have compiled dataset consisting of 100 million images sourced from open resources such as Common Crawl. Each image in this dataset is paired with fine-grained description, which is initially synthesized by caption model and subsequently refined through humans. On average, these descriptions comprise approximately 300 text tokens per image. Description data serves as robust resource for modal alignment and enhancing understanding in further training. 6.1.3. Instruction Data To train MiniMax-VL-01, we construct comprehensive and diverse instruction-based dataset by synthesizing an extensive range of question-answer (QA) pairs involving visual inputs. These QA pairs are meticulously designed to cover wide array of image-related tasks, such as text extraction, object localization, and geometry problem solving. The dataset generation process prioritizes both diversity and realism, ensuring that the instructions capture varying degrees of complexity and linguistic styles. During training, we apply an augmentation strategy by randomly sampling different types of QA prompts with balanced probabilities, thereby enabling the model to generalize effectively across 35 MiniMax-01: Scaling Foundation Models with Lightning Attention diverse instructional formats and interaction patterns. 6.1.4. Data Distribution To demonstrate the diversity of our VLM data, we uniformly sample 1 million imageinstruction pairs from the instruction data and use another VLM to assign concise tag (e.g., object localization) that represents the primary capability required for each pair. This analysis yielded around 50,000 unique tags, and the top 2,817 tags appeared more than 10 times. The distribution of these prominent tags is visualized in Figure 17, where we further group these top tags into 14 major categories. 6.2. Architecture 6.2.1. Overall Architecture Our MiniMax-VL-01 architecture adheres to the ViT-MLP-LLM paradigm, which has been widely embraced in numerous multimodal large language models (MLLMs). The architecture consists of three main components: Vision Transformer (ViT) with 303 million parameters for visual encoding, two-layer MLP projector initialized randomly for image adaptation, and the MiniMax-Text-01 model serving as the foundational large language model (LLM). Figure 17 Visualization of top tags of sampled instruction data. The category and percentage for each group of clustered tags are displayed in the inner layer, only top-10 tags of each group are displayed for clarity. We implement dynamic resolution strategy by resizing the input image according to predefined grid configuration list, ranging from 336336 to 20162016, while maintaining standard thumbnail at resolution of 336 336. The resized images are subsequently partitioned into non-overlapping patches, each measuring 336 336. Both the image patches and the thumbnail are independently encoded, and their encoded features are concatenated to construct comprehensive image feature representation. In contrast to traditional approaches that rely on pooling or other downsampling techniques to compress feature representations, our model leverages its powerful capacity for processing long sequences, allowing for the direct utilization of raw high-dimensional features during training. This strategy mitigates potential information loss and substantially improves the models adaptability to multi-scale inputs. Moreover, by projecting both image patches and thumbnails into unified feature space, our method significantly enhances the models robustness and representational expressiveness when handling diverse and complex visual inputs. 6.2.2. Vision Encoder We employ lightweight ViT-L/14 (Dosovitskiy et al., 2021) as the foundational structure for our vision encoder and train it from scratch. Following standard pipeline, the input image tensor is initially processed through convolutional layer to extract discrete patches, to which absolute 36 MiniMax-01: Scaling Foundation Models with Lightning Attention positional embeddings are subsequently appended. The resulting tensors are then passed through series of multi-head residual attention blocks. This architecture is particularly effective in capturing intricate visual details and the complex interrelationships within images. We utilize contrastive learning to enhance the alignment between corresponding image-caption pairs while diminishing the alignment between non-corresponding pairs. Specifically, we follow the approach introduced in CoCa (Yu et al., 2022), which augments image-text contrastive learning with an additional decoder and image-text cross-attention mechanisms. The network is jointly optimized using combination of contrastive loss and cross-entropy loss. Our ViT-L/14 model is initially trained at resolution of 224 224 for 37 billion image-caption pairs and subsequently fine-tuned at 336 336 for 1.2 billion pairs. For both resolutions, the captions are truncated to 76 tokens. Our ViT-L/14 encoder achieves zero-shot classification accuracy of 80.55% at 336 336 resolution on the ImageNet-1K dataset. 6.3. Training Recipes We employ four-stage training strategy to enable the model to progressively develop comprehensive multimodal understanding capabilities while retaining its language understanding skills. Additionally, the models question-answering and instruction-following abilities, as well as its alignment with human preferences, are methodically refined throughout these stages. Stage I: Modality alignment. In this stage, our primary objective is to achieve alignment between visual and text tokens by enabling the model to accurately generate appropriate captions for given images. To this end, we update the weights of both the image adapter and the vision encoder to optimize their performance in this multimodal task. During this phase, we utilize total of 80 billion tokens sampled from our image description dataset. Empirically, we have found that increasing the image resolution does not yield improvements in downstream task accuracy. Therefore, all images are processed at fixed resolution of 336 336 to reduce computational costs. Stage II: Enhancement of Vision Understanding. This stage can be regarded as standard instruction tuning phase, during which all model parameters are open to updates. The primary goal is to align the models output with human instructions and enhance its ability to perform diverse range of vision understanding tasks. To achieve this, the model is trained using 420 billion multimodal tokens sampled from our instruction datasets, combined with MiniMax-Text-01 post-training data in ratio of 20:1. This approach ensures that the language modeling capability is maintained while the model acquires new multimodal capabilities. Stage III: Enhancement of User Experience. This stage is designed to further enhance the models capabilities in real-world scenarios and when handling challenging user inputs. We curate sophisticated multimodal data using images sourced from applications that people commonly interact with. Conversations are meticulously labeled to emulate authentic user input and to ensure the provision of accurate, helpful, and diverse responses across multiple conversational turns. The data construction for this stage is guided by an independent human-labeled test set that prioritizes not only accuracy but also the overall quality in terms of user experience. The resulting dataset comprises 44.8 billion multimodal tokens and is trained for one epoch. Stage IV: Enhancement of Preference. In the final stage, we utilize Direct Preference Optimization (DPO) to further enhance model performance and user experience. We construct training dataset consisting of 40,000', 'summary': '<h2>Изложение раздела статьи о модели MiniMax-VL-01</h2>\n<p>В данном разделе описывается процесс обучения модели MiniMax-VL-01, которая представляет собой мультимодальную модель, способную обрабатывать как изображения, так и текст. Обучение включает в себя несколько этапов, начиная с подготовки данных и заканчивая тонкой настройкой модели.</p>\n<p><strong>6.1. Мультимодальные данные</strong></p>\n<p>Для обучения модели использовался обширный набор данных, состоящий из трех основных типов:</p>\n<ul>\n<li><strong>Данные с подписями (Caption Data):</strong> Набор из 694 миллионов пар "изображение-подпись", полученных из интернет-источников. Для 180 миллионов изображений подписи были уточнены. Во время обучения использовались как исходные, так и уточненные подписи с равной вероятностью.</li>\n<li><strong>Данные с описаниями (Description Data):</strong> Набор из 100 миллионов изображений из открытых источников, каждое из которых сопровождается подробным описанием. Описания были сгенерированы моделью, а затем отредактированы людьми. В среднем описания содержат около 300 текстовых токенов. Этот тип данных используется для улучшения выравнивания модальностей и понимания модели.</li>\n<li><strong>Инструкционные данные (Instruction Data):</strong> Набор данных с парами "вопрос-ответ", разработанный для обучения модели выполнению различных задач, связанных с изображениями, таких как извлечение текста, локализация объектов и решение геометрических задач. При обучении использовались различные типы вопросов с равной вероятностью, чтобы модель могла обобщать свои знания.</li>\n</ul>\n<p>Для демонстрации разнообразия инструкционных данных было проведено исследование, в ходе которого 1 миллиону пар "изображение-инструкция" были присвоены теги, описывающие основную задачу. Было выделено около 50 000 уникальных тегов, которые были сгруппированы в 14 основных категорий.</p>\n<p><strong>6.2. Архитектура</strong></p>\n<p>Архитектура MiniMax-VL-01 построена по принципу ViT-MLP-LLM и состоит из трех основных компонентов:</p>\n<ul>\n<li><strong>Vision Transformer (ViT):</strong> Визуальный энкодер с 303 миллионами параметров, отвечающий за кодирование изображений.</li>\n<li><strong>Двухслойный MLP-проектор:</strong> Служит для адаптации признаков изображения к пространству признаков языковой модели.</li>\n<li><strong>MiniMax-Text-01:</strong> Большая языковая модель (LLM), которая является основой для обработки текста.</li>\n</ul>\n<p>Модель использует стратегию динамического разрешения, при которой размер входного изображения изменяется в соответствии с заданным списком конфигураций от 336x336 до 2016x2016, при этом сохраняется стандартная миниатюра с разрешением 336x336. Изображения разбиваются на неперекрывающиеся патчи, которые кодируются отдельно, а затем их признаки объединяются. Такой подход позволяет избежать потери информации и улучшает адаптивность модели к различным масштабам.</p>\n<p>В качестве визуального энкодера используется облегченная модель ViT-L/14, обученная с нуля. Для улучшения выравнивания между изображениями и подписями используется контрастное обучение, дополненное механизмом кросс-внимания. Модель ViT-L/14 сначала обучается при разрешении 224x224 на 37 миллиардах пар "изображение-подпись", а затем дообучается при разрешении 336x336 на 1.2 миллиардах пар. Достигнутая точность классификации на наборе данных ImageNet-1K составляет 80,55% при разрешении 336x336.</p>\n<p><strong>6.3. Этапы обучения</strong></p>\n<p>Обучение модели MiniMax-VL-01 разделено на четыре этапа:</p>\n<ul>\n<li><strong>Этап I: Выравнивание модальностей.</strong> На этом этапе основная цель состоит в выравнивании визуальных и текстовых токенов. Модель обучается генерировать подписи к изображениям. Обновляются веса визуального энкодера и адаптера. Используется 80 миллиардов токенов из набора данных с описаниями. Все изображения обрабатываются с разрешением 336x336.</li>\n<li><strong>Этап II: Улучшение визуального понимания.</strong> На этом этапе происходит тонкая настройка модели для выполнения различных задач, связанных с пониманием изображений. Обучение проводится на 420 миллиардах мультимодальных токенов из набора инструкционных данных, а также данных для дообучения MiniMax-Text-01 в соотношении 20:1.</li>\n<li><strong>Этап III: Улучшение пользовательского опыта.</strong> На этом этапе модель обучается на данных, имитирующих реальное взаимодействие пользователя с приложениями. Данные включают в себя диалоги с метками, обеспечивающими точные и полезные ответы. Используется 44,8 миллиарда мультимодальных токенов.</li>\n<li><strong>Этап IV: Улучшение предпочтений.</strong> На этом этапе используется Direct Preference Optimization (DPO) для дальнейшего улучшения производительности и пользовательского опыта. Используется набор данных из 40 000 примеров.</li>\n</ul>\n<p>В целом, процесс обучения MiniMax-VL-01 направлен на создание мощной мультимодальной модели, способной эффективно обрабатывать изображения и текст, а также отвечать на вопросы и следовать инструкциям.</p>'}, {'title': 'Multi-stage training strategy for image-text pair generation', 'content': ' image-text pairs through the following process: Prompt Selection. Prompts are curated from both instruction data and real user interaction data. These prompts are selected to cover wide range of general scenarios and to specifically address 37 MiniMax-01: Scaling Foundation Models with Lightning Attention persistent issues identified after Stage III, such as occasional repetitive outputs in complex OCR scenarios. Response Generation. We employ diverse strategies, including: generating multiple candidate responses by varying sampling temperature parameters; creating response variants through image weakening in specific scenarios; and using MiniMax-Text-01 to deliberately introduce hallucinations or errors into high-quality responses to generate contrastive samples in specific scenarios. Reward Assignment. Large language models, particularly MiniMax-Text-01, are utilized as evaluators in this stage. Multi-dimensional evaluation criteria are designed to enable systematic and comprehensive assessment of the relationships among prompts, ground truth answers, and generated responses. Pair Construction. Based on the evaluation results, we select the highest-scoring responses as positive samples and the lowest-scoring ones as negative samples, while discarding pairs with insignificant score differences. In addition to incorporating image-text pairs, we also include significant proportion of pure text pairs, as elaborated in Section 5.4.1. It is noteworthy that when Direct Preference Optimization (DPO) is applied to highly capable foundation models, there is propensity for overfitting. To counteract this issue, we adopt an early stopping strategy, which involves terminating the training process prior to the completion of full epoch. This approach is designed to preserve the models generalization capabilities. By following this multi-stage training strategy, we ensure that our model not only demonstrates proficiency in understanding and generating high-quality text but also aligns with human values and safety standards. This comprehensive approach to training allows us to strike balance between model performance and ethical considerations, thereby producing model that is both effective and responsible. 6.4. Benchmarks To assess the performance of our vision-language model, we maintain diverse set of benchmarks, including MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024b), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2023), OlympiadBench (He et al., 2024a), MMLongBench-Doc (Ma et al., 2024), MEGA-Bench (Chen et al., 2024a) and an in-house benchmark. These benchmarks help evaluate the models abilities in various areas, including knowledge, visual reasoning, mathematics, science, long context handling, and user experience. We detail our evaluation configuration for each benchmark in Appendix D. As shown in Table 13, MiniMax-VL-01 achieves competitive performance across various vision-language tasks, demonstrating the following key strengths and limitations: Common Downstream Tasks. In standard vision-language downstream tasks, MiniMax-VL-01 exhibits performance on par with GPT-4o, particularly excelling in visual question answering. This strong performance is attributed to its extensive multi-stage training process, enabling the model to effectively understand and reason across visual and textual inputs. However, MiniMax-VL-01 still struggles with advanced mathematical reasoning tasks, as assessed by OlympiadBench (He et al., 2024a). Long Context. We assess MiniMax-VL-01s capability for long-context comprehension and retrieval using MMLongBench-Doc (Ma et al., 2024). The results show that our model outperforms most counterparts, except GPT-4o-11-20. Despite its strong performance overall, MiniMax-VL-01 demonstrates noticeable gap in both single-page (acc: 47.3%) and cross-page (acc: 28.4%) subsets. 38 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 13 Performance of MiniMax-VL-01 on academic and in-house benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2-VL72B-Inst. InternVL 2.5-78B LLama3.2-90B MiniMaxVL-01 MMMU val+dev MMMU-Pro full ChartQA relaxed DocVQA OCRBench AI2D MathVista testmini OlympiadBenchfull 63. 54.5 88.1 91.1 806 83.1 62. 25.2 72.0 54.7 90.8 94.2 82.0 65.4 28.4 Knowledge 68.4 50. Visual Q&A 88.7 91.5 800 70.6 57. 88.3 92.9 846 Mathematics & Sciences 80.9 70. 32.1 85.1 73.1 46.1 Long Context 64. 43.2 91.2 97.1 856 84.4 69. 21.9 66.5 47.3 91.5 96.1 86.8 68.4 25.1 62.1 36.0 85. 90.1 805 78.9 57.3 19.3 68. 52.7 91.7 96.4 865 83.3 68. 24.2 M-LongDocacc 41.4 31.4 26.2 31. 11.6 19.7 13.9 32.5 MEGA-Benchmacro 49. 51.4 45.9 53.9 46.8 45.3 19. 47.4 Comprehensive In-house Benchmark 62.3 47.0 49. 72.1 40.6 34.8 13.6 56.6 Evaluated following 0-shot CoT setting. User Experience Comprehensive Benchmark. On the recently introduced MEGA-Bench (Chen et al., 2024a), realistic and comprehensive evaluation suite, MiniMax-VL-01 shows competitive overall capabilities, surpassing existing open-source vision LLMs. While it excels in diverse sub-tasks such as knowledge and coding, the model faces challenges in more complex tasks, including planning and metric assessments. In-house User Experience Benchmark. While academic benchmarks often focus on problemsolving, they frequently fail to capture the nuances of real-world user interactions with models. To bridge this gap, we develop an in-house benchmark comprising 90 diverse image-related tasks, each designed with tailored and challenging instructions. The images and instructions in the benchmark are strictly deduplicated to not overlap with the training set at any stage. Task relevance is manually verified, with detailed checklist annotated for each sample to ensure precise evaluation. The final test set consists of 524 meticulously annotated samples in both Chinese and English, but Chinese is primarily used. We illustrate some samples in Appendix C. In win-rate comparison against top-leading vision-language model, our model outperforms all open-source models and approaches the performance of GPT-4o-11-20 with narrow margin. 7. Conclusion and Future work In this report, we present MiniMax-Text-01 and MiniMax-VL-01, two novel models developed entirely from the ground up. These models demonstrate top-tier performance across standard benchmarks, particularly excelling in long-context processing with the ability to handle context windows of up to 4 million tokens. Our research findings challenge the prevailing assumption that state-of-the-art MiniMax-01: Scaling Foundation Models with Lightning Attention language models must be built upon traditional attention mechanisms. By strategically integrating linear attention with optimized hardware utilization and carefully designing training recipes, we have successfully expanded the context window by an order of magnitude. This breakthrough not only enhances the efficiency and scalability of LLMs but also paves the way for future models to support even longer context windows and facilitate the development of more sophisticated AI agents. To promote collaboration and advancement in the field, we have made our model publicly available at https://github.com/MiniMax-AI. For general use and evaluation, we provide Chatbot with online search capabilities (https://www.hailuo.ai/) and the online API (https://intl.minimaxi.com). We are committed to keeping this series open source and will release updates as we develop improved models. While MiniMax-Text-01 and MiniMax-VL-01 show strong performance in general language and vision-language tasks, we acknowledge several limitations that necessitate further exploration: 1. Long-Context Evaluation: Current evaluation datasets for long-context retrieval tasks are primarily designed for artificial or simplified scenarios, and the assessment of long-text reasoning capabilities remains limited in practical applications such as document analysis. We plan to enhance long-context retrieval in more realistic settings and expand the evaluation of longcontext reasoning across wider array of tasks. 2. Model Architecture: The model currently retains 1/8 component with vanilla softmax attention. We are investigating more efficient architectures that can eliminate softmax attention entirely, potentially enabling unlimited context windows without computational overhead. 3. Complex Programming Tasks: The models performance on advanced programming tasks is to be improved, as the coding dataset in our pre-training stage is still limited at the moment. We are continuously improving training data selection and refining continue training procedures to address these limitations in the next model version', 'summary': '<h2>Обучение модели для работы с изображениями и текстом</h2>\n<p>Для обучения модели, способной обрабатывать пары "изображение-текст", используется многоступенчатый процесс. Он включает в себя следующие этапы:</p>\n<p><strong>1. Выбор запросов (Prompt Selection).</strong> Запросы берутся как из обучающих данных, так и из реальных взаимодействий пользователей. Цель – охватить широкий спектр общих сценариев и, в частности, решить проблемы, выявленные на предыдущих этапах обучения, например, повторяющиеся ответы в сложных задачах оптического распознавания текста (OCR).</p>\n<p><strong>2. Генерация ответов (Response Generation).</strong> Используются различные стратегии:\n   * Генерация нескольких вариантов ответа путем изменения параметра температуры (sampling temperature).\n   * Создание вариантов ответов путем "ослабления" изображения в определенных ситуациях.\n   * Использование текстовой модели (MiniMax-Text-01) для намеренного внесения ошибок или "галлюцинаций" в качественные ответы, чтобы получить контрастные примеры для обучения.</p>\n<p><strong>3. Присвоение оценок (Reward Assignment).</strong> Большие языковые модели, в частности MiniMax-Text-01, используются в качестве оценщиков. Разработаны многомерные критерии оценки, позволяющие систематически и всесторонне анализировать взаимосвязи между запросами, "правильными" ответами и сгенерированными ответами.</p>\n<p><strong>4. Формирование пар (Pair Construction).</strong> На основе результатов оценки выбираются ответы с наивысшим баллом в качестве положительных примеров, а с наименьшим – в качестве отрицательных. Пары с незначительной разницей в оценках отбрасываются. Кроме пар "изображение-текст", также включается значительная доля пар "чистый текст".</p>\n<p><strong>Важное замечание:</strong> При применении метода Direct Preference Optimization (DPO) к мощным моделям существует склонность к переобучению. Для борьбы с этим используется стратегия ранней остановки обучения, которая заключается в завершении процесса обучения до завершения полной эпохи. Это помогает сохранить обобщающие способности модели.</p>\n<p>Благодаря такому многоступенчатому подходу модель не только демонстрирует умение понимать и генерировать качественный текст, но и соответствует человеческим ценностям и стандартам безопасности. Это позволяет достичь баланса между производительностью модели и этическими соображениями, создавая эффективную и ответственную модель.</p>\n<h2>Оценка производительности</h2>\n<p>Для оценки производительности модели используются различные бенчмарки, включая: MMMU, MMMU-Pro, ChartQA, DocVQA, OCRBench, AI2D, MathVista, OlympiadBench, MMLongBench-Doc, MEGA-Bench и внутренний бенчмарк. Эти бенчмарки позволяют оценить способности модели в различных областях, таких как знания, визуальное рассуждение, математика, естественные науки, обработка длинного контекста и пользовательский опыт.</p>\n<p>Модель MiniMax-VL-01 демонстрирует конкурентоспособную производительность в различных задачах, связанных с обработкой изображений и текста.</p>\n<p><strong>Ключевые сильные и слабые стороны:</strong></p>\n<ul>\n<li><strong>Стандартные задачи:</strong> Модель показывает производительность на уровне GPT-4o, особенно в задачах визуального вопросно-ответного анализа. Это достигается благодаря многоступенчатому процессу обучения, который позволяет модели эффективно понимать и рассуждать на основе визуальных и текстовых данных.</li>\n<li><strong>Математические задачи:</strong> Модель испытывает трудности в решении сложных математических задач, что было выявлено с помощью бенчмарка OlympiadBench.</li>\n<li><strong>Длинный контекст:</strong> Модель превосходит большинство аналогов в задачах понимания и извлечения информации из длинного контекста, используя бенчмарк MMLongBench-Doc. Однако, она демонстрирует заметный разрыв с GPT-4o-11-20 в этой области.</li>\n<li><strong>MEGA-Bench:</strong> Модель показывает конкурентоспособные результаты на реалистичном и всестороннем бенчмарке MEGA-Bench, превосходя другие открытые модели. При этом модель испытывает трудности в более сложных задачах, таких как планирование и оценка метрик.</li>\n<li><strong>Внутренний бенчмарк:</strong> Для оценки пользовательского опыта был разработан внутренний бенчмарк, состоящий из 90 разнообразных задач, связанных с изображениями. Модель превосходит все открытые аналоги и приближается к производительности GPT-4o-11-20.</li>\n</ul>\n<p>В заключение, модель MiniMax-VL-01 демонстрирует сильные результаты в различных задачах, связанных с обработкой изображений и текста, но нуждается в дальнейшем улучшении в областях математического рассуждения и обработки длинного контекста.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture (1)', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source (1)', '#optimization (1)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В статье представлена серия моделей MiniMax-01, включающая в себя MiniMax-Text-01 (текстовую модель) и MiniMax-VL-01 (модель для обработки текста и изображений). Эти модели сравнимы по производительности с лучшими моделями, но при этом превосходят их по возможностям обработки длинных контекстов.</p>
<p>Ключевым элементом является "молниеносное внимание" (lightning attention) и его эффективное масштабирование. Для максимального использования вычислительных ресурсов, эта технология интегрирована с архитектурой Mixture of Experts (MoE), создавая модель с 32 экспертами и общим количеством параметров в 456 миллиардов. При этом для каждого токена активируется только 45.9 миллиарда параметров.</p>
<p>Разработана оптимизированная стратегия параллельной обработки и эффективные техники перекрытия вычислений и коммуникаций для MoE и "молниеносного внимания". Это позволяет эффективно обучать и использовать модели с сотнями миллиардов параметров на контекстах длиной в миллионы токенов.</p>
<p>Контекстное окно MiniMax-Text-01 может достигать 1 миллиона токенов во время обучения и до 4 миллионов токенов во время использования (inference), при этом сохраняется доступная стоимость. Модель MiniMax-VL-01, обрабатывающая текст и изображения, была создана путем дообучения на 512 миллиардах токенов, содержащих и текст, и изображения.</p>
<p>Эксперименты на стандартных и внутренних наборах данных показали, что модели MiniMax-01 соответствуют производительности передовых моделей, таких как GPT-4o и Claude-3.5-Sonnet, при этом предлагая в 20-32 раза большее контекстное окно.</p>
<p>Модели MiniMax-01 опубликованы в открытом доступе на GitHub.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Experimental setup</div>
                <div class="summary_text"><p>В данном исследовании проводилось обучение моделей с использованием различных механизмов внимания: softmax с FlashAttention-2, lightning attention и гибридного lightning attention. Модели обучались в разных масштабах, от 70 миллионов до 7 миллиардов параметров. Каждая модель обучалась на наборе данных, содержащем до 300 миллиардов токенов, с длиной контекста 8192. Методология обучения следовала подходу, предложенному в работе Chinchilla, где потери при обучении являются прямым индикатором производительности модели на тестовых данных.</p>
<p>Для каждой архитектуры модели и длины обучающей последовательности поддерживался единый глобальный размер пакета в 4 миллиона токенов. Использовался оптимизатор Adam с начальной скоростью обучения 3e-4 и весовым коэффициентом 0.1. Из-за ограниченных вычислительных ресурсов применялся фиксированный планировщик скорости обучения для всех экспериментов.</p>
<p>Для оценки моделей использовался разнообразный набор бенчмарков, включая BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (легкий и сложный варианты), OpenBookQA, Needle in Haystack (NIAH) и SCROLLS. Каждый бенчмарк оценивает различные способности моделей.</p>
<p><em>Комментарий: Таким образом, в исследовании проводилось систематическое сравнение разных архитектур внимания при обучении моделей разных размеров, с использованием фиксированных гиперпараметров и единого подхода к обучению. Это позволяет оценить влияние архитектуры внимания на производительность модели.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Scaling laws</div>
                <div class="summary_text"><p>В данном исследовании изучается, как размер модели (N) и размер обучающего набора данных (D) влияют на ошибку обучения (L) при различных вычислительных ресурсах (C). Цель – понять, как масштабирование этих параметров влияет на производительность модели.</p>
<p>Для начала устанавливаются степенные зависимости между L и C, следуя методологии Chinchilla. Это позволяет вывести коэффициенты для оптимального размера модели (N_opt * C^a) и оптимального размера набора данных (D_opt * C^b).</p>
<p>В качестве основы для анализа используется функция, связывающая ошибку обучения с размером модели/данных. Изначально применялась формула L(X) = (X0/X)^αX, но последующие исследования показали, что формула L(X) = ε + (X0/X)^αX лучше подходит для аппроксимации, где ε – это неустранимая ошибка. Для упрощения, все эти формы объединены в L(X) = βX * X^(-αX), что позволяет напрямую сравнивать масштабируемость моделей на основе параметров αX и βX.</p>
<p>Результаты этих зависимостей представлены в таблице и на графике. Из них следует, что при одинаковых вычислительных ресурсах модели с механизмом "lightning attention" (облегченным вниманием) склонны использовать больше параметров и токенов, но при этом достигают меньшей ошибки, чем модели с обычным softmax вниманием.</p>
<p>Также показано, что более крупные модели и модели с гибридным "lightning attention" достигают наилучших результатов на различных бенчмарках (CSR, NIAH и SCROLLS). Производительность оценивалась на моделях с тремя механизмами внимания, с количеством параметров от 410 миллионов до 7 миллиардов.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Benchmarking training speed and performance of attention mechanisms</div>
                <div class="summary_text"><h2>Анализ производительности и архитектурных решений в моделях с вниманием</h2>
<p>В данном разделе статьи рассматривается скорость обучения различных механизмов внимания и проводится анализ гибридных архитектур, а также исследуется влияние разных вариантов нормализации на производительность моделей.</p>
<p><strong>Скорость обучения механизмов внимания</strong></p>
<p>Сравнивается скорость обучения моделей с 3 миллиардами параметров, использующих softmax внимание, lightning внимание и гибридное lightning внимание. Скорость измеряется в токенах, обработанных на GPU в секунду (TGS). Для сравнения также включены линейные модели HGRN2 и Mamba2. </p>
<p>В ходе тестов контекстная длина постепенно увеличивалась до достижения предела памяти на GPU H800. Результаты показывают, что lightning внимание обеспечивает постоянную скорость обучения независимо от длины последовательности и превосходит FlashAttention2.  В то время как softmax внимание замедляется при увеличении длины последовательности.</p>
<p><strong>Гибридные архитектуры</strong></p>
<p>Исследуются гибридные архитектуры, в которых линейное внимание (например, cosformer2 или hgrn2) комбинируется с softmax вниманием. В моделях hybrid-cosformer2 и hybrid-hgrn2 каждые восемь слоев линейного внимания заменяются на softmax внимание. Эксперименты показывают, что гибридная модель с lightning вниманием (hybrid-lightning) демонстрирует наилучшие результаты по сравнению с другими гибридными моделями.</p>
<p><strong>Сравнение гибридных моделей с окнами внимания</strong></p>
<p>Также сравниваются гибридные модели с lightning вниманием и модели с "окнами" внимания (sliding window attention), где softmax внимание применяется только в пределах заданного окна. Результаты показывают, что гибридная модель с lightning вниманием превосходит все модели с окнами внимания, особенно по метрике NIAH.</p>
<p><strong>Обсуждение результатов</strong></p>
<p>Анализ показывает, что, хотя линейные модели внимания эффективны с точки зрения вычислений, они не подходят для больших языковых моделей (LLM) из-за их неспособности к извлечению информации, что важно для обучения в контексте. В отличие от них, гибридные модели не только не уступают softmax вниманию, но и превосходят его в задачах извлечения и экстраполяции.</p>
<p><strong>Механизм работы softmax внимания</strong></p>
<p>Механизм softmax внимания может быть представлен в виде линейной рекуррентной нейронной сети (RNN). На каждом шаге времени t скрытое состояние пересчитывается, начиная с начального времени t0=1, что позволяет модели сохранять информацию за счет повторного просмотра предыдущих данных. Линейные модели не обладают этим механизмом пересчета, что ограничивает их способность удерживать входные данные.</p>
<p><strong>Вместимость (Capacity) механизмов внимания</strong></p>
<p>Вместимость RNN определяется размером ее рекуррентного состояния. Вместимость softmax внимания составляет O(d), а вместимость lightning внимания - O(d^2/h), где d - размерность модели, а h - число голов внимания. Поскольку d &gt; h, lightning внимание имеет большую вместимость, чем softmax внимание, что объясняет превосходство гибридной модели с lightning вниманием в задачах извлечения и экстраполяции.</p>
<p><strong>Абляция модулей в MoE</strong></p>
<p>Проводятся эксперименты по абляции модулей в архитектуре Mixture of Experts (MoE). Сравнивается гибридное lightning внимание и softmax внимание, а также нормализация до слоя (Pre-Layer Normalization) и нормализация после слоя (Post-Layer Normalization).</p>
<p><strong>Гибридное lightning внимание vs Softmax внимание в MoE</strong></p>
<p>Замена некоторых слоев softmax внимания на lightning внимание в архитектуре MoE с 28 миллиардами параметров улучшает точность на большинстве бенчмарков.</p>
<p><strong>Pre-Layer Normalization vs Post-Layer Normalization</strong></p>
<p>Эксперименты на моделях с 60 миллиардами параметров показывают, что Post-Layer Normalization с использованием DeepNorm превосходит Pre-Layer Normalization по всем метрикам. PostNorm сохраняет эффективную глубину модели, в то время как PreNorm может уменьшать ее.</p>
<p><strong>Спецификация модели</strong></p>
<p>После выбора архитектурных решений следующим шагом является масштабирование модели, что требует тщательной настройки гиперпараметров. Основная цель - достичь баланса между производительностью и эффективностью вывода. Для обеспечения эффективности одно-устройственного вывода, общий размер параметров модели ограничен 500 миллиардами.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing MoE architecture for efficient distributed training</div>
                <div class="summary_text"><h2>Оптимизация MoE и длинных контекстов в модели MiniMax-Text-01</h2>
<p>В статье рассматриваются оптимизации, примененные к архитектуре MoE (Mixture of Experts) и к обучению с длинными контекстами в модели MiniMax-Text-01.</p>
<h3>Оптимизация MoE</h3>
<p>Основная цель оптимизации MoE — минимизировать накладные расходы на коммуникацию, особенно для моделей, использующих all-to-all (a2a) коммуникацию. Для этого вводится схема перекрытия вычислений на основе группировки токенов. В этой схеме a2a коммуникация выполняется внутри группы параллелизма экспертов (EP), и она перекрывается с обработкой токенов из разных групп экспертов. Чтобы гарантировать правильность коммуникации, каждая группа процессов выполняет коммуникационные операции последовательно. Из-за этого a2a коммуникации между разными группами не могут перекрываться, что приводит к простоям.</p>
<p>Однако, при более детальном анализе, выявился компромисс, специфичный для конфигурации экспертов модели MiniMax-Text-01. При использовании параллелизма тензоров (TP) для разделения параметров экспертов, вычислительная интенсивность становится слишком низкой, что снижает эффективность вычислений. Но отказ от TP приводит к слишком большому количеству параметров, что требует активации параллелизма конвейера (PP). Проблема в том, что PP не уменьшает объем памяти, необходимый для хранения активаций. Это особенно плохо для обучения моделей с длинными контекстами, так как увеличение потребления памяти не дает пропорциональных преимуществ в скорости обучения.</p>
<p>Поэтому была разработана новая стратегия разделения параметров, которая балансирует использование памяти и вычислительную интенсивность. Вводятся новые группы процессов:</p>
<ul>
<li><strong>ETP (Expert Tensor Parallel)</strong>: управляет разделением весов экспертов.</li>
<li><strong>EDP (Expert Data Parallel)</strong>: инкапсулирует параллелизм данных идентичных экспертов.</li>
</ul>
<p>Общее количество GPU, участвующих в обучении (world_size), должно удовлетворять двум условиям, связывающим размеры групп параллелизма:</p>
<p><code>world_size = sizePP * sizeDP * sizeCP * sizeTP</code>
<code>world_size = sizePP * sizeEDP * sizeETP * sizeEP</code></p>
<p>Такая конфигурация позволяет гибко распределять экспертов, управлять разделением их весов и независимо настраивать алгоритм ZeRO (Zero Redundancy Optimizer). Это позволяет полностью отделить стратегии параллелизма компонентов MoE от стратегий не-MoE компонентов. ETP можно гибко настраивать для достижения оптимального баланса между использованием памяти и вычислительной интенсивностью.</p>
<p>Для снижения накладных расходов на коммуникацию используется стратегия перекрытия EP-ETP. Это максимизирует использование как сетевых, так и вычислительных ресурсов. Коммуникации внутри одной группы процессов выполняются последовательно, поэтому более длительное время вычислений позволяет перекрывать больше коммуникаций и создавать возможности для перекрытия коммуникаций между разными группами, что повышает производительность.</p>
<p>Количество групп требует компромисса. Теоретически, только разделив работу на достаточно большое количество групп, можно достичь достаточного перекрытия между коммуникацией и вычислениями. Однако на практике слишком большое количество групп может усложнить планирование и привести к ограничению по CPU. Так как доля ETP в архитектуре MoE невелика, необходима настройка на основе конкретных условий.</p>
<p>Представленные оптимизации позволили сбалансировать использование памяти и вычислительную интенсивность для экспертов в структуре MoE модели MiniMax-Text-01. Кроме того, удалось снизить накладные расходы на коммуникацию компонента MoE на 50% по сравнению с состоянием до оптимизации, что привело к значительному повышению эффективности обучения.</p>
<h3>Оптимизация длинных контекстов</h3>
<p>Основная проблема при обучении с длинными контекстами заключается в том, что реальные обучающие примеры трудно стандартизировать до одинаковой длины. Использование паддинга для выравнивания длины приводит к значительным вычислительным потерям, особенно при обучении на последовательностях длиной в 1 миллион токенов.</p>
<p>Для решения этой проблемы используется техника "data-packing", когда разные примеры конкатенируются в последовательность. Это минимизирует вычислительные потери во время вычислений.</p>
<h4>Varlen Ring Attention</h4>
<p>Алгоритм кольцевого внимания (ring attention) эффективно разделяет данные, обеспечивая неограниченную масштабируемость. Однако существующие реализации не оптимизированы для формата data-packing. FlashAttention предоставляет интерфейс varlen (переменной длины), но не имеет реализации кольцевого внимания. TransformerEngine использует Context Parallel (CP), но это приводит к потерям при использовании data-packing, так как алгоритм делит последовательность на сегменты, длина которых должна быть кратна <code>2 * sizeCP</code>.</p>
<p>Чтобы избежать потерь, связанных с паддингом, был разработан алгоритм Varlen Ring Attention. Он применяет кольцевое внимание непосредственно ко всей последовательности после data-packing. Ключевое изменение заключается в различении смещения маски внимания, соответствующей каждой последовательности. Таким образом, обычные вычисления внимания превращаются в вычисления внимания с переменной длиной.</p>
<h4>Улучшенный параллелизм последовательностей линейного внимания (LASP)</h4>
<p>Алгоритм LASP (Linear Attention Sequence Parallelism) использует группу коммуникации CP для расширения длинных последовательностей. Но LASP требует, чтобы все ранги CP участвовали в операциях send-recv для обмена промежуточными результатами key-value (KV). Это создает последовательную зависимость между рангами CP, что препятствует параллелизму вычислений.</p>
<p>Для полного использования параллельных возможностей GPU предложен оптимизированный подход, который устраняет зависимости во время вычислений. Это позволяет полностью раскрыть параллелизм системы и повысить эффективность обучения.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing inference in lightning attention mechanism</div>
                <div class="summary_text"><h2>Оптимизация вывода для механизма Lightning Attention</h2>
<p>В данном разделе описываются оптимизации, которые были внедрены для повышения эффективности механизма Lightning Attention при выводе (inference). Изначально, реализация Lightning Attention была ориентирована на исследовательские цели и не подходила для практического применения, особенно в задачах вывода, где важна скорость работы.  Оптимизация вывода критически важна, поскольку именно от нее зависит долгосрочная стоимость использования обученной модели. Для этого были реализованы четыре ключевые стратегии оптимизации:</p>
<p><strong>1. Слияние пакетных ядер (Batched Kernel Fusion):</strong></p>
<p>Были объединены несколько операций, связанных с памятью, и расширена их поддержка для работы со всеми входными данными в пакете. На этапе предварительной обработки (prefill) выполняется слияние ядер для обработки тензоров Q, K и V, включая добавление паддинга (padding) по размерности последовательности, разделение на блоки, настройку внутреннего макета и вычисление значений затухания. На этапе декодирования выполняется слияние ядер для вычисления KV и обновления префиксного KV-кеша. Это слияние позволяет уменьшить объем промежуточных результатов, операций доступа к памяти, повысить эффективность доступа к памяти и уменьшить общую задержку на 10% на этапе декодирования и при работе с короткими текстовыми вводами. Эти оптимизации особенно заметны на GPU H20 по сравнению с H800.</p>
<p><strong>2. Раздельное выполнение prefill и декодирования (Separated Prefill and Decoding Execution):</strong></p>
<p>Механизм Lightning Attention для длинных последовательностей разделяет вычисления внутри блоков и между блоками. Однако, это не оптимально для вывода, особенно на этапе декодирования, где длина токена обычно равна 1. Поскольку вычислительное ядро для токенов длины 1 в основном ограничено скоростью доступа к памяти и требует лишь небольшого количества потоковых мультипроцессоров (SM) GPU, предлагается стратегия разделения обработки токенов длины 1 и токенов большей длины. Для этого используются два отдельных ядра CUDA, которые запускаются параллельно в двух отдельных потоках, что повышает эффективность вычислений и обеспечивает сбалансированное использование GPU, особенно при смешанных вводах. Например, в пакете из 20 элементов, где все элементы имеют префиксный KV-кеш, а 1-2 элемента имеют длину 50, а остальные 1, такой подход позволяет снизить задержку до уровня обработки только более длинных элементов (например, с 100 мс до 50 мс).</p>
<p><strong>3. Многоуровневый паддинг (Multi-level Padding):</strong></p>
<p>Применяя паддинг к тензорам Q, K, V по размерности последовательности, можно разбить вычисления внутри и между блоками на множество идентичных матричных умножений. Это позволяет эффективно использовать интерфейс StrideBatchedMatmul для максимизации параллельной обработки. Изначально, размер блока для паддинга был 256, что соответствовало параметрам обучения. Однако, при использовании префиксного кеша, длина последовательностей в пакете обычно меньше 256, что приводит к лишним вычислениям. Чтобы это исправить, были добавлены дополнительные варианты сегментации: 32, 64 и 128. Это позволяет динамически выбирать размер блока с минимальными накладными расходами на паддинг, в зависимости от текущей длины входной последовательности.</p>
<p><strong>4. Расширение StridedBatchedMatmul (StridedBatchedMatmul Extension):</strong></p>
<p>Для управления операциями StridedBatchedMatmul используется оптимизированная функция cublasGemmStridedBatchedEx из библиотеки NVIDIA cuBLAS, что обеспечивает высокую производительность и универсальность на различных аппаратных архитектурах. Также ведется работа по расширению слияния ядер для повышения эффективности вычислений на GPU Hopper. Поскольку размер блока для разделения последовательности составляет 256, операции GEMM (General Matrix-Matrix Multiplication) с матрицами 256x256 могут использовать инструкции WGMMA для вычислений. Для повышения эффективности доступа к памяти используются асинхронные операции Tensor Memory Accelerator (TMA), а некоторые задачи предварительной и постобработки выполняются асинхронно на ядрах CUDA. Цель состоит в том, чтобы динамически регулировать количество этапов конвейера для достижения оптимальной производительности на GPU H20 и H800.</p>
<p>Благодаря этим оптимизациям, коэффициент использования вычислительных ресурсов модели (MFU) превышает 75% на GPU H20 при выводе. В частности, при сравнении задержки операций внимания и операций Feed-Forward Network (FFN) в структуре MoE, softmax внимание занимает 95% задержки при длине последовательности 1 024 000 токенов, в то время как Lightning Attention – менее 12%. Реализация Lightning Attention эффективно обрабатывает неоднородные пакетные данные с различными длинами последовательностей, особенно когда некоторые элементы используют префиксное кеширование, а другие нет.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing LLM performance through repetition-aware deduplication strategies</div>
                <div class="summary_text"><p>В статье рассматривается влияние повторения данных на обучение больших языковых моделей (LLM). Исследования показывают, что повторное использование одних и тех же данных может негативно сказаться на производительности и обобщающей способности модели. Поэтому для оптимизации обучения LLM необходимо применять стратегии дедупликации данных.</p>
<p>Недавние исследования показали, что повторное обучение на высококачественных документах может улучшить производительность модели. Однако, авторы статьи считают, что предыдущие эксперименты неадекватно оценивали влияние повторения, так как эффективность данных не является постоянной на протяжении всего процесса обучения.</p>
<p>Для более точной оценки, авторы предлагают новый экспериментальный подход, учитывающий повторение данных. Сначала проводится глобальная дедупликация данных, чтобы удалить дубликаты. Затем, оставшиеся документы подвергаются выборочному уменьшению, чтобы частота их повторения соответствовала финальному расписанию обучения, но при этом учитывались ограничения вычислительных ресурсов. Такой подход отличается от предыдущих экспериментов, где напрямую использовались распределения данных, аналогичные тем, что применялись на финальном этапе обучения.</p>
<p>Результаты показывают, что низкокачественные данные приводят к значительному снижению производительности после двух эпох обучения. В то же время, высококачественные данные могут эффективно обучаться до четырех эпох. Предложенный подход позволяет получить результаты, более точно соответствующие результатам, полученным при использовании больших вычислительных ресурсов. Таким образом, контролируя повторение и качество обучающих данных, можно добиться более эффективного обучения моделей.</p>
<p>Кроме того, в статье описана стратегия обучения модели, которая включает в себя:
* <strong>Начальное предварительное обучение:</strong> используется инициализация параметров модели методом Xavier, оптимизатор AdamW, длина последовательности 8192 токена, и динамически изменяемый размер батча. Размер батча увеличивается по мере обучения, основываясь на зависимости между потерей обучения и критическим размером батча. Скорость обучения начинается с линейного разогрева, затем идет обучение с постоянной скоростью, а в конце обучения скорость обучения экспоненциально снижается.
* <strong>Расширение контекста:</strong> контекстная длина модели постепенно увеличивается до 1 миллиона токенов. Модель демонстрирует способность обрабатывать последовательности до 4 миллионов токенов, несмотря на то, что обучалась на контекстах до 1 миллиона токенов. Используется трехступенчатая процедура обучения для увеличения количества данных с длинным контекстом, при этом сохраняя характеристики распределения важных доменов. Для стабилизации обучения применяется линейная интерполяция весов. Также отмечается, что метрика NIAH (задача поиска иглы в стоге сена) недостаточно эффективна для мониторинга производительности модели на протяжении всего обучения. Поэтому используются более сложные задачи, сложность которых возрастает по мере обучения.
* <strong>Пост-тренировка:</strong> для улучшения общих характеристик модели, ее способности работать с длинным контекстом и применимости в реальных условиях, используется разнообразный набор высококачественных промптов, а также иерархическая система вознаграждения, оценивающая ответы по различным критериям (правильность, правдивость, полезность, безопасность). Обучение состоит из этапов контролируемой тонкой настройки (SFT), автономного и онлайн обучения с подкреплением (RL). Безопасность модели обеспечивается с помощью методов интеллектуального анализа данных и специализированного вредоносного обучения.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Enhancing long-context processing in reward models</div>
                <div class="summary_text"><h2>Подробное изложение раздела статьи о обучении модели MiniMax-01</h2>
<p>В данной статье описывается процесс обучения модели MiniMax-01, включающий несколько этапов, направленных на создание мощной системы, способной обрабатывать как короткие, так и очень длинные контексты. Модель была протестирована на различных академических и внутренних бенчмарках, где продемонстрировала превосходные результаты.</p>
<p><strong>5.1. Коллекция подсказок (промптов)</strong></p>
<p>Для обучения модели была собрана обширная коллекция из миллионов разнообразных и качественных промптов из разных источников. Каждый промпт был категоризирован по типу задачи, области знаний и уровню сложности.  Процесс сбора включал механизмы фильтрации для удаления дубликатов и поддержания оптимального распределения сложности. Коллекция охватывает широкий спектр областей, включая длинный контекст, программирование, математику, логические рассуждения, творческое письмо, вызовы функций, общие знания и сценарии, связанные с безопасностью.</p>
<p><strong>5.2. Модель вознаграждения (Reward Model)</strong></p>
<p>Модель вознаграждения оценивает ответы по четырем ключевым параметрам, чтобы обеспечить соответствие принципам разработки модели:</p>
<ul>
<li><strong>Корректность:</strong> Для ответов, которые можно строго проверить, используется система оценки. В математических и логических задачах модель MiniMax-Text-01 генерирует бинарные сигналы вознаграждения на основе соответствия ответа. Решения по программированию проходят тестирование в изолированной среде, где производительность оценивается по успешности тестов.</li>
<li><strong>Правдивость:</strong> Оценка фактической точности ответа. Процесс включает выборку ответов, декомпозицию утверждений, кластеризацию, проверку с помощью краудсорсинга и автоматическое сравнение с использованием продвинутых языковых моделей для получения оценок правдивости.</li>
<li><strong>Полезность:</strong> Оценивается соответствие инструкциям пользователя с помощью детерминированных и вероятностных подходов. Автоматическая проверка правил дополняется оценкой людьми по критериям связности, глубины, контекстной релевантности и стилистической уместности. Итоговая оценка полезности формируется на основе взвешенной суммы нескольких оценочных сигналов.</li>
<li><strong>Безвредность:</strong> Оценка безопасности, соответствия контента и юридических требований, основанная на принципах Constitutional AI. Используются тщательно откалиброванные промпты, проверенные с помощью разметки людьми, а MiniMax-Text-01 обеспечивает стандартизированные оценки безопасности.</li>
</ul>
<p><strong>5.3. Обучение с учителем (Supervised Fine-Tuning, SFT)</strong></p>
<p>Для создания обучающего набора данных SFT используется многоступенчатый процесс с применением экспертных моделей, обученных итеративно через SFT и RL.  Метод отклоняющей выборки используется для генерации высококачественных ответов экспертами, с отбором лучших демонстраций на основе иерархии вознаграждения. Процесс отбора ответов также включает фильтры n-грамм и семантической схожести для обеспечения разнообразия и качества обучающих данных.</p>
<p><strong>5.4. Обучение с подкреплением (Reinforcement Learning, RL)</strong></p>
<ul>
<li><strong>5.4.1. Офлайн обучение с подкреплением:</strong>  Применяется метод Direct Preference Optimization (DPO) для оптимизации производительности модели на различных распределениях промптов. Основное внимание уделяется промптам, которые соответствуют распределению, использованному на этапе SFT. Эксперименты показали, что нет существенной разницы между использованием промптов, обученных на SFT, и их необученными аналогами. Для DPO обучения генерируются ответы с разными параметрами температуры, которые затем оцениваются с помощью модели вознаграждения. На основе этих оценок формируются пары предпочтений для обучения DPO.</li>
<li><strong>5.4.2. Онлайн обучение с подкреплением:</strong>  Онлайн обучение демонстрирует более высокую эффективность использования данных и обобщающую способность. Для улучшения производительности модели, особенно в задачах математического рассуждения, используется онлайн RL с акцентом на разнообразие промптов и приоритетом промптов со средним уровнем успешности. Во время онлайн RL используются промпты, не участвовавшие в SFT, так как повторное использование старых промптов приводило к насыщению модели. Применяется модифицированный метод Group Relative Policy Optimization (GRPO) со следующими ключевыми инновациями:<ul>
<li><strong>Ограничение весов важности выборки:</strong> Для предотвращения нестабильности градиента при обработке токенов с большим отношением политик и отрицательным преимуществом, вводится дополнительное ограничение, которое исключает такие случаи из функции потерь.</li>
<li><strong>Оптимизация дивергенции KL:</strong> Для стабилизации градиента переформулируется член дивергенции KL, что сохраняет согласованность политики при снижении дисперсии градиента.</li>
<li><strong>Сбалансированная оценка преимущества:</strong> Обеспечивается равномерный вклад вознаграждений между положительными и отрицательными примерами, что эффективно в ситуациях со скошенными распределениями.</li>
</ul>
</li>
</ul>
<p><strong>5.5. Обеспечение безопасности</strong></p>
<p>Безопасность модели тщательно прорабатывается на этапах SFT и RL. Для достижения баланса между безвредностью и полезностью используется подход, включающий следующие компоненты:</p>
<ul>
<li><strong>5.5.1. Создание обучающих данных:</strong>  Формируются высококачественные данные для обучения с акцентом на разнообразие и точность. Это включает в себя несколько методологий сбора данных для охвата широкого спектра сценариев безопасности:<ul>
<li><strong>Промпты, специфичные для категорий безопасности:</strong> Используются стандарты классификации безопасности и мнения экспертов для создания промптов для конкретных категорий безопасности.</li>
<li><strong>Сбор пользовательских данных:</strong>  Собираются реальные вопросы пользователей из различных веб-документов для включения аутентичных запросов, связанных с безопасностью.</li>
</ul>
</li>
</ul></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Enhancing model robustness through prompt augmentation and long-context training</div>
                <div class="summary_text"><h2>Улучшение безопасности и возможностей модели MiniMax-Text-01</h2>
<p>В данном разделе описываются методы, использованные для улучшения безопасности и производительности модели MiniMax-Text-01, особенно в контексте обработки длинных текстов.</p>
<p><strong>5.5.1. Расширение набора данных для безопасности</strong></p>
<p>Для улучшения безопасности модели и её устойчивости к атакам, использовался метод расширения набора данных. На основе собранных типовых запросов для атак (red team attack prompts), модель MiniMax-Text-01 ранней версии генерировала дополнительные похожие запросы. Это позволило увеличить разнообразие сценариев, связанных с безопасностью, и сделать защитные механизмы модели более надежными.</p>
<p><strong>5.5.2. Генерация ответов с использованием модели вознаграждения за безопасность</strong></p>
<p>Для генерации безопасных и подходящих ответов использовалась модель вознаграждения за безопасность (harmless reward model), разработанная на основе набора подробных правил безопасности. Чтобы избежать необоснованных отказов модели, в правила безопасности были включены принципы полезности. Это позволило достичь баланса между безопасностью и полезностью, обеспечивая безопасные ответы без ущерба для функциональности модели. В результате, система демонстрирует надежную защиту от злоупотреблений, сохраняя при этом высокую производительность в рамках целевого использования.</p>
<p><strong>5.6. Методология обучения с адаптацией к длинному контексту</strong></p>
<p>Предложена многоэтапная методология обучения для улучшения способности модели обрабатывать длинные тексты. Эта методология направлена на оптимизацию обработки длинных последовательностей, сохраняя при этом эффективность на более коротких последовательностях. Базовая частота RoPE (метод позиционного кодирования) поддерживается на уровне 10 миллионов на протяжении всего обучения, что обеспечивает согласованность в позиционном кодировании.</p>
<ul>
<li>
<p><strong>Этап I: Начальное обучение на коротком контексте.</strong> На этом этапе модель обучается на последовательностях длиной до 8192 токенов с использованием метода SFT (Supervised Fine-Tuning). Это обеспечивает базовую компетентность в обработке стандартных запросов и ответов. Длинные запросы, превышающие 8192 токена, на этом этапе исключаются.</p>
</li>
<li>
<p><strong>Этап II: Обучение на расширенном контексте.</strong> Длина последовательности значительно увеличивается до 1 032 192 токенов. На этом этапе используются обучающие примеры разной длины, включая 50% длинных запросов. Это позволяет модели адаптироваться к обработке больших объемов контекста.</p>
</li>
<li>
<p><strong>Этап III: Оптимизация предпочтений на коротком контексте.</strong> Длина последовательности возвращается к 8192 токенам, и используется метод DPO (Direct Preference Optimization). Это обеспечивает оптимальную производительность на стандартных размерах контекста, сохраняя при этом ранее полученные навыки.</p>
</li>
<li>
<p><strong>Этап IV: Оптимизация предпочтений на длинном контексте.</strong> Усиливается способность модели обрабатывать длинный контекст с помощью DPO на последовательностях длиной 1 032 192 токенов. Этот этап аналогичен этапу III, но использует только данные с длинным контекстом.</p>
</li>
<li>
<p><strong>Этап V: Онлайн-обучение с подкреплением.</strong> Завершающий этап - онлайн-обучение с подкреплением на коротком контексте (8192 токена).</p>
</li>
</ul>
<p><strong>5.7. Оценка на академических бенчмарках</strong></p>
<p>Модель MiniMax-Text-01 была протестирована на различных открытых бенчмарках для оценки её способностей в обработке как короткого, так и длинного контекста. Эти тесты показывают, что модель является лидером среди открытых моделей в задачах извлечения информации из длинного контекста, понимания, обучения в контексте и запросов, основанных на знаниях. При этом модель также хорошо справляется с задачами по математике, рассуждению и кодированию, а также демонстрирует высокую полезность в реальных сценариях использования.</p>
<p><strong>5.7.1. Основные бенчмарки</strong></p>
<ul>
<li><strong>MMLU и MMLU-Pro:</strong> Оценивают знания модели в различных областях.</li>
<li><strong>SimpleQA и C-SimpleQA:</strong> Проверяют знания модели и ее способность отвечать на фактические вопросы, особенно в контексте китайской культуры.</li>
<li><strong>GPQA и DROP:</strong> Оценивают способности модели к рассуждению и пониманию прочитанного.</li>
<li><strong>GSM8k и MATH:</strong> Проверяют навыки решения математических задач.</li>
<li><strong>HumanEval и MBPP Plus:</strong> Оценивают навыки программирования.</li>
<li><strong>IFEval:</strong> Оценивает способность модели интерпретировать и выполнять сложные инструкции.</li>
<li><strong>Arena-Hard-Auto:</strong> Отражает соответствие модели человеческим предпочтениям.</li>
</ul>
<p>Результаты показывают, что MiniMax-Text-01 превосходит большинство моделей по многим параметрам, включая C-SimpleQA, MMLU, IFEval и Arena-Hard. Кроме того, модель показывает хорошую производительность в математических задачах и задачах на кодирование, сравнимую с результатами других передовых моделей.</p>
<p><strong>5.7.2. Бенчмарки для длинного контекста</strong></p>
<p>Оценка возможностей модели по работе с длинным контекстом была переориентирована на более сложные задачи, поскольку NIAH (ранее использовавшийся тест) оказался слишком простым. Тестирование проводится по трем основным направлениям:</p>
<ol>
<li><strong>Извлечение информации из длинного контекста</strong></li>
<li><strong>Понимание длинного контекста</strong></li>
<li><strong>Обучение в длинном контексте</strong></li>
</ol>
<p>Таким образом, данный раздел описывает комплексный подход к обучению и оценке модели MiniMax-Text-01, направленный на повышение ее безопасности, эффективности и способности обрабатывать длинные тексты.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Long-context retrieval</div>
                <div class="summary_text"><p>В этом разделе рассматриваются возможности модели по работе с длинным контекстом, а именно, её память и способность к пониманию.</p>
<p><strong>Память в длинном контексте</strong></p>
<p>Для оценки способности модели извлекать информацию из длинного контекста, используется несколько тестов. Помимо стандартного теста "Иголка в стоге сена" (NIAH), предложен более сложный вариант — "Многораундовая иголка в стоге сена" (MR-NIAH). MR-NIAH имитирует диалоговые ситуации, где в истории переписки содержатся запросы, а в последнем раунде модель должна повторить ответ на один из предыдущих запросов. Стога сена варьируются по длине от 2 тысяч до 1 миллиона токенов (до 2000 взаимодействий), а "иголки" (запросы на повторение) вставляются на 25%, 50% и 75% длины диалога. Качество ответа оценивается по трем компонентам.</p>
<p>Результаты MR-NIAH показывают, что модель MiniMax-Text-01 демонстрирует высокую производительность на разных длинах последовательностей как на английском, так и на китайском языках. По сравнению с другими моделями (GPT, Claude, Gemini), MiniMax-Text-01 меньше теряет в качестве при увеличении длины ввода, что говорит о её надежности при работе с длинным контекстом.</p>
<p><strong>Понимание длинного контекста</strong></p>
<p>Для оценки способности модели понимать длинный контекст и делать логические выводы на его основе, используются два набора данных: Ruler и LongBench-V2.</p>
<ul>
<li><strong>Ruler</strong> включает 13 различных задач, в том числе задачи на многошаговое рассуждение и агрегацию данных. Тестирование проводится на последовательностях длиной до 1 миллиона токенов.</li>
<li><strong>LongBench-V2</strong> содержит задачи на ответы на вопросы разной сложности, охватывающие различные типы контекста: одиночные и множественные документы, диалоги, репозитории кода и структурированные данные. Тестирование проводится в двух режимах: с использованием цепочки рассуждений (CoT) и без неё. Длина текста разделена на три категории: короткий (до 32 тысяч слов), средний (32-128 тысяч слов) и длинный (128 тысяч - 2 миллиона слов).</li>
</ul>
<p>Результаты показывают, что MiniMax-Text-01 отлично справляется с задачами на рассуждение в длинном контексте из набора Ruler. На длине ввода 64 тысячи токенов модель показывает сравнимые результаты с лидерами (GPT-4o, Claude-3.5-Sonnet), но начиная со 128 тысяч токенов MiniMax-Text-01 значительно превосходит все остальные модели, особенно в ситуациях с ультра-длинным контекстом (1 миллион токенов).</p>
<p>В LongBench-V2 модель также показывает отличные результаты, достигая наилучших показателей среди всех протестированных моделей в режиме с CoT и демонстрируя высокую эффективность и без CoT.</p>
<p>В целом, MiniMax-Text-01 демонстрирует исключительные возможности в понимании длинного контекста, особенно в задачах, требующих сложных рассуждений. Устойчивость и надежность модели в работе с длинными последовательностями объясняется гибридной архитектурой с использованием RoPE и тщательно настроенными алгоритмами обучения.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluating long in-context learning in lifelong models</div>
                <div class="summary_text"><h2>Оценка способности к обучению в контексте</h2>
<p>В этом разделе оценивается способность модели обучаться на основе контекста, что является ключевой областью исследований в области непрерывного обучения. Для оценки этой способности используется набор данных MTOB (Machine Translation from One Book), предназначенный для машинного перевода с использованием одного учебника.</p>
<p>Задача заключается в переводе между английским и языком каламанг, который имеет очень мало открытых данных и, следовательно, практически не представлен в обучающем корпусе модели. Ожидается, что модель освоит этот язык, опираясь исключительно на части грамматического пособия и 375 примеров перевода, которые предоставляются в контексте для каждого запроса перевода. Длина контекста составляет 81 тысячу токенов в режиме "половина книги" и 133 тысячи токенов в режиме "полная книга".</p>
<p>Результаты представлены в таблице 11. На рисунке 16 показано изменение метрики ChrF (eng kalam) в процессе обучения с использованием длинного контекста. Тщательный анализ данных предварительного обучения показал, что контент, связанный с языком каламанг, представлен в очень малом объеме. В результате, показатель ChrF (eng kalam) для нашей модели в сценарии без контекста является самым низким, в то время как другие сравниваемые модели, вероятно, имели в своих данных предварительного или последующего обучения больше данных на каламанге. </p>
<p>Однако, в сценариях с контекстом (половина и полная книга) наша модель превосходит все остальные модели по метрике ChrF (eng kalam). Также наша модель показывает сравнимые результаты с другими моделями по метрике BLEURT (kalam eng).</p>
<p>В процессе расширения контекста, как описано в разделе 4.2, наблюдалось постепенное улучшение способности к обучению в контексте, что подтверждается данными MTOB и показано на рисунке 16. Хотя были изучены некоторые работы, направленные на улучшение способностей к обучению в контексте, авторы считают, что эта способность должна быть лишь одним из аспектов возможностей рассуждения моделей с длинным контекстом. Поэтому планируется провести углубленное исследование качества и масштаба данных с длинным контекстом, чтобы в дальнейшем улучшить возможности рассуждения модели в этом направлении.</p>
<p><strong>Комментарий:</strong> <em>В этом разделе статьи описывается, как модель справляется с задачей перевода на редкий язык, основываясь только на предоставленном контексте. Это демонстрирует способность модели к обучению "на лету" и является важным аспектом для моделей с длинным контекстом.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Dataset and training overview</div>
                <div class="summary_text"><h2>Изложение раздела статьи о модели MiniMax-VL-01</h2>
<p>В данном разделе описывается процесс обучения модели MiniMax-VL-01, которая представляет собой мультимодальную модель, способную обрабатывать как изображения, так и текст. Обучение включает в себя несколько этапов, начиная с подготовки данных и заканчивая тонкой настройкой модели.</p>
<p><strong>6.1. Мультимодальные данные</strong></p>
<p>Для обучения модели использовался обширный набор данных, состоящий из трех основных типов:</p>
<ul>
<li><strong>Данные с подписями (Caption Data):</strong> Набор из 694 миллионов пар "изображение-подпись", полученных из интернет-источников. Для 180 миллионов изображений подписи были уточнены. Во время обучения использовались как исходные, так и уточненные подписи с равной вероятностью.</li>
<li><strong>Данные с описаниями (Description Data):</strong> Набор из 100 миллионов изображений из открытых источников, каждое из которых сопровождается подробным описанием. Описания были сгенерированы моделью, а затем отредактированы людьми. В среднем описания содержат около 300 текстовых токенов. Этот тип данных используется для улучшения выравнивания модальностей и понимания модели.</li>
<li><strong>Инструкционные данные (Instruction Data):</strong> Набор данных с парами "вопрос-ответ", разработанный для обучения модели выполнению различных задач, связанных с изображениями, таких как извлечение текста, локализация объектов и решение геометрических задач. При обучении использовались различные типы вопросов с равной вероятностью, чтобы модель могла обобщать свои знания.</li>
</ul>
<p>Для демонстрации разнообразия инструкционных данных было проведено исследование, в ходе которого 1 миллиону пар "изображение-инструкция" были присвоены теги, описывающие основную задачу. Было выделено около 50 000 уникальных тегов, которые были сгруппированы в 14 основных категорий.</p>
<p><strong>6.2. Архитектура</strong></p>
<p>Архитектура MiniMax-VL-01 построена по принципу ViT-MLP-LLM и состоит из трех основных компонентов:</p>
<ul>
<li><strong>Vision Transformer (ViT):</strong> Визуальный энкодер с 303 миллионами параметров, отвечающий за кодирование изображений.</li>
<li><strong>Двухслойный MLP-проектор:</strong> Служит для адаптации признаков изображения к пространству признаков языковой модели.</li>
<li><strong>MiniMax-Text-01:</strong> Большая языковая модель (LLM), которая является основой для обработки текста.</li>
</ul>
<p>Модель использует стратегию динамического разрешения, при которой размер входного изображения изменяется в соответствии с заданным списком конфигураций от 336x336 до 2016x2016, при этом сохраняется стандартная миниатюра с разрешением 336x336. Изображения разбиваются на неперекрывающиеся патчи, которые кодируются отдельно, а затем их признаки объединяются. Такой подход позволяет избежать потери информации и улучшает адаптивность модели к различным масштабам.</p>
<p>В качестве визуального энкодера используется облегченная модель ViT-L/14, обученная с нуля. Для улучшения выравнивания между изображениями и подписями используется контрастное обучение, дополненное механизмом кросс-внимания. Модель ViT-L/14 сначала обучается при разрешении 224x224 на 37 миллиардах пар "изображение-подпись", а затем дообучается при разрешении 336x336 на 1.2 миллиардах пар. Достигнутая точность классификации на наборе данных ImageNet-1K составляет 80,55% при разрешении 336x336.</p>
<p><strong>6.3. Этапы обучения</strong></p>
<p>Обучение модели MiniMax-VL-01 разделено на четыре этапа:</p>
<ul>
<li><strong>Этап I: Выравнивание модальностей.</strong> На этом этапе основная цель состоит в выравнивании визуальных и текстовых токенов. Модель обучается генерировать подписи к изображениям. Обновляются веса визуального энкодера и адаптера. Используется 80 миллиардов токенов из набора данных с описаниями. Все изображения обрабатываются с разрешением 336x336.</li>
<li><strong>Этап II: Улучшение визуального понимания.</strong> На этом этапе происходит тонкая настройка модели для выполнения различных задач, связанных с пониманием изображений. Обучение проводится на 420 миллиардах мультимодальных токенов из набора инструкционных данных, а также данных для дообучения MiniMax-Text-01 в соотношении 20:1.</li>
<li><strong>Этап III: Улучшение пользовательского опыта.</strong> На этом этапе модель обучается на данных, имитирующих реальное взаимодействие пользователя с приложениями. Данные включают в себя диалоги с метками, обеспечивающими точные и полезные ответы. Используется 44,8 миллиарда мультимодальных токенов.</li>
<li><strong>Этап IV: Улучшение предпочтений.</strong> На этом этапе используется Direct Preference Optimization (DPO) для дальнейшего улучшения производительности и пользовательского опыта. Используется набор данных из 40 000 примеров.</li>
</ul>
<p>В целом, процесс обучения MiniMax-VL-01 направлен на создание мощной мультимодальной модели, способной эффективно обрабатывать изображения и текст, а также отвечать на вопросы и следовать инструкциям.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Multi-stage training strategy for image-text pair generation</div>
                <div class="summary_text"><h2>Обучение модели для работы с изображениями и текстом</h2>
<p>Для обучения модели, способной обрабатывать пары "изображение-текст", используется многоступенчатый процесс. Он включает в себя следующие этапы:</p>
<p><strong>1. Выбор запросов (Prompt Selection).</strong> Запросы берутся как из обучающих данных, так и из реальных взаимодействий пользователей. Цель – охватить широкий спектр общих сценариев и, в частности, решить проблемы, выявленные на предыдущих этапах обучения, например, повторяющиеся ответы в сложных задачах оптического распознавания текста (OCR).</p>
<p><strong>2. Генерация ответов (Response Generation).</strong> Используются различные стратегии:
   * Генерация нескольких вариантов ответа путем изменения параметра температуры (sampling temperature).
   * Создание вариантов ответов путем "ослабления" изображения в определенных ситуациях.
   * Использование текстовой модели (MiniMax-Text-01) для намеренного внесения ошибок или "галлюцинаций" в качественные ответы, чтобы получить контрастные примеры для обучения.</p>
<p><strong>3. Присвоение оценок (Reward Assignment).</strong> Большие языковые модели, в частности MiniMax-Text-01, используются в качестве оценщиков. Разработаны многомерные критерии оценки, позволяющие систематически и всесторонне анализировать взаимосвязи между запросами, "правильными" ответами и сгенерированными ответами.</p>
<p><strong>4. Формирование пар (Pair Construction).</strong> На основе результатов оценки выбираются ответы с наивысшим баллом в качестве положительных примеров, а с наименьшим – в качестве отрицательных. Пары с незначительной разницей в оценках отбрасываются. Кроме пар "изображение-текст", также включается значительная доля пар "чистый текст".</p>
<p><strong>Важное замечание:</strong> При применении метода Direct Preference Optimization (DPO) к мощным моделям существует склонность к переобучению. Для борьбы с этим используется стратегия ранней остановки обучения, которая заключается в завершении процесса обучения до завершения полной эпохи. Это помогает сохранить обобщающие способности модели.</p>
<p>Благодаря такому многоступенчатому подходу модель не только демонстрирует умение понимать и генерировать качественный текст, но и соответствует человеческим ценностям и стандартам безопасности. Это позволяет достичь баланса между производительностью модели и этическими соображениями, создавая эффективную и ответственную модель.</p>
<h2>Оценка производительности</h2>
<p>Для оценки производительности модели используются различные бенчмарки, включая: MMMU, MMMU-Pro, ChartQA, DocVQA, OCRBench, AI2D, MathVista, OlympiadBench, MMLongBench-Doc, MEGA-Bench и внутренний бенчмарк. Эти бенчмарки позволяют оценить способности модели в различных областях, таких как знания, визуальное рассуждение, математика, естественные науки, обработка длинного контекста и пользовательский опыт.</p>
<p>Модель MiniMax-VL-01 демонстрирует конкурентоспособную производительность в различных задачах, связанных с обработкой изображений и текста.</p>
<p><strong>Ключевые сильные и слабые стороны:</strong></p>
<ul>
<li><strong>Стандартные задачи:</strong> Модель показывает производительность на уровне GPT-4o, особенно в задачах визуального вопросно-ответного анализа. Это достигается благодаря многоступенчатому процессу обучения, который позволяет модели эффективно понимать и рассуждать на основе визуальных и текстовых данных.</li>
<li><strong>Математические задачи:</strong> Модель испытывает трудности в решении сложных математических задач, что было выявлено с помощью бенчмарка OlympiadBench.</li>
<li><strong>Длинный контекст:</strong> Модель превосходит большинство аналогов в задачах понимания и извлечения информации из длинного контекста, используя бенчмарк MMLongBench-Doc. Однако, она демонстрирует заметный разрыв с GPT-4o-11-20 в этой области.</li>
<li><strong>MEGA-Bench:</strong> Модель показывает конкурентоспособные результаты на реалистичном и всестороннем бенчмарке MEGA-Bench, превосходя другие открытые модели. При этом модель испытывает трудности в более сложных задачах, таких как планирование и оценка метрик.</li>
<li><strong>Внутренний бенчмарк:</strong> Для оценки пользовательского опыта был разработан внутренний бенчмарк, состоящий из 90 разнообразных задач, связанных с изображениями. Модель превосходит все открытые аналоги и приближается к производительности GPT-4o-11-20.</li>
</ul>
<p>В заключение, модель MiniMax-VL-01 демонстрирует сильные результаты в различных задачах, связанных с обработкой изображений и текста, но нуждается в дальнейшем улучшении в областях математического рассуждения и обработки длинного контекста.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-15 09:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    