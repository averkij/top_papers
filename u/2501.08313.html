
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 —è–Ω–≤–∞—Ä—è</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-14.html">‚¨ÖÔ∏è <span id="prev-date">14.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-16.html">‚û°Ô∏è <span id="next-date">16.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 —è–Ω–≤–∞—Ä—è', 'en': 'January 15', 'zh': '1Êúà15Êó•'};
        let feedDateNext = {'ru': '16.01', 'en': '01/16', 'zh': '1Êúà16Êó•'};
        let feedDatePrev = {'ru': '14.01', 'en': '01/14', 'zh': '1Êúà14Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2501.08313', 'title': 'MiniMax-01: Scaling Foundation Models with Lightning Attention', 'url': 'https://huggingface.co/papers/2501.08313', 'abstract': 'We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-01-14', 'pub_date_card': {'ru': '14 —è–Ω–≤–∞—Ä—è', 'en': 'January 14', 'zh': '1Êúà14Êó•'}, 'hash': 'a57d7b1914e7383a', 'authors': ['MiniMax', 'Aonian Li', 'Bangwei Gong', 'Bo Yang', 'Boji Shan', 'Chang Liu', 'Cheng Zhu', 'Chunhao Zhang', 'Congchao Guo', 'Da Chen', 'Dong Li', 'Enwei Jiao', 'Gengxin Li', 'Guojun Zhang', 'Haohai Sun', 'Houze Dong', 'Jiadai Zhu', 'Jiaqi Zhuang', 'Jiayuan Song', 'Jin Zhu', 'Jingtao Han', 'Jingyang Li', 'Junbin Xie', 'Junhao Xu', 'Junjie Yan', 'Kaishun Zhang', 'Kecheng Xiao', 'Kexi Kang', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Zheng', 'Linbo Chai', 'Long Xing', 'Meizhi Ju', 'Mingyuan Chi', 'Mozhi Zhang', 'Peikai Huang', 'Pengcheng Niu', 'Pengfei Li', 'Pengyu Zhao', 'Qi Yang', 'Qidi Xu', 'Qiexiang Wang', 'Qin Wang', 'Qiuhui Li', 'Ruitao Leng', 'Shengmin Shi', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tao Huang', 'Tianrun Liang', 'Weigao Sun', 'Weixuan Sun', 'Weiyu Cheng', 'Wenkai Li', 'Xiangjun Song', 'Xiao Su', 'Xiaodong Han', 'Xinjie Zhang', 'Xinzhu Hou', 'Xu Min', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yingjie Zhu', 'Yipeng Zhou', 'Yiran Zhong', 'Yongyi Hu', 'Yuanxiang Fan', 'Yue Yu', 'Yufeng Yang', 'Yuhao Li', 'Yunan Huang', 'Yunji Li', 'Yunpeng Huang', 'Yunzhi Xu', 'Yuxin Mao', 'Zehan Li', 'Zekang Li', 'Zewei Tao', 'Zewen Ying', 'Zhaoyang Cong', 'Zhen Qin', 'Zhenhua Fan', 'Zhihang Yu', 'Zhuo Jiang', 'Zijia Wu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2501.08313.jpg', 'data': {'categories': ['#open_source', '#architecture', '#optimization', '#benchmark', '#long_context', '#training'], 'emoji': 'üöÄ', 'ru': {'title': 'MiniMax-01: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π MiniMax-01, –≤–∫–ª—é—á–∞—è MiniMax-Text-01 –∏ MiniMax-VL-01, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ä–∞–≤–Ω–∏–º—ã —Å –ª—É—á—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –æ–±–ª–∞–¥–∞—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –í –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è lightning attention –∏ –µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å Mixture of Experts (MoE). –ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç 32 —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏ 456 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö 45,9 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ MiniMax-Text-01 –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–æ 4 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.'}, 'en': {'title': 'Unleashing Long Contexts with MiniMax-01 Models', 'desc': 'The MiniMax-01 series introduces advanced models, MiniMax-Text-01 and MiniMax-VL-01, designed to handle longer contexts effectively. These models utilize lightning attention and a Mixture of Experts (MoE) architecture, featuring 32 experts and a staggering 456 billion parameters, optimizing the activation of 45.9 billion parameters per token. By implementing efficient parallel strategies and computation-communication overlap techniques, the models can train and infer on extensive datasets, reaching context windows of up to 1 million tokens during training and 4 million during inference. Performance evaluations indicate that MiniMax-01 models rival leading models like GPT-4o and Claude-3.5-Sonnet while significantly extending context capabilities.'}, 'zh': {'title': 'MiniMax-01ÔºöË∂ÖÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜMiniMax-01Á≥ªÂàóÔºåÂåÖÊã¨MiniMax-Text-01ÂíåMiniMax-VL-01ÔºåËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñáÊó∂ÂÖ∑Êúâ‰ºòË∂äÁöÑËÉΩÂäõ„ÄÇÊ†∏ÂøÉÊäÄÊúØÊòØÈó™ÁîµÊ≥®ÊÑèÂäõÂíåÈ´òÊïàÁöÑÊâ©Â±ïËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜÊúÄÂ§ßÂåñËÆ°ÁÆóËÉΩÂäõÔºåÊàë‰ª¨Â∞ÜÂÖ∂‰∏é‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoEÔºâÁªìÂêàÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™Êã•Êúâ32‰∏™‰∏ìÂÆ∂Âíå4560‰∫øÂèÇÊï∞ÁöÑÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËøô‰∫õÊ®°ÂûãÂú®Ê†áÂáÜÂíåÂÜÖÈÉ®Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§ü‰∏éÊúÄÂÖàËøõÁöÑÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂêåÊó∂Êèê‰æõ20Âà∞32ÂÄçÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇ'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.', 'summary': '<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π MiniMax-01, –≤–∫–ª—é—á–∞—é—â–∞—è –≤ —Å–µ–±—è MiniMax-Text-01 (—Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å) –∏ MiniMax-VL-01 (–º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π). –≠—Ç–∏ –º–æ–¥–µ–ª–∏ —Å—Ä–∞–≤–Ω–∏–º—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –ª—É—á—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∏—Ö –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.</p>\n<p>–ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è "–º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ" (lightning attention) –∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —ç—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture of Experts (MoE), —Å–æ–∑–¥–∞–≤–∞—è –º–æ–¥–µ–ª—å —Å 32 —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ 456 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤. –ü—Ä–∏ —ç—Ç–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 45.9 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>\n<p>–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –¥–ª—è MoE –∏ "–º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è". –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ç–Ω—è–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –¥–ª–∏–Ω–æ–π –≤ –º–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤.</p>\n<p>–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ MiniMax-Text-01 –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ 4 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (inference), –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å. –ú–æ–¥–µ–ª—å MiniMax-VL-01, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –ø—É—Ç–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ 512 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∏ —Ç–µ–∫—Å—Ç, –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ MiniMax-01 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4o –∏ Claude-3.5-Sonnet, –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–¥–ª–∞–≥–∞—è –≤ 20-32 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ.</p>\n<p>–ú–æ–¥–µ–ª–∏ MiniMax-01 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –Ω–∞ GitHub.</p>'}, {'title': 'Experimental setup', 'content': 'We conducted training on softmax (equipped with FlashAttention-2 (Dao, 2024)), lightning attention, and hybrid-lightning attention models across various scales: 70 million, 160 million, 410 million, 1 billion, 3 billion, and 7 billion parameters. Each model was trained on dataset consisting of up to 300 billion tokens, with context length of 8192. Our training methodology follows the approach proposed by Chinchilla (Hoffmann et al., 2022), where the training loss serves as direct indicator of test performance. For each model architecture and training sequence length, we maintained 8 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 2 Summary of Scaling Laws: It shows the relationships between loss (ùêø), optimal model size (ùëÅùëúùëùùë°), and optimal dataset size (ùê∑ùëúùëùùë°) as functions of computational budget (ùê∂). It reveals that, given the same budget, the hybrid model uses more parameters and tokens but achieves lower loss. Arch Softmax Attention Lightning Attention Hybrid-lightning ùêø(ùê∂) 3.7087ùê∂ 0.0798 3.5391ùê∂ 0.0768 3.4797ùê∂ 0.0763 ùëÅùëúùëùùë° (ùê∂) (1.82 108)ùê∂0.7118 (2.74 108)ùê∂0.6470 (2.57 108)ùê∂0.6670 ùê∑ùëúùëùùë° (ùê∂) (2.56 1010)ùê∂0.5102 (4.43 1010)ùê∂0.4684 (3.70 1010)ùê∂0.4707 Figure 6 Summary of Scaling Laws. Training curves (left) span models from 70M to 7B parameters. Optimal model size (center) and training tokens (right) are derived based on specified compute budget estimation. uniform global batch size of 4 million tokens. The Adam optimizer was employed, configured with learning rate of 3e-4 and weight decay of 0.1. fixed learning rate scheduler was applied across all experiments due to constrained computational resources. We employ diverse set of evaluation benchmarks, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (both easy and challenge variants) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), Needle in Haystack (NIAH) (Shen et al., 2024), and SCROLLS (Shaham et al., 2022). Each benchmark assesses distinct capabilities of the models.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: softmax —Å FlashAttention-2, lightning attention –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ lightning attention. –ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö, –æ—Ç 70 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º –¥–æ 300 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 8192. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–ª–∞ –ø–æ–¥—Ö–æ–¥—É, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º—É –≤ —Ä–∞–±–æ—Ç–µ Chinchilla, –≥–¥–µ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —è–≤–ª—è—é—Ç—Å—è –ø—Ä—è–º—ã–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–î–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –¥–ª–∏–Ω—ã –æ–±—É—á–∞—é—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª—Å—è –µ–¥–∏–Ω—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –≤ 4 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam —Å –Ω–∞—á–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è 3e-4 –∏ –≤–µ—Å–æ–≤—ã–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º 0.1. –ò–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –≤–∫–ª—é—á–∞—è BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (–ª–µ–≥–∫–∏–π –∏ —Å–ª–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç—ã), OpenBookQA, Needle in Haystack (NIAH) –∏ SCROLLS. –ö–∞–∂–¥—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –µ–¥–∏–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—É—á–µ–Ω–∏—é. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.</em></p>'}, {'title': 'Scaling laws', 'content': 'We fit the scaling curves based on our experiments over the above mentioned settings, where we alter the model size (ùëÅ) and dataset size (ùê∑) for different computational budget (ùê∂) and observe the corresponding training loss (ùêø) that serving as an estimator of test loss. We begin by establishing power-law relationships between ùêø and ùê∂, following Chinchillas methodology (Hoffmann et al., 2022). Using the fitted curve, we derive coefficients for optimal model size ùëÅùëúùëùùë° ùê∂ùëé and optimal dataset size ùê∑ùëúùëùùë° ùê∂ùëè. The original scaling laws (Kaplan et al., 2020) use ùêø(ùëã) = (ùëã0/ùëã)ùõºùëã , while subsequent studies (Clark et al., 2022; Gao et al., 2024; Henighan et al., 2020; Hoffmann et al., 2022) employ ùêø(ùëã) = ùúñ + (ùëã0/ùëã)ùõºùëã for better fitting, where ùúñ denotes the irreducible loss. For simplicity, we unify these forms into ùêø(ùëã) = ùõΩùëã ùëã ùõºùëã , facilitating direct comparison of scaling capabilities based on ùõºùëã and ùõΩùëã . The summary of scaling laws is shown in Table 2 and Figure 6. It can be intuitively understood that given the same computational budget, models with lightning attention tend to utilize more parameters and tokens, yet they achieve lower loss compared to models with pure softmax attention. 9 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 7 Larger models and hybrid-lightning attention achieve the best performance across benchmarks. Performance is evaluated on CSR (Common Sense Reasoning), NIAH (Needle in Haystack), and SCROLLS benchmarks using three attention mechanism models from 410M to 7B parameters.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–∑—É—á–∞–µ—Ç—Å—è, –∫–∞–∫ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (N) –∏ —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (D) –≤–ª–∏—è—é—Ç –Ω–∞ –æ—à–∏–±–∫—É –æ–±—É—á–µ–Ω–∏—è (L) –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö (C). –¶–µ–ª—å ‚Äì –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ç–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.</p>\n<p>–î–ª—è –Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Å—Ç–µ–ø–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É L –∏ C, —Å–ª–µ–¥—É—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ Chinchilla. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–≤–µ—Å—Ç–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (N_opt * C^a) –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (D_opt * C^b).</p>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è, —Å–≤—è–∑—ã–≤–∞—é—â–∞—è –æ—à–∏–±–∫—É –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏/–¥–∞–Ω–Ω—ã—Ö. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å —Ñ–æ—Ä–º—É–ª–∞ L(X) = (X0/X)^Œ±X, –Ω–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ñ–æ—Ä–º—É–ª–∞ L(X) = Œµ + (X0/X)^Œ±X –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏, –≥–¥–µ Œµ ‚Äì —ç—Ç–æ –Ω–µ—É—Å—Ç—Ä–∞–Ω–∏–º–∞—è –æ—à–∏–±–∫–∞. –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è, –≤—Å–µ —ç—Ç–∏ —Ñ–æ—Ä–º—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ L(X) = Œ≤X * X^(-Œ±X), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Œ±X –∏ Œ≤X.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ –∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ. –ò–∑ –Ω–∏—Ö —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö –º–æ–¥–µ–ª–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º "lightning attention" (–æ–±–ª–µ–≥—á–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º) —Å–∫–ª–æ–Ω–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∏–≥–∞—é—Ç –º–µ–Ω—å—à–µ–π –æ—à–∏–±–∫–∏, —á–µ–º –º–æ–¥–µ–ª–∏ —Å –æ–±—ã—á–Ω—ã–º softmax –≤–Ω–∏–º–∞–Ω–∏–µ–º.</p>\n<p>–¢–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–æ–¥–µ–ª–∏ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º "lightning attention" –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö (CSR, NIAH –∏ SCROLLS). –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å —Ç—Ä–µ–º—è –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è, —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç 410 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.</p>'}, {'title': 'Benchmarking training speed and performance of attention mechanisms', 'content': 'We assess the end-to-end training speed of softmax attention, lightning attention, and hybridlightning models with 3 billion parameters by measuring the tokens processed per GPU per second (TGS). For completeness, we also included popular linear models such as HGRN2 and Mamba2 in our evaluation. For the speed benchmark, the training context length was gradually increased until reaching the out-ofmemory limit on single-node H800 GPUs. As illustrated in Fig. 8, lightning attention achieves constant training speed irrespective of the sequence length and is the sole linear model that outperforms FlashAttention2. Figure 8 The training speed of various attention mechanisms, including softmax, lightning, hybridlightning, HGRN2, and Mamba2, was benchmarked across sequence lengths ranging from 1,024 to 65,536. Performance was measured in terms of training speed, reported as tokens processed per GPU per second (TGS). 10 MiniMax-01: Scaling Foundation Models with Lightning Attention 2.2.3. Hybrid Architecture Our preliminary experiments with the hybrid architecture have yielded promising results, motivating us to delve deeper into its potential through two variants: hybrid-cosformer2 and hybrid-hgrn2. In the hybrid-cosformer2 model, we replace the linear attention layers in the cosformer2 architecture with softmax attention layers at intervals of every eight layers. This substitution strategy is similarly applied in the hybrid-hgrn2 model. We conduct experiments using consistent setups to evaluate the downstream performance of these alternatives. Our findings, as summarized in Table 3, indicate that the hybrid-lightning model achieves the best performance. Table 3 Benchmarking various hybrid-linear models with 1 Billion Parameters. We present the average CSR score, weighted average accuracy for NIAH, and the average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: TGS (token per gpu per second), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, and SCR (SCROLLS). Hybrid-linear Arch. TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR Hybrid-cosformer2 Hybrid-hgrn2 Hybrid-lightning 23.3K 70.29 45.63 51.46 29.5K 70.89 51.23 56.51 33.4K 70.73 50.41 55.80 55.77 59.68 59.93 26.11 28.50 27. 30.60 46.64 32.40 49.87 32.80 49.55 43.6 91.8 95.7 10.9 10.8 13.3 In addition to linear models, sliding window attention can also achieve linear computational complexity by appropriately adjusting the window size. As it is grounded in softmax attention, it serves as robust baseline for evaluating linear architectures. Therefore, we incorporated the hybrid-window approach by replacing the sliding window attention with full softmax attention every eight layers. We evaluated various window sizes of SWA ranging from 256 to 1024. Our results indicate that larger window sizes lead to slower training speeds compared to the hybrid-lightning model. To compare these models under equivalent speed conditions, we did not consider window sizes larger than 1024. As shown in Table 4, the hybrid-lightning model outperforms all other models across all metrics, particularly excelling in the NIAH benchmark. Table 4 Benchmark comparison of hybrid-lightning and hybrid-window Models. Metrics include average CSR score, weighted NIAH accuracy, and average SCROLLS score. Higher scores indicate better performance across all tasks. Abbreviations: PS (parameter size, billion), W.S. (window size of SWA), HS (HellaSwag), WG (WinoGrande), OBQA (OpenBookQA), NIAH, SCR (SCROLLS), TGS (token per gpu per second). P.S 1B 3B W.S. Arch. 256 512 1024 TGS PIQA HS WG ARC-E ARC-C OBQA CSR NIAH SCR 35.6K 70.29 48.68 53.35 Hybrid35.1K 70.95 48.19 52.33 window 33.6K 69.75 47.80 53.12 Hybrid-lightning 33.4K 70.73 50.41 55.80 16.1K 73.83 59.70 59.59 Hybrid15.8K 73.29 60.00 59.04 window 15.4K 74.27 59.02 57.85 Hybrid-lightning 15.1K 74.21 61.06 59. 48.61 32.60 47.70 30.00 31.60 48.02 32.80 49.55 54.31 35.00 36.00 53.97 33.00 53.44 35.80 55.16 28.75 27.22 28.33 27.65 33.62 32.51 31.91 34.90 57.95 57.53 57.53 59.93 64.10 62.96 64.56 65.49 10.6 11.9 10.6 13.3 14.2 14.2 13.3 14.7 46.8 25.7 53.9 95.7 40.9 57.9 41.6 98.0 256 512 2.2.4. Discussion Based on our analysis of scaling law experiment, downstream performance and speed comparison, we conclude that while pure linear attention models are computationally efficient, they are not suitable 11 MiniMax-01: Scaling Foundation Models with Lightning Attention for LLMs. This is due to their inherent inability to perform retrieval, capability that is essential for in-context learning. In contrast, our hybrid model not only matches but also surpasses softmax attention in both retrieval and extrapolation tasks. This outcome is somewhat counterintuitive. To understand this phenomenon, consider the following explanation of softmax attention: = Softmax(QK/ ùëë)V. (10) It can be rewritten into linear recurrent form as: ùë†0 ùë° = 0, ùë° = ùë† ùëó1 ùë† ùëó ùë° + exp(qùë°kùëá ùëó / ùëë), ùë° = (ùë† ùëó1 ùëó ùë° /ùë† ùëó ùë° )o ùëó1 ùë° + (1 ùë† ùëó1 ùë° /ùë† ùëó ùë° )v ùëó, oùë° = oùë° ùë°, ùëó = 1, . . . , ùë°. (11) Note that the linear recurrence form of lightning attention is as follows: kv0 = 0, kv ùëó = kv ùëó1 + ùëóv ùëó ùëó = kv ùëó ùëó, ùëó = 1, . . . , ùë°. (12) The softmax attention mechanism can be interpreted as linear RNN (Qin et al., 2024a). At each time step ùë°, the hidden state is recalculated starting from the initial time ùë°0 = 1, process often described as "Going Through Book." This method enables the model to accurately retain input information by systematically revisiting previous data. In contrast, linear models lack this recomputation process, which hinders their ability to effectively retain input data. Let us define the capacity of an RNN as the size of its recurrent state. Upon closer examination of Eq. 11, we can deduce that the capacity of softmax attention is ùëÇ(ùëë). In contrast, as illustrated in Eq. 12, the capacity of lightning attention is ùëÇ(ùëë2/‚Ñé). Given that ùëë > ‚Ñé, it follows that lightning attention possesses larger capacity than softmax attention. Consequently, the hybrid-lightning model exhibits superior retrieval and extrapolation capabilities compared to models relying solely on softmax attention. 2.3. Module Ablations in MoE Based on the conclusions from previous sections, we conduct two additional sets of ablation experiments to validate module choices within the MoE architecture on larger scale: (1) Hybrid-lightning attention versus softmax attention: To verify the advantages of the hybrid lightning attention in the MoE. (2) Pre-Layer Normalization versus Post-Layer Normalization: In our hybrid architecture, the effective depth of the model plays significant role. Thus, we expect to find better normalization algorithm for the deep model. Hybrid-lightning Attention versus Softmax Attention. We perform small-scale comparative analysis between softmax attention and hybrid-lightning attention within the MoE architecture. Specifically, we use 28 billion parameter MoE with 5 billion activation parameters that utilize softmax attention as the base model. For every 8 consecutive layers in the base model, we systematically replace softmax attention with lightning attention in the first 7 layers. Both the base model and the modified model are trained on 1 trillion tokens. As shown in Table 5, the results reveal that substituting certain softmax attention layers with lightning attention improves accuracy across most benchmarks. Pre Layer Normalization versus Post Layer Normalization. Pre Layer Normalization(Baevski and Auli, 2018; Child et al., 2019; Wang et al., 2019) (PreNorm), which applies normalization layers before residual connections and attention mechanisms, has demonstrated enhanced stability and performance in LLMs. Since PreNorm allows gradients to flow more directly from the output to the input through residual connections, bypassing the sub-layers to certain extent, it reduces the effective depth of the model. In contrast, Post Layer Normalization(Wang et al., 2019) (PostNorm) applies normalization after the residual connection and attention mechanisms, thereby preserving 12 MiniMax-01: Scaling Foundation Models with Lightning Attention the models effective depth. However, PostNorm can be prone to vanishing and exploding gradients, presenting significant challenges in training LLMs. Most existing LLMs predominantly use PreNorm, as the performance differences between wider and deeper networks in the conventional Transformer architecture are often negligible, and training stability is prioritized. The experiments are performed on models with 9.3 billion activation parameters and total of 60 billion parameters, each consisting of 48 layers that employ different normalization methods. Both models are trained on 500 billion tokens. For PostNorm, we utilize DeepNorm (Wang et al., 2024a) to ensure more stable training. As illustrated in Table 5, PostNorm consistently outperforms PreNorm across all evaluated metrics. Table 5 Module Ablations. Abbreviations: BBH (BIG-Bench Hard), DROP (Discrete Reasoning Over Paragraphs), MMLU (Massive Multitask Language Understanding), CMMLU (Massive Multitask Language Understanding in Chinese), GSM8k (Grade School Math 8K), ARC-C (Arc-Challenge), WG (WinoGrande). Arch. BBH DROP MMLU CMMLU MATH Softmax Hybrid-lightning Pre Layer Norm. Post Layer Norm. 28.2 32.2 29.9 32.6 27.4 29.0 26.8 27. 49.3 49.5 43.9 50.2 47.3 46.0 41.8 49.2 4.6 6.8 4.8 5. GSM8k 18.8 18.5 12.2 16.8 ARC-C WG 46.4 47.4 43.5 46.2 65.6 67. 65.5 65.4 2.4. Model Spec Upon finalizing the architecture of the models modules, the subsequent step entails scaling up the model, which necessitates meticulous design of the models hyperparameters across various dimensions. Our primary goal is to strike balance between performance and inference efficiency. Single-device inference offers superior efficiency compared to multi-device implementations by eliminating cross-machine communication overhead. Consequently, we constrain the models total parameters to 500B, ensuring compatibility with single-node inference on an 8 80G configuration for sequences up to 1M tokens under 8-bit quantization. Given our limited training budget, we formulate the following optimization problem to determine optimal parameter allocations: min ùëÉall,ùëÉact ùêø(ùëÉall, ùëÉact, ùëá) subject to ùê∂compute(ùëÉall, ùëÉact, ùëá) < ùê∂ and ùëÉall < 500ùêµ, (13) where ùêø denotes the loss, ùëÉall and ùëÉact represent the total and activation parameter counts respectively, ùëá is the number of training tokens, ùê∂compute denotes the computational costs (dependent on parameter counts and data consumption), and ùê∂ signifies the budget constraint. Through comparative experiments on small-scale models, we first establish optimal ranges for several key variables: (1) the mixing ratio between softmax and linear attention mechanisms; (2) the depth-to-width ratio of the model architecture; (3) the ratio of linear attention memory size to hidden size; (4) the ratio of activated FFN to attention; (5) the proportion of dimensions utilizing RoPE for softmax attention. Our experiments reveal that the hybrid architecture demonstrates particular sensitivity to layer depth, with deeper models consistently outperforming shallower counterparts. Notably, shallow models require substantially more softmax attention layers to achieve comparable performance, underlining the efficiency advantages of deeper architectures. We also observe that increasing linear attention memory size significantly enhances model performance, and implementing RoPE on half of the softmax attention dimensions enables length extrapolation without performance degradation. 13 MiniMax-01: Scaling Foundation Models with Lightning Attention Based on these optimized architectural variables, we employ established scaling laws (Clark et al., 2022; Hoffmann et al., 2022) to determine the optimal model size. We train models with activation parameters ranging from 44 million to 1.2 billion across 500 billion tokens, utilizing 16, 32, and 64 experts. However, we find the predictions from these methods become less reliable when extrapolating to larger model with 9.3 billion parameters. To address this limitation and achieve more accurate predictions, we propose the following formula: ùêø(ùëÉact, ùëá ùê∏) = ùëë + ùëéùëÉùõº act + ùëèùëá ùõΩ + ùëê(ùëÉactùëá)ùõæ, (14) where ùêø(ùëÉact, ùëá ùê∏) represents the loss conditioned on the number of experts, while ùëé, ùëè, ùëê, ùëë, ùõº, ùõΩ, and ùõæ are parameters to be fitted in relation to the number of experts. Based on the predictions of Eq. 13 and Eq. 14, we have identified candidate model with 45.9 billion activation parameters and 456 billion total parameters as the optimal configuration. 3. Computation Optimization In this section, we present our computation part, including the training and inference. In this project, we have dynamically changing GPU cluster, where the number of H800 GPUs ranges from 1500 to 2500. An efficient architecture necessitates robust implementation optimization to fully harness its computational benefits at scale. To scale our novel architecture to the requisite size, we present three key optimization strategies that primarily address the following three challenges: 1. Mitigating the all-to-all (a2a) communication overhead during the training of Mixture of Experts (MoE) architecture is persistent challenge. The configuration we choose for our experts, specifically opting for large models, imposes substantial demands on GPU memory. Therefore, the primary challenge lies in achieving an optimal equilibrium between memory utilization, computational efficiency, and the overhead associated with all-to-all communication. 2. As we endeavor to support at least 1 million token context window in both training and inference, the accurate distribution of tokens within such an extensive context window across different GPUs becomes imperative for this colossal model. This necessity, however, inevitably introduces additional communication overhead. As result, devising strategies to minimize this overhead, particularly in the context of our hybrid architecture, presents significant challenge. 3. The current implementation of the lightning attention mechanism is specifically optimized for training processes. However, in the inference scenario, the challenge arises in effectively managing real-world batched inputs, which may encompass variable sequence lengths and specific inputs that inco', 'summary': '<h2>–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –∞ —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.</p>\n<p><strong>–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>\n<p>–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö softmax –≤–Ω–∏–º–∞–Ω–∏–µ, lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ. –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ—Ä—è–µ—Ç—Å—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞ GPU –≤ —Å–µ–∫—É–Ω–¥—É (TGS). –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–µ–Ω—ã –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ HGRN2 –∏ Mamba2. </p>\n<p>–í —Ö–æ–¥–µ —Ç–µ—Å—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–ª–∞—Å—å –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–¥–µ–ª–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU H800. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç FlashAttention2.  –í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ softmax –≤–Ω–∏–º–∞–Ω–∏–µ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>\n<p><strong>–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</strong></p>\n<p>–ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, cosformer2 –∏–ª–∏ hgrn2) –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è —Å softmax –≤–Ω–∏–º–∞–Ω–∏–µ–º. –í –º–æ–¥–µ–ª—è—Ö hybrid-cosformer2 –∏ hybrid-hgrn2 –∫–∞–∂–¥—ã–µ –≤–æ—Å–µ–º—å —Å–ª–æ–µ–≤ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ softmax –≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º (hybrid-lightning) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–∫–Ω–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>\n<p>–¢–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –∏ –º–æ–¥–µ–ª–∏ —Å "–æ–∫–Ω–∞–º–∏" –≤–Ω–∏–º–∞–Ω–∏—è (sliding window attention), –≥–¥–µ softmax –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å –æ–∫–Ω–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ –º–µ—Ç—Ä–∏–∫–µ NIAH.</p>\n<p><strong>–û–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</strong></p>\n<p>–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ, —Ö–æ—Ç—è –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –æ–Ω–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏–∑-–∑–∞ –∏—Ö –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –Ω–∏—Ö, –≥–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–µ —É—Å—Ç—É–ø–∞—é—Ç softmax –≤–Ω–∏–º–∞–Ω–∏—é, –Ω–æ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –µ–≥–æ –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏.</p>\n<p><strong>–ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã softmax –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>\n<p>–ú–µ—Ö–∞–Ω–∏–∑–º softmax –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (RNN). –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—Ä–µ–º–µ–Ω–∏ t —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è, –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ t0=1, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞ —Å—á–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –æ–±–ª–∞–¥–∞—é—Ç —ç—Ç–∏–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–µ—Ä–µ—Å—á–µ—Ç–∞, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.</p>\n<p><strong>–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (Capacity) –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>\n<p>–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å RNN –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä–æ–º –µ–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å softmax –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç O(d), –∞ –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å lightning –≤–Ω–∏–º–∞–Ω–∏—è - O(d^2/h), –≥–¥–µ d - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –∞ h - —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É d &gt; h, lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏–º–µ–µ—Ç –±–æ–ª—å—à—É—é –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å, —á–µ–º softmax –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏.</p>\n<p><strong>–ê–±–ª—è—Ü–∏—è –º–æ–¥—É–ª–µ–π –≤ MoE</strong></p>\n<p>–ü—Ä–æ–≤–æ–¥—è—Ç—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –∞–±–ª—è—Ü–∏–∏ –º–æ–¥—É–ª–µ–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture of Experts (MoE). –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏ softmax –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ —Å–ª–æ—è (Pre-Layer Normalization) –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ —Å–ª–æ—è (Post-Layer Normalization).</p>\n<p><strong>–ì–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ vs Softmax –≤–Ω–∏–º–∞–Ω–∏–µ –≤ MoE</strong></p>\n<p>–ó–∞–º–µ–Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–µ–≤ softmax –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE —Å 28 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.</p>\n<p><strong>Pre-Layer Normalization vs Post-Layer Normalization</strong></p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å 60 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Post-Layer Normalization —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DeepNorm –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Pre-Layer Normalization –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º. PostNorm —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –≥–ª—É–±–∏–Ω—É –º–æ–¥–µ–ª–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ PreNorm –º–æ–∂–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å –µ–µ.</p>\n<p><strong>–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–∏</strong></p>\n<p>–ü–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å - –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤—ã–≤–æ–¥–∞. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ-—É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞, –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω 500 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏.</p>'}, {'title': 'Optimizing MoE architecture for efficient distributed training', 'content': 'rporate prefix caching. It is noteworthy that the existing open-source frameworks in the industry currently lack the necessary mature technical support to adequately address these challenges. Thus, we independently and comprehensively reinvent our distributed training and inference framework, thereby successfully addressing these challenges with the desired level of efficiency. 3.1. MoE Optimization The primary objective in optimizing the MoE architecture is to minimize communication overhead, particularly for MoE models that utilize all-to-all (a2a) communication. To address this, We implement token-grouping-based overlap scheme, as illustrated in Figure 9. In this scheme, the a2a communication is performed within the expert parallel (EP) communication group, and it overlaps with the processing of tokens from different expert groups. To ensure the correctness of the communication 14 MiniMax-01: Scaling Foundation Models with Lightning Attention results, we restrict each ProcessGroup to execute communication operators sequentially. As result, a2a communications across different groups cannot overlap, leading to the emergence of idle time. 0 0 expert Device 1 Device 0 Idle time a2a-combine a2a-dispatch w/ EP overlap w/o EP overlap This approach leads to significant performance improvements. However, upon more detailed analysis, we identified critical tradeoff specific to the expert configuration of the MiniMax-Text-01 model. When Tensor Parallelism (TP) is employed to partition the expert parameters, the computational intensity becomes excessively low, thereby hindering the efficiency of the computation. However, opting not to use TP leads to an excessively large parameter count, which necessitates the activation of larger Pipeline Parallelism (PP) configuration. The challenge emerges because PP does not reduce the memory footprint required for storing activations. This limitation is particularly detrimental for training models with long contexts, as the increase in memory consumption does not provide proportional benefits in terms of computational efficiency or training speed. Consequently, it is imperative to develop new parameter partitioning strategy that adeptly balances memory usage and computational intensity to optimize the training process for our specific model and task. Figure 9 Expert Parallel (EP) Overlap Illustration. Chunk tokens into 2 groups thus computation can overlap with communication between different groups. Time 0 1 1 To achieve enhanced efficiency, we first introduce novel ProcessGroup, termed ETP (Expert Tensor Parallel), which is specifically designed to manage the weight partitioning of experts. Concurrently, we propose another distinct ProcessGroup, named EDP (Expert Data Parallel), to encapsulate the data parallelism of identical experts. In our system, we define the total number of GPUs involved in training as ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí. The system must satisfy two key conditions: ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí = ùë†ùëñùëßùëíùëÉùëÉ ùë†ùëñùëßùëíùê∑ùëÉ ùë†ùëñùëßùëíùê∂ùëÉ ùë†ùëñùëßùëíùëá ùëÉ and ùë§ùëúùëüùëôùëë_ùë†ùëñùëßùëí = ùë†ùëñùëßùëíùëÉùëÉ ùë†ùëñùëßùëíùê∏ùê∑ùëÉ ùë†ùëñùëßùëíùê∏ùëá ùëÉ ùë†ùëñùëßùëíùê∏ùëÉ (15) (16) This configuration empowers the MoE component with the flexibility to define the distribution of experts, manage the weight partitioning of experts, and independently configure the ZeRO (Zero Redundancy Optimizer) algorithm (Rajbhandari et al., 2020). Based on this implementation, we are able to completely decouple the parallel strategies of the MoE components from those of the non-MoE components. Building upon this modification, we can flexibly configure the ETP to achieve an optimal balance between memory usage and computational intensity. Furthermore, to mitigate communication overhead, we design an EP-ETP overlap strategy. This strategy aims to maximize the utilization of both network resources and computational resources, as illustrated in Figure 10 (a). Since communications within the same process group must be executed sequentially, extended periods of computation not only facilitate overlap with greater number of communications but also create additional opportunities for communications across different process groups to overlap, leading to enhanced overall performance as illustrated in Figure 10 (b). When determining the number of groups, several trade-offs must be considered. Theoretically, only by dividing the workload into sufficiently large number of groups can we achieve ample overlap between communication and computation, as illustrated in Figure 10 (c). However, in practice, an excessive number of groups can significantly increase the complexity of scheduling and introduce the 15 MiniMax-01: Scaling Foundation Models with Lightning Attention w/o EP + ETP overlap Idle time Waste time a2a-dispatch allgather expert reduce scatter a2a-combine (a) Device Device 1 Device 2 Device 3 w/ EP + ETP overlap 0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 w/o EP + ETP overlap a2a-dispatch allgather expert reduce scatter a2a-combine w/ EP + ETP overlap Device 0 0 0 (b) Device Device 2 Device 3 0 1 1 1 2 2 3 3 (c) Device 0 Device 1 Time 0 0 0 0 1 1 1 2 2 3 3 3 0 1 1 1 Figure 10 EP-ETP Overlap Illustration. (a) EP-ETP overlap with the lower computation portion. (b) EP-ETP overlap with the higher computation portion. (c) EP-ETP overlap with fewer groups. Compared with (a) and (b), it shows that if the compute time cost is longer, the efficiency will be better. Comparing with (b) and (c), it shows that fewer groups will lead to insufficient overlap. risk of becoming CPU-bound. Given that the proportion of ETP (Expert Tensor Parallel) in the overall MoE (Mixture of Experts) architecture is not substantial, it is crucial to make adjustments based on the specific context and requirements. Through the aforementioned optimization strategies, we achieve balanced configuration of storage and computational intensity for the specific expert specifications in the MoE (Mixture of Experts) structure of the MiniMax-Text-01 model. Furthermore, based on these optimizations, we reduce the pure communication overhead of the MoE component by 50% compared to the preoptimization state, resulting in significant improvement in training efficiency. 3.2. Long Context Optimization significant challenge in long context training is that real training samples are difficult to standardize into uniform length. The conventional approach of using padding to make samples the same length leads to substantial computational waste. In the context of training at the 1M sequence length scale, this waste becomes particularly significant. To address this issue, we adopt data formatting technique during training where different samples are concatenated end-to-end along the sequence dimension. We refer to this technique as "data-packing". This format minimizes computational waste during the computation process, thereby conserving computational resources. 3.2.1. Varlen Ring Attention For Softmax Attention, the ring attention algorithm (Liu et al., 2024a) offers an effective method to partition data, thereby enabling unlimited scalability. However, the existing implementations MiniMax-01: Scaling Foundation Models with Lightning Attention causal compute causal varlen compute non-causal compute non-causal varlen compute (a) (b) Figure 11 Ring Attention v.s. Varlen Ring Attention. (a) No data packing in ring attention. (b) Pack 3 samples with different lengths in varlen ring attention. are not optimized to efficiently handle the ring attention mechanism for the data-packing format. In the case of FlashAttention (Dao, 2024), while it provides varlen (variable length) interface to accommodate the data-packing format, there is no corresponding ring attention implementation available. Regarding TransformerEngine (NVIDIA, 2023), the implementation incorporates Context Parallel (CP) ProcessGroup to support the ring attention algorithm. However, this approach poses risk of computational resource waste when dealing with the data-packing format. This is because the algorithm divides each sequence into 2 ùë†ùëñùëßùëíùê∂ùëÉ segments and applies the ring attention mechanism to each segment. Consequently, this approach restricts each sequence to length that must be an integer multiple of 2 ùë†ùëñùëßùëíùê∂ùëÉ. In scenarios where the sample distribution is unknown and the CP size is set to large value, this can lead to significant padding, resulting in the waste of computational resources. Motivated by the principle of not making assumptions about the sample distribution, we redesign the algorithm and name it Varlen Ring Attention. This approach avoids the excessive padding and subsequent computational waste associated with traditional methods by applying the ring attention algorithm directly to the entire sequence after data-packing. Specifically, the implementation involves distinguishing the offset of the attention mask corresponding to each sequence within the ring attention computation. The key modification is to transform the original causal computations into varlen causal computations and similarly convert the non-causal computations into varlen non-causal computations, shown in Figure 11. 3.2.2. Improved Linear Attention Sequence Parallelism For lightning attention, the LASP (Linear Attention Sequence Parallelism) algorithm (Sun et al., 2024) leverages the communication group of CP to facilitate the expansion of long sequences. As illustrated in Figure 12 (a), the LASP algorithm mandates that all CP ranks engage in send-recv operations to exchange intermediate key-value (ùêæùëâ) block results. This requirement imposes sequential dependency among the CP ranks, thereby compelling the computation to be performed in serial manner. Consequently, this sequential dependency significantly impedes the overall efficiency of the training process, as the inherent parallelism of the system is not fully exploited. To fully harness the parallel computing capabilities of GPU devices, we propose an optimized approach that refines the computational and communication workflow to eliminate dependencies during the computation process. This optimization effectively transforms serial computation into parallelized one. The enhanced approach, termed LASP+ (Figure 12 (b)), operates as follows: 1. Local Prefix Sum Calculation: Each computing node i.e., the CP rank, initiates the process by 17 MiniMax-01: Scaling Foundation Models with Lightning Attention block size padding init ùêæùëâ = ùêæ0ùëâ0 with zeros shape [ùëë, ùëë] init diag with decay shape [ùëë, ùëë] input sequence, shape [ùë†, ‚Ñé, ùëë] [‚Ñé, ùë†, ùëë] ùëÑ,ùêæ,ùëâ have the same shape, split sequence with CP ùëÑ2 ùêæ2 ùëâ2 ùëÑ4 ùêæ4 ùëâ4 4 ùëÑ3 ùêæ3 ùëâ3 3 recv ùëÑ6 ùêæ6 ùëâ 6 ùëÑ5 ùêæ5 ùëâ5 5 recv ùëÑ7 ùêæ7 ùëâ7 recv 8 deprecate send sum(0-2) sum(0-3) send sum(0-4) sum(0-5) send sum(0-6) sum(0-7) deprecate output sequence, shape [‚Ñé, ùë†, ùëë] [ùë†, ‚Ñé, ùëë] Initialize Phase (a) ùëÑ ùêæ ùëâ ùëÑ1 ùêæ1 ùëâ1 ùëÇùëñùëõùë°ùëüùëé ùëñ = (ùëÑùëñùêæùëñ ùëÄ)ùëâùëñ [‚Ñé, ùêµ, ùëë] ùêæùëñùëâùëñ = ùê∑ ùêæùëñ ùëâùëñ [‚Ñé, ùëë, ùëë] ùêæùëâ ùëÇùëñùëõùë°ùëíùëü ùëñ = ùê∑ ùëÑùëñ ùêæùëâ [‚Ñé, ùêµ, ùëë] ùëÇùëñ = ùëÇùëñùëõùë°ùëíùëü ùëñ + ùëÇùëñùëõùë°ùëüùëé ùëñ 0 ùêæùëâ += ùêæùëñùëâùëñ sum(0-1) ùëÇùëñùëõùë°ùëüùëé ùëñ = (ùëÑùëñùêæùëñ ùëÄ)ùëâùëñ [‚Ñé, ùêµ, ùëë] ùêæùëñùëâùëñ = ùê∑ ùêæùëñ ùëâùëñ [‚Ñé, ùëë, ùëë] 1 2 3 4 5 7 8 deprecate local prefix sum ùêæùëâùêø sum(1-2) sum(3-4) sum(5-6) sum(7-8) (b) allgather across ranks global prefix sum ùêæùëâùê∫ sum(0) sum(0-1) sum(0-2) sum(0-3) sum(0-4) sum(0-5) sum(0-6) sum(0-7) ùëÇùëñùëõùë°ùëíùëü ùëñ = ùê∑ ùëÑùëñ ùêæùëâùê∫ [‚Ñé, ùêµ, ùëë] ùëÇùëñ = ùëÇùëñùëõùë°ùëíùëü ùëñ + ùëÇùëñùëõùë°ùëüùëé ùëñ output sequence, shape [‚Ñé, ùë†, ùëë] [ùë†, ‚Ñé, ùëë] Figure 12 Difference of LASP Algorithm and LASP+ Algorithm. (a) LASP Algorithm. 1. Initialization Phase: initializing KV to zero and the diagonal decay matrix. 2. Data Partitioning and Padding: partitioning the Q, K, and matrices along the sequence dimension into CP size (4 segments illustrated in the figure) blocks, dividing each block into smaller blocks based on the BlockSize ùêµ and padding the remaining part (e.g. Q7, K7, V7) that cannot be divided evenly by ùêµ. 3. Intra-block Computation: performing intra-block of each CP rank computations in parallel. 4. Inter-block Computation and Communication: starting from CP rank 0, computing the inter-block portion of the current ùëÑùëñ with all previous KV blocks and the prefix sum ùêæùëñùëâùëñ. Different CP ranks communicate data through send-recv operations. (b) LASP+ Algorithm. Building upon figure (a), each CP rank computes the local prefix sum ùêæùëâùêø and performs AllGather operation to synchronize, then selects the local prefix sum ùêæùëâùêø to compute the global prefix sum ùêæùëâùê∫. The remaining computational components are same as (a). 18 MiniMax-01: Scaling Foundation Models with Lightning Attention independently calculating its local prefix sum, denoted as ùêæùëâùêø. 2. Global Synchronization via AllGather: Following the local calculations, an AllGather operation is performed to synchronize the information from all nodes globally. This step ensures that each node has access to the necessary data from all other nodes. 3. Prefix Sum Computation: Each node selects the specific CP ranks ùêæùëâùêø on which to perform the prefix sums, decision based on its assigned computation order. By implementing these steps, the LASP+ approach effectively removes the original dependencies between the computation nodes. This elimination of dependencies facilitates fully parallelized computation process, thereby significantly enhancing the overall efficiency and throughput of the system. The transformation from serial to parallel computation not only leverages the full potential of GPU devices but also ensures that the training process can be executed more rapidly and with greater scalability. The proposed modifications, while incurring additional costs in terms of increased total communication volume and temporary memory usage, are unequivocally justified by the substantial performance benefits they confer. These enhancements significantly outweigh the associated overhead in communication and memory consumption. Through comprehensive testing and verification, it is empirically demonstrated that the computation speed in the LASP+ approach can attain up to 1/ùëÅùëùùëêùëõ of the original LASP algorithm, where ùëÅùëùùëêùëõ denotes the number of parallel computing nodes. Furthermore, the overhead introduced by the AllGather operation is minimal, which is consistent with our anticipations and underscores the efficacy of the optimization. Building upon the LASP+ framework, we further introduce support for the varlen feature to effectively manage the data-packing data structure. This enhancement is particularly beneficial for handling batched samples that comprise inputs with unequal token lengths. The ', 'summary': '<h2>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01</h2>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE (Mixture of Experts) –∏ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01.</p>\n<h3>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE</h3>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ MoE ‚Äî –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö all-to-all (a2a) –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é. –î–ª—è —ç—Ç–æ–≥–æ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤. –í —ç—Ç–æ–π —Å—Ö–µ–º–µ a2a –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (EP), –∏ –æ–Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ß—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ a2a –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏ –Ω–µ –º–æ–≥—É—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—Ç—å—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø—Ä–æ—Å—Ç–æ—è–º.</p>\n<p>–û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ, –≤—ã—è–≤–∏–ª—Å—è –∫–æ–º–ø—Ä–æ–º–∏—Å—Å, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤ (TP) –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ–π, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ù–æ –æ—Ç–∫–∞–∑ –æ—Ç TP –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ (PP). –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ PP –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–ª–æ—Ö–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, —Ç–∞–∫ –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –Ω–µ –¥–∞–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.</p>\n<p>–ü–æ—ç—Ç–æ–º—É –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å. –í–≤–æ–¥—è—Ç—Å—è –Ω–æ–≤—ã–µ –≥—Ä—É–ø–ø—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:</p>\n<ul>\n<li><strong>ETP (Expert Tensor Parallel)</strong>: —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.</li>\n<li><strong>EDP (Expert Data Parallel)</strong>: –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–∞–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.</li>\n</ul>\n<p>–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU, —É—á–∞—Å—Ç–≤—É—é—â–∏—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏ (world_size), –¥–æ–ª–∂–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å –¥–≤—É–º —É—Å–ª–æ–≤–∏—è–º, —Å–≤—è–∑—ã–≤–∞—é—â–∏–º —Ä–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞:</p>\n<p><code>world_size = sizePP * sizeDP * sizeCP * sizeTP</code>\n<code>world_size = sizePP * sizeEDP * sizeETP * sizeEP</code></p>\n<p>–¢–∞–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —É–ø—Ä–∞–≤–ª—è—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –∏—Ö –≤–µ—Å–æ–≤ –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º ZeRO (Zero Redundancy Optimizer). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ MoE –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–µ-MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. ETP –º–æ–∂–Ω–æ –≥–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é.</p>\n<p>–î–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è EP-ETP. –≠—Ç–æ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∫ —Å–µ—Ç–µ–≤—ã—Ö, —Ç–∞–∫ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø–æ—ç—Ç–æ–º—É –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>\n<p>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø —Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, —Ç–æ–ª—å–∫–æ —Ä–∞–∑–¥–µ–ª–∏–≤ —Ä–∞–±–æ—Ç—É –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø, –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏. –û–¥–Ω–∞–∫–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –º–æ–∂–µ—Ç —É—Å–ª–æ–∂–Ω–∏—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—é –ø–æ CPU. –¢–∞–∫ –∫–∞–∫ –¥–æ–ª—è ETP –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE –Ω–µ–≤–µ–ª–∏–∫–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π.</p>\n<p>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª–∏–ª–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ MoE –º–æ–¥–µ–ª–∏ MiniMax-Text-01. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —É–¥–∞–ª–æ—Å—å —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ MoE –Ω–∞ 50% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É –ø–æ–≤—ã—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.</p>\n<h3>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤</h3>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ç—Ä—É–¥–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ç–µ—Ä—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤.</p>\n<p>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ "data-packing", –∫–æ–≥–¥–∞ —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>\n<h4>Varlen Ring Attention</h4>\n<p>–ê–ª–≥–æ—Ä–∏—Ç–º –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (ring attention) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ data-packing. FlashAttention –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å varlen (–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã), –Ω–æ –Ω–µ –∏–º–µ–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. TransformerEngine –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Context Parallel (CP), –Ω–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä—è–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ data-packing, —Ç–∞–∫ –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–ª–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫—Ä–∞—Ç–Ω–∞ <code>2 * sizeCP</code>.</p>\n<p>–ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Ç–µ—Ä—å, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º, –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º Varlen Ring Attention. –û–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–ª—å—Ü–µ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∫–æ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ data-packing. –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ —Å–º–µ—â–µ–Ω–∏—è –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–±—ã—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π.</p>\n<h4>–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (LASP)</h4>\n<p>–ê–ª–≥–æ—Ä–∏—Ç–º LASP (Linear Attention Sequence Parallelism) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä—É–ø–ø—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ CP –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ù–æ LASP —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –≤—Å–µ —Ä–∞–Ω–≥–∏ CP —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏ –≤ –æ–ø–µ—Ä–∞—Ü–∏—è—Ö send-recv –¥–ª—è –æ–±–º–µ–Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ key-value (KV). –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —Ä–∞–Ω–≥–∞–º–∏ CP, —á—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>\n<p>–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π GPU –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞—Å–∫—Ä—ã—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Å–∏—Å—Ç–µ–º—ã –∏ –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.</p>'}, {'title': 'Optimizing inference in lightning attention mechanism', 'content': 'process involves the following steps: 1). Padding to Block Size: Each input within the batch is padded to ensure that its length is multiple of the predefined block size, which is set to 256. This padding step is crucial for aligning the data structure with the computational requirements of the kernel. 2). Sequential Concatenation: After padding, the inputs are sequentially concatenated. This concatenation facilitates the use of single kernel to perform parallel computations across multiple batches. By organizing the data in this manner, we can efficiently leverage the parallel processing capabilities of the GPU, thereby optimizing computational performance. The integration of the varlen feature with the LASP+ framework ensures that the system can handle diverse input lengths without compromising on efficiency. This approach not only simplifies the computational workflow but also maximizes resource utilization by enabling the processing of multiple batches concurrently. 3.3. Lightning Attention Inference Optimization The initial implementation of the lightning attention mechanism is primarily research-oriented and not yet suitable for practical applications, especially for inference. However, the optimization of inference processes is of paramount importance in real-world scenarios, as the long-term cost of deploying trained model is predominantly determined by the efficiency of its inference. To this end, we implement four optimization strategies for lightning attention: batched kernel fusion, separated prefill and decoding execution, multi-level padding, and strided batched matmul extension. 19 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.1. Batched Kernel Fusion We fuse multiple memory-bound kernels and extend support to accommodate all batch inputs. In the prefill phase, we perform kernel fusion for processing the ùëÑ, ùêæ, and ùëâ tensors, including padding in the sequence dimension, partitioning into blocks, adjusting the internal layout, and computing the decay values. In the decoding phase, we perform kernel fusion for the computation of ùêæùëâ and the updating of the prefix ùêæùëâ cache. These kernel fusions reduce intermediate result storage and memory access operations, thereby significantly improving memory access efficiency and reducing end-to-end latency by 10% in the decoding phase and short-text input scenarios. By the way, these optimizations can bring very noticeable benefits on H20 compared to H800. 3.3.2. Separated Prefill and Decoding Execution The implementation of the lightning attention mechanism for long sequence computations primarily revolves around the differentiation between intra-block and inter-block computations. However, this approach is not optimal for inference tasks, particularly in the decoding phase, where the token length is consistently equal to 1. Given that the computational kernel for tokens of length 1 is predominantly memory-bound and necessitates only limited number of GPU Streaming Multiprocessors (SMs), we propose strategy that segregates the processing of tokens with length of 1 from those with length greater than 1. This is achieved by employing two distinct kernels. Subsequently, we utilize two separate CUDA streams to schedule these kernels in parallel, thereby enhancing computational efficiency and ensuring balanced GPU utilization, especially in scenarios involving mixed inputs. For instance, in batch size of 20, where all inputs contain prefix key-value (KV) cache, and the scenario includes one or two inputs with token length of 50 while the remaining inputs have token length of 1, this approach can significantly reduce latency. Specifically, the latency can be approximately equivalent to that of processing only the longer inputs, demonstrating reduction from 100 milliseconds to 50 milliseconds. 3.3.3. Multi-level Padding By applying padding to the ùëÑ, ùêæ, ùëâ tensors along the sequence dimension, the intra-block and interblock components can be effectively decomposed into multiple identical matrix multiplications. This decomposition is particularly advantageous as it aligns seamlessly with the StrideBatchedMatmul interface, thereby facilitating the maximization of parallel processing capabilities. Initially, the block size for padding was set to 256, configuration that was consistent with the training parameters. However, upon the implementation of the prefix cache technique, it is observed that the token lengths within batch typically fall below 256. This discrepancy led to redundant computations within each matrix multiplication operation. To address this inefficiency and minimize unnecessary computations, we propose the introduction of additional segmentation options, specifically 32, 64, and 128. This multi-level padding approach enables the dynamic selection of the computational scale that incurs the minimal padding overhead, based on the current input sequence length. By adopting this approach, the utilization of computational resources is optimized, ensuring that the system operates with increased efficiency and reduced redundancy. This strategic adjustment not only conserves computational resources but also contributes to the overall performance enhancement of the system. 20 MiniMax-01: Scaling Foundation Models with Lightning Attention 3.3.4. StridedBatchedMatmul Extension We utilize the optimized function cublasGemmStridedBatchedEx from the NVIDIA cuBLAS Library to manage StridedBatchedMatmul operations, thereby ensuring both high performance and versatility across diverse hardware architectures. Concurrently, we are in the process of implementing more extensive kernel fusion strategy, with the objective of substantially improving the computational efficiency of Hopper GPUs. Given that our sequence partitioning block size is configured to 256, the associated General Matrix-Matrix Multiplication (GEMM) operations, which involve matrices of dimensions 256x256, can leverage warpgroup-wide WGMMA instructions for computation. To further enhance memory access efficiency, we integrate the asynchronous operations of the Tensor Memory Accelerator (TMA) and delegate certain preprocessing and postprocessing computational tasks to be executed asynchronously on the CUDA Cores. Ultimately, our goal is to dynamically regulate the number of pipeline stages to adaptively attain optimal performance across both H20 and H800 GPU architectures. This adaptive control mechanism will ensure that the system can efficiently handle varying workloads and hardware configurations, thus maximizing overall computational throughput and resource utilization. By implementing the aforementioned optimizations, we achieve Model Flops Utilization (MFU) exceeding 75% on the H20 GPU for end-to-end inference tasks (Chowdhery et al., 2023). Specifically, in our MiniMax-Text-01 and MiniMax-VL-01 inference, when considering the latency ratio between the attention operation and the Feed-Forward Network (FFN) operation within the MoE structure, the softmax attention constitutes 95% of the latency at sequence length of 1,024,000 tokens. In contrast, the lightning attention implementation contributes to less than 12% of the latency under the same conditions. Our lightning attention implementation exhibits remarkable efficiency in managing heterogeneous batch inputs, which are characterized by diverse sequence lengths. This efficiency is particularly evident in scenarios where some inputs incorporate the prefix caching strategy while others do not. The reduction in latency not only enhances the overall speed of the inference process but also ensures that the system can handle wide range of input types with minimal performance degradation. This adaptability underscores the robustness and versatility of our lightning attention approach in real-world applications. 4. Pre-Training In this section, we provide an overview of the pre-training methodology for MiniMax-Text-01. First, we detail the meticulous construction of our pre-training corpus, with particular emphasis on data quality, standardized formatting, and mixing strategies to maximize model performance. Subsequently, we outline our innovative data experimentation framework, which enables rapid and resource-efficient evaluation of data effectiveness while minimizing computational costs. Lastly, we present an in-depth analysis of the models training hyper-parameters and present hierarchical training approach, which enables context length scaling up to 4 million tokens. 4.1. Data 4.1.1. Pre-training Corpus The pre-training corpus for MiniMax-Text-01 encompasses comprehensive and meticulously curated dataset, incorporating diverse sources including academic literature, books, web content, and 21 MiniMax-01: Scaling Foundation Models with Lightning Attention programming code. We enhance corpus quality through several strategic dimensions: Data Quality Enhancement. Superior data quality is fundamental for Large Language Models. We implement sophisticated filtering pipeline, combining rule-based cleaning and deduplication procedures aligned with established practices (Penedo et al., 2023, 2024; Rae et al., 2021). To assess document quality at granular level, we utilize our previous-generation model as the reward labeler (a MoE model with 5B activations and 60B total parameters). Initially, we evaluate multiple quality dimensions including coherence, conciseness, educational value, helpfulness, knowledge richness, and categorical relevance. Through comprehensive analysis, we identify significant correlations among these metrics and ultimately focus on three key dimensions: knowledge depth, practical helpfulness, and categorical distribution, while maintaining other metrics as secondary validation indicators. Data Formatting Optimization. The content from websites and books, once appropriately extracted and cleaned, can naturally be used as high-quality textbooks (Gunasekar et al., 2023) without further formatting. For dialogue and question-answering data, the sequential nature of text inherently captures conversational logic and question-answer relationships. Although humans benefit from additional formatting (e.g., Markdown) for readability and comprehension, we find that heavy formatting can actually diminish data diversity and quality by introducing fixed patterns that constrain the natural variation present in human conversations. Ultimately, to maintain format generalization capabilities and accommodate human preferences in alignment, we implement nested document format with versatile templates for dialogue and QA data, carefully balancing natural comprehension with structural consistency across various interaction patterns. Data Mixture Investigation. We develop sophisticated approach to tuning the data distribution, leveraging our three primary quality metrics. Based on the experiment paradigm detailed in the subsequent section, we discover that while high-scoring content on knowledge depth and helpfulness generally yielded superior performance in capability assessments, completely eliminating lower-scoring content can adversely affect downstream task performance. Therefore, we implement balanced sampling strategy, beginning with uniform distribution across the base corpus, and then adjusting sampling weights to favor high-quality content while maintaining sufficient representation of diverse categories. 4.1.2. Tokenization For tokenization, we employ byte-level Byte Pair Encoding (BPE) (Brown et al., 2020; Shibata et al., 1999), incorporating the pre-tokenizer methodology. We strategically up-sample multilingual content, to enhance the corresponding compression efficiency. The resulting vocabulary size is set to 200K tokens. 4.1.3. Data Experiment To systematically evaluate our design choices regarding pre-training data quality, format, and composition, we conduct extensive ablation experiments. These experiments involve training multiple small-scale MoE models using comparable token quantities but varying data characteristics. This approach enables us to isolate and measure the impact of individual data attributes while maintaining computational efficiency. 22 MiniMax-01: Scaling Foundation Models with Lightning Attention 4.1.3.1 Paradigm Formulation. We conduct Data Experiments to systematically compare the performance of different model variants. Specifically, we formulate experiments as statistical hypothesis tests that compare evaluation metric distributions between baseline model and models trained with different data configurations. When testing the effectiveness of new data corpus D, we formulate our alternative > ùúáùëábaseline, where ùúá represents the weighted average performance metric and ùëá hypothesis as ùêª1 : ùúáùëáD denotes the distribution of evaluation values across test samples. Evaluation. We carefully design our evaluation norms to ensure meaningful insights. We look at wide range of multiple-choice benchmarks, discarding choice indices in query formulation and look at the likelihoods of completion. We observe the distributions of sample-wise log-normalized accuracy log accnorm2, defined as log accnorm2 (ùë•) = log softmaxùëù (ùëêùê∂ùë• ) (cid:110) ( ùëù(ùëê)) (cid:111) , ùëñ (ùëê) = ùëùùëñ (ùëê) where ùëù bytes(c) is the byte-normalized probability of choice ùëê for sample ùëñ. We choose bytewise normalization to exclude the effect of tokenizer, while alleviating the disfavor towards longer choices. We conduct extensive experiments to ensure that this metric is stable across training, while maintaining the discriminative power of the metric, which is quantified by the ratio Œîobvious/ùúéseed, where Œîobvious represents the obvious difference in performance between models and ùúéseed denotes the standard deviation across different random seeds. Experiment Efficiency & Setup. With such statistical setup, we are able to conduct power analysis to decide minimal test sample size while maintaining the MDE (Minimal Detectable Effect) at similar level as our training variance, and guaranteeing 95% confidence level and 80% power for decision making. With the confidence methodologies set, we conduct simple scaling experiments on token amount and the model size, and eventually land at an experiment step of training MoEs of 1B activation and 8B total parameters with 40B tokens of data, where data mixture comprises 20B web documents and 20B data of hypothesi', 'summary': '<h2>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ Lightning Attention</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤–Ω–µ–¥—Ä–µ–Ω—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ Lightning Attention –ø—Ä–∏ –≤—ã–≤–æ–¥–µ (inference). –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Lightning Attention –±—ã–ª–∞ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ü–µ–ª–∏ –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏–ª–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤—ã–≤–æ–¥–∞, –≥–¥–µ –≤–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã.  –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –∏–º–µ–Ω–Ω–æ –æ—Ç –Ω–µ–µ –∑–∞–≤–∏—Å–∏—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:</p>\n<p><strong>1. –°–ª–∏—è–Ω–∏–µ –ø–∞–∫–µ—Ç–Ω—ã—Ö —è–¥–µ—Ä (Batched Kernel Fusion):</strong></p>\n<p>–ë—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–∞–º—è—Ç—å—é, –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –∏—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ –≤—Å–µ–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤ –ø–∞–∫–µ—Ç–µ. –ù–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (prefill) –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–ª–∏—è–Ω–∏–µ —è–¥–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ Q, K –∏ V, –≤–∫–ª—é—á–∞—è –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞ (padding) –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–ª–æ–∫–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –º–∞–∫–µ—Ç–∞ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞—Ç—É—Ö–∞–Ω–∏—è. –ù–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–ª–∏—è–Ω–∏–µ —è–¥–µ—Ä –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è KV –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ KV-–∫–µ—à–∞. –≠—Ç–æ —Å–ª–∏—è–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—ä–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ–ø–µ—Ä–∞—Ü–∏–π –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏, –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏ —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—â—É—é –∑–∞–¥–µ—Ä–∂–∫—É –Ω–∞ 10% –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –≤–≤–æ–¥–∞–º–∏. –≠—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω—ã –Ω–∞ GPU H20 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å H800.</p>\n<p><strong>2. –†–∞–∑–¥–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ prefill –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (Separated Prefill and Decoding Execution):</strong></p>\n<p>–ú–µ—Ö–∞–Ω–∏–∑–º Lightning Attention –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–æ–≤ –∏ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤—ã–≤–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–¥–µ –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞ –æ–±—ã—á–Ω–æ —Ä–∞–≤–Ω–∞ 1. –ü–æ—Å–∫–æ–ª—å–∫—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ —è–¥—Ä–æ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª–∏–Ω—ã 1 –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏ —Ç—Ä–µ–±—É–µ—Ç –ª–∏—à—å –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ (SM) GPU, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª–∏–Ω—ã 1 –∏ —Ç–æ–∫–µ–Ω–æ–≤ –±–æ–ª—å—à–µ–π –¥–ª–∏–Ω—ã. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —è–¥—Ä–∞ CUDA, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ –¥–≤—É—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –≤–≤–æ–¥–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –ø–∞–∫–µ—Ç–µ –∏–∑ 20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–º–µ—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å–Ω—ã–π KV-–∫–µ—à, –∞ 1-2 —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–º–µ—é—Ç –¥–ª–∏–Ω—É 50, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ 1, —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ —É—Ä–æ–≤–Ω—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 100 –º—Å –¥–æ 50 –º—Å).</p>\n<p><strong>3. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–¥–¥–∏–Ω–≥ (Multi-level Padding):</strong></p>\n<p>–ü—Ä–∏–º–µ–Ω—è—è –ø–∞–¥–¥–∏–Ω–≥ –∫ —Ç–µ–Ω–∑–æ—Ä–∞–º Q, K, V –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∏ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å StrideBatchedMatmul –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ, —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ –±—ã–ª 256, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –æ–±—É—á–µ–Ω–∏—è. –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ –∫–µ—à–∞, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –ø–∞–∫–µ—Ç–µ –æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ 256, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª–∏—à–Ω–∏–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º. –ß—Ç–æ–±—ã —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: 32, 64 –∏ 128. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏ –Ω–∞ –ø–∞–¥–¥–∏–Ω–≥, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–π –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>\n<p><strong>4. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ StridedBatchedMatmul (StridedBatchedMatmul Extension):</strong></p>\n<p>–î–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ StridedBatchedMatmul –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è cublasGemmStridedBatchedEx –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ NVIDIA cuBLAS, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö. –¢–∞–∫–∂–µ –≤–µ–¥–µ—Ç—Å—è —Ä–∞–±–æ—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Å–ª–∏—è–Ω–∏—è —è–¥–µ—Ä –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ GPU Hopper. –ü–æ—Å–∫–æ–ª—å–∫—É —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 256, –æ–ø–µ—Ä–∞—Ü–∏–∏ GEMM (General Matrix-Matrix Multiplication) —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ 256x256 –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ WGMMA –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ Tensor Memory Accelerator (TMA), –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –Ω–∞ —è–¥—Ä–∞—Ö CUDA. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU H20 –∏ H800.</p>\n<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏ (MFU) –ø—Ä–µ–≤—ã—à–∞–µ—Ç 75% –Ω–∞ GPU H20 –ø—Ä–∏ –≤—ã–≤–æ–¥–µ. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–π –≤–Ω–∏–º–∞–Ω–∏—è –∏ –æ–ø–µ—Ä–∞—Ü–∏–π Feed-Forward Network (FFN) –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ MoE, softmax –≤–Ω–∏–º–∞–Ω–∏–µ –∑–∞–Ω–∏–º–∞–µ—Ç 95% –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 1 024 000 —Ç–æ–∫–µ–Ω–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Lightning Attention ‚Äì –º–µ–Ω–µ–µ 12%. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Lightning Attention —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –ø–∞–∫–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞ –¥—Ä—É–≥–∏–µ –Ω–µ—Ç.</p>'}, {'title': 'Optimizing LLM performance through repetition-aware deduplication strategies', 'content': 'The incorporation of repeated data has been empirically demonstrated to introduce several detrimental effects on the models performance and generalization capabilities (Hernandez et al., 2022). Consequently, implementing deduplication strategies is essential for optimizing LLM performance. Recent studies (Abdin et al., 2024; Penedo et al., 2024) suggest that repeatedly training high-quality documents can lead to enhanced downstream performance, with certain high-quality domains being trained up to 50 times, where the repetition is measured by MinHash similarity(Broder, 1997; Lee et al., 2022). However, our empirical analysis reveals that their experimental paradigm is inadequate for assessing the impact of repetition, as data efficiency is not consistent throughout the training process. To achieve better alignment with the results of the full training, we introduce novel repetitionaware experimental framework. Specifically, we first perform global deduplication on the dataset to remove redundant entries. Then, we down-sample the documents to align the repetition frequency with the requirements of the final training schedule while adhering to the budget constraints of our ablation experiments, different from the previous experimental setups which directly adopted data distributions identical or similar to those used in the final training stage. Our findings indicate that low-quality data suffer substantial decrease in performance after training for more than two epochs, while high-quality data can be effectively trained for up to four epochs, similar to previous 23 MiniMax-01: Scaling Foundation Models with Lightning Attention observations (Muennighoff et al., 2023). Notably, the solution derived from the proposed framework yields better alignment with the results obtained using considerably more computational resources. By carefully controlling the repetition and quality of the training data, we achieve more efficient and effective data mixture, ultimately leading to better model performance. 4.2. Training Strategy Initial Pre-training. We initialize all model parameters using the Xavier initialization method (Glorot and Bengio, 2010), the scaling factors of DeepNorm (Wang et al., 2024a) are set to ùõº = (2ùëÅ)0.25 and ùõΩ = (8ùëÅ) 0.25, where ùëÅ denotes the number of layers. We employ the AdamW optimizer (Loshchilov and Hutter, 2019) with ùõΩ1 = 0.9, ùõΩ2 = 0.95, and the weight decay is set to 0.1. The training sequence length is 8192, and the batch size is progressively scaled from an initial size of 16M to 32M at 69B tokens, to 64M at 790B tokens, and finally to 128M at 4.7T tokens, where it remains until the end of training. The schedule is designed based on the correlation between training loss and the critical batch size (McCandlish et al., 2018). It is argued that training at the critical batch size yields near-optimal balance between training time and data efficiency (Kaplan et al., 2020). Following this, we fit power-law relationship between the loss and the critical batch size on data from smaller models, as shown in Figure 13. The batch size is doubled when the corresponding loss is reached. The learning rate schedule begins with linear warm-up over 500 iterations to peak value of 2 104, followed by training with constant learning rate for 7.2T tokens. In the latter stages of training, we notice anomalous gradient norm values. This issue is attributed to an excessively high learning rate and we adjusted lr to 1.3 104 for the remaining 3.2T tokens. During the fast decay phase, we train 1T tokens and exponentially decrease the learning rate to 3 105. Additionally, the MoE auxiliary loss coefficient is set to 0.01. Figure 13 The power-law fit for the training loss and the critical batch size, utilizing data from models ranging from 50M to 600M in activated parameters counts. We mark the points where the batch size is doubled with dashed gray lines. Long-Context Extension. We incrementally expand the models training context length to 1M tokens. Due to our architectures effective length extrapolation capabilities, the model successfully demonstrates its ability to process sequences up to 4M tokens in the vanilla Needle-In-AHaystack retrieval task (NIAH) test 2, despite only being trained on contexts up to 1M tokens, as illustrated in Figure 14. Specifically, we employ three-stage training procedure to systematically upsample long-context data across diverse length ranges, while preserving the distributional characteristics of critical domains to preserve short-context evaluation performances steady. The details of the training data mixture, RoPE base frequency, and training length are shown in Table 6. We also mix in 10% of high-quality long-context question-answering data with similar length distribution as long-context pre-training data during the last 20% of training cycles in each stage(Parmar et al., 2024). To mitigate potential instabilities resulting from distributional shifts, we utilize linear interpolation of source-specific weights throughout the transitional phase. This method facilitates gradual and controlled evolution of the data distribution towards the desired target distribution, thereby ensuring training stability 2Same as Gemini (Team et al., 2024a), we use Paul Graham (https://paulgraham.com/articles.html) as the haystack and The special magic {city} number is: {number} as the needle. 24 MiniMax-01: Scaling Foundation Models with Lightning Attention Figure 14 4 Million vanilla Needle-In-A-Haystack retrieval task pressure test on MiniMax-Text-01. The token interval is 32K when it is less than 1M, and the token interval is 0.5M when it is greater than 1M. and preserving convergence properties. Additionally, our findings indicate that NIAH is inadequate for effectively monitoring the models performance throughout the training process. This is primarily because NIAH metric performance reaches its peak score early on, specifically within the initial 128K training steps. To tackle this limitation, we evaluate the models intermediate checkpoints using more demanding tasks, which are designed to increase in complexity as training progresses. Notably, despite the escalating difficulty of these tasks, we consistently observe steady improvement in the models performance metrics. This sustained upward trajectory clearly demonstrates the critical importance and necessity of implementing long-context continual pretraining. More details are given in Section 5.7.2. Table 6 Long-Context Extension Recipe. For clarity, we categorize the data as follows: data with fewer than 32K tokens are labeled as Short; data ranging from 32K to 128K tokens are labeled as Medium; and data exceeding 128K tokens are categorized as Long. Training Length RoPE Frequency # Tokens Short (%) Medium (%) Long (%) 128K 512K 1M 5M 10M 10M 300B 32B 26B 30 35 30 70 35 30 0 30 40 5. Post-training In this section, we present thorough post-training framework designed to enhance the models general performance, long-context capability, and real-world applicability. Our approach begins with the creation of diverse, high-quality prompt dataset, accompanied by hierarchical reward system that evaluates responses across multiple dimensions: correctness, truthfulness, helpfulness, and harmlessness. The training process consists of Supervised Fine-Tuning (SFT), Offline and Online Reinforcement Learning (RL). Through these phases, we systematically align the model with our defined objectives. Model safety is ensured through exhaustive data mining techniques and specialized harml', 'summary': '<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è LLM –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ù–µ–¥–∞–≤–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –û–¥–Ω–∞–∫–æ, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–µ–∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ –≤–ª–∏—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.</p>\n<p>–î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã. –ó–∞—Ç–µ–º, –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è –≤—ã–±–æ—Ä–æ—á–Ω–æ–º—É —É–º–µ–Ω—å—à–µ–Ω–∏—é, —á—Ç–æ–±—ã —á–∞—Å—Ç–æ—Ç–∞ –∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —É—á–∏—Ç—ã–≤–∞–ª–∏—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –Ω–∞–ø—Ä—è–º—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –¥–≤—É—Ö —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –¥–æ —á–µ—Ç—ã—Ä–µ—Ö —ç–ø–æ—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:\n* <strong>–ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –º–µ—Ç–æ–¥–æ–º Xavier, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 8192 —Ç–æ–∫–µ–Ω–∞, –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–º–µ–Ω—è–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞. –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–æ—Ç–µ—Ä–µ–π –æ–±—É—á–µ–Ω–∏—è –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞, –∑–∞—Ç–µ–º –∏–¥–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é, –∞ –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è.\n* <strong>–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –¥–ª–∏–Ω–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 4 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç—Ä–µ—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤–µ—Å–æ–≤. –¢–∞–∫–∂–µ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∞ NIAH (–∑–∞–¥–∞—á–∞ –ø–æ–∏—Å–∫–∞ –∏–≥–ª—ã –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞) –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è.\n* <strong>–ü–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞:</strong> –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –º–æ–¥–µ–ª–∏, –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –Ω–∞–±–æ—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∞—è –æ—Ç–≤–µ—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å). –û–±—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (SFT), –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∏ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.</p>'}, {'title': 'Enhancing long-context processing in reward models', 'content': 'ess reward model. We introduce novel multi-stage training methodology that significantly enhances the models capacity to process extended contexts while maintaining optimal MiniMax-01: Scaling Foundation Models with Lightning Attention performance on shorter sequences. This approach results in robust system capable of handling complex, real-world scenarios. Extensive evaluations conducted across both academic and in-house benchmarks demonstrate that our model achieves top performance across all tasks, while establishing new standards of extremely long-context processing. 5.1. Prompt Collection Our extensive prompt collection encompasses millions of diverse, high-quality queries from various sources. We develop tagging system that categorizes each prompt based on task type, knowledge domain, and difficulty level. The collection process incorporates sophisticated filtering mechanisms to eliminate redundant prompts while maintaining an optimal difficulty distribution. The prompt set spans various domains including long-context, programming, math, logical reasoning, creative writing, function calling, general-knowledge, and safety-related scenarios. 5.2. Reward Model Our reward model framework evaluates responses across four critical dimensions to ensure alignment with our core principles: Correctness. We implement rigorous evaluation system for responses that can be strictly validated. For mathematical and reasoning tasks, we utilize early-version MiniMax-Text-01 to generate binary reward signals based on answer consistency. Programming solutions undergo comprehensive testing in secured sandbox environment, with performance metrics derived from test case success rates. Truthfulness. We employ verification pipeline to assess the factual accuracy of the response. The process involves systematic response sampling, statement decomposition and clustering, crowd-sourced verification, and automated comparison using advanced language models to generate truthfulness scores. Helpfulness. Our evaluation framework assesses compliance with user instructions through both deterministic and probabilistic approaches. We implement automated rule-based constraint verification systems complemented by human evaluation of key metrics including coherence, depth, contextual relevance, and stylistic appropriateness. The final helpfulness score combines multiple evaluation signals through weighted scoring system. Harmlessness. Building upon Constitutional AI principles (Bai et al., 2022b), we develop evaluation criteria encompassing safety protocols, content appropriateness, and legal compliance. Our assessment system leverages carefully calibrated prompts validated against human annotations, with early-version MiniMax-Text-01 providing standardized safety evaluations. 5.3. Supervised Fine-Tuning Our SFT dataset construction involves multi-stage process utilizing domain-specific expert models trained through iterative SFT and RL cycles. We implement rejection sampling (Bai et al., 2022a; Dubey et al., 2024) to generate high-quality responses by the experts, sampling multiple variations per prompt across different temperature settings to select optimal demonstrations measured by the reward hierarchy. The response selection process further incorporates both n-gram and semantic similarity filters to ensure maximum diversity and quality in the training data. 26 MiniMax-01: Scaling Foundation Models with Lightning Attention 5.4. Reinforcement Learning 5.4.1. Offline Reinforcement Learning We incorporate the offline RL phase, i.e., Direct Preference Optimization (DPO) (Rafailov et al., 2023), to optimize the models performance across diverse prompt distributions, owing to its simplicity and ease of data construction for long-context scenarios. We specifically focus on prompts that maintain distributional consistency with those utilized in the SFT stage. To evaluate the impact of prompt selection, we conduct comparative experiments using two prompt categories: SFT-trained prompts and SFT-untrained but homologous prompts. Empirical results demonstrate negligible performance variations between SFT-trained prompts and their untrained counterparts. Thus, we adopt the SFTtrained ones for the offline RL phase. The experimental protocol involves generating responses with varying temperature parameters for each prompt, followed by systematic evaluation using the reward models described in Section 5.2. We then identify the best and the worst responses to construct preference pairs for DPO training. 5.4.2. Online Reinforcement Learning Online learning demonstrates superior sample efficiency and cross-domain generalization capabilities compared to offline learning methodologies. Therefore, we implement online RL to improve model performance, particularly in mathematical reasoning tasks. Our approach emphasizes prompt diversity and prioritizes prompts with moderate success rates to maximize information gain during policy updates. Notably, we employ SFT-untrained prompts during online RL, as our empirical observations indicate that reusing prompts from previous phases resulted in model saturation, characterized by diminished response perplexity. We propose modified Group Relative Policy Optimization (GRPO) (Shao et al., 2024) approach incorporating the following key innovations: Importance Sampling Weight Clipping. The conventional PPO/GRPO implementation employs one-sided clipping (Schulman et al., 2017; Shao et al., 2024), sometimes leading to gradient instability when processing tokens with large policy ratio and negative advantage. To address this issue, we implement additional clipping that abandoned this case in the loss function, which effectively regulates the importance sampling magnitude and mitigates noise propagation. KL Divergence Optimization. Due to the similar gradient instability issue, we reformulate the KL divergence term through theoretical analysis of the variance-bias trade-off to further stabilize gradient behavior, resulting in ùîªùêæùêø(ùúÉ) = ùîºùë° [SG(ùúãùúÉ(ùëéùë° ùë†ùë°) ùúãref(ùëéùë° ùë†ùë°)) log ùúãùúÉ(ùëéùë° ùë†ùë°)], where SG() denotes the stop-gradient operator. This formulation maintains policy consistency while reducing gradient variance. Balanced Advantage Estimation. We also ensure equitable reward contributions between positive and negative examples, which proves particularly effective in scenarios with skewed distributions. This approach maintains stable training dynamics by regulating the absolute magnitude of rewards across different example groups. 5.5. Safety Alignment The safety alignment of our model is meticulously addressed throughout both the SFT and RL stages. To strike an optimal balance between the models harmlessness and helpfulness, we employ an approach that encompasses the following key components. MiniMax-01: Scaling Foundation Models with Lightning Attention 5.5.1. Training Data Construction We construct high-quality alignment training data with focus on ensuring data diversity and accuracy. This involves the implementation of several data collection methodologies designed to cover broad spectrum of safety scenarios: Safety-Category Specific Prompts. Leveraging established safety classification standards and insights from safety and domain experts, we generate tailored prompts for specific safety categories. This ensures that the model is exposed to comprehensive set of safety-related scenarios. Real-World User Data Collection. We collect real-world user questions from various web documents to incorporate authentic and diverse safety-related queries into our training ', 'summary': '<h2>–ü–æ–¥—Ä–æ–±–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –æ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ MiniMax-01</h2>\n<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ MiniMax-01, –≤–∫–ª—é—á–∞—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ—â–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω–æ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ, —Ç–∞–∫ –∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≥–¥–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.</p>\n<p><strong>5.1. –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫ (–ø—Ä–æ–º–ø—Ç–æ–≤)</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ —Å–æ–±—Ä–∞–Ω–∞ –æ–±—à–∏—Ä–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏–∑ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ö–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç –±—ã–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏, –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –∏ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.  –ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –≤–∫–ª—é—á–∞–ª –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–≤–æ—Ä—á–µ—Å–∫–æ–µ –ø–∏—Å—å–º–æ, –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π, –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.</p>\n<p><strong>5.2. –ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (Reward Model)</strong></p>\n<p>–ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –ø–æ —á–µ—Ç—ã—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏:</p>\n<ul>\n<li><strong>–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å:</strong> –î–ª—è –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏. –í –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–¥–µ–ª—å MiniMax-Text-01 –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–∞. –†–µ—à–µ–Ω–∏—è –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –ø—Ä–æ—Ö–æ–¥—è—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ, –≥–¥–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤.</li>\n<li><strong>–ü—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∫–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞. –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É –æ—Ç–≤–µ—Ç–æ–≤, –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é, –ø—Ä–æ–≤–µ—Ä–∫—É —Å –ø–æ–º–æ—â—å—é –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏.</li>\n<li><strong>–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–º–æ—â—å—é –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª –¥–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–æ–π –ª—é–¥—å–º–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å–≤—è–∑–Ω–æ—Å—Ç–∏, –≥–ª—É–±–∏–Ω—ã, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —É–º–µ—Å—Ç–Ω–æ—Å—Ç–∏. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.</li>\n<li><strong>–ë–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö Constitutional AI. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—Ç–∫–∏ –ª—é–¥—å–º–∏, –∞ MiniMax-Text-01 –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</li>\n</ul>\n<p><strong>5.3. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-Tuning, SFT)</strong></p>\n<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö SFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —á–µ—Ä–µ–∑ SFT –∏ RL.  –ú–µ—Ç–æ–¥ –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, —Å –æ—Ç–±–æ—Ä–æ–º –ª—É—á—à–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –æ—Ç–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã n-–≥—Ä–∞–º–º –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>5.4. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning, RL)</strong></p>\n<ul>\n<li><strong>5.4.1. –û—Ñ–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong>  –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–µ—Ç–æ–¥ Direct Preference Optimization (DPO) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –ø—Ä–æ–º–ø—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–º—É –Ω–∞ —ç—Ç–∞–ø–µ SFT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–ø—Ç–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ SFT, –∏ –∏—Ö –Ω–µ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –î–ª—è DPO –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –æ—Ü–µ–Ω–æ–∫ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO.</li>\n<li><strong>5.4.2. –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong>  –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–Ω–ª–∞–π–Ω RL —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø—Ä–æ–º–ø—Ç–æ–≤ —Å–æ —Å—Ä–µ–¥–Ω–∏–º —É—Ä–æ–≤–Ω–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏. –í–æ –≤—Ä–µ–º—è –æ–Ω–ª–∞–π–Ω RL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–æ–º–ø—Ç—ã, –Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–≤—à–∏–µ –≤ SFT, —Ç–∞–∫ –∫–∞–∫ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –Ω–∞—Å—ã—â–µ–Ω–∏—é –º–æ–¥–µ–ª–∏. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Group Relative Policy Optimization (GRPO) —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º–∏:<ul>\n<li><strong>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏:</strong> –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å –±–æ–ª—å—à–∏–º –æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –ø–æ–ª–∏—Ç–∏–∫ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º, –≤–≤–æ–¥–∏—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–∫–ª—é—á–∞–µ—Ç —Ç–∞–∫–∏–µ —Å–ª—É—á–∞–∏ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.</li>\n<li><strong>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ KL:</strong> –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç—Å—è —á–ª–µ–Ω –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ KL, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.</li>\n<li><strong>–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong> –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤–∫–ª–∞–¥ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö —Å–æ —Å–∫–æ—à–µ–Ω–Ω—ã–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5.5. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏</strong></p>\n<p>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö SFT –∏ RL. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –±–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:</p>\n<ul>\n<li><strong>5.5.1. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong>  –§–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ö–≤–∞—Ç–∞ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:<ul>\n<li><strong>–ü—Ä–æ–º–ø—Ç—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –º–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</li>\n<li><strong>–°–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong>  –°–æ–±–∏—Ä–∞—é—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.</li>\n</ul>\n</li>\n</ul>'}, {'title': 'Enhancing model robustness through prompt augmentation and long-context training', 'content': 'data. Prompt Augmentation. We instruct early-version MiniMax-Text-01 to generate additional related prompts based on the collected typical red team attack prompts. This approach aims to expand the diversity of safety scenarios and enhance the robustness of the models safety mechanisms. 5.5.2. Response Generation with Harmless Reward Model To generate safe and appropriate responses, we employ harmless reward model (Bai et al., 2022b) that is developed based on set of detailed safety rules. To prevent the model from producing unreasonable refusals, we carefully integrate principles of helpfulness into the safety rules. This integration plays crucial role in achieving balanced output capability, enabling the model to provide safer responses without compromising its utility to the user. The resulting safety-aligned system demonstrates robust protection against potential misuse while maintaining high performance across intended use cases. 5.6. Training Methodology with Long-Context Adaptation We propose systematic multi-stage training methodology to enhance the models capacity for processing extended contexts, as shown in Tab. 7. This approach is methodically designed to optimize long-sequence handling while maintaining performance efficacy on conventional shorter sequences. The RoPE base frequency is maintained at 10 million throughout the post-training phase to ensure consistency in positional encoding. Stage I: Initial Short-Context Training. The first stage implements SFT with sequences constrained to 8,192 tokens. This foundational phase establishes baseline competency in processing standardlength queries and responses, which constitute the majority of practical applications. We remove the long-context prompts that are longer than 8,192 tokens in this stage. Stage II: Extended Context Training. The second stage implements significant extension of the sequence length to 1,032,192 tokens. This phase incorporates training samples across diverse sequence lengths with 50% long-context prompts, facilitating comprehensive model adaptation to extensive contextual processing. The strategic expansion of the sequence length is fundamental to achieving robust long-context capabilities. Stage III: Short-Context Preference Optimization. In this phase, we revert to 8,192 tokens for sequence length and implement Direct Preference Optimization (DPO). This calibration ensures optimal performance on conventional context sizes while maintaining the previously acquired capabilities. Stage IV: Long-Context Preference Optimization. The fourth stage focuses on reinforcing longcontext processing capabilities through DPO with sequences of 1,032,192 tokens. This phase employs 28 MiniMax-01: Scaling Foundation Models with Lightning Attention training protocols analogous to Stage III with entirely long-context data, adapted for extended sequence lengths. Stage V: Online Reinforcement Learning. The final stage implements short-context Online Reinforcement Learning with sequence length of 8,192 tokens. More details have been outlined in Section 5.4.2. Table 7 Training Recipe for Post-training Alignment. Stage Stage II Stag III Stage IV Stage Sequence Length Epoch Batch Size Max LR Min LR LR Decay 8192 2 128 1e-5 1e-6 Cosine 1032192 2 80 3e-6 3e-6 Constant 8192 1 64 5e-7 5e-8 Cosine 1032192 1 64 5e-7 5e-7 Constant 8192 1 512 1e-6 1e-7 Cosine 5.7. Academic Benchmarks We observe and report open-source shortand long-context benchmarks that highlight our models capabilities across various aspects. Along with the user-oriented evaluations we will discuss in Section 5.8, we show that MiniMax-Text-01 is leading open-source model that achieves top performance in long-context retrieval, understanding, long in-context learning and knowledge-based requests, while performing well in math, reasoning, and code tasks and demonstrating strong usefulness in real-user assistant scenarios. 5.7.1. Core Benchmarks MMLU (Hendrycks et al., 2021a) and MMLU-Pro (Wang et al., 2024b) are widely adopted datasets that assess the extent of models knowledge across broad range of domains. We further observe SimpleQA (Wei et al., 2024), factuality benchmark that challenges the models knowledge boundary, and C-SimpleQA (He et al., 2024b) which is an adapted version of SimpleQA under the Chinese culture. For the observation of reasoning capabilities, we evaluate on GPQA (Rein et al., 2024) for graduate-level knowledge reasoning, and DROP (Dua et al., 2019) for reading comprehension reasoning. We test our models performance on math problem-solving with grade-school-level task GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) that spans from AMC-8 to AIMElevel across 7 subjects. We monitor our models coding capability by observing the Pass@1 rate on HumanEval (Chen et al., 2021) and MBPP Plus (Austin et al., 2021; Liu et al., 2023) datasets. To test the models ability to interpret and execute detailed and nuanced instructions, we evaluate the IFEval (Zhou et al., 2023) benchmark. Furthermore, we observe Arena-Hard-Auto (Li et al., 2024b) that reflects the alignment to human preferences. We adopt greedy decoding and zero-shot chain-of-thought strategy (Wei et al., 2022) in evaluating our instruction-tuned model. We compare with other leading and open-source LLMs, which we evaluate under the same setting, if not reported. We present the performance of MiniMax-Text-01 in Table 8. As shown, MiniMax-Text-01 exhibits remarkable performance across most dimensions. It surpasses all models on C-SimpleQA with its more extensive knowledge boundary under Chinese culture. MiniMax-Text-01 also achieves top-3 performance across MMLU, IFEval, and Arena-Hard, showing its exceptional capability of applying its comprehensive knowledge within given constraints to well satisfy user queries and align with human preferences. Meanwhile, it achieves better MATH pass@1 rate than GPT-4o, Claude-3.5-Sonnet, and Llama-3.1-405B, and exhibits comparable 29 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 8 Performance of MiniMax-Text-01 on core academic benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2.572B-Inst. DeepSeekV3 Llama-3.1405B-Inst. MiniMaxText-01 MMLU MMLU-Pro SimpleQA C-SimpleQA IFEval (avg) Arena-Hard 85.7 74.4 39. 64.6 84.1 92.4 GPQA (diamond) DROP (F1) 46.0 89. GSM8k MATH MBPP + HumanEval 95.6 76.6 76. 90.2 88.3 78.0 28.1 56.8 90. 87.6 65.0 88.8 96.9 74.1 75. 93.7 Evaluated following 0-shot CoT setting. General 86.5 76.4 26. 63.3 88.4 72.7 Reasoning 62.1 89. 86.8 75.8 23.4 59.4 89.4 85. 59.1 89.2 Mathematics 95.2 84.6 75. 86.6 Coding 95.4 83.9 75.9 89. 86.1 71.1 10.3 52.2 87.2 81. 49.0 85.0 95.8 81.8 77.0 86. 88.5 75.9 24.9 64.8 87.3 91. 59.1 91.0 96.7 84.6 78.8 92. 88.6 73.3 23.2 54.7 86.4 63. 50.7 92.5 96.7 73.8 73.0 89. 88.5 75.7 23.7 67.4 89.1 89. 54.4 87.8 94.8 77.4 71.7 86. performance with instructed Qwen2.5-72B on HumanEval. Moreover, MiniMax-Text-01 achieves 54.4 on GPQA Diamond, which exceeds most open-source instruction-tuned LLMs and the latest version of GPT-4o. 5.7.2. Long Benchmarks As previously discussed in the long-context extension part of section 4.2, the NIAH task is kind of simplistic for our model, rendering it insufficient for observing the models optimization progress. Consequently, we shift our evaluation to more challenging tasks. Our current long-context evaluation framework focuses on three primary dimensions: (1) Long-Context Retrieval, (2) Long-Context Understanding, and (3) Long In-Context Learnin', 'summary': '<h2>–£–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ MiniMax-Text-01</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ MiniMax-Text-01, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.</p>\n<p><strong>5.5.1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏</strong></p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ –µ—ë —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –∞—Ç–∞–∫–∞–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞—Ç–∞–∫ (red team attack prompts), –º–æ–¥–µ–ª—å MiniMax-Text-01 —Ä–∞–Ω–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é, –∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º–∏.</p>\n<p><strong>5.5.2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å</strong></p>\n<p>–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (harmless reward model), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–∞ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–∫–∞–∑–æ–≤ –º–æ–¥–µ–ª–∏, –≤ –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, —Å–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —Ü–µ–ª–µ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.</p>\n<p><strong>5.6. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –∫ –¥–ª–∏–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É</strong></p>\n<p>–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã. –≠—Ç–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE (–º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏.</p>\n<ul>\n<li>\n<p><strong>–≠—Ç–∞–ø I: –ù–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ SFT (Supervised Fine-Tuning). –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –î–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ 8192 —Ç–æ–∫–µ–Ω–∞, –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è.</p>\n</li>\n<li>\n<p><strong>–≠—Ç–∞–ø II: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 1 032 192 —Ç–æ–∫–µ–Ω–æ–≤. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã, –≤–∫–ª—é—á–∞—è 50% –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.</p>\n</li>\n<li>\n<p><strong>–≠—Ç–∞–ø III: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫ 8192 —Ç–æ–∫–µ–Ω–∞–º, –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ DPO (Direct Preference Optimization). –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–Ω–µ–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏.</p>\n</li>\n<li>\n<p><strong>–≠—Ç–∞–ø IV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –£—Å–∏–ª–∏–≤–∞–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é DPO –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 1 032 192 —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —ç—Ç–∞–ø –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω —ç—Ç–∞–ø—É III, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>\n</li>\n<li>\n<p><strong>–≠—Ç–∞–ø V: –û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.</strong> –ó–∞–≤–µ—Ä—à–∞—é—â–∏–π —ç—Ç–∞–ø - –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (8192 —Ç–æ–∫–µ–Ω–∞).</p>\n</li>\n</ul>\n<p><strong>5.7. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö</strong></p>\n<p>–ú–æ–¥–µ–ª—å MiniMax-Text-01 –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ, —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —è–≤–ª—è–µ—Ç—Å—è –ª–∏–¥–µ—Ä–æ–º —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–Ω–∏–º–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–Ω–∞–Ω–∏—è—Ö. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.</p>\n<p><strong>5.7.1. –û—Å–Ω–æ–≤–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏</strong></p>\n<ul>\n<li><strong>MMLU –∏ MMLU-Pro:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</li>\n<li><strong>SimpleQA –∏ C-SimpleQA:</strong> –ü—Ä–æ–≤–µ—Ä—è—é—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–∏—Ç–∞–π—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã.</li>\n<li><strong>GPQA –∏ DROP:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—é –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ.</li>\n<li><strong>GSM8k –∏ MATH:</strong> –ü—Ä–æ–≤–µ—Ä—è—é—Ç –Ω–∞–≤—ã–∫–∏ —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.</li>\n<li><strong>HumanEval –∏ MBPP Plus:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.</li>\n<li><strong>IFEval:</strong> –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.</li>\n<li><strong>Arena-Hard-Auto:</strong> –û—Ç—Ä–∞–∂–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.</li>\n</ul>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MiniMax-Text-01 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–æ –º–Ω–æ–≥–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –≤–∫–ª—é—á–∞—è C-SimpleQA, MMLU, IFEval –∏ Arena-Hard. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>\n<p><strong>5.7.2. –ë–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></p>\n<p>–û—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±—ã–ª–∞ –ø–µ—Ä–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –ø–æ—Å–∫–æ–ª—å–∫—É NIAH (—Ä–∞–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–π—Å—è —Ç–µ—Å—Ç) –æ–∫–∞–∑–∞–ª—Å—è —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç—ã–º. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ —Ç—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º:</p>\n<ol>\n<li><strong>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></li>\n<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></li>\n<li><strong>–û–±—É—á–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</strong></li>\n</ol>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ MiniMax-Text-01, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ –µ–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã.</p>'}, {'title': 'Long-context retrieval', 'content': 'This dimension assesses the models memory capabilities, which serve as the foundation for almost all long-context tasks. In addition to vanilla k-M NIAH (Kamradt, 2023), we construct more challenging variation to assess our Long-Context Retrieval performance, namely Multi-Round NeedlesIn-A-Haystack (MR-NIAH), serving as crucial back up for retrieval tasks in long multi-turn dialogue contexts, revealing the fundamental capabilities for building lifelong companion AI assistants. Similar to Multi-round co-reference resolution (MRCR) (Vodrahalli et al., 2024) which is not open-source, we construct haystacks of MR-NIAH as history dialogues, where user queries are synthetic but explicit requests of event descriptions and creative writing. In the last round, the query requests the model to repeat the response of one of the history requests. The haystacks span from 2K to 1M tokens (up to around 2000 interactions), and each needle request is injected at 25%, 50%, and 75% of the 30 MiniMax-01: Scaling Foundation Models with Lightning Attention conversation, respectively. Each ground truth response contains three core components, and we look at an adjusted recall corr. comp. . We show case illustration in Appendix B.2. Figure 15 illustrates comparison results of MR-NIAH. Our model (MiniMax-Text-01, red line) shows strong performance across wide range of sequence lengths in both English and Chinese evaluations. Compared to competing baselines (e.g., GPT, Claude, and Gemini variants), our model also shows less performance degradation at large input lengths, underscoring its robustness for long-context retrieval tasks. 5.7.2.2 Long-Context Understanding This dimension measures the models longcontext understanding ability which contains logical reasoning skills based on long-context inputs. We utilize two comprehensive longcontext QA datasets, Ruler (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2024) to evaluate this aspect. Ruler includes 13 different tasks and notably introduces multi-hop tracing and aggregation tasks to evaluate the complex reasoning abilities of models. We test Ruler up to sequence length of 1M tokens. LongBench-V2 encompasses question-answering tasks of varying difficulty levels across multiple context types, including single and multi-document, multi-turn dialogue, code repositories, and long structured data, among others. Following LongBenchV2 (Bai et al., 2024), we consider two test modes: w/o CoT and w/ CoT, and the text lengths are categorized as follows: Short, ranging from 0 to 32K words; Medium, spanning from 32K to 128K words; and Long, covering 128K to 2M words. Figure 15 MR-NIAH in English and Chinese. As Table 9 illustrates, our model exhibits notable strengths in processing Rulers long-context reasoning tasks. While performance at the 64k input level remains competitive with leading models (including GPT-4o and Claude-3.5-Sonnet) with minimal variation, MiniMax-Text-01 establishes distinct advantage beginning at 128k, achieving impressive scores and surpassing all benchmark models. This superiority becomes particularly pronounced in ultra-long-context scenarios (such as 1M), where MiniMax-Text-01 maintains its commanding lead. Moreover, as evident in Table 103, MiniMax-Text-01 exhibits outstanding capabilities in LongBench-V2s long-context reasoning tasks. The model achieves state-of-the-art results among all evaluated systems in the w/ CoT setting, while also displaying remarkable effectiveness in scenarios w/o CoT. Overall, MiniMax-Text-01 demonstrates exceptional capability in long-context understanding especially reasoning tasks, both with and without CoT reasoning, particularly excelling in scenarios requiring complex reasoning. The exceptional robustness and stability of the model in processing long-context understanding tasks can be attributed to the hybrid architecture with half RoPE and carefully tuned training recipes for both pre-training and alignment, which enhance the models ability to handle long sequences effectively. 3We present the other models performance reported at https://longbench2.github.io/ 31 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 9 Performance comparison of MiniMax-Text-01 on Ruler. Model 4k 8k 16k 32k 64k 128k 256k 512k 1M GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) MiniMax-Text-01 0.970 0.965 0.962 0.960 0.963 0.921 0.960 0.960 0.960 0.961 0.890 0.957 0.960 0.951 0. 0.888 0.950 0.958 0.957 0.954 0.884 0.952 0.938 0.937 0.943 - 0.938 0.917 0.860 0.947 - - 0.916 0.797 0.945 - - 0.861 0.709 0.928 - - 0.850 - 0. Table 10 Performance comparison of MiniMax-Text-01 on LongBench v2. Model Human overall easy hard short medium long 53.7 100.0 25.1 47.2 59. 53.7 w/ CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 51.4 46.7 - 43.5 56.5 54.2 55.2 - 47.9 66.1 w/o CoT GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Deepseek-V3 Qwen2.5-72B-Inst. MiniMax-Text-01 50.1 41.0 48.7 42.1 52.9 57.4 46.9 - 42.7 60.9 49.7 41.5 - 40.8 50.5 45.6 37.3 - 41.8 47.9 59.6 53.9 - 48.9 61. 53.3 46.1 - 45.6 58.9 48.6 41.9 - 40.9 56.7 52.4 38.6 - 38.1 52.6 43.5 44.4 - 39.8 47.2 40.2 37.0 - 44.4 43.', 'summary': '<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∞ –∏–º–µ–Ω–Ω–æ, –µ—ë –ø–∞–º—è—Ç—å –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é.</p>\n<p><strong>–ü–∞–º—è—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤. –ü–æ–º–∏–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ "–ò–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞" (NIAH), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî "–ú–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–∞—è –∏–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞" (MR-NIAH). MR-NIAH –∏–º–∏—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏, –≥–¥–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –∑–∞–ø—Ä–æ—Å—ã, –∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Ä–∞—É–Ω–¥–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –æ–¥–∏–Ω –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –°—Ç–æ–≥–∞ —Å–µ–Ω–∞ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –ø–æ –¥–ª–∏–Ω–µ –æ—Ç 2 —Ç—ã—Å—è—á –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–¥–æ 2000 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π), –∞ "–∏–≥–æ–ª–∫–∏" (–∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ) –≤—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ 25%, 50% –∏ 75% –¥–ª–∏–Ω—ã –¥–∏–∞–ª–æ–≥–∞. –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã MR-NIAH –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å MiniMax-Text-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∫–∞–∫ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ (GPT, Claude, Gemini), MiniMax-Text-01 –º–µ–Ω—å—à–µ —Ç–µ—Ä—è–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –≤–≤–æ–¥–∞, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ—ë –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>\n<p><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–µ–ª–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: Ruler –∏ LongBench-V2.</p>\n<ul>\n<li><strong>Ruler</strong> –≤–∫–ª—é—á–∞–µ—Ç 13 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, –≤ —Ç–æ–º —á–∏—Å–ª–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤.</li>\n<li><strong>LongBench-V2</strong> —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –¥–∏–∞–ª–æ–≥–∏, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫–æ–¥–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ –¥–≤—É—Ö —Ä–µ–∂–∏–º–∞—Ö: —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ –±–µ–∑ –Ω–µ—ë. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –∫–æ—Ä–æ—Ç–∫–∏–π (–¥–æ 32 —Ç—ã—Å—è—á —Å–ª–æ–≤), —Å—Ä–µ–¥–Ω–∏–π (32-128 —Ç—ã—Å—è—á —Å–ª–æ–≤) –∏ –¥–ª–∏–Ω–Ω—ã–π (128 —Ç—ã—Å—è—á - 2 –º–∏–ª–ª–∏–æ–Ω–∞ —Å–ª–æ–≤).</li>\n</ul>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MiniMax-Text-01 –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ –Ω–∞–±–æ—Ä–∞ Ruler. –ù–∞ –¥–ª–∏–Ω–µ –≤–≤–æ–¥–∞ 64 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ª–∏–¥–µ—Ä–∞–º–∏ (GPT-4o, Claude-3.5-Sonnet), –Ω–æ –Ω–∞—á–∏–Ω–∞—è —Å–æ 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤ MiniMax-Text-01 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö —Å —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤).</p>\n<p>–í LongBench-V2 –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∂–∏–º–µ —Å CoT –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –±–µ–∑ CoT.</p>\n<p>–í —Ü–µ–ª–æ–º, MiniMax-Text-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RoPE –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è.</p>'}, {'title': 'Evaluating long in-context learning in lifelong models', 'content': 'This dimension evaluates the models ability to learn from context, core area of research in lifelong learning. We benchmark our Long In-Context Learning capability with the MTOB (Machine Translation from One Book) (Tanzer et al., 2024) dataset. The task requires model to translate between English and Kalamang, language that is very limited in open data and thus within the training corpus, and the LLM is expected to learn the language only from parts of grammar book and 375 translation examples, all given in the context for each translation query (Appendix B.1). The context length is 81K tokens under half-book setting and 133K tokens under total-book setting. We present our results in Table 11. Figure 16 Changes of eng kalam (ChrF) during the whole long-context extension training process. We carefully examined the pre-training data and found that only very small amount of data contains Kalamang-related content. As result, the eng kalam (ChrF) score of our model is the lowest in the no-context scenario, while other models we compared with likely have had their pre-train or post-train data enhanced with relevant Kalamang data. As well as the delta half and full book metrics, our model surpasses all models in terms of the eng kalam (ChrF) metric. And our model also has comparable performance with other models on kalam eng (BLEURT) metric. 32 MiniMax-01: Scaling Foundation Models with Lightning Attention In the course of long-context extension, as described in section 4.2, we observed gradual enhancement in In-Context Learning ability, as indicated by MTOB, illustrated in Figure 16. While we have explored some remarkable works(Agarwal et al., 2024; Dong et al., 2024) specifically aimed at improving In-Context Learning capabilities, we believe that such ability should merely be one aspect of the reasoning capabilities of long-context models. Therefore, we plan to conduct in-depth research on long-context data quality and scale from more fundamental perspective to further enhance the long-context reasoning capabilities of our model. Table 11 Performance comparison of MiniMax-Text-01 on MTOB. Context Type no context half book full book Œî half book Œî full book GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 eng kalam (ChrF) 54.30 9.90 20.22 53.62 16.79 53.68 12.20 49.50 16.55 48.48 6.0 51.74 kalam eng (BLEURT) - 55.65 57.90 53.30 45.94 51.60 GPT-4o (11-20) Claude-3.5-Sonnet (10-22) Gemini-1.5-Pro (002) Gemini-2.0-Flash (exp) Qwen-Long MiniMax-Text-01 33.20 31.42 32.02 33.80 30.13 33.65 58.30 59.70 61.52 57.50 53.14 57.10 - 62.30 63.09 57.00 32.15 58. 44.40 33.39 36.89 37.30 31.92 45.7 25.10 28.28 29.50 23.70 23.01 23.45 - 35.42 41.11 41.10 29.39 45.6 - 30.88 31.07 23.20 2.02 24.35 5.8. User-in-the-loop While achieving top performance on the core open-source benchmarks, we realize that academic evaluations lack an understanding of real-world user interactions. Hence, we also focus on monitoring and improving user experience through our Hailuo AI 4 by incorporating user-in-the-loop evaluations based on real-world cases and adapting tools for better usability and performance in practical applications. 5.8.1. In-House Evaluations We maintain series of in-house evaluations that include: (1) automatic assessments of General Assistant capabilities, Knowledge Q&A, Creative Writing, Hard Capability, Instruction Following, Coding, Safety, and Long Context, and (2) expert human evaluations. Its worth noting that since our test queries are primarily derived from Hailuo AI user interactions, significant portion of our in-house samples are in Mandarin and deeply rooted in Chinese cultural contexts. Our results indicate notable discrepancy between performance on academic benchmarks and actual user experience, where leading open-source and commercial models can underperform when used as interactive assistants. We show in Table 12 5 that, through our dedicated efforts, MiniMaxText-01 is able to handle these situations quite well. In general, our model outperforms other models 4https://www.hailuo.ai/ 5We omit scores for in-applicable models. 33 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 12 Performance comparison of MiniMax-Text-01 on in-house benchmarks. General Assistant Hard Capability Creative Writing Knowledge Q&A Instruction Following Coding Safety Long Context GPT-4o (11-20) GPT-4o (08-06) GPT-4o (05-13) Claude-3.5-Sonnet (10-22) Claude-3.5-Sonnet (06-20) Gemini-2.0-Flash (exp) Qwen2.5-72B-Inst. DeepSeek-V3 Llama-3.1-405B-Inst. MiniMax-Text70.9 63.5 67.7 66.8 60.5 70. 66.4 66.8 53.3 73.9 73.5 62. 63.3 68.3 67.4 61.8 66.1 68. - 64.8 70.3 66 58.3 54. 51.0 70.0 61.7 64.6 63.6 81. 69.2 68.0 69.6 52.0 51.8 75. 68.9 77.0 46.0 78.6 50.4 49. 49.6 61.5 64.4 39.9 34.1 51. 50.3 46.3 94.0 93.6 93.2 94. 93.6 86.5 93.9 94.0 87.6 90. 85.4 79.7 79.7 92.9 95.0 66. - 74.9 70.7 90.9 86.2 58. 77.2 47.1 47.1 81.9 81.5 77. 60.3 93.8 in common Assistant scenarios, particularly when compared to open-source counterparts. This superiority is most evident in our Creative Writing (Appendix B.5, B.7, B.6) and Knowledge Q&A collections, where it aligns more closely with user intentions than other models, delivering accurate and detailed responses to wide range of queries. In productivity scenarios that require Long Context (Appendix B.3), such as document translation, summarization, and analysis, our model demonstrates high proficiency and reliability. Moreover, we prioritize the safety of our model, as it achieves top-tier performance on our established in-house Safety benchmarks. Meanwhile, we are agile in gathering and updating complex productivity scenarios with multilevel instruction following requests at which our model fails and current LLMs cannot master, constructing our Harder Capability and Instruction Following in-house evaluations. While leading LLMs tend to underperform in these sets, these requests reflect our models limitations when given multi-level instructions, which stems primarily from insufficient training data for specific instruction types. Moving forward, we are committed to substantially expanding our training dataset with high-quality, targeted content to address these gaps and improve model capabilities. 5.8.2. Search in Hailuo AI During user interaction case studies, we find models capability to utilize search tools can compensate for the limited knowledge boundary by accessing real-time, extensive, and precise information from the web. To maximize the models benefits from search while minimizing additional performance degradation, we first carefully pre-define the scope of search scenarios, which cover approximately 30 40% of user queries, including but not limited to precision-demanding, domain-specific, and time-sensitive requests. Meanwhile, to ensure seamless conversation experience, we define the system as invoking tools directly through special tokens, which avoid the complexity of multi-step planning (Chen et al., 2024b) or chain-of-thought reasoning6 that might disrupt the natural flow of the interactions. We create SFT datasets comprising search and non-search decisions across diverse domains, while carefully controlling for other interaction features unrelated to search decisions, such as conversation length, to maintain uniform data distribution across each dimension and prevent overfitting. Importantly, we employ the corresponding reward model of each sample to ensure response quality, failing at which would introduce suboptimal samples into the training 6https://docs.anthropic.com/en/docs/build-with-claude/tool-use 34 MiniMax-01: Scaling Foundation Models with Lightning Attention data, potentially affecting the models fundamental capabilities. The search decision boundary was calibrated to align with the models knowledge boundaries, discarding samples that our model already masters from the search corpus, such as general Chinese knowledge Q&A. After careful assessments by human evaluation experts, we conclude that our models use of the search tool extensively improved user experience, landing at performance leap from 58% to 71.5% on our out-of-domain Hailuo AI end-to-end evaluation (Appendix B.9). Since we are unsure whether other LLM-based assistants include similar search tools, we refrain from making unfair performance comparisons. 6. Vision-language Model By integrating an image encoder and an image adapter into our MiniMax-Text-01 model, we develop MiniMax-VL-01, which extends the capabilities of the model to visual understanding tasks. To ensure robust visual understanding, we design proprietary dataset and implement multi-stage training strategy, where the newly introduced image encoder and adapter first undergo large-scale visual pre-training, followed by comprehensive fi', 'summary': '<h2>–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MTOB (Machine Translation from One Book), –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–≥–æ —É—á–µ–±–Ω–∏–∫–∞.</p>\n<p>–ó–∞–¥–∞—á–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –º–µ–∂–¥—É –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ —è–∑—ã–∫–æ–º –∫–∞–ª–∞–º–∞–Ω–≥, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –æ—á–µ–Ω—å –º–∞–ª–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ –º–æ–¥–µ–ª–∏. –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –æ—Å–≤–æ–∏—Ç —ç—Ç–æ—Ç —è–∑—ã–∫, –æ–ø–∏—Ä–∞—è—Å—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —á–∞—Å—Ç–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Å–æ–±–∏—è –∏ 375 –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø–µ—Ä–µ–≤–æ–¥–∞. –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 81 —Ç—ã—Å—è—á—É —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–∂–∏–º–µ "–ø–æ–ª–æ–≤–∏–Ω–∞ –∫–Ω–∏–≥–∏" –∏ 133 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–∂–∏–º–µ "–ø–æ–ª–Ω–∞—è –∫–Ω–∏–≥–∞".</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ 11. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 16 –ø–æ–∫–∞–∑–∞–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ ChrF (eng kalam) –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –¢—â–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —è–∑—ã–∫–æ–º –∫–∞–ª–∞–º–∞–Ω–≥, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ–Ω—å –º–∞–ª–æ–º –æ–±—ä–µ–º–µ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å ChrF (eng kalam) –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è —Å–∞–º—ã–º –Ω–∏–∑–∫–∏–º, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏–º–µ–ª–∏ –≤ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–ª–∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–∞–ª–∞–º–∞–Ω–≥–µ. </p>\n<p>–û–¥–Ω–∞–∫–æ, –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø–æ–ª–æ–≤–∏–Ω–∞ –∏ –ø–æ–ª–Ω–∞—è –∫–Ω–∏–≥–∞) –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ ChrF (eng kalam). –¢–∞–∫–∂–µ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ BLEURT (kalam eng).</p>\n<p>–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ 4.2, –Ω–∞–±–ª—é–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –¥–∞–Ω–Ω—ã–º–∏ MTOB –∏ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 16. –•–æ—Ç—è –±—ã–ª–∏ –∏–∑—É—á–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–∏—à—å –æ–¥–Ω–∏–º –∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü–æ—ç—Ç–æ–º—É –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –º–∞—Å—à—Ç–∞–±–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —á—Ç–æ–±—ã –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —É–ª—É—á—à–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong> <em>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è, –∫–∞–∫ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —Ä–µ–¥–∫–∏–π —è–∑—ã–∫, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –æ–±—É—á–µ–Ω–∏—é "–Ω–∞ –ª–µ—Ç—É" –∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</em></p>'}, {'title': 'Dataset and training overview', 'content': 'ne-tuning of the entire pipeline. In the following section, we begin with comprehensive description of the dataset used for training our image encoder and vision-language model. Subsequently, we provide an in-depth overview of the model architecture, followed by an exposition of our four-stage training regimen. We conclude the section by presenting our benchmark results. 6.1. Multimodal Data 6.1.1. Caption Data To pre-train the vision encoder, we curate substantial image-caption dataset by aggregating and filtering data from internet sources. Our Vision Transformer (ViT) is trained using 694 million unique image-caption pairs. To enhance data quality, we acquire refined captions for 180 million images within these pairs. During the training process, we employ an augmentation strategy by randomly sampling raw and refined captions with equal probability (ùëù = 0.5). 6.1.2. Description Data In existing vision-language models, the utility of descriptive imagery for model training has been well-documented (Li et al., 2024a, 2022, 2023; Schuhmann et al., 2021). To further explore this avenue, we have compiled dataset consisting of 100 million images sourced from open resources such as Common Crawl. Each image in this dataset is paired with fine-grained description, which is initially synthesized by caption model and subsequently refined through humans. On average, these descriptions comprise approximately 300 text tokens per image. Description data serves as robust resource for modal alignment and enhancing understanding in further training. 6.1.3. Instruction Data To train MiniMax-VL-01, we construct comprehensive and diverse instruction-based dataset by synthesizing an extensive range of question-answer (QA) pairs involving visual inputs. These QA pairs are meticulously designed to cover wide array of image-related tasks, such as text extraction, object localization, and geometry problem solving. The dataset generation process prioritizes both diversity and realism, ensuring that the instructions capture varying degrees of complexity and linguistic styles. During training, we apply an augmentation strategy by randomly sampling different types of QA prompts with balanced probabilities, thereby enabling the model to generalize effectively across 35 MiniMax-01: Scaling Foundation Models with Lightning Attention diverse instructional formats and interaction patterns. 6.1.4. Data Distribution To demonstrate the diversity of our VLM data, we uniformly sample 1 million imageinstruction pairs from the instruction data and use another VLM to assign concise tag (e.g., object localization) that represents the primary capability required for each pair. This analysis yielded around 50,000 unique tags, and the top 2,817 tags appeared more than 10 times. The distribution of these prominent tags is visualized in Figure 17, where we further group these top tags into 14 major categories. 6.2. Architecture 6.2.1. Overall Architecture Our MiniMax-VL-01 architecture adheres to the ViT-MLP-LLM paradigm, which has been widely embraced in numerous multimodal large language models (MLLMs). The architecture consists of three main components: Vision Transformer (ViT) with 303 million parameters for visual encoding, two-layer MLP projector initialized randomly for image adaptation, and the MiniMax-Text-01 model serving as the foundational large language model (LLM). Figure 17 Visualization of top tags of sampled instruction data. The category and percentage for each group of clustered tags are displayed in the inner layer, only top-10 tags of each group are displayed for clarity. We implement dynamic resolution strategy by resizing the input image according to predefined grid configuration list, ranging from 336336 to 20162016, while maintaining standard thumbnail at resolution of 336 336. The resized images are subsequently partitioned into non-overlapping patches, each measuring 336 336. Both the image patches and the thumbnail are independently encoded, and their encoded features are concatenated to construct comprehensive image feature representation. In contrast to traditional approaches that rely on pooling or other downsampling techniques to compress feature representations, our model leverages its powerful capacity for processing long sequences, allowing for the direct utilization of raw high-dimensional features during training. This strategy mitigates potential information loss and substantially improves the models adaptability to multi-scale inputs. Moreover, by projecting both image patches and thumbnails into unified feature space, our method significantly enhances the models robustness and representational expressiveness when handling diverse and complex visual inputs. 6.2.2. Vision Encoder We employ lightweight ViT-L/14 (Dosovitskiy et al., 2021) as the foundational structure for our vision encoder and train it from scratch. Following standard pipeline, the input image tensor is initially processed through convolutional layer to extract discrete patches, to which absolute 36 MiniMax-01: Scaling Foundation Models with Lightning Attention positional embeddings are subsequently appended. The resulting tensors are then passed through series of multi-head residual attention blocks. This architecture is particularly effective in capturing intricate visual details and the complex interrelationships within images. We utilize contrastive learning to enhance the alignment between corresponding image-caption pairs while diminishing the alignment between non-corresponding pairs. Specifically, we follow the approach introduced in CoCa (Yu et al., 2022), which augments image-text contrastive learning with an additional decoder and image-text cross-attention mechanisms. The network is jointly optimized using combination of contrastive loss and cross-entropy loss. Our ViT-L/14 model is initially trained at resolution of 224 224 for 37 billion image-caption pairs and subsequently fine-tuned at 336 336 for 1.2 billion pairs. For both resolutions, the captions are truncated to 76 tokens. Our ViT-L/14 encoder achieves zero-shot classification accuracy of 80.55% at 336 336 resolution on the ImageNet-1K dataset. 6.3. Training Recipes We employ four-stage training strategy to enable the model to progressively develop comprehensive multimodal understanding capabilities while retaining its language understanding skills. Additionally, the models question-answering and instruction-following abilities, as well as its alignment with human preferences, are methodically refined throughout these stages. Stage I: Modality alignment. In this stage, our primary objective is to achieve alignment between visual and text tokens by enabling the model to accurately generate appropriate captions for given images. To this end, we update the weights of both the image adapter and the vision encoder to optimize their performance in this multimodal task. During this phase, we utilize total of 80 billion tokens sampled from our image description dataset. Empirically, we have found that increasing the image resolution does not yield improvements in downstream task accuracy. Therefore, all images are processed at fixed resolution of 336 336 to reduce computational costs. Stage II: Enhancement of Vision Understanding. This stage can be regarded as standard instruction tuning phase, during which all model parameters are open to updates. The primary goal is to align the models output with human instructions and enhance its ability to perform diverse range of vision understanding tasks. To achieve this, the model is trained using 420 billion multimodal tokens sampled from our instruction datasets, combined with MiniMax-Text-01 post-training data in ratio of 20:1. This approach ensures that the language modeling capability is maintained while the model acquires new multimodal capabilities. Stage III: Enhancement of User Experience. This stage is designed to further enhance the models capabilities in real-world scenarios and when handling challenging user inputs. We curate sophisticated multimodal data using images sourced from applications that people commonly interact with. Conversations are meticulously labeled to emulate authentic user input and to ensure the provision of accurate, helpful, and diverse responses across multiple conversational turns. The data construction for this stage is guided by an independent human-labeled test set that prioritizes not only accuracy but also the overall quality in terms of user experience. The resulting dataset comprises 44.8 billion multimodal tokens and is trained for one epoch. Stage IV: Enhancement of Preference. In the final stage, we utilize Direct Preference Optimization (DPO) to further enhance model performance and user experience. We construct training dataset consisting of 40,000', 'summary': '<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –æ –º–æ–¥–µ–ª–∏ MiniMax-VL-01</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ MiniMax-VL-01, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç. –û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –º–æ–¥–µ–ª–∏.</p>\n<p><strong>6.1. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–æ–≤:</p>\n<ul>\n<li><strong>–î–∞–Ω–Ω—ã–µ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ (Caption Data):</strong> –ù–∞–±–æ—Ä –∏–∑ 694 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å", –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –î–ª—è 180 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥–ø–∏—Å–∏ –±—ã–ª–∏ —É—Ç–æ—á–Ω–µ–Ω—ã. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –∫–∞–∫ –∏—Å—Ö–æ–¥–Ω—ã–µ, —Ç–∞–∫ –∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ —Å —Ä–∞–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.</li>\n<li><strong>–î–∞–Ω–Ω—ã–µ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ (Description Data):</strong> –ù–∞–±–æ—Ä –∏–∑ 100 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º. –û–ø–∏—Å–∞–Ω–∏—è –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥–µ–ª—å—é, –∞ –∑–∞—Ç–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –ª—é–¥—å–º–∏. –í —Å—Ä–µ–¥–Ω–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–∫–æ–ª–æ 300 —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.</li>\n<li><strong>–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Instruction Data):</strong> –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø–∞—Ä–∞–º–∏ "–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç", —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –æ–±–æ–±—â–∞—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è.</li>\n</ul>\n<p>–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –≤ —Ö–æ–¥–µ –∫–æ—Ç–æ—Ä–æ–≥–æ 1 –º–∏–ª–ª–∏–æ–Ω—É –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è" –±—ã–ª–∏ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã —Ç–µ–≥–∏, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–µ –æ—Å–Ω–æ–≤–Ω—É—é –∑–∞–¥–∞—á—É. –ë—ã–ª–æ –≤—ã–¥–µ–ª–µ–Ω–æ –æ–∫–æ–ª–æ 50 000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ 14 –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.</p>\n<p><strong>6.2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong></p>\n<p>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MiniMax-VL-01 –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ViT-MLP-LLM –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:</p>\n<ul>\n<li><strong>Vision Transformer (ViT):</strong> –í–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä —Å 303 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.</li>\n<li><strong>–î–≤—É—Ö—Å–ª–æ–π–Ω—ã–π MLP-–ø—Ä–æ–µ–∫—Ç–æ—Ä:</strong> –°–ª—É–∂–∏—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.</li>\n<li><strong>MiniMax-Text-01:</strong> –ë–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM), –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.</li>\n</ul>\n<p>–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –æ—Ç 336x336 –¥–æ 2016x2016, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–∏–Ω–∏–∞—Ç—é—Ä–∞ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 336x336. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –Ω–µ–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø–∞—Ç—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–¥–∏—Ä—É—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ, –∞ –∑–∞—Ç–µ–º –∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–∞—Å—à—Ç–∞–±–∞–º.</p>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ViT-L/14, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –Ω—É–ª—è. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å ViT-L/14 —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 224x224 –Ω–∞ 37 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å", –∞ –∑–∞—Ç–µ–º –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 336x336 –Ω–∞ 1.2 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä. –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ImageNet-1K —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 80,55% –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 336x336.</p>\n<p><strong>6.3. –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è</strong></p>\n<p>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MiniMax-VL-01 —Ä–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞:</p>\n<ul>\n<li><strong>–≠—Ç–∞–ø I: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –æ—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –∞–¥–∞–ø—Ç–µ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 80 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 336x336.</li>\n<li><strong>–≠—Ç–∞–ø II: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ 420 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è MiniMax-Text-01 –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 20:1.</li>\n<li><strong>–≠—Ç–∞–ø III: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏. –î–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –¥–∏–∞–ª–æ–≥–∏ —Å –º–µ—Ç–∫–∞–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–º–∏ —Ç–æ—á–Ω—ã–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 44,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.</li>\n<li><strong>–≠—Ç–∞–ø IV: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Direct Preference Optimization (DPO) –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 40 000 –ø—Ä–∏–º–µ—Ä–æ–≤.</li>\n</ul>\n<p>–í —Ü–µ–ª–æ–º, –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è MiniMax-VL-01 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ—â–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç, –∞ —Ç–∞–∫–∂–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>'}, {'title': 'Multi-stage training strategy for image-text pair generation', 'content': ' image-text pairs through the following process: Prompt Selection. Prompts are curated from both instruction data and real user interaction data. These prompts are selected to cover wide range of general scenarios and to specifically address 37 MiniMax-01: Scaling Foundation Models with Lightning Attention persistent issues identified after Stage III, such as occasional repetitive outputs in complex OCR scenarios. Response Generation. We employ diverse strategies, including: generating multiple candidate responses by varying sampling temperature parameters; creating response variants through image weakening in specific scenarios; and using MiniMax-Text-01 to deliberately introduce hallucinations or errors into high-quality responses to generate contrastive samples in specific scenarios. Reward Assignment. Large language models, particularly MiniMax-Text-01, are utilized as evaluators in this stage. Multi-dimensional evaluation criteria are designed to enable systematic and comprehensive assessment of the relationships among prompts, ground truth answers, and generated responses. Pair Construction. Based on the evaluation results, we select the highest-scoring responses as positive samples and the lowest-scoring ones as negative samples, while discarding pairs with insignificant score differences. In addition to incorporating image-text pairs, we also include significant proportion of pure text pairs, as elaborated in Section 5.4.1. It is noteworthy that when Direct Preference Optimization (DPO) is applied to highly capable foundation models, there is propensity for overfitting. To counteract this issue, we adopt an early stopping strategy, which involves terminating the training process prior to the completion of full epoch. This approach is designed to preserve the models generalization capabilities. By following this multi-stage training strategy, we ensure that our model not only demonstrates proficiency in understanding and generating high-quality text but also aligns with human values and safety standards. This comprehensive approach to training allows us to strike balance between model performance and ethical considerations, thereby producing model that is both effective and responsible. 6.4. Benchmarks To assess the performance of our vision-language model, we maintain diverse set of benchmarks, including MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024b), AI2D (Kembhavi et al., 2016), MathVista (Lu et al., 2023), OlympiadBench (He et al., 2024a), MMLongBench-Doc (Ma et al., 2024), MEGA-Bench (Chen et al., 2024a) and an in-house benchmark. These benchmarks help evaluate the models abilities in various areas, including knowledge, visual reasoning, mathematics, science, long context handling, and user experience. We detail our evaluation configuration for each benchmark in Appendix D. As shown in Table 13, MiniMax-VL-01 achieves competitive performance across various vision-language tasks, demonstrating the following key strengths and limitations: Common Downstream Tasks. In standard vision-language downstream tasks, MiniMax-VL-01 exhibits performance on par with GPT-4o, particularly excelling in visual question answering. This strong performance is attributed to its extensive multi-stage training process, enabling the model to effectively understand and reason across visual and textual inputs. However, MiniMax-VL-01 still struggles with advanced mathematical reasoning tasks, as assessed by OlympiadBench (He et al., 2024a). Long Context. We assess MiniMax-VL-01s capability for long-context comprehension and retrieval using MMLongBench-Doc (Ma et al., 2024). The results show that our model outperforms most counterparts, except GPT-4o-11-20. Despite its strong performance overall, MiniMax-VL-01 demonstrates noticeable gap in both single-page (acc: 47.3%) and cross-page (acc: 28.4%) subsets. 38 MiniMax-01: Scaling Foundation Models with Lightning Attention Table 13 Performance of MiniMax-VL-01 on academic and in-house benchmarks. Tasks GPT-4o (11-20) Claude-3.5Sonnet (10-22) Gemini-1.5Pro (002) Gemini-2.0Flash (exp) Qwen2-VL72B-Inst. InternVL 2.5-78B LLama3.2-90B MiniMaxVL-01 MMMU val+dev MMMU-Pro full ChartQA relaxed DocVQA OCRBench AI2D MathVista testmini OlympiadBenchfull 63. 54.5 88.1 91.1 806 83.1 62. 25.2 72.0 54.7 90.8 94.2 82.0 65.4 28.4 Knowledge 68.4 50. Visual Q&A 88.7 91.5 800 70.6 57. 88.3 92.9 846 Mathematics & Sciences 80.9 70. 32.1 85.1 73.1 46.1 Long Context 64. 43.2 91.2 97.1 856 84.4 69. 21.9 66.5 47.3 91.5 96.1 86.8 68.4 25.1 62.1 36.0 85. 90.1 805 78.9 57.3 19.3 68. 52.7 91.7 96.4 865 83.3 68. 24.2 M-LongDocacc 41.4 31.4 26.2 31. 11.6 19.7 13.9 32.5 MEGA-Benchmacro 49. 51.4 45.9 53.9 46.8 45.3 19. 47.4 Comprehensive In-house Benchmark 62.3 47.0 49. 72.1 40.6 34.8 13.6 56.6 Evaluated following 0-shot CoT setting. User Experience Comprehensive Benchmark. On the recently introduced MEGA-Bench (Chen et al., 2024a), realistic and comprehensive evaluation suite, MiniMax-VL-01 shows competitive overall capabilities, surpassing existing open-source vision LLMs. While it excels in diverse sub-tasks such as knowledge and coding, the model faces challenges in more complex tasks, including planning and metric assessments. In-house User Experience Benchmark. While academic benchmarks often focus on problemsolving, they frequently fail to capture the nuances of real-world user interactions with models. To bridge this gap, we develop an in-house benchmark comprising 90 diverse image-related tasks, each designed with tailored and challenging instructions. The images and instructions in the benchmark are strictly deduplicated to not overlap with the training set at any stage. Task relevance is manually verified, with detailed checklist annotated for each sample to ensure precise evaluation. The final test set consists of 524 meticulously annotated samples in both Chinese and English, but Chinese is primarily used. We illustrate some samples in Appendix C. In win-rate comparison against top-leading vision-language model, our model outperforms all open-source models and approaches the performance of GPT-4o-11-20 with narrow margin. 7. Conclusion and Future work In this report, we present MiniMax-Text-01 and MiniMax-VL-01, two novel models developed entirely from the ground up. These models demonstrate top-tier performance across standard benchmarks, particularly excelling in long-context processing with the ability to handle context windows of up to 4 million tokens. Our research findings challenge the prevailing assumption that state-of-the-art MiniMax-01: Scaling Foundation Models with Lightning Attention language models must be built upon traditional attention mechanisms. By strategically integrating linear attention with optimized hardware utilization and carefully designing training recipes, we have successfully expanded the context window by an order of magnitude. This breakthrough not only enhances the efficiency and scalability of LLMs but also paves the way for future models to support even longer context windows and facilitate the development of more sophisticated AI agents. To promote collaboration and advancement in the field, we have made our model publicly available at https://github.com/MiniMax-AI. For general use and evaluation, we provide Chatbot with online search capabilities (https://www.hailuo.ai/) and the online API (https://intl.minimaxi.com). We are committed to keeping this series open source and will release updates as we develop improved models. While MiniMax-Text-01 and MiniMax-VL-01 show strong performance in general language and vision-language tasks, we acknowledge several limitations that necessitate further exploration: 1. Long-Context Evaluation: Current evaluation datasets for long-context retrieval tasks are primarily designed for artificial or simplified scenarios, and the assessment of long-text reasoning capabilities remains limited in practical applications such as document analysis. We plan to enhance long-context retrieval in more realistic settings and expand the evaluation of longcontext reasoning across wider array of tasks. 2. Model Architecture: The model currently retains 1/8 component with vanilla softmax attention. We are investigating more efficient architectures that can eliminate softmax attention entirely, potentially enabling unlimited context windows without computational overhead. 3. Complex Programming Tasks: The models performance on advanced programming tasks is to be improved, as the coding dataset in our pre-training stage is still limited at the moment. We are continuously improving training data selection and refining continue training procedures to address these limitations in the next model version', 'summary': '<h2>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º</h2>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–æ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—Ä—ã "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç", –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:</p>\n<p><strong>1. –í—ã–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ (Prompt Selection).</strong> –ó–∞–ø—Ä–æ—Å—ã –±–µ—Ä—É—Ç—Å—è –∫–∞–∫ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¶–µ–ª—å ‚Äì –æ—Ö–≤–∞—Ç–∏—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –æ–±—â–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –æ—Ç–≤–µ—Ç—ã –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (OCR).</p>\n<p><strong>2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ (Response Generation).</strong> –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –ø—É—Ç–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (sampling temperature).\n   * –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ –ø—É—Ç–µ–º "–æ—Å–ª–∞–±–ª–µ–Ω–∏—è" –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö.\n   * –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MiniMax-Text-01) –¥–ª—è –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–≥–æ –≤–Ω–µ—Å–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∏–ª–∏ "–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π" –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.</p>\n<p><strong>3. –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ (Reward Assignment).</strong> –ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ MiniMax-Text-01, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω—â–∏–∫–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, "–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏" –æ—Ç–≤–µ—Ç–∞–º–∏ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.</p>\n<p><strong>4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (Pair Construction).</strong> –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –±–∞–ª–ª–æ–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º ‚Äì –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ü–∞—Ä—ã —Å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü–µ–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è. –ö—Ä–æ–º–µ –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç", —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è –ø–∞—Ä "—á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç".</p>\n<p><strong>–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ:</strong> –ü—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO) –∫ –º–æ—â–Ω—ã–º –º–æ–¥–µ–ª—è–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –î–ª—è –±–æ—Ä—å–±—ã —Å —ç—Ç–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π —ç–ø–æ—Ö–∏. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–æ–±—â–∞—é—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.</p>\n<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º—É –ø–æ–¥—Ö–æ–¥—É –º–æ–¥–µ–ª—å –Ω–µ —Ç–æ–ª—å–∫–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–º–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ —ç—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —Å–æ–∑–¥–∞–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.</p>\n<h2>–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –≤–∫–ª—é—á–∞—è: MMMU, MMMU-Pro, ChartQA, DocVQA, OCRBench, AI2D, MathVista, OlympiadBench, MMLongBench-Doc, MEGA-Bench –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫. –≠—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∑–Ω–∞–Ω–∏—è, –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç.</p>\n<p>–ú–æ–¥–µ–ª—å MiniMax-VL-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞.</p>\n<p><strong>–ö–ª—é—á–µ–≤—ã–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:</strong></p>\n<ul>\n<li><strong>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏:</strong> –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ GPT-4o, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –±–ª–∞–≥–æ–¥–∞—Ä—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</li>\n<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏:</strong> –ú–æ–¥–µ–ª—å –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ OlympiadBench.</li>\n<li><strong>–î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:</strong> –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∞–Ω–∞–ª–æ–≥–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –±–µ–Ω—á–º–∞—Ä–∫ MMLongBench-Doc. –û–¥–Ω–∞–∫–æ, –æ–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ —Å GPT-4o-11-20 –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.</li>\n<li><strong>MEGA-Bench:</strong> –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MEGA-Bench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫.</li>\n<li><strong>–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫:</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 90 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –∞–Ω–∞–ª–æ–≥–∏ –∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GPT-4o-11-20.</li>\n</ul>\n<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –º–æ–¥–µ–ª—å MiniMax-VL-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –Ω–æ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —É–ª—É—á—à–µ–Ω–∏–∏ –≤ –æ–±–ª–∞—Å—Ç—è—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture (1)', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source (1)', '#optimization (1)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π MiniMax-01, –≤–∫–ª—é—á–∞—é—â–∞—è –≤ —Å–µ–±—è MiniMax-Text-01 (—Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å) –∏ MiniMax-VL-01 (–º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π). –≠—Ç–∏ –º–æ–¥–µ–ª–∏ —Å—Ä–∞–≤–Ω–∏–º—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –ª—É—á—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∏—Ö –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.</p>
<p>–ö–ª—é—á–µ–≤—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è "–º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ" (lightning attention) –∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —ç—Ç–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture of Experts (MoE), —Å–æ–∑–¥–∞–≤–∞—è –º–æ–¥–µ–ª—å —Å 32 —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –æ–±—â–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ 456 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤. –ü—Ä–∏ —ç—Ç–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 45.9 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>
<p>–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –¥–ª—è MoE –∏ "–º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è". –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ç–Ω—è–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –¥–ª–∏–Ω–æ–π –≤ –º–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤.</p>
<p>–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ MiniMax-Text-01 –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ 4 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (inference), –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å. –ú–æ–¥–µ–ª—å MiniMax-VL-01, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –ø—É—Ç–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ 512 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∏ —Ç–µ–∫—Å—Ç, –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.</p>
<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ MiniMax-01 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4o –∏ Claude-3.5-Sonnet, –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–¥–ª–∞–≥–∞—è –≤ 20-32 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ.</p>
<p>–ú–æ–¥–µ–ª–∏ MiniMax-01 –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ –Ω–∞ GitHub.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Experimental setup</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: softmax —Å FlashAttention-2, lightning attention –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ lightning attention. –ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö, –æ—Ç 70 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º –¥–æ 300 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 8192. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–ª–∞ –ø–æ–¥—Ö–æ–¥—É, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º—É –≤ —Ä–∞–±–æ—Ç–µ Chinchilla, –≥–¥–µ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —è–≤–ª—è—é—Ç—Å—è –ø—Ä—è–º—ã–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–î–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –¥–ª–∏–Ω—ã –æ–±—É—á–∞—é—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª—Å—è –µ–¥–∏–Ω—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –≤ 4 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam —Å –Ω–∞—á–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—É—á–µ–Ω–∏—è 3e-4 –∏ –≤–µ—Å–æ–≤—ã–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º 0.1. –ò–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.</p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –≤–∫–ª—é—á–∞—è BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC (–ª–µ–≥–∫–∏–π –∏ —Å–ª–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç—ã), OpenBookQA, Needle in Haystack (NIAH) –∏ SCROLLS. –ö–∞–∂–¥—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π.</p>
<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –µ–¥–∏–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—É—á–µ–Ω–∏—é. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Scaling laws</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–∑—É—á–∞–µ—Ç—Å—è, –∫–∞–∫ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (N) –∏ —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (D) –≤–ª–∏—è—é—Ç –Ω–∞ –æ—à–∏–±–∫—É –æ–±—É—á–µ–Ω–∏—è (L) –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö (C). –¶–µ–ª—å ‚Äì –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ç–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.</p>
<p>–î–ª—è –Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Å—Ç–µ–ø–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É L –∏ C, —Å–ª–µ–¥—É—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ Chinchilla. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–≤–µ—Å—Ç–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ (N_opt * C^a) –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (D_opt * C^b).</p>
<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è, —Å–≤—è–∑—ã–≤–∞—é—â–∞—è –æ—à–∏–±–∫—É –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞–∑–º–µ—Ä–æ–º –º–æ–¥–µ–ª–∏/–¥–∞–Ω–Ω—ã—Ö. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å —Ñ–æ—Ä–º—É–ª–∞ L(X) = (X0/X)^Œ±X, –Ω–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ñ–æ—Ä–º—É–ª–∞ L(X) = Œµ + (X0/X)^Œ±X –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏, –≥–¥–µ Œµ ‚Äì —ç—Ç–æ –Ω–µ—É—Å—Ç—Ä–∞–Ω–∏–º–∞—è –æ—à–∏–±–∫–∞. –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è, –≤—Å–µ —ç—Ç–∏ —Ñ–æ—Ä–º—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ L(X) = Œ≤X * X^(-Œ±X), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Œ±X –∏ Œ≤X.</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ –∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ. –ò–∑ –Ω–∏—Ö —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö –º–æ–¥–µ–ª–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º "lightning attention" (–æ–±–ª–µ–≥—á–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º) —Å–∫–ª–æ–Ω–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∏–≥–∞—é—Ç –º–µ–Ω—å—à–µ–π –æ—à–∏–±–∫–∏, —á–µ–º –º–æ–¥–µ–ª–∏ —Å –æ–±—ã—á–Ω—ã–º softmax –≤–Ω–∏–º–∞–Ω–∏–µ–º.</p>
<p>–¢–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–æ–¥–µ–ª–∏ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º "lightning attention" –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö (CSR, NIAH –∏ SCROLLS). –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å —Ç—Ä–µ–º—è –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è, —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç 410 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Benchmarking training speed and performance of attention mechanisms</div>
                <div class="summary_text"><h2>–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º</h2>
<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –∞ —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π.</p>
<p><strong>–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>
<p>–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å 3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö softmax –≤–Ω–∏–º–∞–Ω–∏–µ, lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ. –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ—Ä—è–µ—Ç—Å—è –≤ —Ç–æ–∫–µ–Ω–∞—Ö, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞ GPU –≤ —Å–µ–∫—É–Ω–¥—É (TGS). –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–µ–Ω—ã –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ HGRN2 –∏ Mamba2. </p>
<p>–í —Ö–æ–¥–µ —Ç–µ—Å—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–ª–∞—Å—å –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–µ–¥–µ–ª–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU H800. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç FlashAttention2.  –í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ softmax –≤–Ω–∏–º–∞–Ω–∏–µ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>
<p><strong>–ì–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã</strong></p>
<p>–ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ª–∏–Ω–µ–π–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, cosformer2 –∏–ª–∏ hgrn2) –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è —Å softmax –≤–Ω–∏–º–∞–Ω–∏–µ–º. –í –º–æ–¥–µ–ª—è—Ö hybrid-cosformer2 –∏ hybrid-hgrn2 –∫–∞–∂–¥—ã–µ –≤–æ—Å–µ–º—å —Å–ª–æ–µ–≤ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ softmax –≤–Ω–∏–º–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º (hybrid-lightning) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.</p>
<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–∫–Ω–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>
<p>–¢–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –∏ –º–æ–¥–µ–ª–∏ —Å "–æ–∫–Ω–∞–º–∏" –≤–Ω–∏–º–∞–Ω–∏—è (sliding window attention), –≥–¥–µ softmax –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å –æ–∫–Ω–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ –º–µ—Ç—Ä–∏–∫–µ NIAH.</p>
<p><strong>–û–±—Å—É–∂–¥–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</strong></p>
<p>–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ, —Ö–æ—Ç—è –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –æ–Ω–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∏–∑-–∑–∞ –∏—Ö –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –Ω–∏—Ö, –≥–∏–±—Ä–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–µ —É—Å—Ç—É–ø–∞—é—Ç softmax –≤–Ω–∏–º–∞–Ω–∏—é, –Ω–æ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –µ–≥–æ –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏.</p>
<p><strong>–ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã softmax –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>
<p>–ú–µ—Ö–∞–Ω–∏–∑–º softmax –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (RNN). –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—Ä–µ–º–µ–Ω–∏ t —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è, –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ t0=1, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞ —Å—á–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –æ–±–ª–∞–¥–∞—é—Ç —ç—Ç–∏–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–µ—Ä–µ—Å—á–µ—Ç–∞, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.</p>
<p><strong>–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (Capacity) –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è</strong></p>
<p>–í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å RNN –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä–æ–º –µ–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å softmax –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç O(d), –∞ –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å lightning –≤–Ω–∏–º–∞–Ω–∏—è - O(d^2/h), –≥–¥–µ d - —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –∞ h - —á–∏—Å–ª–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É d &gt; h, lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏–º–µ–µ—Ç –±–æ–ª—å—à—É—é –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å, —á–µ–º softmax –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å lightning –≤–Ω–∏–º–∞–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏.</p>
<p><strong>–ê–±–ª—è—Ü–∏—è –º–æ–¥—É–ª–µ–π –≤ MoE</strong></p>
<p>–ü—Ä–æ–≤–æ–¥—è—Ç—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –∞–±–ª—è—Ü–∏–∏ –º–æ–¥—É–ª–µ–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Mixture of Experts (MoE). –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –∏ softmax –≤–Ω–∏–º–∞–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ —Å–ª–æ—è (Pre-Layer Normalization) –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ —Å–ª–æ—è (Post-Layer Normalization).</p>
<p><strong>–ì–∏–±—Ä–∏–¥–Ω–æ–µ lightning –≤–Ω–∏–º–∞–Ω–∏–µ vs Softmax –≤–Ω–∏–º–∞–Ω–∏–µ –≤ MoE</strong></p>
<p>–ó–∞–º–µ–Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–µ–≤ softmax –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ lightning –≤–Ω–∏–º–∞–Ω–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE —Å 28 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.</p>
<p><strong>Pre-Layer Normalization vs Post-Layer Normalization</strong></p>
<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Å 60 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Post-Layer Normalization —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DeepNorm –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Pre-Layer Normalization –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º. PostNorm —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –≥–ª—É–±–∏–Ω—É –º–æ–¥–µ–ª–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ PreNorm –º–æ–∂–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å –µ–µ.</p>
<p><strong>–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–∏</strong></p>
<p>–ü–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å - –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤—ã–≤–æ–¥–∞. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ-—É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞, –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω 500 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing MoE architecture for efficient distributed training</div>
                <div class="summary_text"><h2>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE –∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01</h2>
<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE (Mixture of Experts) –∏ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01.</p>
<h3>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è MoE</h3>
<p>–û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ MoE ‚Äî –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö all-to-all (a2a) –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é. –î–ª—è —ç—Ç–æ–≥–æ –≤–≤–æ–¥–∏—Ç—Å—è —Å—Ö–µ–º–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤. –í —ç—Ç–æ–π —Å—Ö–µ–º–µ a2a –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (EP), –∏ –æ–Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ß—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, –∫–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ a2a –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏ –Ω–µ –º–æ–≥—É—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—Ç—å—Å—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø—Ä–æ—Å—Ç–æ—è–º.</p>
<p>–û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ, –≤—ã—è–≤–∏–ª—Å—è –∫–æ–º–ø—Ä–æ–º–∏—Å—Å, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –º–æ–¥–µ–ª–∏ MiniMax-Text-01. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤ (TP) –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ–π, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ù–æ –æ—Ç–∫–∞–∑ –æ—Ç TP –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ (PP). –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ PP –Ω–µ —É–º–µ–Ω—å—à–∞–µ—Ç –æ–±—ä–µ–º –ø–∞–º—è—Ç–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–ª–æ—Ö–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, —Ç–∞–∫ –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –Ω–µ –¥–∞–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.</p>
<p>–ü–æ—ç—Ç–æ–º—É –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å. –í–≤–æ–¥—è—Ç—Å—è –Ω–æ–≤—ã–µ –≥—Ä—É–ø–ø—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:</p>
<ul>
<li><strong>ETP (Expert Tensor Parallel)</strong>: —É–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.</li>
<li><strong>EDP (Expert Data Parallel)</strong>: –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –¥–∞–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.</li>
</ul>
<p>–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU, —É—á–∞—Å—Ç–≤—É—é—â–∏—Ö –≤ –æ–±—É—á–µ–Ω–∏–∏ (world_size), –¥–æ–ª–∂–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å –¥–≤—É–º —É—Å–ª–æ–≤–∏—è–º, —Å–≤—è–∑—ã–≤–∞—é—â–∏–º —Ä–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞:</p>
<p><code>world_size = sizePP * sizeDP * sizeCP * sizeTP</code>
<code>world_size = sizePP * sizeEDP * sizeETP * sizeEP</code></p>
<p>–¢–∞–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–∏–±–∫–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —É–ø—Ä–∞–≤–ª—è—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –∏—Ö –≤–µ—Å–æ–≤ –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º ZeRO (Zero Redundancy Optimizer). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ MoE –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–µ-MoE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤. ETP –º–æ–∂–Ω–æ –≥–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é.</p>
<p>–î–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è EP-ETP. –≠—Ç–æ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∫ —Å–µ—Ç–µ–≤—ã—Ö, —Ç–∞–∫ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø–æ—ç—Ç–æ–º—É –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>
<p>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø —Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, —Ç–æ–ª—å–∫–æ —Ä–∞–∑–¥–µ–ª–∏–≤ —Ä–∞–±–æ—Ç—É –Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø, –º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–µ–π –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏. –û–¥–Ω–∞–∫–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä—É–ø–ø –º–æ–∂–µ—Ç —É—Å–ª–æ–∂–Ω–∏—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—é –ø–æ CPU. –¢–∞–∫ –∫–∞–∫ –¥–æ–ª—è ETP –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE –Ω–µ–≤–µ–ª–∏–∫–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π.</p>
<p>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª–∏–ª–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ MoE –º–æ–¥–µ–ª–∏ MiniMax-Text-01. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —É–¥–∞–ª–æ—Å—å —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ MoE –Ω–∞ 50% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É –ø–æ–≤—ã—à–µ–Ω–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.</p>
<h3>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤</h3>
<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ç—Ä—É–¥–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ç–µ—Ä—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤.</p>
<p>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ "data-packing", –∫–æ–≥–¥–∞ —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠—Ç–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>
<h4>Varlen Ring Attention</h4>
<p>–ê–ª–≥–æ—Ä–∏—Ç–º –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (ring attention) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ data-packing. FlashAttention –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å varlen (–ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã), –Ω–æ –Ω–µ –∏–º–µ–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. TransformerEngine –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Context Parallel (CP), –Ω–æ —ç—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä—è–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ data-packing, —Ç–∞–∫ –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–ª–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã, –¥–ª–∏–Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫—Ä–∞—Ç–Ω–∞ <code>2 * sizeCP</code>.</p>
<p>–ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Ç–µ—Ä—å, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º, –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º Varlen Ring Attention. –û–Ω –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–ª—å—Ü–µ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∫–æ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ data-packing. –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–∏ —Å–º–µ—â–µ–Ω–∏—è –º–∞—Å–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–±—ã—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω–æ–π.</p>
<h4>–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (LASP)</h4>
<p>–ê–ª–≥–æ—Ä–∏—Ç–º LASP (Linear Attention Sequence Parallelism) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä—É–ø–ø—É –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ CP –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ù–æ LASP —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –≤—Å–µ —Ä–∞–Ω–≥–∏ CP —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏ –≤ –æ–ø–µ—Ä–∞—Ü–∏—è—Ö send-recv –¥–ª—è –æ–±–º–µ–Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ key-value (KV). –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —Ä–∞–Ω–≥–∞–º–∏ CP, —á—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</p>
<p>–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π GPU –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞—Å–∫—Ä—ã—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Å–∏—Å—Ç–µ–º—ã –∏ –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing inference in lightning attention mechanism</div>
                <div class="summary_text"><h2>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ Lightning Attention</h2>
<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤–Ω–µ–¥—Ä–µ–Ω—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ Lightning Attention –ø—Ä–∏ –≤—ã–≤–æ–¥–µ (inference). –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Lightning Attention –±—ã–ª–∞ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ü–µ–ª–∏ –∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏–ª–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤—ã–≤–æ–¥–∞, –≥–¥–µ –≤–∞–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã.  –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–≤–æ–¥–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞, –ø–æ—Å–∫–æ–ª—å–∫—É –∏–º–µ–Ω–Ω–æ –æ—Ç –Ω–µ–µ –∑–∞–≤–∏—Å–∏—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:</p>
<p><strong>1. –°–ª–∏—è–Ω–∏–µ –ø–∞–∫–µ—Ç–Ω—ã—Ö —è–¥–µ—Ä (Batched Kernel Fusion):</strong></p>
<p>–ë—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–∞–º—è—Ç—å—é, –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –∏—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ –≤—Å–µ–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤ –ø–∞–∫–µ—Ç–µ. –ù–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (prefill) –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–ª–∏—è–Ω–∏–µ —è–¥–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ Q, K –∏ V, –≤–∫–ª—é—á–∞—è –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞–¥–¥–∏–Ω–≥–∞ (padding) –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–ª–æ–∫–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –º–∞–∫–µ—Ç–∞ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞—Ç—É—Ö–∞–Ω–∏—è. –ù–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–ª–∏—è–Ω–∏–µ —è–¥–µ—Ä –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è KV –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ KV-–∫–µ—à–∞. –≠—Ç–æ —Å–ª–∏—è–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—ä–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ–ø–µ—Ä–∞—Ü–∏–π –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏, –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏ —É–º–µ–Ω—å—à–∏—Ç—å –æ–±—â—É—é –∑–∞–¥–µ—Ä–∂–∫—É –Ω–∞ 10% –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –≤–≤–æ–¥–∞–º–∏. –≠—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω—ã –Ω–∞ GPU H20 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å H800.</p>
<p><strong>2. –†–∞–∑–¥–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ prefill –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (Separated Prefill and Decoding Execution):</strong></p>
<p>–ú–µ—Ö–∞–Ω–∏–∑–º Lightning Attention –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–æ–≤ –∏ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤—ã–≤–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–¥–µ –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞ –æ–±—ã—á–Ω–æ —Ä–∞–≤–Ω–∞ 1. –ü–æ—Å–∫–æ–ª—å–∫—É –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ —è–¥—Ä–æ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª–∏–Ω—ã 1 –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å—é –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏ —Ç—Ä–µ–±—É–µ—Ç –ª–∏—à—å –Ω–µ–±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ (SM) GPU, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª–∏–Ω—ã 1 –∏ —Ç–æ–∫–µ–Ω–æ–≤ –±–æ–ª—å—à–µ–π –¥–ª–∏–Ω—ã. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —è–¥—Ä–∞ CUDA, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ –¥–≤—É—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –≤–≤–æ–¥–∞—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –ø–∞–∫–µ—Ç–µ –∏–∑ 20 —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–º–µ—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å–Ω—ã–π KV-–∫–µ—à, –∞ 1-2 —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–º–µ—é—Ç –¥–ª–∏–Ω—É 50, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ 1, —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ —É—Ä–æ–≤–Ω—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 100 –º—Å –¥–æ 50 –º—Å).</p>
<p><strong>3. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–¥–¥–∏–Ω–≥ (Multi-level Padding):</strong></p>
<p>–ü—Ä–∏–º–µ–Ω—è—è –ø–∞–¥–¥–∏–Ω–≥ –∫ —Ç–µ–Ω–∑–æ—Ä–∞–º Q, K, V –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –º–æ–∂–Ω–æ —Ä–∞–∑–±–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∏ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—á–Ω—ã—Ö —É–º–Ω–æ–∂–µ–Ω–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å StrideBatchedMatmul –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ, —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ –±—ã–ª 256, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –æ–±—É—á–µ–Ω–∏—è. –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ –∫–µ—à–∞, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –ø–∞–∫–µ—Ç–µ –æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ 256, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ª–∏—à–Ω–∏–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º. –ß—Ç–æ–±—ã —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: 32, 64 –∏ 128. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏ –Ω–∞ –ø–∞–¥–¥–∏–Ω–≥, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–π –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>
<p><strong>4. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ StridedBatchedMatmul (StridedBatchedMatmul Extension):</strong></p>
<p>–î–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ StridedBatchedMatmul –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è cublasGemmStridedBatchedEx –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ NVIDIA cuBLAS, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö. –¢–∞–∫–∂–µ –≤–µ–¥–µ—Ç—Å—è —Ä–∞–±–æ—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Å–ª–∏—è–Ω–∏—è —è–¥–µ—Ä –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ GPU Hopper. –ü–æ—Å–∫–æ–ª—å–∫—É —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 256, –æ–ø–µ—Ä–∞—Ü–∏–∏ GEMM (General Matrix-Matrix Multiplication) —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ 256x256 –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ WGMMA –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ Tensor Memory Accelerator (TMA), –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –Ω–∞ —è–¥—Ä–∞—Ö CUDA. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ GPU H20 –∏ H800.</p>
<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏ (MFU) –ø—Ä–µ–≤—ã—à–∞–µ—Ç 75% –Ω–∞ GPU H20 –ø—Ä–∏ –≤—ã–≤–æ–¥–µ. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–π –≤–Ω–∏–º–∞–Ω–∏—è –∏ –æ–ø–µ—Ä–∞—Ü–∏–π Feed-Forward Network (FFN) –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ MoE, softmax –≤–Ω–∏–º–∞–Ω–∏–µ –∑–∞–Ω–∏–º–∞–µ—Ç 95% –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 1 024 000 —Ç–æ–∫–µ–Ω–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Lightning Attention ‚Äì –º–µ–Ω–µ–µ 12%. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Lightning Attention —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–µ –ø–∞–∫–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞ –¥—Ä—É–≥–∏–µ –Ω–µ—Ç.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Optimizing LLM performance through repetition-aware deduplication strategies</div>
                <div class="summary_text"><p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è LLM –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–ù–µ–¥–∞–≤–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –û–¥–Ω–∞–∫–æ, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–µ–∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏ –≤–ª–∏—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.</p>
<p>–î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã. –ó–∞—Ç–µ–º, –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è –≤—ã–±–æ—Ä–æ—á–Ω–æ–º—É —É–º–µ–Ω—å—à–µ–Ω–∏—é, —á—Ç–æ–±—ã —á–∞—Å—Ç–æ—Ç–∞ –∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —É—á–∏—Ç—ã–≤–∞–ª–∏—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –≥–¥–µ –Ω–∞–ø—Ä—è–º—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è.</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —Å–Ω–∏–∂–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –¥–≤—É—Ö —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å—Å—è –¥–æ —á–µ—Ç—ã—Ä–µ—Ö —ç–ø–æ—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º–æ–∂–Ω–æ –¥–æ–±–∏—Ç—å—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>
<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:
* <strong>–ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –º–µ—Ç–æ–¥–æ–º Xavier, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 8192 —Ç–æ–∫–µ–Ω–∞, –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–º–µ–Ω—è–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞. –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–æ—Ç–µ—Ä–µ–π –æ–±—É—á–µ–Ω–∏—è –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ä–∞–∑–æ–≥—Ä–µ–≤–∞, –∑–∞—Ç–µ–º –∏–¥–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç—å—é, –∞ –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è.
* <strong>–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –¥–ª–∏–Ω–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 4 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç—Ä–µ—Ö—Å—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤. –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤–µ—Å–æ–≤. –¢–∞–∫–∂–µ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∞ NIAH (–∑–∞–¥–∞—á–∞ –ø–æ–∏—Å–∫–∞ –∏–≥–ª—ã –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞) –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è.
* <strong>–ü–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞:</strong> –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –º–æ–¥–µ–ª–∏, –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –Ω–∞–±–æ—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∞—è –æ—Ç–≤–µ—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å). –û–±—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç—Ç–∞–ø–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (SFT), –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∏ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Enhancing long-context processing in reward models</div>
                <div class="summary_text"><h2>–ü–æ–¥—Ä–æ–±–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –æ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ MiniMax-01</h2>
<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ MiniMax-01, –≤–∫–ª—é—á–∞—é—â–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ—â–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω–æ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ, —Ç–∞–∫ –∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –≥–¥–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.</p>
<p><strong>5.1. –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫ (–ø—Ä–æ–º–ø—Ç–æ–≤)</strong></p>
<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª–∞ —Å–æ–±—Ä–∞–Ω–∞ –æ–±—à–∏—Ä–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏–∑ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ö–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç –±—ã–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏, –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –∏ —É—Ä–æ–≤–Ω—é —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.  –ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –≤–∫–ª—é—á–∞–ª –º–µ—Ö–∞–Ω–∏–∑–º—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ö–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–≤–æ—Ä—á–µ—Å–∫–æ–µ –ø–∏—Å—å–º–æ, –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π, –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.</p>
<p><strong>5.2. –ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (Reward Model)</strong></p>
<p>–ú–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã –ø–æ —á–µ—Ç—ã—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏:</p>
<ul>
<li><strong>–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å:</strong> –î–ª—è –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏. –í –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–¥–µ–ª—å MiniMax-Text-01 –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–∞. –†–µ—à–µ–Ω–∏—è –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –ø—Ä–æ—Ö–æ–¥—è—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ, –≥–¥–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤.</li>
<li><strong>–ü—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∫–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞. –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É –æ—Ç–≤–µ—Ç–æ–≤, –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é, –ø—Ä–æ–≤–µ—Ä–∫—É —Å –ø–æ–º–æ—â—å—é –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç–∏.</li>
<li><strong>–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–º–æ—â—å—é –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª –¥–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–æ–π –ª—é–¥—å–º–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å–≤—è–∑–Ω–æ—Å—Ç–∏, –≥–ª—É–±–∏–Ω—ã, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —É–º–µ—Å—Ç–Ω–æ—Å—Ç–∏. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.</li>
<li><strong>–ë–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç—å:</strong> –û—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö Constitutional AI. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—Ç–∫–∏ –ª—é–¥—å–º–∏, –∞ MiniMax-Text-01 –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</li>
</ul>
<p><strong>5.3. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-Tuning, SFT)</strong></p>
<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö SFT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —á–µ—Ä–µ–∑ SFT –∏ RL.  –ú–µ—Ç–æ–¥ –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, —Å –æ—Ç–±–æ—Ä–æ–º –ª—É—á—à–∏—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ü—Ä–æ—Ü–µ—Å—Å –æ—Ç–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã n-–≥—Ä–∞–º–º –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>
<p><strong>5.4. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning, RL)</strong></p>
<ul>
<li><strong>5.4.1. –û—Ñ–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong>  –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–µ—Ç–æ–¥ Direct Preference Optimization (DPO) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –ø—Ä–æ–º–ø—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–º—É –Ω–∞ —ç—Ç–∞–ø–µ SFT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–ø—Ç–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ SFT, –∏ –∏—Ö –Ω–µ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –î–ª—è DPO –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –æ—Ü–µ–Ω–æ–∫ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO.</li>
<li><strong>5.4.2. –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong>  –û–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–Ω–ª–∞–π–Ω RL —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø—Ä–æ–º–ø—Ç–æ–≤ —Å–æ —Å—Ä–µ–¥–Ω–∏–º —É—Ä–æ–≤–Ω–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏. –í–æ –≤—Ä–µ–º—è –æ–Ω–ª–∞–π–Ω RL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–æ–º–ø—Ç—ã, –Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–≤—à–∏–µ –≤ SFT, —Ç–∞–∫ –∫–∞–∫ –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –Ω–∞—Å—ã—â–µ–Ω–∏—é –º–æ–¥–µ–ª–∏. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ Group Relative Policy Optimization (GRPO) —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º–∏:<ul>
<li><strong>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏:</strong> –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å –±–æ–ª—å—à–∏–º –æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –ø–æ–ª–∏—Ç–∏–∫ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º, –≤–≤–æ–¥–∏—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–∫–ª—é—á–∞–µ—Ç —Ç–∞–∫–∏–µ —Å–ª—É—á–∞–∏ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.</li>
<li><strong>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ KL:</strong> –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç—Å—è —á–ª–µ–Ω –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ KL, —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.</li>
<li><strong>–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:</strong> –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –≤–∫–ª–∞–¥ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö —Å–æ —Å–∫–æ—à–µ–Ω–Ω—ã–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏.</li>
</ul>
</li>
</ul>
<p><strong>5.5. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏</strong></p>
<p>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö SFT –∏ RL. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –±–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:</p>
<ul>
<li><strong>5.5.1. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong>  –§–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–π —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ö–≤–∞—Ç–∞ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:<ul>
<li><strong>–ü—Ä–æ–º–ø—Ç—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –º–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.</li>
<li><strong>–°–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong>  –°–æ–±–∏—Ä–∞—é—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–µ–±-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.</li>
</ul>
</li>
</ul></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Enhancing model robustness through prompt augmentation and long-context training</div>
                <div class="summary_text"><h2>–£–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ MiniMax-Text-01</h2>
<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ MiniMax-Text-01, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.</p>
<p><strong>5.5.1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏</strong></p>
<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ –µ—ë —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –∞—Ç–∞–∫–∞–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞—Ç–∞–∫ (red team attack prompts), –º–æ–¥–µ–ª—å MiniMax-Text-01 —Ä–∞–Ω–Ω–µ–π –≤–µ—Ä—Å–∏–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é, –∏ —Å–¥–µ–ª–∞—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º–∏.</p>
<p><strong>5.5.2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å</strong></p>
<p>–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (harmless reward model), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–∞ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–∫–∞–∑–æ–≤ –º–æ–¥–µ–ª–∏, –≤ –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, —Å–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —Ü–µ–ª–µ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.</p>
<p><strong>5.6. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –∫ –¥–ª–∏–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É</strong></p>
<p>–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã. –≠—Ç–∞ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö. –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE (–º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ 10 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏.</p>
<ul>
<li>
<p><strong>–≠—Ç–∞–ø I: –ù–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ SFT (Supervised Fine-Tuning). –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –î–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–µ 8192 —Ç–æ–∫–µ–Ω–∞, –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è.</p>
</li>
<li>
<p><strong>–≠—Ç–∞–ø II: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 1 032 192 —Ç–æ–∫–µ–Ω–æ–≤. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã, –≤–∫–ª—é—á–∞—è 50% –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.</p>
</li>
<li>
<p><strong>–≠—Ç–∞–ø III: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∫ 8192 —Ç–æ–∫–µ–Ω–∞–º, –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ DPO (Direct Preference Optimization). –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞–Ω–µ–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏.</p>
</li>
<li>
<p><strong>–≠—Ç–∞–ø IV: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</strong> –£—Å–∏–ª–∏–≤–∞–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é DPO –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 1 032 192 —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —ç—Ç–∞–ø –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω —ç—Ç–∞–ø—É III, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>
</li>
<li>
<p><strong>–≠—Ç–∞–ø V: –û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.</strong> –ó–∞–≤–µ—Ä—à–∞—é—â–∏–π —ç—Ç–∞–ø - –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (8192 —Ç–æ–∫–µ–Ω–∞).</p>
</li>
</ul>
<p><strong>5.7. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö</strong></p>
<p>–ú–æ–¥–µ–ª—å MiniMax-Text-01 –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ, —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å —è–≤–ª—è–µ—Ç—Å—è –ª–∏–¥–µ—Ä–æ–º —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–Ω–∏–º–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –∑–∞–ø—Ä–æ—Å–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∑–Ω–∞–Ω–∏—è—Ö. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.</p>
<p><strong>5.7.1. –û—Å–Ω–æ–≤–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏</strong></p>
<ul>
<li><strong>MMLU –∏ MMLU-Pro:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</li>
<li><strong>SimpleQA –∏ C-SimpleQA:</strong> –ü—Ä–æ–≤–µ—Ä—è—é—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–∏—Ç–∞–π—Å–∫–æ–π –∫—É–ª—å—Ç—É—Ä—ã.</li>
<li><strong>GPQA –∏ DROP:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—é –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ.</li>
<li><strong>GSM8k –∏ MATH:</strong> –ü—Ä–æ–≤–µ—Ä—è—é—Ç –Ω–∞–≤—ã–∫–∏ —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.</li>
<li><strong>HumanEval –∏ MBPP Plus:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.</li>
<li><strong>IFEval:</strong> –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏.</li>
<li><strong>Arena-Hard-Auto:</strong> –û—Ç—Ä–∞–∂–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.</li>
</ul>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MiniMax-Text-01 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–æ –º–Ω–æ–≥–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –≤–∫–ª—é—á–∞—è C-SimpleQA, MMLU, IFEval –∏ Arena-Hard. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>
<p><strong>5.7.2. –ë–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></p>
<p>–û—Ü–µ–Ω–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±—ã–ª–∞ –ø–µ—Ä–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –ø–æ—Å–∫–æ–ª—å–∫—É NIAH (—Ä–∞–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–π—Å—è —Ç–µ—Å—Ç) –æ–∫–∞–∑–∞–ª—Å—è —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç—ã–º. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ —Ç—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º:</p>
<ol>
<li><strong>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></li>
<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></li>
<li><strong>–û–±—É—á–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</strong></li>
</ol>
<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ MiniMax-Text-01, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏–µ –µ–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Long-context retrieval</div>
                <div class="summary_text"><p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∞ –∏–º–µ–Ω–Ω–æ, –µ—ë –ø–∞–º—è—Ç—å –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é.</p>
<p><strong>–ü–∞–º—è—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</strong></p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤. –ü–æ–º–∏–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ "–ò–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞" (NIAH), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî "–ú–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–∞—è –∏–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞" (MR-NIAH). MR-NIAH –∏–º–∏—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏, –≥–¥–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –∑–∞–ø—Ä–æ—Å—ã, –∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Ä–∞—É–Ω–¥–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –æ–¥–∏–Ω –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –°—Ç–æ–≥–∞ —Å–µ–Ω–∞ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –ø–æ –¥–ª–∏–Ω–µ –æ—Ç 2 —Ç—ã—Å—è—á –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–¥–æ 2000 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π), –∞ "–∏–≥–æ–ª–∫–∏" (–∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ) –≤—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –Ω–∞ 25%, 50% –∏ 75% –¥–ª–∏–Ω—ã –¥–∏–∞–ª–æ–≥–∞. –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º.</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã MR-NIAH –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å MiniMax-Text-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∫–∞–∫ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ (GPT, Claude, Gemini), MiniMax-Text-01 –º–µ–Ω—å—à–µ —Ç–µ—Ä—è–µ—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –¥–ª–∏–Ω—ã –≤–≤–æ–¥–∞, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ—ë –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>
<p><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong></p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–µ–ª–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: Ruler –∏ LongBench-V2.</p>
<ul>
<li><strong>Ruler</strong> –≤–∫–ª—é—á–∞–µ—Ç 13 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, –≤ —Ç–æ–º —á–∏—Å–ª–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤.</li>
<li><strong>LongBench-V2</strong> —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–¥–∞—á–∏ –Ω–∞ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –¥–∏–∞–ª–æ–≥–∏, —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫–æ–¥–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ –¥–≤—É—Ö —Ä–µ–∂–∏–º–∞—Ö: —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ –±–µ–∑ –Ω–µ—ë. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –∫–æ—Ä–æ—Ç–∫–∏–π (–¥–æ 32 —Ç—ã—Å—è—á —Å–ª–æ–≤), —Å—Ä–µ–¥–Ω–∏–π (32-128 —Ç—ã—Å—è—á —Å–ª–æ–≤) –∏ –¥–ª–∏–Ω–Ω—ã–π (128 —Ç—ã—Å—è—á - 2 –º–∏–ª–ª–∏–æ–Ω–∞ —Å–ª–æ–≤).</li>
</ul>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MiniMax-Text-01 –æ—Ç–ª–∏—á–Ω–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ –Ω–∞–±–æ—Ä–∞ Ruler. –ù–∞ –¥–ª–∏–Ω–µ –≤–≤–æ–¥–∞ 64 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ª–∏–¥–µ—Ä–∞–º–∏ (GPT-4o, Claude-3.5-Sonnet), –Ω–æ –Ω–∞—á–∏–Ω–∞—è —Å–æ 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤ MiniMax-Text-01 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö —Å —É–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤).</p>
<p>–í LongBench-V2 –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ—Å—Ç–∏–≥–∞—è –Ω–∞–∏–ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∂–∏–º–µ —Å CoT –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –±–µ–∑ CoT.</p>
<p>–í —Ü–µ–ª–æ–º, MiniMax-Text-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RoPE –∏ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluating long in-context learning in lifelong models</div>
                <div class="summary_text"><h2>–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ</h2>
<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤–æ–π –æ–±–ª–∞—Å—Ç—å—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MTOB (Machine Translation from One Book), –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–≥–æ —É—á–µ–±–Ω–∏–∫–∞.</p>
<p>–ó–∞–¥–∞—á–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–≤–æ–¥–µ –º–µ–∂–¥—É –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ —è–∑—ã–∫–æ–º –∫–∞–ª–∞–º–∞–Ω–≥, –∫–æ—Ç–æ—Ä—ã–π –∏–º–µ–µ—Ç –æ—á–µ–Ω—å –º–∞–ª–æ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ–±—É—á–∞—é—â–µ–º –∫–æ—Ä–ø—É—Å–µ –º–æ–¥–µ–ª–∏. –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –æ—Å–≤–æ–∏—Ç —ç—Ç–æ—Ç —è–∑—ã–∫, –æ–ø–∏—Ä–∞—è—Å—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —á–∞—Å—Ç–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Å–æ–±–∏—è –∏ 375 –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø–µ—Ä–µ–≤–æ–¥–∞. –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 81 —Ç—ã—Å—è—á—É —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–∂–∏–º–µ "–ø–æ–ª–æ–≤–∏–Ω–∞ –∫–Ω–∏–≥–∏" –∏ 133 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ä–µ–∂–∏–º–µ "–ø–æ–ª–Ω–∞—è –∫–Ω–∏–≥–∞".</p>
<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ 11. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 16 –ø–æ–∫–∞–∑–∞–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ ChrF (eng kalam) –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –¢—â–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —è–∑—ã–∫–æ–º –∫–∞–ª–∞–º–∞–Ω–≥, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ–Ω—å –º–∞–ª–æ–º –æ–±—ä–µ–º–µ. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å ChrF (eng kalam) –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è —Å–∞–º—ã–º –Ω–∏–∑–∫–∏–º, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏–º–µ–ª–∏ –≤ —Å–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–ª–∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–∞–ª–∞–º–∞–Ω–≥–µ. </p>
<p>–û–¥–Ω–∞–∫–æ, –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø–æ–ª–æ–≤–∏–Ω–∞ –∏ –ø–æ–ª–Ω–∞—è –∫–Ω–∏–≥–∞) –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ ChrF (eng kalam). –¢–∞–∫–∂–µ –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–µ BLEURT (kalam eng).</p>
<p>–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ 4.2, –Ω–∞–±–ª—é–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –¥–∞–Ω–Ω—ã–º–∏ MTOB –∏ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 16. –•–æ—Ç—è –±—ã–ª–∏ –∏–∑—É—á–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–∏—à—å –æ–¥–Ω–∏–º –∏–∑ –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ü–æ—ç—Ç–æ–º—É –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –º–∞—Å—à—Ç–∞–±–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —á—Ç–æ–±—ã –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —É–ª—É—á—à–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.</p>
<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong> <em>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è, –∫–∞–∫ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —Ä–µ–¥–∫–∏–π —è–∑—ã–∫, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –æ–±—É—á–µ–Ω–∏—é "–Ω–∞ –ª–µ—Ç—É" –∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Dataset and training overview</div>
                <div class="summary_text"><h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –æ –º–æ–¥–µ–ª–∏ MiniMax-VL-01</h2>
<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ MiniMax-VL-01, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–∞–∫ –∏ —Ç–µ–∫—Å—Ç. –û–±—É—á–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –º–æ–¥–µ–ª–∏.</p>
<p><strong>6.1. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</strong></p>
<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–æ–≤:</p>
<ul>
<li><strong>–î–∞–Ω–Ω—ã–µ —Å –ø–æ–¥–ø–∏—Å—è–º–∏ (Caption Data):</strong> –ù–∞–±–æ—Ä –∏–∑ 694 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å", –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –î–ª—è 180 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–¥–ø–∏—Å–∏ –±—ã–ª–∏ —É—Ç–æ—á–Ω–µ–Ω—ã. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –∫–∞–∫ –∏—Å—Ö–æ–¥–Ω—ã–µ, —Ç–∞–∫ –∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ —Å —Ä–∞–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.</li>
<li><strong>–î–∞–Ω–Ω—ã–µ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ (Description Data):</strong> –ù–∞–±–æ—Ä –∏–∑ 100 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º. –û–ø–∏—Å–∞–Ω–∏—è –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥–µ–ª—å—é, –∞ –∑–∞—Ç–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –ª—é–¥—å–º–∏. –í —Å—Ä–µ–¥–Ω–µ–º –æ–ø–∏—Å–∞–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–∫–æ–ª–æ 300 —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.</li>
<li><strong>–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Instruction Data):</strong> –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø–∞—Ä–∞–º–∏ "–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç", —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–µ—à–µ–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –æ–±–æ–±—â–∞—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è.</li>
</ul>
<p>–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –≤ —Ö–æ–¥–µ –∫–æ—Ç–æ—Ä–æ–≥–æ 1 –º–∏–ª–ª–∏–æ–Ω—É –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è" –±—ã–ª–∏ –ø—Ä–∏—Å–≤–æ–µ–Ω—ã —Ç–µ–≥–∏, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–µ –æ—Å–Ω–æ–≤–Ω—É—é –∑–∞–¥–∞—á—É. –ë—ã–ª–æ –≤—ã–¥–µ–ª–µ–Ω–æ –æ–∫–æ–ª–æ 50 000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –≤ 14 –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.</p>
<p><strong>6.2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</strong></p>
<p>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MiniMax-VL-01 –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ViT-MLP-LLM –∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:</p>
<ul>
<li><strong>Vision Transformer (ViT):</strong> –í–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä —Å 303 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.</li>
<li><strong>–î–≤—É—Ö—Å–ª–æ–π–Ω—ã–π MLP-–ø—Ä–æ–µ–∫—Ç–æ—Ä:</strong> –°–ª—É–∂–∏—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.</li>
<li><strong>MiniMax-Text-01:</strong> –ë–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM), –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.</li>
</ul>
<p>–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –æ—Ç 336x336 –¥–æ 2016x2016, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–∏–Ω–∏–∞—Ç—é—Ä–∞ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 336x336. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –Ω–µ–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø–∞—Ç—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–¥–∏—Ä—É—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ, –∞ –∑–∞—Ç–µ–º –∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–∞—Å—à—Ç–∞–±–∞–º.</p>
<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±–ª–µ–≥—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ViT-L/14, –æ–±—É—á–µ–Ω–Ω–∞—è —Å –Ω—É–ª—è. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å ViT-L/14 —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 224x224 –Ω–∞ 37 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å", –∞ –∑–∞—Ç–µ–º –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 336x336 –Ω–∞ 1.2 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä. –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö ImageNet-1K —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 80,55% –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏ 336x336.</p>
<p><strong>6.3. –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è</strong></p>
<p>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MiniMax-VL-01 —Ä–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞:</p>
<ul>
<li><strong>–≠—Ç–∞–ø I: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –æ—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ—Å–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –∞–¥–∞–ø—Ç–µ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 80 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏. –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 336x336.</li>
<li><strong>–≠—Ç–∞–ø II: –£–ª—É—á—à–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ 420 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è MiniMax-Text-01 –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 20:1.</li>
<li><strong>–≠—Ç–∞–ø III: –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏. –î–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –¥–∏–∞–ª–æ–≥–∏ —Å –º–µ—Ç–∫–∞–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–º–∏ —Ç–æ—á–Ω—ã–µ –∏ –ø–æ–ª–µ–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 44,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.</li>
<li><strong>–≠—Ç–∞–ø IV: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π.</strong> –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Direct Preference Optimization (DPO) –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 40 000 –ø—Ä–∏–º–µ—Ä–æ–≤.</li>
</ul>
<p>–í —Ü–µ–ª–æ–º, –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è MiniMax-VL-01 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ—â–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç, –∞ —Ç–∞–∫–∂–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Multi-stage training strategy for image-text pair generation</div>
                <div class="summary_text"><h2>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º</h2>
<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —Å–ø–æ—Å–æ–±–Ω–æ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—Ä—ã "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç", –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:</p>
<p><strong>1. –í—ã–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ (Prompt Selection).</strong> –ó–∞–ø—Ä–æ—Å—ã –±–µ—Ä—É—Ç—Å—è –∫–∞–∫ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¶–µ–ª—å ‚Äì –æ—Ö–≤–∞—Ç–∏—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –æ–±—â–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –æ—Ç–≤–µ—Ç—ã –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (OCR).</p>
<p><strong>2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ (Response Generation).</strong> –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
   * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞ –ø—É—Ç–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (sampling temperature).
   * –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ –ø—É—Ç–µ–º "–æ—Å–ª–∞–±–ª–µ–Ω–∏—è" –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö.
   * –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MiniMax-Text-01) –¥–ª—è –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–≥–æ –≤–Ω–µ—Å–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∏–ª–∏ "–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π" –≤ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.</p>
<p><strong>3. –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ (Reward Assignment).</strong> –ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ MiniMax-Text-01, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω—â–∏–∫–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, "–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏" –æ—Ç–≤–µ—Ç–∞–º–∏ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏.</p>
<p><strong>4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (Pair Construction).</strong> –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã —Å –Ω–∞–∏–≤—ã—Å—à–∏–º –±–∞–ª–ª–æ–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º ‚Äì –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ü–∞—Ä—ã —Å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–Ω–∏—Ü–µ–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è. –ö—Ä–æ–º–µ –ø–∞—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç", —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–ª—è –ø–∞—Ä "—á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç".</p>
<p><strong>–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ:</strong> –ü—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO) –∫ –º–æ—â–Ω—ã–º –º–æ–¥–µ–ª—è–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é. –î–ª—è –±–æ—Ä—å–±—ã —Å —ç—Ç–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π —ç–ø–æ—Ö–∏. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–æ–±—â–∞—é—â–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.</p>
<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º—É –ø–æ–¥—Ö–æ–¥—É –º–æ–¥–µ–ª—å –Ω–µ —Ç–æ–ª—å–∫–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–º–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏ –∏ —ç—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —Å–æ–∑–¥–∞–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.</p>
<h2>–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –≤–∫–ª—é—á–∞—è: MMMU, MMMU-Pro, ChartQA, DocVQA, OCRBench, AI2D, MathVista, OlympiadBench, MMLongBench-Doc, MEGA-Bench –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫. –≠—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∑–Ω–∞–Ω–∏—è, –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç.</p>
<p>–ú–æ–¥–µ–ª—å MiniMax-VL-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞.</p>
<p><strong>–ö–ª—é—á–µ–≤—ã–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:</strong></p>
<ul>
<li><strong>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏:</strong> –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ GPT-4o, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –±–ª–∞–≥–æ–¥–∞—Ä—è –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</li>
<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏:</strong> –ú–æ–¥–µ–ª—å –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é –±–µ–Ω—á–º–∞—Ä–∫–∞ OlympiadBench.</li>
<li><strong>–î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:</strong> –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∞–Ω–∞–ª–æ–≥–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –±–µ–Ω—á–º–∞—Ä–∫ MMLongBench-Doc. –û–¥–Ω–∞–∫–æ, –æ–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ —Å GPT-4o-11-20 –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.</li>
<li><strong>MEGA-Bench:</strong> –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MEGA-Bench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫.</li>
<li><strong>–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫:</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 90 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –∞–Ω–∞–ª–æ–≥–∏ –∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GPT-4o-11-20.</li>
</ul>
<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –º–æ–¥–µ–ª—å MiniMax-VL-01 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –Ω–æ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º —É–ª—É—á—à–µ–Ω–∏–∏ –≤ –æ–±–ª–∞—Å—Ç—è—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-01-15 09:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-15 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    