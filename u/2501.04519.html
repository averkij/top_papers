
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. January 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 —è–Ω–≤–∞—Ä—è</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-10.html">‚¨ÖÔ∏è <span id="prev-date">10.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-14.html">‚û°Ô∏è <span id="next-date">14.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 —è–Ω–≤–∞—Ä—è', 'en': 'January 13', 'zh': '1Êúà13Êó•'};
        let feedDateNext = {'ru': '14.01', 'en': '01/14', 'zh': '1Êúà14Êó•'};
        let feedDatePrev = {'ru': '10.01', 'en': '01/10', 'zh': '1Êúà10Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2501.04519', 'title': 'rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking', 'url': 'https://huggingface.co/papers/2501.04519', 'abstract': 'We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising "deep\nthinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs\' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-01-08', 'pub_date_card': {'ru': '8 —è–Ω–≤–∞—Ä—è', 'en': 'January 8', 'zh': '1Êúà8Êó•'}, 'hash': 'b065003de5fa3bde', 'authors': ['Xinyu Guan', 'Li Lyna Zhang', 'Yifei Liu', 'Ning Shang', 'Youran Sun', 'Yi Zhu', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2501.04519.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#small_models', '#dataset'], 'emoji': 'üßÆ', 'ru': {'title': '–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –±–æ–ª—å—à–∏–µ –∑–∞–¥–∞—á–∏: rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç rStar-Math - –ø–æ–¥—Ö–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–∞–ª—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (SLM) –¥–æ—Å—Ç–∏—á—å –∏–ª–∏ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –º–µ—Ç–æ–¥—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS) —Å –¥–≤—É–º—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ SLM: –ø–æ–ª–∏—Ç–∏–∫–æ–π –∏ –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å–∏–Ω—Ç–µ–∑–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å SLM –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Empowering Small Models to Excel in Math Reasoning', 'desc': 'The paper introduces rStar-Math, a framework that enhances the math reasoning abilities of small language models (SLMs) without relying on larger models. It employs Monte Carlo Tree Search (MCTS) to enable deep thinking, allowing the SLM to perform guided search during problem-solving. Key innovations include a code-augmented Chain of Thought (CoT) data synthesis method for generating verified reasoning paths, a refined process preference model (PPM) for better reward training, and a self-evolution strategy for iterative improvement. As a result, rStar-Math significantly boosts the performance of SLMs on math benchmarks, achieving state-of-the-art results in various assessments.'}, 'zh': {'title': 'Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜÊñ∞Á™ÅÁ†¥', 'desc': 'rStar-MathÂ±ïÁ§∫‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõ‰∏äÂèØ‰ª•‰∏éOpenAIÁöÑo1Áõ∏Â™≤ÁæéÔºåÁîöËá≥Ë∂ÖË∂äÂÆÉÔºåËÄåÊó†ÈúÄ‰ªéÊõ¥Âº∫Â§ßÁöÑÊ®°Âûã‰∏≠Ëí∏È¶è„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÂÆûÁé∞‚ÄúÊ∑±Â∫¶ÊÄùËÄÉ‚ÄùÔºåÂú®ÊµãËØïÊó∂Áî±SLMÈ©±Âä®ÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÊåáÂØºÊï∞Â≠¶Á≠ñÁï•SLMËøõË°åÊêúÁ¥¢„ÄÇrStar-MathÂºïÂÖ•‰∫Ü‰∏âÈ°πÂàõÊñ∞Êù•Ëß£ÂÜ≥ËÆ≠ÁªÉ‰∏§‰∏™SLMÁöÑÊåëÊàòÔºåÂåÖÊã¨Êñ∞È¢ñÁöÑ‰ª£Á†ÅÂ¢ûÂº∫ÁöÑÈìæÂºèÊé®ÁêÜÊï∞ÊçÆÂêàÊàêÊñπÊ≥ïÂíåÊõ¥ÊúâÊïàÁöÑËøáÁ®ãÂÅèÂ•ΩÊ®°ÂûãÔºàPPMÔºâËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÁªèËøáÂõõËΩÆËá™ÊàëËøõÂåñÔºårStar-MathÂú®747,000‰∏™Êï∞Â≠¶ÈóÆÈ¢ò‰∏äÁîüÊàê‰∫ÜÊï∞Áôæ‰∏á‰∏™ÂêàÊàêËß£Ôºå‰ΩøSLMsÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising "deep thinking" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs\' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–π, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –∏–ª–∏ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ OpenAI o1 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç "–≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è" —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS). –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –≤—ã—Å—Ç—É–ø–∞—é—â–∞—è –≤ —Ä–æ–ª–∏ "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏", –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–æ–∏—Å–∫ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—è—Å—å –Ω–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞, —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ SLM.</p>\n<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è —ç—Ç–∏—Ö –¥–≤—É—Ö SLM:</p>\n<ol>\n<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö CoT —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∫–æ–¥–æ–º:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö, –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è SLM, –æ—Ç–≤–µ—á–∞—é—â–µ–π –∑–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–∏—Ç–∏–∫—É. (CoT - Chain of Thought, –º–µ—Ç–æ–¥, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç, –Ω–æ –∏ —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–∏–≤–µ–¥—à–∏—Ö –∫ –Ω–µ–º—É)</li>\n<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–±–µ–≥–∞–µ—Ç –ø—Ä—è–º–æ–≥–æ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω –æ–±—É—á–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM). (–¢.–µ. –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —à–∞–≥–∏, –∞ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π)</li>\n<li><strong>–†–µ—Ü–µ–ø—Ç —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è:</strong> –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –ø–æ–ª–∏—Ç–∏–∫—É, –∏ PPM —Å—Ç—Ä–æ—è—Ç—Å—è —Å –Ω—É–ª—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</li>\n</ol>\n<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è 4 —Ä–∞—É–Ω–¥–∞–º —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è 747 —Ç—ã—Å—è—á –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, rStar-Math –ø–æ–≤—ã—à–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π SLM –¥–æ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH, rStar-Math —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Qwen2.5-Math-7B —Å 58.8% –¥–æ 90.0% –∏ Phi3-mini-3.8B —Å 41.4% –¥–æ 86.4%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –Ω–∞ +4.5% –∏ +0.9% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ USA Math Olympiad (AIME) rStar-Math —Ä–µ—à–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 53.3% (8 –∏–∑ 15) –∑–∞–¥–∞—á, —á—Ç–æ —Å—Ç–∞–≤–∏—Ç –µ–≥–æ –≤ —Ç–æ–ø 20% —Å–∞–º—ã—Ö —Å–ø–æ—Å–æ–±–Ω—ã—Ö —É—á–µ–Ω–∏–∫–æ–≤ —Å—Ç–∞—Ä—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ö–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ —Å—Å—ã–ª–∫–µ https://github.com/microsoft/rStar.</p>'}, {'title': 'Recent Studies', 'content': 'Equal contribution. Project leader; correspondence to lzhani@microsoft.com Xinyu Guan and Youran Sun did this work during the internship at MSRA. Xinyu Guan (2001gxy@gmail.com) is with Peking University, Youran Sun is with Tsinghua University. Figure 1: The overview of rStar-Math. In the test-time compute paradigm, the key is to train powerful policy model that generates promising solution steps and reliable reward model that accurately evaluates them, both of which depend on high-quality training data. Unfortunately, it is well-known that off-the-shelf high-quality math reasoning data is scarce, and synthesizing high-quality math data faces fundamental challenges. For the policy model, it is challenging to distinguish erroneous reasoning steps from the correct ones, complicating the elimination of low-quality data. It is worth noting that in math reasoning, correct final answer does not ensure the correctness of the entire reasoning trace [Lanham et al., 2023]. Incorrect intermediate steps significantly decrease data quality. As for the reward model, process reward modeling (PRM) shows great potential by providing fine-grained feedback on intermediate steps [Lightman et al., 2023]. However, the training data is even scarcer in this regard: accurate step-by-step feedback requires intense human labeling efforts and is impractical to scale, while those automatic annotation attempts show limited gains due to noisy reward scores [Luo et al., 2024, Wang et al., 2024c, Chen et al., 2024]. Due to the above challenges, existing distill-based data synthesis approaches to training policy models, e.g., scaling up GPT4-distilled CoT data [Tang et al., 2024, Huang et al., 2024], have shown diminishing returns and cannot exceed the capability of their teacher model; meanwhile, as of today, training reliable PRMs for math reasoning remains an open question. In this work, we introduce rStar-Math, self-evolvable System 2-style reasoning approach that achieves the state-of-the-art math reasoning, rivaling and sometimes even surpassing OpenAI o1 on challenging math competition benchmarks with model size as small as 7 billion. Unlike solutions relying on superior LLMs for data synthesis, rStar-Math leverages smaller language models (SLMs) with Monte Carlo Tree Search (MCTS) to establish self-evolutionary process, iteratively generating higher-quality training data. To achieve self-evolution, rStar-Math introduces three key innovations. First, novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values. Specifically, math problem-solving is decomposed into multi-step generation within MCTS. At each step, the SLM serving as the policy model samples candidate nodes, each generating one-step CoT and the corresponding Python code. To verify the generation quality, only nodes with successful Python code execution are retained, thus mitigating errors in intermediate steps. Moreover, extensive MCTS rollouts automatically assign Q-value to each intermediate step based on its contribution: steps contributing to more trajectories that lead to the correct answer are given higher Q-values and considered higher quality. This ensures that the reasoning trajectories generated by SLMs consist of correct, high-quality intermediate steps. Second, novel method that trains an SLM acting as process preference model, i.e., PPM to implement the desired PRM, that reliably predicts reward label for each math reasoning step. The PPM leverages the fact that, although Q-values are still not precise enough to score each reasoning step despite using extensive MCTS rollouts, the Q-values can reliably distinguish positive (correct) steps from negative (irrelevant/incorrect) ones. Thus the training method constructs preference pairs for each step based on Q-values and uses pairwise ranking loss [Ouyang et al., 2022] to optimize PPMs score prediction for each reasoning step, achieving reliable labeling. This approach avoids conventional methods that directly use Q-values as reward labels [Luo et al., 2024, Chen et al., 2024], which are inherently noisy and imprecise in stepwise reward assignment. Finally, four-round self-evolution recipe that progressively builds both frontier policy model and PPM from scratch. We begin by curating dataset of 747k math word problems from publicly available sources. In each round, we use the latest policy model and PPM to perform MCTS, 2 generating increasingly high-quality training data using the above two methods to train stronger policy model and PPM for next round. Each round achieves progressive refinement: (1) stronger policy SLM, (2) more reliable PPM, (3) generating better reasoning trajectories via PPM-augmented MCTS, and (4) improving training data coverage to tackle more challenging and even competitionlevel math problems. Extensive experiments across four SLMs (1.5B-7B) and seven math reasoning tasks demonstrate the effectiveness of rStar-Math. Remarkably, rStar-Math improves all four SLMs, matching or even surpassing OpenAI o1 on challenging math benchmarks. On MATH benchmark, with 8 search trajectories, rStar-Math boosts Qwen2.5-Math-7B from 58.8% to 89.4% and Qwen2.5-Math-1.5B from 51.2% to 87.8%. With 64 trajectories, the scores rise to 90% and 88.4%, outperforming o1-preview by 4.5% and 2.6% and matching o1-minis 90%. On the Olympiad-level AIME 2024, rStar-Math solves on average 53.3% (8/15) of the problems, exceeding o1-preview by 8.7% and all other open-sourced LLMs. We further conduct comprehensive experiments to verify the superiority of step-by-step verified reasoning trajectories over state-of-the-art data synthesis baselines, as well as the PPMs effectiveness compared to outcome reward models and value-based PRMs. Finally, we present key findings from rStar-Math deep thinking, including the intrinsic self-reflection capability and PPMs preference for theorem-applications intermediate steps.', 'summary': '<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω rStar-Math, –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–º–∏—Ç–∏—Ä—É—é—â–∏–π –º—ã—à–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Ç–∏–ø–∞ (System 2). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è OpenAI o1, –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).</p>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –Ω–µ—Ö–≤–∞—Ç–∫–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.  –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ –¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω –≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç, —ç—Ç–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–µ—à–µ–Ω–∏—è.  –û—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Å–Ω–∏–∂–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞, —á—Ç–æ –æ—á–µ–Ω—å —Ç—Ä—É–¥–æ–µ–º–∫–æ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–æ–∫–∞ –Ω–µ –¥–∞–µ—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑-–∑–∞ —à—É–º–∞ –≤ –æ—Ü–µ–Ω–∫–∞—Ö.  –ü–æ—ç—Ç–æ–º—É, –ø–æ–¥—Ö–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–µ–¥–µ–ª–∞ –∏ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–≤–æ–∏—Ö —É—á–∏—Ç–µ–ª–µ–π.</p>\n<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –∏ –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS) –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.  –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–¥–∞:</strong>  –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö MCTS. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ SLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —Ä–µ—à–µ–Ω–∏—è –≤ –≤–∏–¥–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞ –Ω–∞ Python.  –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ —à–∞–≥–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–¥ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è.  –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–∞–∂–¥–æ–º—É —à–∞–≥—É Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥ —à–∞–≥–∞ –≤ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –®–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—â–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è.</li>\n<li><strong>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ (PPM):</strong> –≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É (reward) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ä–µ—à–µ–Ω–∏—è.  –î–ª—è –æ–±—É—á–µ–Ω–∏—è PPM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ Q-–∑–Ω–∞—á–µ–Ω–∏—è, —Ö–æ—Ç—è –∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏, –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–∞–¥–µ–∂–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.  –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Å–æ–∑–¥–∞—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π, –∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–Ω–∂–∏—Ä—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä—è–º–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∑–∞—à—É–º–ª–µ–Ω—ã.</li>\n<li><strong>–ß–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è:</strong> –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–æ —Å–±–æ—Ä–∞ 747 —Ç—ã—Å—è—á –∑–∞–¥–∞—á –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Ä–µ—à–µ–Ω–∏—è (policy model) –∏ PPM –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è MCTS –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ó–∞—Ç–µ–º –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—Ç—Å—è –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.  –ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É—Å–∏–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–∏, –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ PPM, —É–ª—É—á—à–µ–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–µ—à–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –æ—Ö–≤–∞—Ç–∞ –∑–∞–¥–∞—á.</li>\n</ol>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ SLM (–æ—Ç 1.5 –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –Ω–∞ —Å–µ–º–∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å rStar-Math.  –ù–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, rStar-Math –ø–æ–≤—ã—Å–∏–ª —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B —Å 58.8% –¥–æ 89.4%, –∞ Qwen2.5-Math-1.5B —Å 51.2% –¥–æ 87.8%. –° 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –ø–æ–∏—Å–∫–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç 90% –∏ 88.4% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –∏ –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-minis. –ù–∞ –æ–ª–∏–º–ø–∏–∞–¥–µ AIME 2024, rStar-Math —Ä–µ—à–∏–ª 53.3% –∑–∞–¥–∞—á, —á—Ç–æ –Ω–∞ 8.7% –±–æ–ª—å—à–µ, —á–µ–º o1-preview. –ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–∞ rStar-Math –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤.</p>'}, {'title': 'Related Works', 'content': 'Math Data Synthesis. Advancements in LLM math reasoning have largely relied on curating high-quality CoT data, with most leading approaches being GPT-distilled, using frontier models like GPT-4 for synthesis [Wang et al., 2024b, Gou et al., 2023, Luo et al., 2023]. Notable works include NuminaMath [Jia LI and Polu, 2024a] and MetaMath [Yu et al., 2023b]. While effective, this limits reasoning to the capabilities of the teacher LLM. Hard problems that the teacher LLM cannot solve are excluded in the training set. Even solvable problems may contain error-prone intermediate steps, which are hard to detect. Although rejection sampling methods [Yuan et al., 2023, Brown et al., 2024] can improve data quality, they do not guarantee correct intermediate steps. As result, scaling up CoT data has diminishing returns, with gains nearing saturatione.g., OpenMathInstruct-2 [Toshniwal et al., 2024] only sees 3.9% boost on MATH despite an 8 increase in dataset size. Scaling Test-time Compute has introduced new scaling laws, allowing LLMs to improve performance across by generating multiple samples and using reward models for best-solution selection [Snell et al., 2024, Wu et al., 2024, Brown et al., 2024]. Various test-time search methods have been proposed [Kang et al., 2024, Wang et al., 2024a], including random sampling [Wang et al., 2023] and tree-search methods [Yao et al., 2024, Hao et al., 2023, Zhang et al., 2024b, Qi et al., 2024] like MCTS. However, open-source methods for scaling test-time computation have shown limited gains in math reasoning, often due to policy LLM or reward model limitations. rStar-Math addresses this by iteratively evolving the policy LLM and reward model, achieving System 2 mathematical reasoning performance comparable to OpenAI o1 [OpenAI, 2024]. Reward Models are crucial for effective System 2 reasoning but are challenging to obtain. Recent works include LLM-as-a-Judge for verification [Zheng et al., 2023, Qi et al., 2024] and specialized reward models like Outcome Reward Model [Yang et al., 2024, Yu et al., 2023a] and Process Reward Model (PRM) [Lightman et al., 2024]. While PRMs offer promising dense, step-level reward signals for complex reasoning [Luo et al., 2024, Wang et al., 2024c], collecting step-level annotations remains an obstacle. While Kang et al. [2024], Wang et al. [2024a] rely on costly human-annotated datasets like PRM800k [Lightman et al., 2024], recent approaches [Wang et al., 2024c, Luo et al., 2024] explore automated annotation via Monte Carlo Sampling or MCTS. However, they struggle to generate precise reward scores, which limits performance gains. rStar-Math introduces novel process preference reward (PPM) that eliminates the need for accurate step-level reward score annotation.', 'summary': '<p><strong>–°–∏–Ω—Ç–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö</strong></p>\n<p>–£—Å–ø–µ—Ö–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –º–Ω–æ–≥–æ–º –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥—É Chain-of-Thought (CoT). –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤–µ–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö —Ä–∞–±–æ—Ç –≤–∫–ª—é—á–∞—é—Ç NuminaMath –∏ MetaMath. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ–±—É—á–∞—é—â–µ–π LLM. –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—â–∞—è LLM –Ω–µ –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å, –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –î–∞–∂–µ –≤ —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å.</p>\n<p>–ú–µ—Ç–æ–¥—ã –æ—Ç–±–æ—Ä–∞ —Å –æ—Ç–±—Ä–∞–∫–æ–≤–∫–æ–π –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö CoT –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –æ—Ç–¥–∞—á–∏, –∫–æ–≥–¥–∞ –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –Ω–∞—Å—ã—â–µ–Ω–∏—é. –ù–∞–ø—Ä–∏–º–µ—Ä, OpenMathInstruct-2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª–∏—à—å 3.9% –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤–æ—Å—å–º–∏–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (test-time compute) –≤–≤–µ–ª–æ –Ω–æ–≤—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è LLM —É–ª—É—á—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –ë—ã–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –º–µ—Ç–æ–¥—ã –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ MCTS. –û–¥–Ω–∞–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–µ—Ç–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —É—Å–ø–µ—Ö–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —á–∞—Å—Ç–æ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å–∞–º–æ–π LLM –∏–ª–∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>\n<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è LLM –∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å OpenAI o1. –ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å. –ù–µ–¥–∞–≤–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ "—Å—É–¥—å–∏" –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Outcome Reward Model –∏ Process Reward Model (PRM). PRM –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–æ —Å–±–æ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π.</p>\n<p>–í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, –¥—Ä—É–≥–∏–µ –∏—Å—Å–ª–µ–¥—É—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é Monte Carlo Sampling –∏–ª–∏ MCTS. –û–¥–Ω–∞–∫–æ, –æ–Ω–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. rStar-Math –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM), –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤.</p>'}, {'title': 'Methodology', 'content': '3.1 Design Choices MCTS for Effective System 2 Reasoning. We aim to train math policy SLM and process reward model (PRM), and integrating both within Monte Carlo Tree Search (MCTS) for System 2 deep thinking. MCTS is chosen for two key reasons. First, it breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other 3 System 2 methods like Best-of-N [Brown et al., 2024] or self-consistency [Wang et al., 2023], which require generating full solutions in one inference. Second, the step-by-step generation in MCTS naturally yields step-level training data for both models. Standard MCTS rollout automatically assign Q-value to each step based on its contribution to the final correct answer, obviating the need for human-generated step-level annotations for process reward model training. Ideally, advanced LLMs such as GPT-4 could be integrated within MCTS to generate training data. However, this approach faces two key challenges. First, even these powerful models struggle to consistently solve difficult problems, such as Olympiad-level mathematics. Consequently, the resulting training data would primarily consist of simpler solvable problems, limiting its diversity and quality. Second, annotating per-step Q-values demands extensive MCTS rollouts; insufficient tree exploration can lead to spurious Q-value assignments, such as overestimating suboptimal steps. Given that each rollout involves multiple single-step generations and these models are computationally expensive, increasing rollouts significantly raises inference costs. Overview. To this end, we explore using two 7B SLMs (a policy SLM and PRM) to generate higherquality training data, with their smaller size allowing for extensive MCTS rollouts on accessible hardware (e.g., 440GB A100 GPUs). However, self-generating data presents greater challenges for SLMs, due to their weaker capabilities. SLMs frequently fail to generate correct solutions, and even when the final answer is correct, the intermediate steps are often flawed or of poor quality. Moreover, SLMs solve fewer challenging problems compared to advanced models like GPT-4. This section introduces our methodology, as illustrated in Fig. 1. To mitigate errors and low-quality intermediate steps, we introduce code-augmented CoT synthetic method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories, annotated with Q-values. To further improve SLM performance on challenging problems, we introduce four-round self-evolution recipe. In each round, both the policy SLM and the reward model are updated to stronger versions, progressively tackling more difficult problems and generating higher-quality training data. Finally, we present novel process reward model training approach that eliminates the need for precise per-step reward annotations, yielding the more effective process preference model (PPM). 3.2 Step-by-Step Verified Reasoning Trajectory We start by introducing our method for generating step-by-step verified reasoning trajectories with per-step Q-value annotations. Given problem and policy model , we run the standard MCTS to incrementally construct search tree for step-by-step solution exploration. As shown in Fig. 1(a), the root node represents question x, while child nodes correspond to intermediate steps generated by . root-to-leaf path ending at terminal node sd forms trajectory = s1 s2 ... sd, with each step si assigned Q-value Q(si). From the search tree , we extract solution trajectories = {t1, t2, ..., tn}(n 1). Our goal is to select high-quality trajectories from to construct the training set. For this purpose, we introduce code-augmented CoT synthesis method to filter out low-quality generations and perform extensive rollouts to improve the reliability of Q-value accuracy. Code-augmented CoT Generation. Prior MCTS approaches primarily generate natural language (NL) CoTs [Qi et al., 2024, Zhang et al., 2024a]. However, LLMs often suffer from hallucination, producing incorrect or irrelevant steps yet still arrive at the correct answer by chance [Lanham et al., 2023]. These flawed steps are challenging to detect and eliminate. To address this, we propose novel code execution augmented CoT. As shown in Fig. 2, the policy model generates one-step NL CoT alongside its corresponding Python code, where the NL CoT is embedded as Python comment. Only generations with successfully executed Python code are retained as valid candidates. Figure 2: An example of Code-augmented CoT. Specifically, starting from the initial root node x, we perform multiple MCTS iterations through selection, expansion, rollout, and back-propagation. At step i, we collect the latest reasoning trajectory s1 s2 ... si1 as the current state. Based on this state, we prompt (see Appendix A.3) the policy model to generate candidates si,0, ..., si,n1 for step i. Python code execution is then employed to filter valid nodes. As shown in Fig. 2, each generation si,j is concatenated with the code from all previous steps, forming s1 s2 ... si1 si,j. Candidates that execute successfully are retained as valid nodes and scored by the PPM, which assigns Q-value q(si). Then, we use the well-known Upper Confidence bounds for Trees (UCT) [Kocsis and Szepesv√°ri, 2006] to select the best node among the candidates. This selection process is mathematically represented as: UCT(s) = Q(s) + (cid:115) ln Nparent(s) (s) ; where Q(s) = q(s) (s) (1) where (s) denotes the number of visits to node s, and Nparent(s) is the visit count of ss parent node. The predicted reward q(s) is provided by the PPM and will be updated through back-propagation. is constant that balances exploitation and exploration. Extensive Rollouts for Q-value Annotation. Accurate Q-value Q(s) annotation in Eq. 1 is crucial for guiding MCTS node selection towards correct problem-solving paths and identifying high-quality steps within trajectories. To improve Q-value reliability, we draw inspiration from Go players, who retrospectively evaluate the reward of each move based on game outcomes. Although initial estimates may be imprecise, repeated gameplay refines these evaluations over time. Similarly, in each rollout, we update the Q-value of each step based on its contribution to achieving the correct final answer. After extensive MCTS rollouts, steps consistently leading to correct answers achieve higher Q-values, occasional successes yield moderate Q-values, and consistently incorrect steps receive low Q-values. Specifically, we introduce two self-annotation methods to obtain these step-level Q-values. Fig. 1(c) shows the detailed setting in the four rounds of self-evolution. Terminal-guided annotation. During the first two rounds, when the PPM is unavailable or insufficiently accurate, we use terminal-guided annotation. Formally, let q(si)k denote the value for step si after back-propagation in the kth rollout. Following AlphaGo [Silver et al., 2017] and rStar [Qi et al., 2024], we score each intermediate node based on its contribution to the final correct answer: q(si)k = q(si)k1 + q(sd)k; where the initial value q(si)0 = 0 in the first rollout. If this step frequently leads to correct answer, its value will increase; otherwise, it decreases. Terminal nodes are scored as q(sd) = 1 for correct answers and q(sd) = 1 otherwise, as shown in Fig. 1. (2) PRM-augmented annotation. Starting from the third round, we use PPM to score each step for more effective generation. Compared to terminal-guided annotation, which requires multiple rollouts for meaningful value, PPM directly predicts non-zero initial value. PPM-augmented MCTS also helps the policy model to generate higher-quality steps, guiding solutions towards correct paths. Formally, for step si, PPM predicts an initial q(si)0 value based on the partial trajectory: q(si)0 = (x s1 s2 ... si1 si) This value will be updated based on terminal nodes q(sd) value through MCTS back-propagation in Eq. 2. For terminal node sd, we do not use PRM for scoring during training data generation. Instead, we assign more accurate score based on ground truth labels as terminal-guided rewarding. (3) 3.3 Process Preference Model Process reward models, which provide granular step-level reward signals, is highly desirable for solving challenging math problems. However, obtaining high-quality step-level training data remains an open challenge. Existing methods rely on human annotations [Lightman et al., 2023] or MCTSgenerated scores [Zhang et al., 2024a, Chen et al., 2024] to assign score for each step. These scores then serve as training targets, with methods such as MSE loss [Chen et al., 2024] or pointwise loss [Wang et al., 2024c, Luo et al., 2024, Zhang et al., 2024a] used to minimize the difference between predicted and labeled scores. As result, the precision of these annotated step-level reward scores directly determines the effectiveness of the resulting process reward model. Unfortunately, precise per-step scoring remains unsolved challenge. Although our extensive MCTS rollouts improve the reliability of Q-values, precisely evaluating fine-grained step quality presents 5 major obstacle. For instance, among set of correct steps, it is difficult to rank them as best, secondbest, or average and then assign precise scores. Similarly, among incorrect steps, differentiating the worst from moderately poor steps poses analogous challenges. Even expert human annotation struggles with consistency, particularly at scale, leading to inherent noise in training labels. We introduce novel training method that trains process preference model (PPM) by constructing step-level positive-negative preference pairs. As shown in Fig. 1(b), instead of using Q-values as direct reward labels, we use them to select steps from MCTS tree for preference pair construction. For each step, we select two candidates with the highest Q-values as positive steps and two with the lowest as negative steps. Critically, the selected positive steps must lead to correct final answer, while negative steps must lead to incorrect answers. For intermediate steps (except the final answer step), the positive and negative pairs share the same preceding steps. For the final answer step, where identical reasoning trajectories rarely yield different final answers, we relax this restriction. We select two correct trajectories with the highest average Q-values as positive examples and two incorrect trajectories with the lowest average Q-values as negative examples. Following [Ouyang et al., 2022], we define our loss function using the standard Bradley-Terry model with pairwise ranking loss: Lppm(Œ∏) = when is not final answer step, ypos E(x,ypos = s1 ... si1 spos (5) Here, rŒ∏(x, yi) denotes the output of the PPM, where is the problem and is the trajectory from the first step to the ith step. D)[log(œÉ(rŒ∏(x, ypos ,yneg ; yneg = s1 ... si1 sneg ) rŒ∏(x, yneg )))] (4) i 1 2 2 3.4 Self-Evolved Deep Thinking 3.4.1 Training with Step-by-Step Verified Reasoning Trajectory Math Problems Collection. We collect large dataset of 747k math word problems with final answer ground-truth labels, primarily from NuminaMath [Jia LI and Polu, 2024a] and MetaMath [Yu et al., 2023b]. Notably, only competition-level problems (e.g., Olympiads and AIME/AMC) from NuminaMath are included, as we observe that grade-school-level problems do not significantly improve LLM complex math reasoning. To augment the limited competition-level problems, we follow [Li et al., 2024] and use GPT-4 to synthesize new problems based on the seed problems in 7.5k MATH train set and 3.6k AMC-AIME training split. However, GPT-4 often generated unsolvable problems or incorrect solutions for challenging seed problems. To filter these, we prompt GPT-4 to generate 10 solutions per problem, retaining only those with at least 3 consistent solutions. Reasoning Trajectories Collection. Instead of using the original solutions in the 747k math dataset, we conduct extensive MCTS rollouts (Sec. 3.2) to generate higher-quality step-by-step verified reasoning trajectories. In each self-evolution round, we perform 16 rollouts per math problem, which leads to 16 reasoning trajectories. Problems are then categories by difficulty based on the correct ratio of the generated trajectories: easy (all solutions are correct), medium (a mix of correct and incorrect solutions) and hard (all solutions are incorrect). For hard problems with no correct trajectories, an additional MCTS with 16 rollouts is performed. After that, all step-by-step trajectories and their annotated Q-values are collected and filtered to train the policy SLM and process preference model. Supervised Fine-tuning the Policy SLM. Through extensive experiments, we find that selecting high-quality reasoning trajectories is the key for fine-tuning frontier math LLM. While methods such as GPT-distillation and Best-of-N can include low-quality or erroneous intermediate steps, more effective approach ensures that every step in the trajectory is of high quality. To achieve this, we use per-step Q-values to select optimal trajectories from MCTS rollouts. Specifically, for each math problem, we select the top-2 trajectories with the highest average Q-values among those leading to correct answers as SFT training data. Training PPM. The PPM is initialized from the fine-tuned policy model, with its next-token prediction head replaced by scalar-value head consisting of linear layer and tanh function to constrain outputs to the range [-1, 1]. We filter out math problems where all solution trajectories are fully correct or incorrect. For problems with mixed outcomes, we select two positive and two negative examples for each step based on Q-values, which are used as preference pairs for training data. 3.4.2 Recipe for Self-Evolution Due to the weaker capabilities of SLMs, we perform four rounds of MCTS deep thinking to progressively generate higher-quality data and expand the training set with more challenging math problems. Table 2: Percentage of the 747k math problems correctly solved in each round. Only problems have correct solutions are included in the training set. The first round uses DeepSeek-Coder-Instruct as the policy LLM, while later rounds use our fine-tuned 7B policy SLM. # models in MCTS GSM-level MATH-level Olympiad-level All Round 1 DeepSeek-Coder-V2-Instruct Round 2 Round 3 Round policy SLM-r1 policy SLM-r2, PPM-r2 policy SLM-r3, PPM-r3 96.61% 97.88% 98.15% 98.15% 67.36% 67.40% 88.69% 94.53% 20.99% 56.04% 62.16% 80.58% 60.17% 66.60% 77.86% 90.25% Table 3: Pass@1 accuracy of the resulting policy SLM in each round, showing continuous improvement until surpassing the bootstrap model. Round# MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 DeepSeek-Coder-V2-Instruct (bootstrap model) Base (Qwen2.5-Math-7B) policy SLM-r1 policy SLM-r2 policy SLM-r3 policy SLM-r4 75.3 58.8 69.6 73.6 75.8 78. 13.3 0.0 3.3 10.0 16.7 26.7 57.5 22.5 30.0 35.0 45.0 47.5 37.6 21.8 34.7 39.0 44.1 47. 46.2 41.6 44.5 45.7 49.6 52.5 94.9 91.6 88.4 89.1 89.3 89.7 64.7 51.7 57.4 59.7 62.8 65. Each round uses MCTS to generate step-by-step verified reasoning trajectories, which are then used to train the new policy SLM and PPM. The new models are then applied in next round to generate higher-quality training data. Fig. 1(c) and Table 2 detail the models used for data generation in each round, along with the identifiers of the trained policy model and PPM. Next, we outline the details and specific improvements targeted in each round. Round 1: Bootstrapping an initial strong policy SLM-r1. To enable SLMs to self-generate reasonably good training data, we perform bootstrap round to fine-tune an initial strong policy model, denoted as SLM-r1. As shown in Table 2, we run MCTS with DeepSeek-Coder-V2-Instruct (236B) to collect the SFT data. With no available reward model in this round, we use terminal-guided annotation for Q-values and limit MCTS to 8 rollouts for efficiency. For correct solutions, the top-2 trajectories with the highest average Q-values are selected as SFT data. We also train PPM-r1, but the limited rollouts yields unreliable Q-values, affecting the effectiveness of PPM-r1 ( Table 4). Round 2: Training reliable PPM-r2. In this round, with the policy model updated to the 7B SLM-r1, we conduct extensive MCTS rollouts for more reliable Q-value annotation and train the first reliable reward model, PPM-r2. Specifically, we perform 16 MCTS rollouts per problem. The resulting step-by-step verified reasoning trajectories show significant improvements in both quality and Q-value precision. As shown in Table 4, PPM-r2 is notably more effective than in the bootstrap round. Moreover, the policy SLM-r2 also continues to improve as expected  (Table 3)  . Round 3: PPM-augmented MCTS to significantly improve data quality. With the reliable PPM-r2, we perform PPM-augmented MCTS in this round to generate data, leading to significantly higher-quality trajectories that cover more math and Olympiad-level problems in the training set  (Table 2)  . The generated reasoning trajectories and self-annotated Q-values are then used to train the new policy SLM-r3 and PPM-r3, both of which show significant improvements. Round 4: Solving challenging math problems. After the third round, while grade school and MATH problems achieve high success rates, only 62.16% of Olympiad-level problems are included in the training set. This is NOT solely due to weak reasoning abilities in our SLMs, as many Olympiad problems remain unsolved by GPT-4 or o1. To improve coverage, we adopt straightforward strategy. For unsolved problems after 16 MCTS rollouts, we perform an additional 64 rollouts, and if needed, increase to 128. We also conduct multiple MCTS tree expansions with different random seeds. This boosts the success rate of Olympiad-level problems to 80.58%. After four rounds of self-evolution, 90.25% of the 747k math problems are successfully covered into the training set, as shown in Table 2. Among the remaining unsolved problems, significant portion consists of synthetic questions. We manually review random sample of 20 problems and find that 19 are incorrectly labeled with wrong answers. Based on this, we conclude that the remaining unsolved problems are of low quality and thus terminate the self-evolution at round 4. 7 Table 4: The quality of PPM consistently improves across rounds. The policy model has been fixed with policy SLM-r1 for fair comparison. Round# MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 PPM-r1 PPM-r2 PPM-r3 PPM-r4 75.2 84.1 85.2 87. 10.0 26.7 33.3 43.3 57.5 75.0 77.5 77.5 35.7 52.7 59.5 61.5 45.4 54.2 55.6 56.8 90.9 93.3 93.9 94.2 60.3 73.0 76.6 77.', 'summary': '<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 3.1-3.3 —Å—Ç–∞—Ç—å–∏ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ MCTS –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á</h2>\n<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (MCTS) –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –¥–≤—É–º—è –º–æ–¥–µ–ª—è–º–∏: –ø–æ–ª–∏—Ç–∏–∫–æ–π (SLM) –∏ –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PRM). MCTS –≤—ã–±—Ä–∞–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ –¥–≤—É–º –ø—Ä–∏—á–∏–Ω–∞–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Best-of-N –∏–ª–∏ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –ø–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ MCTS –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–º—É —à–∞–≥—É, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –µ–≥–æ –≤–∫–ª–∞–¥–µ –≤ –∏—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, —á—Ç–æ –∏–∑–±–∞–≤–ª—è–µ—Ç –æ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>\n<p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö MCTS —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –¥–≤—É–º—è –ø—Ä–æ–±–ª–µ–º–∞–º–∏. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–∂–µ —Ç–∞–∫–∏–µ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ –º–æ–≥—É—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö, —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS; –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ª–æ–∂–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤. –£—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –∫–∞–∂–¥—ã–π –ø—Ä–æ–≥–æ–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π, –∞ LLM —è–≤–ª—è—é—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º–∏, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.</p>\n<p>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–≤–µ 7B –º–æ–¥–µ–ª–∏ SLM (–æ–¥–Ω–∞ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –æ–¥–Ω–∞ –¥–ª—è PRM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏. –û–¥–Ω–∞–∫–æ, —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–æ–ª—å—à—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è SLM –∏–∑-–∑–∞ –∏—Ö –º–µ–Ω—å—à–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. SLM —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∏ –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç –æ—à–∏–±–æ—á–Ω—ã–º–∏ –∏–ª–∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.</p>\n<p>–î–ª—è —Å–º—è–≥—á–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—à–∞–≥–æ–≤—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏. –î–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ SLM –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ä–µ—Ü–µ–ø—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞—É–Ω–¥–∞. –í –∫–∞–∂–¥–æ–º —Ä–∞—É–Ω–¥–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–µ—à–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.</p>\n<p>–ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–ø—É—Å–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ MCTS –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π. –ö–æ—Ä–Ω–µ–≤–æ–π —É–∑–µ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–æ–ø—Ä–æ—Å, –∞ –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —à–∞–≥–∞–º, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏. –ü—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –¥–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —É–∑–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é, –∫–∞–∂–¥–æ–º—É —à–∞–≥—É –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π.</p>\n<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ CoT, –∑–¥–µ—Å—å –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–π CoT –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤–º–µ—Å—Ç–µ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º –Ω–∞ Python, –≥–¥–µ CoT –≤—Å—Ç—Ä–æ–µ–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è. –°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ–¥ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É —Å–ª—É—á–∞–π–Ω–æ. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ MCTS, –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞. –û—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª—å—é PRM, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –ó–∞—Ç–µ–º, —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ UCT –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∏–π —É–∑–µ–ª.</p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –∏–≥—Ä–æ–∫–∞–º –≤ –ì–æ, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–∞ –∏–≥—Ä—ã. –ü–æ—Å–ª–µ –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS, —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è, –∞ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–≤–µ—Ä–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –Ω–∏–∑–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –º–µ—Ç–æ–¥–∞ —Å–∞–º–æ–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö —Ä–∞—É–Ω–¥–∞—Ö, –∫–æ–≥–¥–∞ PRM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞) –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è PRM (–Ω–∞—á–∏–Ω–∞—è —Å —Ç—Ä–µ—Ç—å–µ–≥–æ —Ä–∞—É–Ω–¥–∞). –í –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ —à–∞–≥–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, –ø—Ä–∏–≤–µ–ª –ª–∏ –æ–Ω –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É –≤ –∫–æ–Ω—Ü–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ PRM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è.</p>\n<p>–ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤, –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –û–¥–Ω–∞–∫–æ, –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∏—Ö –æ—Å—Ç–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É –∏–ª–∏ –Ω–∞ –æ—Ü–µ–Ω–∫–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ MCTS. –¢–æ—á–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (PPM), –∫–æ—Ç–æ—Ä—ã–π –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä—è–º—ã—Ö –º–µ—Ç–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –≤—ã–±–æ—Ä–∞ —à–∞–≥–æ–≤ –∏–∑ –¥–µ—Ä–µ–≤–∞ MCTS –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–∞—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –¥–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å —Å–∞–º—ã–º–∏ –≤—ã—Å–æ–∫–∏–º–∏ Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ –∏ –¥–≤–∞ —Å —Å–∞–º—ã–º–∏ –Ω–∏–∑–∫–∏–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ó–∞—Ç–µ–º PPM –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>'}, {'title': 'Evaluation', 'content': '4.1 Setup Evaluation Datasets. We evaluate rStar-Math on diverse mathematical benchmarks. In addition to the widely-used GSM8K [Cobbe et al., 2021], we include challenging benchmarks from multiple domains: (i) competition and Olympiad-level benchmarks, such as MATH-500 [Lightman et al., 2023], AIME 2024 [AI-MO, 2024a], AMC 2023 [AI-MO, 2024b] and Olympiad Bench [He et al., 2024]. Specifically, AIME is the exams designed to challenge the brightest high school math students in American, with the 2024 dataset comprising 30 problems from AIME and II exams; (ii) collegelevel math problems from College Math [Tang et al., 2024] and (iii) out-of-domain math benchmark: GaoKao (Chinese College Entrance Exam) En 2023 [Liao et al., 2024]. Base Models and Setup. rStar-Math is general approach applicable to various LLMs. To show its effectiveness and generalizability, we use SLMs of different sizes as the base policy models: Qwen2.5-Math-1.5B [Qwen, 2024b], Phi3-mini-Instruct (3B) [Microsoft, 2024, Abdin et al., 2024], Qwen2-Math-7B [Qwen, 2024a] and Qwen2.5-Math-7B [Qwen, 2024c]. Among these, Phi3-miniInstruct is general-purpose SLM without specialization in math reasoning. Due to limited GPU resources, we performed 4 rounds of self-evolution exclusively on Qwen2.5Math-7B, yielding 4 evolved policy SLMs  (Table 3)  and 4 PPMs  (Table 4)  . For the other 3 policy LLMs, we fine-tune them using step-by-step verified trajectories generated from Qwen2.5-Math-7Bs 4th round. The final PPM from this round is then used as the reward model for the 3 policy SLMs. Baselines. rStar-Math is System 2 method. We compare it against three strong baselines representing both System 1 and System 2 approaches: (i) Frontier LLMs, including GPT-4o, the latest Claude, OpenAI o1-preview and o1-mini. We measure their accuracy on AMC 2023, Olympiad Bench, College Math, Gaokao and GSM8K, with accuracy numbers for other benchmarks are taken from public technical reports [Team, 2024a]. (ii) Open-sourced superior reasoning models, including DeepSeek-Coder-v2-Instruct, Mathstral [Team, 2024b], NuminaMath-72B [Jia LI and Polu, 2024a], and LLaMA3.1 [Dubey et al., 2024], which represent the current mainstream System 1 approaches for improving LLM math reasoning. (iii) Both System 1 and System 2 performance of the base models trained from the original models teams, including Instruct versions (e.g., Qwen2.5-Math-7B-Instruct) and Best-of-N (e.g., Qwen2.5-Math-72B-Instruct+Qwen2.5-Math-RM-72B). Notably, the reward model used for the three Qwen base models is 72B ORM, significantly larger than our 7B PPM. Evaluation Metric. We report Pass@1 accuracy for all baselines. For System 2 baselines, we use default evaluation settings, such as default thinking time for o1-mini and o1-preview. For Qwen models with Best-of-N, we re-evaluate MATH-500, AIME/AMC accuracy; other benchmarks results are from their technical reports. For fair comparison, rStar-Math run MCTS to generate the same number of solutions as Qwen. Specifically, for AIME/AMC, we generate 16 trajectories for AIME/AMC and 8 for other benchmarks, using PPM to select the best solution. We also report performance with increased test-time computation using 64 trajectories, denoted as rStar-Math64. 4.2 Main Results Results on diverse challenging math benchmarks. Table 5 shows the results of rStar-Math with comparing to state-of-the-art reasoning models. We highlight three key observations: (1) rStar-Math significantly improves SLMs math reasoning capabilities, achieving performance comparable to or surpassing OpenAI o1 with substantially smaller model size (1.5B-7B). For example, Qwen2.5Math-7B, originally at 58.8% accuracy on MATH, improved dramatically to 90.0% with rStar-Math, outperforming o1-preview and Claude 3.5 Sonnet while matching o1-mini. On the College Math benchmark, rStar-Math exceeds o1-mini by 2.7%. On AIME 2024, rStar-Math scored 53.3%, ranking just below o1-mini, with the 7B model solving 8/15 problems in both AIME and II, placing in the top 20% of the brightest high school math students. Notably, 8 of the unsolved problems were 8 Table 5: The results of rStar-Math and other frontier LLMs on the most challenging math benchmarks. rStar-Math64 shows the Pass@1 accuracy achieved when sampling 64 trajectories. Competition and College Level Method MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K OOD Gaokao En 2023 Model Frontier LLMs GPT-4o Claude3.5-Sonnet GPT-o1-preview GPT-o1-mini Open-Sourced Reasoning LLMs System 1 DeepSeek-Coder-V2-Instruct System 1 Mathstral-7B-v0.1 System 1 NuminaMath-72B-CoT System 1 LLaMA3.1-8B-Instruct System 1 LLaMA3.1-70B-Instruct Qwen2.5-Math-72B-Instruct System 1 Qwen2.5-Math-72B-Instruct+72B ORM System System 1 System 1 - - 76.6 78.3 85.5 90.0 75.3 57.8 64.0 51.4 65.4 85.6 85.8 9.3 16.0 44.6 56.7 13.3 0.0 3.3 6.7 23.3 30.0 36.7 47.5 - 90.0 95. 57.5 37.5 70.0 25.0 50.0 70.0 72.5 43.3 - - 65.3 37.6 21.5 32.6 15.4 27.7 49.0 54.5 Phi3-mini-Instruct (base model) rStar-Math (3.8B SLM+7B PPM) rStar-Math64 (3.8B SLM+7B PPM) System 1 System 2 System 2 41.4 85.4 86. 3.33 40.0 43.3 7.5 77.5 80.0 12.3 59.3 60.3 General Base Model: Phi3-mini-Instruct (3.8B) System 1 Qwen2.5-Math-1.5B (base model) Qwen2.5-Math-1.5B-Instruct System 1 Qwen2.5-Math-1.5B-Instruct+72B ORM System 2 rStar-Math (1.5B SLM+7B PPM) System 2 rStar-Math64 (1.5B SLM+7B PPM) System 2 Math-Specialized Base Model: Qwen2.5-Math-1.5B 16.7 38.1 47.3 63.5 64. 51.2 60.0 83.4 87.8 88.6 22.5 60.0 72.5 80.0 85.0 0.0 10.0 20.0 46.7 46.7 Math-Specialized Base Model: Qwen2-Math-7B Qwen2-Math-7B (base model) Qwen2-Math-7B-Instruct Qwen2-Math-7B-Instruct+72B ORM rStar-Math (7B SLM+7B PPM) rStar-Math64 (7B SLM+7B PPM) System 1 System 1 System 2 System 2 System 53.4 73.2 83.4 88.2 88.6 3.3 13.3 23.3 43.3 46.7 25.0 62.5 62.5 80.0 85.0 17.3 38.2 47.6 63.1 63.4 System 1 Qwen2.5-Math-7B (base model) Qwen2.5-Math-7B-Instruct System 1 Qwen2.5-Math-7B-Instruct+72B ORM System 2 rStar-Math (7B SLM+7B PPM) System 2 rStar-Math64 (7B SLM+7B PPM) System 2 Math-Specialized Base Model: Qwen2.5-Math-7B 21.8 41.6 49.9 65.3 65. 58.8 82.6 88.4 89.4 90.0 22.5 62.5 75.0 87.5 87.5 0.0 6.0 26.7 50.0 53.3 48.5 - - 57.8 46.2 33.7 39.7 33.8 42.5 49.5 50.6 33.1 58.0 59. 38.4 47.7 50.2 59.0 59.3 39.4 45.9 47.9 58.4 59.3 41.6 46.8 49.6 59.0 60.5 92.9 96.4 - 94.8 94.9 84.9 90.8 76.6 94.1 95.9 96.4 85.7 94.5 94. 74.6 84.8 94.1 94.3 94.8 80.4 89.9 95.1 94.6 94.8 91.6 95.2 97.9 95.0 95.2 67.5 - - 78.4 64.7 46.0 58.4 38.4 54.0 71.9 76.9 37.1 77.1 77. 46.5 65.5 73.0 77.7 79.5 47.3 62.1 71.9 78.2 79.2 51.7 66.8 75.1 80.5 81.3 geometry-based, requiring visual understanding, capability rStar-Mathcurrently does not support. (2) Despite using smaller policy models (1.5B-7B) and reward models (7B), rStar-Math significantly outperforms state-of-the-art System 2 baselines. Compared to Qwen Best-of-N baselines, which use the same base models (Qwen2-Math-7B, Qwen2.5-Math-1.5B/7B) but 10 larger reward model (Qwen2.5-Math-RM-72B), rStar-Math consistently improves the reasoning accuracy of all base models to state-of-the-art levels. Even against Best-of-N with 10 larger Qwen2.5-Math-72BInstruct policy model, rStar-Math surpasses it on all benchmarks except GSM8K, using the same number of sampled solutions. (3) Beyond well-known benchmarks like MATH, GSM8K, and AIME, which may risk over-optimization, rStar-Math shows strong generalizability on other challenging math benchmarks, including Olympiad Bench, College Math, and the Chinese College Entrance Math Exam (Gaokao), setting new state-of-the-art scores. As discussed in Sec. 3.4, our training set is primarily sourced from public datasets, with no specific optimizations for these benchmarks. Scaling up test-time computation. rStar-Math uses MCTS to augment the policy model, searching solutions guided by the PPM. By increasing test-time computation, it explores more trajectories, potentially improving performance. In Fig. 3, we show the impact of test-time compute scaling by comparing the accuracy of the official Qwen Best-of-N across different numbers of sampled trajectories on four challenging math benchmarks. Sampling only one trajectory corresponds to the policy LLMs Pass@1 accuracy, indicating fallback to System 1 reasoning. We highlight two key Figure 3: Reasoning performance under scaling up the test-time compute. observations: (1) With only 4 trajectories, rStar-Math significantly outperforms Best-of-N baselines, exceeding o1-preview and approaching o1-mini, demonstrating its effectiveness. (2) Scaling test-time compute improves reasoning accuracy across all benchmarks, though with varying trends. On Math, AIME, and Olympiad Bench, rStar-Math shows saturation or slow improvement at 64 trajectories, while on College Math, performance continues to improve steadily. 4.3 Ablation Study and Analysis We ablate the effectiveness of our three innovations. For System 2-style inference, Pass@1 accuracy is measured with 16 trajectories for AIME and AMC, and 8 for other benchmarks. Table 6: The continuously improved math reasoning capabilities through rStar-Math self-evolved deep thinking. Starting from round 2, the 7B base model powered by rStar-Math surpasses GPT-4o. Round# GPT-4o Base 7B model rStar-Math Round 1 rStar-Math Round 2 rStar-Math Round 3 rStar-Math Round 4 MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 76.6 58.8 75.2 86.6 87.0 89.4 9. 0.0 10.0 43.3 46.7 50.0 47.5 22.5 57.5 75.0 80.0 87.5 43.3 21.8 35.7 59.4 61.6 65.3 48. 41.6 45.4 55.6 56.5 59.0 92.9 91.6 90.9 94.0 94.2 95.0 67.5 51.7 60.3 76.4 77.1 80.5 The effectiveness of self-evolution. The impressive results in Table 5 are achieved after 4 rounds of rStar-Math self-evolved deep thinking. Table 6 shows the math reasoning performance in each round, demonstrating continuous improvement in accuracy. In round 1, the main improvement comes from applying SFT to the base model. Round 2 brings significant boost with the application of stronger PPM in MCTS, which unlocks the full potential of System 2 deep reasoning. Notably, starting from round 2, rStar-Math outperforms GPT-4o. Rounds 3 and 4 show further improvements, driven by stronger System 2 reasoning through better policy SLMs and PPMs. The effectiveness of step-by-step verified reasoning trajectory. rStar-Math generates step-by-step verified reasoning trajectories, which eliminate error intermediate steps and further expand training set with more challenging problems. To evaluate its effectiveness, we use the data generated from round 4 as SFT training data and compare it against three strong baselines: (i) GPT-distillation, which includes open-sourced CoT solutions synthesized using GPT-4, such as MetaMath [Yu et al., 2023b], NuminaMath-CoT [Jia LI and Polu, 2024b]; (ii) Random sampling from self-generation, which use the same policy model (i.e., policy SLM-r3) to randomly generate trajectories; (iii) Rejection sampling, where 32 trajectories are randomly sampled from the policy model, with high-quality solutions ranked by our trained ORM (appendix A.1). For fairness, we select two correct trajectories for each math problem in baseline (ii) and (iii). All SFT experiments use the same training recipe. Table 7 shows the math reasoning accuracy of Qwen2.5-Math-7B fine-tuned on different datasets. We highlight two observations: (i) Fine-tuning with our step-by-step verified trajectories significantly outperforms all other baselines. This is primarily due to our PPM-augmented MCTS for code-augmented CoT synthesis, which provides denser verification during math solution generation. It proves more effective than both random sampling, which lacks verification, and rejection sampling, 10 Table 7: Ablation study on the effectiveness of our step-by-step verified reasoning trajectories as the SFT dataset. We report the SFT accuracy of Qwen2.5-Math-7B fine-tuned with different datasets. Dataset MATH AIME AMC Olympiad Bench College Math GSM8K GaokaoEn GPT-4o - GPT4-distillation (Open-sourced) MetaMath NuminaMath-CoT 76.6 55.2 69. Self-generation by policy SLM-r3 Random sample Rejection sampling 72.4 73.4 Step-by-step verified (ours) 78.4 9.3 3.33 10.0 10.0 13.3 26. 47.5 32.5 50.0 45.0 47.5 47.5 43.3 19.1 37.2 41.0 44.7 47. 48.5 39.2 43.4 48.0 50.8 52.5 92.9 85.1 89.8 87.5 89.3 89. 67.5 43.6 59.5 57.1 61.7 65.7 where ORM provides only sparse verification. (ii) Even randomly sampled code-augmented CoT solutions from our SLM yields comparable or better performance than GPT-4 synthesized NuminaMath and MetaMath datasets. This indicates that our policy SLMs, after rounds of self-evolution, can generate high-quality math solutions. These results demonstrates the huge potential of our method to self-generate higher-quality reasoning data without relying on advanced LLM distillation. The effectiveness of PPM. We train both strong ORM and Q-value score-based PRM (PQM) for comparison. To ensure fair evaluation, we use the highest-quality training data: the step-by-step verified trajectories generated in round 4, with selected math problems matching those used for PPM training. Similar to PPM, we use step-level Q-values as to select positive and negative trajectories for each math problem. The ORM is trained using pairwise ranking loss [Ouyang et al., 2022], while the PQM follows [Chen et al., 2024, Zhang et al., 2024a] to use Q-values as reward labels and optimize with MSE loss. Detailed training settings are provided in Appendix A.1. Table 8: Ablation study on the reward model. Process reward models (PQM and PPM) outperform ORM, with PPM pushing the frontier of math reasoning capabilities. RM Inference MATH AIME AMC Olympiad Bench College Math GSM8K GaokaoEn o1-mini - ORM Best-of-N PQM PPM MCTS MCTS 90.0 82.6 88.2 89.4 56.7 26.7 46.7 50.0 95.0 65.0 85.0 87. 65.3 55.1 62.9 65.3 55.6 55.5 57.6 59.0 94.8 92.3 94.6 95. 78.6 72.5 79.5 80.5 Table 8 compares the performance of ORM, PQM, and PPM for System 2 reasoning using our final round policy model. ORM provides reward signals only at the end of problem solving, so we use the Best-of-N method, while PRM and PPM leverage MCTS-driven search. As shown in Table 8, both PQM and PPM outperform ORM by providing denser step-level reward signals, leading to higher accuracy on complex math reasoning tasks. However, PQM struggles on more challenging benchmarks, such as MATH and Olympiad Bench, due to the inherent imprecision of Q-values. In contrast, PPM constructs step-level preference data for training, enabling our 7B policy model to achieve comparable or superior performance to o1-mini across all benchmarks.', 'summary': '<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 4.1 "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö" –∏ 4.2 "–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã" —Å—Ç–∞—Ç—å–∏ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ rStar-Math, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.</p>\n<p><strong>4.1 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ rStar-Math –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –≤–∫–ª—é—á–∞—è:</p>\n<ul>\n<li><strong>GSM8K:</strong> —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li><strong>–ó–∞–¥–∞—á–∏ —É—Ä–æ–≤–Ω—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –æ–ª–∏–º–ø–∏–∞–¥:</strong><ul>\n<li><strong>MATH-500:</strong> –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á.</li>\n<li><strong>AIME 2024:</strong> –∑–∞–¥–∞—á–∏, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ª—É—á—à–∏—Ö —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤ –°–®–ê.</li>\n<li><strong>AMC 2023:</strong> –∑–∞–¥–∞—á–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã.</li>\n<li><strong>Olympiad Bench:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è.</li>\n</ul>\n</li>\n<li><strong>–ó–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ —É—Ä–æ–≤–Ω—è –∫–æ–ª–ª–µ–¥–∂–∞:</strong><ul>\n<li><strong>College Math:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏—Ö –∫—É—Ä—Å–æ–≤.</li>\n</ul>\n</li>\n<li><strong>–ó–∞–¥–∞—á–∏ –∏–∑ –¥—Ä—É–≥–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏:</strong><ul>\n<li><strong>GaoKao (–∫–∏—Ç–∞–π—Å–∫–∏–π –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω –≤ –∫–æ–ª–ª–µ–¥–∂) En 2023:</strong> –∑–∞–¥–∞—á–∏ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ —ç–∫–∑–∞–º–µ–Ω–∞.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</strong></p>\n<p>rStar-Math —è–≤–ª—è–µ—Ç—Å—è –æ–±—â–∏–º –ø–æ–¥—Ö–æ–¥–æ–º, –ø—Ä–∏–º–µ–Ω–∏–º—ã–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM). –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤:</p>\n<ul>\n<li>Qwen2.5-Math-1.5B</li>\n<li>Phi3-mini-Instruct (3B) - –æ–±—â–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li>Qwen2-Math-7B</li>\n<li>Qwen2.5-Math-7B</li>\n</ul>\n<p>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ 4 —Ä–∞—É–Ω–¥–∞ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–≥–æ –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ 4 —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–≤—à–∏—Ö –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (policy SLM) –∏ 4 –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è (PPM). –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Qwen2.5-Math-7B –Ω–∞ 4 —Ä–∞—É–Ω–¥–µ. –ò—Ç–æ–≥–æ–≤–∞—è PPM –∏–∑ —ç—Ç–æ–≥–æ —Ä–∞—É–Ω–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è –¥–ª—è —ç—Ç–∏—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏.</p>\n<p><strong>–ë–∞–∑–æ–≤—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è</strong></p>\n<p>rStar-Math –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ—Ç–æ–¥–∞–º "–°–∏—Å—Ç–µ–º—ã 2" (—Ç—Ä–µ–±—É—é—â–∏–º –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è). –û–Ω —Å—Ä–∞–≤–Ω–∏–≤–∞–ª—Å—è —Å —Ç—Ä–µ–º—è —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–º–∏ –∫–∞–∫ "–°–∏—Å—Ç–µ–º—É 1" (–±—ã—Å—Ç—Ä–æ–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ), —Ç–∞–∫ –∏ "–°–∏—Å—Ç–µ–º—É 2":</p>\n<ul>\n<li><strong>–ü–µ—Ä–µ–¥–æ–≤—ã–µ LLM:</strong> GPT-4o, Claude, OpenAI o1-preview –∏ o1-mini.</li>\n<li><strong>–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:</strong> DeepSeek-Coder-v2-Instruct, Mathstral, NuminaMath-72B –∏ LLaMA3.1.</li>\n<li><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥–∞–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤:</strong> –≤–∫–ª—é—á–∞—è –≤–µ—Ä—Å–∏–∏ Instruct (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-7B-Instruct) –∏ Best-of-N (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-72B-Instruct+Qwen2.5-Math-RM-72B).</li>\n</ul>\n<p><strong>–ú–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏</strong></p>\n<p>–î–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–µ—Ç—Ä–∏–∫–∞ Pass@1 (–¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏). –î–ª—è –º–æ–¥–µ–ª–µ–π "–°–∏—Å—Ç–µ–º—ã 2" –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤—Ä–µ–º—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –¥–ª—è o1-mini –∏ o1-preview. –î–ª—è –º–æ–¥–µ–ª–µ–π Qwen Best-of-N –±—ã–ª–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ MATH-500 –∏ AIME/AMC. –î–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è rStar-Math –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç–∞–∫–æ–µ –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π, –∫–∞–∫ –∏ Qwen. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è AIME/AMC –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å 16 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ - 8. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å PPM –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏), –æ–±–æ–∑–Ω–∞—á–µ–Ω–Ω–∞—è –∫–∞–∫ rStar-Math64.</p>\n<p><strong>4.2 –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</strong></p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ 5 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã rStar-Math –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ SLM</strong>, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π OpenAI o1, –ø—Ä–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—å—à–µ–º —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ (1.5B-7B). –ù–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5Math-7B, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–≤—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å 58.8% –Ω–∞ MATH, —É–ª—É—á—à–∏–ª–∞—Å—å –¥–æ 90.0% —Å rStar-Math, –ø—Ä–µ–≤–∑–æ–π–¥—è o1-preview –∏ Claude 3.5 Sonnet –∏ –¥–æ—Å—Ç–∏–≥–Ω—É–≤ —É—Ä–æ–≤–Ω—è o1-mini. –ù–∞ College Math rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç o1-mini –Ω–∞ 2.7%. –ù–∞ AIME 2024 rStar-Math –Ω–∞–±—Ä–∞–ª 53.3%, —É—Å—Ç—É–ø–∏–≤ —Ç–æ–ª—å–∫–æ o1-mini.</li>\n<li><strong>rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç "–°–∏—Å—Ç–µ–º—É 2"</strong>, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–Ω—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (1.5B-7B) –∏ –º–æ–¥–µ–ª–µ–π –ø–æ–æ—â—Ä–µ–Ω–∏—è (7B). –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen Best-of-N, rStar-Math –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–∞–∂–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Best-of-N —Å 72B –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏ rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–µ –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫—Ä–æ–º–µ GSM8K.</li>\n<li><strong>rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö</strong>, –≤–∫–ª—é—á–∞—è Olympiad Bench, College Math –∏ Gaokao, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Ä–µ–∫–æ—Ä–¥—ã.</li>\n</ol>\n<p><strong>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</strong></p>\n<p>rStar-Math –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—è –ø–æ–∏—Å–∫ —Ä–µ—à–µ–Ω–∏–π –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º PPM. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>–£–∂–µ —Å 4 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Best-of-N</strong>, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-mini.</li>\n<li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π</strong> –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ö–æ—Ç—è –∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–ø–∞–º–∏. –ù–∞ Math, AIME –∏ Olympiad Bench rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—Å—ã—â–µ–Ω–∏–µ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏ 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞ College Math –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è.</li>\n</ol>'}, {'title': 'Findings and Discussions', 'content': 'The emergence of intrinsic self-reflection capability. key breakthrough in OpenAI o1 is its intrinsic self-reflection capability. When the model makes an error, it recognizes the mistake and can self-correct with correct answer [Noam Brown and Lightman, 2024]. Yet it has consistently been found to be largely ineffective in open-sourced LLMs. The community has actively explored various approaches, including self-correction [Huang et al., 2023, Kumar et al., 2024], self-reflection [Renze and Guven, 2024, Shinn et al., 2024], to explicitly train or prompt LLMs to develop such capability. In our experiments, we unexpectedly observe that our MCTS-driven deep thinking exhibits selfreflection during problem-solving. As shown in Fig. 4, the model initially formalizes an equation using SymPy in the first three steps, which would lead to an incorrect answer (left branch). Interestingly, in the fourth step (right branch), the policy model recognizes the low quality of its earlier steps and refrains from continuing along the initial problem-solving path. Instead, it backtracks and resolves the problem using new, simpler approach, ultimately arriving at the correct answer. An additional example of self-correction is provided in AppendixA.2. Notably, no self-reflection training data or prompt was included, suggesting that advanced System 2 reasoning can foster intrinsic self-reflection. 11 Figure 4: An example of intrinsic self-reflection during rStar-Math deep thinking. Figure 5: Pass@1 accuracy of policy models and their accuracy after applying System 2 reasoning with various reward models, shows that reward models primarily determine the final performance. PPM shapes the reasoning boundary in System 2 deep thinking. Both the policy and reward models are crucial for System 2 deep reasoning. Our experiments show that once the policy model attains reasonably strong capability level, (see Appendix A.1 ), the PPM becomes the key determinant of the upper performance limit. Fig. 5 summarizes the accuracy of policy models of different sizes, as well as the improvements achieved with reward models. Despite variations in Pass@1 accuracy due to differences in training strategies, datasets, and model scales, the reward model proves to be the dominant factor in System 2 reasoning. For instance, although the SFT accuracy of rStar-Math-7B is lower than Qwen2.5-Math-72B-Instruct, pairing it with our 7B PPM allows rStar-Math to outperform the 72B policy model with Qwen 72B ORM. Moreover, despite varying Pass@1 accuracy across our three policy SLM sizes, the final reasoning accuracy converges after applying the PPM. PPM spots theorem-application steps. When solving challenging math problems, identifying and applying relevant theorems or key conclusions often form the cornerstone of successful problemsolving [Xin et al., 2024]. In our experiments, we find that during rStar-Math problem-solving, our PPM effectively identifies critical theorem-application intermediate steps within policy models deep thinking process. These steps are predicted with high reward scores, guiding the policy model to generate the correct solution. Appendix A.2 provides examples where the PPM successfully identifies key theorems such as Fermats little theorem [Weisstein, a], Vietas formulas [Weisstein, b], the AM-GM inequality [amg], the Pythagorean theorem [pyt], and the Shoelace Theorem [sho], etc. 12 Generalization discussions. rStar-Math offers general methodology for improving LLM reasoning applicable to various domains. First, rStar-Math can generalize to more challenging math tasks, such as theorem proving, though its current focus is on word problems due to dataset limitations. Nonetheless, rStar-Math demonstrates the potential to prove mathematical statements. As shown in Appendix A.2, it successfully proves an Olympiad-level problem involving Fermats Little Theorem, providing step-by-step correct proof through its deep reasoning process. Second, rStar-Mathcan generalize to other domains, such as code and commonsense reasoning. Notably, synthesizing stepby-step verified training trajectories for general reasoning requires mechanism to provide feedback on whether given trajectory reaches the desired output at the end of MCTS rollout. For instance, in code reasoning, this could involve designing extensive test cases; in general reasoning, feedback could be obtained through human labeling or mutual verification with another LLM [Qi et al., 2024].', 'summary': '<p><strong>–í–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏</strong></p>\n<p>–ö–ª—é—á–µ–≤—ã–º –ø—Ä–æ—Ä—ã–≤–æ–º –≤ –º–æ–¥–µ–ª–∏ OpenAI o1 —è–≤–ª—è–µ—Ç—Å—è –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–ø—É—Å–∫–∞–µ—Ç –æ—à–∏–±–∫—É, –æ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –µ—ë –∏ –º–æ–∂–µ—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –≤—ã–¥–∞–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –û–¥–Ω–∞–∫–æ, –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM (–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö) —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π. –°–æ–æ–±—â–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—é –∏ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∏–ª–∏ –ø–æ–±—É–¥–∏—Ç—å LLM –∫ —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.</p>\n<p>–í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ, –±—ã–ª–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ MCTS (–º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–µ—Ä–µ–≤–∞), –ø—Ä–æ—è–≤–ª—è–µ—Ç —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ, –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–º –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 4, –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è SymPy, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–≤–µ—Ä–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É (–ª–µ–≤–∞—è –≤–µ—Ç–≤—å). –û–¥–Ω–∞–∫–æ, –Ω–∞ —á–µ—Ç–≤—ë—Ä—Ç–æ–º —à–∞–≥–µ (–ø—Ä–∞–≤–∞—è –≤–µ—Ç–≤—å), –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤ –∏ –æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –ø–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º—É –ø—É—Ç–∏. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞–∑–∞–¥ –∏ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–π, –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥, –≤ –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –Ω–∏–∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏–ª–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –°–∏—Å—Ç–µ–º—ã 2 –º–æ–∂–µ—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏.</p>\n<p><strong>–†–æ–ª—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏</strong></p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç —Ä–µ—à–∞—é—â—É—é —Ä–æ–ª—å –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –°–∏—Å—Ç–µ–º—ã 2. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–º–µ–Ω–Ω–æ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–º –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–∏—Å—É–Ω–æ–∫ 5 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏) –∏–∑-–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—Å—à—Ç–∞–±–∞—Ö –º–æ–¥–µ–ª–µ–π, –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –°–∏—Å—Ç–µ–º—ã 2. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ—Ç—è —Ç–æ—á–Ω–æ—Å—Ç—å SFT (–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º) –º–æ–¥–µ–ª–∏ rStar-Math-7B –Ω–∏–∂–µ, —á–µ–º —É Qwen2.5-Math-72B-Instruct, –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å PPM 7B, rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ 72B —Å Qwen 72B ORM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Ä–∞–∑–º–µ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å—Ö–æ–¥–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è PPM.</p>\n<p><strong>PPM –≤—ã—è–≤–ª—è–µ—Ç —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º</strong></p>\n<p>–ü—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–æ—Ä–µ–º –∏–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–æ–≤ —á–∞—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–æ–π —É—Å–ø–µ—à–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á rStar-Math, PPM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏. –≠—Ç–∏ —à–∞–≥–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª—è—è –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ PPM —É—Å–ø–µ—à–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–æ—Ä–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞–ª–∞—è —Ç–µ–æ—Ä–µ–º–∞ –§–µ—Ä–º–∞, —Ñ–æ—Ä–º—É–ª—ã –í–∏–µ—Ç–∞, –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ö–æ—à–∏, —Ç–µ–æ—Ä–µ–º–∞ –ü–∏—Ñ–∞–≥–æ—Ä–∞ –∏ —Ñ–æ—Ä–º—É–ª–∞ –ø–ª–æ—â–∞–¥–∏ –ì–∞—É—Å—Å–∞.</p>\n<p><strong>–û–±—Å—É–∂–¥–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è</strong></p>\n<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–±—â—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, –ø—Ä–∏–º–µ–Ω–∏–º—É—é –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º, —Ö–æ—Ç—è –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –ö–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2, –æ–Ω —É—Å–ø–µ—à–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É —É—Ä–æ–≤–Ω—è –æ–ª–∏–º–ø–∏–∞–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ª–æ–π —Ç–µ–æ—Ä–µ–º—ã –§–µ—Ä–º–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–æ-–≤—Ç–æ—Ä—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫–æ–¥ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —Å–∏–Ω—Ç–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ —Ç–æ–º, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª–∏ –¥–∞–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–æ–Ω—Ü–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è MCTS. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ –∫–æ–¥–µ —ç—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤; –≤ –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–ª–∏ –≤–∑–∞–∏–º–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –¥—Ä—É–≥–æ–π LLM.</p>'}, {'title': 'Conclusion', 'content': 'In this work, we present rStar-Math, self-evolved System 2 deep thinking approach that significantly boosts the math reasoning capabilities of small LLMs, achieving state-of-the-art OpenAI o1-level performance. Our approach demonstrates that SLMs can self-generate high-quality training data for frontier-level math reasoning. Extensive experiments across four different-sized SLMs and challenging math benchmarks demonstrate the superiority of rStar-Math, with achieving leading results while outperforming existing math reasoning LLMs and Best-of-N baselines. We also reveal key findings, including the emergence of self-reflection and the effectiveness of the PPM in identifying critical intermediate steps, such as theorem-application steps. Finally, rStar-Math can achieve further improvements by collecting more challenging math problems, we leave this as future work.', 'summary': '<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è –ø–æ–¥—Ö–æ–¥ "–°–∏—Å—Ç–µ–º—ã 2" –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —É –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ OpenAI o1, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥–æ–≤—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.</p>\n<p>–ù–∞—à –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ SLM –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–µ—Ä–µ–¥–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ. –û–±—à–∏—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —á–µ—Ç—ã—Ä—å–º—è SLM —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Å–ª–æ–∂–Ω—ã–º–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Å—Ç–∞–º–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ rStar-Math. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ "Best-of-N".</p>\n<p>–¢–∞–∫–∂–µ –º—ã –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ—è–≤–ª–µ–Ω–∏–µ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å PPM (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ, Post-Processing Method) –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º. –ù–∞–∫–æ–Ω–µ—Ü, rStar-Math –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π –∑–∞ —Å—á–µ—Ç —Å–±–æ—Ä–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –º—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ —Å—É—Ç–∏ —Å—Ç–∞—Ç—å–∏:</strong></p>\n<ul>\n<li><strong>–°–∏—Å—Ç–µ–º–∞ 2 –º—ã—à–ª–µ–Ω–∏—è:</strong> –≠—Ç–æ –æ—Ç—Å—ã–ª–∫–∞ –∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏, –≥–¥–µ "–°–∏—Å—Ç–µ–º–∞ 1" - —ç—Ç–æ –±—ã—Å—Ç—Ä–æ–µ, –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∞ "–°–∏—Å—Ç–µ–º–∞ 2" - –º–µ–¥–ª–µ–Ω–Ω–æ–µ, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ. rStar-Math, –≤–∏–¥–∏–º–æ, –∏–º–∏—Ç–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –æ–±–¥—É–º–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.</li>\n<li><strong>SLM (Small Language Models):</strong>  –†–µ—á—å –∏–¥–µ—Ç –æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –±–æ–ª—å—à–∏–º –º–æ–¥–µ–ª—è–º –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–æ rStar-Math –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li><strong>OpenAI o1-level performance:</strong> –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ —Å –æ–¥–Ω–æ–π –∏–∑ –º–æ–¥–µ–ª–µ–π OpenAI, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —ç—Ç–∞–ª–æ–Ω–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò.</li>\n<li><strong>–°–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è:</strong>  –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –º–æ–º–µ–Ω—Ç, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —É–ª—É—á—à–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.</li>\n<li><strong>PPM:</strong> –ú–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–ª—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ —à–∞–≥–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏.</li>\n<li><strong>Best-of-N baselines:</strong>  –ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –∏–∑ N —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤. rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —ç—Ç–æ—Ç –º–µ—Ç–æ–¥.</li>\n</ul>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment', '#architecture', '#audio', '#benchmark (1)', '#cv', '#data', '#dataset (1)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source', '#optimization (1)', '#plp', '#rag', '#reasoning (1)', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–π, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –∏–ª–∏ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ OpenAI o1 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç "–≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è" —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS). –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –≤—ã—Å—Ç—É–ø–∞—é—â–∞—è –≤ —Ä–æ–ª–∏ "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏", –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–æ–∏—Å–∫ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—è—Å—å –Ω–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞, —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ SLM.</p>
<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è —ç—Ç–∏—Ö –¥–≤—É—Ö SLM:</p>
<ol>
<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö CoT —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∫–æ–¥–æ–º:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö, –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è SLM, –æ—Ç–≤–µ—á–∞—é—â–µ–π –∑–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–∏—Ç–∏–∫—É. (CoT - Chain of Thought, –º–µ—Ç–æ–¥, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç, –Ω–æ –∏ —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–∏–≤–µ–¥—à–∏—Ö –∫ –Ω–µ–º—É)</li>
<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–±–µ–≥–∞–µ—Ç –ø—Ä—è–º–æ–≥–æ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω –æ–±—É—á–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM). (–¢.–µ. –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —à–∞–≥–∏, –∞ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π)</li>
<li><strong>–†–µ—Ü–µ–ø—Ç —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è:</strong> –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –ø–æ–ª–∏—Ç–∏–∫—É, –∏ PPM —Å—Ç—Ä–æ—è—Ç—Å—è —Å –Ω—É–ª—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</li>
</ol>
<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è 4 —Ä–∞—É–Ω–¥–∞–º —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è 747 —Ç—ã—Å—è—á –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, rStar-Math –ø–æ–≤—ã—à–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π SLM –¥–æ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH, rStar-Math —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Qwen2.5-Math-7B —Å 58.8% –¥–æ 90.0% –∏ Phi3-mini-3.8B —Å 41.4% –¥–æ 86.4%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –Ω–∞ +4.5% –∏ +0.9% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ USA Math Olympiad (AIME) rStar-Math —Ä–µ—à–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 53.3% (8 –∏–∑ 15) –∑–∞–¥–∞—á, —á—Ç–æ —Å—Ç–∞–≤–∏—Ç –µ–≥–æ –≤ —Ç–æ–ø 20% —Å–∞–º—ã—Ö —Å–ø–æ—Å–æ–±–Ω—ã—Ö —É—á–µ–Ω–∏–∫–æ–≤ —Å—Ç–∞—Ä—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ö–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ —Å—Å—ã–ª–∫–µ https://github.com/microsoft/rStar.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Recent Studies</div>
                <div class="summary_text"><p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω rStar-Math, –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–º–∏—Ç–∏—Ä—É—é—â–∏–π –º—ã—à–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Ç–∏–ø–∞ (System 2). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è OpenAI o1, –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).</p>
<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –Ω–µ—Ö–≤–∞—Ç–∫–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.  –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ –¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω –≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç, —ç—Ç–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–µ—à–µ–Ω–∏—è.  –û—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Å–Ω–∏–∂–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞, —á—Ç–æ –æ—á–µ–Ω—å —Ç—Ä—É–¥–æ–µ–º–∫–æ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–æ–∫–∞ –Ω–µ –¥–∞–µ—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑-–∑–∞ —à—É–º–∞ –≤ –æ—Ü–µ–Ω–∫–∞—Ö.  –ü–æ—ç—Ç–æ–º—É, –ø–æ–¥—Ö–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–µ–¥–µ–ª–∞ –∏ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–≤–æ–∏—Ö —É—á–∏—Ç–µ–ª–µ–π.</p>
<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –∏ –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS) –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.  –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è:</p>
<ol>
<li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–¥–∞:</strong>  –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö MCTS. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ SLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —Ä–µ—à–µ–Ω–∏—è –≤ –≤–∏–¥–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞ –Ω–∞ Python.  –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ —à–∞–≥–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–¥ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è.  –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–∞–∂–¥–æ–º—É —à–∞–≥—É Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥ —à–∞–≥–∞ –≤ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –®–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—â–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è.</li>
<li><strong>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ (PPM):</strong> –≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É (reward) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ä–µ—à–µ–Ω–∏—è.  –î–ª—è –æ–±—É—á–µ–Ω–∏—è PPM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ Q-–∑–Ω–∞—á–µ–Ω–∏—è, —Ö–æ—Ç—è –∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏, –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–∞–¥–µ–∂–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.  –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Å–æ–∑–¥–∞—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π, –∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–Ω–∂–∏—Ä—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä—è–º–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∑–∞—à—É–º–ª–µ–Ω—ã.</li>
<li><strong>–ß–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è:</strong> –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–æ —Å–±–æ—Ä–∞ 747 —Ç—ã—Å—è—á –∑–∞–¥–∞—á –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Ä–µ—à–µ–Ω–∏—è (policy model) –∏ PPM –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è MCTS –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ó–∞—Ç–µ–º –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—Ç—Å—è –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.  –ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É—Å–∏–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–∏, –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ PPM, —É–ª—É—á—à–µ–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–µ—à–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –æ—Ö–≤–∞—Ç–∞ –∑–∞–¥–∞—á.</li>
</ol>
<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ SLM (–æ—Ç 1.5 –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –Ω–∞ —Å–µ–º–∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å rStar-Math.  –ù–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, rStar-Math –ø–æ–≤—ã—Å–∏–ª —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B —Å 58.8% –¥–æ 89.4%, –∞ Qwen2.5-Math-1.5B —Å 51.2% –¥–æ 87.8%. –° 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –ø–æ–∏—Å–∫–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç 90% –∏ 88.4% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –∏ –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-minis. –ù–∞ –æ–ª–∏–º–ø–∏–∞–¥–µ AIME 2024, rStar-Math —Ä–µ—à–∏–ª 53.3% –∑–∞–¥–∞—á, —á—Ç–æ –Ω–∞ 8.7% –±–æ–ª—å—à–µ, —á–µ–º o1-preview. –ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–∞ rStar-Math –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Related Works</div>
                <div class="summary_text"><p><strong>–°–∏–Ω—Ç–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö</strong></p>
<p>–£—Å–ø–µ—Ö–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –º–Ω–æ–≥–æ–º –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥—É Chain-of-Thought (CoT). –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤–µ–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö —Ä–∞–±–æ—Ç –≤–∫–ª—é—á–∞—é—Ç NuminaMath –∏ MetaMath. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ–±—É—á–∞—é—â–µ–π LLM. –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—â–∞—è LLM –Ω–µ –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å, –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –î–∞–∂–µ –≤ —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å.</p>
<p>–ú–µ—Ç–æ–¥—ã –æ—Ç–±–æ—Ä–∞ —Å –æ—Ç–±—Ä–∞–∫–æ–≤–∫–æ–π –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö CoT –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –æ—Ç–¥–∞—á–∏, –∫–æ–≥–¥–∞ –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –Ω–∞—Å—ã—â–µ–Ω–∏—é. –ù–∞–ø—Ä–∏–º–µ—Ä, OpenMathInstruct-2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª–∏—à—å 3.9% –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤–æ—Å—å–º–∏–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</p>
<p>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (test-time compute) –≤–≤–µ–ª–æ –Ω–æ–≤—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è LLM —É–ª—É—á—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –ë—ã–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –º–µ—Ç–æ–¥—ã –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ MCTS. –û–¥–Ω–∞–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–µ—Ç–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —É—Å–ø–µ—Ö–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —á–∞—Å—Ç–æ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å–∞–º–æ–π LLM –∏–ª–∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>
<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è LLM –∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å OpenAI o1. –ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å. –ù–µ–¥–∞–≤–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ "—Å—É–¥—å–∏" –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Outcome Reward Model –∏ Process Reward Model (PRM). PRM –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–æ —Å–±–æ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π.</p>
<p>–í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, –¥—Ä—É–≥–∏–µ –∏—Å—Å–ª–µ–¥—É—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é Monte Carlo Sampling –∏–ª–∏ MCTS. –û–¥–Ω–∞–∫–æ, –æ–Ω–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. rStar-Math –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM), –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Methodology</div>
                <div class="summary_text"><h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 3.1-3.3 —Å—Ç–∞—Ç—å–∏ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ MCTS –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á</h2>
<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (MCTS) –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –¥–≤—É–º—è –º–æ–¥–µ–ª—è–º–∏: –ø–æ–ª–∏—Ç–∏–∫–æ–π (SLM) –∏ –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PRM). MCTS –≤—ã–±—Ä–∞–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ –¥–≤—É–º –ø—Ä–∏—á–∏–Ω–∞–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Best-of-N –∏–ª–∏ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –ø–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ MCTS –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–º—É —à–∞–≥—É, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –µ–≥–æ –≤–∫–ª–∞–¥–µ –≤ –∏—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, —á—Ç–æ –∏–∑–±–∞–≤–ª—è–µ—Ç –æ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>
<p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö MCTS —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –¥–≤—É–º—è –ø—Ä–æ–±–ª–µ–º–∞–º–∏. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–∂–µ —Ç–∞–∫–∏–µ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ –º–æ–≥—É—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö, —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS; –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ª–æ–∂–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤. –£—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –∫–∞–∂–¥—ã–π –ø—Ä–æ–≥–æ–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π, –∞ LLM —è–≤–ª—è—é—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º–∏, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.</p>
<p>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–≤–µ 7B –º–æ–¥–µ–ª–∏ SLM (–æ–¥–Ω–∞ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –æ–¥–Ω–∞ –¥–ª—è PRM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏. –û–¥–Ω–∞–∫–æ, —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–æ–ª—å—à—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è SLM –∏–∑-–∑–∞ –∏—Ö –º–µ–Ω—å—à–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. SLM —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∏ –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç –æ—à–∏–±–æ—á–Ω—ã–º–∏ –∏–ª–∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.</p>
<p>–î–ª—è —Å–º—è–≥—á–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—à–∞–≥–æ–≤—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏. –î–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ SLM –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ä–µ—Ü–µ–ø—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞—É–Ω–¥–∞. –í –∫–∞–∂–¥–æ–º —Ä–∞—É–Ω–¥–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–µ—à–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.</p>
<p>–ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–ø—É—Å–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ MCTS –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π. –ö–æ—Ä–Ω–µ–≤–æ–π —É–∑–µ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–æ–ø—Ä–æ—Å, –∞ –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —à–∞–≥–∞–º, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏. –ü—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –¥–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —É–∑–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é, –∫–∞–∂–¥–æ–º—É —à–∞–≥—É –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π.</p>
<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ CoT, –∑–¥–µ—Å—å –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–π CoT –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤–º–µ—Å—Ç–µ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º –Ω–∞ Python, –≥–¥–µ CoT –≤—Å—Ç—Ä–æ–µ–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è. –°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ–¥ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É —Å–ª—É—á–∞–π–Ω–æ. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ MCTS, –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞. –û—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª—å—é PRM, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –ó–∞—Ç–µ–º, —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ UCT –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∏–π —É–∑–µ–ª.</p>
<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –∏–≥—Ä–æ–∫–∞–º –≤ –ì–æ, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–∞ –∏–≥—Ä—ã. –ü–æ—Å–ª–µ –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS, —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è, –∞ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–≤–µ—Ä–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –Ω–∏–∑–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –º–µ—Ç–æ–¥–∞ —Å–∞–º–æ–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö —Ä–∞—É–Ω–¥–∞—Ö, –∫–æ–≥–¥–∞ PRM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞) –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è PRM (–Ω–∞—á–∏–Ω–∞—è —Å —Ç—Ä–µ—Ç—å–µ–≥–æ —Ä–∞—É–Ω–¥–∞). –í –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ —à–∞–≥–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, –ø—Ä–∏–≤–µ–ª –ª–∏ –æ–Ω –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É –≤ –∫–æ–Ω—Ü–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ PRM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è.</p>
<p>–ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤, –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –û–¥–Ω–∞–∫–æ, –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∏—Ö –æ—Å—Ç–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É –∏–ª–∏ –Ω–∞ –æ—Ü–µ–Ω–∫–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ MCTS. –¢–æ—á–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (PPM), –∫–æ—Ç–æ—Ä—ã–π –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä—è–º—ã—Ö –º–µ—Ç–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –≤—ã–±–æ—Ä–∞ —à–∞–≥–æ–≤ –∏–∑ –¥–µ—Ä–µ–≤–∞ MCTS –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–∞—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –¥–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å —Å–∞–º—ã–º–∏ –≤—ã—Å–æ–∫–∏–º–∏ Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ –∏ –¥–≤–∞ —Å —Å–∞–º—ã–º–∏ –Ω–∏–∑–∫–∏–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ó–∞—Ç–µ–º PPM –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.04519/x2.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Evaluation</div>
                <div class="summary_text"><h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 4.1 "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö" –∏ 4.2 "–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã" —Å—Ç–∞—Ç—å–∏ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.</h2>
<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ rStar-Math, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.</p>
<p><strong>4.1 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö</strong></p>
<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ rStar-Math –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –≤–∫–ª—é—á–∞—è:</p>
<ul>
<li><strong>GSM8K:</strong> —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>
<li><strong>–ó–∞–¥–∞—á–∏ —É—Ä–æ–≤–Ω—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –æ–ª–∏–º–ø–∏–∞–¥:</strong><ul>
<li><strong>MATH-500:</strong> –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á.</li>
<li><strong>AIME 2024:</strong> –∑–∞–¥–∞—á–∏, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ª—É—á—à–∏—Ö —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤ –°–®–ê.</li>
<li><strong>AMC 2023:</strong> –∑–∞–¥–∞—á–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã.</li>
<li><strong>Olympiad Bench:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è.</li>
</ul>
</li>
<li><strong>–ó–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ —É—Ä–æ–≤–Ω—è –∫–æ–ª–ª–µ–¥–∂–∞:</strong><ul>
<li><strong>College Math:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏—Ö –∫—É—Ä—Å–æ–≤.</li>
</ul>
</li>
<li><strong>–ó–∞–¥–∞—á–∏ –∏–∑ –¥—Ä—É–≥–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏:</strong><ul>
<li><strong>GaoKao (–∫–∏—Ç–∞–π—Å–∫–∏–π –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω –≤ –∫–æ–ª–ª–µ–¥–∂) En 2023:</strong> –∑–∞–¥–∞—á–∏ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ —ç–∫–∑–∞–º–µ–Ω–∞.</li>
</ul>
</li>
</ul>
<p><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</strong></p>
<p>rStar-Math —è–≤–ª—è–µ—Ç—Å—è –æ–±—â–∏–º –ø–æ–¥—Ö–æ–¥–æ–º, –ø—Ä–∏–º–µ–Ω–∏–º—ã–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM). –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤:</p>
<ul>
<li>Qwen2.5-Math-1.5B</li>
<li>Phi3-mini-Instruct (3B) - –æ–±—â–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>
<li>Qwen2-Math-7B</li>
<li>Qwen2.5-Math-7B</li>
</ul>
<p>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ 4 —Ä–∞—É–Ω–¥–∞ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–≥–æ –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ 4 —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–≤—à–∏—Ö –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (policy SLM) –∏ 4 –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è (PPM). –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Qwen2.5-Math-7B –Ω–∞ 4 —Ä–∞—É–Ω–¥–µ. –ò—Ç–æ–≥–æ–≤–∞—è PPM –∏–∑ —ç—Ç–æ–≥–æ —Ä–∞—É–Ω–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è –¥–ª—è —ç—Ç–∏—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏.</p>
<p><strong>–ë–∞–∑–æ–≤—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è</strong></p>
<p>rStar-Math –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ—Ç–æ–¥–∞–º "–°–∏—Å—Ç–µ–º—ã 2" (—Ç—Ä–µ–±—É—é—â–∏–º –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è). –û–Ω —Å—Ä–∞–≤–Ω–∏–≤–∞–ª—Å—è —Å —Ç—Ä–µ–º—è —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–º–∏ –∫–∞–∫ "–°–∏—Å—Ç–µ–º—É 1" (–±—ã—Å—Ç—Ä–æ–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ), —Ç–∞–∫ –∏ "–°–∏—Å—Ç–µ–º—É 2":</p>
<ul>
<li><strong>–ü–µ—Ä–µ–¥–æ–≤—ã–µ LLM:</strong> GPT-4o, Claude, OpenAI o1-preview –∏ o1-mini.</li>
<li><strong>–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:</strong> DeepSeek-Coder-v2-Instruct, Mathstral, NuminaMath-72B –∏ LLaMA3.1.</li>
<li><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥–∞–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤:</strong> –≤–∫–ª—é—á–∞—è –≤–µ—Ä—Å–∏–∏ Instruct (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-7B-Instruct) –∏ Best-of-N (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-72B-Instruct+Qwen2.5-Math-RM-72B).</li>
</ul>
<p><strong>–ú–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏</strong></p>
<p>–î–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–µ—Ç—Ä–∏–∫–∞ Pass@1 (–¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏). –î–ª—è –º–æ–¥–µ–ª–µ–π "–°–∏—Å—Ç–µ–º—ã 2" –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤—Ä–µ–º—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –¥–ª—è o1-mini –∏ o1-preview. –î–ª—è –º–æ–¥–µ–ª–µ–π Qwen Best-of-N –±—ã–ª–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ MATH-500 –∏ AIME/AMC. –î–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è rStar-Math –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç–∞–∫–æ–µ –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π, –∫–∞–∫ –∏ Qwen. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è AIME/AMC –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å 16 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ - 8. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å PPM –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏), –æ–±–æ–∑–Ω–∞—á–µ–Ω–Ω–∞—è –∫–∞–∫ rStar-Math64.</p>
<p><strong>4.2 –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</strong></p>
<p>–í —Ç–∞–±–ª–∏—Ü–µ 5 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã rStar-Math –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>
<ol>
<li><strong>rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ SLM</strong>, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π OpenAI o1, –ø—Ä–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—å—à–µ–º —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ (1.5B-7B). –ù–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5Math-7B, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–≤—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å 58.8% –Ω–∞ MATH, —É–ª—É—á—à–∏–ª–∞—Å—å –¥–æ 90.0% —Å rStar-Math, –ø—Ä–µ–≤–∑–æ–π–¥—è o1-preview –∏ Claude 3.5 Sonnet –∏ –¥–æ—Å—Ç–∏–≥–Ω—É–≤ —É—Ä–æ–≤–Ω—è o1-mini. –ù–∞ College Math rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç o1-mini –Ω–∞ 2.7%. –ù–∞ AIME 2024 rStar-Math –Ω–∞–±—Ä–∞–ª 53.3%, —É—Å—Ç—É–ø–∏–≤ —Ç–æ–ª—å–∫–æ o1-mini.</li>
<li><strong>rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç "–°–∏—Å—Ç–µ–º—É 2"</strong>, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–Ω—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (1.5B-7B) –∏ –º–æ–¥–µ–ª–µ–π –ø–æ–æ—â—Ä–µ–Ω–∏—è (7B). –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen Best-of-N, rStar-Math –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–∞–∂–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Best-of-N —Å 72B –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏ rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–µ –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫—Ä–æ–º–µ GSM8K.</li>
<li><strong>rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö</strong>, –≤–∫–ª—é—á–∞—è Olympiad Bench, College Math –∏ Gaokao, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Ä–µ–∫–æ—Ä–¥—ã.</li>
</ol>
<p><strong>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</strong></p>
<p>rStar-Math –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—è –ø–æ–∏—Å–∫ —Ä–µ—à–µ–Ω–∏–π –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º PPM. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>
<ol>
<li><strong>–£–∂–µ —Å 4 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Best-of-N</strong>, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-mini.</li>
<li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π</strong> –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ö–æ—Ç—è –∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–ø–∞–º–∏. –ù–∞ Math, AIME –∏ Olympiad Bench rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—Å—ã—â–µ–Ω–∏–µ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏ 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞ College Math –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è.</li>
</ol></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.04519/extracted/6117756/scalinglaws.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Findings and Discussions</div>
                <div class="summary_text"><p><strong>–í–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏</strong></p>
<p>–ö–ª—é—á–µ–≤—ã–º –ø—Ä–æ—Ä—ã–≤–æ–º –≤ –º–æ–¥–µ–ª–∏ OpenAI o1 —è–≤–ª—è–µ—Ç—Å—è –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–ø—É—Å–∫–∞–µ—Ç –æ—à–∏–±–∫—É, –æ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –µ—ë –∏ –º–æ–∂–µ—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –≤—ã–¥–∞–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –û–¥–Ω–∞–∫–æ, –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM (–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö) —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π. –°–æ–æ–±—â–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—é –∏ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∏–ª–∏ –ø–æ–±—É–¥–∏—Ç—å LLM –∫ —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.</p>
<p>–í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ, –±—ã–ª–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ MCTS (–º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–µ—Ä–µ–≤–∞), –ø—Ä–æ—è–≤–ª—è–µ—Ç —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ, –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–º –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 4, –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è SymPy, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–≤–µ—Ä–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É (–ª–µ–≤–∞—è –≤–µ—Ç–≤—å). –û–¥–Ω–∞–∫–æ, –Ω–∞ —á–µ—Ç–≤—ë—Ä—Ç–æ–º —à–∞–≥–µ (–ø—Ä–∞–≤–∞—è –≤–µ—Ç–≤—å), –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤ –∏ –æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –ø–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º—É –ø—É—Ç–∏. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞–∑–∞–¥ –∏ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–π, –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥, –≤ –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –Ω–∏–∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏–ª–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –°–∏—Å—Ç–µ–º—ã 2 –º–æ–∂–µ—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏.</p>
<p><strong>–†–æ–ª—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏</strong></p>
<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç —Ä–µ—à–∞—é—â—É—é —Ä–æ–ª—å –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –°–∏—Å—Ç–µ–º—ã 2. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–º–µ–Ω–Ω–æ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–º –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–∏—Å—É–Ω–æ–∫ 5 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏) –∏–∑-–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—Å—à—Ç–∞–±–∞—Ö –º–æ–¥–µ–ª–µ–π, –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –°–∏—Å—Ç–µ–º—ã 2. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ—Ç—è —Ç–æ—á–Ω–æ—Å—Ç—å SFT (–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º) –º–æ–¥–µ–ª–∏ rStar-Math-7B –Ω–∏–∂–µ, —á–µ–º —É Qwen2.5-Math-72B-Instruct, –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å PPM 7B, rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ 72B —Å Qwen 72B ORM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Ä–∞–∑–º–µ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å—Ö–æ–¥–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è PPM.</p>
<p><strong>PPM –≤—ã—è–≤–ª—è–µ—Ç —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º</strong></p>
<p>–ü—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–æ—Ä–µ–º –∏–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–æ–≤ —á–∞—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–æ–π —É—Å–ø–µ—à–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á rStar-Math, PPM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏. –≠—Ç–∏ —à–∞–≥–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª—è—è –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ PPM —É—Å–ø–µ—à–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–æ—Ä–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞–ª–∞—è —Ç–µ–æ—Ä–µ–º–∞ –§–µ—Ä–º–∞, —Ñ–æ—Ä–º—É–ª—ã –í–∏–µ—Ç–∞, –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ö–æ—à–∏, —Ç–µ–æ—Ä–µ–º–∞ –ü–∏—Ñ–∞–≥–æ—Ä–∞ –∏ —Ñ–æ—Ä–º—É–ª–∞ –ø–ª–æ—â–∞–¥–∏ –ì–∞—É—Å—Å–∞.</p>
<p><strong>–û–±—Å—É–∂–¥–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è</strong></p>
<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–±—â—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, –ø—Ä–∏–º–µ–Ω–∏–º—É—é –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º, —Ö–æ—Ç—è –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –ö–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2, –æ–Ω —É—Å–ø–µ—à–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É —É—Ä–æ–≤–Ω—è –æ–ª–∏–º–ø–∏–∞–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ª–æ–π —Ç–µ–æ—Ä–µ–º—ã –§–µ—Ä–º–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–æ-–≤—Ç–æ—Ä—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫–æ–¥ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —Å–∏–Ω—Ç–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ —Ç–æ–º, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª–∏ –¥–∞–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–æ–Ω—Ü–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è MCTS. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ –∫–æ–¥–µ —ç—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤; –≤ –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–ª–∏ –≤–∑–∞–∏–º–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –¥—Ä—É–≥–æ–π LLM.</p></div>
                <div class="images"><img class="summary_image" src='https://arxiv.org/html/2501.04519/x3.png'/></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Conclusion</div>
                <div class="summary_text"><p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è –ø–æ–¥—Ö–æ–¥ "–°–∏—Å—Ç–µ–º—ã 2" –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —É –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ OpenAI o1, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥–æ–≤—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.</p>
<p>–ù–∞—à –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ SLM –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–µ—Ä–µ–¥–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ. –û–±—à–∏—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —á–µ—Ç—ã—Ä—å–º—è SLM —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Å–ª–æ–∂–Ω—ã–º–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Å—Ç–∞–º–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ rStar-Math. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ "Best-of-N".</p>
<p>–¢–∞–∫–∂–µ –º—ã –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ—è–≤–ª–µ–Ω–∏–µ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å PPM (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ, Post-Processing Method) –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º. –ù–∞–∫–æ–Ω–µ—Ü, rStar-Math –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π –∑–∞ —Å—á–µ—Ç —Å–±–æ—Ä–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –º—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.</p>
<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ —Å—É—Ç–∏ —Å—Ç–∞—Ç—å–∏:</strong></p>
<ul>
<li><strong>–°–∏—Å—Ç–µ–º–∞ 2 –º—ã—à–ª–µ–Ω–∏—è:</strong> –≠—Ç–æ –æ—Ç—Å—ã–ª–∫–∞ –∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏, –≥–¥–µ "–°–∏—Å—Ç–µ–º–∞ 1" - —ç—Ç–æ –±—ã—Å—Ç—Ä–æ–µ, –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∞ "–°–∏—Å—Ç–µ–º–∞ 2" - –º–µ–¥–ª–µ–Ω–Ω–æ–µ, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ. rStar-Math, –≤–∏–¥–∏–º–æ, –∏–º–∏—Ç–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –æ–±–¥—É–º–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.</li>
<li><strong>SLM (Small Language Models):</strong>  –†–µ—á—å –∏–¥–µ—Ç –æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –±–æ–ª—å—à–∏–º –º–æ–¥–µ–ª—è–º –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–æ rStar-Math –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>
<li><strong>OpenAI o1-level performance:</strong> –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ —Å –æ–¥–Ω–æ–π –∏–∑ –º–æ–¥–µ–ª–µ–π OpenAI, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —ç—Ç–∞–ª–æ–Ω–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò.</li>
<li><strong>–°–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è:</strong>  –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –º–æ–º–µ–Ω—Ç, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —É–ª—É—á—à–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.</li>
<li><strong>PPM:</strong> –ú–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–ª—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ —à–∞–≥–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏.</li>
<li><strong>Best-of-N baselines:</strong>  –ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –∏–∑ N —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤. rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —ç—Ç–æ—Ç –º–µ—Ç–æ–¥.</li>
</ul></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-01-13 12:46',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-13 12:46')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-13 12:46')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    