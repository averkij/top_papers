
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 1 paper. December 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 декабря</span> | <span id="title-articles-count">1 paper</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-12.html">⬅️ <span id="prev-date">12.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-16.html">➡️ <span id="next-date">16.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 декабря', 'en': 'December 13', 'zh': '12月13日'};
        let feedDateNext = {'ru': '16.12', 'en': '12/16', 'zh': '12月16日'};
        let feedDatePrev = {'ru': '12.12', 'en': '12/12', 'zh': '12月12日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': '2412.09605', 'title': 'AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials', 'url': 'https://huggingface.co/papers/2412.09605', 'abstract': 'Graphical User Interface (GUI) agents hold great potential for automating\ncomplex tasks across diverse digital environments, from web applications to\ndesktop software. However, the development of such agents is hindered by the\nlack of high-quality, multi-step trajectory data required for effective\ntraining. Existing approaches rely on expensive and labor-intensive human\nannotation, making them unsustainable at scale. To address this challenge, we\npropose AgentTrek, a scalable data synthesis pipeline that generates\nhigh-quality GUI agent trajectories by leveraging web tutorials. Our method\nautomatically gathers tutorial-like texts from the internet, transforms them\ninto task goals with step-by-step instructions, and employs a visual-language\nmodel agent to simulate their execution in a real digital environment. A\nVLM-based evaluator ensures the correctness of the generated trajectories. We\ndemonstrate that training GUI agents with these synthesized trajectories\nsignificantly improves their grounding and planning performance over the\ncurrent models. Moreover, our approach is more cost-efficient compared to\ntraditional human annotation methods. This work underscores the potential of\nguided replay with web tutorials as a viable strategy for large-scale GUI agent\ntraining, paving the way for more capable and autonomous digital agents.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-12-12', 'pub_date_card': {'ru': '12 декабря', 'en': 'December 12', 'zh': '12月12日'}, 'hash': '298173f67e05442e', 'authors': ['Yiheng Xu', 'Dunjie Lu', 'Zhennan Shen', 'Junli Wang', 'Zekun Wang', 'Yuchen Mao', 'Caiming Xiong', 'Tao Yu'], 'affiliations': ['Salesforce Research', 'University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2412.09605.jpg', 'data': {'categories': ['#data', '#dataset', '#training', '#optimization', '#agents', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'AgentTrek: Революция в обучении GUI-агентов с помощью веб-руководств', 'desc': 'Статья представляет AgentTrek - масштабируемый конвейер для синтеза данных, который генерирует высококачественные траектории агентов графического интерфейса пользователя (GUI) с помощью веб-руководств. Метод автоматически собирает тексты, похожие на учебные пособия, из интернета, преобразует их в цели задач с пошаговыми инструкциями и использует агента на основе визуально-языковой модели для симуляции их выполнения в реальной цифровой среде. Обучение GUI-агентов с использованием этих синтезированных траекторий значительно улучшает их производительность по сравнению с текущими моделями. Этот подход более экономичен по сравнению с традиционными методами аннотации данных человеком.'}, 'en': {'title': 'Automating GUI Agent Training with Web Tutorials', 'desc': 'This paper introduces AgentTrek, a novel approach for generating high-quality training data for Graphical User Interface (GUI) agents. It addresses the challenge of obtaining multi-step trajectory data by automatically synthesizing it from web tutorials, thus eliminating the need for costly human annotation. The method involves transforming tutorial texts into actionable task goals and using a visual-language model to simulate the execution of these tasks. The results show that training GUI agents with these synthesized trajectories enhances their performance in grounding and planning, making the process more efficient and scalable.'}, 'zh': {'title': '利用网络教程提升GUI代理训练效率', 'desc': '本文提出了一种名为AgentTrek的数据合成管道，用于生成高质量的图形用户界面（GUI）代理轨迹。该方法通过自动收集网络教程文本，将其转化为任务目标和逐步指令，并利用视觉语言模型代理在真实数字环境中模拟执行。与传统的人类标注方法相比，我们的方法在成本上更具效率，同时显著提高了GUI代理的基础和规划性能。此研究展示了利用网络教程进行引导重放的潜力，为大规模GUI代理训练开辟了新的可能性。'}}, 'clean_sections': [{'title': 'Abstract', 'content': 'Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.', 'summary': '<p>В статье рассматривается проблема нехватки качественных данных для обучения агентов, управляющих графическим интерфейсом (GUI). Такие агенты могут автоматизировать сложные задачи в различных цифровых средах, но их разработка затруднена из-за отсутствия размеченных данных с последовательностью действий (траекторий). Существующие методы разметки требуют больших затрат времени и ресурсов, так как предполагают ручную работу.</p>\n<p>Чтобы решить эту проблему, авторы предлагают AgentTrek – масштабируемый конвейер для синтеза данных. Он автоматически генерирует траектории действий агента, используя веб-инструкции. Метод работает следующим образом:</p>\n<ol>\n<li><strong>Сбор инструкций:</strong> AgentTrek автоматически собирает текстовые инструкции, похожие на обучающие материалы, из интернета.</li>\n<li><strong>Преобразование в задачи:</strong> Эти тексты преобразуются в задачи с пошаговыми инструкциями.</li>\n<li><strong>Моделирование выполнения:</strong> Агент, использующий визуально-языковую модель (VLM), имитирует выполнение этих инструкций в реальной цифровой среде.</li>\n<li><strong>Оценка корректности:</strong> VLM-модель также используется для оценки правильности сгенерированных траекторий.</li>\n</ol>\n<p>Результаты показывают, что обучение GUI-агентов с использованием синтезированных таким образом траекторий значительно улучшает их способность понимать контекст и планировать действия по сравнению с существующими моделями. Кроме того, предложенный метод более экономичен, чем традиционная ручная разметка.</p>\n<p>Таким образом, статья демонстрирует, что использование веб-инструкций для обучения агентов является перспективной стратегией для создания более мощных и автономных цифровых агентов.</p>'}, {'title': 'Introduction', 'content': 'Graphical User Interfaces (GUIs) are fundamental medium for human-computer interaction, enabling users to perform tasks across various digital pl atforms. Automating GUI operations through agentic automation has the potential to significantly enhance productivity by enabling autonomous task completion using human-centric tools. Additionally, this approach can foster the development of advanced AI systems capable of learning from rich digital environments. Recent advancements in large language models (LLMs) have endowed the models with powerful abilities in understanding, reasoning, and decisionmaking, which are essential for the evolution of GUI agents in diverse contexts such as web (Zheng et al., 2024), desktop (Xie et al., 2024), and mobile applications (Zhang et al., 2023). Despite these advancements, the performance of GUI agents remains suboptimal. Contemporary Large Language Models (LLMs) are primarily engineered and trained on datasets optimized for generating informative responses (Ouyang et al., 2022; OpenAI, 2024). Their architecture and training paradigms are Figure 1: Expected Agent Trajectories Equal contribution 1 Figure 2: Overview of the AgentTrek Pipeline: (1) Automatic Tutorial Collection from the Internet: Tutorial-related data is extracted and filtered from internet sources using heuristic methods and FastText model. An LLM processes the filtered textual data, transforming it into structured tutorials. (2) Trajectory data collection via guided replay: VLM agent interacts with the real digital environment guided by tutorials, while high-quality trajectory data, including observations, actions, and reasoning, is collected. Another VLM evaluator acts as judger to further improve the effectiveness of the synthetic dataset. (3) Training and fine-tuning with replay data: The collected trajectory data is used to train and fine-tune GUI agent models, which are evaluated on standard agent benchmarks, demonstrating significant improvements. not inherently designed to make complex, sequential action decisions that require long-term observation and historical context. Consequently, training GUI agents with multi-step trajectory data is crucial to improving their capabilities. High-quality agent trajectories contain several key components: high-level goal, sequence of interleaved observations, natural language reasoning, and grounded actions (as shown in Figure 1). Unfortunately, such data is not readily available on the internet like textual or image data, as it involves complex situational reasoning and multimodal interactivity. Existing approaches typically rely on human annotation to collect these trajectories (Deng et al., 2024; Rawles et al., 2023; Li et al., 2024), process that is both expensive and not scalable. To address this data scarcity, data synthesis has emerged as vital approach in AI system development. Synthesizing agent trajectories presents significant challenges due to the need for interwoven natural language instructions, visual observations, and context-specific actions that must be accurately grounded in the GUI environment. Although there have been some successful applications of LLMs in data synthesis pipelines (Ye et al., 2022; Peng et al., 2023; Qin et al., 2023), these complexities still make GUI trajectory synthesis particularly demanding. In this work, we present AgentTrek, scalable data synthesis pipeline specifically designed for training GUI agents. We begin by automatically gathering and filtering tutorial-like text from the web, which describes GUI tasks and workflows in web environments. These tutorials are then transformed into agent tasks with high-level objectives and detailed step-by-step instructions. Using visual-language model (VLM) agent, we simulate the execution of these tasks, guided by the synthesized tutorials. An evaluator model is also employed to subsequently verify whether the goal was successfully achieved. Through this comprehensive pipeline, we efficiently generated large volume of high-quality web agent trajectories. Our experimental results demonstrate that training GUI agent models with these synthesized web agent trajectories not only improves their performance but also enables them to surpass the capabilities of their initial teacher models, which is the replay model GPT-4 in our case. Compared to traditional human-annotated data pipelines, our method is significantly more cost-effective, emphasizing the scalability and economic viability of the AgentTrek pipeline. We introduce AgentTrek, novel pipeline that leverages web tutorials to synthesize highquality web agent trajectory data at scale, effectively bridging the gap between LLM capabilities and the demanding need for multi-step, context-rich training data for GUI agents. 2 Extensive experiments demonstrate that agents trained with our synthesized data outperform those trained on existing datasets in both grounding and planning capabilities, validating the effectiveness of AgentTrek. Our pipeline significantly reduces the cost and scalability obstacles of human-annotated data collection, providing practical approach for large-scale GUI agent training through data synthesis. Table 1: Comparison of AgentTrek with other trajectory datasets for training. For the calculation of dataset size and average steps, see Appendix A. Datasets RUSS ScreenAgent WebLINX MM-Mind2Web GUIAct Size 80 203 969 1009 2482 AgentTrek (Ours) 10398 Average Steps HTML AxTree Intermediate Reasoning Video Matching Screenshot Website Task Inst. Level 5.4 4.3 18.8 7.3 6.7 12. Yes No Yes Yes No Yes No No No No No Yes No Yes No No No Yes No No No No No Yes No Yes Yes No Yes Yes 22 - 155 137 121 Low High & Low High & Low High High 127 High & Low', 'summary': '<p>В данной статье рассматривается проблема обучения агентов, взаимодействующих с графическим интерфейсом пользователя (GUI). Такие агенты могут автоматизировать выполнение задач на различных цифровых платформах, что значительно повышает производительность. Современные большие языковые модели (LLM) обладают мощными способностями к пониманию, рассуждению и принятию решений, но они не оптимизированы для сложных последовательных действий, требующих долгосрочного наблюдения и учета контекста. Поэтому для обучения GUI-агентов необходимы высококачественные траектории, содержащие цель, последовательность наблюдений, рассуждения на естественном языке и действия, привязанные к GUI.</p>\n<p>Проблема заключается в том, что такие данные сложно получить. В отличие от текстовых или изображений, траектории агентов требуют сложного ситуационного рассуждения и мультимодального взаимодействия. Существующие подходы полагаются на дорогостоящую ручную разметку. Чтобы решить эту проблему, в статье предлагается новый конвейер синтеза данных под названием AgentTrek.</p>\n<p>AgentTrek автоматически собирает и фильтрует обучающий текст из интернета, описывающий задачи и рабочие процессы в веб-среде. Эти руководства преобразуются в задачи агента с целями и пошаговыми инструкциями. Затем агент, использующий модель VLM (визуально-языковая модель), имитирует выполнение этих задач, опираясь на синтезированные руководства. Дополнительно используется модель-оценщик, которая проверяет успешность достижения цели.</p>\n<p>С помощью этого конвейера авторы статьи смогли сгенерировать большой объем высококачественных траекторий веб-агентов. Эксперименты показали, что обучение GUI-агентов на этих синтезированных данных не только улучшает их производительность, но и позволяет им превзойти возможности своих исходных моделей-учителей (в данном случае GPT-4). В сравнении с традиционными конвейерами с ручной разметкой, метод AgentTrek значительно более экономичен и масштабируем.</p>\n<p>Таким образом, AgentTrek представляет собой новый подход к синтезу данных для обучения GUI-агентов, используя веб-руководства для создания высококачественных траекторий. Это позволяет преодолеть нехватку данных и снизить затраты на обучение, что делает возможным крупномасштабное обучение GUI-агентов.</p>'}, {'title': 'Method', 'content': 'We introduce pipeline to collect and process GUI tutorials from the internet for training visual language models (VLMs) in web automation tasks. The method comprises three main steps: 1. Collecting Tutorials: We extract web interaction tutorials from large datasets using keyword filtering and language models to identify and standardize relevant content. 2. Guided Replay: An agent uses these tutorials to perform tasks in web environment, interacting with real websites while we record its actions and thoughts. 3. Model Training: We train visual agent model that relies on screenshots and standard GUI actions, enhancing its web navigation capabilities with the collected data. This approach enables efficient training of VLMs without extensive manual annotation, offering scalable solution for automating web tasks. 2.1 AUTOMATIC TUTORIALS COLLECTION FROM INTERNET We first extract web interaction tutorials from Redpajama dataset (Computer, 2023). rule-based heuristic filter is applied to create preliminary dataset, subset of which is annotated by an advanced LLM to generate labeled samples for training effective FastText classification model (Joulin et al., 2017), the tutorial classifier. This classifier further enhances the data quality through filtering. Finally, LLMs are employed to tag and paraphrase the raw text of tutorials into standardized format, preparing them for the replay phase in Section 2.2. 2.1.1 PREFILTER FUNCTION Although GUI tutorials are abundant online, they constitute only small fraction of web content, making pre-filter essential for identifying relevant content. Similar patterns often appear in tutorials, such as distinctive keywords like click and type, as well as platform-specific terms like macOS and Windows. We compiled rule-based filter using keyword lists sourced from official websites and forums. Leveraging RedPajama data with over 20 billion URLs, our pre-filter applies Keyword Matching in the first 38k words, evaluates samples based on Length, and filters them by URL Format for relevance. Validated using 180 positive and 105 negative ground-truth samples, the prefilter achieved 92.69% recall rate on positive samples, ensuring both diversity and quantity. After filtering, the dataset size is reduced from 20.8 billion to 68.8 million entries (Figure 4). 3 Figure 3: Overview of the tutorial filtering and classification pipeline. Starting with Redpajama, the data is prefiltered, annotated by an advanced LLM, and used to train tutorial classifier. The classifier further filters the raw text, which is then paraphrased into structured tutorials with task descriptions, prerequisites, and step-by-step instructions. 2.1.2 LLM LABELER While initial rule-based filtering narrows the context, the proportion of true positive tutorial content remains low. To improve the quality and relevance of selected tutorials, we leverage an advanced LLM, GPT-4O MINI, for automated labeling, due to its strong ability to comprehend and analyze complex, information-dense text. Prior to full implementation, we tested GPT-4O MINI on manually annotated ground-truth validation set, where it achieved an F1 score nearly 90%. In cases where human and LLM annotations conflicted, the LLM demonstrated the ability to identify tutorial-related content in lengthy texts that humans might overlook. This, along with the validation set result, suggests that GPT-4O MINI may surpass human performance in webpage labeling, enabling efficient generation of large labeled dataset for training in the following section. 2.1.3 FASTTEXT FILTER Following the automated labeling process, we employed FastText, an n-gram-based deep learning model, as our classifier. FastText classifies tutorial text segments as tutorial or non-tutorial, with binary output and confidence score to enhance the accuracy of tutorial selection. To train the model, we combined LLM-labeled data with human-labeled samples, creating dataset of approximately 90,000 examples. The train-test split is 95:5, with the model demonstrating strong classification performance. Using this classifier, we further curated the initial filtered dataset, collecting approximately 18.8 million tutorial-like web text samples. Prefilter LLM FastText 0.69 0.885 0.895 Metric Precision Recall F1 0.60 0.89 0. 0.61 0.885 0.895 Table 2: Performance of Filters. 2.1.4 TAG & PARAPHRASE After filtering the raw tutorial content using the FastText model, we then tag and paraphrase the content for further processing, including extracting meta-information and formatting the tutorials according to standardized template. To handle the length and noise of the raw tutorial data, we employed GPT-4O MINI, streamlining the tagging and paraphrasing while ensuring the output aligned with the comprehensive template and gold-standard examples. The key components of the template include specifying the Platform and Target (e.g., macOS, Windows, browser, or app), providing concise Task Description, listing Prerequisites needed Figure 4: The data flow during the early stages of our pipeline. before starting, outlining Step-by-Step Instructions for completing the task, and detailing the Expected Outcome. The cost for tagging and paraphrasing 1,000 entries is approximately 0.89 dollars.', 'summary': '<p>Этот раздел описывает процесс автоматического сбора обучающих материалов (туториалов) по взаимодействию с графическим интерфейсом (GUI) из интернета. Этот процесс является первым шагом в создании набора данных для обучения визуальных языковых моделей (VLMs) для автоматизации веб-задач.</p>\n<p><strong>2.1 Автоматический сбор туториалов из интернета</strong></p>\n<p>Процесс состоит из нескольких этапов:</p>\n<ol>\n<li>\n<p><strong>Предварительная фильтрация (Pre-filter):</strong></p>\n<ul>\n<li>Используется набор эвристических правил, основанных на ключевых словах (например, "кликнуть", "ввести"), характерных для туториалов, а также на терминах, связанных с конкретными платформами (например, "macOS", "Windows").</li>\n<li>Применяется к большому набору данных Redpajama (более 20 миллиардов URL).</li>\n<li>Фильтрует данные на основе ключевых слов, длины текста и формата URL.</li>\n<li>На этом этапе достигается высокая полнота (recall) в 92.69% при отборе релевантных туториалов, что позволяет отсеять большую часть нерелевантных данных, сократив их количество с 20.8 миллиарда до 68.8 миллиона записей.</li>\n</ul>\n</li>\n<li>\n<p><strong>Разметка с помощью большой языковой модели (LLM Labeler):</strong></p>\n<ul>\n<li>Используется продвинутая языковая модель GPT-4O MINI для автоматической разметки данных. Она анализирует текст и определяет, является ли он туториалом.</li>\n<li>Модель предварительно тестируется на вручную размеченном наборе данных, где показывает F1-меру около 90%.</li>\n<li>В случаях расхождения между разметкой человека и LLM, модель иногда обнаруживала туториалы в длинных текстах, которые человек мог пропустить. Это позволило получить более качественные данные для обучения.</li>\n</ul>\n</li>\n<li>\n<p><strong>Фильтрация с помощью FastText:</strong></p>\n<ul>\n<li>Обучается модель FastText (модель глубокого обучения, основанная на n-граммах) для классификации текстовых сегментов как туториал или не туториал.</li>\n<li>Для обучения используется смесь данных, размеченных LLM и вручную, всего около 90 000 примеров.</li>\n<li>Модель демонстрирует хорошие показатели классификации и позволяет дополнительно отфильтровать данные, оставив около 18.8 миллиона примеров, похожих на туториалы.</li>\n</ul>\n</li>\n<li>\n<p><strong>Тегирование и перефразирование (Tag &amp; Paraphrase):</strong></p>\n<ul>\n<li>После фильтрации с помощью FastText, отобранные туториалы тегируются и перефразируются в стандартизированный формат.</li>\n<li>Для этой задачи также используется GPT-4O MINI, который извлекает метаданные и форматирует туториалы в соответствии с шаблоном.</li>\n<li>Шаблон включает в себя:<ul>\n<li><strong>Платформу и цель:</strong> (например, macOS, Windows, браузер или приложение).</li>\n<li><strong>Краткое описание задачи.</strong></li>\n<li><strong>Необходимые условия</strong> для начала выполнения задачи.</li>\n<li><strong>Пошаговые инструкции</strong> для выполнения задачи.</li>\n<li><strong>Ожидаемый результат.</strong></li>\n</ul>\n</li>\n<li>Этот этап позволяет стандартизировать туториалы для дальнейшего использования. Стоимость обработки 1000 записей составляет примерно 0.89 доллара.</li>\n</ul>\n</li>\n</ol>\n<p>В итоге, этот процесс позволяет автоматически собирать и подготавливать большое количество туториалов для обучения визуальных языковых моделей, что является важным шагом для автоматизации веб-задач.</p>'}, {'title': 'Trajectory Data Collection via Guided Replay', 'content': 'Figure 5: Overview of Guided Replay data collection and evaluation pipeline. VLM agent is provided with the filtered and formatted tutorials, then observes and interacts with the real environment during execution, while all the actions and intermediate thoughts are recorded as data trajectory. The final result is evaluated by an advanced VLM to ensure the correctness. 2.2.1 TRAJECTORY DATA DEFINITION The trajectory data generated by our pipeline is designed to enhance an agents web navigation capabilities by integrating high-level planning, low-level instructions, and grounded operations. Each data instance includes the following components: Task Information. Detailed task metadata, including platform, task description, prerequisites, instructions, and expected outcomes, which support both planning and execution. Post-processed Textual Trajectory. Refined after replay, highlighting key elements for model finetuning. This includes Task Metadata, summarizing the task to encourage adaptive decision-making, Observations to provide visual context, Intermediate Reasoning offering insights into the agents decision-making process, and Action Sequence to capture detailed element information for web interactions. Screenshots and Video Recordings. Visual records of the entire process for comprehensive documentation. Reproducible Native Trace. Captured via Playwright, including DOM snapshots, HTML, network flow, and action sequences, allowing full reconstruction and detailed analysis of agent-environment interactions.', 'summary': '<p>В этом разделе описывается процесс сбора и оценки данных для обучения агента веб-навигации, а также структура самих данных.</p>\n<p><strong>Процесс сбора данных (Guided Replay):</strong></p>\n<ol>\n<li><strong>Подготовка:</strong> Агенту (VLM agent) предоставляются отфильтрованные и отформатированные обучающие материалы (tutorials).</li>\n<li><strong>Взаимодействие:</strong> Агент взаимодействует с реальной веб-средой, выполняя задания из обучающих материалов.</li>\n<li><strong>Запись:</strong> В процессе взаимодействия записываются все действия агента, его промежуточные размышления и результаты. Это формирует траекторию выполнения задачи.</li>\n<li><strong>Оценка:</strong> Полученный результат оценивается с помощью продвинутой VLM (большой языковой модели), чтобы убедиться в правильности выполнения задачи.</li>\n</ol>\n<p><strong>Структура данных траектории:</strong></p>\n<p>Данные траектории предназначены для улучшения способностей агента к веб-навигации за счет интеграции планирования высокого уровня, инструкций низкого уровня и обоснованных действий. Каждая запись данных включает в себя:</p>\n<ul>\n<li><strong>Информация о задаче (Task Information):</strong> Подробные метаданные о задаче, такие как платформа, описание задачи, необходимые условия, инструкции и ожидаемые результаты. Эта информация помогает агенту как в планировании, так и в выполнении задачи.</li>\n<li><strong>Обработанная текстовая траектория (Post-processed Textual Trajectory):</strong> Траектория, доработанная после повторного прохождения (replay). Она содержит:<ul>\n<li><strong>Метаданные задачи (Task Metadata):</strong> Краткое описание задачи для адаптивного принятия решений.</li>\n<li><strong>Наблюдения (Observations):</strong> Визуальный контекст, полученный агентом во время выполнения задачи.</li>\n<li><strong>Промежуточные рассуждения (Intermediate Reasoning):</strong> Записи о процессе принятия решений агентом.</li>\n<li><strong>Последовательность действий (Action Sequence):</strong> Детальная информация о взаимодействиях агента с веб-элементами.</li>\n</ul>\n</li>\n<li><strong>Скриншоты и видеозаписи (Screenshots and Video Recordings):</strong> Визуальная документация всего процесса для более полного анализа.</li>\n<li><strong>Воспроизводимый нативный след (Reproducible Native Trace):</strong> Запись, полученная с помощью Playwright, включающая в себя снимки DOM, HTML, сетевой трафик и последовательность действий. Это позволяет полностью реконструировать взаимодействие агента со средой и провести детальный анализ.</li>\n</ul>\n<p><strong>В заключение:</strong> Данные траектории, полученные в процессе Guided Replay, содержат подробную информацию о том, как агент взаимодействует с веб-средой. Эта информация включает в себя как текстовые описания, так и визуальные данные, что позволяет обучать агентов веб-навигации более эффективно.</p>'}, {'title': 'Guided Replay with Tutorials', 'content': 'Although we have collected and processed high-quality tutorials, significant gap remains in acquiring the grounding data crucial for training more effective agent model. To address this, we leverage BrowserGym (Drouin et al., 2024) to enable the model to replay tasks under the guidance of the generated tutorials. Figure 6: Guided replay example. This example demonstrates an agents execution of finding the return policy for mens football apparel, showcasing its actions alongside the corresponding inner thoughts. BrowserGym is versatile environment for web task automation in the Chromium browser, enabling Visual Language Model (VLM) agents to execute web-based operations (Drouin et al., 2024). Agents are provided with tagged and paraphrased tutorials and target web url, allowing them to navigate directly to the task scene. Step-by-step instructions guide the agent through the task, with the expected outcome determining task completion. The agents initial observations include the webpages viewport screenshot and accessibility tree (AXTree), but the HTML file is excluded due to its size and irrelevance to visual agents. Actions are executed using Playwright (Microsoft, 2023) functions such as click, select option, and clear, while Playwright also records detailed traces, including target elements, coordinates, screenshots, and DOM snapshots at the same time, along with agents internal thoughts between actions. Token consumption is about 8,027 per step and 86,114 per task. With GPT-4O-08-06, replaying 1,000 tasks costs approximately 215 dollars. Cost detail see 2.2.3 EVALUATION OF TRAJECTORY Although large amount of guided replay data has been recorded, it is crucial to extract the effective segments that can truly contribute to enhancing the agents performance. Recent work by (Pan et al., 2024) highlights the potential of Visual Language Models (VLMs) in evaluating trajectory data using recorded images and interaction processes as input. VLMs are highly scalable, capable of processing large datasets concurrently at low cost, and provide transparent evaluations. Therefore, we implemented VLM Evaluator to further improve our data quality. VLM Evaluator Design. To ensure trajectory data quality, we define effectiveness based on two criteria: adherence to task instructions and successful completion of core components. We employ GPT-4O as the backbone of our VLM evaluator, using structured prompt to assess recorded trajectories. The evaluator receives the task description d, the agents action history a, and inner thoughts for each step. The sequential format is: {task description; inner thought 1; action 1; inner thought 2; action 2; ...}, as illustrated in Figure 5. The VLM provides trajectory-level assessment and performs stepwise analysis, offering justifications for any ineffective trajectory and identifying the earliest point of failure. Validation on Human-Annotated Set. Although the capabilities of Vision Language Models (VLMs) are well-recognized, validation is essential. To assess the automatic evaluators perfor6 Table 3: Evaluator Accuracy Comparison Table 4: Cost Breakdown Trajectory Evaluator Replayed Web Tutorials GPT-4o WebArena Results GPT-4V Cap. + GPT-4 Cap. + Mixtral Acc. 84.0% 80.6% 82.1% 74.4% Phase Cost/1k ($) Model T&P Replay Eval Total 0.89 215.36 3.10 219.35 gpt-4o-mini gpt-4o gpt-4o mance, we manually reviewed 1,081 trajectories and created validation set of 558 samples with human-annotated justifications. As shown in the Table 3, despite handling different input formats across various task scenarios, the evaluator achieved high performance metrics. And according to the observation shown in Appendix D, evaluator often applys stricter standards than human evaluators. This demonstrates its robustness in accurately identifying effective trajectories. 2.3 TRAIN AND FINE-TUNE THE MODEL WITH TRAJECTORY DATA We chose purely visual agent model that relies exclusively on screenshot-based observations, rather than incorporating accessibility trees or textual representations, for several reasons. First, GUIs are inherently visual, and mapping instructions to visual elements aligns more closely with human cognitive processes. Second, Textual representations, such as HTML or accessibility trees, are often verbose, which leads to heavy overhead for computation. Second, Different websites can have varying structures for their textual representation while image-based representations allow the model to unify its observations across diverse platforms with varying resolutions, improving generalization. 2.3.1 PURE VISION & GUI ACTION FRAMEWORK In this work, we propose to unify observation and action space via pure vision and standard pyautogui commands with pluggable action system. Using pure vision as input eliminates the need for the model to understand different UI source codes across platforms, even when visually similar elements are written differently in HTML. Additionally, HTML input typically costs an average of 4,000 tokens per step. In contrast, recent VLMs with high-resolution multimodal understanding, such as Qwen2-VL, require only 1,200 tokens for 720p image. This significantly lowers the computational cost while maintaining sufficient visual information for the task. For action, we hoose the widely used standard pyautogui action space with pluggable action system. Most web agent leverage playwright action sapce. But playwright actions incline to interact with html selector element instead of visual ui element. Therefore, we use pyautogui commands to unify basic GUI operations on web. Since we collect the data from website by playwright, we need to map the playwright actions to pyautogui actions as shown in the Figure 9. In addition, we utilize pluggable action system to cover specific playwright action like select option. 2.3.2 MODEL ARCHITECTURE AND TRAINING Unlike agents that rely on structured UI representations like accessibility trees, vision-based grounding requires models to map intents directly to visual observations. For this, we chose Qwen2VL (Wang et al., 2024), which uses NaViT as an image encoder with dynamic resolution support Dehghani et al. (2023). By removing absolute position embeddings and incorporating 2D-RoPE Su et al. (2024), Qwen2-VL can process images of any resolution, efficiently converting them into variable visual tokens. This makes Qwen2-VL ideal for GUI agents, as it can encode high-resolution images with fewer token costs, making it well-suited for our tasks. Our training process, starting with VLM capable of high-resolution image understanding, consists of one tunnig stage. We use data from AgentTrek Data to enhance VLM capabilities in grounding and planning.', 'summary': '<h2>Изложение раздела статьи: Улучшение данных для обучения агента</h2>\n<p>Для обучения более эффективных моделей агентов, авторы статьи столкнулись с проблемой нехватки качественных данных. Чтобы решить эту проблему, они использовали среду BrowserGym, которая позволяет модели "переигрывать" задачи, опираясь на сгенерированные ранее обучающие материалы.</p>\n<p><strong>BrowserGym</strong> - это среда для автоматизации веб-задач в браузере Chromium. Она позволяет агентам, использующим модели VLM (Visual Language Model), выполнять действия на веб-страницах. Агентам предоставляются размеченные и перефразированные инструкции, а также URL целевой страницы. Пошаговые инструкции ведут агента через задачу, а ее успешное выполнение определяется достижением ожидаемого результата.</p>\n<p>Наблюдения агента включают скриншот видимой области веб-страницы и дерево доступности (AXTree). HTML-файл исключается из-за его большого размера и нерелевантности для визуальных агентов. Действия выполняются с помощью функций Playwright, таких как клик, выбор опции и очистка поля. Playwright также записывает подробные трассировки, включая целевые элементы, координаты, скриншоты и DOM-снимки, а также внутренние размышления агента между действиями.</p>\n<p>Стоимость обработки данных довольно высока: около 8027 токенов на шаг и 86114 токенов на задачу. Переигрывание 1000 задач с использованием GPT-4O-08-06 обходится примерно в 215 долларов.</p>\n<p><strong>Оценка траекторий</strong></p>\n<p>После записи большого объема данных о переигранных задачах, авторы статьи столкнулись с необходимостью выделения эффективных сегментов, которые действительно могут улучшить производительность агента. Для этого они разработали <strong>VLM Evaluator</strong> (оценщик на основе VLM), который оценивает качество записанных траекторий, используя изображения и последовательность действий агента. VLM-модели хорошо масштабируются, способны обрабатывать большие объемы данных параллельно и обеспечивают прозрачную оценку.</p>\n<p><strong>Дизайн VLM Evaluator</strong></p>\n<p>Эффективность траекторий определяется двумя критериями: соблюдение инструкций и успешное выполнение ключевых компонентов задачи. В качестве основы оценщика используется модель GPT-4O. Оценщик получает описание задачи, историю действий агента и его внутренние размышления на каждом шаге. Данные представлены в последовательном формате: {описание задачи; размышление 1; действие 1; размышление 2; действие 2; ...}. VLM предоставляет общую оценку траектории и проводит пошаговый анализ, обосновывая неэффективные траектории и определяя момент первой ошибки.</p>\n<p><strong>Валидация на данных с ручной разметкой</strong></p>\n<p>Для оценки производительности автоматического оценщика авторы создали набор данных из 558 траекторий с ручной разметкой, основанной на анализе 1081 траектории. Результаты показали высокую точность оценщика, несмотря на различия в форматах входных данных и сценариях задач. Более того, оценщик часто применял более строгие стандарты, чем люди-оценщики, что говорит о его надежности в выявлении эффективных траекторий.</p>\n<p><strong>Обучение и дообучение модели</strong></p>\n<p>Для обучения агента авторы выбрали модель, основанную исключительно на визуальных данных (скриншотах), а не на деревьях доступности или текстовых представлениях. Это обусловлено тем, что графические интерфейсы по своей природе визуальны, и сопоставление инструкций с визуальными элементами больше соответствует человеческому восприятию. Кроме того, текстовые представления часто бывают многословными и приводят к большим вычислительным затратам.</p>\n<p><strong>Фреймворк для визуального управления и действий в GUI</strong></p>\n<p>В статье предлагается унифицировать пространство наблюдений и действий с помощью визуальных данных и стандартных команд pyautogui с подключаемой системой действий. Использование визуальных данных устраняет необходимость для модели понимать различия в исходном коде интерфейса на разных платформах. Кроме того, визуальные данные требуют меньшего количества токенов для обработки по сравнению с HTML.</p>\n<p>В качестве пространства действий выбраны стандартные команды pyautogui, которые позволяют унифицировать основные операции с GUI. Поскольку данные собираются с веб-сайтов с помощью Playwright, необходимо преобразовать действия Playwright в действия pyautogui. Кроме того, используется подключаемая система действий для охвата специфических действий Playwright, таких как выбор опции.</p>\n<p><strong>Архитектура модели и обучение</strong></p>\n<p>Для обучения модели, которая способна сопоставлять намерения напрямую с визуальными наблюдениями, была выбрана модель Qwen2-VL, использующая NaViT в качестве кодировщика изображений. Qwen2-VL способна обрабатывать изображения любого разрешения, эффективно преобразуя их в переменные визуальные токены. Это делает Qwen2-VL идеальным выбором для агентов GUI, поскольку она может кодировать изображения высокого разрешения с меньшими затратами токенов.</p>\n<p>Процесс обучения состоит из одного этапа дообучения. Используются данные из AgentTrek Data для улучшения возможностей VLM в области привязки и планирования.</p>'}, {'title': 'Experimental Setup', 'content': 'Agent Training. For the text-based web agent, we utilize 6,000 agent trajectories, included accessibility tree as observations and playwright actions as the agents action space, from the AgentTrek dataset to fine-tune the Qwen2 series LLMs of various sizes, including 7B, 32B, and 72B. For the vision-only Web agent, we fine-tune Qwen2-VL Wang et al. (2024) using 10,000 selected agent trajectories from the dataset. Evaluation For Text-based Web Agent. To demostrate the capability of the text-based agent, we select WebArena Zhou et al. (2023) as evaluation benchmark. WebArena, based on real websites, creates multiple virtual environments and uses various evaluation methods to assess the task completion rate, making it more suitable for real-world task completion evaluation. Evaluation For Vision-based Web Agent. To validate the effectiveness of our dataset, it is essential to demonstrate its impact on improving both the grounding and planning capabilities of the model. Therefore, we will evaluate the models performance on two benchmarks that assess these abilities. (1) ScreenSpot (Cheng et al., 2024), which is GUI visual grounding benchmark comprising 1.2K single-step instructions and target element bounding boxes. It spans mobile, desktop, and web environments, categorizing elements into text and icons. Given that our data originates exclusively from the web, we focus solely on web-based performance. (2) Multimodal-Mind2Web (Deng et al., 2024), the multimodal extension of the web agent benchmark Mind2Web (Deng et al., 2024). It consists of three categories, namely, cross-task, cross-website and cross-domain, ranked by their degree of deviation from the training data. 3.2 MAIN RESULTS WebArena. Through the experimental results from table 5, we can obtain that: (1) AgentTreks textual trajectories significantly boost performance on WebArena, surpassing open-source baselines and GPT-4o. (2) Considering that WebArena is an OOD web agent benchmark featuring self-hosted websites, strong results on WebArena confirm that AgentTreks data generalizes well to unseen domains. Table 5: Comparison of task success rate on WebArena Model WebArena CodeLlama-7B-Instruct (Ou et al., 2024) LLaMa3-chat-8B (Ou et al., 2024) Qwen2.5-7B-Instruct LLama3-chat-70B (Ou et al., 2024) GPT-4o(Zhou et al., 2023) GPT-4(Ou et al., 2024) Synatra-CodeLlama-7B (Ou et al., 2024) AutoWebGLM (OOD SFT) (Lai et al., 2024) AutoWebGLM (In-domain RFT) (Lai et al., 2024) Qwen2.5-7B-Instruct w/ AgentTrek Qwen2.5-32B-Instruct w/ AgentTrek 0.00 3.32 3.80 7.02 13.10 14.41 6.28 8.50 18.20 10.46 16. ScreenSpot. Fine-tuning with the AgentTrek dataset significantly improved Qwen2-VLs grounding ability for both text and icon-based tasks, more than doubling its baseline performance and surpass8 ing several models on the ScreenSpot benchmark. This demonstrates the strong impact of AgentTrek in enhancing the models grounding capabilities for web-based GUI tasks. Table 6: Comparison of grounding performance on ScreenSpot Web Grounding Model Text Icon/Widget Average GPT-4 (Cheng et al., 2024) GPT-4o (Cheng et al., 2024) Qwen2-VL-7B SeeClick (Cheng et al., 2024) CogAgent (Cheng et al., 2024) GPT-4 + OmniParser (Lu et al., 2024) Qwen2-VL-7B w/ AgentTrek 9.2 12.2 35.2 55.7 70.4 81.3 81.7 8.8 7.8 25.7 32.5 28.6 51. 51.5 9.0 10.1 30.7 44.7 50.7 67.0 67.4 Mind2Web. The baseline Qwen2-VL-7B model is excluded due to its poor ability to locate target elements, critical requirement for web-based tasks. As result, only the fine-tuned models are included in the table. Fine-tuning with the AgentTrek dataset greatly enhanced Qwen2-VLs performance, particularly in the Operation F1 metric, where it surpassed both GPT-3.5 and GPT-4 in all settings. The combination of AgentTrek and Mind2Web datasets delivers the best results across all metrics and settings. While fine-tuning with Mind2Web alone yields strong performance, the model still benefits from the addition of AgentTrek data. These findings highlight the complementary strengths of the two datasets: AgentTrek provides precise, grounded data, while Mind2Web contributes valuable resources for tackling complex web-based tasks. Additional details on result sources are provided in Appendix J.2. Table 7: Performance comparison across different methods and evaluation settings. H, I, AT, M2W stand for HTML, Image, AgentTrek, Mind2Web Obs Model Method Cross-Task Cross-Website Cross-Domain Ele.Acc Op.F Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR HTML + Image GPT-3.5 GPT-4 GPT-4 GPT-4 Choice Choice Choice SoM Qwen2-VL Vision + AT + M2W Vision + AT + M2W Vision 19.4 40. 46.4 29.6 45.5 54.8 60.8 59.2 63.1 73.4 - 84.9 89.5 88.9 16.8 32. 40.2 20.3 40.9 50.9 55.7 14.9 30.2 38.0 20.1 40.8 52.9 57.6 56.5 61. 67.8 - 82.8 83.9 88.1 14.1 27.0 32.4 13.9 35.1 44.9 51.4 25.2 35. 42.4 27.0 48.6 51.8 56.0 57.9 61.9 69.3 - 84.1 86.8 87.5 24.1 29. 36.8 23.7 42.1 47.7 52.', 'summary': '<p>В этой статье рассматривается обучение агентов для взаимодействия с веб-страницами, как с текстовой, так и с визуальной информацией.</p>\n<p><strong>Обучение агентов.</strong> Для обучения текстового веб-агента использовались 6000 траекторий агента из набора данных AgentTrek. В качестве наблюдений использовалось дерево доступности (accessibility tree), а в качестве действий агента — действия Playwright. Модели Qwen2 различных размеров (7B, 32B и 72B) были дообучены на этих данных. Для визуального веб-агента была дообучена модель Qwen2-VL, используя 10000 траекторий агента из того же набора данных.</p>\n<p><strong>Оценка текстового веб-агента.</strong> Для оценки способностей текстового агента был выбран бенчмарк WebArena, который основан на реальных веб-сайтах и использует различные методы оценки, включая процент успешного выполнения задач. Это делает его более подходящим для оценки в реальных условиях.</p>\n<p><strong>Оценка визуального веб-агента.</strong> Для оценки визуального агента использовались два бенчмарка, которые проверяют способность модели к привязке (grounding) и планированию.\n*   <strong>ScreenSpot</strong> — бенчмарк для визуальной привязки, содержащий 1200 инструкций и ограничивающих рамок целевых элементов. Он охватывает мобильные, десктопные и веб-среды, классифицируя элементы на текст и иконки. Так как данные для обучения были получены только из веб-среды, оценка проводилась только в веб-среде.\n*   <strong>Multimodal-Mind2Web</strong> — мультимодальное расширение бенчмарка Mind2Web, состоящее из трех категорий (cross-task, cross-website и cross-domain), ранжированных по степени отклонения от обучающих данных.</p>\n<p><strong>Основные результаты.</strong></p>\n<p><strong>WebArena.</strong> Эксперименты показали, что использование траекторий AgentTrek значительно повышает производительность на WebArena, превосходя открытые базовые модели и GPT-4o. Сильные результаты на WebArena, который является бенчмарком для веб-агентов, работающих с самостоятельно размещенными сайтами, подтверждают, что данные AgentTrek хорошо обобщаются на невидимые домены.</p>\n<p><strong>ScreenSpot.</strong> Дообучение с использованием AgentTrek значительно улучшило способность модели Qwen2-VL к привязке как для текстовых, так и для иконочных задач, более чем удвоив базовую производительность и превзойдя несколько моделей на бенчмарке ScreenSpot. Это демонстрирует сильное влияние AgentTrek на улучшение способностей модели к привязке для веб-задач с графическим интерфейсом.</p>\n<p><strong>Mind2Web.</strong> Базовая модель Qwen2-VL-7B была исключена из оценки из-за ее низкой способности находить целевые элементы. Дообучение с использованием AgentTrek значительно улучшило производительность Qwen2-VL, особенно в метрике Operation F1, где она превзошла GPT-3.5 и GPT-4 во всех настройках. Комбинация данных AgentTrek и Mind2Web дает наилучшие результаты по всем метрикам и настройкам. Хотя дообучение только на данных Mind2Web также дает хорошие результаты, добавление данных AgentTrek все равно улучшает производительность. Это подчеркивает взаимодополняющие сильные стороны двух наборов данных: AgentTrek предоставляет точные, привязанные к контексту данные, а Mind2Web предоставляет ценные ресурсы для решения сложных веб-задач.</p>'}, {'title': 'Analysis', 'content': 'With our AgentTrek pipeline, we generate large-scale trajectory data that excels in three areas. First, the dataset offers extensive diversity, covering multiple domains and task types, and benefiting from internet-sourced tutorials that enhance task execution. Our experiment showed 230% performance increase when agents followed detailed instructions. Second, the data is gathered from real-world web environments, avoiding the limitations of simulations. Starting with RedPajama, we filtered and processed 23,430 tutorials, producing 10,398 successful trajectories from 127 websites. Third, the data is comprehensive, capturing highand low-level task details, including DOM/HTML structures, AXTree snapshots, video recordings, and screenshots. This rich data improves the agents performance on long-horizon tasks, and with per-trajectory cost of just $0.551, our pipeline offers an efficient, scalable solution for data generation. 4.1 IMPORTANCE OF TUTORIALS Tutorials extracted from the internet play crucial role in guiding the replay process. First, they ensure diversity in the generated trajectories. Tutorials often have distinct task goals, and even when they target the same objective, they may offer different execution methods, enriching the trajectory 9 data. Second, tutorials significantly improve the agents execution. We tested the agent on 400 tasks, replaying them twice: once with tutorials and once using only high-level goals. The results show that step-by-step instructions greatly enhanced performance. Without tutorials, only 63 effective trajectories were generated (15.78% of the total). With tutorials, the agent produced 208 effective trajectories (52%), marking an increase of over 230%, demonstrating the importance of detailed instructions in improving reliability and effectiveness. For further analysis, please see Appendix B', 'summary': '<p><strong>4.1 Важность обучающих материалов</strong></p>\n<p>Обучающие материалы, полученные из интернета, играют ключевую роль в процессе генерации траекторий действий агента. Во-первых, они обеспечивают разнообразие генерируемых траекторий. Различные обучающие материалы часто имеют уникальные цели, а даже при схожих целях могут предлагать разные способы их достижения. Это значительно обогащает данные о траекториях.</p>\n<p>Во-вторых, обучающие материалы заметно улучшают выполнение задач агентом. В ходе эксперимента агенту было предложено выполнить 400 задач дважды: один раз, следуя инструкциям из обучающих материалов, и второй раз, опираясь только на высокоуровневые цели. Результаты показали, что пошаговые инструкции значительно повышают эффективность. Без обучающих материалов было сгенерировано всего 63 успешные траектории (15.78% от общего числа). С использованием обучающих материалов количество успешных траекторий возросло до 208 (52%), что более чем на 230% больше. Это демонстрирует важность подробных инструкций для повышения надежности и эффективности работы агента. (Более подробный анализ можно найти в Приложении B).</p>\n<p><em>Комментарий: В этом разделе подчеркивается, что использование обучающих материалов из интернета не только обеспечивает разнообразие данных, но и значительно повышает качество работы агента за счет предоставления подробных пошаговых инструкций.</em></p>'}, {'title': 'Data Composition', 'content': 'Figure 7: The distribution of website with domains involved in our dataset To summarize the data flow through our pipeline: First, we filter tutorial data from the RedPajama web snapshot. Next, the filtered data is paraphrased for clarity and classification. We then gather up-to-date data from mainstream websites for replay and, finally, collect effective trajectory data from the replays. After filtering RedPajamas vast dataset, we retained over 18.8 million entries. By applying criteria such as recency and popularity, 23,430 tutorials were prepared for replay. With success rate of 44.4%, we generated 10,398 trajectories, covering 127 websites across 11 distinct categories. 4.3 COMPARISON WITH EXISTING WORK AND RESEARCH CHALLENGES AgentTrek generates comprehensive, large-scale trajectory data, excelling in several key areas as shown in Table 1 (Niu et al., 2024; L`u et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024). First, with nearly 5k verified trajectories and an average of 12.1 steps per trajectory, our dataset provides strong foundation for training and evaluating agents on longhorizon web tasks. Second, it is the most complete dataset to date, incorporating DOM/HTML structures, AXTree data, intermediate reasoning steps, full video recordings, and corresponding screenshots for each action. Moreover, despite full automation without human intervention, our dataset maintains diversity across 120 websites and 12 distinct task categories. By leveraging modern large language models (LLMs), we can extract both high-level task objectives and detailed step-by-step instructions, offering flexibility for future use. Finally, our pipeline significantly reduces the cost and scalability challenges of human-annotated data collection. With success rate factored in, the cost per trajectory is just $0.551, making our approach both efficient and scalable for large-scale data generation. Cost detail see C', 'summary': '<p><strong>Распределение веб-сайтов и сравнение с существующими работами</strong></p>\n<p>В рамках исследования был разработан конвейер обработки данных, который начинается с фильтрации обучающих материалов из веб-снимка RedPajama. Эти данные затем перефразируются для ясности и классификации. Далее, собираются актуальные данные с популярных веб-сайтов для воспроизведения (replay), и, наконец, из этих воспроизведений извлекаются эффективные траектории действий. После фильтрации из огромного набора данных RedPajama было сохранено более 18.8 миллионов записей. Применив критерии актуальности и популярности, для воспроизведения было подготовлено 23 430 обучающих материалов. С коэффициентом успешности 44.4% было сгенерировано 10 398 траекторий, охватывающих 127 веб-сайтов в 11 различных категориях. Распределение веб-сайтов по доменам представлено на рисунке 7 (не приводится).</p>\n<p>В сравнении с существующими работами (Niu et al., 2024; Lu et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024), AgentTrek генерирует полные, крупномасштабные данные о траекториях, превосходя их по нескольким ключевым параметрам (см. Таблицу 1, не приводится). Во-первых, с почти 5 тысячами проверенных траекторий и в среднем 12.1 шага на траекторию, набор данных обеспечивает прочную основу для обучения и оценки агентов в задачах веб-навигации с длинными горизонтами. Во-вторых, это самый полный набор данных на сегодняшний день, включающий DOM/HTML структуры, данные AXTree, промежуточные этапы рассуждений, полные видеозаписи и соответствующие скриншоты для каждого действия. Более того, несмотря на полную автоматизацию без вмешательства человека, набор данных поддерживает разнообразие по 120 веб-сайтам и 12 различным категориям задач. Благодаря использованию современных больших языковых моделей (LLM) можно извлекать как высокоуровневые цели задач, так и подробные пошаговые инструкции, что обеспечивает гибкость для будущего использования. Наконец, конвейер значительно снижает затраты и проблемы масштабируемости, связанные со сбором данных, аннотированных людьми. С учетом коэффициента успешности, стоимость одной траектории составляет всего 0.551 доллара, что делает этот подход эффективным и масштабируемым для генерации данных в больших объемах. Подробная информация о стоимости приведена в приложении C (не приводится).</p>'}, {'title': 'Related Work', 'content': 'LLM-based Agents. LLM-based agents are autonomous systems that leverage large language models (Brown et al., 2020) to interact with real-world websites and os environments. These agents can 10 understand natural language instructions and perform wide range of complex tasks across various domains, such as e-commerce, online assistance, and knowledge navigation (Nakano et al., 2021; Cheng et al., 2024). Recent efforts in this space include models like SeeAct (Zheng et al., 2024) and WebVoyager (He et al., 2024), which aim to generalize agent behavior to real-world websites. While LLM-based agents have shown promise, challenges remain in the need for agent specified data. Our work extends this line of research by introducing cost-effective pipeline to generate comprehensive agent trajectory data, advancing the state-of-the-art in data synthesis for agent-based applications. Agent Data. As agents gain increasing popularity, the demand for efficient and scalable data is becoming both larger and more urgent. However, most existing data primarily serve as supplements to various benchmarks (Zhou et al., 2023; Li et al., 2023; Deng et al., 2024), with few datasets specifically designed for agent trajectory analysis. Furthermore, these datasets are often limited by the need for human annotation, which hampers scalability. In our work, our pipeline managed to automatically generate comprehensive agent trajectory data in cost-efficient manner, paving the way for new direction in data synthesis within the field of agents. Automatic Evaluation for Digital Agents. Recently, there has been growing interest in automating the evaluation of digital agents using Vision-Language Models (VLMs) and Large Language Models (LLMs). These methods leverage models to assess agent performance in real-world tasks. Research in this area spans several dimensions: some works focus on trajectory-level success (Pan et al., 2024), while others evaluate stepwise success based on adherence to instructions (Wornow et al., 2024). Additionally, evaluations are conducted across various task environments, such as web-based platforms and mobile operating systems like Android and iOS (Pan et al., 2024). In our work, we prompt VLM, GPT-4o, as an autonomous evaluator, using agents interacton process as inputs to assess whether the agent has successfully completed tasks at the trajectory level.', 'summary': '<p><strong>Агенты на основе больших языковых моделей (LLM)</strong></p>\n<p>Агенты, основанные на больших языковых моделях (LLM), представляют собой автономные системы, которые используют LLM для взаимодействия с веб-сайтами и операционными системами. Эти агенты способны понимать инструкции на естественном языке и выполнять разнообразные сложные задачи в различных областях, таких как электронная коммерция, онлайн-помощь и навигация по знаниям. Примеры таких моделей включают SeeAct и WebVoyager, которые стремятся обобщить поведение агентов на реальных веб-сайтах.</p>\n<p>Несмотря на многообещающие результаты, существуют проблемы, связанные с необходимостью в специализированных данных для обучения агентов. Большинство существующих наборов данных служат лишь дополнением к различным бенчмаркам, и лишь немногие из них специально разработаны для анализа траекторий агентов. Кроме того, эти наборы данных часто ограничены необходимостью ручной разметки, что затрудняет масштабирование. В данной работе представлен экономичный конвейер для автоматической генерации полных данных о траекториях агентов, что открывает новые возможности в синтезе данных для агентских приложений.</p>\n<p><strong>Автоматическая оценка цифровых агентов</strong></p>\n<p>В последнее время растет интерес к автоматизации оценки цифровых агентов с использованием моделей "зрение-язык" (VLM) и LLM. Эти методы используют модели для оценки производительности агентов в реальных задачах. Исследования в этой области охватывают несколько направлений: некоторые работы сосредоточены на успехе на уровне траектории, в то время как другие оценивают успех на каждом шаге, основываясь на соблюдении инструкций. Кроме того, оценки проводятся в различных средах задач, таких как веб-платформы и мобильные операционные системы, такие как Android и iOS. В данной работе в качестве автономного оценщика используется VLM (GPT-4o), который анализирует процесс взаимодействия агентов, чтобы определить, успешно ли агент выполнил задачи на уровне траектории.</p>'}]}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture', '#audio', '#benchmark', '#cv', '#data (1)', '#dataset (1)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal', '#open_source', '#optimization (1)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (1)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            
            <div class="summaries">
                <div class="summary_title">Abstract</div>
                <div class="summary_text"><p>В статье рассматривается проблема нехватки качественных данных для обучения агентов, управляющих графическим интерфейсом (GUI). Такие агенты могут автоматизировать сложные задачи в различных цифровых средах, но их разработка затруднена из-за отсутствия размеченных данных с последовательностью действий (траекторий). Существующие методы разметки требуют больших затрат времени и ресурсов, так как предполагают ручную работу.</p>
<p>Чтобы решить эту проблему, авторы предлагают AgentTrek – масштабируемый конвейер для синтеза данных. Он автоматически генерирует траектории действий агента, используя веб-инструкции. Метод работает следующим образом:</p>
<ol>
<li><strong>Сбор инструкций:</strong> AgentTrek автоматически собирает текстовые инструкции, похожие на обучающие материалы, из интернета.</li>
<li><strong>Преобразование в задачи:</strong> Эти тексты преобразуются в задачи с пошаговыми инструкциями.</li>
<li><strong>Моделирование выполнения:</strong> Агент, использующий визуально-языковую модель (VLM), имитирует выполнение этих инструкций в реальной цифровой среде.</li>
<li><strong>Оценка корректности:</strong> VLM-модель также используется для оценки правильности сгенерированных траекторий.</li>
</ol>
<p>Результаты показывают, что обучение GUI-агентов с использованием синтезированных таким образом траекторий значительно улучшает их способность понимать контекст и планировать действия по сравнению с существующими моделями. Кроме того, предложенный метод более экономичен, чем традиционная ручная разметка.</p>
<p>Таким образом, статья демонстрирует, что использование веб-инструкций для обучения агентов является перспективной стратегией для создания более мощных и автономных цифровых агентов.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Introduction</div>
                <div class="summary_text"><p>В данной статье рассматривается проблема обучения агентов, взаимодействующих с графическим интерфейсом пользователя (GUI). Такие агенты могут автоматизировать выполнение задач на различных цифровых платформах, что значительно повышает производительность. Современные большие языковые модели (LLM) обладают мощными способностями к пониманию, рассуждению и принятию решений, но они не оптимизированы для сложных последовательных действий, требующих долгосрочного наблюдения и учета контекста. Поэтому для обучения GUI-агентов необходимы высококачественные траектории, содержащие цель, последовательность наблюдений, рассуждения на естественном языке и действия, привязанные к GUI.</p>
<p>Проблема заключается в том, что такие данные сложно получить. В отличие от текстовых или изображений, траектории агентов требуют сложного ситуационного рассуждения и мультимодального взаимодействия. Существующие подходы полагаются на дорогостоящую ручную разметку. Чтобы решить эту проблему, в статье предлагается новый конвейер синтеза данных под названием AgentTrek.</p>
<p>AgentTrek автоматически собирает и фильтрует обучающий текст из интернета, описывающий задачи и рабочие процессы в веб-среде. Эти руководства преобразуются в задачи агента с целями и пошаговыми инструкциями. Затем агент, использующий модель VLM (визуально-языковая модель), имитирует выполнение этих задач, опираясь на синтезированные руководства. Дополнительно используется модель-оценщик, которая проверяет успешность достижения цели.</p>
<p>С помощью этого конвейера авторы статьи смогли сгенерировать большой объем высококачественных траекторий веб-агентов. Эксперименты показали, что обучение GUI-агентов на этих синтезированных данных не только улучшает их производительность, но и позволяет им превзойти возможности своих исходных моделей-учителей (в данном случае GPT-4). В сравнении с традиционными конвейерами с ручной разметкой, метод AgentTrek значительно более экономичен и масштабируем.</p>
<p>Таким образом, AgentTrek представляет собой новый подход к синтезу данных для обучения GUI-агентов, используя веб-руководства для создания высококачественных траекторий. Это позволяет преодолеть нехватку данных и снизить затраты на обучение, что делает возможным крупномасштабное обучение GUI-агентов.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Method</div>
                <div class="summary_text"><p>Этот раздел описывает процесс автоматического сбора обучающих материалов (туториалов) по взаимодействию с графическим интерфейсом (GUI) из интернета. Этот процесс является первым шагом в создании набора данных для обучения визуальных языковых моделей (VLMs) для автоматизации веб-задач.</p>
<p><strong>2.1 Автоматический сбор туториалов из интернета</strong></p>
<p>Процесс состоит из нескольких этапов:</p>
<ol>
<li>
<p><strong>Предварительная фильтрация (Pre-filter):</strong></p>
<ul>
<li>Используется набор эвристических правил, основанных на ключевых словах (например, "кликнуть", "ввести"), характерных для туториалов, а также на терминах, связанных с конкретными платформами (например, "macOS", "Windows").</li>
<li>Применяется к большому набору данных Redpajama (более 20 миллиардов URL).</li>
<li>Фильтрует данные на основе ключевых слов, длины текста и формата URL.</li>
<li>На этом этапе достигается высокая полнота (recall) в 92.69% при отборе релевантных туториалов, что позволяет отсеять большую часть нерелевантных данных, сократив их количество с 20.8 миллиарда до 68.8 миллиона записей.</li>
</ul>
</li>
<li>
<p><strong>Разметка с помощью большой языковой модели (LLM Labeler):</strong></p>
<ul>
<li>Используется продвинутая языковая модель GPT-4O MINI для автоматической разметки данных. Она анализирует текст и определяет, является ли он туториалом.</li>
<li>Модель предварительно тестируется на вручную размеченном наборе данных, где показывает F1-меру около 90%.</li>
<li>В случаях расхождения между разметкой человека и LLM, модель иногда обнаруживала туториалы в длинных текстах, которые человек мог пропустить. Это позволило получить более качественные данные для обучения.</li>
</ul>
</li>
<li>
<p><strong>Фильтрация с помощью FastText:</strong></p>
<ul>
<li>Обучается модель FastText (модель глубокого обучения, основанная на n-граммах) для классификации текстовых сегментов как туториал или не туториал.</li>
<li>Для обучения используется смесь данных, размеченных LLM и вручную, всего около 90 000 примеров.</li>
<li>Модель демонстрирует хорошие показатели классификации и позволяет дополнительно отфильтровать данные, оставив около 18.8 миллиона примеров, похожих на туториалы.</li>
</ul>
</li>
<li>
<p><strong>Тегирование и перефразирование (Tag &amp; Paraphrase):</strong></p>
<ul>
<li>После фильтрации с помощью FastText, отобранные туториалы тегируются и перефразируются в стандартизированный формат.</li>
<li>Для этой задачи также используется GPT-4O MINI, который извлекает метаданные и форматирует туториалы в соответствии с шаблоном.</li>
<li>Шаблон включает в себя:<ul>
<li><strong>Платформу и цель:</strong> (например, macOS, Windows, браузер или приложение).</li>
<li><strong>Краткое описание задачи.</strong></li>
<li><strong>Необходимые условия</strong> для начала выполнения задачи.</li>
<li><strong>Пошаговые инструкции</strong> для выполнения задачи.</li>
<li><strong>Ожидаемый результат.</strong></li>
</ul>
</li>
<li>Этот этап позволяет стандартизировать туториалы для дальнейшего использования. Стоимость обработки 1000 записей составляет примерно 0.89 доллара.</li>
</ul>
</li>
</ol>
<p>В итоге, этот процесс позволяет автоматически собирать и подготавливать большое количество туториалов для обучения визуальных языковых моделей, что является важным шагом для автоматизации веб-задач.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Trajectory Data Collection via Guided Replay</div>
                <div class="summary_text"><p>В этом разделе описывается процесс сбора и оценки данных для обучения агента веб-навигации, а также структура самих данных.</p>
<p><strong>Процесс сбора данных (Guided Replay):</strong></p>
<ol>
<li><strong>Подготовка:</strong> Агенту (VLM agent) предоставляются отфильтрованные и отформатированные обучающие материалы (tutorials).</li>
<li><strong>Взаимодействие:</strong> Агент взаимодействует с реальной веб-средой, выполняя задания из обучающих материалов.</li>
<li><strong>Запись:</strong> В процессе взаимодействия записываются все действия агента, его промежуточные размышления и результаты. Это формирует траекторию выполнения задачи.</li>
<li><strong>Оценка:</strong> Полученный результат оценивается с помощью продвинутой VLM (большой языковой модели), чтобы убедиться в правильности выполнения задачи.</li>
</ol>
<p><strong>Структура данных траектории:</strong></p>
<p>Данные траектории предназначены для улучшения способностей агента к веб-навигации за счет интеграции планирования высокого уровня, инструкций низкого уровня и обоснованных действий. Каждая запись данных включает в себя:</p>
<ul>
<li><strong>Информация о задаче (Task Information):</strong> Подробные метаданные о задаче, такие как платформа, описание задачи, необходимые условия, инструкции и ожидаемые результаты. Эта информация помогает агенту как в планировании, так и в выполнении задачи.</li>
<li><strong>Обработанная текстовая траектория (Post-processed Textual Trajectory):</strong> Траектория, доработанная после повторного прохождения (replay). Она содержит:<ul>
<li><strong>Метаданные задачи (Task Metadata):</strong> Краткое описание задачи для адаптивного принятия решений.</li>
<li><strong>Наблюдения (Observations):</strong> Визуальный контекст, полученный агентом во время выполнения задачи.</li>
<li><strong>Промежуточные рассуждения (Intermediate Reasoning):</strong> Записи о процессе принятия решений агентом.</li>
<li><strong>Последовательность действий (Action Sequence):</strong> Детальная информация о взаимодействиях агента с веб-элементами.</li>
</ul>
</li>
<li><strong>Скриншоты и видеозаписи (Screenshots and Video Recordings):</strong> Визуальная документация всего процесса для более полного анализа.</li>
<li><strong>Воспроизводимый нативный след (Reproducible Native Trace):</strong> Запись, полученная с помощью Playwright, включающая в себя снимки DOM, HTML, сетевой трафик и последовательность действий. Это позволяет полностью реконструировать взаимодействие агента со средой и провести детальный анализ.</li>
</ul>
<p><strong>В заключение:</strong> Данные траектории, полученные в процессе Guided Replay, содержат подробную информацию о том, как агент взаимодействует с веб-средой. Эта информация включает в себя как текстовые описания, так и визуальные данные, что позволяет обучать агентов веб-навигации более эффективно.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Guided Replay with Tutorials</div>
                <div class="summary_text"><h2>Изложение раздела статьи: Улучшение данных для обучения агента</h2>
<p>Для обучения более эффективных моделей агентов, авторы статьи столкнулись с проблемой нехватки качественных данных. Чтобы решить эту проблему, они использовали среду BrowserGym, которая позволяет модели "переигрывать" задачи, опираясь на сгенерированные ранее обучающие материалы.</p>
<p><strong>BrowserGym</strong> - это среда для автоматизации веб-задач в браузере Chromium. Она позволяет агентам, использующим модели VLM (Visual Language Model), выполнять действия на веб-страницах. Агентам предоставляются размеченные и перефразированные инструкции, а также URL целевой страницы. Пошаговые инструкции ведут агента через задачу, а ее успешное выполнение определяется достижением ожидаемого результата.</p>
<p>Наблюдения агента включают скриншот видимой области веб-страницы и дерево доступности (AXTree). HTML-файл исключается из-за его большого размера и нерелевантности для визуальных агентов. Действия выполняются с помощью функций Playwright, таких как клик, выбор опции и очистка поля. Playwright также записывает подробные трассировки, включая целевые элементы, координаты, скриншоты и DOM-снимки, а также внутренние размышления агента между действиями.</p>
<p>Стоимость обработки данных довольно высока: около 8027 токенов на шаг и 86114 токенов на задачу. Переигрывание 1000 задач с использованием GPT-4O-08-06 обходится примерно в 215 долларов.</p>
<p><strong>Оценка траекторий</strong></p>
<p>После записи большого объема данных о переигранных задачах, авторы статьи столкнулись с необходимостью выделения эффективных сегментов, которые действительно могут улучшить производительность агента. Для этого они разработали <strong>VLM Evaluator</strong> (оценщик на основе VLM), который оценивает качество записанных траекторий, используя изображения и последовательность действий агента. VLM-модели хорошо масштабируются, способны обрабатывать большие объемы данных параллельно и обеспечивают прозрачную оценку.</p>
<p><strong>Дизайн VLM Evaluator</strong></p>
<p>Эффективность траекторий определяется двумя критериями: соблюдение инструкций и успешное выполнение ключевых компонентов задачи. В качестве основы оценщика используется модель GPT-4O. Оценщик получает описание задачи, историю действий агента и его внутренние размышления на каждом шаге. Данные представлены в последовательном формате: {описание задачи; размышление 1; действие 1; размышление 2; действие 2; ...}. VLM предоставляет общую оценку траектории и проводит пошаговый анализ, обосновывая неэффективные траектории и определяя момент первой ошибки.</p>
<p><strong>Валидация на данных с ручной разметкой</strong></p>
<p>Для оценки производительности автоматического оценщика авторы создали набор данных из 558 траекторий с ручной разметкой, основанной на анализе 1081 траектории. Результаты показали высокую точность оценщика, несмотря на различия в форматах входных данных и сценариях задач. Более того, оценщик часто применял более строгие стандарты, чем люди-оценщики, что говорит о его надежности в выявлении эффективных траекторий.</p>
<p><strong>Обучение и дообучение модели</strong></p>
<p>Для обучения агента авторы выбрали модель, основанную исключительно на визуальных данных (скриншотах), а не на деревьях доступности или текстовых представлениях. Это обусловлено тем, что графические интерфейсы по своей природе визуальны, и сопоставление инструкций с визуальными элементами больше соответствует человеческому восприятию. Кроме того, текстовые представления часто бывают многословными и приводят к большим вычислительным затратам.</p>
<p><strong>Фреймворк для визуального управления и действий в GUI</strong></p>
<p>В статье предлагается унифицировать пространство наблюдений и действий с помощью визуальных данных и стандартных команд pyautogui с подключаемой системой действий. Использование визуальных данных устраняет необходимость для модели понимать различия в исходном коде интерфейса на разных платформах. Кроме того, визуальные данные требуют меньшего количества токенов для обработки по сравнению с HTML.</p>
<p>В качестве пространства действий выбраны стандартные команды pyautogui, которые позволяют унифицировать основные операции с GUI. Поскольку данные собираются с веб-сайтов с помощью Playwright, необходимо преобразовать действия Playwright в действия pyautogui. Кроме того, используется подключаемая система действий для охвата специфических действий Playwright, таких как выбор опции.</p>
<p><strong>Архитектура модели и обучение</strong></p>
<p>Для обучения модели, которая способна сопоставлять намерения напрямую с визуальными наблюдениями, была выбрана модель Qwen2-VL, использующая NaViT в качестве кодировщика изображений. Qwen2-VL способна обрабатывать изображения любого разрешения, эффективно преобразуя их в переменные визуальные токены. Это делает Qwen2-VL идеальным выбором для агентов GUI, поскольку она может кодировать изображения высокого разрешения с меньшими затратами токенов.</p>
<p>Процесс обучения состоит из одного этапа дообучения. Используются данные из AgentTrek Data для улучшения возможностей VLM в области привязки и планирования.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Experimental Setup</div>
                <div class="summary_text"><p>В этой статье рассматривается обучение агентов для взаимодействия с веб-страницами, как с текстовой, так и с визуальной информацией.</p>
<p><strong>Обучение агентов.</strong> Для обучения текстового веб-агента использовались 6000 траекторий агента из набора данных AgentTrek. В качестве наблюдений использовалось дерево доступности (accessibility tree), а в качестве действий агента — действия Playwright. Модели Qwen2 различных размеров (7B, 32B и 72B) были дообучены на этих данных. Для визуального веб-агента была дообучена модель Qwen2-VL, используя 10000 траекторий агента из того же набора данных.</p>
<p><strong>Оценка текстового веб-агента.</strong> Для оценки способностей текстового агента был выбран бенчмарк WebArena, который основан на реальных веб-сайтах и использует различные методы оценки, включая процент успешного выполнения задач. Это делает его более подходящим для оценки в реальных условиях.</p>
<p><strong>Оценка визуального веб-агента.</strong> Для оценки визуального агента использовались два бенчмарка, которые проверяют способность модели к привязке (grounding) и планированию.
*   <strong>ScreenSpot</strong> — бенчмарк для визуальной привязки, содержащий 1200 инструкций и ограничивающих рамок целевых элементов. Он охватывает мобильные, десктопные и веб-среды, классифицируя элементы на текст и иконки. Так как данные для обучения были получены только из веб-среды, оценка проводилась только в веб-среде.
*   <strong>Multimodal-Mind2Web</strong> — мультимодальное расширение бенчмарка Mind2Web, состоящее из трех категорий (cross-task, cross-website и cross-domain), ранжированных по степени отклонения от обучающих данных.</p>
<p><strong>Основные результаты.</strong></p>
<p><strong>WebArena.</strong> Эксперименты показали, что использование траекторий AgentTrek значительно повышает производительность на WebArena, превосходя открытые базовые модели и GPT-4o. Сильные результаты на WebArena, который является бенчмарком для веб-агентов, работающих с самостоятельно размещенными сайтами, подтверждают, что данные AgentTrek хорошо обобщаются на невидимые домены.</p>
<p><strong>ScreenSpot.</strong> Дообучение с использованием AgentTrek значительно улучшило способность модели Qwen2-VL к привязке как для текстовых, так и для иконочных задач, более чем удвоив базовую производительность и превзойдя несколько моделей на бенчмарке ScreenSpot. Это демонстрирует сильное влияние AgentTrek на улучшение способностей модели к привязке для веб-задач с графическим интерфейсом.</p>
<p><strong>Mind2Web.</strong> Базовая модель Qwen2-VL-7B была исключена из оценки из-за ее низкой способности находить целевые элементы. Дообучение с использованием AgentTrek значительно улучшило производительность Qwen2-VL, особенно в метрике Operation F1, где она превзошла GPT-3.5 и GPT-4 во всех настройках. Комбинация данных AgentTrek и Mind2Web дает наилучшие результаты по всем метрикам и настройкам. Хотя дообучение только на данных Mind2Web также дает хорошие результаты, добавление данных AgentTrek все равно улучшает производительность. Это подчеркивает взаимодополняющие сильные стороны двух наборов данных: AgentTrek предоставляет точные, привязанные к контексту данные, а Mind2Web предоставляет ценные ресурсы для решения сложных веб-задач.</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Analysis</div>
                <div class="summary_text"><p><strong>4.1 Важность обучающих материалов</strong></p>
<p>Обучающие материалы, полученные из интернета, играют ключевую роль в процессе генерации траекторий действий агента. Во-первых, они обеспечивают разнообразие генерируемых траекторий. Различные обучающие материалы часто имеют уникальные цели, а даже при схожих целях могут предлагать разные способы их достижения. Это значительно обогащает данные о траекториях.</p>
<p>Во-вторых, обучающие материалы заметно улучшают выполнение задач агентом. В ходе эксперимента агенту было предложено выполнить 400 задач дважды: один раз, следуя инструкциям из обучающих материалов, и второй раз, опираясь только на высокоуровневые цели. Результаты показали, что пошаговые инструкции значительно повышают эффективность. Без обучающих материалов было сгенерировано всего 63 успешные траектории (15.78% от общего числа). С использованием обучающих материалов количество успешных траекторий возросло до 208 (52%), что более чем на 230% больше. Это демонстрирует важность подробных инструкций для повышения надежности и эффективности работы агента. (Более подробный анализ можно найти в Приложении B).</p>
<p><em>Комментарий: В этом разделе подчеркивается, что использование обучающих материалов из интернета не только обеспечивает разнообразие данных, но и значительно повышает качество работы агента за счет предоставления подробных пошаговых инструкций.</em></p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Data Composition</div>
                <div class="summary_text"><p><strong>Распределение веб-сайтов и сравнение с существующими работами</strong></p>
<p>В рамках исследования был разработан конвейер обработки данных, который начинается с фильтрации обучающих материалов из веб-снимка RedPajama. Эти данные затем перефразируются для ясности и классификации. Далее, собираются актуальные данные с популярных веб-сайтов для воспроизведения (replay), и, наконец, из этих воспроизведений извлекаются эффективные траектории действий. После фильтрации из огромного набора данных RedPajama было сохранено более 18.8 миллионов записей. Применив критерии актуальности и популярности, для воспроизведения было подготовлено 23 430 обучающих материалов. С коэффициентом успешности 44.4% было сгенерировано 10 398 траекторий, охватывающих 127 веб-сайтов в 11 различных категориях. Распределение веб-сайтов по доменам представлено на рисунке 7 (не приводится).</p>
<p>В сравнении с существующими работами (Niu et al., 2024; Lu et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024), AgentTrek генерирует полные, крупномасштабные данные о траекториях, превосходя их по нескольким ключевым параметрам (см. Таблицу 1, не приводится). Во-первых, с почти 5 тысячами проверенных траекторий и в среднем 12.1 шага на траекторию, набор данных обеспечивает прочную основу для обучения и оценки агентов в задачах веб-навигации с длинными горизонтами. Во-вторых, это самый полный набор данных на сегодняшний день, включающий DOM/HTML структуры, данные AXTree, промежуточные этапы рассуждений, полные видеозаписи и соответствующие скриншоты для каждого действия. Более того, несмотря на полную автоматизацию без вмешательства человека, набор данных поддерживает разнообразие по 120 веб-сайтам и 12 различным категориям задач. Благодаря использованию современных больших языковых моделей (LLM) можно извлекать как высокоуровневые цели задач, так и подробные пошаговые инструкции, что обеспечивает гибкость для будущего использования. Наконец, конвейер значительно снижает затраты и проблемы масштабируемости, связанные со сбором данных, аннотированных людьми. С учетом коэффициента успешности, стоимость одной траектории составляет всего 0.551 доллара, что делает этот подход эффективным и масштабируемым для генерации данных в больших объемах. Подробная информация о стоимости приведена в приложении C (не приводится).</p></div>
                <div class="images"></div>
            </div>
            <div class="summaries">
                <div class="summary_title">Related Work</div>
                <div class="summary_text"><p><strong>Агенты на основе больших языковых моделей (LLM)</strong></p>
<p>Агенты, основанные на больших языковых моделях (LLM), представляют собой автономные системы, которые используют LLM для взаимодействия с веб-сайтами и операционными системами. Эти агенты способны понимать инструкции на естественном языке и выполнять разнообразные сложные задачи в различных областях, таких как электронная коммерция, онлайн-помощь и навигация по знаниям. Примеры таких моделей включают SeeAct и WebVoyager, которые стремятся обобщить поведение агентов на реальных веб-сайтах.</p>
<p>Несмотря на многообещающие результаты, существуют проблемы, связанные с необходимостью в специализированных данных для обучения агентов. Большинство существующих наборов данных служат лишь дополнением к различным бенчмаркам, и лишь немногие из них специально разработаны для анализа траекторий агентов. Кроме того, эти наборы данных часто ограничены необходимостью ручной разметки, что затрудняет масштабирование. В данной работе представлен экономичный конвейер для автоматической генерации полных данных о траекториях агентов, что открывает новые возможности в синтезе данных для агентских приложений.</p>
<p><strong>Автоматическая оценка цифровых агентов</strong></p>
<p>В последнее время растет интерес к автоматизации оценки цифровых агентов с использованием моделей "зрение-язык" (VLM) и LLM. Эти методы используют модели для оценки производительности агентов в реальных задачах. Исследования в этой области охватывают несколько направлений: некоторые работы сосредоточены на успехе на уровне траектории, в то время как другие оценивают успех на каждом шаге, основываясь на соблюдении инструкций. Кроме того, оценки проводятся в различных средах задач, таких как веб-платформы и мобильные операционные системы, такие как Android и iOS. В данной работе в качестве автономного оценщика используется VLM (GPT-4o), который анализирует процесс взаимодействия агентов, чтобы определить, успешно ли агент выполнил задачи на уровне траектории.</p></div>
                <div class="images"></div>
            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-13 10:43',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-13 10:43')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-13 10:43')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    