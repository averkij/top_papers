
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 260 papers. July 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Июль 2025</span> | <span id="title-articles-count">260 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-06.html">⬅️ <span id="prev-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-08.html">➡️ <span id="next-date">08.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Июль 2025', 'en': 'July 2025', 'zh': '7月2025年'};
        let feedDateNext = {'ru': '08.2025', 'en': '08/2025', 'zh': '8月2025年'};
        let feedDatePrev = {'ru': '06.2025', 'en': '06/2025', 'zh': '6月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.01006', 'title': 'GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.01006', 'abstract': 'A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.', 'score': 135, 'issue_id': 4595, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '174c869b64ed9ae7', 'authors': ['Wenyi Hong', 'Wenmeng Yu', 'Xiaotao Gu', 'Guo Wang', 'Guobing Gan', 'Haomiao Tang', 'Jiale Cheng', 'Ji Qi', 'Junhui Ji', 'Lihang Pan', 'Shuaiqi Duan', 'Weihan Wang', 'Yan Wang', 'Yean Cheng', 'Zehai He', 'Zhe Su', 'Zhen Yang', 'Ziyang Pan', 'Aohan Zeng', 'Baoxu Wang', 'Boyan Shi', 'Changyu Pang', 'Chenhui Zhang', 'Da Yin', 'Fan Yang', 'Guoqing Chen', 'Jiazheng Xu', 'Jiali Chen', 'Jing Chen', 'Jinhao Chen', 'Jinghao Lin', 'Jinjiang Wang', 'Junjie Chen', 'Leqi Lei', 'Leyi Pan', 'Mingzhi Zhang', 'Qinkai Zheng', 'Sheng Yang', 'Shi Zhong', 'Shiyu Huang', 'Shuyuan Zhao', 'Siyan Xue', 'Shangqin Tu', 'Shengbiao Meng', 'Tianshu Zhang', 'Tianwei Luo', 'Tianxiang Hao', 'Tianle Gong', 'Wenkai Li', 'Wei Jia', 'Xin Lyu', 'Xuancheng Huang', 'Yanling Wang', 'Yadong Xue', 'Yanfeng Wang', 'Yifan An', 'Yifan Du', 'Yiming Shi', 'Yiheng Huang', 'Yilin Niu', 'Yuan Wang', 'Yuanchang Yue', 'Yuchen Li', 'Yutao Zhang', 'Yuxuan Zhang', 'Zhanxiao Du', 'Zhenyu Hou', 'Zhao Xue', 'Zhengxiao Du', 'Zihan Wang', 'Peng Zhang', 'Debing Liu', 'Bin Xu', 'Juanzi Li', 'Minlie Huang', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01006.jpg', 'data': {'categories': ['#open_source', '#rl', '#architecture', '#reasoning', '#multimodal', '#benchmark', '#training'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное рассуждение на новом уровне', 'desc': 'Представлена модель GLM-4.1V-Thinking - мультимодальная модель для обработки текста и изображений. Она использует масштабное предобучение и обучение с подкреплением для улучшения рассуждений на различных задачах. Модель достигает передовых результатов на 28 публичных бенчмарках, превосходя более крупные модели. GLM-4.1V-9B-Thinking демонстрирует конкурентоспособные результаты даже по сравнению с закрытыми моделями вроде GPT-4 на сложных задачах.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with GLM-4.1V-Thinking', 'desc': 'GLM-4.1V-Thinking is a vision-language model that enhances multimodal reasoning through extensive pre-training and reinforcement learning. The model is built on a strong vision foundation, which is crucial for achieving high performance across various tasks. By employing Reinforcement Learning with Curriculum Sampling, it maximizes its capabilities in areas like STEM problem solving and video understanding. The model has been open-sourced and shows superior performance compared to other models of similar size, making it a significant contribution to the field of AI.'}, 'zh': {'title': 'GLM-4.1V-Thinking：多模态推理的新高度', 'desc': 'GLM-4.1V-Thinking 是一种视觉-语言模型，旨在提升通用多模态推理能力。通过大规模预训练和强化学习，该模型在多个任务上实现了最先进的性能。我们开发了一个强大的视觉基础模型，并通过课程采样的强化学习方法进一步提升了模型的能力。该模型在28个公共基准测试中表现优异，超越了许多同类模型，展示了其在长文档理解和STEM推理等复杂任务中的竞争力。'}}}, {'id': 'https://huggingface.co/papers/2507.01001', 'title': 'SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks', 'url': 'https://huggingface.co/papers/2507.01001', 'abstract': "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.", 'score': 32, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'f3c20682e2dcf410', 'authors': ['Yilun Zhao', 'Kaiyan Zhang', 'Tiansheng Hu', 'Sihong Wu', 'Ronan Le Bras', 'Taira Anderson', 'Jonathan Bragg', 'Joseph Chee Chang', 'Jesse Dodge', 'Matt Latzke', 'Yixin Liu', 'Charles McGrady', 'Xiangru Tang', 'Zihang Wang', 'Chen Zhao', 'Hannaneh Hajishirzi', 'Doug Downey', 'Arman Cohan'], 'affiliations': ['Allen Institute for AI', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01001.jpg', 'data': {'categories': ['#open_source', '#science', '#benchmark', '#survey', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Коллективный разум в оценке ИИ для научной литературы', 'desc': 'SciArena - это платформа для оценки фундаментальных моделей в задачах работы с научной литературой, использующая коллективные суждения оценщиков для ранжирования моделей. Платформа поддерживает 23 модели с открытым исходным кодом и проприетарные модели, собрав более 13 000 голосов от доверенных исследователей из различных научных областей. Анализ собранных данных подтверждает разнообразие вопросов, их соответствие реальным потребностям литературы и высокую согласованность оценок участвующих исследователей. На основе собранных данных о предпочтениях авторы также выпустили бенчмарк SciArena-Eval для оценки точности моделей в определении качества ответов.'}, 'en': {'title': 'Empowering Scientific Model Evaluation through Community Collaboration', 'desc': 'SciArena is a collaborative platform designed to evaluate foundation models specifically for tasks related to scientific literature. It utilizes community voting to rank models, moving away from traditional benchmarks and fostering direct engagement from researchers. The platform supports a variety of models and has gathered extensive voting data, demonstrating strong agreement among participants in their evaluations. Additionally, SciArena introduces a meta-evaluation benchmark, SciArena-Eval, to enhance automated evaluation systems by comparing model assessments with human judgments.'}, 'zh': {'title': 'SciArena：科学文献任务的社区评估平台', 'desc': 'SciArena是一个社区驱动的平台，用于评估基础模型在科学文献任务上的表现。与传统的科学文献理解基准不同，SciArena通过社区投票的方式直接参与研究者，利用集体智慧对模型性能进行评估。该平台支持23个开源和专有的基础模型，并收集了来自不同科学领域的研究者的超过13,000个投票。我们还推出了SciArena-Eval，一个基于收集的偏好数据的元评估基准，旨在促进文献任务的自动评估系统的研究。'}}}, {'id': 'https://huggingface.co/papers/2506.23115', 'title': 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings', 'url': 'https://huggingface.co/papers/2506.23115', 'abstract': 'MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.', 'score': 30, 'issue_id': 4592, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'd7fecdae218ccf8e', 'authors': ['Haonan Chen', 'Hong Liu', 'Yuping Luo', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Corporation', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23115.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#optimization', '#multimodal', '#alignment', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MoCa: революция в мультимодальном встраивании', 'desc': 'MoCa - это двухэтапная система для улучшения предобученных причинно-следственных визуально-языковых моделей для мультимодального встраивания. Она вводит двунаправленное внимание, масштабирование с помощью неразмеченных данных и разнообразные цели обучения. Первый этап включает совместную реконструкцию для улучшения двунаправленных рассуждений с учетом контекста. Второй этап использует разнообразные семантически богатые мультимодальные данные для улучшения обобщения и выравнивания.'}, 'en': {'title': 'MoCa: Enhancing Multimodal Embedding with Bidirectional Attention', 'desc': 'MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'MoCa：双向多模态嵌入的创新框架', 'desc': 'MoCa是一个两阶段框架，旨在增强预训练的因果视觉语言模型在多模态嵌入中的表现。它通过引入双向注意力机制、利用未标记数据进行扩展以及多样化的训练目标来解决现有方法的局限性。第一阶段通过联合重建目标来提高文本和图像输入的去噪能力，增强双向上下文感知推理。第二阶段则利用丰富的多模态数据进行异构对比微调，从而提高模型的泛化能力和对齐效果。'}}}, {'id': 'https://huggingface.co/papers/2507.00432', 'title': 'Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.00432', 'abstract': 'Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.', 'score': 29, 'issue_id': 4592, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c4a7e4dd11865858', 'authors': ['Maggie Huan', 'Yuetai Li', 'Tuney Zheng', 'Xiaoyu Xu', 'Seungone Kim', 'Minxin Du', 'Radha Poovendran', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'M-A-P', 'The Hong Kong Polytechnic University', 'University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.00432.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#transfer_learning', '#optimization', '#math'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением превосходит обучение с учителем в обобщении навыков решения задач', 'desc': 'Исследование показывает, что модели, обученные с помощью обучения с подкреплением (RL), лучше обобщают навыки решения математических задач на другие области, чем модели, настроенные с помощью обучения с учителем (SFT). Анализ латентного пространства и распределения токенов выявил, что SFT вызывает значительный сдвиг в представлении и выводе, в то время как RL сохраняет общую структуру. Эксперименты проводились на моделях Qwen3-14B с использованием только математических данных, но разных методов обучения. Результаты указывают на необходимость пересмотра стандартных подходов к обучению моделей рассуждения, особенно в отношении использования данных, полученных с помощью SFT.'}, 'en': {'title': 'Rethinking Training: Reinforcement Learning for Better Generalization', 'desc': 'This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models.'}, 'zh': {'title': '重新思考推理模型的训练方法', 'desc': '这篇论文探讨了强化学习（RL）调优模型在数学问题解决能力上的表现，发现其在其他领域的泛化能力优于监督微调（SFT）模型。研究表明，虽然许多模型在数学任务上表现出色，但它们在其他任务上的迁移能力却较差。通过对Qwen3-14B模型的实验，发现RL调优模型能够在多个领域中保持良好的泛化能力，而SFT模型则容易遗忘其通用能力。结果提示我们需要重新审视现有的训练方法，尤其是对SFT数据的依赖。'}}}, {'id': 'https://huggingface.co/papers/2506.19852', 'title': 'Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation', 'url': 'https://huggingface.co/papers/2506.19852', 'abstract': 'Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4times longer while reducing training costs by up to 4.4times compared to direct fine-tuning and accelerating inference by up to 3.7times compared to dense attention inference.', 'score': 24, 'issue_id': 4594, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': 'c195d9c32370fcf1', 'authors': ['Xingyang Li', 'Muyang Li', 'Tianle Cai', 'Haocheng Xi', 'Shuo Yang', 'Yujun Lin', 'Lvmin Zhang', 'Songlin Yang', 'Jinbo Hu', 'Kelly Peng', 'Maneesh Agrawala', 'Ion Stoica', 'Kurt Keutzer', 'Song Han'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.19852.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#architecture', '#inference', '#training'], 'emoji': '🎥', 'ru': {'title': 'Радиальное внимание: эффективная генерация видео с сохранением качества', 'desc': "Статья представляет новый механизм внимания под названием 'Радиальное внимание' для моделей диффузии видео. Этот метод использует явление пространственно-временного затухания энергии, что позволяет значительно повысить эффективность вычислений при сохранении качества генерируемого видео. Радиальное внимание имеет сложность O(n log n) и позволяет моделям генерировать более длинные видео с меньшими вычислительными затратами. Эксперименты показывают, что этот подход ускоряет обучение и инференс в несколько раз по сравнению с стандартным плотным вниманием."}, 'en': {'title': 'Radial Attention: Efficient Video Generation with Sparse Attention', 'desc': 'This paper introduces Radial Attention, a new sparse attention mechanism designed to enhance the efficiency of video generation in diffusion models. It leverages the concept of Spatiotemporal Energy Decay, which explains how attention scores decrease as the distance between tokens increases, similar to how signals weaken over distance. By using a static attention mask that focuses on nearby tokens and reduces the attention window with temporal distance, Radial Attention achieves a computational complexity of O(n log n), making it much faster than traditional O(n^2) dense attention. The results show that this method not only speeds up training and inference but also maintains high video quality, allowing for longer video generation with reduced costs.'}, 'zh': {'title': '径向注意力：高效视频生成的新机制', 'desc': '本论文提出了一种名为径向注意力（Radial Attention）的稀疏注意力机制，旨在提高扩散模型在视频生成中的效率，同时保持视频质量。我们发现视频扩散模型中存在一种现象，称为时空能量衰减，随着空间和时间距离的增加，注意力得分会减小。径向注意力通过将能量衰减转化为指数衰减的计算密度，显著提高了计算效率，复杂度为O(n log n)，远优于传统的O(n^2)密集注意力。实验结果表明，径向注意力在多个视频生成模型中保持了视频质量，并实现了训练和推理速度的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.20639', 'title': 'DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.20639', 'abstract': "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.", 'score': 15, 'issue_id': 4593, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '20d886d0a4cd5bb6', 'authors': ['Shansan Gong', 'Ruixiang Zhang', 'Huangjie Zheng', 'Jiatao Gu', 'Navdeep Jaitly', 'Lingpeng Kong', 'Yizhe Zhang'], 'affiliations': ['Apple', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.20639.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#optimization', '#dataset', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие потенциала диффузионных языковых моделей для генерации кода', 'desc': 'Статья исследует применение диффузионных языковых моделей (dLLM) для генерации кода. Авторы анализируют уникальные процессы шумоподавления в dLLM и их отличия от авторегрессивных моделей. Они предлагают новый метод обучения с подкреплением под названием coupled-GRPO для улучшения производительности. Эксперименты показывают значительное повышение эффективности генерации кода с использованием предложенного подхода.'}, 'en': {'title': 'Unlocking Code Generation with Diffusion Models', 'desc': 'This paper explores the use of diffusion large language models (dLLMs) for code generation, highlighting their unique denoising processes compared to traditional autoregressive models. The authors introduce a novel reinforcement learning sampling scheme called coupled-GRPO, which enhances the training efficiency and performance of the dLLM named DiffuCoder. By analyzing the decoding behavior of DiffuCoder, the study reveals that dLLMs can flexibly adjust their generation strategies and improve diversity in output. The findings demonstrate that dLLMs have significant potential for coding tasks, providing a new framework for effective code generation.'}, 'zh': {'title': '扩散大语言模型：代码生成的新选择', 'desc': '本文探讨了扩散大语言模型（dLLMs）在代码生成中的应用，揭示了其独特的去噪过程。dLLMs与自回归模型相比，能够在整个序列上进行去噪，具有全球规划和迭代优化的特点，特别适合代码生成。我们提出了一种新颖的强化学习采样方案coupled-GRPO，以提高训练效率并减少标记对数估计的方差。实验结果表明，coupled-GRPO显著提升了DiffuCoder在代码生成基准上的表现，并减少了对自回归因果解码的依赖。'}}}, {'id': 'https://huggingface.co/papers/2506.21277', 'title': 'HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context', 'url': 'https://huggingface.co/papers/2506.21277', 'abstract': 'A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.', 'score': 10, 'issue_id': 4592, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '15a38ef84e7820fa', 'authors': ['Qize Yang', 'Shimin Yao', 'Weixuan Chen', 'Shenghao Fu', 'Detao Bai', 'Jiaxing Zhao', 'Boyuan Sun', 'Bowen Yin', 'Xihan Wei', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.21277.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#games', '#rl', '#multimodal', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодального рассуждения с помощью обучения с подкреплением', 'desc': 'Предложен подход на основе обучения с подкреплением для улучшения мультимодального рассуждения в больших языковых моделях. Метод решает проблемы понимания контекста и упрощения, используя контекстные, форматные, точностные и логические награды. Авторы вводят новый бенчмарк IntentBench для оценки понимания сложных человеческих намерений и эмоций. Предложенный метод показывает превосходную производительность на IntentBench по сравнению с другими открытыми мультимодальными моделями.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Reinforcement Learning', 'desc': 'This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions.'}, 'zh': {'title': '强化学习提升多模态推理能力', 'desc': '本论文提出了一种基于强化学习的方法，以增强多模态推理能力，解决上下文理解和捷径问题。我们发现现有多模态推理模型存在全球上下文理解不足和捷径问题，这会导致模型错误解读多模态信息。为了解决这些问题，我们强调模型需要在多模态输入中清晰理解全球上下文，并通过上下文奖励、格式奖励和准确性奖励来确保对多模态信息的准确解读。我们的研究在IntentBench基准测试中表现优异，展示了在理解复杂人类意图和情感方面的先进性能。'}}}, {'id': 'https://huggingface.co/papers/2507.00951', 'title': 'Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact', 'url': 'https://huggingface.co/papers/2507.00951', 'abstract': 'The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.', 'score': 8, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '056b6a5007ee5fc2', 'authors': ['Rizwan Qureshi', 'Ranjan Sapkota', 'Abbas Shah', 'Amgad Muneer', 'Anas Zafar', 'Ashmal Vayani', 'Maged Shoman', 'Abdelrahman B. M. Eldaly', 'Kai Zhang', 'Ferhat Sadak', 'Shaina Raza', 'Xinqi Fan', 'Ravid Shwartz-Ziv', 'Hong Yan', 'Vinjia Jain', 'Aman Chadha', 'Manoj Karkee', 'Jia Wu', 'Philip Torr', 'Seyedali Mirjalili'], 'affiliations': ['Amazon Research (Work done outside Amazon)', 'Center for Data Science, New York University, NYU, NY, USA', 'Center for research in Computer Vision, University of Central Florida, Orlando, FL, USA', 'Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley, Brisbane, QLD 4006, Australia', 'Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA', 'Department of Electrical Engineering, City University of Hong Kong, SAR China', 'Department of Electronics Engineering, Mehran University of Engineering & Technology, Jamshoro, Sindh, Pakistan', 'Department of Engineering Science, University of Oxford, UK', 'Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA', 'Department of Mechanical Engineering, Bartin University, Bartin Turkey', 'Intelligent Transportation Systems, University of Tennessee, Oakridge, TN, USA', 'Manchester Metropolitan University, Manchester, UK', 'Meta Research (Work done outside Meta)', 'University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary', 'Vector Institute, Toronto Canada'], 'pdf_title_img': 'assets/pdf/title_img/2507.00951.jpg', 'data': {'categories': ['#agents', '#agi', '#rl', '#architecture', '#multimodal', '#rag', '#ethics', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Путь к AGI: объединяя модульность, память и мультиагентность', 'desc': 'Статья представляет междисциплинарный подход к достижению искусственного общего интеллекта (AGI). Авторы подчеркивают важность модульного рассуждения, памяти и координации между несколькими агентами. Особое внимание уделяется интеграции нейросимволических систем и обучения с подкреплением для преодоления ограничений современных моделей. В работе также рассматриваются стратегии генерализации и роль мультимодальных моделей в развитии AGI.'}, 'en': {'title': 'Towards True Intelligence: Integrating Memory, Reasoning, and Adaptation for AGI', 'desc': 'This paper explores the interdisciplinary approach to achieving Artificial General Intelligence (AGI) by integrating concepts from various fields such as cognitive neuroscience and psychology. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing intelligent systems. The authors propose that true intelligence is not just about scaling models but involves the orchestration of memory and reasoning capabilities. They also highlight the potential of neurosymbolic systems and reinforcement learning to create more adaptive and flexible AI agents.'}, 'zh': {'title': '跨学科推动人工通用智能的实现', 'desc': '这篇论文探讨了实现人工通用智能（AGI）的跨学科方法，强调了模块化推理、持久记忆和多智能体协调的重要性。论文分析了通用智能的架构和认知基础，提出了结合检索、规划和动态工具使用的Agentic RAG框架，以实现更灵活的行为。我们还讨论了信息压缩、测试时适应和无训练方法等泛化策略，作为实现灵活、领域无关智能的关键路径。最后，论文指出了在实现AGI过程中面临的科学、技术和伦理挑战。'}}}, {'id': 'https://huggingface.co/papers/2507.00339', 'title': 'Training for X-Ray Vision: Amodal Segmentation, Amodal Content\n  Completion, and View-Invariant Object Representation from Multi-Camera Video', 'url': 'https://huggingface.co/papers/2507.00339', 'abstract': 'Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,', 'score': 8, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'a0d52ad7a093d23d', 'authors': ['Alexander Moore', 'Amar Saini', 'Kylie Cancilla', 'Doug Poland', 'Carmen Carrano'], 'affiliations': ['Lawrence Livermore National Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.00339.jpg', 'data': {'categories': ['#dataset', '#cv'], 'emoji': '📹', 'ru': {'title': 'Многокамерное видео для амодальной сегментации и восстановления контента', 'desc': 'MOVi-MC-AC - это новый набор данных для амодальной сегментации и восстановления контента, содержащий видео с нескольких камер и сложными сценами бытовых предметов. Он включает уникальные идентификаторы объектов для отслеживания между кадрами и камерами, а также разметку для амодальной сегментации и восстановления скрытых частей объектов. Датасет содержит около 5,8 миллионов размеченных экземпляров объектов, что делает его крупнейшим в своей области. MOVi-MC-AC предоставляет новые возможности для задач обнаружения, отслеживания и сегментации объектов с использованием глубокого обучения.'}, 'en': {'title': 'Revolutionizing Amodal Segmentation with Multi-Camera Insights', 'desc': 'This paper presents MOVi-MC-AC, a groundbreaking dataset for amodal segmentation and content completion in complex scenes. It introduces the concept of using multiple cameras to capture a scene, allowing for better tracking and identification of objects from different perspectives. The dataset includes approximately 5.8 million labeled object instances, providing a rich resource for training deep learning models in computer vision. By offering ground-truth amodal content, it addresses limitations in previous datasets and enhances the understanding of occluded objects in cluttered environments.'}, 'zh': {'title': '多摄像头下的无模态分割新突破', 'desc': '本论文介绍了MOVi-MC-AC数据集，这是迄今为止最大的无模态分割和无模态内容数据集。该数据集模拟了多摄像头视频中的杂乱场景，提供了对象的遮挡掩码和特征。通过在多个独特摄像头视角之间识别和跟踪对象，MOVi-MC-AC为深度学习在计算机视觉领域的研究做出了重要贡献。数据集中包含约580万个对象实例，并提供了真实的无模态内容标签。'}}}, {'id': 'https://huggingface.co/papers/2506.21545', 'title': 'Data Efficacy for Language Model Training', 'url': 'https://huggingface.co/papers/2506.21545', 'abstract': 'DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.  \t\t\t\t\tAI-generated summary \t\t\t\t Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.', 'score': 6, 'issue_id': 4596, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'b19a54a5dd4e35c8', 'authors': ['Yalun Dai', 'Yangyu Huang', 'Xin Zhang', 'Wenshan Wu', 'Chong Li', 'Wenhui Lu', 'Shijie Cao', 'Li Dong', 'Scarlett Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.21545.jpg', 'data': {'categories': ['#optimization', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективность данных - ключ к улучшению языковых моделей', 'desc': 'DELT - это новая парадигма для улучшения производительности языковых моделей путем оптимизации организации обучающих данных. Она включает три компонента: оценку данных, отбор данных и упорядочивание данных. Авторы предлагают новый метод оценки данных LQS, учитывающий обучаемость и качество каждого образца, а также метод упорядочивания данных FO для решения проблем забывания модели и смещения распределения данных. Эксперименты показывают, что DELT позволяет значительно улучшить производительность языковых моделей без увеличения объема данных или размера модели.'}, 'en': {'title': 'Maximizing Language Model Performance through Data Efficacy', 'desc': 'The paper introduces DELT, a new approach to improve language model performance by focusing on data efficacy, which is about how well training data is organized. It consists of three main components: Data Scoring, Data Selection, and Data Ordering, which work together to enhance model training without needing more data or larger models. A key innovation is the Learnability-Quality Scoring (LQS), which evaluates data samples based on their learnability and quality. The results show that using DELT can significantly boost performance, especially when combining LQS with Folding Ordering, while also achieving data efficiency.'}, 'zh': {'title': '提升语言模型性能的新方法：DELT', 'desc': 'DELT是一种提高语言模型性能的新方法，专注于数据的有效性。它包括数据评分、选择和排序三个部分，旨在优化训练数据的组织方式。通过设计学习质量评分（LQS）和折叠排序（FO），DELT能够在不增加数据规模或模型大小的情况下显著提升模型性能。实验结果表明，数据有效性与数据效率可以结合使用，从而为语言模型训练提供新的思路。'}}}, {'id': 'https://huggingface.co/papers/2507.00162', 'title': 'FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion', 'url': 'https://huggingface.co/papers/2507.00162', 'abstract': 'Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences.', 'score': 4, 'issue_id': 4600, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '2e49553aab1de98a', 'authors': ['Yu Lu', 'Yi Yang'], 'affiliations': ['ReLER, CCAI, Zhejiang University, Hangzhou, 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00162.jpg', 'data': {'categories': ['#games', '#optimization', '#video'], 'emoji': '🎬', 'ru': {'title': 'FreeLong: Прорыв в генерации длинных видео с сохранением качества', 'desc': 'Статья представляет FreeLong и FreeLong++, новые подходы к генерации длинных видео с использованием моделей машинного обучения. Авторы выявили проблему искажения высокочастотных компонентов при увеличении длины видео и предложили решение, балансирующее распределение частот во время процесса денойзинга. FreeLong++ использует многоветвенную архитектуру с несколькими ветвями внимания, работающими на разных временных масштабах. Эти методы позволяют генерировать более длинные видео с улучшенной временной согласованностью и визуальной точностью без дополнительного обучения моделей.'}, 'en': {'title': 'Enhancing Long Video Generation with FreeLong Framework', 'desc': 'This paper introduces FreeLong, a novel framework aimed at improving the generation of longer videos from text prompts while maintaining high visual quality and temporal consistency. The authors identify a problem called high-frequency distortion, which occurs when short-video generation models are applied to longer sequences, leading to degraded video quality. FreeLong addresses this by blending low-frequency features that capture overall video semantics with high-frequency features that preserve fine details. The enhanced version, FreeLong++, expands this concept into a multi-branch architecture, allowing for better frequency fusion and improved performance in generating longer videos with coherent transitions and controllable elements.'}, 'zh': {'title': '长视频生成的新突破', 'desc': '最近视频生成模型的进展使得从文本提示生成高质量短视频成为可能。然而，将这些模型扩展到更长的视频仍然是一个重大挑战，主要是由于时间一致性和视觉保真度的下降。我们的研究表明，简单地将短视频生成模型应用于较长序列会导致明显的质量下降。为了解决这个问题，我们提出了FreeLong框架，通过在去噪过程中平衡长视频特征的频率分布，结合全局低频特征和局部高频特征，从而保持细节和语义的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.23329', 'title': 'IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as\n  Agentic Inverse Rendering', 'url': 'https://huggingface.co/papers/2506.23329', 'abstract': 'Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This "understanding-by-creating" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.', 'score': 4, 'issue_id': 4604, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'a1469fc6d15316f8', 'authors': ['Parker Liu', 'Chenxin Li', 'Zhengxin Li', 'Yipeng Wu', 'Wuyang Li', 'Zhiqin Yang', 'Zhenyuan Zhang', 'Yunlong Lin', 'Sirui Han', 'Brandon Y. Feng'], 'affiliations': ['CUHK', 'EPFL', 'HKUST', 'MIT', 'TJU', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2506.23329.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#cv', '#interpretability'], 'emoji': '🎨', 'ru': {'title': 'Понимание сцены через её активное воссоздание', 'desc': "Статья представляет IR3D-Bench - новый бенчмарк для оценки понимания сцен моделями компьютерного зрения и языка (VLM). В отличие от традиционных тестов на распознавание, IR3D-Bench требует от моделей активного воссоздания 3D-структуры изображения с помощью инструментов программирования и рендеринга. Этот подход 'понимание через создание' позволяет оценить генеративные способности моделей и их умение использовать инструменты. Бенчмарк включает набор метрик для оценки геометрической точности, пространственных отношений и правдоподобности воссозданных сцен."}, 'en': {'title': 'Understanding Scenes by Creating, Not Just Recognizing', 'desc': "This paper introduces IR3D-Bench, a new benchmark designed to test the understanding capabilities of Vision-Language Models (VLMs) through active creation rather than just recognition. It employs the analysis-by-synthesis approach, where Vision-Language Agents (VLAs) use programming and rendering tools to reconstruct the 3D structure of images. This method emphasizes 'understanding-by-creating', which assesses the generative abilities of VLAs beyond traditional descriptive tasks. The study also presents metrics for evaluating various aspects of the generated scenes, revealing current limitations in visual precision among state-of-the-art VLMs."}, 'zh': {'title': '通过创造理解场景的能力', 'desc': '本文介绍了IR3D-Bench，这是一个新的基准测试，旨在评估视觉语言模型（VLMs）在理解场景方面的能力。与传统的被动识别不同，IR3D-Bench要求视觉语言代理（VLAs）通过编程和渲染工具主动创建输入图像的3D结构。该方法基于分析-合成范式，强调通过创造来理解，而不仅仅是描述。初步实验显示，尽管当前的VLM在工具使用上表现良好，但在视觉精度方面仍存在局限。'}}}, {'id': 'https://huggingface.co/papers/2506.23009', 'title': 'MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.23009', 'abstract': 'Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance.', 'score': 4, 'issue_id': 4610, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'd5d1bf85d7b72633', 'authors': ['Jian Chen', 'Wenye Ma', 'Penghang Liu', 'Wei Wang', 'Tengwei Song', 'Ming Li', 'Chenguang Wang', 'Ruiyi Zhang', 'Changyou Chen'], 'affiliations': ['Duke University', 'King Abdullah University of Science and Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23009.jpg', 'data': {'categories': ['#training', '#benchmark', '#synthetic', '#games', '#dataset', '#multimodal'], 'emoji': '🎼', 'ru': {'title': 'MusiXQA: новый рубеж в понимании нотных листов искусственным интеллектом', 'desc': 'Статья представляет MusiXQA - первый всеобъемлющий датасет для оценки и улучшения мультимодальных больших языковых моделей (MLLM) в понимании нотных листов. Датасет содержит синтетические нотные листы с аннотациями, охватывающими различные аспекты музыкальной нотации. Авторы выявили значительные ограничения современных MLLM в этой области. Они также разработали модель Phi-3-MusiX, специально настроенную на этом датасете, которая показала существенное улучшение производительности по сравнению с методами на основе GPT.'}, 'en': {'title': 'Unlocking Music Sheet Understanding for MLLMs', 'desc': 'This paper introduces MusiXQA, a new dataset designed to improve the understanding of music sheets by Multimodal Large Language Models (MLLMs). The dataset includes high-quality synthetic music sheets with detailed annotations, allowing MLLMs to perform various visual question-answering tasks related to music notation. The authors demonstrate that current MLLMs struggle with music sheet interpretation, highlighting the need for specialized training. They also present Phi-3-MusiX, an MLLM fine-tuned on MusiXQA, which shows improved performance compared to existing models like GPT.'}, 'zh': {'title': '乐谱理解的新突破：MusiXQA与Phi-3-MusiX', 'desc': '多模态大型语言模型（MLLMs）在自然图像、文本丰富的文档和图形设计方面表现出色，但在乐谱理解方面的能力仍然未被充分探索。为了解决这个问题，我们推出了MusiXQA，这是第一个全面评估和推动MLLMs在乐谱理解方面进展的数据集。MusiXQA包含高质量的合成乐谱，配有结构化注释，涵盖音符音高和时值、和弦、谱号、调号/拍号和文本，支持多样的视觉问答任务。通过广泛的评估，我们揭示了当前最先进的MLLMs在这一领域的显著局限性，并开发了Phi-3-MusiX，一个在我们的数据集上微调的MLLM，显著提升了性能。'}}}, {'id': 'https://huggingface.co/papers/2506.22960', 'title': 'Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images', 'url': 'https://huggingface.co/papers/2506.22960', 'abstract': 'PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  \t\t\t\t\tAI-generated summary \t\t\t\t A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that "Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality." In response, California\'s Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.', 'score': 4, 'issue_id': 4593, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'c946e3ac9bf6133a', 'authors': ['Shreyas Dixit', 'Ashhar Aziz', 'Shashwat Bajpai', 'Vasu Sharma', 'Aman Chadha', 'Vinija Jain', 'Amitava Das'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'BITS Pilani Hyderabad, India', 'IIIT Delhi, India', 'Meta AI, USA', 'Stanford University, USA', 'VIIT Pune, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.22960.jpg', 'data': {'categories': ['#security', '#synthetic', '#data', '#open_source', '#multimodal', '#cv'], 'emoji': '🔐', 'ru': {'title': 'Непобедимые водяные знаки для эпохи генеративного ИИ', 'desc': 'PECCAVI - это устойчивая техника водяных знаков для изображений, которая противостоит атакам визуального перефразирования и искажениям. Она использует неизменяемые точки (NMP) и многоканальное встраивание водяных знаков в частотной области. PECCAVI стратегически размещает водяные знаки в NMP и применяет шумовую полировку для противодействия обратному инжинирингу. Эта техника является моделенезависимой и обещает быть открытым исходным кодом.'}, 'en': {'title': 'PECCAVI: Watermarking Resilience Against Visual Paraphrase Attacks', 'desc': 'PECCAVI is a novel image watermarking technique designed to withstand visual paraphrase attacks and various distortions. It utilizes Non-Melting Points (NMPs) to strategically embed watermarks, ensuring that the essential features of the image remain intact. The method employs multi-channel frequency domain watermarking and incorporates noisy burnishing to protect against reverse-engineering attempts. This approach is model-agnostic, making it applicable across different systems, and all resources will be made available to the public.'}, 'zh': {'title': 'PECCAVI：抵御视觉改写的水印新技术', 'desc': 'PECCAVI是一种强大的图像水印技术，能够抵御视觉改写攻击和失真。它利用非熔化点（NMPs）和多通道频域水印技术，将水印嵌入图像的核心语义区域。该技术还采用了噪声烧灼方法，以防止逆向工程攻击，增强水印的耐久性。PECCAVI不依赖于特定模型，所有相关资源和代码将开源。'}}}, {'id': 'https://huggingface.co/papers/2507.00606', 'title': 'Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies', 'url': 'https://huggingface.co/papers/2507.00606', 'abstract': 'Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.', 'score': 2, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'a9828e151d8e7eb8', 'authors': ['Tao Xiong', 'Xavier Hu', 'Wenyan Fan', 'Shengyu Zhang'], 'affiliations': ['Dalian University of Technology, Dalian, LiaoNing, China', 'Independent, Hangzhou, ZheJiang, China', 'Zhejiang University, Hangzhou, ZheJiang, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00606.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Автономное рассуждение для языковых моделей без промптов', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) под названием Mixture of Reasoning (MoR). MoR позволяет моделям автономно применять различные стратегии рассуждений без необходимости ручной разработки промптов. Метод включает две фазы: генерацию шаблонов цепочек рассуждений и создание датасета для дообучения модели. Эксперименты показывают значительное улучшение производительности модели по сравнению с базовыми методами.'}, 'en': {'title': 'Empowering LLMs with Autonomous Reasoning through Mixture of Reasoning', 'desc': "This paper presents a new framework called Mixture of Reasoning (MoR) that improves the performance of large language models (LLMs) by integrating various reasoning strategies. Unlike traditional methods that depend on specific prompts for each task, MoR allows LLMs to adapt their reasoning autonomously. The framework consists of two main phases: Thought Generation, which creates templates for reasoning chains, and SFT Dataset Construction, which pairs these templates with datasets for fine-tuning. Experimental results demonstrate that MoR enhances the model's performance significantly, making it a versatile solution for a wide range of tasks without the need for manual prompt design."}, 'zh': {'title': '推理混合：无提示自适应推理的新方法', 'desc': '大型语言模型（LLMs）在复杂任务中表现出色，得益于先进的提示技术，如思维链（CoT）和思维树（ToT），但它们对手动设计的特定任务提示的依赖限制了适应性和效率。我们提出了推理混合（MoR）框架，将多样的推理策略嵌入LLMs中，实现自主、任务自适应的推理，而无需外部提示工程。MoR包括两个阶段：思维生成，使用像GPT-4o这样的模型创建推理链模板，以及SFT数据集构建，将模板与基准数据集配对进行监督微调。实验表明，MoR显著提升了性能，MoR150在使用CoT提示时达到了0.730（提高2.2%），与基线相比提高了13.5%，提供了一种可推广的解决方案，以实现跨多样任务的稳健推理。'}}}, {'id': 'https://huggingface.co/papers/2507.00476', 'title': 'FreNBRDF: A Frequency-Rectified Neural Material Representation', 'url': 'https://huggingface.co/papers/2507.00476', 'abstract': 'Accurate material modeling is crucial for achieving photorealistic rendering, bridging the gap between computer-generated imagery and real-world photographs. While traditional approaches rely on tabulated BRDF data, recent work has shifted towards implicit neural representations, which offer compact and flexible frameworks for a range of tasks. However, their behavior in the frequency domain remains poorly understood. To address this, we introduce FreNBRDF, a frequency-rectified neural material representation. By leveraging spherical harmonics, we integrate frequency-domain considerations into neural BRDF modeling. We propose a novel frequency-rectified loss, derived from a frequency analysis of neural materials, and incorporate it into a generalizable and adaptive reconstruction and editing pipeline. This framework enhances fidelity, adaptability, and efficiency. Extensive experiments demonstrate that \\ours improves the accuracy and robustness of material appearance reconstruction and editing compared to state-of-the-art baselines, enabling more structured and interpretable downstream tasks and applications.', 'score': 1, 'issue_id': 4608, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c539d168bcfbab61', 'authors': ['Chenliang Zhou', 'Zheyuan Hu', 'Cengiz Oztireli'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.00476.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Частотно-оптимизированные нейронные BRDF для фотореалистичного рендеринга', 'desc': 'Статья представляет FreNBRDF - новый подход к нейронному моделированию материалов с учетом частотной области. Авторы используют сферические гармоники для интеграции частотного анализа в нейронные BRDF. Предложен частотно-корректированный loss и адаптивный пайплайн для реконструкции и редактирования материалов. Эксперименты показывают, что FreNBRDF превосходит современные методы по точности и надежности моделирования внешнего вида материалов.'}, 'en': {'title': 'Enhancing Material Modeling with Frequency-Rectified Neural Representations', 'desc': 'This paper presents FreNBRDF, a new method for modeling materials in computer graphics using neural networks. It focuses on improving the accuracy of material representation by incorporating frequency-domain analysis through spherical harmonics. The authors introduce a frequency-rectified loss function that enhances the training of neural BRDF models, making them more adaptable and efficient. Experimental results show that this approach significantly outperforms existing methods in reconstructing and editing material appearances, leading to better photorealistic rendering.'}, 'zh': {'title': '频率校正，提升材料建模的准确性', 'desc': '准确的材料建模对于实现逼真的渲染至关重要，能够缩小计算机生成图像与真实照片之间的差距。传统方法依赖于表格化的BRDF数据，而最近的研究则转向隐式神经表示，提供了紧凑且灵活的框架。我们提出了FreNBRDF，这是一种频率校正的神经材料表示，通过利用球谐函数将频域考虑整合到神经BRDF建模中。我们的框架在材料外观重建和编辑方面提高了准确性和鲁棒性，支持更结构化和可解释的下游任务和应用。'}}}, {'id': 'https://huggingface.co/papers/2506.24019', 'title': 'Ella: Embodied Social Agents with Lifelong Memory', 'url': 'https://huggingface.co/papers/2506.24019', 'abstract': "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.", 'score': 1, 'issue_id': 4612, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'e3bbc5a001cf1269', 'authors': ['Hongxin Zhang', 'Zheyuan Zhang', 'Zeyuan Wang', 'Zunzhe Zhang', 'Lixing Fang', 'Qinhong Zhou', 'Chuang Gan'], 'affiliations': ['Johns Hopkins University', 'Tsinghua University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.24019.jpg', 'data': {'categories': ['#3d', '#agents', '#multimodal', '#agi'], 'emoji': '🤖', 'ru': {'title': 'Элла: воплощенный ИИ с непрерывным обучением в социальном 3D-мире', 'desc': 'Статья представляет Эллу - воплощенного социального агента, способного к непрерывному обучению в 3D-мире. Ключевой особенностью Эллы является структурированная долговременная мультимодальная система памяти, включающая семантическую и эпизодическую составляющие. Интеграция этой системы памяти с фундаментальными моделями позволяет Элле принимать решения, планировать деятельность и развивать социальные отношения. Эксперименты показали эффективность Эллы в достижении целей через наблюдение и социальное взаимодействие в динамичном 3D-мире.'}, 'en': {'title': 'Ella: Lifelong Learning in a Social 3D World', 'desc': "The paper presents Ella, an embodied social agent designed for lifelong learning in a 3D open world. Ella utilizes a structured multimodal memory system that includes semantic memory for knowledge organization and episodic memory for capturing experiences. This system allows Ella to make informed decisions, plan activities, and build social relationships through interactions with other agents. The results demonstrate Ella's effectiveness in influencing and cooperating with peers, emphasizing the benefits of integrating structured memory with foundation models for enhanced embodied intelligence."}, 'zh': {'title': 'Ella：终身学习的具身社交代理', 'desc': '本文介绍了Ella，一个能够在3D开放世界中进行终身学习的具身社交代理。Ella的核心能力是一个结构化的长期多模态记忆系统，能够有效地存储、更新和检索信息。该系统包括以名称为中心的语义记忆和捕捉多模态经验的时空情节记忆。通过将这一终身记忆系统与基础模型结合，Ella能够在与其他智能生物共存的环境中进行决策、规划日常活动和建立社交关系。'}}}, {'id': 'https://huggingface.co/papers/2506.22973', 'title': 'Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions', 'url': 'https://huggingface.co/papers/2506.22973', 'abstract': "A novel lossy compression method using learnable confidence scores improves storage and computational efficiency in 3D Gaussian Splatting without sacrificing visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting", 'score': 0, 'issue_id': 4606, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '20ec91d26e253f96', 'authors': ['AmirHossein Naghi Razlighi', 'Elaheh Badali Golezani', 'Shohreh Kasaei'], 'affiliations': ['Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.22973.jpg', 'data': {'categories': ['#3d', '#inference', '#open_source', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Эффективное сжатие 3D-сцен без потери качества', 'desc': 'Представлен новый метод сжатия с потерями для 3D Gaussian Splatting, использующий обучаемые оценки достоверности на основе бета-распределения. Метод позволяет уменьшить объем хранения и вычислительные затраты без ущерба для визуального качества. Подход оптимизирует достоверность каждого сплата с помощью функций потерь, учитывающих реконструкцию, что позволяет отбрасывать малозначимые сплаты. Метод применим к любым вариантам Gaussian Splatting и предоставляет новую метрику оценки качества сцены.'}, 'en': {'title': 'Optimizing 3D Rendering with Learnable Confidence Scores', 'desc': 'This paper introduces a new method for compressing 3D Gaussian Splatting data using learnable confidence scores, which enhances both storage and computational efficiency. The method employs Beta distributions to model the confidence of each splat, allowing for the removal of less important splats without losing visual quality. By optimizing these confidence scores through reconstruction-aware losses, the approach effectively balances compression and fidelity. The technique is versatile and can be integrated with various Gaussian Splatting architectures, providing a new metric for scene quality assessment.'}, 'zh': {'title': '提升3D高斯点云压缩效率的新方法', 'desc': '本文提出了一种新颖的有损压缩方法，利用可学习的置信度评分来提高3D高斯点云的存储和计算效率，同时不牺牲视觉质量。3D高斯点云渲染通常会产生大量的点，导致存储和计算开销过大。我们的方法通过重建感知损失优化每个点的置信度，从而在保留视觉保真度的同时，去除低置信度的点。该方法不依赖于特定架构，适用于任何高斯点云变体，并且通过平均置信度值提供了一种新的场景质量评估指标。'}}}, {'id': 'https://huggingface.co/papers/2507.01949', 'title': 'Kwai Keye-VL Technical Report', 'url': 'https://huggingface.co/papers/2507.01949', 'abstract': "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.", 'score': 94, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': 'ca23195c7fa1bb87', 'authors': ['Kwai Keye Team', 'Biao Yang', 'Bin Wen', 'Changyi Liu', 'Chenglong Chu', 'Chengru Song', 'Chongling Rao', 'Chuan Yi', 'Da Li', 'Dunju Zang', 'Fan Yang', 'Guorui Zhou', 'Hao Peng', 'Haojie Ding', 'Jiaming Huang', 'Jiangxia Cao', 'Jiankang Chen', 'Jingyun Hua', 'Jin Ouyang', 'Kaibing Chen', 'Kaiyu Jiang', 'Kaiyu Tang', 'Kun Gai', 'Shengnan Zhang', 'Siyang Mao', 'Sui Huang', 'Tianke Zhang', 'Tingting Gao', 'Wei Chen', 'Wei Yuan', 'Xiangyu Wu', 'Xiao Hu', 'Xingyu Lu', 'Yang Zhou', 'Yi-Fan Zhang', 'Yiping Yang', 'Yulong Chen', 'Zhenhua Wu', 'Zhenyu Li', 'Zhixin Ling', 'Ziming Li', 'Dehua Ma', 'Di Xu', 'Haixuan Gao', 'Hang Li', 'Jiawei Guo', 'Jing Wang', 'Lejian Ren', 'Muhao Wei', 'Qianqian Wang', 'Qigen Hu', 'Shiyao Wang', 'Tao Yu', 'Xinchen Luo', 'Yan Li', 'Yiming Liang', 'Yuhang Hu', 'Zeyi Lu', 'Zhuoran Yang', 'Zixing Zhang'], 'affiliations': ['Kuaishou Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.01949.jpg', 'data': {'categories': ['#reasoning', '#rl', '#video', '#dataset', '#benchmark', '#training', '#multimodal', '#alignment'], 'emoji': '🎥', 'ru': {'title': 'Kwai Keye-VL: Прорыв в понимании коротких видео с помощью мультимодального ИИ', 'desc': 'Статья представляет Kwai Keye-VL - мультимодальную языковую модель с 8 миллиардами параметров, разработанную для понимания коротких видео. Модель обучена на массивном наборе данных объемом более 600 миллиардов токенов с акцентом на видеоконтент. Инновационный процесс обучения включает четырехэтапное предварительное обучение и двухфазное пост-обучение, направленное на улучшение рассуждений и следование инструкциям. Keye-VL достигает передовых результатов на эталонных тестах по видео, сохраняя высокую производительность в задачах обработки изображений.'}, 'en': {'title': 'Revolutionizing Short-Video Understanding with Keye-VL', 'desc': 'This paper presents Kwai Keye-VL, a multimodal large language model designed to improve understanding of short-form videos, which are increasingly popular. Keye-VL is built on a vast dataset of over 600 billion tokens, focusing on video content, and employs a unique training strategy that includes a four-stage pre-training and a two-phase post-training process. The model enhances its reasoning abilities through a novel data mixture that encourages different modes of thinking, followed by reinforcement learning to refine its outputs. Evaluations demonstrate that Keye-VL outperforms existing models on video benchmarks while maintaining strong performance on general image tasks.'}, 'zh': {'title': '短视频理解的新突破：Kwai Keye-VL', 'desc': '本论文介绍了一种名为Kwai Keye-VL的多模态大语言模型，专注于短视频理解。该模型拥有80亿个参数，旨在提升对动态、信息密集型短视频的理解能力，同时保持强大的通用视觉-语言能力。Keye-VL的开发基于两个核心支柱：一个超过6000亿个标记的高质量数据集，特别强调视频内容，以及一种创新的训练方法，包括四阶段的预训练和两阶段的后训练过程。通过强化学习和对齐步骤，Keye-VL在公共视频基准测试中取得了最先进的结果，并在一般图像任务中保持竞争力。'}}}, {'id': 'https://huggingface.co/papers/2507.01945', 'title': 'LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory', 'url': 'https://huggingface.co/papers/2507.01945', 'abstract': 'Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.', 'score': 60, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': 'cf167e3958c2df99', 'authors': ['Nan Chen', 'Mengqi Huang', 'Yihao Meng', 'Zhendong Mao'], 'affiliations': ['Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.01945.jpg', 'data': {'categories': ['#video', '#open_source', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'LongAnimation: Революция в автоматической колоризации длинных анимаций', 'desc': 'Исследователи представили новый подход к автоматической колоризации длинных анимаций под названием LongAnimation. Эта система использует динамическую глобально-локальную парадигму для достижения согласованности цветов в долгосрочной перспективе. LongAnimation включает в себя модуль SketchDiT для извлечения гибридных эталонных признаков, динамическую глобально-локальную память (DGLM) для адаптивного слияния глобальных и локальных особенностей, а также функцию поощрения согласованности цветов. Эксперименты показали эффективность системы как для коротких (14 кадров), так и для длинных (в среднем 500 кадров) анимаций в задаче колоризации анимации с открытым доменом.'}, 'en': {'title': 'Achieving Long-Term Color Consistency in Animation Colorization', 'desc': 'This paper presents LongAnimation, a framework designed to automate the colorization of long animations, addressing the high labor costs associated with traditional methods. It critiques existing approaches that focus on short-term colorization and local feature fusion, which often overlook the importance of global color consistency. The proposed method utilizes a Dynamic Global-Local Memory (DGLM) to dynamically integrate global features with current generation data, ensuring a cohesive color palette throughout the animation. Additionally, a Color Consistency Reward is introduced to enhance the smoothness of transitions between video segments, demonstrating effectiveness in both short-term and long-term animation colorization tasks.'}, 'zh': {'title': '动态全局-局部范式实现动画色彩一致性', 'desc': '动画上色是动画产业生产中的重要环节，长时间动画的上色成本高昂。因此，基于视频生成模型的自动化长时间动画上色具有重要的研究价值。现有研究主要集中在短期上色，采用局部范式来实现局部片段之间的平滑过渡，但忽视了全局信息，导致长期色彩一致性不足。本研究提出了一种动态全局-局部范式，通过动态提取与当前生成相关的全局色彩一致特征，提出了LongAnimation框架，有效维护了短期和长期的色彩一致性。'}}}, {'id': 'https://huggingface.co/papers/2507.01634', 'title': 'Depth Anything at Any Condition', 'url': 'https://huggingface.co/papers/2507.01634', 'abstract': 'We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.   Project Page: https://ghost233lism.github.io/depthanything-AC-page   Code: https://github.com/HVision-NKU/DepthAnythingAC', 'score': 34, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '48dee9247e2393f0', 'authors': ['Boyuan Sun', 'Modi Jin', 'Bowen Yin', 'Qibin Hou'], 'affiliations': ['VCIP, School of Computer Science, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01634.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#data', '#training', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Универсальная оценка глубины изображений в любых условиях', 'desc': 'DepthAnything-AC - это модель монокулярной оценки глубины, способная работать в различных условиях окружающей среды. Она решает проблемы предыдущих моделей, которые плохо справлялись со сложными условиями освещения, погоды и искажениями датчиков. Авторы предлагают парадигму обучения с регуляризацией согласованности без учителя, требующую небольшого количества немаркированных данных. Также они вводят ограничение пространственного расстояния для улучшения семантических границ и деталей.'}, 'en': {'title': 'Mastering Depth Estimation in Any Environment', 'desc': 'DepthAnything-AC is a monocular depth estimation model designed to perform well in various challenging environmental conditions. It addresses the limitations of previous models that struggle with issues like lighting changes and weather effects. The model uses an unsupervised consistency regularization approach, allowing it to learn effectively from a small amount of unlabeled data. Additionally, it incorporates a Spatial Distance Constraint to improve the accuracy of depth estimation by focusing on the relationships between different image patches.'}, 'zh': {'title': '在任何条件下的深度估计新突破', 'desc': '本文介绍了一种名为Depth Anything at Any Condition（DepthAnything-AC）的单目深度估计模型，能够在多种环境条件下进行有效的深度估计。以往的深度估计模型在一般场景中表现良好，但在复杂的开放世界环境中，如光照变化和恶劣天气下，表现不佳。为了解决数据稀缺和从受损图像生成高质量伪标签的困难，本文提出了一种无监督一致性正则化微调方法，仅需少量未标记数据。实验结果表明，DepthAnything-AC在多种基准测试中展现了零样本能力，包括真实世界的恶劣天气基准和合成损坏基准。'}}}, {'id': 'https://huggingface.co/papers/2507.01925', 'title': 'A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective', 'url': 'https://huggingface.co/papers/2507.01925', 'abstract': 'The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.', 'score': 19, 'issue_id': 4618, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '28708b74dd1e7612', 'authors': ['Yifan Zhong', 'Fengshuo Bai', 'Shaofei Cai', 'Xuchuan Huang', 'Zhang Chen', 'Xiaowei Zhang', 'Yuanfei Wang', 'Shaoyang Guo', 'Tianrui Guan', 'Ka Nam Lui', 'Zhiquan Qi', 'Yitao Liang', 'Yuanpei Chen', 'Yaodong Yang'], 'affiliations': ['Institute for AI, Peking University', 'PKU-PsiBot Joint Lab', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01925.jpg', 'data': {'categories': ['#survey', '#reasoning', '#multimodal', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Единый взгляд на модели зрения-языка-действия через призму токенизации действий', 'desc': 'Эта статья посвящена моделям зрения-языка-действия (VLA) в машинном обучении. Авторы предлагают унифицированную структуру для понимания различных подходов к VLA, основанную на концепции токенов действий. Они выделяют восемь типов токенов действий, включая языковое описание, код, возможности, траектории и другие. Исследование анализирует сильные и слабые стороны каждого типа токенов и определяет перспективные направления для будущих исследований в области VLA.'}, 'en': {'title': 'Unifying Vision-Language-Action Models through Action Tokenization', 'desc': 'This paper discusses the progress of vision-language-action (VLA) models, which integrate visual and linguistic inputs to perform actions in the physical world. It identifies a common framework among these models, where a series of VLA modules process inputs to generate action tokens that convey actionable information. The authors categorize these action tokens into various types, such as language descriptions and trajectories, highlighting the importance of how they are formulated. The survey aims to clarify the role of action tokens in VLA development, assess their strengths and weaknesses, and suggest future research directions to enhance the effectiveness of VLA models.'}, 'zh': {'title': '统一行动标记，推动VLA模型发展', 'desc': '本文探讨了视觉-语言-行动（VLA）模型在多模态理解、推理和生成方面的进展。尽管当前的VLA模型方法多样，但可以统一在一个框架下，处理视觉和语言输入，生成逐步编码的行动标记。文章还指出，VLA模型的主要设计选择在于行动标记的形式化方式，包括语言描述、代码、可用性、轨迹、目标状态等。通过对现有VLA研究的分类和解读，本文旨在识别改进领域，并为未来研究提供指导。'}}}, {'id': 'https://huggingface.co/papers/2507.01953', 'title': 'FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model', 'url': 'https://huggingface.co/papers/2507.01953', 'abstract': 'We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.', 'score': 12, 'issue_id': 4617, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': 'de2dfee54bea9af5', 'authors': ['Yukang Cao', 'Chenyang Si', 'Jinghao Wang', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.01953.jpg', 'data': {'categories': ['#cv'], 'emoji': '🔄', 'ru': {'title': 'FreeMorph: Революция в морфинге изображений без дополнительного обучения', 'desc': 'FreeMorph - это первый метод морфинга изображений, не требующий дополнительной настройки и способный работать с изображениями различной семантики и компоновки. В отличие от существующих методов, основанных на дообучении предобученных диффузионных моделей, FreeMorph обеспечивает высококачественный морфинг без необходимости обучения для каждого конкретного случая. Метод включает в себя два ключевых нововведения: сферическую интерполяцию с учетом управления и ориентированную на шаги тенденцию изменения. Эксперименты показывают, что FreeMorph превосходит существующие методы, работая в 10-50 раз быстрее и устанавливая новый стандарт качества в области морфинга изображений.'}, 'en': {'title': 'Revolutionizing Image Morphing with Tuning-Free Efficiency', 'desc': 'FreeMorph is a novel method for image morphing that does not require tuning, making it efficient and effective for images with different meanings or layouts. It overcomes limitations of previous techniques that needed fine-tuning of diffusion models, which could lead to time delays and inconsistencies. The method introduces a guidance-aware spherical interpolation to enhance the quality of transitions and reduce identity loss, while also employing a step-oriented variation trend for smoother morphing. Evaluations show that FreeMorph is significantly faster and achieves superior results compared to existing methods, setting a new benchmark in the field.'}, 'zh': {'title': 'FreeMorph：无需调优的高效图像变形方法', 'desc': '本文介绍了FreeMorph，这是一种首个无需调优的图像变形方法，能够处理具有不同语义或布局的输入。与依赖于微调预训练扩散模型的现有方法不同，FreeMorph无需针对每个实例进行训练，能够高保真地实现图像变形。尽管调优自由的方法在效率上具有优势，但由于多步去噪过程的非线性特性和预训练扩散模型的偏差，保持高质量结果仍然面临挑战。我们通过引入指导感知的球面插值设计和步骤导向的变化趋势，成功解决了这些问题，使FreeMorph在速度上比现有方法快10到50倍，并建立了图像变形的新标准。'}}}, {'id': 'https://huggingface.co/papers/2507.01957', 'title': 'Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation', 'url': 'https://huggingface.co/papers/2507.01957', 'abstract': 'We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256times256 res.) and 1024 to 48 (512times512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4times lower latency than previous parallelized autoregressive models.', 'score': 10, 'issue_id': 4621, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': 'b2594c8c1eebcb0c', 'authors': ['Zhuoyang Zhang', 'Luke J. Huang', 'Chengyue Wu', 'Shang Yang', 'Kelly Peng', 'Yao Lu', 'Song Han'], 'affiliations': ['First Intelligence', 'MIT', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2507.01957.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Революция в скорости генерации изображений без потери качества', 'desc': 'Статья представляет новый метод Locality-aware Parallel Decoding (LPD) для ускорения авторегрессивной генерации изображений. Авторы вводят две ключевые техники: гибкое параллелизованное авторегрессивное моделирование и локально-ориентированное упорядочивание генерации. Эти техники позволяют значительно сократить количество шагов генерации без ухудшения качества изображений. В результате достигается как минимум 3.4-кратное снижение задержки по сравнению с предыдущими параллелизованными авторегрессивными моделями.'}, 'en': {'title': 'Accelerating Image Generation with Locality-aware Parallel Decoding', 'desc': 'This paper introduces Locality-aware Parallel Decoding (LPD) to improve the speed of autoregressive image generation. Traditional methods face high latency due to memory constraints when predicting the next patch of an image. The authors propose two innovative techniques: a flexible architecture for parallelized autoregressive modeling and a locality-aware generation ordering that optimizes the order of patch generation. These advancements significantly reduce the number of generation steps and latency while maintaining high image quality, outperforming previous models.'}, 'zh': {'title': '加速自回归图像生成的新方法', 'desc': '我们提出了一种局部感知并行解码（LPD）方法，以加速自回归图像生成。传统的自回归图像生成依赖于下一个补丁的预测，这一过程受内存限制，导致延迟较高。我们引入了灵活的并行自回归建模和局部感知生成顺序两项关键技术，以实现高并行性并保持生成质量。通过这些设计，我们将生成步骤从256减少到20，并在不影响质量的情况下，显著降低了延迟。'}}}, {'id': 'https://huggingface.co/papers/2507.01544', 'title': 'MARVIS: Modality Adaptive Reasoning over VISualizations', 'url': 'https://huggingface.co/papers/2507.01544', 'abstract': 'Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16\\% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis', 'score': 7, 'issue_id': 4624, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '547bee35865d6ddd', 'authors': ['Benjamin Feuer', 'Lennart Purucker', 'Oussama Elachqar', 'Chinmay Hegde'], 'affiliations': ['NYU', 'Oumi.AI', 'University of Freiburg'], 'pdf_title_img': 'assets/pdf/title_img/2507.01544.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#training', '#open_source', '#dataset', '#science'], 'emoji': '🔮', 'ru': {'title': 'MARVIS: универсальный метод для точных предсказаний в разных модальностях без дополнительного обучения', 'desc': 'MARVIS - это метод, позволяющий небольшим мультимодальным моделям предсказывать данные различных модальностей с высокой точностью. Он преобразует латентные пространства в визуальные представления, используя навыки пространственных рассуждений VLM для их интерпретации. MARVIS достигает конкурентоспособных результатов в различных доменах, превосходя Gemini на 16% в среднем, без необходимости в специфическом обучении. Метод не раскрывает персональную информацию и доступен в открытом исходном коде.'}, 'en': {'title': 'Unlocking Versatility in Vision-Language Models with MARVIS', 'desc': 'This paper introduces MARVIS, a novel method that enhances small vision-language models by allowing them to predict various data modalities without the need for training. MARVIS converts latent embeddings into visual representations, enabling the models to apply their reasoning capabilities effectively across different domains. The approach demonstrates competitive performance in areas like vision, audio, and biological data, outperforming existing models like Gemini by 16% on average. Importantly, MARVIS maintains privacy by not requiring any domain-specific training or exposing personally identifiable information.'}, 'zh': {'title': 'MARVIS：小型模型的多模态预测新方法', 'desc': '本论文提出了一种名为MARVIS的方法，旨在提高小型视觉语言模型在多种数据模态上的预测准确性。MARVIS通过将潜在嵌入空间转化为视觉表示，利用视觉语言模型的空间和细粒度推理能力，成功解读和利用这些表示。该方法无需特定领域的训练，能够在视觉、音频、生物和表格数据等领域中实现竞争性的性能。MARVIS在不暴露个人可识别信息的情况下，使用单个3B参数模型的表现超越了Gemini，接近专业方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2507.00316', 'title': 'μ^2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for\n  Radiology Report Generation', 'url': 'https://huggingface.co/papers/2507.00316', 'abstract': 'Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose mu^2LLM, a textbf{mu}ltiscale textbf{mu}ltimodal large language models for RRG tasks. The novel {mu}^2Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasetdemonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned mu^2LLMs on limited data for RRG tasks.', 'score': 7, 'issue_id': 4633, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '3456781439257bdc', 'authors': ['Siyou Li', 'Pengyao Qin', 'Huanan Wu', 'Dong Nie', 'Arun J. Thirunavukarasu', 'Juntao Yu', 'Le Zhang'], 'affiliations': ['Guangdong University of Technology, Guangdong, China', 'Meta Inc. US', 'Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK', 'School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK', 'School of Engineering, College of Engineering and Physical Sciences, University of Birmingham, Birmingham, UK', 'William Harvey Research Institute, NIHR Barts Biomedical Research Centre, Queen Mary University London, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2507.00316.jpg', 'data': {'categories': ['#optimization', '#low_resource', '#training', '#dataset', '#healthcare', '#multimodal'], 'emoji': '🏥', 'ru': {'title': 'Мультимодальные языковые модели улучшают автоматическую генерацию радиологических отчетов', 'desc': 'Статья представляет mu^2LLM - новый подход к автоматической генерации радиологических отчетов на основе КТ-снимков. Авторы предлагают мультимодальную и мультимасштабную архитектуру, объединяющую визуальные и текстовые признаки через специальный токенизатор mu^2Tokenizer. Модель оптимизируется с помощью метода прямой оптимизации предпочтений (DPO) под руководством GREEN-RedLlama. Эксперименты на четырех крупных медицинских датасетах показывают превосходство предложенного метода над существующими подходами.'}, 'en': {'title': 'Enhancing Radiology Reports with Multiscale Multimodal Models', 'desc': 'This paper presents a new approach called mu^2LLM for automated radiology report generation (RRG) from CT scans. It addresses two main challenges: extracting relevant information from complex imaging data and evaluating the quality of generated reports compared to expert-written ones. The proposed mu^2Tokenizer combines visual and textual features to improve the quality of the reports, using a method called direct preference optimization (DPO). Experimental results show that mu^2LLM outperforms existing methods, demonstrating its effectiveness even with limited data.'}, 'zh': {'title': '多模态大语言模型提升放射学报告生成', 'desc': '自动化放射学报告生成（RRG）旨在从临床影像（如CT扫描）中生成详细的文本报告，以提高诊断的准确性和效率。RRG面临两个主要挑战：一是从影像数据中提取相关信息的复杂性，二是客观评估模型生成报告与专家撰写报告之间差异的困难。为了解决这些挑战，我们提出了mu^2LLM，这是一种用于RRG任务的多尺度多模态大语言模型。我们的实验结果表明，mu^2LLM在有限数据上优于现有方法，展示了其在RRG任务中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.22868', 'title': 'STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing', 'url': 'https://huggingface.co/papers/2506.22868', 'abstract': 'STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.', 'score': 4, 'issue_id': 4621, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '94371810be905c93', 'authors': ['Junsung Lee', 'Junoh Kang', 'Bohyung Han'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2506.22868.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'STR-Match: Революция в редактировании видео с помощью ИИ', 'desc': 'STR-Match - это алгоритм редактирования видео без обучения, использующий латентную оптимизацию и новую метрику STR. Он использует 2D пространственное и 1D временное внимание в диффузионных моделях текст-в-видео для создания пространственно-временно согласованных и визуально привлекательных отредактированных видео. STR-Match превосходит существующие методы по визуальному качеству и пространственно-временной согласованности. Алгоритм способен сохранять ключевые визуальные атрибуты исходного видео даже при значительных преобразованиях домена.'}, 'en': {'title': 'Enhancing Video Editing with STR-Match: Coherence Meets Quality', 'desc': 'STR-Match is a novel video editing algorithm that enhances the quality and coherence of AI-generated videos. It utilizes a unique STR score to assess the relevance of pixels across time and space, ensuring that the edited videos maintain visual appeal and temporal consistency. By employing 2D spatial attention and 1D temporal attention, STR-Match avoids the complexity of 3D attention mechanisms while still achieving impressive results. The method is training-free and effectively handles significant domain transformations, outperforming existing techniques in visual quality and spatiotemporal coherence.'}, 'zh': {'title': 'STR-Match：时空一致的视觉视频编辑新方法', 'desc': 'STR-Match是一种无训练的视频编辑算法，旨在生成视觉上吸引人且时空一致的视频。它通过引入新颖的STR评分，利用2D空间注意力和1D时间模块来捕捉相邻帧之间的像素相关性。与传统方法相比，STR-Match避免了计算开销大的3D注意力机制，能够在显著的领域转换下保持视频的关键视觉特征。实验结果表明，STR-Match在视觉质量和时空一致性方面始终优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.23552', 'title': 'JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching', 'url': 'https://huggingface.co/papers/2506.23552', 'abstract': 'The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web', 'score': 3, 'issue_id': 4615, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '69157bacab4dea7d', 'authors': ['Mingi Kwon', 'Joonghyuk Shin', 'Jaeseok Jung', 'Jaesik Park', 'Youngjung Uh'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23552.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Единая модель для синтеза речи и анимации лица', 'desc': 'JAM-Flow - это унифицированная модель для одновременного синтеза лицевой анимации и речи. Она использует технологию flow matching и новую архитектуру Multi-Modal Diffusion Transformer (MM-DiT) с модулями Motion-DiT и Audio-DiT. Модель обучается с помощью инпейнтинга и поддерживает различные входные данные, включая текст, аудио и референсное движение. JAM-Flow представляет собой значительный прогресс в мультимодальном генеративном моделировании, обеспечивая целостный аудиовизуальный синтез.'}, 'en': {'title': 'Unified Synthesis of Speech and Facial Motion with JAM-Flow', 'desc': 'This paper presents JAM-Flow, a new framework that combines facial motion and speech synthesis into one model. It uses advanced techniques like flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to allow for effective interaction between audio and visual data. The model includes specialized components for handling both motion and audio, ensuring that each modality retains its unique characteristics while working together. By training with a unique inpainting-style objective, JAM-Flow can generate synchronized talking heads from various inputs, making it a significant advancement in multi-modal generative modeling.'}, 'zh': {'title': '统一面部运动与语音的生成模型', 'desc': '这篇论文提出了一种名为JAM-Flow的统一框架，旨在同时合成面部运动和语音。该方法利用流匹配和新颖的多模态扩散变换器（MM-DiT）架构，集成了专门的运动和音频模块。通过选择性联合注意力层，这些模块实现了有效的跨模态交互，同时保留了各自模态的优势。JAM-Flow支持多种条件输入，能够在单一模型中实现文本驱动的同步人头生成和音频驱动的动画等任务。'}}}, {'id': 'https://huggingface.co/papers/2507.00472', 'title': 'ARIG: Autoregressive Interactive Head Generation for Real-time\n  Conversations', 'url': 'https://huggingface.co/papers/2507.00472', 'abstract': 'Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.', 'score': 2, 'issue_id': 4628, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '083f371459a06cdd', 'authors': ['Ying Guo', 'Xi Liu', 'Cheng Zhen', 'Pengfei Yan', 'Xiaoming Wei'], 'affiliations': ['Vision AI Department, Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2507.00472.jpg', 'data': {'categories': ['#games', '#agents', '#optimization', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'ARIG: Реалистичная генерация движений виртуального собеседника в реальном времени', 'desc': 'Статья представляет новый подход к генерации интерактивных движений головы виртуального агента в режиме реального времени. Авторы предлагают авторегрессионную модель ARIG, использующую диффузионный процесс для более точного предсказания движений в непрерывном пространстве. Модель включает модули для понимания интерактивного поведения (IBU) и детального понимания состояний разговора (CSU). Эксперименты подтвердили эффективность предложенного подхода для создания реалистичных интерактивных движений виртуального агента.'}, 'en': {'title': 'Real-Time Realism in Virtual Conversations with ARIG', 'desc': 'This paper presents a new method for generating realistic interactions in virtual agents, focusing on head motion during conversations. The proposed autoregressive framework, called ARIG, allows for real-time motion prediction by modeling it as a continuous process rather than using discrete codes. It enhances interaction realism by incorporating interactive behavior understanding (IBU) and conversational state understanding (CSU), which analyze both short-term and long-term conversational dynamics. The results from extensive experiments demonstrate that ARIG significantly improves the quality and smoothness of virtual agent interactions compared to previous methods.'}, 'zh': {'title': '提升虚拟代理交互真实感的自回归生成框架', 'desc': '本论文研究了交互式头部生成，以提高虚拟代理的实时交互能力。我们提出了一种基于自回归（AR）的框架ARIG，通过非向量量化的AR过程来建模运动预测，从而实现更准确的连续空间预测。为了增强交互的真实感，我们强调了交互行为理解（IBU）和详细的对话状态理解（CSU），通过双轨双模态信号来总结短期行为，并对长期行为进行上下文理解。实验结果验证了我们模型的有效性，显示出在实时生成和交互真实感方面的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.09477', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs', 'url': 'https://huggingface.co/papers/2507.09477', 'abstract': 'This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.', 'score': 47, 'issue_id': 4862, 'pub_date': '2025-07-13', 'pub_date_card': {'ru': '13 июля', 'en': 'July 13', 'zh': '7月13日'}, 'hash': 'da4aa711048f0a7f', 'authors': ['Yangning Li', 'Weizhi Zhang', 'Yuyao Yang', 'Wei-Chieh Huang', 'Yaozu Wu', 'Junyu Luo', 'Yuanchen Bei', 'Henry Peng Zou', 'Xiao Luo', 'Yusheng Zhao', 'Chunkit Chan', 'Yankai Chen', 'Zhongfen Deng', 'Yinghui Li', 'Hai-Tao Zheng', 'Dongyuan Li', 'Renhe Jiang', 'Ming Zhang', 'Yangqiu Song', 'Philip S. Yu'], 'affiliations': ['HKUST', 'Peking University', 'The University of Tokyo', 'Tsinghua University', 'University of California, Los Angeles', 'University of Illinois Chicago', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.09477.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#survey', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Объединение извлечения информации и рассуждений для создания более мощных языковых моделей', 'desc': 'Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LLM) для улучшения фактической точности и многоступенчатого вывода. В статье рассматриваются подходы, сочетающие Retrieval-Augmented Generation (RAG) и методы рассуждений, что позволяет преодолеть ограничения каждого из них по отдельности. Авторы выделяют синергетические фреймворки RAG-Reasoning, демонстрирующие передовые результаты на задачах, требующих обширных знаний. Обзор также очерчивает направления будущих исследований в этой области.'}, 'en': {'title': 'Enhancing LLMs with Synergized Retrieval and Reasoning', 'desc': 'This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs.'}, 'zh': {'title': '推理与检索的协同提升大型语言模型的能力', 'desc': '这篇论文调查了大型语言模型中的推理与检索的结合，以提高事实准确性和多步推理能力。检索增强生成（RAG）通过引入外部知识来提升大型语言模型的事实性，但在需要多步推理的问题上表现不足。论文提出了统一的推理-检索视角，展示了如何通过先进的推理优化RAG的每个阶段，并强调了新兴的协同RAG-推理框架。最后，论文分类了方法、数据集和开放挑战，并概述了未来研究方向，以构建更有效、适应多模态、可信赖和以人为本的RAG-推理系统。'}}}, {'id': 'https://huggingface.co/papers/2507.12465', 'title': 'PhysX: Physical-Grounded 3D Asset Generation', 'url': 'https://huggingface.co/papers/2507.12465', 'abstract': 'PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.', 'score': 20, 'issue_id': 4861, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'ece62f7e4ecd0487', 'authors': ['Ziang Cao', 'Zhaoxi Chen', 'Linag Pan', 'Ziwei Liu'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.12465.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#games', '#3d', '#open_source', '#dataset'], 'emoji': '🧱', 'ru': {'title': 'Физически достоверная генерация 3D-объектов', 'desc': 'PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет PhysXNet с аннотациями физических характеристик 3D-моделей и разработали фреймворк PhysXGen для интеграции физических знаний в процесс генерации. PhysXGen использует двухветвевую архитектуру для моделирования связей между 3D-структурами и физическими свойствами. Эксперименты показали превосходную производительность и способность к обобщению предложенного метода.'}, 'en': {'title': 'Bridging 3D Generation with Real-World Physics', 'desc': 'PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations.'}, 'zh': {'title': '物理驱动的3D资产生成新方法', 'desc': 'PhysX提出了一种新的方法来解决3D生成模型中缺乏物理属性的问题。它引入了PhysXNet，这是一个物理注释的数据集，系统地标注了五个基础维度，包括绝对尺度、材料、可用性、运动学和功能描述。通过PhysXGen框架，物理知识被整合到3D资产生成中，利用双分支架构建模3D结构与物理属性之间的潜在关联。实验结果表明，该框架在生成具有可信物理预测的3D资产方面表现优越，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.12415', 'title': 'SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?', 'url': 'https://huggingface.co/papers/2507.12415', 'abstract': 'SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  \t\t\t\t\tAI-generated summary \t\t\t\t Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.', 'score': 17, 'issue_id': 4864, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '6be5b41deae78198', 'authors': ['Xinyi He', 'Qian Liu', 'Mingzhe Du', 'Lin Yan', 'Zhijie Fan', 'Yiming Huang', 'Zejian Yuan', 'Zejun Ma'], 'affiliations': ['National University of Singapore', 'TikTok', 'University of California San Diego', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12415.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'SWE-Perf: Новый рубеж в оценке оптимизации кода с помощью LLM', 'desc': 'SWE-Perf - это бенчмарк для оценки способностей больших языковых моделей (LLM) в оптимизации производительности кода на основе реальных репозиториев. Он содержит 140 тщательно отобранных примеров из популярных GitHub-репозиториев, включающих кодовую базу, целевые функции, тесты производительности и экспертные патчи. Оценка существующих методов показала значительный разрыв между возможностями LLM и экспертным уровнем оптимизации. SWE-Perf открывает новые направления исследований в области применения LLM для повышения производительности кода.'}, 'en': {'title': 'Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf', 'desc': 'SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area.'}, 'zh': {'title': 'SWE-Perf：评估代码性能优化的新基准', 'desc': 'SWE-Perf是一个基准测试，用于评估大型语言模型在代码性能优化方面的表现。该基准测试使用真实的代码库数据，专注于软件工程中的代码性能优化。虽然大型语言模型在代码生成和错误修复方面表现出色，但它们在提升代码性能方面的能力尚未得到充分探索。通过对140个精心挑选的实例进行评估，SWE-Perf揭示了现有大型语言模型与专家级优化性能之间的显著差距，指出了这一新兴领域中的重要研究机会。'}}}, {'id': 'https://huggingface.co/papers/2507.12463', 'title': 'MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding', 'url': 'https://huggingface.co/papers/2507.12463', 'abstract': 'A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.', 'score': 15, 'issue_id': 4864, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'c5061caaead150e7', 'authors': ['Renjie Li', 'Ruijie Ye', 'Mingyang Wu', 'Hao Frank Yang', 'Zhiwen Fan', 'Hezhen Hu', 'Zhengzhong Tu'], 'affiliations': ['Brown University', 'Johns Hopkins University', 'Texas A&M University', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.12463.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'MMHU: Комплексный анализ поведения человека для безопасного автономного вождения', 'desc': 'Представлен крупномасштабный бенчмарк MMHU для анализа поведения человека в контексте автономного вождения. Он включает богатые аннотации и разнообразные источники данных, охватывающие 57 тысяч клипов с движениями людей и 1,73 миллиона кадров. MMHU позволяет оценивать различные задачи, включая прогнозирование движения и ответы на вопросы о поведении. Бенчмарк предоставляет комплексный набор для оценки понимания человеческого поведения в системах автономного вождения.'}, 'en': {'title': 'MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving', 'desc': 'The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios.'}, 'zh': {'title': 'MMHU：自动驾驶人类行为分析的新基准', 'desc': '本文提出了一个名为MMHU的大规模基准，用于分析自动驾驶中的人类行为。该基准包含丰富的注释和多样的数据来源，涵盖了人类运动、轨迹、意图等多个方面。数据集包括57,000个运动片段和173万帧，来源于知名的驾驶数据集和YouTube等平台。我们还开发了一个人机协作的注释流程，以生成详细的行为描述，并对多个任务进行了基准测试，包括运动预测和行为问答。'}}}, {'id': 'https://huggingface.co/papers/2507.11949', 'title': 'MOSPA: Human Motion Generation Driven by Spatial Audio', 'url': 'https://huggingface.co/papers/2507.11949', 'abstract': 'A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.', 'score': 12, 'issue_id': 4862, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '60ad5621a3842ae5', 'authors': ['Shuyang Xu', 'Zhiyang Dou', 'Mingyi Shi', 'Liang Pan', 'Leo Ho', 'Jingbo Wang', 'Yuan Liu', 'Cheng Lin', 'Yuexin Ma', 'Wenping Wang', 'Taku Komura'], 'affiliations': ['Macau University of Science and Technology', 'Shanghai AI Lab', 'ShanghaiTech University', 'Texas A&M University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.11949.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#benchmark', '#open_source', '#dataset'], 'emoji': '🎧', 'ru': {'title': 'Реалистичная анимация движений под пространственное аудио', 'desc': 'Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в ответ на пространственное аудио. Для обучения модели был создан новый набор данных SAM, содержащий разнообразные пространственные аудио и соответствующие им движения. MOSPA использует эффективный механизм слияния для захвата взаимосвязи между движениями тела и пространственным звуком. Модель достигла лучших результатов в этой задаче по сравнению с существующими подходами.'}, 'en': {'title': 'Bridging Sound and Motion: MOSPA for Realistic Human Animation', 'desc': 'The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area.'}, 'zh': {'title': '空间音频驱动的人类运动生成新突破', 'desc': '本文介绍了一种基于扩散的生成框架MOSPA，用于建模人类在空间音频刺激下的运动。我们创建了首个综合性的空间音频驱动人类运动（SAM）数据集，包含多样化和高质量的空间音频与运动数据。MOSPA通过有效的融合机制，准确捕捉身体运动与空间音频之间的关系，能够生成多样且真实的人类运动。经过广泛的实验验证，我们的方法在这一任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.11412', 'title': 'Seq vs Seq: An Open Suite of Paired Encoders and Decoders', 'url': 'https://huggingface.co/papers/2507.11412', 'abstract': 'The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.', 'score': 11, 'issue_id': 4871, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '569bfad741b150bd', 'authors': ['Orion Weller', 'Kathryn Ricci', 'Marc Marone', 'Antoine Chaffin', 'Dawn Lawrie', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University', 'LightOn'], 'pdf_title_img': 'assets/pdf/title_img/2507.11412.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Ettin: новый стандарт для энкодеров и декодеров в машинном обучении', 'desc': 'Исследователи представили набор моделей Ettin, включающий энкодер-только и декодер-только архитектуры от 17 миллионов до 1 миллиарда параметров. Модели обучены на 2 триллионах токенов и превосходят существующие аналоги в своих категориях. Подтверждено, что энкодеры лучше справляются с задачами классификации и поиска, а декодеры - с генеративными задачами. Показано, что адаптация декодера к задачам энкодера (и наоборот) путем дообучения уступает использованию изначально правильной архитектуры.'}, 'en': {'title': 'Unlocking the Power of Encoder and Decoder Models in NLP', 'desc': 'This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.'}, 'zh': {'title': 'Ettin模型：编码器与解码器的完美结合', 'desc': '本文介绍了一种新的模型套件Ettin，包含配对的编码器和解码器模型，参数范围从1700万到10亿，训练数据达到2万亿个标记。研究表明，编码器模型在分类和检索任务中表现优异，而解码器模型在生成任务中更具优势。通过相同的训练方法，Ettin模型在各自的类别中达到了最新的性能，超越了现代的BERT和Llama 3.2等模型。我们还发现，继续训练解码器模型以适应编码器任务的效果不如直接使用编码器模型。'}}}, {'id': 'https://huggingface.co/papers/2507.11527', 'title': 'DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering', 'url': 'https://huggingface.co/papers/2507.11527', 'abstract': "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.", 'score': 10, 'issue_id': 4861, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': 'e6f20729b2c748f9', 'authors': ['Yinsheng Li', 'Zhen Dong', 'Yi Shao'], 'affiliations': ['Department of Civil Engineering McGill University', 'NVIDIA', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2507.11527.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#open_source', '#long_context', '#agents'], 'emoji': '📐', 'ru': {'title': 'DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании', 'desc': 'DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в задаче проверки технических чертежей. Он оценивает способности агентов в понимании структурированных данных, выполнении функций, следовании инструкциям и критическом мышлении. Бенчмарк содержит 12 типов задач, основанных на реальных чертежах, с 46 специальными функциями и инструментами, всего 1920 заданий. DrafterBench предлагает детальный анализ точности выполнения задач и статистики ошибок, чтобы глубже понять возможности агентов и определить цели для улучшения интеграции LLM в инженерные приложения.'}, 'en': {'title': 'DrafterBench: Evaluating LLMs for Technical Drawing Mastery', 'desc': 'DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications.'}, 'zh': {'title': 'DrafterBench：评估LLM代理的技术图纸修订能力', 'desc': 'DrafterBench是一个开源基准，用于评估大型语言模型（LLM）代理在技术图纸修订中的能力。它涵盖了结构化数据理解、功能执行、指令遵循和批判性推理等多个方面。该基准包含来自真实绘图文件的十二种任务，提供了46种定制功能和1920个任务。DrafterBench旨在深入分析代理的能力，帮助识别在工程应用中整合LLM的改进目标。'}}}, {'id': 'https://huggingface.co/papers/2507.02857', 'title': 'AnyI2V: Animating Any Conditional Image with Motion Control', 'url': 'https://huggingface.co/papers/2507.02857', 'abstract': 'AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.', 'score': 5, 'issue_id': 4864, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '5a06d615e806a525', 'authors': ['Ziye Li', 'Hao Luo', 'Xincheng Shuai', 'Henghui Ding'], 'affiliations': ['DAMO Academy, Alibaba group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.02857.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальная анимация изображений с контролем движения', 'desc': 'AnyI2V - это фреймворк для анимации условных изображений с пользовательскими траекториями движения без необходимости обучения. Он поддерживает различные типы данных, включая сетки и облака точек, что позволяет создавать более гибкие и универсальные видео. AnyI2V превосходит существующие методы текст-в-видео и изображение-в-видео, предоставляя точный контроль над пространственным расположением и движением генерируемого контента. Фреймворк также поддерживает смешанные условные входные данные и позволяет выполнять перенос стиля и редактирование с помощью LoRA и текстовых подсказок.'}, 'en': {'title': 'AnyI2V: Freedom in Motion-Controlled Video Generation', 'desc': 'AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control.'}, 'zh': {'title': 'AnyI2V：无训练的灵活视频生成框架', 'desc': 'AnyI2V是一个无需训练的框架，可以根据用户定义的运动轨迹为条件图像添加动画，支持多种数据类型，从而实现灵活的视频生成。该方法解决了现有文本到视频（T2V）和图像到视频（I2V）合成中的动态运动信号和空间约束整合问题。与传统方法不同，AnyI2V不依赖于真实图像，允许更高的可编辑性，并支持混合条件输入和风格转移。实验结果表明，AnyI2V在空间和运动控制的视频生成方面表现优越，提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2507.12462', 'title': 'SpatialTrackerV2: 3D Point Tracking Made Easy', 'url': 'https://huggingface.co/papers/2507.12462', 'abstract': 'SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos. Going beyond modular pipelines built on off-the-shelf components for 3D tracking, our approach unifies the intrinsic connections between point tracking, monocular depth, and camera pose estimation into a high-performing and feedforward 3D point tracker. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, with a fully differentiable and end-to-end architecture, allowing scalable training across a wide range of datasets, including synthetic sequences, posed RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50times faster.', 'score': 4, 'issue_id': 4867, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'a30fc76bcf7ffd92', 'authors': ['Yuxi Xiao', 'Jianyuan Wang', 'Nan Xue', 'Nikita Karaev', 'Yuri Makarov', 'Bingyi Kang', 'Xing Zhu', 'Hujun Bao', 'Yujun Shen', 'Xiaowei Zhou'], 'affiliations': ['Ant Group', 'Bytedance Seed', 'Oxford', 'Pixelwise AI', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12462.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Единая архитектура для быстрого и точного 3D-трекинга в монокулярном видео', 'desc': 'SpatialTrackerV2 - это метод отслеживания 3D точек в монокулярных видео с прямой связью. Он объединяет отслеживание точек, монокулярную оценку глубины и оценку положения камеры в единую сквозную архитектуру. Модель декомпозирует 3D движение в мировом пространстве на геометрию сцены, эго-движение камеры и попиксельное движение объектов. SpatialTrackerV2 превосходит существующие методы 3D-трекинга на 30% и соответствует точности ведущих подходов динамической 3D-реконструкции, работая при этом в 50 раз быстрее.'}, 'en': {'title': 'Unified 3D Point Tracking at Lightning Speed', 'desc': 'SpatialTrackerV2 is a novel method for tracking 3D points in monocular videos using a feed-forward architecture. It combines point tracking, monocular depth estimation, and camera pose estimation into a single, efficient model. This approach breaks down 3D motion into components like scene geometry and camera movement, enabling it to learn from diverse datasets effectively. As a result, SpatialTrackerV2 achieves a 30% improvement over existing methods and operates 50 times faster than traditional dynamic 3D reconstruction techniques.'}, 'zh': {'title': '高效快速的3D点跟踪新方法', 'desc': 'SpatialTrackerV2是一种用于单目视频的前馈3D点跟踪方法。它将点跟踪、单目深度和相机姿态估计整合到一个统一的端到端架构中，从而实现高性能和快速处理。该方法将世界空间中的3D运动分解为场景几何、相机自运动和逐像素的物体运动，支持在多种数据集上进行可扩展训练。通过从异构数据中联合学习几何和运动，SpatialTrackerV2的性能比现有的3D跟踪方法提高了30%，并且运行速度比领先的动态3D重建方法快50倍。'}}}, {'id': 'https://huggingface.co/papers/2507.09025', 'title': 'Lizard: An Efficient Linearization Framework for Large Language Models', 'url': 'https://huggingface.co/papers/2507.09025', 'abstract': "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.", 'score': 4, 'issue_id': 4861, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '3490901c2a32da3d', 'authors': ['Chien Van Nguyen', 'Ruiyi Zhang', 'Hanieh Deilamsalehy', 'Puneet Mathur', 'Viet Dac Lai', 'Haoliang Wang', 'Jayakumar Subramanian', 'Ryan A. Rossi', 'Trung Bui', 'Nikos Vlassis', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2507.09025.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training', '#long_context'], 'emoji': '🦎', 'ru': {'title': 'Lizard: эффективные языковые модели с бесконечным контекстом', 'desc': 'Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратичные архитектуры для эффективной генерации с бесконечным контекстом. Он использует гибридный механизм внимания, сочетающий линейное внимание с гейтингом и оконное внимание с мета-памятью. Lizard позволяет адаптивно управлять памятью, поддерживает вывод с постоянным объемом памяти и обеспечивает сильную обобщаемость по длине. Эксперименты показывают, что Lizard почти без потерь восстанавливает производительность исходной модели на стандартных задачах языкового моделирования.'}, 'en': {'title': 'Lizard: Efficient Infinite-Context Generation for Transformers', 'desc': 'Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks.'}, 'zh': {'title': 'Lizard：高效无限上下文生成的新框架', 'desc': 'Lizard是一个线性化框架，旨在将基于Transformer的大型语言模型（LLMs）转变为灵活的亚二次架构，以实现高效的无限上下文生成。该框架通过引入一种亚二次注意力机制，克服了传统softmax注意力在上下文长度增加时的内存和计算瓶颈，同时保持输出质量。Lizard还结合了门控模块，支持自适应内存控制和常量内存推理，增强了模型设计的灵活性。通过混合门控线性注意力和滑动窗口注意力，Lizard能够有效捕捉长距离依赖和细粒度的局部交互，显著提升了模型性能。'}}}, {'id': 'https://huggingface.co/papers/2507.05065', 'title': 'Replacing thinking with tool usage enables reasoning in small language\n  models', 'url': 'https://huggingface.co/papers/2507.05065', 'abstract': 'A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have established a new machine learning paradigm based on scaling up compute at inference time as well as at training time. In that line of work, a combination of Supervised Fine-Tuning (SFT) on synthetic demonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is used for training Large Language Models to expend extra compute during inference in the form of "thoughts" expressed in natural language. In this paper, we propose to instead format these tokens as a multi-turn interaction trace with a stateful tool. At each turn, the new state of the tool is appended to the context of the model, whose job is to generate the tokens necessary to control the tool via a custom DSL. We benchmark this approach on the problem of repairing malfunctioning Python code, and show that this constrained setup allows for faster sampling of experience and a denser reward signal, allowing even models of size up to 3B parameters to learn how to proficiently expend additional compute on the task.', 'score': 3, 'issue_id': 4866, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '31f9493dba1b5054', 'authors': ['Corrado Rainone', 'Tim Bakker', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.05065.jpg', 'data': {'categories': ['#rl', '#optimization', '#plp', '#training', '#transfer_learning', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'Эффективное обучение LLM через взаимодействие с инструментами', 'desc': 'Статья предлагает новый подход к обучению больших языковых моделей (LLM), форматируя токены как многоходовый след взаимодействия со статическим инструментом. Это позволяет ускорить сэмплирование и получить более плотные сигналы вознаграждения для таких задач, как исправление кода на Python. Метод использует специальный DSL для управления инструментом и добавляет новое состояние инструмента в контекст модели на каждом шаге. Эксперименты показали, что даже модели размером до 3 миллиардов параметров могут эффективно использовать дополнительные вычисления для решения задачи.'}, 'en': {'title': 'Empowering Language Models with Stateful Interaction for Enhanced Learning', 'desc': "This paper introduces a novel method for training Large Language Models (LLMs) by using a multi-turn interaction trace with a stateful tool. Instead of relying solely on traditional training methods, the approach allows the model to generate tokens that control the tool through a custom domain-specific language (DSL). This setup enhances the model's ability to learn from its interactions, particularly in tasks like repairing Python code, by providing faster sampling and richer reward signals. The results demonstrate that even smaller models, with up to 3 billion parameters, can effectively utilize additional computational resources to improve their performance."}, 'zh': {'title': '多轮交互：提升大型语言模型的学习效率', 'desc': '本文提出了一种新的方法，将令牌格式化为多轮交互轨迹，并使用有状态的工具来训练大型语言模型。这种方法可以加快采样速度，并为修复Python代码等任务提供更密集的奖励信号。通过在每一轮中将工具的新状态附加到模型的上下文中，模型能够生成控制工具所需的令牌。实验表明，即使是参数量达到30亿的模型，也能有效学习如何在任务中合理使用额外的计算资源。'}}}, {'id': 'https://huggingface.co/papers/2507.11764', 'title': 'AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles', 'url': 'https://huggingface.co/papers/2507.11764', 'abstract': "Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", 'score': 2, 'issue_id': 4871, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '73387831285ecb12', 'authors': ['Matteo Fasulo', 'Luca Babboni', 'Luca Tedeschini'], 'affiliations': ['Department of Computer Science and Engineering (DISI) - University of Bologna'], 'pdf_title_img': 'assets/pdf/title_img/2507.11764.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#architecture', '#low_resource', '#training'], 'emoji': '🌐', 'ru': {'title': 'Сентимент-усиленные трансформеры покоряют многоязычность', 'desc': 'Статья представляет метод улучшения классификаторов на основе трансформеров для обнаружения субъективности в многоязычных и zero-shot сценариях. Авторы интегрировали оценки сентимента, полученные от вспомогательной модели, в представления предложений. Эксперименты показали, что такой подход значительно повышает производительность, особенно F1-меру для субъективного класса. Метод продемонстрировал высокие результаты в соревновании CLEF 2025 CheckThat!, заняв первое место для греческого языка.'}, 'en': {'title': 'Boosting Subjectivity Detection with Sentiment-Enhanced Transformers', 'desc': "This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model's ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek."}, 'zh': {'title': '情感增强变换器提升多语言主观性检测', 'desc': '本文介绍了AI Wizards在CLEF 2025 CheckThat! Lab Task 1中的参与，旨在对新闻文章中的句子进行主观性检测。我们提出了一种增强型的变换器分类器，通过将情感分数与句子表示结合，来提高模型在单语、多语和零样本设置下的表现。实验结果表明，情感特征的整合显著提升了主观性F1分数，尤其是在希腊语中取得了第一名的优异成绩。我们还通过优化决策阈值来解决各语言间的类别不平衡问题。'}}}, {'id': 'https://huggingface.co/papers/2507.07451', 'title': 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.07451', 'abstract': 'RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present RLEP\\, -- \\,Reinforcement Learning with Experience rePlay\\, -- \\,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further research.', 'score': 2, 'issue_id': 4867, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '18c4eecefe9abb01', 'authors': ['Hongzhi Zhang', 'Jia Fu', 'Jingyuan Zhang', 'Kai Fu', 'Qi Wang', 'Fuzheng Zhang', 'Guorui Zhou'], 'affiliations': ['Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2507.07451.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#dataset', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'RLEP: Ускорение обучения языковых моделей через воспроизведение успешного опыта', 'desc': 'RLEP - это фреймворк обучения с подкреплением, использующий повторное воспроизведение опыта для улучшения обучения больших языковых моделей. Он фокусируется на высококачественных примерах, что приводит к более быстрой сходимости и улучшенной производительности на математических бенчмарках. RLEP использует двухфазный подход: сначала собирает проверенные траектории, а затем воспроизводит их во время последующего обучения. Этот метод позволяет избежать бесполезного исследования и сосредоточиться на перспективных путях рассуждений.'}, 'en': {'title': 'RLEP: Accelerating Learning with Experience Replay', 'desc': 'RLEP is a reinforcement learning framework designed to improve the training of large language models by utilizing experience replay. It operates in two phases: first, it collects high-quality training examples, and then it replays these examples during the training process. This method helps the model focus on successful strategies and reduces the time spent on ineffective exploration. As a result, RLEP achieves faster convergence and better performance on math-related tasks, significantly enhancing accuracy on various benchmarks.'}, 'zh': {'title': 'RLEP：高效强化学习与经验重放的结合', 'desc': 'RLEP是一种强化学习框架，结合了经验重放，旨在提高大型语言模型的训练效率。该框架通过收集经过验证的轨迹，并在后续训练中重放这些轨迹，来优化学习过程。通过重放高质量的示例，RLEP能够引导模型避免无效的探索，专注于有前景的推理路径。实验结果表明，RLEP在多个数学基准测试中显著提高了模型的准确性，且所需的更新次数大幅减少。'}}}, {'id': 'https://huggingface.co/papers/2507.14683', 'title': 'MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization', 'url': 'https://huggingface.co/papers/2507.14683', 'abstract': 'The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.', 'score': 70, 'issue_id': 4937, 'pub_date': '2025-07-19', 'pub_date_card': {'ru': '19 июля', 'en': 'July 19', 'zh': '7月19日'}, 'hash': '47799c3d5002f685', 'authors': ['Xingxuan Li', 'Yao Xiao', 'Dianwen Ng', 'Hai Ye', 'Yue Deng', 'Xiang Lin', 'Bin Wang', 'Zhanfeng Mo', 'Chong Zhang', 'Yueyi Zhang', 'Zonglin Yang', 'Ruilin Li', 'Lei Lei', 'Shihao Xu', 'Han Zhao', 'Weiling Chen', 'Feng Ji', 'Lidong Bing'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.14683.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#open_source', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Открытые модели для математических рассуждений на новом уровне', 'desc': 'MiroMind-M1 - это серия открытых языковых моделей для математических рассуждений, достигающих передовых результатов на соответствующих бенчмарках. Модели обучаются в два этапа: сначала на корпусе из 719 тысяч математических задач с верифицированными решениями, затем с помощью обучения с подкреплением на 62 тысячах сложных задач. Авторы представляют новый алгоритм Context-Aware Multi-Stage Policy Optimization для повышения эффективности обучения с подкреплением. Все ресурсы, включая модели, датасеты и конфигурации, открыто опубликованы для воспроизводимости результатов.'}, 'en': {'title': 'Open-Source Models for Superior Mathematical Reasoning', 'desc': "The MiroMind-M1 series introduces open-source reasoning language models that excel in mathematical reasoning tasks through a two-stage training approach. The first stage involves supervised fine-tuning (SFT) on a large dataset of math problems, while the second stage employs reinforcement learning with verified responses (RLVR) to refine the model's reasoning capabilities. To improve training efficiency, the authors propose a novel Context-Aware Multi-Stage Policy Optimization algorithm that adapts training based on context and problem complexity. By providing complete access to models, datasets, and training configurations, this work aims to enhance transparency and reproducibility in the development of reasoning language models."}, 'zh': {'title': '开源推理模型的透明性与先进性', 'desc': 'MiroMind-M1系列是一个开源推理语言模型，通过两阶段训练过程和上下文感知多阶段策略优化，在数学推理基准测试中取得了最先进的表现。这些模型首先在经过精心挑选的719K数学推理问题上进行监督微调，然后在62K具有挑战性的问题上进行强化学习验证。为了提高强化学习验证过程的鲁棒性和效率，提出了一种新的算法，结合了长度渐进训练和自适应重复惩罚。我们希望通过发布完整的模型、数据集和训练配置，促进研究的可重复性和社区的进步。'}}}, {'id': 'https://huggingface.co/papers/2507.15846', 'title': 'GUI-G^2: Gaussian Reward Modeling for GUI Grounding', 'url': 'https://huggingface.co/papers/2507.15846', 'abstract': 'Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.', 'score': 54, 'issue_id': 4936, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'd36bacfa3f66add9', 'authors': ['Fei Tang', 'Zhangxuan Gu', 'Zhengxi Lu', 'Xuyang Liu', 'Shuheng Shen', 'Changhua Meng', 'Wen Wang', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Ant Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15846.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🖱️', 'ru': {'title': 'Гауссово моделирование для точного взаимодействия с GUI', 'desc': 'Статья представляет новый подход к обучению моделей машинного обучения для взаимодействия с графическим пользовательским интерфейсом (GUI). Авторы предлагают метод GUI Gaussian Grounding Rewards (GUI-G^2), который моделирует элементы интерфейса как непрерывные гауссовы распределения на плоскости интерфейса. Этот метод включает в себя гауссовы точечные награды для точной локализации и награды за покрытие для оценки пространственного выравнивания. Эксперименты показывают, что GUI-G^2 значительно превосходит современные методы на нескольких бенчмарках, демонстрируя улучшение до 24.7% на ScreenSpot-Pro.'}, 'en': {'title': 'Revolutionizing GUI Interaction with Continuous Gaussian Rewards', 'desc': 'This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks.'}, 'zh': {'title': '高斯奖励框架提升GUI交互精度', 'desc': '本论文提出了一种新的奖励框架，称为GUI Gaussian Grounding Rewards（GUI-G^2），用于将自然语言指令映射到图形用户界面（GUI）的精确位置。与传统的二元奖励方法不同，GUI-G^2通过将GUI元素建模为连续的高斯分布，提供了更丰富的梯度信号，促进了模型的优化。该框架结合了高斯点奖励和覆盖奖励，能够更好地处理不同元素的尺度，并提高了模型在界面变化中的鲁棒性。实验结果表明，GUI-G^2在多个基准测试中显著优于现有的最先进方法，展示了其在GUI交互任务中的新范式。'}}}, {'id': 'https://huggingface.co/papers/2507.14843', 'title': 'The Invisible Leash: Why RLVR May Not Escape Its Origin', 'url': 'https://huggingface.co/papers/2507.14843', 'abstract': "Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions.", 'score': 38, 'issue_id': 4940, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'bb8fd850ce625ea5', 'authors': ['Fang Wu', 'Weihao Xuan', 'Ximing Lu', 'Zaid Harchaoui', 'Yejin Choi'], 'affiliations': ['RIKEN AIP', 'Stanford University', 'University of Tokyo', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.14843.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#optimization', '#rl'], 'emoji': '🔍', 'ru': {'title': 'RLVR: повышение точности ценой ограничения исследования', 'desc': 'Исследование анализирует метод обучения с подкреплением с проверяемыми вознаграждениями (RLVR) в контексте решения сложных логических задач. Авторы обнаружили, что RLVR повышает точность модели, но ограничивает ее способность находить новые решения. Теоретический анализ показывает, что RLVR ограничен возможностями базовой модели и действует как механизм консервативного перевзвешивания. Эмпирические эксперименты подтверждают, что RLVR улучшает показатель pass@1, но сужает область исследования, потенциально упуская правильные, но недопредставленные решения.'}, 'en': {'title': 'Balancing Precision and Exploration in RLVR', 'desc': "This paper investigates Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on AI's problem-solving abilities. It finds that while RLVR improves precision in generating high-reward outputs, it limits exploration, which can hinder the discovery of novel solutions. The study introduces a theoretical framework showing that RLVR acts as a conservative mechanism, unable to sample solutions with zero initial probability. Empirical results indicate that although RLVR enhances performance metrics like pass@1, it often reduces the diversity of solutions, suggesting a need for new strategies to encourage exploration."}, 'zh': {'title': '强化学习与可验证奖励的探索限制', 'desc': '强化学习与可验证奖励（RLVR）在提高精度方面表现出色，但却限制了探索能力，可能导致无法发现新颖的解决方案。研究表明，RLVR的效果受到基础模型的支持限制，无法采样初始概率为零的解决方案。虽然RLVR在提高精度方面表现稳定，但其对探索的压缩可能会忽视一些正确但代表性不足的解决方案。未来的算法创新可能需要引入显式探索机制或混合策略，以便在未被充分代表的解决方案区域中注入概率质量。'}}}, {'id': 'https://huggingface.co/papers/2507.15061', 'title': 'WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization', 'url': 'https://huggingface.co/papers/2507.15061', 'abstract': 'A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.', 'score': 26, 'issue_id': 4936, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': '16ab84cfe7ace89e', 'authors': ['Zhengwei Tao', 'Jialong Wu', 'Wenbiao Yin', 'Junkai Zhang', 'Baixuan Li', 'Haiyang Shen', 'Kuan Li', 'Liwen Zhang', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.15061.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#reasoning', '#benchmark'], 'emoji': '🕸️', 'ru': {'title': 'Формализация для синтеза данных: новый подход к обучению ИИ-агентов поиску информации', 'desc': 'WebShaper - это фреймворк для синтеза наборов данных для задач поиска информации, основанный на формализации с использованием теории множеств и Проекций Знаний. Он позволяет улучшить производительность агентов на основе больших языковых моделей (LLM) в открытых задачах. WebShaper систематически формализует задачи поиска информации и использует многоэтапный процесс расширения для создания сложных вопросов. Эксперименты показывают, что WebShaper достигает наилучших результатов среди открытых агентов поиска информации на бенчмарках GAIA и WebWalkerQA.'}, 'en': {'title': 'Enhancing LLM Agents with Structured Data Synthesis', 'desc': 'WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks.'}, 'zh': {'title': 'WebShaper：提升信息检索智能体性能的创新框架', 'desc': 'WebShaper是一个基于形式化驱动的框架，利用集合论和知识投影技术合成信息检索数据集，从而提升大型语言模型（LLM）驱动的智能体在开放式任务中的表现。该框架通过系统化的形式化过程，确保信息结构与推理结构的一致性，解决了现有方法中常见的数据不一致问题。WebShaper的核心是知识投影（KP）概念，通过KP操作组合实现对推理结构的精确控制。实验结果表明，WebShaper在GAIA和WebWalkerQA基准测试中，达到了开源信息检索智能体的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2507.11061', 'title': 'Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling', 'url': 'https://huggingface.co/papers/2507.11061', 'abstract': 'A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.', 'score': 24, 'issue_id': 4939, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '4c8104d951622fec', 'authors': ['Hayeon Kim', 'Ji Ha Jang', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'INMC & IPAI Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2507.11061.jpg', 'data': {'categories': ['#3d'], 'emoji': '✏️', 'ru': {'title': 'Точное локальное 3D-редактирование с помощью робастных масок и улучшенной регуляризации', 'desc': 'RoMaP - это новая система для точного локального 3D-редактирования, использующая генерацию робастных 3D-масок и улучшенную регуляризацию функции потерь SDS. Она включает модуль 3D-GALP для создания согласованных сегментаций частей объекта с разных ракурсов. RoMaP также применяет регуляризованную функцию потерь SDS с дополнительными регуляризаторами, включая L1-якорную потерю через метод SLaMP. Эксперименты показывают, что RoMaP достигает наилучших результатов в локальном 3D-редактировании как реконструированных, так и сгенерированных гауссовых сцен и объектов.'}, 'en': {'title': 'RoMaP: Revolutionizing Local 3D Editing with Precision and Flexibility', 'desc': 'The paper introduces RoMaP, a new framework designed to enhance local 3D editing by generating robust 3D masks and improving the Score Distillation Sampling (SDS) loss regularization. It addresses challenges in achieving precise edits in 3D content, particularly with Gaussian Splatting, by utilizing a 3D-Geometry Aware Label Prediction (3D-GALP) module for accurate part segmentations. The framework also incorporates a regularized SDS loss that includes an L1 anchor loss to ensure modifications are confined to specific areas while maintaining overall coherence. Experimental results show that RoMaP outperforms existing methods in local 3D editing, providing a more flexible and effective approach for part-level modifications.'}, 'zh': {'title': 'RoMaP：精确局部3D编辑的新框架', 'desc': 'RoMaP是一个新颖的局部3D编辑框架，旨在通过强大的3D掩模生成和增强的SDS损失正则化来提高精确的局部3D编辑能力。该框架引入了3D几何感知标签预测模块，利用球谐系数建模视角依赖的标签变化，从而实现准确一致的部分分割。通过结合标准SDS损失和额外的正则化项，RoMaP能够在目标区域内进行高质量的部分编辑，同时保持上下文的一致性。实验结果表明，RoMaP在重建和生成的高斯场景及物体上实现了最先进的局部3D编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2507.15852', 'title': 'SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction', 'url': 'https://huggingface.co/papers/2507.15852', 'abstract': 'Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.', 'score': 21, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '8d4e431fe003417f', 'authors': ['Zhixiong Zhang', 'Shuangrui Ding', 'Xiaoyi Dong', 'Songxin He', 'Jianfan Lin', 'Junsong Tang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.15852.jpg', 'data': {'categories': ['#cv', '#interpretability', '#benchmark', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Концептуальное понимание для улучшения сегментации объектов в видео', 'desc': 'Статья представляет новый подход к сегментации объектов в видео под названием Segment Concept (SeC). SeC использует большие мультимодальные модели для создания концептуальных представлений объектов, что позволяет лучше справляться со сложными сценариями. Авторы также представляют новый датасет SeCVOS для оценки методов сегментации в сложных семантических сценариях. SeC показывает значительное улучшение результатов по сравнению с существующими методами на этом датасете.'}, 'en': {'title': 'Revolutionizing Video Object Segmentation with Conceptual Understanding', 'desc': 'This paper introduces Segment Concept (SeC), a new framework for Video Object Segmentation (VOS) that enhances object tracking and segmentation in videos. Unlike traditional methods that rely heavily on appearance matching, SeC focuses on building high-level, object-centric representations using Large Vision-Language Models (LVLMs). This approach allows the model to better understand and adapt to complex visual changes and occlusions, leading to improved segmentation accuracy. The authors also present a new benchmark, SeCVOS, to evaluate VOS methods in challenging scenarios, where SeC demonstrates significant performance improvements over existing techniques.'}, 'zh': {'title': '概念驱动的视频目标分割新突破', 'desc': '视频目标分割（VOS）是计算机视觉中的一项核心任务，要求模型在视频帧中跟踪和分割目标物体。尽管近年来取得了一些进展，但现有技术在处理剧烈的视觉变化、遮挡和复杂场景变化时仍然落后于人类能力。为了解决这一问题，我们提出了Segment Concept（SeC），它通过构建和利用高层次的以对象为中心的表示，转变了传统的特征匹配方法。SeC结合了大型视觉语言模型（LVLMs），在推理过程中形成全面的语义表示，从而实现对后续帧的稳健分割。'}}}, {'id': 'https://huggingface.co/papers/2507.15493', 'title': 'GR-3 Technical Report', 'url': 'https://huggingface.co/papers/2507.15493', 'abstract': 'A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.', 'score': 20, 'issue_id': 4938, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '5e91567240893b65', 'authors': ['Chilam Cheang', 'Sijin Chen', 'Zhongren Cui', 'Yingdong Hu', 'Liqun Huang', 'Tao Kong', 'Hang Li', 'Yifeng Li', 'Yuxiao Liu', 'Xiao Ma', 'Hao Niu', 'Wenxuan Ou', 'Wanli Peng', 'Zeyu Ren', 'Haixin Shi', 'Jiawen Tian', 'Hongtao Wu', 'Xin Xiao', 'Yuyang Xiao', 'Jiafeng Xu', 'Yichu Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.15493.jpg', 'data': {'categories': ['#optimization', '#robotics', '#agents', '#training', '#agi'], 'emoji': '🤖', 'ru': {'title': 'GR-3: Шаг к универсальным роботам-помощникам', 'desc': 'Статья представляет GR-3 - крупномасштабную модель визуально-языкового действия (VLA) для робототехники. Модель демонстрирует исключительные способности к обобщению на новые объекты, среды и инструкции, включая абстрактные концепции. GR-3 может эффективно дообучаться на минимальном количестве человеческих траекторий, что позволяет быстро адаптироваться к новым условиям. Модель превосходит существующие базовые методы в широком спектре сложных задач, включая длительные и требующие ловкости операции.'}, 'en': {'title': 'GR-3: A Leap Towards Generalist Robots for Everyday Tasks', 'desc': "The paper presents GR-3, a large-scale vision-language-action model that excels in generalizing across various robotic tasks. It can adapt quickly to new environments and instructions with minimal human input, making it efficient for fine-tuning. GR-3 is particularly effective in performing complex tasks that require dexterity and coordination, such as bi-manual manipulation. The model's training combines web-scale data and imitation learning, leading to superior performance compared to existing methods."}, 'zh': {'title': 'GR-3：通用机器人政策的未来', 'desc': 'GR-3是一个大型的视觉-语言-动作模型，能够在复杂的机器人任务中表现出色。它具有很强的泛化能力，能够适应新物体、新环境和抽象概念的指令。该模型可以通过少量的人类轨迹数据进行高效的微调，快速适应新环境。通过与网络规模的视觉-语言数据共同训练，GR-3在长时间和灵巧任务中表现出强大的性能，展示了其在日常生活中辅助人类的潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.15778', 'title': 'Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR', 'url': 'https://huggingface.co/papers/2507.15778', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.', 'score': 15, 'issue_id': 4936, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '8e0f7bdfedf50691', 'authors': ['Jiakang Wang', 'Runze Liu', 'Fuzheng Zhang', 'Xiu Li', 'Guorui Zhou'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15778.jpg', 'data': {'categories': ['#rl', '#optimization', '#reasoning', '#training', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Точное обучение с подкреплением: улучшение рассуждений ИИ с помощью энтропийно-адаптивного подхода', 'desc': 'Статья представляет новый метод обучения с подкреплением для улучшения рассуждающих способностей больших языковых моделей, называемый Archer. Этот подход учитывает энтропию токенов и применяет различные ограничения к токенам знаний и рассуждений. Archer использует более слабую KL-регуляризацию и более высокие пороги отсечения для токенов рассуждений, чтобы стимулировать исследование, сохраняя при этом фактические знания. Экспериментальные результаты показывают, что Archer превосходит предыдущие методы RLVR на нескольких бенчмарках математических рассуждений и генерации кода.'}, 'en': {'title': 'Archer: Smart Token Training for Better Reasoning in LLMs', 'desc': 'This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size.'}, 'zh': {'title': '提升推理能力的双重令牌强化学习', 'desc': '本文提出了一种新的强化学习方法，称为Archer，旨在提高大型语言模型的推理能力。Archer通过双重令牌约束和同步更新，分别对知识相关的低熵令牌和推理相关的高熵令牌施加不同的训练信号。与以往的算法不同，Archer在推理令牌上使用较弱的KL正则化，以鼓励探索，同时对知识令牌施加更强的约束，以保持事实知识的准确性。实验结果表明，Archer在多个数学推理和代码生成基准测试中显著优于之前的RLVR方法。'}}}, {'id': 'https://huggingface.co/papers/2507.15597', 'title': 'Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos', 'url': 'https://huggingface.co/papers/2507.15597', 'abstract': 'Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.', 'score': 14, 'issue_id': 4942, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'c48aaae53a1f9330', 'authors': ['Hao Luo', 'Yicheng Feng', 'Wanpeng Zhang', 'Sipeng Zheng', 'Ye Wang', 'Haoqi Yuan', 'Jiazheng Liu', 'Chaoyi Xu', 'Qin Jin', 'Zongqing Lu'], 'affiliations': ['BeingBeyond', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.15597.jpg', 'data': {'categories': ['#agi', '#robotics', '#dataset', '#training', '#optimization', '#multimodal', '#data'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов человеческим движениям через видео', 'desc': 'Being-H0 - это модель зрения-языка-действия (VLA), обученная на видео с людьми для решения задач манипуляции. Модель использует физическое обучение по инструкциям и токенизацию движений на уровне частей тела для улучшения точности и обобщаемости. Being-H0 демонстрирует превосходные результаты в генерации движений рук и реальной робототехнической манипуляции. Модель обучается на масштабном наборе данных, включающем захват движений, VR и RGB-видео.'}, 'en': {'title': 'Empowering Robots with Human-Like Dexterity through Vision-Language-Action!', 'desc': 'Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.'}, 'zh': {'title': 'Being-H0：灵巧的视觉-语言-动作模型', 'desc': 'Being-H0 是一种视觉-语言-动作模型，专注于从人类视频中学习，以解决灵巧性和泛化能力的问题。该模型通过物理指令调优和部件级运动标记化，能够生成精确的手部动作并在真实世界中进行机器人操作。与传统模型相比，Being-H0 更好地处理复杂的操作任务，并能有效适应新场景。我们的研究表明，Being-H0 在手部动作生成和指令跟随方面表现出色，且在实际机器人操作中也取得了显著的进展。'}}}, {'id': 'https://huggingface.co/papers/2507.15629', 'title': 'Gaussian Splatting with Discretized SDF for Relightable Assets', 'url': 'https://huggingface.co/papers/2507.15629', 'abstract': '3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.', 'score': 13, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'f2fc3e4b855b88d5', 'authors': ['Zuo-Liang Zhu', 'Jian Yang', 'Beibei Wang'], 'affiliations': ['Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15629.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎨', 'ru': {'title': 'Дискретизированное SDF для улучшенного обратного рендеринга с гауссовским сплаттингом', 'desc': 'Статья представляет новый подход к обратному рендерингу с использованием дискретизированного поля расстояний со знаком (SDF) в контексте 3D гауссовского сплаттинга. Авторы кодируют SDF в каждом гауссиане с помощью дискретных значений, что позволяет связать SDF с прозрачностью гауссианов через специальное преобразование. Для регуляризации дискретных выборок вводится проекционная функция потерь, обеспечивающая согласованность с базовым SDF. Эксперименты показывают, что предложенный метод превосходит существующие подходы к обратному рендерингу на основе гауссианов по качеству перерисовки освещения.'}, 'en': {'title': 'Efficient Inverse Rendering with Discretized SDF and Gaussian Splatting', 'desc': 'This paper presents a novel approach to inverse rendering using a discretized signed distance field (SDF) integrated with 3D Gaussian splatting (3DGS). By encoding the continuous SDF within each Gaussian, the method links SDF with Gaussian opacity, allowing for efficient rendering without the heavy computational costs of ray marching. The authors introduce a projection-based consistency loss to ensure that the discrete samples align with the underlying SDF, improving the quality of relighting. Overall, this approach enhances the performance of Gaussian-based inverse rendering while maintaining low memory usage and simplifying the optimization process.'}, 'zh': {'title': '离散化SDF提升逆向渲染质量', 'desc': '3D高斯点云（3DGS）在新视图合成（NVS）任务中表现出色，但在逆向渲染中仍面临挑战。我们提出了一种离散化的有符号距离场（SDF），通过在每个高斯中编码采样值来表示连续的SDF，从而简化了几何约束的应用。该方法通过SDF与高斯不透明度的转换，避免了光线行进的计算成本，同时提高了重光照质量。实验结果表明，我们的方法在性能上优于现有的基于高斯的逆向渲染方法。'}}}, {'id': 'https://huggingface.co/papers/2507.15028', 'title': 'Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding', 'url': 'https://huggingface.co/papers/2507.15028', 'abstract': 'Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.', 'score': 12, 'issue_id': 4938, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': '7f71d09a9b276de8', 'authors': ['Yuanhan Zhang', 'Yunice Chew', 'Yuhao Dong', 'Aria Leo', 'Bo Hu', 'Ziwei Liu'], 'affiliations': ['Independent Researcher', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15028.jpg', 'data': {'categories': ['#interpretability', '#security', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый рубеж в оценке видео-LLM: человекоподобное понимание реального мира', 'desc': 'Статья представляет новый тест Video Thinking Test (Video-TT) для оценки способности видео-LLM интерпретировать реальные видео на уровне человека. Video-TT состоит из 1000 коротких YouTube-видео с открытыми и провокационными вопросами, оценивающими понимание сложных визуальных нарративов. Тест выявляет существенный разрыв между производительностью видео-LLM и человеческим интеллектом в корректности и устойчивости интерпретации видео. Video-TT направлен на оценку способности моделей точно и устойчиво интерпретировать визуальный контент в сложных условиях.'}, 'en': {'title': 'Bridging the Gap: Evaluating Video LLMs with the Video Thinking Test', 'desc': "This paper discusses the importance of correctness and robustness in video understanding, which are essential for mimicking human intelligence. It highlights that current benchmarks do not adequately measure how well video large language models (LLMs) interpret videos compared to humans. To address this, the authors introduce the Video Thinking Test (Video-TT), designed to evaluate the performance of video LLMs on real-world videos. The test includes 1,000 YouTube Shorts videos with questions that challenge the models' understanding of complex visual narratives, revealing a significant performance gap between the models and human interpreters."}, 'zh': {'title': '视频理解的挑战：人类与模型的差距', 'desc': '本论文探讨了视频理解中的正确性和鲁棒性问题。尽管视频大型语言模型（视频LLMs）取得了一定进展，但现有基准测试未能充分反映这些模型与人类智能在视频解释中的差距。我们提出了视频思维测试（Video-TT），旨在评估视频LLMs是否能像人类一样有效地理解现实世界的视频。测试包含1000个YouTube Shorts视频，每个视频配有一个开放性问题和四个针对视觉和叙事复杂性的对抗性问题，评估结果显示视频LLMs与人类表现之间存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2507.14417', 'title': 'Inverse Scaling in Test-Time Compute', 'url': 'https://huggingface.co/papers/2507.14417', 'abstract': 'Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.  \t\t\t\t\tAI-generated summary \t\t\t\t We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.', 'score': 10, 'issue_id': 4945, 'pub_date': '2025-07-19', 'pub_date_card': {'ru': '19 июля', 'en': 'July 19', 'zh': '7月19日'}, 'hash': 'e6c3904a07b73089', 'authors': ['Aryo Pradipta Gema', 'Alexander Hägele', 'Runjin Chen', 'Andy Arditi', 'Jacob Goldman-Wetzler', 'Kit Fraser-Taliente', 'Henry Sleight', 'Linda Petrini', 'Julian Michael', 'Beatrice Alex', 'Pasquale Minervini', 'Yanda Chen', 'Joe Benton', 'Ethan Perez'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Anthropic Fellows Program, EPFL', 'Anthropic Fellows Program, University of Edinburgh', 'Anthropic Fellows Program, University of Texas at Austin', 'Constellation', 'Independent', 'Scale AI', 'University of Edinburgh', 'University of Edinburgh, Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.14417.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Больше вычислений - не всегда лучше: парадокс масштабирования в моделях рассуждений', 'desc': 'Исследование показывает, что увеличение вычислительной мощности при тестировании крупных моделей рассуждений (LRM) может ухудшить их производительность. Выявлено пять режимов отказа, включая отвлечение на нерелевантную информацию и переобучение на особенностях формулировки задачи. Модели демонстрируют трудности в поддержании фокуса на сложных дедуктивных задачах и могут усиливать проблемные шаблоны рассуждений. Результаты подчеркивают важность оценки моделей на различных длинах рассуждений для выявления и устранения этих проблем.'}, 'en': {'title': 'Longer Reasoning, Lower Accuracy: The Inverse Scaling Dilemma', 'desc': "This paper investigates how increasing the reasoning length of Large Reasoning Models (LRMs) can lead to worse performance, highlighting an inverse relationship between the amount of compute used during testing and the models' accuracy. The authors create evaluation tasks that reveal five specific failure modes, such as models becoming distracted by irrelevant information or overfitting to specific problem framings. They also note that longer reasoning can exacerbate issues like reliance on spurious correlations and difficulties in complex deductive reasoning. Overall, the study emphasizes the need for careful evaluation of LRMs across varying reasoning lengths to uncover and mitigate these performance issues."}, 'zh': {'title': '推理长度与模型性能的反向关系', 'desc': '本研究探讨了大型推理模型在不同推理长度下的表现，发现增加测试时计算量可能会降低性能并加剧特定的推理失败。我们设计了四类评估任务，结果显示推理长度的延长与准确率之间存在反向缩放关系。研究中识别了五种不同的失败模式，包括模型对无关信息的干扰和对问题框架的过拟合等。我们的发现强调了在多样化推理长度下评估模型的重要性，以识别和解决大型推理模型中的这些失败模式。'}}}, {'id': 'https://huggingface.co/papers/2507.14119', 'title': 'NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining', 'url': 'https://huggingface.co/papers/2507.14119', 'abstract': 'An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.', 'score': 10, 'issue_id': 4944, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '4c0e1974ad169d32', 'authors': ['Maksim Kuprashevich', 'Grigorii Alekseenko', 'Irina Tolstykh', 'Georgii Fedorov', 'Bulat Suleimanov', 'Vladimir Dokholyan', 'Aleksandr Gordeev'], 'affiliations': ['Layer Team, SALUTEDEV'], 'pdf_title_img': 'assets/pdf/title_img/2507.14119.jpg', 'data': {'categories': ['#dataset', '#cv', '#diffusion', '#open_source', '#training', '#data'], 'emoji': '🖼️', 'ru': {'title': 'Автоматизированное создание данных для ИИ-редактирования изображений', 'desc': 'Статья представляет автоматизированный конвейер для создания высококачественных триплетов для обучения моделей редактирования изображений. Система использует генеративные модели и настроенный валидатор Gemini для оценки соответствия инструкциям и эстетики без участия человека. Предложенный подход позволяет создавать обучающие данные в большом масштабе без ручной разметки. Авторы выпускают открытый набор данных NHR-Edit из 358 тысяч триплетов и модель Bagel-NHR-Edit, достигающую современного уровня производительности.'}, 'en': {'title': 'Automating Image Editing: High-Fidelity Triplet Generation Without Human Input', 'desc': 'This paper presents an automated pipeline that generates high-quality image editing triplets using generative models, which consist of an original image, an instruction, and the edited image. The system employs a task-tuned validator to ensure that the edits adhere to the specified instructions while maintaining visual appeal and coherence. By automating the mining process, the pipeline significantly increases the volume of training data available for image editing models without requiring human labeling. The authors also introduce the NHR-Edit dataset, containing 358,000 high-quality triplets, and a fine-tuned model that achieves state-of-the-art performance in image editing tasks.'}, 'zh': {'title': '自动化挖掘高保真图像编辑三元组的创新方法', 'desc': '这篇论文介绍了一种自动化的管道，用于挖掘高保真图像编辑三元组，利用生成模型和任务调优的验证器，实现大规模训练而无需人工标注。该系统能够在不同领域、分辨率和风格中自动生成原始图像、指令和编辑图像的三元组，解决了传统方法中对编辑质量评估的不足。通过使用任务调优的验证器，系统直接评分指令遵循性和美学，省去了分割或基础模型的需求。论文还发布了NHR-Edit数据集，包含358k个高质量三元组，推动了这一资源密集型领域的研究。'}}}, {'id': 'https://huggingface.co/papers/2507.15375', 'title': 'STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models', 'url': 'https://huggingface.co/papers/2507.15375', 'abstract': 'Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.', 'score': 7, 'issue_id': 4937, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '2a7d1e1e1882f002', 'authors': ['Cheng-Han Chiang', 'Xiaofei Wang', 'Linjie Li', 'Chung-Ching Lin', 'Kevin Lin', 'Shujie Liu', 'Zhendong Wang', 'Zhengyuan Yang', 'Hung-yi Lee', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15375.jpg', 'data': {'categories': ['#training', '#reasoning', '#audio', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Stitch: Думай и говори одновременно', 'desc': 'Статья представляет новый метод генерации речи для разговорных языковых моделей под названием Stitch. Этот метод позволяет моделям осуществлять внутренний процесс мышления, чередуя генерацию невысказанных рассуждений и произносимых ответов. Stitch использует свободное время во время воспроизведения аудио для генерации следующего фрагмента невысказанных рассуждений, что позволяет модели одновременно думать и говорить. Результаты показывают, что Stitch превосходит базовые модели на 15% в задачах математических рассуждений, сохраняя при этом такую же задержку и производительность на других наборах данных.'}, 'en': {'title': 'Stitch: Simultaneous Thinking and Talking for Enhanced Spoken Language Models', 'desc': 'This paper introduces Stitch, a new method for Spoken Language Models (SLMs) that allows them to think internally while responding to speech. Unlike traditional SLMs that generate responses without prior reasoning, Stitch alternates between generating unspoken reasoning chunks and spoken responses. This approach minimizes latency by utilizing the time taken to play audio responses to continue generating reasoning. As a result, Stitch not only matches the response time of existing models but also improves performance on math reasoning tasks by 15%.'}, 'zh': {'title': '同步思考与表达的口语模型', 'desc': '本论文提出了一种新的口语语言模型生成方法，名为Stitch。该方法通过交替生成无声推理片段和口语响应片段，解决了传统模型在回应前缺乏内在思考过程的问题。Stitch利用口语响应的音频持续时间，充分利用剩余时间生成推理内容，从而实现思考与表达的同步进行。实验结果表明，Stitch在数学推理数据集上比基线模型提高了15%的性能，同时在非推理数据集上表现也与基线模型相当。'}}}, {'id': 'https://huggingface.co/papers/2507.11539', 'title': 'Streaming 4D Visual Geometry Transformer', 'url': 'https://huggingface.co/papers/2507.11539', 'abstract': 'A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.', 'score': 6, 'issue_id': 4937, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '03e472d31e5edcaf', 'authors': ['Dong Zhuo', 'Wenzhao Zheng', 'Jiahe Guo', 'Yuqi Wu', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.11539.jpg', 'data': {'categories': ['#inference', '#long_context', '#optimization', '#benchmark', '#architecture', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Реконструкция 4D-геометрии в реальном времени с помощью потокового трансформера', 'desc': 'Статья представляет потоковый 4D-трансформер визуальной геометрии для реконструкции пространственно-временной геометрии из видео в реальном времени. Модель использует каузальную архитектуру трансформера и временное каузальное внимание для обработки входной последовательности в онлайн-режиме. Для эффективного обучения применяется дистилляция знаний от более плотной двунаправленной модели. Эксперименты показывают, что предложенный подход увеличивает скорость вывода, сохраняя конкурентоспособную производительность.'}, 'en': {'title': 'Real-Time 4D Reconstruction with Streaming Transformers', 'desc': 'This paper presents a streaming 4D visual geometry transformer that utilizes causal attention and knowledge distillation for real-time 4D reconstruction from video data. The model processes input sequences in an online manner, leveraging a causal transformer architecture to maintain high spatial consistency while integrating historical information. By employing temporal causal attention and caching past data, the system achieves efficient long-term reconstruction. The approach is validated through extensive experiments, showing improved inference speed and competitive performance, making it suitable for interactive 4D vision applications.'}, 'zh': {'title': '实时4D重建的创新变换器', 'desc': '本文提出了一种流式4D视觉几何变换器，利用因果注意力和知识蒸馏技术，实现实时的4D重建。该模型采用因果变换器架构，能够在线处理输入序列，并通过缓存历史信息来提高重建效率。通过从密集双向视觉几何变换器中蒸馏知识，模型在训练过程中得以优化。实验结果表明，该模型在保持高空间一致性的同时，显著提高了在线推理速度，适用于可扩展的交互式4D视觉系统。'}}}, {'id': 'https://huggingface.co/papers/2507.15856', 'title': 'Latent Denoising Makes Good Visual Tokenizers', 'url': 'https://huggingface.co/papers/2507.15856', 'abstract': 'Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.', 'score': 5, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '60a696cb47720198', 'authors': ['Jiawei Yang', 'Tianhong Li', 'Lijie Fan', 'Yonglong Tian', 'Yue Wang'], 'affiliations': ['Google DeepMind', 'MIT CSAIL', 'OpenAI', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2507.15856.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧹', 'ru': {'title': 'Шумоподавление как ключ к эффективным визуальным токенизаторам', 'desc': 'Статья представляет новый подход к разработке визуальных токенизаторов для генеративных моделей. Авторы предлагают метод Latent Denoising Tokenizer (l-DeTok), который обучается восстанавливать чистые изображения из зашумленных латентных представлений. Эксперименты на ImageNet показывают превосходство l-DeTok над стандартными токенизаторами для шести различных генеративных моделей. Исследование подчеркивает важность принципа шумоподавления в разработке токенизаторов для генеративного моделирования.'}, 'en': {'title': 'Enhancing Generative Models with Denoising Tokenizers', 'desc': 'This paper explores how visual tokenizers can be improved for generative modeling by focusing on a process called denoising. The authors propose a new tokenizer, the Latent Denoising Tokenizer (l-DeTok), which aligns its embeddings with the goal of reconstructing clean images from corrupted inputs. By training this tokenizer to handle noise and masking, it becomes more effective at generating high-quality outputs. The results show that l-DeTok outperforms traditional tokenizers in various generative models, suggesting that denoising should be a key consideration in future tokenizer designs.'}, 'zh': {'title': '去噪：分词器设计的新原则', 'desc': '本论文探讨了视觉分词器在生成建模中的有效性，提出了对分词器嵌入与去噪目标进行对齐的概念。我们引入了潜在去噪分词器（l-DeTok），该分词器旨在从受到干扰的潜在嵌入中重建干净图像。实验结果表明，l-DeTok在多个生成模型上优于传统分词器，验证了去噪作为分词器设计的重要原则。我们希望这一发现能够为未来的分词器设计提供新的视角。'}}}, {'id': 'https://huggingface.co/papers/2507.15815', 'title': 'LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra', 'url': 'https://huggingface.co/papers/2507.15815', 'abstract': 'We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.', 'score': 4, 'issue_id': 4937, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'ad03ed3ae6e4256b', 'authors': ['Seth Karten', 'Wenzhe Li', 'Zihan Ding', 'Samuel Kleiner', 'Yu Bai', 'Chi Jin'], 'affiliations': ['Princeton University', 'Salesforce Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.15815.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#multimodal', '#rl', '#science'], 'emoji': '🤖', 'ru': {'title': 'Искусственный интеллект как экономист: моделирование и оптимизация экономической политики', 'desc': "Статья представляет новую концепцию под названием 'LLM Economist', которая использует агентное моделирование для разработки и оценки экономической политики в стратегических средах с иерархическим принятием решений. На нижнем уровне ограниченно рациональные агенты-работники выбирают предложение труда для максимизации текстовых функций полезности, изученных в контексте. На верхнем уровне агент-планировщик использует обучение с подкреплением для предложения кусочно-линейных графиков предельных налоговых ставок. Эксперименты показывают, что планировщик сходится к равновесиям, улучшающим совокупное общественное благосостояние по сравнению с решениями Саеза."}, 'en': {'title': 'Harnessing AI for Smarter Economic Policy Design', 'desc': 'The LLM Economist is a new framework that combines agent-based modeling with large language models to evaluate economic policies in complex decision-making environments. It features two levels of agents: lower-level worker agents that optimize their labor supply based on learned utility functions, and an upper-level planner agent that uses reinforcement learning to create tax schedules. This approach allows for realistic simulations of diverse populations and effective mechanism design, all expressed in natural language. The framework shows promising results in improving social welfare through strategic interactions among agents, making it a valuable tool for testing economic policies.'}, 'zh': {'title': '利用大语言模型优化经济政策', 'desc': '本文介绍了一种名为LLM Economist的新框架，利用基于代理的建模来设计和评估具有层级决策的经济政策。在低层次，有限理性的工人代理根据美国人口普查的收入和人口统计数据选择劳动供给，以最大化基于文本的效用函数。在高层次，规划者代理使用上下文强化学习提出与当前美国联邦税率相结合的分段线性边际税率。这种构建使经济模拟具备了优化异质效用、生成大规模人口和机制设计等三种能力，能够在自然语言中进行有效的财政实验。'}}}, {'id': 'https://huggingface.co/papers/2507.15640', 'title': 'Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training', 'url': 'https://huggingface.co/papers/2507.15640', 'abstract': "Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.", 'score': 2, 'issue_id': 4942, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '2ce6b05c03e1226b', 'authors': ['Kailai Yang', 'Xiao Liu', 'Lei Ji', 'Hao Li', 'Yeyun Gong', 'Peng Cheng', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2507.15640.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': '🔀', 'ru': {'title': 'Умное смешивание данных для адаптивного обучения языковых моделей', 'desc': 'Data Mixing Agent - это фреймворк на основе обучения с подкреплением для переобучения больших языковых моделей. Он эффективно перевзвешивает обучающие данные для сбалансансировки производительности между исходными и целевыми областями. Этот подход превосходит сильные базовые модели в достижении сбалансированной производительности в задачах математических рассуждений. Data Mixing Agent хорошо обобщается на новые исходные области, целевые модели и пространства доменов без переобучения.'}, 'en': {'title': 'Reinforcement Learning for Balanced Data Mixing in Language Models', 'desc': 'The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.'}, 'zh': {'title': '数据混合代理：平衡源与目标领域的智能学习', 'desc': '数据混合代理是一种基于模型的框架，利用强化学习有效地重新加权训练数据，以平衡在持续预训练中源领域和目标领域的性能。该方法解决了在小规模特定任务数据上持续预训练时可能出现的灾难性遗忘问题。通过提出数据混合代理，研究者证明了更通用的启发式方法可以被参数化，从而实现端到端的学习。实验结果表明，该代理在数学推理的持续预训练中表现优于强基线，并且在未见过的源领域和目标模型上具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.13428', 'title': '"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models', 'url': 'https://huggingface.co/papers/2507.13428', 'abstract': 'Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.', 'score': 2, 'issue_id': 4943, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '0bcb2373e179ecb8', 'authors': ['Jing Gu', 'Xian Liu', 'Yu Zeng', 'Ashwin Nagarajan', 'Fangrui Zhu', 'Daniel Hong', 'Yue Fan', 'Qianqi Yan', 'Kaiwen Zhou', 'Ming-Yu Liu', 'Xin Eric Wang'], 'affiliations': ['NVIDIA Research', 'Northeastern University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2507.13428.jpg', 'data': {'categories': ['#games', '#interpretability', '#optimization', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Физика в виртуальном мире: новый бенчмарк для оценки реалистичности видеогенерации', 'desc': "Статья представляет PhyWorldBench - комплексный бенчмарк для оценки моделей генерации видео на основе их соответствия законам физики. Бенчмарк охватывает различные уровни физических явлений, от базовых принципов до сложных сценариев, включая категорию 'Анти-физика'. Авторы оценили 12 современных моделей text-to-video на 1050 специально подобранных промптах. В результате были выявлены ключевые проблемы, с которыми сталкиваются модели при соблюдении реальной физики, и даны рекомендации по составлению промптов для повышения физической достоверности."}, 'en': {'title': 'Evaluating Video Generation with Physics: PhyWorldBench', 'desc': "This paper introduces PhyWorldBench, a benchmark for evaluating video generation models based on their ability to simulate physical laws accurately. It assesses models across various physical phenomena, from basic principles like motion and energy conservation to complex interactions involving living beings. A unique 'Anti-Physics' category is included to test models' responses to prompts that contradict real-world physics, ensuring logical consistency in their outputs. The study evaluates 12 leading text-to-video models, revealing significant challenges in maintaining physical realism and providing insights for improving prompt design to enhance adherence to physical principles."}, 'zh': {'title': '评估视频生成模型的物理真实性', 'desc': '视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。本文提出了PhyWorldBench，这是一个全面的基准，用于评估视频生成模型在遵循物理法则方面的表现。我们评估了12个最先进的文本到视频生成模型，并识别出这些模型在遵循现实物理方面面临的主要挑战。'}}}, {'id': 'https://huggingface.co/papers/2507.14295', 'title': 'A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.14295', 'abstract': 'Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let\'s try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback', 'score': 1, 'issue_id': 4945, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '6b7877061c71a067', 'authors': ['Licheng Liu', 'Zihan Wang', 'Linjie Li', 'Chenwei Xu', 'Yiping Lu', 'Han Liu', 'Avirup Sil', 'Manling Li'], 'affiliations': ['IBM Research AI', 'Imperial College London', 'Northwestern University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.14295.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение многоэтапных рассуждений ИИ с помощью минимальной обратной связи', 'desc': 'Статья посвящена улучшению способности больших языковых моделей (LLM) решать задачи в многоэтапном режиме с учетом обратной связи. Авторы предлагают метод обучения с подкреплением (RL) под названием Unary Feedback as Observation (UFO), использующий минимальную унарную обратную связь от пользователя. Эксперименты показывают, что UFO улучшает точность многоэтапных рассуждений до 14% при сохранении производительности в одноэтапном режиме. Также разработаны структуры вознаграждений, поощряющие модели давать тщательные и продуманные ответы на каждом этапе.'}, 'en': {'title': 'Empowering LRMs with Unary Feedback for Better Multi-Turn Reasoning', 'desc': 'This paper addresses the challenges faced by Large Reasoning Models (LRMs) in multi-turn problem solving, particularly their ability to reflect on and revise their answers based on feedback. The authors highlight that traditional Reinforcement Learning (RL) methods often lead to models that struggle with multi-turn interactions and produce repetitive responses. They propose a novel approach called Unary Feedback as Observation (UFO), which utilizes simple unary feedback to enhance both single-turn and multi-turn reasoning capabilities. Experimental results demonstrate that this method improves multi-turn reasoning accuracy significantly while maintaining performance in single-turn tasks.'}, 'zh': {'title': '多轮推理，单元反馈助力', 'desc': '多轮问题解决对大型推理模型（LRMs）至关重要，但也很具挑战性。现有的强化学习（RL）方法通常在单轮范式下训练模型，导致模型在多轮上下文中难以反思和修正答案。我们提出了一种新的方法，称为单元反馈作为观察（UFO），通过使用简单的反馈来提高模型的多轮推理能力。实验结果表明，使用UFO的强化学习训练可以保持单轮性能，并将多轮推理的准确性提高多达14%。'}}}, {'id': 'https://huggingface.co/papers/2507.10935', 'title': 'GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization', 'url': 'https://huggingface.co/papers/2507.10935', 'abstract': "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.", 'score': 1, 'issue_id': 4942, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '8d8109c5462763ac', 'authors': ['Shaowen Tong', 'Zimin Xia', 'Alexandre Alahi', 'Xuming He', 'Yujiao Shi'], 'affiliations': ['Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland', 'ShanghaiTech University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.10935.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#cv'], 'emoji': '🌍', 'ru': {'title': 'GeoDistill: геометрия на службе кросс-видовой локализации', 'desc': 'GeoDistill - это новый подход к кросс-видовой локализации, использующий слабо контролируемое самообучение на основе геометрии. Метод применяет обучение по схеме учитель-ученик с маскированием по полю зрения для улучшения извлечения локальных признаков. Учитель локализует панорамное изображение, а ученик предсказывает положение по ограниченному полю зрения. Такой подход позволяет модели фокусироваться на ключевых особенностях и игнорировать бестекстурные области, повышая точность локализации.'}, 'en': {'title': 'GeoDistill: Enhancing Cross-View Localization with Weak Supervision', 'desc': "This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation."}, 'zh': {'title': 'GeoDistill：高效的跨视角定位解决方案', 'desc': '跨视角定位是通过将地面图像与卫星图像对齐来估计相机的三自由度姿态，这在自动导航和增强现实等大规模户外应用中至关重要。现有方法通常依赖于完全监督学习，这需要昂贵的真实姿态标注。我们提出了GeoDistill，一个几何引导的弱监督自蒸馏框架，利用教师-学生学习和基于视场(FoV)的掩蔽来增强局部特征学习，从而实现稳健的跨视角定位。GeoDistill通过对齐学生模型和教师模型的预测，帮助学生模型专注于关键特征，提高了定位精度并减少了不确定性。'}}}, {'id': 'https://huggingface.co/papers/2507.14102', 'title': 'UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography', 'url': 'https://huggingface.co/papers/2507.14102', 'abstract': 'Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL', 'score': 0, 'issue_id': 4945, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': 'd4150eeda5b606d4', 'authors': ['Shravan Venkatraman', 'Pavan Kumar S', 'Rakesh Raj Madavan', 'Chandrakala S'], 'affiliations': ['Shiv Nadar University, Chennai, India', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2507.14102.jpg', 'data': {'categories': ['#healthcare', '#training', '#data'], 'emoji': '🔬', 'ru': {'title': 'UGPL: Умный анализ КТ от общего к частному', 'desc': 'UGPL - это новый метод анализа КТ-изображений, использующий принцип прогрессивного обучения от общего к частному. Система сначала выявляет области диагностической неопределенности, а затем детально анализирует эти критические участки. UGPL применяет эвиденциальное глубокое обучение для количественной оценки неопределенности прогнозов и извлечения информативных фрагментов изображения. Эксперименты показали, что UGPL превосходит современные методы в точности обнаружения аномалий почек, рака легких и COVID-19 на КТ-снимках.'}, 'en': {'title': 'Enhancing CT Image Classification with Uncertainty-Guided Learning', 'desc': 'This paper presents UGPL, a novel framework for improving the classification of CT images by focusing on areas of diagnostic uncertainty. Unlike traditional methods that analyze images uniformly, UGPL first identifies ambiguous regions and then conducts a detailed examination of these areas. It utilizes evidential deep learning to measure predictive uncertainty, which helps in selecting informative patches while preserving spatial diversity. The results show that UGPL significantly enhances accuracy in detecting kidney abnormalities, lung cancer, and COVID-19 compared to existing techniques.'}, 'zh': {'title': '不确定性引导的渐进学习，提升CT图像分类精度', 'desc': '本文提出了一种名为UGPL的不确定性引导渐进学习框架，用于提高计算机断层扫描（CT）图像的分类准确性。UGPL通过首先识别诊断模糊区域，然后对这些关键区域进行详细分析，实现了从全局到局部的分析。该方法利用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取信息丰富的图像块，保持空间多样性。实验结果表明，UGPL在肾脏异常、肺癌和COVID-19检测中均显著优于现有方法，准确率分别提高了3.29%、2.46%和8.08%。'}}}, {'id': 'https://huggingface.co/papers/2507.02592', 'title': 'WebSailor: Navigating Super-human Reasoning for Web Agent', 'url': 'https://huggingface.co/papers/2507.02592', 'abstract': "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.", 'score': 74, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '0a8cc61c0251e5da', 'authors': ['Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Liwen Zhang', 'Litu Ou', 'Jialong Wu', 'Wenbiao Yin', 'Baixuan Li', 'Zhengwei Tao', 'Xinyu Wang', 'Weizhou Shen', 'Junkai Zhang', 'Dingchu Zhang', 'Xixi Wu', 'Yong Jiang', 'Ming Yan', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.02592.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#rl'], 'emoji': '🧭', 'ru': {'title': 'WebSailor: навигация в океане неопределенности для ИИ', 'desc': 'Статья представляет новый метод обучения языковых моделей под названием WebSailor. Он направлен на преодоление когнитивных ограничений человека в задачах поиска сложной информации. WebSailor использует генерацию задач с высокой неопределенностью и обучение с подкреплением для развития способности моделей систематически снижать неопределенность. Результаты показывают, что WebSailor позволяет открытым моделям достичь производительности проприетарных систем в сложных информационных задачах.'}, 'en': {'title': 'Empowering Open-Source Agents to Compete with the Best', 'desc': 'This paper discusses advancements in training large language models (LLMs) to surpass human cognitive limitations. It highlights the success of proprietary systems like DeepResearch, which excel in complex information-seeking tasks due to their unique reasoning abilities. The authors introduce WebSailor, a post-training methodology that enhances LLMs by generating high-uncertainty tasks and employing a novel reinforcement learning algorithm called Duplicating Sampling Policy Optimization (DUPO). The results show that WebSailor significantly improves the performance of open-source agents, enabling them to compete with proprietary models in challenging information retrieval scenarios.'}, 'zh': {'title': '超越认知局限，提升信息检索能力', 'desc': '本论文探讨了超越人类认知局限性在大型语言模型（LLM）训练中的重要性。我们提出的WebSailor方法通过系统性地减少在广阔信息环境中导航时的极端不确定性，来提升模型的推理能力。该方法结合了结构化采样和信息模糊化等技术，生成新的高不确定性任务，并采用高效的强化学习算法进行训练。实验结果表明，WebSailor在复杂的信息检索任务中显著优于所有开源代理，缩小了与专有代理的能力差距。'}}}, {'id': 'https://huggingface.co/papers/2507.02813', 'title': 'LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion', 'url': 'https://huggingface.co/papers/2507.02813', 'abstract': 'Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.', 'score': 49, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '726c080e7ea88c4b', 'authors': ['Fangfu Liu', 'Hao Li', 'Jiawei Chi', 'Hanyang Wang', 'Minghui Yang', 'Fudong Wang', 'Yueqi Duan'], 'affiliations': ['Ant Group', 'NTU', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.02813.jpg', 'data': {'categories': ['#open_source', '#3d', '#games', '#multimodal', '#diffusion'], 'emoji': '🏙️', 'ru': {'title': 'Генерация 3D-сцен с языковым пониманием по нескольким изображениям', 'desc': 'LangScene-X - это новая генеративная система для восстановления и понимания 3D-сцен с открытым словарем на основе 2D-изображений. Она использует видео-диффузионную модель TriMap для генерации согласованной мультимодальной информации (RGB, нормали, сегментация) из ограниченного числа ракурсов. Система включает языковой квантованный компрессор (LQC) для эффективного кодирования языковых эмбеддингов, что позволяет обобщать на новые сцены без переобучения. LangScene-X превосходит современные методы по качеству и обобщаемости при реконструкции 3D-сцен с языковой привязкой.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Language-Embedded Insights', 'desc': 'This paper presents LangScene-X, a new framework for creating 3D structures from 2D images using language information. It addresses the limitations of previous methods that relied on dense-view reconstructions, which often resulted in poor quality when only limited views were available. LangScene-X utilizes a TriMap video diffusion model to generate consistent visual and semantic data from sparse inputs, enhancing the reconstruction process. Additionally, it introduces a Language Quantized Compressor to efficiently encode language embeddings, allowing for better generalization across different scenes without needing to retrain for each one.'}, 'zh': {'title': 'LangScene-X：从稀疏视图生成一致的3D场景', 'desc': '本文提出了一种新颖的生成框架LangScene-X，用于从稀疏视图中恢复3D结构并进行场景理解。该框架结合了语言信息和多模态数据，能够生成一致的3D场景。我们首先训练了一个TriMap视频扩散模型，从稀疏输入中生成外观、几何和语义信息。通过引入语言量化压缩器（LQC），我们实现了跨场景的泛化，避免了逐场景的重新训练。'}}}, {'id': 'https://huggingface.co/papers/2506.23918', 'title': 'Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers', 'url': 'https://huggingface.co/papers/2506.23918', 'abstract': 'Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.', 'score': 49, 'issue_id': 4640, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '8526b6b1e8d4b31e', 'authors': ['Zhaochen Su', 'Peng Xia', 'Hangyu Guo', 'Zhenhua Liu', 'Yan Ma', 'Xiaoye Qu', 'Jiaqi Liu', 'Yanshu Li', 'Kaide Zeng', 'Zhengyuan Yang', 'Linjie Li', 'Yu Cheng', 'Heng Ji', 'Junxian He', 'Yi R. Fung'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'UIUC', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.23918.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning', '#alignment', '#survey'], 'emoji': '🧠', 'ru': {'title': 'От мышления об изображениях к мышлению изображениями: новая эра мультимодального ИИ', 'desc': 'Эта статья описывает новую парадигму в мультимодальном искусственном интеллекте, где визуальная информация используется как динамический инструмент мышления, а не просто статичный входной контекст. Авторы выделяют три ключевых этапа развития этой парадигмы: от внешнего исследования инструментов через программную манипуляцию к внутреннему воображению. В работе представлен всесторонний обзор методов, характерных для каждого этапа, а также анализ критически важных эталонных тестов и трансформационных приложений. Статья также определяет значительные проблемы и намечает перспективные направления будущих исследований в области мультимодального ИИ.'}, 'en': {'title': 'Transforming Vision into Dynamic Thought in AI', 'desc': "This paper discusses the advancements in multimodal reasoning, particularly focusing on the 'think with images' paradigm. It highlights the limitations of traditional text-based reasoning, which treats visual data as static, leading to a disconnect between perception and thought. The authors propose a three-stage framework that evolves from using images as tools to integrating them into cognitive processes, allowing for dynamic manipulation of visual information. The survey aims to provide a structured overview of this emerging field, outlining foundational principles, core methods, evaluation benchmarks, and future research directions."}, 'zh': {'title': '从图像思考到思考图像的转变', 'desc': '这篇论文探讨了多模态推理的最新进展，特别是文本链式思维（CoT）在语言推理中的应用。作者指出，传统的文本中心方法将视觉视为静态背景，导致感知数据与符号思维之间存在“语义差距”。论文提出了一种新的思维模式，强调视觉信息在思维过程中的动态作用，使其成为可操作的认知工作空间。通过建立三阶段框架，论文为未来的多模态人工智能研究提供了清晰的路线图。'}}}, {'id': 'https://huggingface.co/papers/2507.01352', 'title': 'Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy', 'url': 'https://huggingface.co/papers/2507.01352', 'abstract': 'Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.', 'score': 38, 'issue_id': 4638, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '955bbdefa8606d12', 'authors': ['Chris Yuhao Liu', 'Liang Zeng', 'Yuzhen Xiao', 'Jujie He', 'Jiacai Liu', 'Chaojie Wang', 'Rui Yan', 'Wei Shen', 'Fuxiang Zhang', 'Jiacheng Xu', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['2050 Research, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01352.jpg', 'data': {'categories': ['#open_source', '#rlhf', '#training', '#alignment', '#data', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Синергия человека и ИИ для создания передовых моделей вознаграждения', 'desc': 'Статья представляет новый набор данных предпочтений SynPref-40M, содержащий 40 миллионов пар предпочтений, созданный с помощью двухэтапного процесса, сочетающего человеческую аннотацию и масштабируемость ИИ. На основе этих данных авторы разработали набор моделей вознаграждения Skywork-Reward-V2 с параметрами от 0.6B до 8B. Модели демонстрируют высокую эффективность в различных задачах, включая соответствие человеческим предпочтениям, объективную корректность и безопасность. Исследование подчеркивает важность качественной курации данных и синергии человека и ИИ для улучшения моделей вознаграждения в обучении с подкреплением на основе обратной связи от человека (RLHF).'}, 'en': {'title': 'Unlocking Human Preferences with Skywork-Reward-V2', 'desc': 'This paper addresses the limitations of current reward models (RMs) in reinforcement learning from human feedback (RLHF), which struggle to accurately reflect complex human preferences. The authors propose a new large-scale preference dataset, SynPref-40M, containing 40 million preference pairs, to improve the training of RMs. They introduce a two-stage human-AI curation pipeline that combines human annotation with AI scalability to ensure high-quality data. The resulting Skywork-Reward-V2 models, trained on a refined subset of this dataset, demonstrate superior performance across various benchmarks, showcasing the importance of quality data in enhancing reward model effectiveness.'}, 'zh': {'title': '提升奖励模型的质量与性能', 'desc': '本论文探讨了奖励模型在从人类反馈中进行强化学习的重要性。当前的开放奖励模型在评估基准上表现不佳，无法有效捕捉人类偏好的复杂性。为了解决这一问题，作者提出了一个包含4000万对偏好的大规模数据集SynPref-40M，并设计了一个人机协作的两阶段数据处理流程。通过高质量的数据标注和AI的自动化处理，作者训练了Skywork-Reward-V2系列奖励模型，展示了其在多项基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2507.02321', 'title': 'Heeding the Inner Voice: Aligning ControlNet Training via Intermediate\n  Features Feedback', 'url': 'https://huggingface.co/papers/2507.02321', 'abstract': 'InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).', 'score': 37, 'issue_id': 4646, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'f340ac71ebc152be', 'authors': ['Nina Konovalova', 'Maxim Nikolaev', 'Andrey Kuznetsov', 'Aibek Alanov'], 'affiliations': ['AIRI, Russia', 'HSE University, Russia', 'Innopolis, Russia', 'Sber, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.02321.jpg', 'data': {'categories': ['#diffusion', '#cv', '#alignment', '#training'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль генерации изображений на всех этапах диффузии', 'desc': 'InnerControl - это новый метод обучения для улучшения пространственного контроля в диффузионных моделях генерации изображений по тексту. Он использует легковесные сверточные зонды для реконструкции входных сигналов управления на всех этапах диффузии. Метод минимизирует расхождение между предсказанными и целевыми условиями на протяжении всего процесса, что улучшает точность контроля и качество генерации. В сочетании с существующими техниками InnerControl достигает передовых результатов для различных методов кондиционирования.'}, 'en': {'title': 'Enhancing Spatial Consistency in Diffusion Models with InnerControl', 'desc': 'InnerControl is a novel training strategy designed to enhance spatial consistency in text-to-image diffusion models. It utilizes lightweight convolutional probes to reconstruct control signals from intermediate features during the denoising process. By applying an alignment loss that minimizes the difference between predicted and target conditions at every diffusion step, InnerControl significantly improves control fidelity and overall generation quality. This approach, when combined with existing methods like ControlNet++, achieves state-of-the-art results across various conditioning techniques.'}, 'zh': {'title': 'InnerControl：提升扩散模型的空间一致性与生成质量', 'desc': 'InnerControl是一种训练策略，旨在增强文本到图像扩散模型在所有扩散步骤中的空间一致性。通过训练轻量级卷积探针，InnerControl能够在每个去噪步骤中从中间UNet特征中重建输入控制信号（如边缘和深度）。这种方法有效地提取信号，即使在高度噪声的潜在空间中，也能为训练提供伪真实控制。通过最小化整个扩散过程中的预测条件与目标条件之间的差异，InnerControl显著提高了控制的准确性和生成的质量。'}}}, {'id': 'https://huggingface.co/papers/2507.02025', 'title': 'IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction', 'url': 'https://huggingface.co/papers/2507.02025', 'abstract': 'We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model.', 'score': 33, 'issue_id': 4642, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '5e8a0f59d9493778', 'authors': ['The IntFold Team', 'Leon Qiao', 'Wayne Bai', 'He Yan', 'Gary Liu', 'Nova Xi', 'Xiang Zhang'], 'affiliations': ['IntelliGen AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.02025.jpg', 'data': {'categories': ['#architecture', '#training', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'IntFold: Гибкий инструмент для точного прогнозирования биомолекулярных структур', 'desc': 'IntFold - это новая управляемая базовая модель для предсказания как общих, так и специализированных биомолекулярных структур. Она демонстрирует точность прогнозирования, сравнимую с современной моделью AlphaFold3, используя улучшенное пользовательское ядро внимания. IntFold может быть адаптирована для предсказания аллостерических состояний, ограниченных структур и аффинности связывания с помощью индивидуальных адаптеров. Модель также включает новый блок оценки уверенности для определения качества докинга, что особенно полезно для сложных целей, таких как комплексы антитело-антиген.'}, 'en': {'title': 'IntFold: Advancing Biomolecular Structure Prediction with Precision and Flexibility', 'desc': 'IntFold is a new foundation model designed for predicting the structures of biomolecules, both in general and specialized contexts. It achieves high accuracy in predictions, rivaling the leading model AlphaFold3, by employing a unique attention mechanism. The model is versatile, allowing for adaptations to predict various states and properties of biomolecules, such as allosteric states and binding affinities, through the use of specific adapters. Additionally, IntFold includes a confidence head that assesses the quality of docking predictions, particularly for complex targets like antibody-antigen interactions, and shares valuable insights from its training process.'}, 'zh': {'title': 'IntFold：生物分子结构预测的新突破', 'desc': '我们介绍了IntFold，这是一种可控的基础模型，用于一般和专业的生物分子结构预测。IntFold的预测准确性与最先进的AlphaFold3相当，同时采用了更优的定制注意力核。除了标准的结构预测，IntFold还可以通过使用单独的适配器来预测变构状态、受限结构和结合亲和力。此外，我们引入了一种新颖的置信度头，以评估对接质量，为抗体-抗原复合物等具有挑战性的目标提供更细致的评估。'}}}, {'id': 'https://huggingface.co/papers/2507.02092', 'title': 'Energy-Based Transformers are Scalable Learners and Thinkers', 'url': 'https://huggingface.co/papers/2507.02092', 'abstract': 'Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question "Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.', 'score': 21, 'issue_id': 4642, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '9f33fd27885f443d', 'authors': ['Alexi Gladstone', 'Ganesh Nanduru', 'Md Mofijul Islam', 'Peixuan Han', 'Hyeonjeong Ha', 'Aman Chadha', 'Yilun Du', 'Heng Ji', 'Jundong Li', 'Tariq Iqbal'], 'affiliations': ['Amazon GenAI', 'Harvard University', 'Stanford University', 'UIUC', 'UVA'], 'pdf_title_img': 'assets/pdf/title_img/2507.02092.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'EBTs: Обучение мышлению через неконтролируемое обучение', 'desc': "Эта статья представляет новый класс моделей - Energy-Based Transformers (EBTs), которые обучаются проверять совместимость входных данных и предсказаний. EBTs переформулируют задачу предсказания как оптимизацию относительно верификатора, что позволяет им применять методы мышления 'Системы 2' без дополнительного обучения. Исследования показывают, что EBTs масштабируются быстрее, чем стандартные трансформеры, и демонстрируют лучшую производительность на задачах обработки текста и изображений. Авторы утверждают, что EBTs представляют собой многообещающую новую парадигму для масштабирования как обучающих, так и мыслительных способностей моделей."}, 'en': {'title': 'Energy-Based Transformers: Scaling Learning and Thinking in AI', 'desc': 'This paper introduces Energy-Based Transformers (EBTs), a new type of model that enhances inference-time computation by mimicking human System 2 Thinking. EBTs learn to verify the compatibility between inputs and predictions without needing additional supervision, making them more generalizable across different modalities and tasks. The authors demonstrate that EBTs can scale faster than existing models like Transformer++ and achieve superior performance in both language and image tasks. Overall, EBTs represent a significant advancement in the efficiency and effectiveness of machine learning models.'}, 'zh': {'title': '能量基础变换器：无监督学习的新思维方式', 'desc': '本文探讨了一种新的模型——能量基础变换器（EBTs），旨在通过无监督学习来实现更好的推理能力。EBTs通过显式验证输入与候选预测之间的兼容性，将预测问题重新框架为优化问题，从而提高模型的性能。研究表明，EBTs在训练过程中比传统的Transformer++方法具有更快的扩展速度，并在推理时在语言任务上提高了29%的性能。总体而言，EBTs在大多数下游任务中表现优于现有模型，显示出更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.02652', 'title': 'Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search', 'url': 'https://huggingface.co/papers/2507.02652', 'abstract': 'Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.', 'score': 17, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'e833cc483ac9b10c', 'authors': ['Jiajie Jin', 'Xiaoxi Li', 'Guanting Dong', 'Yuyao Zhang', 'Yutao Zhu', 'Yang Zhao', 'Hongjin Qian', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.02652.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#benchmark', '#multimodal', '#agents'], 'emoji': '🧠', 'ru': {'title': 'HiRA: Иерархический подход к сложному информационному поиску', 'desc': 'Статья представляет HiRA - иерархическую систему для сложных информационных запросов. HiRA разделяет стратегическое планирование и специализированное выполнение задач, что позволяет эффективнее обрабатывать многоэтапные запросы. Система декомпозирует сложные задачи на подзадачи и назначает их специализированным агентам с внешними инструментами. Эксперименты показали, что HiRA превосходит современные RAG-системы и агентные подходы по качеству ответов и эффективности.'}, 'en': {'title': 'HiRA: Enhancing Search Efficiency through Hierarchical Reasoning', 'desc': 'This paper presents HiRA, a new framework designed to improve complex information retrieval tasks by separating high-level planning from detailed execution. Traditional methods often struggle because they use a single model for both tasks, which can lead to inefficiencies. HiRA addresses this by breaking down complex search tasks into smaller subtasks, each handled by specialized agents that have their own tools and reasoning abilities. The results show that HiRA outperforms existing systems in both the quality of answers and overall efficiency, demonstrating the benefits of this hierarchical approach.'}, 'zh': {'title': 'HiRA：分层框架提升搜索效率与质量', 'desc': '在现实世界的搜索场景中，复杂的信息需求需要深度推理和知识综合，而传统的检索增强生成（RAG）管道难以有效应对。当前的推理方法存在一个根本性限制：它们使用单一模型处理高层次规划和详细执行，导致推理效率低下和可扩展性有限。本文提出了HiRA，一个分层框架，将战略规划与专业执行分开。我们的研究表明，HiRA在复杂的跨模态深度搜索基准测试中显著优于现有的RAG和基于代理的系统，提升了答案质量和系统效率。'}}}, {'id': 'https://huggingface.co/papers/2507.02754', 'title': 'Fast and Simplex: 2-Simplicial Attention in Triton', 'url': 'https://huggingface.co/papers/2507.02754', 'abstract': 'Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.', 'score': 16, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'a9492490e5a70bc4', 'authors': ['Aurko Roy', 'Timothy Chou', 'Sai Surya Duvvuri', 'Sijia Chen', 'Jiecao Yu', 'Xiaodong Wang', 'Manzil Zaheer', 'Rohan Anil'], 'affiliations': ['Department of Computer Science University of Texas at Austin', 'Meta Menlo Park, CA'], 'pdf_title_img': 'assets/pdf/title_img/2507.02754.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности языковых моделей с помощью 2-симплициального внимания', 'desc': 'В статье исследуется архитектура 2-симплициального трансформера, обобщающая стандартное внимание на основе скалярного произведения до трилинейных функций. Авторы показывают, что эта архитектура достигает лучшей эффективности использования токенов по сравнению со стандартными трансформерами. При фиксированном бюджете токенов модели сопоставимого размера превосходят аналоги со скалярным произведением в задачах математики, программирования, рассуждений и логики. Количественно преимущества выражаются в изменении показателя степени в законах масштабирования для задач, связанных со знаниями и рассуждениями.'}, 'en': {'title': 'Enhancing Token Efficiency with 2-Simplicial Transformers', 'desc': 'This paper explores the limitations of current scaling laws in machine learning, particularly in the context of large language models that are no longer purely compute-bound due to their reliance on vast datasets. It introduces the 2-simplicial Transformer, a new architecture that enhances standard attention mechanisms by using trilinear functions, which improves token efficiency. The authors show that this new architecture allows models to perform better on various tasks, such as mathematics and reasoning, while using the same number of tokens. By quantifying the improvements, they reveal that the 2-simplicial attention modifies the scaling laws, leading to better performance in knowledge and reasoning tasks compared to traditional dot-product attention.'}, 'zh': {'title': '提升标记效率的2-单纯形变换器', 'desc': '最近的研究表明，训练损失与模型大小和标记数量呈幂律关系，达到计算最优模型需要同时扩大模型大小和标记数量。然而，这些缩放法则假设数据是无限的，并主要适用于计算受限的环境。随着现代大型语言模型越来越依赖于大规模互联网数据集，这种假设变得不再有效。因此，我们需要优先考虑标记效率的架构。'}}}, {'id': 'https://huggingface.co/papers/2507.02694', 'title': 'Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers', 'url': 'https://huggingface.co/papers/2507.02694', 'abstract': "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.", 'score': 16, 'issue_id': 4641, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'd7b392be540c08ba', 'authors': ['Zhijian Xu', 'Yilun Zhao', 'Manasi Patwardhan', 'Lovekesh Vig', 'Arman Cohan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.02694.jpg', 'data': {'categories': ['#multimodal', '#rag', '#dataset', '#benchmark', '#synthetic', '#science'], 'emoji': '🔬', 'ru': {'title': 'ИИ на страже научной объективности: LLM как помощник в рецензировании', 'desc': 'Статья посвящена использованию больших языковых моделей (LLM) для помощи в процессе рецензирования научных работ, особенно в выявлении ограничений исследований. Авторы представляют таксономию типов ограничений в научных исследованиях и создают бенчмарк LimitGen для оценки способности LLM поддерживать ранние этапы обратной связи. LimitGen включает синтетический набор данных и реальные ограничения, написанные людьми. Исследователи улучшают способность LLM выявлять ограничения, дополняя их поиском по научной литературе.'}, 'en': {'title': 'Empowering Peer Review with AI: LimitGen for Identifying Research Limitations', 'desc': "This paper addresses the challenges of peer review in scientific research due to the increasing number of publications. It introduces a taxonomy of limitation types specifically for AI research, which helps in understanding the weaknesses of scientific papers. The authors present LimitGen, a benchmark designed to evaluate how well large language models (LLMs) can assist in identifying these limitations and provide feedback. By incorporating literature retrieval, the study enhances LLMs' ability to generate relevant and constructive critiques of research papers."}, 'zh': {'title': '提升同行评审的智能化支持', 'desc': '同行评审是科学研究的重要环节，但随着出版物数量的增加，这一过程面临越来越大的挑战。本文提出了一种全面的科学研究局限性分类法，特别关注人工智能领域。我们介绍了LimitGen，这是第一个评估大型语言模型（LLM）在支持早期反馈和补充人类评审方面能力的基准测试。通过结合文献检索，我们增强了LLM系统识别研究局限性的能力，从而能够提供更具体和建设性的反馈。'}}}, {'id': 'https://huggingface.co/papers/2507.02726', 'title': 'Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving', 'url': 'https://huggingface.co/papers/2507.02726', 'abstract': 'Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.', 'score': 13, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '42e132c4863440b8', 'authors': ['Matthieu Zimmer', 'Xiaotong Ji', 'Rasul Tutunov', 'Anthony Bordg', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'Lagrange Center', 'UCL Centre for AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.02726.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#benchmark', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самогенерируемые цели улучшают автоматическое доказательство теорем', 'desc': 'Статья представляет новый подход к автоматическому доказательству теорем с использованием больших языковых моделей (LLM). Авторы предлагают фреймворк sG-MDP, в котором агенты генерируют и преследуют подцели на основе текущего состояния доказательства. Они применяют алгоритмы, подобные Monte Carlo Tree Search, для решения sG-MDP. Результатом является система Bourbaki (7B), которая достигает новых рекордных результатов на бенчмарке PutnamBench для моделей своего масштаба.'}, 'en': {'title': 'Empowering Reasoning with Self-Generated Goals in Theorem Proving', 'desc': 'This paper addresses the difficulties that large language models (LLMs) face in reasoning tasks, particularly in automated theorem proving (ATP) where rewards are sparse and proofs are complex. The authors propose a novel framework called self-generated goal-conditioned MDPs (sG-MDPs), which allows agents to create and pursue subgoals based on the current state of the proof. By structuring goal generation, the problem becomes easier to navigate and search. The framework is implemented in a system called Bourbaki (7B), which utilizes multiple LLMs to enhance subgoal generation and tactic synthesis, achieving state-of-the-art results on the challenging PutnamBench benchmark.'}, 'zh': {'title': '自生成目标助力推理挑战', 'desc': '本文探讨了大型语言模型（LLMs）在自动定理证明（ATP）中的推理挑战，尤其是在稀疏奖励和证明规模庞大的情况下。为了解决这些问题，提出了一种新的框架——自生成目标条件马尔可夫决策过程（sG-MDPs），使得智能体能够根据不断变化的证明状态生成和追求子目标。通过这种结构化的目标生成，问题变得更易于搜索。最后，应用类似蒙特卡洛树搜索（MCTS）的算法解决sG-MDP，并在Bourbaki（7B）系统中实现，成功在PutnamBench上解决了26个问题，创造了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2507.02778', 'title': 'Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs', 'url': 'https://huggingface.co/papers/2507.02778', 'abstract': 'Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic \'Self-Correction Blind Spot\' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.', 'score': 9, 'issue_id': 4645, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '4b54d8d384329494', 'authors': ['Ken Tsui'], 'affiliations': ['Independent'], 'pdf_title_img': 'assets/pdf/title_img/2507.02778.jpg', 'data': {'categories': ['#reasoning', '#training', '#alignment', '#benchmark', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Самокоррекция: ключ к надежности LLM', 'desc': 'Исследование выявляет, что у больших языковых моделей (LLM) есть проблема с самокоррекцией, так как они обучаются на данных без ошибок. Это приводит к тому, что модели не могут исправлять собственные ошибки, хотя и могут находить ошибки в пользовательском вводе. Введение фразы "Wait" помогает моделям активировать способность к самокоррекции, что значительно уменьшает количество ошибок. Работа подчеркивает важность улучшения надежности LLM через обучение на данных с ошибками и их исправлениями.'}, 'en': {'title': 'Unlocking Self-Correction in Language Models', 'desc': "This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the 'Self-Correction Blind Spot', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word 'Wait' to prompts can significantly enhance the models' self-correction abilities, indicating that these capabilities can be activated with the right cues."}, 'zh': {'title': '激活自我纠正，提升语言模型的可靠性', 'desc': '这篇论文介绍了自我纠正基准（Self-Correction Bench），用于测量大型语言模型（LLM）在自我纠正方面的盲点。研究发现，主要在无错误的响应上进行训练会导致模型在自身输出中无法纠正相同的错误。通过对14个模型进行测试，发现平均有64.5%的盲点率。简单地在输出中添加“等待”一词可以将盲点减少89.3%，这表明模型具备自我纠正的能力，但需要激活。'}}}, {'id': 'https://huggingface.co/papers/2507.01004', 'title': 'ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention', 'url': 'https://huggingface.co/papers/2507.01004', 'abstract': 'A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.', 'score': 7, 'issue_id': 4645, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c104abc218e38a97', 'authors': ['Yuhong Chou', 'Zehao Liu', 'Ruijie Zhu', 'Xinyi Wan', 'Tianjian Li', 'Congying Chu', 'Qian Liu', 'Jibin Wu', 'Zejun Ma'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'TikTok', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2507.01004.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'ZeCO: Революция в обучении языковых моделей с ультрадлинными последовательностями', 'desc': 'ZeCO - это новый метод параллелизма последовательностей с нулевыми накладными расходами на коммуникацию, разработанный для эффективного обучения больших языковых моделей с ультрадлинными последовательностями на нескольких устройствах. В основе ZeCO лежит новый примитив коллективной коммуникации All-Scan, который обеспечивает каждому рангу SP точное начальное состояние оператора при минимальных затратах на коммуникацию. Теоретически доказана оптимальность ZeCO, показывающая, что он вносит лишь незначительные временные и пространственные накладные расходы. Эмпирически продемонстрировано, что на 256 GPU с длиной последовательности 8M ZeCO достигает ускорения на 60% по сравнению с современными методами SP.'}, 'en': {'title': 'ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead', 'desc': 'This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs.'}, 'zh': {'title': 'ZeCO：高效训练超长序列的大型语言模型', 'desc': '本文介绍了一种新的零通信开销序列并行方法ZeCO，旨在高效训练具有超长序列的大型语言模型。ZeCO通过引入All-Scan这一新的集体通信原语，显著减少了设备间的通信开销，从而实现了接近线性的可扩展性。与传统的序列并行方法相比，ZeCO在多个设备上处理1M序列时的训练时间与在单个设备上处理16k序列的时间相当。实验结果表明，ZeCO在256个GPU上处理8M序列时，相较于现有的最佳序列并行方法实现了60%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2506.22813', 'title': 'Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models', 'url': 'https://huggingface.co/papers/2506.22813', 'abstract': "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.", 'score': 6, 'issue_id': 4644, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '762c8f77cac2babf', 'authors': ['Zhuojun Ding', 'Wei Wei', 'Chenghao Fan'], 'affiliations': ['School of Computer Science & Technology, Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.22813.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#transfer_learning', '#training', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Динамическое объединение экспертов для адаптивного извлечения информации', 'desc': 'Статья представляет новый подход SaM для адаптации моделей извлечения информации к различным предметным областям. Вместо обучения единой модели, SaM динамически выбирает и объединяет предобученные экспертные модели на этапе вывода. Этот метод улучшает обобщение на новые домены без дополнительного обучения и обеспечивает масштабируемость. Эксперименты показывают, что SaM превосходит единую модель в среднем на 10% по различным бенчмаркам.'}, 'en': {'title': 'Dynamic Expert Selection for Enhanced Domain Adaptation', 'desc': 'The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance.'}, 'zh': {'title': '动态选择与合并专家模型，提升跨领域性能', 'desc': '监督微调（SFT）广泛应用于将大型语言模型（LLM）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，标注这些细粒度标签和训练特定领域的模型成本高昂。现有方法通常在多个领域训练统一模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都能惠及目标领域。我们提出了SaM框架，在推理时动态选择和合并专家模型，从而提高了跨领域的泛化能力，而无需额外训练。'}}}, {'id': 'https://huggingface.co/papers/2507.01663', 'title': 'AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training', 'url': 'https://huggingface.co/papers/2507.01663', 'abstract': 'Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.', 'score': 5, 'issue_id': 4639, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '8a3f43a4a9e735d7', 'authors': ['Zhenyu Han', 'Ansheng You', 'Haibo Wang', 'Kui Luo', 'Guang Yang', 'Wenqi Shi', 'Menglong Chen', 'Sicheng Zhang', 'Zeshun Lan', 'Chunshi Deng', 'Huazhong Ji', 'Wenjie Liu', 'Yu Huang', 'Yixiang Zhang', 'Chenyi Pan', 'Jing Wang', 'Xin Huang', 'Chunsheng Li', 'Jianping Wu'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2507.01663.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#rl'], 'emoji': '🔄', 'ru': {'title': 'AsyncFlow: Асинхронное обучение с подкреплением для больших языковых моделей', 'desc': 'AsyncFlow - это новая асинхронная потоковая система обучения с подкреплением для эффективной пост-обработки больших языковых моделей. Она вводит распределенный модуль хранения и передачи данных, обеспечивающий унифицированное управление данными и детальное планирование в потоковом режиме. AsyncFlow использует асинхронный рабочий процесс на основе модели производитель-потребитель для минимизации простоев вычислений. Система показывает среднее улучшение пропускной способности в 1,59 раза по сравнению с современными аналогами.'}, 'en': {'title': 'AsyncFlow: Revolutionizing RL for Large Language Models', 'desc': 'This paper introduces AsyncFlow, a new framework for reinforcement learning (RL) that enhances the post-training phase of large language models (LLMs). It addresses scalability issues found in traditional RL frameworks by implementing a distributed data storage and transfer system, which allows for efficient data management and scheduling. The framework also features an asynchronous workflow that reduces idle computation time by optimizing the timing of parameter updates. Overall, AsyncFlow is designed to be modular and customizable, making it easier to integrate with various training and inference engines while improving throughput significantly.'}, 'zh': {'title': 'AsyncFlow：高效的异步流式强化学习框架', 'desc': '强化学习（RL）在大型语言模型（LLM）的后训练阶段变得至关重要。传统的任务共存RL框架面临可扩展性瓶颈，而任务分离的RL框架在复杂数据流和资源闲置方面存在挑战。为了解决这些问题，我们提出了AsyncFlow，一个高效的异步流式RL框架，能够实现自动化的管道重叠和动态负载平衡。我们的实验表明，与最先进的基线相比，AsyncFlow在吞吐量上平均提高了1.59倍。'}}}, {'id': 'https://huggingface.co/papers/2506.23121', 'title': 'CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for\n  Multi-Organ Segmentation', 'url': 'https://huggingface.co/papers/2506.23121', 'abstract': 'Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP\\_SAM2.git.', 'score': 2, 'issue_id': 4660, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '925d84357946a3a1', 'authors': ['Xinlei Yu', 'Chanmiao Wang', 'Hui Jin', 'Ahmed Elazab', 'Gangyong Jia', 'Xiang Wan', 'Changqing Zou', 'Ruiquan Ge'], 'affiliations': ['Hangzhou Dianzi University, Hangzhou, China', 'Shenzhen Research Institute of Big Data, Shenzhen, China', 'Shenzhen University, Shenzhen, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.23121.jpg', 'data': {'categories': ['#cv', '#healthcare'], 'emoji': '\U0001fa7b', 'ru': {'title': 'CRISP-SAM2: Революция в сегментации медицинских изображений с помощью кросс-модального ИИ', 'desc': 'Статья представляет новую модель CRISP-SAM2 для сегментации нескольких органов на медицинских изображениях. Модель использует кросс-модальное взаимодействие и семантические подсказки на основе SAM2, что позволяет улучшить детализацию и устранить зависимость от геометрических подсказок. CRISP-SAM2 применяет механизм прогрессивного кросс-внимания для объединения визуальной и текстовой информации, а также стратегию семантических подсказок для улучшения распознавания сложных целей. Эксперименты на семи публичных наборах данных показали превосходство CRISP-SAM2 над существующими моделями.'}, 'en': {'title': 'Enhancing Medical Image Segmentation with CRISP-SAM2', 'desc': 'This paper presents CRISP-SAM2, a new model for multi-organ medical segmentation that improves upon existing methods by addressing common issues like detail accuracy and reliance on geometric prompts. The model utilizes a cross-modal interaction mechanism to integrate visual and textual data, enhancing the understanding of medical images. By implementing a semantic prompting strategy, CRISP-SAM2 reduces dependency on geometric cues, allowing for better identification of complex organ structures. Experimental results show that this model significantly outperforms previous approaches across multiple datasets, confirming its effectiveness in medical image processing.'}, 'zh': {'title': 'CRISP-SAM2：提升多脏器医学分割的创新模型', 'desc': '多脏器医学分割是医学图像处理中的重要环节，帮助医生进行准确诊断和制定有效治疗方案。尽管该领域已有显著进展，但现有的多脏器分割模型常常面临细节不准确、依赖几何提示和空间信息丢失等问题。为了解决这些挑战，我们提出了一种新模型CRISP-SAM2，基于SAM2的跨模态交互和语义提示。该模型通过逐步的跨注意力交互机制，将视觉和文本输入转换为跨模态上下文语义，从而增强对视觉信息的细致理解。'}}}, {'id': 'https://huggingface.co/papers/2506.21546', 'title': 'HalluSegBench: Counterfactual Visual Reasoning for Segmentation\n  Hallucination Evaluation', 'url': 'https://huggingface.co/papers/2506.21546', 'abstract': 'HalluSegBench provides a benchmark for evaluating hallucinations in vision-language segmentation models by analyzing counterfactual scene edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in vision-language segmentation has significantly advanced grounded visual understanding. However, these models often exhibit hallucinations by producing segmentation masks for objects not grounded in the image content or by incorrectly labeling irrelevant regions. Existing evaluation protocols for segmentation hallucination primarily focus on label or textual hallucinations without manipulating the visual context, limiting their capacity to diagnose critical failures. In response, we introduce HalluSegBench, the first benchmark specifically designed to evaluate hallucinations in visual grounding through the lens of counterfactual visual reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual instance pairs spanning 281 unique object classes, and a set of newly introduced metrics that quantify hallucination sensitivity under visually coherent scene edits. Experiments on HalluSegBench with state-of-the-art vision-language segmentation models reveal that vision-driven hallucinations are significantly more prevalent than label-driven ones, with models often persisting in false segmentation, highlighting the need for counterfactual reasoning to diagnose grounding fidelity.', 'score': 2, 'issue_id': 4655, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'd577c3d7e6c15a10', 'authors': ['Xinzhuo Li', 'Adheesh Juvekar', 'Xingyou Liu', 'Muntasir Wahed', 'Kiet A. Nguyen', 'Ismini Lourentzou'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.21546.jpg', 'data': {'categories': ['#reasoning', '#hallucinations', '#benchmark', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Раскрываем галлюцинации ИИ через контрфактический анализ', 'desc': 'HalluSegBench - это новый бенчмарк для оценки галлюцинаций в моделях сегментации изображений с использованием языка. Он включает набор данных из 1340 пар контрфактических изображений и новые метрики для количественной оценки чувствительности к галлюцинациям при визуально согласованном редактировании сцен. Эксперименты показали, что галлюцинации, вызванные визуальной составляющей, более распространены, чем вызванные текстовыми метками. Бенчмарк подчеркивает необходимость контрфактического рассуждения для диагностики точности привязки моделей к изображениям.'}, 'en': {'title': 'Evaluating Hallucinations in Vision-Language Models with Counterfactuals', 'desc': 'HalluSegBench is a new benchmark designed to evaluate hallucinations in vision-language segmentation models by using counterfactual scene edits. It addresses the limitations of existing evaluation methods that mainly focus on label or textual hallucinations without altering the visual context. The benchmark includes a dataset of 1340 counterfactual instance pairs across 281 object classes and introduces metrics to measure hallucination sensitivity. Experiments show that vision-driven hallucinations are more common than label-driven ones, emphasizing the importance of counterfactual reasoning for assessing model accuracy in visual grounding.'}, 'zh': {'title': 'HalluSegBench：评估视觉幻觉的新基准', 'desc': 'HalluSegBench是一个用于评估视觉-语言分割模型中幻觉现象的基准。该基准通过分析反事实场景编辑，帮助识别模型在图像内容中未正确定位的对象。研究发现，现有的评估方法主要关注标签或文本幻觉，而忽视了视觉上下文的变化。HalluSegBench提供了1340对反事实实例和新的评估指标，揭示了视觉驱动的幻觉现象比标签驱动的更为普遍，强调了反事实推理在诊断模型准确性中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.13334', 'title': 'A Survey of Context Engineering for Large Language Models', 'url': 'https://huggingface.co/papers/2507.13334', 'abstract': 'Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.', 'score': 131, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': 'e0191e89e0360224', 'authors': ['Lingrui Mei', 'Jiayu Yao', 'Yuyao Ge', 'Yiwei Wang', 'Baolong Bi', 'Yujun Cai', 'Jiazhi Liu', 'Mingyu Li', 'Zhong-Zhi Li', 'Duzhen Zhang', 'Chenlin Zhou', 'Jiayi Mao', 'Tianze Xia', 'Jiafeng Guo', 'Shenghua Liu'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'Peking University', 'The University of Queensland', 'Tsinghua University', 'University of California, Merced', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.13334.jpg', 'data': {'categories': ['#data', '#survey', '#multimodal', '#long_context', '#architecture', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Контекстная инженерия: оптимизация входных данных для раскрытия потенциала LLM', 'desc': 'Эта статья представляет собой обзор области контекстной инженерии для больших языковых моделей (LLM). Авторы систематизируют компоненты и методы оптимизации контекстной информации, подаваемой в LLM во время вывода. Рассматриваются такие темы как извлечение и генерация контекста, обработка контекста, системы памяти и мультиагентные системы. Выявлен существенный разрыв между способностью моделей понимать сложный контекст и генерировать столь же сложные выходные данные.'}, 'en': {'title': 'Optimizing Context for Superior AI Outputs', 'desc': 'This paper introduces Context Engineering, a new approach to optimize the information provided to Large Language Models (LLMs) for better performance. It breaks down the process into key components like context retrieval, processing, and management, and shows how these can be combined in advanced systems like retrieval-augmented generation and multi-agent systems. The authors analyze over 1300 research papers to highlight a significant gap: while LLMs can understand complex contexts well, they struggle to generate sophisticated long-form content. The paper aims to provide a roadmap for future research to enhance the capabilities of LLMs in generating high-quality outputs.'}, 'zh': {'title': '优化上下文，提升语言模型能力', 'desc': '本文介绍了上下文工程（Context Engineering），这是一个系统优化大型语言模型（LLMs）信息负载的正式学科。研究表明，LLMs的性能主要取决于推理过程中提供的上下文信息。我们对上下文工程进行了全面的分类，分析了其基础组件及其在智能系统中的复杂实现。通过对1300多篇研究论文的系统分析，本文揭示了当前模型在生成复杂长文本输出方面的显著局限性，并强调了未来研究的优先方向。'}}}, {'id': 'https://huggingface.co/papers/2507.13348', 'title': 'VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2507.13348', 'abstract': 'VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.', 'score': 60, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '1f1a89020e55859c', 'authors': ['Senqiao Yang', 'Junyi Li', 'Xin Lai', 'Bei Yu', 'Hengshuang Zhao', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKU', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2507.13348.jpg', 'data': {'categories': ['#cv', '#optimization', '#training', '#rl', '#games'], 'emoji': '🔍', 'ru': {'title': 'Умное зрение: эффективность через адаптивное разрешение', 'desc': "VisionThink - это новый подход к обработке визуальных токенов в задачах компьютерного зрения и обработки естественного языка. Он динамически регулирует разрешение изображений и обработку визуальных токенов для повышения эффективности. Метод улучшает производительность на задачах оптического распознавания символов (OCR), одновременно уменьшая использование токенов в более простых задачах. VisionThink использует обучение с подкреплением и стратегию 'LLM-as-Judge' для успешного применения в задачах визуального вопросно-ответного анализа (VQA)."}, 'en': {'title': 'Dynamic Resolution for Efficient Vision-Language Processing', 'desc': 'VisionThink is a novel approach that optimizes image resolution and visual token processing for vision-language tasks. It intelligently adjusts the resolution of images based on the complexity of the task, allowing for efficient processing by reducing unnecessary visual tokens in simpler tasks. The model uses reinforcement learning to determine when to request higher-resolution images, enhancing performance on OCR tasks while maintaining accuracy in general visual question answering (VQA) tasks. This dynamic token compression strategy leads to improved efficiency and effectiveness in handling various vision-language challenges.'}, 'zh': {'title': '动态调整，提升视觉语言任务效率', 'desc': 'VisionThink 是一种动态调整图像分辨率和视觉标记处理的方法，旨在提高视觉语言任务的效率和效果。该方法在光学字符识别（OCR）任务中表现出色，同时在简单任务中减少了视觉标记的使用。通过智能判断图像分辨率是否足够，VisionThink 可以在需要时请求更高分辨率的图像。与传统的固定压缩方法不同，VisionThink 根据具体情况自主决定是否压缩标记，从而在保持性能的同时节省计算资源。'}}}, {'id': 'https://huggingface.co/papers/2507.13347', 'title': 'π^3: Scalable Permutation-Equivariant Visual Geometry Learning', 'url': 'https://huggingface.co/papers/2507.13347', 'abstract': 'A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.', 'score': 43, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '006f7b52edb3a67a', 'authors': ['Yifan Wang', 'Jianjun Zhou', 'Haoyi Zhu', 'Wenzheng Chang', 'Yang Zhou', 'Zizun Li', 'Junyi Chen', 'Jiangmiao Pang', 'Chunhua Shen', 'Tong He'], 'affiliations': ['SII', 'Shanghai AI Lab', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2507.13347.jpg', 'data': {'categories': ['#open_source', '#cv', '#optimization', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Революция в реконструкции 3D сцен без опорных кадров', 'desc': 'Статья представляет новую нейронную сеть π^3 для реконструкции визуальной геометрии без фиксированного опорного вида. В отличие от предыдущих методов, π^3 использует полностью перестановочно-эквивариантную архитектуру для предсказания аффинно-инвариантных поз камеры и масштабно-инвариантных локальных карт точек. Такой подход делает модель устойчивой к порядку входных данных и хорошо масштабируемой. π^3 достигает лучших результатов в оценке положения камеры, глубины и реконструкции плотной карты точек.'}, 'en': {'title': 'Revolutionizing Visual Geometry with Permutation-Equivariance', 'desc': 'The paper presents pi^3, a novel permutation-equivariant neural network designed for visual geometry reconstruction without relying on a fixed reference view. Traditional methods often depend on a specific viewpoint, which can introduce biases and lead to inaccuracies. In contrast, pi^3 utilizes a fully permutation-equivariant architecture to predict camera poses and point maps that are invariant to scale and reference frames. This innovative approach enhances robustness and scalability, allowing pi^3 to achieve state-of-the-art results in tasks like camera pose estimation and depth estimation.'}, 'zh': {'title': '无参考视角的视觉几何重建新方法', 'desc': '本文介绍了一种名为pi^3的神经网络，它在视觉几何重建中不依赖于固定的参考视角。传统方法通常将重建锚定在特定的视点，这种偏置可能导致不稳定和失败。与此不同，pi^3采用完全的置换等变架构，能够在没有参考框架的情况下预测仿射不变的相机姿态和尺度不变的局部点图。该模型的设计使其对输入顺序具有内在的鲁棒性，并且具有很高的可扩展性，从而在相机姿态估计、单目/视频深度估计和密集点图重建等任务中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.13332', 'title': 'The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner', 'url': 'https://huggingface.co/papers/2507.13332', 'abstract': 'TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.', 'score': 38, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': 'd03adfbd7cec2623', 'authors': ['Zhouqi Hua', 'Wenwei Zhang', 'Chengqi Lyu', 'Yuzhe Gu', 'Songyang Gao', 'Kuikun Liu', 'Kai Chen'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.13332.jpg', 'data': {'categories': ['#data', '#synthetic', '#training', '#architecture', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Имитация машины Тьюринга для улучшения обобщающей способности языковых моделей', 'desc': 'Метод TAIL имитирует процессы выполнения машины Тьюринга для улучшения способности больших языковых моделей (LLM) к обобщению на более длинные последовательности. Он синтезирует данные цепочки рассуждений, которые имитируют процесс выполнения машины Тьюринга, линейно расширяя шаги рассуждений до атомарных состояний. TAIL значительно улучшает способность к обобщению по длине и производительность модели Qwen2.5-7B на различных задачах, используя только синтетические данные. Эксперименты показывают, что ключевые концепции машины Тьюринга необходимы для TAIL для обобщения по длине.'}, 'en': {'title': 'Enhancing LLMs with Turing Machine Imitation for Better Generalization', 'desc': 'The paper introduces TAIL, a novel method that enhances the performance of large language models (LLMs) by mimicking the execution processes of Turing Machines. It addresses the challenge of length generalization, enabling LLMs to solve longer sequences than those seen during training. TAIL synthesizes chain-of-thought data to improve reasoning capabilities and reduce shortcut learning, which often leads to poor generalization. The method demonstrates significant improvements in performance across various tasks using synthetic data, highlighting the importance of Turing Machine concepts in LLM reasoning.'}, 'zh': {'title': '提升LLM长度泛化能力的图灵机模仿学习', 'desc': '本文提出了一种名为TAIL的方法，旨在提高大型语言模型（LLM）的长度泛化能力。TAIL通过模拟图灵机的执行过程，合成链式思维数据，从而减少了快捷学习现象。该方法通过线性扩展推理步骤，改善了动态和长距离数据访问的难度。实验结果表明，TAIL在多个任务上显著提升了模型的性能，展示了图灵机的关键概念在长度泛化中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.13344', 'title': 'Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models', 'url': 'https://huggingface.co/papers/2507.13344', 'abstract': 'A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .', 'score': 36, 'issue_id': 4889, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '02785244ea887bec', 'authors': ['Yudong Jin', 'Sida Peng', 'Xuan Wang', 'Tao Xie', 'Zhen Xu', 'Yifan Yang', 'Yujun Shen', 'Hujun Bao', 'Xiaowei Zhou'], 'affiliations': ['Ant Research', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.13344.jpg', 'data': {'categories': ['#video', '#cv', '#diffusion', '#dataset'], 'emoji': '🎥', 'ru': {'title': 'Улучшение 4D-согласованности в синтезе видов с помощью скользящего шумоподавления', 'desc': 'В статье предлагается новый метод улучшения пространственно-временной согласованности в 4D-диффузионных моделях для высококачественного синтеза видов из видео с редкими ракурсами. Авторы вводят скользящий итеративный процесс шумоподавления в скрытой сетке, кодирующей изображение, позу камеры и человека. Этот подход позволяет диффузионной модели получить большое рецептивное поле и улучшить 4D-согласованность выходных данных при доступном потреблении памяти GPU. Эксперименты показывают, что метод значительно превосходит существующие подходы в синтезе качественных и согласованных видео с новых ракурсов.'}, 'en': {'title': 'Enhancing Video Synthesis with Sliding Iterative Denoising', 'desc': 'This paper presents a new method to improve the quality of video synthesis from sparse-view inputs using 4D diffusion models. The authors introduce a sliding iterative denoising process that enhances spatio-temporal consistency, which is crucial for generating realistic videos. By organizing the data into a latent grid that captures image, camera, and human poses, the method allows for effective denoising across both spatial and temporal dimensions. Experiments show that this approach significantly outperforms previous techniques, producing high-fidelity and consistent videos.'}, 'zh': {'title': '增强4D一致性，合成高保真视图', 'desc': '本文提出了一种滑动迭代去噪过程，以增强4D扩散模型在稀疏视角视频中的时空一致性，从而实现高保真度的视图合成。以往的方法通过利用4D扩散模型生成新视角的视频，但生成的视频往往缺乏时空一致性，影响了合成质量。我们的方法通过定义一个潜在网格，交替沿空间和时间维度进行去噪，最终从去噪后的潜在表示中解码出目标视角的视频。实验结果表明，我们的方法在DNA-Rendering和ActorsHQ数据集上能够合成高质量且一致的新视角视频，显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2507.12841', 'title': 'AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning', 'url': 'https://huggingface.co/papers/2507.12841', 'abstract': "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.", 'score': 35, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '02e4f51787ec491d', 'authors': ['Yiming Ren', 'Zhiqiang Lin', 'Yu Li', 'Gao Meng', 'Weiyun Wang', 'Junjie Wang', 'Zicheng Lin', 'Jifeng Dai', 'Yujiu Yang', 'Wenhai Wang', 'Ruihang Chu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12841.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#games', '#dataset'], 'emoji': '🎛️', 'ru': {'title': 'Универсальный контроль над мультимодальными подписями', 'desc': 'Проект AnyCap представляет комплексное решение для улучшения контролируемости и надежности мультимодального создания подписей. Он включает в себя модель AnyCapModel (ACM), которая повышает контролируемость существующих фундаментальных моделей без их переобучения. Также создан набор данных AnyCapDataset (ACD), охватывающий три модальности и 28 типов пользовательских инструкций. Предложен новый бенчмарк AnyCapEval для более надежной оценки контролируемого создания подписей.'}, 'en': {'title': 'Enhancing Multimodal Captioning Control and Reliability', 'desc': "The AnyCap Project presents a comprehensive framework aimed at improving controllability and reliability in multimodal captioning tasks. It introduces the AnyCapModel (ACM), which enhances existing models' capabilities without the need for retraining, allowing for better alignment with user instructions and modality features. To support this, the AnyCapDataset (ACD) is created, featuring a diverse set of 300,000 high-quality entries across three modalities and 28 types of user instructions. Additionally, the AnyCapEval benchmark offers improved evaluation metrics that separate content accuracy from stylistic fidelity, demonstrating significant performance improvements in caption quality across various models."}, 'zh': {'title': '提升多模态字幕生成的可控性与可靠性', 'desc': 'AnyCap项目提出了一个框架、数据集和评估协议，以增强多模态字幕生成的可控性和可靠性。该项目的核心是AnyCapModel（ACM），它是一个轻量级的插件框架，可以在不重新训练基础模型的情况下，提高现有模型的可控性。为了应对可控多模态字幕生成中的数据稀缺问题，我们构建了AnyCapDataset（ACD），涵盖三种模态、28种用户指令类型和30万个高质量数据条目。此外，我们还提出了AnyCapEval，一个新的基准，提供更可靠的评估指标，通过解耦内容准确性和风格保真度来评估可控字幕生成。'}}}, {'id': 'https://huggingface.co/papers/2507.12142', 'title': 'RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization', 'url': 'https://huggingface.co/papers/2507.12142', 'abstract': 'RiemannLoRA addresses initialization and overparametrization in LoRA by treating LoRA matrices as a smooth manifold, improving convergence speed and performance in LLMs and diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-Rank Adaptation (LoRA) has become a widely adopted standard for parameter-efficient fine-tuning of large language models (LLMs), significantly reducing memory and computational demands. However, challenges remain, including finding optimal initialization strategies or mitigating overparametrization in low-rank matrix factorization. In this work, we propose a novel approach that addresses both of the challenges simultaneously within a unified framework. Our method treats a set of fixed-rank LoRA matrices as a smooth manifold. Considering adapters as elements on this manifold removes overparametrization, while determining the direction of the fastest loss decrease along the manifold provides initialization. Special care is taken to obtain numerically stable and computationally efficient implementation of our method, using best practices from numerical linear algebra and Riemannian optimization. Experimental results on LLM and diffusion model architectures demonstrate that RiemannLoRA consistently improves both convergence speed and final performance over standard LoRA and its state-of-the-art modifications.', 'score': 28, 'issue_id': 4894, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'a22c08f27da2bce8', 'authors': ['Vladimir Bogachev', 'Vladimir Aletov', 'Alexander Molozhavenko', 'Denis Bobkov', 'Vera Soboleva', 'Aibek Alanov', 'Maxim Rakhuba'], 'affiliations': ['AIRI, HSE University', 'HSE University', 'MIPT, ISPRAS'], 'pdf_title_img': 'assets/pdf/title_img/2507.12142.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Геометрия на службе эффективного обучения нейросетей', 'desc': 'RiemannLoRA - это новый подход к инициализации и переобучению в методе LoRA для больших языковых моделей. Он рассматривает матрицы LoRA как гладкое многообразие, что устраняет проблему избыточной параметризации. Метод определяет направление наибыстрейшего уменьшения функции потерь вдоль многообразия для оптимальной инициализации. Эксперименты показывают, что RiemannLoRA улучшает как скорость сходимости, так и конечную производительность по сравнению со стандартным LoRA.'}, 'en': {'title': 'RiemannLoRA: Smooth Solutions for Efficient Fine-Tuning', 'desc': 'RiemannLoRA introduces a new method for improving the initialization and overparametrization issues in Low-Rank Adaptation (LoRA) for large language models (LLMs). By treating LoRA matrices as a smooth manifold, this approach effectively reduces overparametrization and enhances convergence speed. The method also identifies the optimal direction for loss reduction, leading to better initialization strategies. Experimental results show that RiemannLoRA outperforms traditional LoRA techniques in both speed and performance.'}, 'zh': {'title': 'RiemannLoRA：提升LoRA的收敛速度与性能', 'desc': 'RiemannLoRA通过将LoRA矩阵视为光滑流形，解决了LoRA中的初始化和过参数化问题。这种方法在统一框架内同时处理了这两个挑战。通过将适配器视为流形上的元素，消除了过参数化，同时确定沿流形最快损失下降的方向来提供初始化。实验结果表明，RiemannLoRA在大语言模型和扩散模型架构上，显著提高了收敛速度和最终性能。'}}}, {'id': 'https://huggingface.co/papers/2507.12508', 'title': 'MindJourney: Test-Time Scaling with World Models for Spatial Reasoning', 'url': 'https://huggingface.co/papers/2507.12508', 'abstract': 'MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.', 'score': 20, 'issue_id': 4883, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '1a89f50f8edd267e', 'authors': ['Yuncong Yang', 'Jiageng Liu', 'Zheyuan Zhang', 'Siyuan Zhou', 'Reuben Tan', 'Jianwei Yang', 'Yilun Du', 'Chuang Gan'], 'affiliations': ['HKUST', 'Harvard', 'JHU', 'Microsoft Research', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2507.12508.jpg', 'data': {'categories': ['#cv', '#3d', '#benchmark', '#diffusion', '#rl', '#reasoning', '#video'], 'emoji': '🧠', 'ru': {'title': 'MindJourney: 3D-рассуждения для моделей машинного зрения без дообучения', 'desc': 'MindJourney - это фреймворк, который улучшает способности моделей машинного зрения и обработки естественного языка (VLM) к пространственному рассуждению в 3D-пространстве. Он сочетает VLM с управляемой моделью мира, основанной на видеодиффузии. Этот подход позволяет VLM создавать траекторию камеры, а модель мира генерирует соответствующие виды на каждом шаге. MindJourney достигает улучшения производительности на 8% в задачах пространственного рассуждения без дополнительного обучения.'}, 'en': {'title': 'MindJourney: Enhancing 3D Reasoning in Vision-Language Models', 'desc': 'MindJourney is a novel framework that enhances vision-language models (VLMs) by integrating them with a video diffusion-based world model, enabling better spatial reasoning in 3D environments. This approach allows VLMs to generate a camera trajectory and synthesize views dynamically, facilitating improved understanding of 3D dynamics without the need for fine-tuning. By leveraging multi-view evidence during interactive exploration, MindJourney achieves an average performance boost of over 8% on spatial reasoning tasks. This method demonstrates the effectiveness of coupling VLMs with world models for robust 3D reasoning, offering a straightforward solution for enhancing model capabilities at test time.'}, 'zh': {'title': 'MindJourney：提升视觉-语言模型的3D推理能力', 'desc': 'MindJourney 是一种增强视觉-语言模型的框架，通过与基于视频扩散的世界模型结合，实现了3D推理能力的提升。该方法在不进行微调的情况下，显著提高了空间推理任务的表现，尤其是在SAT基准测试中平均提升了8%。传统的视觉-语言模型在处理3D动态时常常表现不佳，而MindJourney通过迭代生成相机轨迹并合成多视图证据，帮助模型更好地理解空间关系。此方法展示了将世界模型与视觉-语言模型结合的潜力，为3D推理提供了一种简单有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2507.13264', 'title': 'Voxtral', 'url': 'https://huggingface.co/papers/2507.13264', 'abstract': 'Voxtral Mini and Voxtral Small are multimodal audio chat models that excel in understanding spoken audio and text, with a 32K context window for handling long audio files and conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.', 'score': 18, 'issue_id': 4892, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '869f860fcaa0550b', 'authors': ['Alexander H. Liu', 'Andy Ehrenberg', 'Andy Lo', 'Clément Denoix', 'Corentin Barreau', 'Guillaume Lample', 'Jean-Malo Delignon', 'Khyathi Raghavi Chandu', 'Patrick von Platen', 'Pavankumar Reddy Muddireddy', 'Sanchit Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Thomas Foubert', 'Abhinav Rastogi', 'Adam Yang', 'Albert Q. Jiang', 'Alexandre Sablayrolles', 'Amélie Héliou', 'Amélie Martin', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste Rozière', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'Clémence Lanfranchi', 'Darius Dabert', 'Devendra Singh Chaplot', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gabrielle Berrada', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jason Rute', 'Jean-Hadrien Chabran', 'Jessica Chudnovsky', 'Joachim Studnia', 'Joep Barmentlo', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Karmesh Yadav', 'Kartik Khandelwal', 'Kush Jain', 'Lélio Renard Lavaud', 'Léonard Blier', 'Lingxiao Zhao', 'Louis Martin', 'Lucile Saulnier', 'Luyu Gao', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Matthieu Dinot', 'Maxime Darrin', 'Maximilian Augustin', 'Mickaël Seznec', 'Neha Gupta', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Philomène Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Rémi Delacourt', 'Romain Sauvestre', 'Roman Soletskyi', 'Sagar Vaze', 'Sandeep Subramanian', 'Saurabh Garg', 'Shashwat Dalal', 'Siddharth Gandhi', 'Sumukh Aithal', 'Szymon Antoniak', 'Teven Le Scao', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'Timothée Lacroix', 'Tom Bewley', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yihan Wan', 'Yunhao Tang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.13264.jpg', 'data': {'categories': ['#benchmark', '#audio', '#small_models', '#multimodal', '#long_context', '#open_source'], 'emoji': '🎙️', 'ru': {'title': 'Мультимодальные модели Voxtral: понимание речи и текста на новом уровне', 'desc': 'Представлены Voxtral Mini и Voxtral Small - мультимодальные модели для аудио-чата, способные понимать как речь, так и текст. Эти модели достигают высоких результатов в различных аудио-бенчмарках, сохраняя при этом сильные текстовые возможности. Voxtral Small превосходит ряд закрытых моделей, оставаясь достаточно компактной для локального запуска. Контекстное окно в 32 тысячи токенов позволяет обрабатывать длинные аудиофайлы и многоэтапные диалоги.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Voxtral Models', 'desc': 'Voxtral Mini and Voxtral Small are advanced multimodal audio chat models designed to understand both spoken audio and text. They utilize a 32K context window, allowing them to process long audio files and extended conversations effectively. The models achieve top performance on various audio benchmarks while maintaining strong text comprehension capabilities. Additionally, Voxtral Small is efficient enough to run locally and surpasses several proprietary models in performance.'}, 'zh': {'title': 'Voxtral：音频与文本的完美结合', 'desc': 'Voxtral Mini和Voxtral Small是两种多模态音频聊天模型，能够理解语音和文本。它们具有32K的上下文窗口，可以处理长达40分钟的音频文件和多轮对话。Voxtral在多个音频基准测试中表现出色，同时保持了强大的文本处理能力。Voxtral Small在本地运行时超越了许多闭源模型，且我们还提供了三个基准测试来评估语音理解模型的知识和常识。'}}}, {'id': 'https://huggingface.co/papers/2507.12956', 'title': 'FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers', 'url': 'https://huggingface.co/papers/2507.12956', 'abstract': "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/.", 'score': 18, 'issue_id': 4887, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '4e0dcacd5b147ff7', 'authors': ['Qiang Wang', 'Mengchao Wang', 'Fan Jiang', 'Yaqi Fan', 'Yonggang Qi', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group', 'Beijing University of Posts and Telecommunications'], 'pdf_title_img': 'assets/pdf/title_img/2507.12956.jpg', 'data': {'categories': ['#dataset', '#games', '#diffusion', '#benchmark', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Оживление портретов: от статики к эмоциональной динамике', 'desc': 'FantasyPortrait - это фреймворк на основе диффузионного трансформера для генерации высококачественных и эмоционально богатых анимаций лиц. Он использует неявные представления и механизм маскированного кросс-внимания для создания анимаций как одного, так и нескольких персонажей. Метод вводит стратегию обучения с усилением выражений для захвата независимой от личности динамики лица. Для управления несколькими персонажами разработан механизм маскированного кросс-внимания, обеспечивающий независимую, но скоординированную генерацию выражений.'}, 'en': {'title': 'Transforming Static Images into Emotion-Rich Animations with FantasyPortrait', 'desc': "FantasyPortrait is a novel framework that utilizes diffusion transformers to create high-quality facial animations from static images, focusing on both single and multi-character scenarios. It overcomes limitations of previous methods that relied on explicit geometric features, which often led to artifacts and failed to capture subtle emotional expressions. The framework employs an expression-augmented learning strategy with implicit representations to enhance the rendering of fine-grained emotions. Additionally, a masked cross-attention mechanism allows for independent expression generation in multi-character settings, minimizing interference between different characters' features."}, 'zh': {'title': '高保真情感面部动画生成的创新框架', 'desc': 'FantasyPortrait是一种基于扩散变换器的框架，能够为单个和多个角色场景生成高保真且富有情感的面部动画。该方法通过隐式表示和掩蔽交叉注意机制，克服了传统方法在面部重现中的伪影问题，并能够捕捉细腻的情感变化。为了支持多角色动画，FantasyPortrait设计了一个掩蔽交叉注意机制，确保独立而协调的表情生成，避免特征干扰。我们的实验表明，FantasyPortrait在定量指标和定性评估中显著优于现有的最先进方法，尤其在复杂的跨重现和多角色场景中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2507.13300', 'title': 'AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research', 'url': 'https://huggingface.co/papers/2507.13300', 'abstract': 'AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.', 'score': 14, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '4836fc7ccb11ed5e', 'authors': ['Yilun Zhao', 'Weiyuan Chen', 'Zhijian Xu', 'Manasi Patwardhan', 'Yixin Liu', 'Chengye Wang', 'Lovekesh Vig', 'Arman Cohan'], 'affiliations': ['TCS Research', 'Yale NLP Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.13300.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#science', '#interpretability'], 'emoji': '🧪', 'ru': {'title': 'AbGen: Выявление пробелов в способностях ИИ проектировать научные эксперименты', 'desc': 'AbGen - это первый бенчмарк для оценки способностей языковых моделей в разработке аблационных исследований для научных работ. Он состоит из 1500 примеров, аннотированных экспертами, на основе 807 статей по обработке естественного языка. Оценка ведущих языковых моделей показала значительный разрыв в производительности между ними и экспертами-людьми. Исследование также выявило ненадежность существующих автоматизированных методов оценки для этой задачи.'}, 'en': {'title': 'Bridging the Gap: Evaluating LLMs in Scientific Ablation Studies', 'desc': 'AbGen is a benchmark created to assess how well large language models (LLMs) can design ablation studies in scientific research. It includes 1,500 examples from 807 NLP papers, where LLMs must generate detailed study designs based on specific research contexts. The evaluation shows that LLMs like DeepSeek-R1-0528 and o4-mini fall short compared to human experts in key areas such as importance and soundness of the designs. Additionally, the study reveals that current automated evaluation methods are unreliable, prompting the development of AbGen-Eval to better assess LLM performance in this context.'}, 'zh': {'title': '评估LLMs在科学研究中的消融研究设计能力', 'desc': 'AbGen是一个新基准，用于评估大型语言模型（LLMs）在设计科学研究的消融研究中的能力。该基准包含1500个专家注释的示例，来源于807篇自然语言处理（NLP）论文。我们的评估显示，领先的LLMs在消融研究设计的重要性、真实性和合理性方面，与人类专家存在显著的性能差距。此外，我们还发现当前的自动评估方法在这一任务中并不可靠，存在与人类评估结果的显著差异。'}}}, {'id': 'https://huggingface.co/papers/2507.12990', 'title': 'Teach Old SAEs New Domain Tricks with Boosting', 'url': 'https://huggingface.co/papers/2507.12990', 'abstract': 'A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.', 'score': 9, 'issue_id': 4889, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '15daa9824ba8ff47', 'authors': ['Nikita Koriagin', 'Yaroslav Aksenov', 'Daniil Laptev', 'Gleb Gerasimov', 'Nikita Balagansky', 'Daniil Gavrilov'], 'affiliations': ['HSE University', 'Moscow Institute of Physics and Technology', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2507.12990.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#interpretability', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Повышение интерпретируемости языковых моделей с помощью остаточного обучения автоэнкодеров', 'desc': 'Статья представляет новый подход к улучшению разреженных автоэнкодеров (Sparse Autoencoders) для интерпретации внутренних представлений больших языковых моделей (LLM). Предложенный метод использует остаточное обучение (residual learning) для захвата специфических особенностей предметной области без полного переобучения модели. Вторичный автоэнкодер обучается моделировать ошибку реконструкции предварительно обученного автоэнкодера на текстах определенной области. Эксперименты показывают значительное улучшение метрик кросс-энтропии и объясненной дисперсии для LLM в специализированных доменах.'}, 'en': {'title': 'Enhancing Sparse Autoencoders with Residual Learning for Domain-Specific Insights', 'desc': 'This paper presents a novel method that enhances Sparse Autoencoders (SAEs) using a residual learning approach to better capture domain-specific features. By training a secondary SAE to model the reconstruction error of a pretrained SAE, the method allows for the integration of new domain knowledge without the need for complete retraining. The results show significant improvements in performance metrics, such as cross-entropy and explained variance, across various specialized domains. This approach not only improves the interpretability of SAEs but also maintains their effectiveness on general tasks, paving the way for targeted mechanistic interpretability in large language models.'}, 'zh': {'title': '残差学习提升稀疏自编码器的领域特征捕捉能力', 'desc': '本文提出了一种残差学习方法，增强了稀疏自编码器（SAE），使其能够捕捉特定领域的特征，而无需重新训练。通过训练一个次级SAE来建模预训练SAE在特定领域文本上的重构误差，有效地捕捉到主模型遗漏的特征。我们在推理过程中将两个模型的输出相加，显著提高了多个专业领域的LLM交叉熵和解释方差指标。该方法能够高效地将新的领域知识融入现有的SAE，同时保持其在一般任务上的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.12720', 'title': 'FLEXITOKENS: Flexible Tokenization for Evolving Language Models', 'url': 'https://huggingface.co/papers/2507.12720', 'abstract': 'FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens', 'score': 6, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '195b799c7dd66533', 'authors': ['Abraham Toluase Owodunni', 'Orevaoghene Ahia', 'Sachin Kumar'], 'affiliations': ['The Ohio State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.12720.jpg', 'data': {'categories': ['#low_resource', '#training', '#architecture', '#dataset', '#optimization', '#multilingual'], 'emoji': '🧩', 'ru': {'title': 'Гибкая токенизация для улучшения языковых моделей', 'desc': 'FLEXITOKENS - это байтовая языковая модель с обучаемым токенизатором, которая снижает чрезмерную фрагментацию токенов и улучшает производительность в многоязычных задачах и задачах с разнообразной морфологией. Модель включает подмодуль, который учится предсказывать границы между байтовыми последовательностями, кодируя их в сегменты переменной длины. В отличие от существующих методов без токенизаторов, FLEXITOKENS использует упрощенную целевую функцию обучения, обеспечивающую большую гибкость при адаптации. Эксперименты показывают, что FLEXITOKENS последовательно снижает чрезмерную фрагментацию токенов и достигает до 10% улучшения производительности в целевых задачах по сравнению с подсловными и другими токенизаторами на основе градиентов.'}, 'en': {'title': 'FLEXITOKENS: Adaptive Tokenization for Enhanced Language Model Performance', 'desc': 'FLEXITOKENS is a novel byte-level language model that introduces a learnable tokenizer to enhance adaptability in tokenization. Traditional subword tokenizers are rigid and often lead to over-fragmentation, especially in multilingual and morphologically diverse contexts. By allowing the model to learn token boundaries dynamically, FLEXITOKENS reduces inefficiencies and improves performance on various tasks. The results show up to a 10% increase in performance compared to existing tokenization methods, demonstrating its effectiveness in handling diverse data distributions.'}, 'zh': {'title': 'FLEXITOKENS：灵活的字节级语言模型', 'desc': 'FLEXITOKENS是一种字节级语言模型，具有可学习的分词器，旨在减少分词过度碎片化的问题。传统的子词分词器在适应新数据时往往不够灵活，导致在处理不同语言或脚本时效率低下。通过引入可学习的分词器，FLEXITOKENS能够根据输入字节序列自适应地预测分界，从而生成可变长度的分段。实验结果表明，FLEXITOKENS在多语言基准测试和形态多样性任务中表现优异，性能提升可达10%。'}}}, {'id': 'https://huggingface.co/papers/2507.04984', 'title': 'TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation', 'url': 'https://huggingface.co/papers/2507.04984', 'abstract': 'Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n (we use n to denote time in videos to avoid notation overload with the timestep t in diffusion models) based on two consecutive neighboring frames I_0 and I_1. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.', 'score': 5, 'issue_id': 4884, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'b8c67ea4defb3288', 'authors': ['Zonglin Lyu', 'Chen Chen'], 'affiliations': ['Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2507.04984.jpg', 'data': {'categories': ['#video', '#training', '#data', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Эффективная интерполяция видеокадров с помощью временно-осведомленной диффузионной модели', 'desc': 'Статья представляет новый метод интерполяции видеокадров под названием TLB-VFI, основанный на диффузионных моделях. Данный подход эффективно извлекает временную информацию из видео с помощью 3D-вейвлет гейтинга и автоэнкодера с учетом времени. TLB-VFI превосходит существующие методы, улучшая показатель FID на 20% на сложных датасетах, при этом используя в 3 раза меньше параметров. Метод также требует в 9000 раз меньше данных для обучения по сравнению с видео-ориентированными диффузионными моделями.'}, 'en': {'title': 'Efficient Video Frame Interpolation with Temporal Awareness', 'desc': 'Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) enhances the process of predicting intermediate video frames by effectively utilizing temporal information. It introduces a novel 3D-wavelet gating and a temporal-aware autoencoder to improve efficiency and reduce the number of parameters needed for training. Compared to existing methods, TLB-VFI achieves a significant performance boost while requiring much less training data and computational resources. This approach not only accelerates the inference process but also maintains high-quality output, making it a promising advancement in video frame interpolation.'}, 'zh': {'title': '高效视频帧插值的新方法', 'desc': '本文提出了一种名为时间感知潜在布朗桥扩散（TLB-VFI）的视频帧插值方法。该方法通过提取丰富的时间信息，显著提高了视频帧插值的效率。与现有方法相比，TLB-VFI在参数数量上减少了三倍，并且训练数据需求降低了9000倍。实验结果表明，该方法在最具挑战性的数据集上相较于最新的图像扩散模型，FID指标提高了20%。'}}}, {'id': 'https://huggingface.co/papers/2507.11589', 'title': 'Einstein Fields: A Neural Perspective To Computational General\n  Relativity', 'url': 'https://huggingface.co/papers/2507.11589', 'abstract': 'Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are Neural Tensor Fields with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields', 'score': 4, 'issue_id': 4891, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '3d0f8c09fa171915', 'authors': ['Sandeep Suresh Cranganore', 'Andrei Bodnar', 'Arturs Berzins', 'Johannes Brandstetter'], 'affiliations': ['Emmi AI GmbH, Linz, Austria', 'LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria', 'University of Manchester, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.11589.jpg', 'data': {'categories': ['#dataset', '#science', '#open_source', '#architecture'], 'emoji': '🌌', 'ru': {'title': 'Einstein Fields: Нейронное сжатие пространства-времени', 'desc': 'Статья представляет Einstein Fields - нейронное представление для сжатия четырехмерных симуляций численной теории относительности в компактные веса нейронной сети. Эта технология позволяет моделировать метрику пространства-времени и выводить физические величины с помощью автоматического дифференцирования. В отличие от обычных нейронных полей, в Einstein Fields динамика возникает естественным образом. Метод демонстрирует многообещающий потенциал для численной теории относительности, включая непрерывное моделирование 4D пространства-времени и эффективное хранение данных.'}, 'en': {'title': 'Revolutionizing Numerical Relativity with Einstein Fields', 'desc': 'Einstein Fields is a novel neural tensor field representation that compresses complex four-dimensional numerical relativity simulations into compact neural network weights. This approach allows for automatic differentiation, enabling the extraction of physical quantities directly from the model. Unlike traditional neural fields, Einstein Fields naturally incorporate dynamics when encoding the spacetime geometry of general relativity. The framework demonstrates advantages such as efficient storage, high accuracy in derivatives, and versatility across different mesh types, making it a significant advancement in the field of numerical relativity.'}, 'zh': {'title': '爱因斯坦场：压缩四维数值相对论的神经网络新方法', 'desc': '爱因斯坦场是一种神经张量场表示，旨在将计算密集型的四维数值相对论模拟压缩为紧凑的隐式神经网络权重。这种方法通过建模度量，能够利用自动微分推导物理量。与传统的神经场不同，爱因斯坦场在编码广义相对论的时空几何时，自然地产生动态效果。该方法展示了在四维时空建模、存储效率和易用性等方面的显著潜力，并提供了一个开源的JAX库，促进了数值相对论的可扩展性和表现力。'}}}, {'id': 'https://huggingface.co/papers/2507.13255', 'title': 'Automating Steering for Safe Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2507.13255', 'abstract': "AutoSteer, a modular inference-time intervention technology, enhances the safety of Multimodal Large Language Models by reducing attack success rates across various threats without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.", 'score': 3, 'issue_id': 4892, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': 'be41528bddf290cd', 'authors': ['Lyucheng Wu', 'Mengru Wang', 'Ziwen Xu', 'Tri Cao', 'Nay Oo', 'Bryan Hooi', 'Shumin Deng'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2507.13255.jpg', 'data': {'categories': ['#benchmark', '#inference', '#multimodal', '#security', '#interpretability'], 'emoji': '🛡️', 'ru': {'title': 'AutoSteer: защита мультимодальных ИИ-систем без переобучения', 'desc': 'AutoSteer - это модульная технология вмешательства во время вывода, которая повышает безопасность мультимодальных больших языковых моделей (MLLM). Она снижает частоту успешных атак для различных угроз без дополнительного обучения модели. AutoSteer включает три ключевых компонента: оценку осведомленности о безопасности, адаптивный зонд безопасности и облегченную отказную головку. Эксперименты показали, что AutoSteer значительно снижает частоту успешных атак для текстовых, визуальных и кросс-модальных угроз, сохраняя при этом общие возможности модели.'}, 'en': {'title': 'Enhancing Safety in Multimodal AI with AutoSteer', 'desc': "AutoSteer is a new technology designed to make Multimodal Large Language Models (MLLMs) safer during their use. It works by implementing a modular system that does not require any changes to the original model, ensuring ease of use. The technology includes a Safety Awareness Score to identify safety issues, an adaptive safety prober to predict harmful outputs, and a Refusal Head to intervene when risks are detected. Tests show that AutoSteer effectively lowers the chances of successful attacks while keeping the model's performance intact."}, 'zh': {'title': 'AutoSteer：提升多模态AI安全性的创新技术', 'desc': 'AutoSteer是一种模块化的推理时干预技术，旨在提高多模态大型语言模型（MLLMs）的安全性。它通过引入安全意识评分（SAS）和自适应安全探测器，自动识别模型内部层次中与安全相关的区别，从而降低攻击成功率。该技术不需要对基础模型进行微调，能够在检测到安全风险时，使用轻量级拒绝头进行干预。实验结果表明，AutoSteer在多种安全关键基准测试中显著降低了文本、视觉和跨模态威胁的攻击成功率，同时保持了模型的通用能力。'}}}, {'id': 'https://huggingface.co/papers/2506.23044', 'title': 'Ovis-U1 Technical Report', 'url': 'https://huggingface.co/papers/2506.23044', 'abstract': 'Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.', 'score': 48, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'caa82e446dde84a7', 'authors': ['Guo-Hua Wang', 'Shanshan Zhao', 'Xinjie Zhang', 'Liangfu Cao', 'Pengxin Zhan', 'Lunhao Duan', 'Shiyin Lu', 'Minghao Fu', 'Xiaohao Chen', 'Jianshan Zhao', 'Yang Li', 'Qing-Guo Chen'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.23044.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#multimodal', '#benchmark'], 'emoji': '🦾', 'ru': {'title': 'Ovis-U1: Новый уровень мультимодального ИИ', 'desc': 'Ovis-U1 - это мультимодальная языковая модель с 3 миллиардами параметров, объединяющая понимание разных типов данных, генерацию изображений по тексту и редактирование изображений. Модель использует диффузионный визуальный декодер и двунаправленный уточнитель токенов для задач генерации изображений. Ovis-U1 применяет новый унифицированный подход к обучению, начиная с языковой модели, что позволяет достичь лучших результатов по сравнению с обучением только на задачах понимания или генерации. Модель демонстрирует высокие результаты в различных бенчмарках, превосходя современные аналоги в мультимодальном понимании, генерации и редактировании изображений.'}, 'en': {'title': 'Ovis-U1: Unifying Text and Image Mastery', 'desc': 'Ovis-U1 is a powerful machine learning model with 3 billion parameters that excels in understanding and generating images from text, as well as editing images. It uses a unique training method that combines language understanding and image generation, leading to improved performance over models that focus on just one of these tasks. The model features a diffusion-based visual decoder and a bidirectional token refiner, which enhance its capabilities in generating high-quality images. Ovis-U1 has achieved impressive scores on various benchmarks, outperforming other state-of-the-art models in multimodal tasks.'}, 'zh': {'title': 'Ovis-U1：多模态统一模型的突破', 'desc': 'Ovis-U1是一个拥有30亿参数的统一模型，结合了多模态理解、文本到图像生成和图像编辑的能力。它采用基于扩散的视觉解码器和双向令牌精炼器，使得图像生成任务的表现与领先模型如GPT-4o相当。与一些使用固定多语言模型进行生成任务的模型不同，Ovis-U1采用了一种新的统一训练方法，从语言模型开始训练。通过将理解和生成任务结合，Ovis-U1在多个基准测试中表现出色，推动了多模态理解、生成和编辑的边界。'}}}, {'id': 'https://huggingface.co/papers/2506.24119', 'title': 'SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.24119', 'abstract': 'Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.', 'score': 26, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'fbb4b5b047892d14', 'authors': ['Bo Liu', 'Leon Guertler', 'Simon Yu', 'Zichen Liu', 'Penghui Qi', 'Daniel Balcells', 'Mickel Liu', 'Cheston Tan', 'Weiyan Shi', 'Min Lin', 'Wee Sun Lee', 'Natasha Jaques'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR', 'National University of Singapore', 'Northeastern University', 'Plastic Labs', 'Sea AI Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.24119.jpg', 'data': {'categories': ['#rl', '#reasoning', '#games', '#transfer_learning', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование ИИ через игры с нулевой суммой', 'desc': 'Статья представляет SPIRAL - фреймворк для самообучения языковых моделей через игру в игры с нулевой суммой против улучшающихся версий самих себя. Этот подход устраняет необходимость в человеческом кураторстве, создавая бесконечный курс прогрессивно усложняющихся задач. Обучение на одной только игре Кун-покер улучшило результаты модели Qwen3-4B-Base на 8.6% в математике и 8.4% в общих рассуждениях. Анализ показал, что перенос навыков происходит через три когнитивных паттерна: систематическую декомпозицию, расчет ожидаемой ценности и анализ по отдельным случаям.'}, 'en': {'title': 'Empowering AI Reasoning through Self-Play in Zero-Sum Games', 'desc': 'The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models.'}, 'zh': {'title': '自我对弈：提升语言模型推理能力的新方法', 'desc': '本文介绍了一种名为SPIRAL的自我对弈框架，旨在通过零和游戏提升语言模型的推理能力。该方法通过模型与自身不断改进的版本进行多轮对弈，消除了对人工监督的需求。SPIRAL能够生成无限的逐步挑战问题，使模型必须适应更强的对手，从而实现自我提升。实验结果表明，使用SPIRAL进行训练的模型在数学和一般推理方面均有显著提升，展示了零和游戏在自主推理发展中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.23858', 'title': 'VMoBA: Mixture-of-Block Attention for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2506.23858', 'abstract': 'VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.', 'score': 25, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '684efafe36e7bbc8', 'authors': ['Jianzong Wu', 'Liang Hou', 'Haotian Yang', 'Xin Tao', 'Ye Tian', 'Pengfei Wan', 'Di Zhang', 'Yunhai Tong'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23858.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'VMoBA: Эффективное внимание для генерации видео', 'desc': 'VMoBA - это новый механизм разреженного внимания для видео-диффузионных моделей (VDM). Он решает проблему квадратичной сложности полных механизмов внимания, ускоряя обучение и инференс. VMoBA использует схему разбиения на блоки 1D-2D-3D, глобальный выбор блоков и выбор на основе порога для адаптации к пространственно-временным паттернам внимания. Эксперименты показывают значительное ускорение при сохранении или улучшении качества генерации видео.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention', 'desc': 'VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality.'}, 'zh': {'title': 'VMoBA：加速视频生成的新稀疏注意力机制', 'desc': 'VMoBA是一种新颖的稀疏注意力机制，专为视频扩散模型（VDMs）设计，旨在解决全注意力机制的平方复杂度问题，从而加速训练和推理。通过对预训练视频变换器的注意力模式进行深入分析，VMoBA能够有效捕捉视频数据的时空特性。该机制通过三项关键改进提升了原有的MoBA框架，包括动态适应时空注意力模式的分层递归块划分方案、优先选择最显著的查询-键块交互的全局块选择，以及基于阈值的块选择来动态确定关注块的数量。实验结果表明，VMoBA在长序列训练中显著加速VDMs的训练，同时在生成质量上与全注意力机制相当或更优。'}}}, {'id': 'https://huggingface.co/papers/2506.24123', 'title': 'Calligrapher: Freestyle Text Image Customization', 'url': 'https://huggingface.co/papers/2506.24123', 'abstract': "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.", 'score': 24, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'ee3cd26fec757450', 'authors': ['Yue Ma', 'Qingyan Bai', 'Hao Ouyang', 'Ka Leong Cheng', 'Qiuyu Wang', 'Hongyu Liu', 'Zichen Liu', 'Haofan Wang', 'Jingye Chen', 'Yujun Shen', 'Qifeng Chen'], 'affiliations': ['Ant Group, China', 'Hong Kong University of Science and Technology, China', 'InstantX, Independent Research Team'], 'pdf_title_img': 'assets/pdf/title_img/2506.24123.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#diffusion', '#dataset'], 'emoji': '✒️', 'ru': {'title': 'Искусственный каллиграф: новый уровень цифровой типографики', 'desc': 'Статья представляет Calligrapher - новую систему для генерации цифровой типографики на основе диффузионных моделей. Ключевые особенности включают самодистилляцию для создания стилистического эталона и локализованное внедрение стиля через обучаемый энкодер. Система использует предобученную генеративную модель текст-изображение и большую языковую модель для автоматического конструирования типографических образцов. Calligrapher превосходит традиционные модели в точности воспроизведения стилистических деталей и позиционирования глифов.'}, 'en': {'title': 'Empowering Digital Typography with Style Consistency', 'desc': "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."}, 'zh': {'title': 'Calligrapher：自动化高质量数字排版的创新框架', 'desc': 'Calligrapher 是一个基于扩散的框架，结合了自蒸馏和局部风格注入技术，旨在生成高质量且风格一致的数字排版。该框架解决了排版定制中风格控制和数据依赖的挑战，提出了三项关键技术贡献。首先，开发了一种自蒸馏机制，利用预训练的文本到图像生成模型和大型语言模型，自动构建以风格为中心的排版基准。其次，通过可训练的风格编码器引入局部风格注入框架，提取参考图像中的强健风格特征，从而提升目标风格的对齐精度。'}}}, {'id': 'https://huggingface.co/papers/2506.22832', 'title': 'Listener-Rewarded Thinking in VLMs for Image Preferences', 'url': 'https://huggingface.co/papers/2506.22832', 'abstract': 'A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model\'s reasoning trace contradicts that of an independent, frozen vision-language model ("listener") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner\'s chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.', 'score': 19, 'issue_id': 4575, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '70be97df80ce7e08', 'authors': ['Alexander Gambashidze', 'Li Pengyi', 'Matvey Skripkin', 'Andrey Galichin', 'Anton Gusarov', 'Konstantin Sobolev', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['Artificial Intelligence Research Institute, Moscow, Russia', 'Skolkovo Institute of Science and Technology, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.22832.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#alignment', '#rl', '#reasoning', '#rlhf'], 'emoji': '👂', 'ru': {'title': 'Слушай и учись: новый метод обучения моделей вознаграждения', 'desc': "Статья представляет новый подход к улучшению моделей вознаграждения для выравнивания генеративных моделей изображений и видео с человеческими предпочтениями. Авторы предлагают метод обучения с подкреплением, называемый listener-augmented Group Relative Policy Optimization (GRPO), который использует независимую модель-'слушателя' для оценки объяснений основной модели. Этот подход позволяет значительно повысить точность и обобщающую способность модели вознаграждения на бенчмарке ImageReward и крупномасштабном наборе данных о человеческих предпочтениях. Результаты демонстрируют эффективность метода для создания моделей, лучше соответствующих нюансам человеческих предпочтений."}, 'en': {'title': 'Enhancing Model Reasoning with Listener-Augmented Rewards', 'desc': "This paper presents a new framework called listener-augmented Group Relative Policy Optimization (GRPO) to improve reward models for aligning vision-language models with human preferences. The framework addresses the issue of reasoning accuracy by incorporating a 'listener' model that re-evaluates the reasoning process of the main model, providing calibrated confidence scores. This approach not only enhances the accuracy of the model's outputs but also reduces contradictions in reasoning when compared to traditional methods. The results show significant improvements in both accuracy and out-of-distribution performance, demonstrating the effectiveness of listener-based rewards in training robust models."}, 'zh': {'title': '增强推理，提升人类偏好对齐的准确性', 'desc': '本文提出了一种增强型的群体相对策略优化框架，通过重新评估推理过程来改善奖励模型，从而提高视觉-语言模型与人类偏好的对齐精度和在分布外的表现。当前的奖励模型在泛化能力上存在不足，且监督微调容易导致记忆化。我们引入了一个“听众”模型，它独立于推理模型，能够对推理过程进行重新评估，并提供更为准确的置信度评分。通过这种方式，我们的奖励机制不仅鼓励推理模型给出正确答案，还促使其生成对独立模型具有说服力的解释。'}}}, {'id': 'https://huggingface.co/papers/2506.17930', 'title': 'Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective', 'url': 'https://huggingface.co/papers/2506.17930', 'abstract': 'A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.', 'score': 16, 'issue_id': 4570, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': '346a389b3fbfb2bd', 'authors': ['Jianyu Wang', 'Zhiqiang Hu', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'MiroMind'], 'pdf_title_img': 'assets/pdf/title_img/2506.17930.jpg', 'data': {'categories': ['#optimization', '#alignment', '#multimodal', '#training'], 'emoji': '✂️', 'ru': {'title': 'Бессмыслица в промптах улучшает работу языковых моделей', 'desc': "В статье представлена новая парадигма проектирования промптов под названием PromptQuine. Исследование показывает, что обрезание случайных демонстраций до 'бессмыслицы' может улучшить производительность больших языковых моделей (LLM) в различных задачах. Этот метод превосходит современные методы автоматической оптимизации промптов. PromptQuine использует эволюционный поиск для автоматического обнаружения стратегии обрезки, используя только небольшие наборы данных."}, 'en': {'title': "Unlocking LLM Potential with 'Gibberish' Prompts!", 'desc': "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."}, 'zh': {'title': '胡言乱语，提升模型表现的秘密', 'desc': '本文提出了一种新的提示设计范式，称为PromptQuine，挑战了传统的大型语言模型（LLM）提示方法。研究表明，将随机示例修剪成看似无意义的“胡言乱语”可以显著提高模型在多种任务上的表现，甚至超越了现有的最佳方法。尽管发现有效的修剪策略并不简单，但PromptQuine框架通过自我发现的方式，自动搜索修剪策略，利用低数据环境进行优化。我们的研究结果希望能为在上下文学习中的机制研究提供指导，并推动更开放的搜索算法的发展，以实现更有效的LLM提示。'}}}, {'id': 'https://huggingface.co/papers/2506.23151', 'title': 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation', 'url': 'https://huggingface.co/papers/2506.23151', 'abstract': 'MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.', 'score': 15, 'issue_id': 4578, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': 'ad8371e7e40b14b4', 'authors': ['Vladislav Bargatin', 'Egor Chistov', 'Alexander Yakovenko', 'Dmitriy Vatolin'], 'affiliations': ['Lomonosov Moscow State University, Moscow, Russia', 'MSU Institute for Artificial Intelligence, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.23151.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#cv', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективная оценка оптического потока с минимальным использованием памяти', 'desc': 'MEMFOF - это метод оптического потока, который эффективно использует память GPU для высокоразрешающих входных данных. Он достигает современного уровня производительности на нескольких эталонных тестах, включая Spring, Sintel и KITTI-2015. MEMFOF использует многокадровую оценку и пересмотренную архитектуру RAFT для оптимизации соотношения точности и потребления памяти. Метод требует всего 2,09 ГБ памяти GPU во время выполнения для входных данных 1080p, что позволяет обучать его на нативном разрешении без обрезки или понижения дискретизации.'}, 'en': {'title': 'MEMFOF: Efficient Optical Flow for High-Resolution Images', 'desc': 'MEMFOF is a novel optical flow estimation method designed to be memory-efficient while maintaining high accuracy for high-resolution images. It significantly reduces GPU memory usage during both training and inference, allowing for full 1080p processing without the need for downsampling. By optimizing design choices from existing architectures and incorporating multi-frame estimation, MEMFOF achieves state-of-the-art performance across various benchmarks. This method not only excels in accuracy but also enhances runtime efficiency, making it a robust solution for real-time applications in optical flow estimation.'}, 'zh': {'title': 'MEMFOF：高效光流估计的新选择', 'desc': 'MEMFOF是一种内存高效的多帧光流估计方法，能够在高分辨率输入下实现最先进的性能，同时减少GPU内存使用。该方法在1080p输入时仅需2.09 GB的GPU内存，训练时为28.5 GB，使其能够在原生1080p下进行训练，而无需裁剪或下采样。MEMFOF通过整合减少的相关体积和高分辨率训练协议，优化了RAFT类架构的设计选择，从而在多个基准测试中实现了卓越的性能。与其他资源密集型方法相比，MEMFOF在准确性和运行效率上均表现出色，验证了其在高分辨率光流估计中的稳健性。'}}}, {'id': 'https://huggingface.co/papers/2506.23542', 'title': 'Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention', 'url': 'https://huggingface.co/papers/2506.23542', 'abstract': 'A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.', 'score': 12, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '9c9286ea4d796818', 'authors': ['Weida Wang', 'Changyong He', 'Jin Zeng', 'Di Qiu'], 'affiliations': ['Google', 'School of Computer Science and Technology, Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23542.jpg', 'data': {'categories': ['#cv', '#optimization', '#dataset', '#graphs', '#synthetic', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Революционное шумоподавление ToF: стабильность и четкость через графовое слияние', 'desc': 'Предложена новая нейронная сеть для шумоподавления в датчиках глубины Time-of-Flight (ToF), использующая инвариантное к движению слияние графов и адаптивные фильтры. Сеть одновременно улучшает временную стабильность и пространственную четкость изображений глубины. Метод основан на самоподобии графовых структур во времени, что позволяет применять межкадровое геометрическое внимание для слияния графов. Экспериментальные результаты показывают, что предложенный подход достигает наилучших показателей точности и согласованности на синтетических и реальных наборах данных.'}, 'en': {'title': 'Enhancing ToF Depth Images with Motion-Invariant Graph Fusion', 'desc': 'This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets.'}, 'zh': {'title': '提升ToF深度图像的去噪效果', 'desc': '本文提出了一种新颖的时间飞行（ToF）深度去噪网络，利用运动不变图融合和自适应滤波器来提高时间稳定性和空间清晰度。以往的研究主要集中在单帧处理或多帧处理，但未考虑不同帧中对应像素的深度变化，导致时间不一致和空间模糊。我们的方法通过图结构的时间自相似性，实现跨帧几何注意力的图融合，并结合图的平滑性先验和ToF噪声分布的数据信度项，构建最大后验去噪问题。实验结果表明，该方案在合成DVToF数据集上实现了最先进的性能，并在真实Kinectv2数据集上展现了良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.16500', 'title': 'SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity', 'url': 'https://huggingface.co/papers/2506.16500', 'abstract': 'SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.', 'score': 11, 'issue_id': 4572, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '8a6cd2aa2a56bf51', 'authors': ['Samir Khaki', 'Xiuyu Li', 'Junxian Guo', 'Ligeng Zhu', 'Chenfeng Xu', 'Konstantinos N. Plataniotis', 'Amir Yazdanbakhsh', 'Kurt Keutzer', 'Song Han', 'Zhijian Liu'], 'affiliations': ['Google DeepMind', 'MIT', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.16500.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'SparseLoRA: Ускорение тонкой настройки LLM без потери качества', 'desc': 'SparseLoRA - это новый метод ускорения тонкой настройки больших языковых моделей (LLM). Он использует контекстную разреженность для динамического выбора подмножества весов для вычисления потерь и градиентов. Метод основан на легковесном SVD-оценщике разреженности, не требующем обучения. Эксперименты показывают, что SparseLoRA снижает вычислительные затраты до 2,2 раз и ускоряет процесс до 1,6 раз, сохраняя точность на различных задачах.'}, 'en': {'title': 'SparseLoRA: Speeding Up LLM Fine-Tuning with Smart Sparsity', 'desc': 'SparseLoRA is a novel method designed to enhance the efficiency of fine-tuning large language models (LLMs) by utilizing a sparse subset of weights for loss and gradient calculations. Unlike previous methods that only reduce the number of trainable parameters, SparseLoRA significantly cuts down on computational costs and speeds up the fine-tuning process. It employs a lightweight, training-free singular value decomposition (SVD) estimator to dynamically select which weights to use, ensuring optimal performance across different layers and training steps. Experimental results demonstrate that SparseLoRA can reduce computational costs by up to 2.2 times and improve speed by up to 1.6 times, all while maintaining high accuracy on various tasks.'}, 'zh': {'title': 'SparseLoRA：加速大规模语言模型微调的稀疏方法', 'desc': 'SparseLoRA是一种新方法，通过动态选择稀疏权重子集来减少大规模语言模型（LLM）微调的计算成本和加速过程。与其他参数高效的微调方法相比，SparseLoRA不仅降低了内存使用，还显著提高了计算效率。该方法使用轻量级的训练无关奇异值分解（SVD）稀疏性估计器，能够在训练过程中实时选择需要计算的权重。实验结果表明，SparseLoRA在保持准确性的同时，计算成本降低了最多2.2倍，速度提升了最多1.6倍。'}}}, {'id': 'https://huggingface.co/papers/2506.17417', 'title': 'Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?', 'url': 'https://huggingface.co/papers/2506.17417', 'abstract': 'Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.', 'score': 9, 'issue_id': 4570, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': '57576e8287f4515e', 'authors': ['Mingyuan Wu', 'Meitang Li', 'Jingcheng Yang', 'Jize Jiang', 'Kaizhuo Yan', 'Zhaoheng Li', 'Minjia Zhang', 'Klara Nahrstedt'], 'affiliations': ['University of Illinois Urbana Champaign', 'University of Michigan Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2506.17417.jpg', 'data': {'categories': ['#rl', '#reasoning', '#inference', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений в визуально-языковых моделях: победа генерации над верификацией', 'desc': 'Исследование показывает, что методы вывода, такие как масштабирование во время декодирования и самоуточнение, улучшают способности рассуждения в визуально-языковых моделях. Генеративные методы, например мажоритарное голосование, оказались эффективнее методов верификации. Интересно, что модели, обученные с помощью обучения с подкреплением, не продемонстрировали ожидаемых преимуществ в самокоррекции. Основной причиной этого является отсутствие у таких моделей надежных возможностей самопроверки в визуальной и текстовой модальностях.'}, 'en': {'title': 'Boosting VLM Reasoning with Inference-Time Techniques', 'desc': 'This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities.'}, 'zh': {'title': '推理时技术提升视觉-语言模型的推理能力', 'desc': '本文探讨了推理时技术如何提升视觉-语言模型（VLM）的推理能力。研究发现，解码时间缩放和自我修正等技术在没有外部知识的情况下显著改善了模型的表现。尽管强化学习（RL）训练的模型未能显示出自我修正的优势，但基于生成的方法在提升性能方面明显优于基于验证的方法。实验结果表明，RL训练的VLM在视觉和文本模态上仍缺乏强大的自我验证能力。'}}}, {'id': 'https://huggingface.co/papers/2506.22598', 'title': 'RExBench: Can coding agents autonomously implement AI research\n  extensions?', 'url': 'https://huggingface.co/papers/2506.22598', 'abstract': 'RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.', 'score': 6, 'issue_id': 4582, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '236eb718627b6f97', 'authors': ['Nicholas Edwards', 'Yukyung Lee', 'Yujun', 'Mao', 'Yulu Qin', 'Sebastian Schuster', 'Najoung Kim'], 'affiliations': ['Boston University', 'University College London', 'University of Vienna'], 'pdf_title_img': 'assets/pdf/title_img/2506.22598.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': '🧪', 'ru': {'title': 'LLM-агенты пока не готовы к самостоятельным научным исследованиям', 'desc': 'RExBench - это новый бенчмарк для оценки способности агентов на основе больших языковых моделей (LLM) автономно реализовывать расширения исследований. Он включает 12 реалистичных задач по реализации экспериментов, основанных на существующих научных работах и кодовых базах. Результаты показывают, что современные агенты не способны успешно выполнять большинство задач без значительной помощи человека. Даже с дополнительными подсказками лучший результат не превышает 40% успешных реализаций.'}, 'en': {'title': 'Evaluating LLM Agents: The Challenge of Autonomous Research Extensions', 'desc': 'RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints.'}, 'zh': {'title': '评估LLM代理的研究扩展能力', 'desc': 'RExBench是一个评估大型语言模型（LLM）代理在自主实现研究扩展能力的基准工具。研究表明，现有的LLM代理在没有人类提示的情况下，无法成功完成大多数研究扩展任务。尽管在提供额外的人类提示后，成功率有所提高，但仍然低于40%。这表明当前的代理在处理现实研究扩展任务时，仍然需要大量的人类指导。'}}}, {'id': 'https://huggingface.co/papers/2506.23219', 'title': 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding', 'url': 'https://huggingface.co/papers/2506.23219', 'abstract': 'UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.', 'score': 5, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '86191833a27a1741', 'authors': ['Jie Feng', 'Shengyuan Wang', 'Tianhui Liu', 'Yanxin Xi', 'Yong Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University, Beijing, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University, China', 'University of Helsinki, Finland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23219.jpg', 'data': {'categories': ['#dataset', '#science', '#training', '#multimodal', '#benchmark', '#open_source'], 'emoji': '🏙️', 'ru': {'title': 'UrbanLLaVA: Революция в анализе городских данных с помощью мультимодального ИИ', 'desc': 'UrbanLLaVA - это мультимодальная большая языковая модель, разработанная для обработки городских данных. Модель способна эффективно работать с четырьмя типами данных одновременно, превосходя существующие модели как в одномодальных, так и в сложных кросс-модальных сценариях. Авторы предложили многоэтапную структуру обучения, которая разделяет улучшение пространственного рассуждения и изучение предметных знаний. Экспериментальные результаты показывают, что UrbanLLaVA превосходит открытые и проприетарные мультимодальные языковые модели в различных городских задачах.'}, 'en': {'title': 'UrbanLLaVA: Revolutionizing Urban Data Processing with Multi-Modal AI', 'desc': 'UrbanLLaVA is a multi-modal large language model specifically designed to handle various urban datasets, excelling in both single-modal and complex cross-modal tasks. It addresses the limitations of existing models by providing a unified framework that processes multiple types of urban data simultaneously. The model is trained using a diverse urban instruction dataset and employs a multi-stage training approach to enhance spatial reasoning and domain knowledge. Experimental results indicate that UrbanLLaVA significantly outperforms other models in urban research, demonstrating strong generalization across different cities.'}, 'zh': {'title': 'UrbanLLaVA：城市数据处理的新突破', 'desc': 'UrbanLLaVA是一种多模态大型语言模型，能够有效处理城市数据集，适用于多种任务。与现有模型相比，它在单模态和复杂跨模态场景中表现更佳。该模型通过构建多样化的城市指令数据集，并采用多阶段训练框架，提升了空间推理和领域知识的学习能力。实验结果表明，UrbanLLaVA在多个城市的任务中均优于其他开源和专有的多模态大型语言模型。'}}}, {'id': 'https://huggingface.co/papers/2506.22992', 'title': 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning', 'url': 'https://huggingface.co/papers/2506.22992', 'abstract': 'MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.', 'score': 5, 'issue_id': 4574, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '19689e84c5482c65', 'authors': ['Yulun Jiang', 'Yekun Chai', 'Maria Brbić', 'Michael Moor'], 'affiliations': ['EPFL', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2506.22992.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'MARBLE: Вызов мультимодальным языковым моделям в сложных рассуждениях', 'desc': 'MARBLE - это сложный мультимодальный бенчмарк для оценки рассуждений, который выявляет ограничения существующих мультимодальных языковых моделей в пошаговом рассуждении и создании планов с учетом пространственных и визуальных ограничений. Бенчмарк состоит из двух задач: M-Portal и M-Cube, требующих составления и понимания многоэтапных планов. Текущие мультимодальные языковые модели показывают низкую производительность на MARBLE, демонстрируя случайные результаты на M-Portal и 0% точности на M-Cube. MARBLE призван стимулировать разработку нового поколения моделей, способных рассуждать и планировать в мультимодальных задачах.'}, 'en': {'title': 'MARBLE: Unveiling the Limits of Multimodal Reasoning in AI', 'desc': 'MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning.'}, 'zh': {'title': 'MARBLE：揭示多模态推理的挑战', 'desc': 'MARBLE是一个具有挑战性的多模态推理基准，旨在揭示现有多模态语言模型在逐步推理和计划制定方面的局限性。该基准包含两个高度挑战性的任务，M-Portal和M-Cube，要求在空间、视觉和物理约束下进行多步骤计划的制定和理解。研究发现，当前的多模态语言模型在MARBLE上的表现不佳，所有12个先进模型在M-Portal上的表现接近随机，而在M-Cube上的准确率为0%。通过揭示多模态语言模型的局限性，MARBLE希望推动下一代模型的发展，使其能够在多模态推理步骤中进行有效的推理和计划。'}}}, {'id': 'https://huggingface.co/papers/2506.23394', 'title': 'Teaching a Language Model to Speak the Language of Tools', 'url': 'https://huggingface.co/papers/2506.23394', 'abstract': 'A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.', 'score': 4, 'issue_id': 4570, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '19b24559e749a260', 'authors': ['Simeon Emanuilov'], 'affiliations': ['Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski'], 'pdf_title_img': 'assets/pdf/title_img/2506.23394.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#benchmark', '#open_source', '#training'], 'emoji': '🔧', 'ru': {'title': 'Многоязычное расширение возможностей ИИ: точные вызовы функций на любом языке', 'desc': 'Представлена методология адаптации языковых моделей для надежного использования инструментов на разных языках, в частности для повышения точности вызова функций на болгарском языке. Исследование включает дообучение серии моделей BgGPT на двуязычном наборе данных с примерами вызовов функций. Разработанная система TUCAN достигает значительного улучшения точности вызова функций по сравнению с базовыми моделями. Результаты демонстрируют практический подход к расширению возможностей использования инструментов за пределами англоязычных систем.'}, 'en': {'title': 'Empowering Multilingual Models for Effective Tool Use', 'desc': 'This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications.'}, 'zh': {'title': '提升多语言工具使用能力的创新方法', 'desc': '本文提出了一种方法，旨在提高语言模型在多语言环境中工具使用的准确性，特别是保加利亚语的功能调用准确性。大多数多语言模型在非英语语言中缺乏可靠的工具使用能力，尤其是在低资源语言中表现不佳。研究通过对BgGPT模型系列进行持续训练，使用一个包含10,035个功能调用示例的双语数据集，来增强模型的工具使用能力。最终，研究推出的TUCAN模型在功能调用准确性上比基础模型提高了28.75%，并且在响应格式上也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.21448', 'title': 'ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing', 'url': 'https://huggingface.co/papers/2506.21448', 'abstract': 'ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io.', 'score': 4, 'issue_id': 4573, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'fefdbdbfb0394a3c', 'authors': ['Huadai Liu', 'Jialei Wang', 'Kaicheng Luo', 'Wen Wang', 'Qian Chen', 'Zhou Zhao', 'Wei Xue'], 'affiliations': ['Hong Kong University of Science and Technology (HKUST)', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21448.jpg', 'data': {'categories': ['#dataset', '#games', '#multimodal', '#benchmark', '#reasoning', '#audio'], 'emoji': '🎬', 'ru': {'title': 'Думай как звукорежиссер: ИИ создает аудио для видео с помощью рассуждений', 'desc': 'ThinkSound - это новая система для генерации высококачественного аудио из видео с использованием рассуждений по цепочке мыслей (Chain-of-Thought) и мультимодальной большой языковой модели. Система работает в три этапа: создание базового звукового ландшафта, интерактивное уточнение звуков отдельных объектов и целенаправленное редактирование с помощью естественно-языковых инструкций. Авторы также представили датасет AudioCoT с аннотациями структурированных рассуждений, связывающих визуальный контент, текстовые описания и синтез звука. Эксперименты показали, что ThinkSound достигает наилучших результатов в генерации аудио из видео по различным метрикам.'}, 'en': {'title': 'ThinkSound: Revolutionizing Video-to-Audio Generation with Chain-of-Thought Reasoning', 'desc': 'ThinkSound is a new framework that enhances video-to-audio generation by using Chain-of-Thought reasoning with a multimodal large language model. It addresses the challenge of creating high-quality audio that accurately reflects the visual content by breaking the process into three stages: foundational foley generation, interactive object-centric refinement, and targeted editing. Each stage utilizes contextually aligned reasoning to guide the audio generation, ensuring that the soundscapes are semantically coherent and tailored to user inputs. The framework also introduces the AudioCoT dataset, which connects visual elements, text descriptions, and sound synthesis, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'ThinkSound：视频生成高保真音频的新方法', 'desc': 'ThinkSound是一个新颖的框架，利用链式思维推理与多模态大语言模型，从视频生成高质量音频，达到了各项基准测试的最先进结果。尽管端到端的视频到音频生成技术已有显著进步，但生成真实捕捉视觉内容细微差别的高保真音频仍然具有挑战性。该框架将生成过程分为三个互补阶段：基础音效生成、交互式对象中心细化和基于自然语言指令的目标编辑。通过引入AudioCoT数据集，ThinkSound在视频到音频生成的实验中表现出色，尤其在音频指标和链式思维指标上均取得了领先成绩。'}}}, {'id': 'https://huggingface.co/papers/2506.22694', 'title': 'VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs', 'url': 'https://huggingface.co/papers/2506.22694', 'abstract': 'A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.', 'score': 3, 'issue_id': 4573, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'a9c9ccf73356a002', 'authors': ['Raghavv Goel', 'Sudhanshu Agrawal', 'Mukul Gagrani', 'Junyoung Park', 'Yifan Zao', 'He Zhang', 'Tian Liu', 'Yiping Yang', 'Xin Yuan', 'Jiuyan Lu', 'Chris Lott', 'Mingu Lee'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.22694.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': '✂️', 'ru': {'title': 'Ускорение генерации текста путем обрезки словаря', 'desc': 'Эта статья представляет технику VocabTrim, которая улучшает спекулятивное декодирование на основе драфтера путем уменьшения словаря языковой модели-драфтера. VocabTrim реконструирует head-часть языковой модели-драфтера, оставляя только ограниченный набор наиболее часто используемых токенов из словаря целевой модели. Хотя это немного снижает коэффициент принятия, значительно уменьшается задержка при создании черновика в условиях ограниченной памяти. Метод показал увеличение скорости генерации на 16% для модели Llama-3.2-3B-Instruct в условиях ограниченной памяти.'}, 'en': {'title': 'Speed Up Drafting with VocabTrim!', 'desc': 'This paper presents VocabTrim, a technique designed to enhance drafter-based speculative decoding by optimizing the vocabulary used in the drafting process. By limiting the vocabulary to only the most frequently sampled tokens from the target model, VocabTrim reduces the inference overhead associated with drafting, particularly in memory-constrained environments. Although this approach may slightly lower the acceptance rate of generated sequences, it significantly accelerates drafting speed, especially on edge devices. The results demonstrate a notable 16% improvement in memory-bound speed-up for Llama-3 models on Spec-Bench, showcasing the effectiveness of this method.'}, 'zh': {'title': 'VocabTrim：提升推测解码速度的关键技术', 'desc': '本文提出了一种名为VocabTrim的技术，旨在通过减少草拟语言模型的词汇量来改善基于草拟的推测解码性能。这种方法可以降低在内存受限环境中的草拟延迟，特别是对于具有大词汇量的目标语言模型。VocabTrim通过重构草拟语言模型的头部，仅保留目标模型中最常被采样的有限词汇，从而提高生成速度。尽管限制词汇会略微降低接受率，但在边缘设备上显著提高了内存受限速度。'}}}, {'id': 'https://huggingface.co/papers/2506.23135', 'title': 'RoboScape: Physics-informed Embodied World Model', 'url': 'https://huggingface.co/papers/2506.23135', 'abstract': 'RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.', 'score': 2, 'issue_id': 4574, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 июня', 'en': 'June 29', 'zh': '6月29日'}, 'hash': '421522bfdd825b96', 'authors': ['Yu Shang', 'Xin Zhang', 'Yinzhou Tang', 'Lei Jin', 'Chen Gao', 'Wei Wu', 'Yong Li'], 'affiliations': ['Manifold AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23135.jpg', 'data': {'categories': ['#optimization', '#science', '#open_source', '#robotics', '#video', '#games', '#3d'], 'emoji': '🤖', 'ru': {'title': 'RoboScape: физически достоверная генерация видео для продвижения воплощенного ИИ', 'desc': 'RoboScape - это унифицированная физически-информированная мировая модель для генерации видео роботов. Она объединяет предсказание временной глубины и обучение динамике ключевых точек для улучшения визуальной достоверности и физической правдоподобности. Модель совместно обучается генерации RGB-видео и физическим знаниям в интегрированной структуре. Эксперименты показывают превосходное качество генерируемых видео и практическую пользу для обучения и оценки политик роботов.'}, 'en': {'title': 'RoboScape: Realistic Robotic Video Generation with Physics Awareness', 'desc': 'RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos.'}, 'zh': {'title': 'RoboScape：提升机器人视频生成的物理真实感', 'desc': 'RoboScape 是一个统一的物理信息世界模型，通过整合时间深度预测和关键点动态学习，提升了机器人视频生成的视觉真实感和物理合理性。当前的世界模型在建模三维几何和运动动态方面存在局限，导致生成的视频在接触丰富的机器人场景中不够真实。本文提出的 RoboScape 通过联合学习 RGB 视频生成和物理知识，采用了两个关键的物理信息联合训练任务，增强了视频渲染中的三维几何一致性。实验结果表明，RoboScape 在多种机器人场景中生成的视频具有更高的视觉真实感和物理合理性，并在下游应用中验证了其实用性。'}}}, {'id': 'https://huggingface.co/papers/2506.22753', 'title': 'Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography', 'url': 'https://huggingface.co/papers/2506.22753', 'abstract': 'The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': 'a52ba639d6bd763f', 'authors': ['Jianing Zhang', 'Jiayi Zhu', 'Feiyu Ji', 'Xiaokang Yang', 'Xiaoyun Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.22753.jpg', 'data': {'categories': ['#hallucinations', '#cv', '#data', '#inference', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Улучшение качества изображений с металинз без масштабных датасетов', 'desc': 'Предложена новая модель Degradation-Modeled Multipath Diffusion для улучшения качества изображений, получаемых с помощью металинз. Модель использует априорную информацию о естественных изображениях и специальные модули для балансировки детализации, точности и визуального качества. Особое внимание уделяется компенсации оптических искажений, характерных для металинз. Система не требует большого набора парных данных или точной оптической калибровки, что упрощает ее применение в реальных условиях.'}, 'en': {'title': 'Enhancing Metalens Imaging with Adaptive Degradation Modeling', 'desc': 'The Degradation-Modeled Multipath Diffusion framework enhances the image quality of metalenses by integrating natural image priors and specialized modules. It effectively addresses the challenges of optical degradation while balancing detail, fidelity, and perceptual quality. Unlike traditional methods that require extensive calibration or large datasets, this approach utilizes pretrained models and a tunable decoder for better control over the inference process. The introduction of a spatially varying degradation-aware attention module allows for adaptive modeling of complex degradations, leading to superior image reconstruction results.'}, 'zh': {'title': '降解建模提升金属透镜图像质量', 'desc': '本文提出了一种降解建模的多路径扩散框架，旨在通过利用自然图像先验来改善金属透镜的图像质量。该框架通过特定模块平衡细节、保真度和感知质量，同时解决光学降解问题。与传统方法不同，我们的方法不依赖于精确的光学校准或庞大的配对数据集，而是使用预训练模型的强大自然图像先验。最终，我们的实验结果表明，该方法在高保真和清晰图像重建方面优于现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2506.17080', 'title': 'Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs', 'url': 'https://huggingface.co/papers/2506.17080', 'abstract': 'Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': 'f35759eeab0ef755', 'authors': ['Ricardo Rei', 'Nuno M. Guerreiro', 'José Pombal', 'João Alves', 'Pedro Teixeirinha', 'Amin Farajian', 'André F. T. Martins'], 'affiliations': ['Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicações', 'MICS, CentraleSupélec, Université Paris-Saclay', 'Unbabel'], 'pdf_title_img': 'assets/pdf/title_img/2506.17080.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#optimization', '#dataset', '#training', '#machine_translation'], 'emoji': '🗼', 'ru': {'title': 'Tower+: Языковые модели, объединяющие перевод и многоязычные задачи', 'desc': 'Tower+ представляет собой набор языковых моделей, достигающих высокой производительности как в задачах перевода, так и в многоязычных задачах общего назначения. Модели используют новую методику обучения, включающую дополнительное предварительное обучение, контролируемую настройку, оптимизацию предпочтений и обучение с подкреплением. Tower+ демонстрирует, что возможно достичь баланса между специализацией на переводе и многоязычными возможностями общего назначения. Модели Tower+ часто превосходят более крупные языковые модели общего назначения в различных задачах.'}, 'en': {'title': 'Tower+: Bridging Translation and General-Purpose Language Tasks', 'desc': 'Tower+ is a suite of fine-tuned language models that excels in both translation and multilingual text tasks. The models are trained using a unique recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning, allowing them to maintain strong general-purpose capabilities while specializing in translation. By carefully curating data at each training stage, Tower+ achieves a balance between translation performance and general tasks like code generation and problem-solving. The results show that even smaller models can outperform larger existing models, demonstrating the effectiveness of this training approach.'}, 'zh': {'title': 'Tower+: 翻译与多语言能力的完美平衡', 'desc': 'Tower+是一套经过精细调优的语言模型，能够在翻译和多语言通用文本任务中表现出色。通过一种新颖的训练方法，包括持续预训练、监督微调、偏好优化和强化学习，Tower+实现了翻译专业化与多语言通用能力之间的平衡。我们在训练的每个阶段精心生成和整理数据，以增强翻译和通用任务的表现。我们的模型在多个规模上开发，较小的模型在特定任务上常常超越更大的通用模型。'}}}, {'id': 'https://huggingface.co/papers/2507.07105', 'title': '4KAgent: Agentic Any Image to 4K Super-Resolution', 'url': 'https://huggingface.co/papers/2507.07105', 'abstract': '4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.', 'score': 49, 'issue_id': 4743, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '3ef4be8673ff0eed', 'authors': ['Yushen Zuo', 'Qi Zheng', 'Mingyang Wu', 'Xinrui Jiang', 'Renjie Li', 'Jian Wang', 'Yide Zhang', 'Gengchen Mai', 'Lihong V. Wang', 'James Zou', 'Xiaoyu Wang', 'Ming-Hsuan Yang', 'Zhengzhong Tu'], 'affiliations': ['CU Boulder', 'California Institute of Technology', 'Snap Inc.', 'Stanford University', 'Texas A&M University', 'Topaz Labs', 'UC Merced', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.07105.jpg', 'data': {'categories': ['#low_resource', '#cv', '#open_source', '#benchmark', '#agents', '#healthcare'], 'emoji': '🔬', 'ru': {'title': '4KAgent: Революция в улучшении изображений с помощью ИИ', 'desc': '4KAgent - это унифицированная агентная система сверхвысокого разрешения, которая улучшает изображения низкого разрешения до 4K. Система состоит из трех основных компонентов: модуля профилирования, агента восприятия и агента восстановления. 4KAgent использует модели компьютерного зрения и обработки естественного языка для анализа входного изображения и создания плана восстановления. Система показывает передовые результаты на 26 различных тестовых наборах данных, охватывающих широкий спектр областей применения.'}, 'en': {'title': 'Transforming Low-Res to Stunning 4K with 4KAgent!', 'desc': '4KAgent is an advanced super-resolution system that enhances low-resolution images to 4K quality using a combination of specialized agents. It features three main components: Profiling for customizing the process, a Perception Agent that analyzes images and creates restoration plans, and a Restoration Agent that implements these plans using a quality-driven approach. The system excels in transforming severely degraded images into high-quality outputs, including a dedicated module for improving facial details in portraits. Rigorous evaluations across various imaging tasks demonstrate its state-of-the-art performance, setting new benchmarks in multiple domains such as natural images, medical imaging, and AI-generated content.'}, 'zh': {'title': '4KAgent：超分辨率图像处理的新纪元', 'desc': '4KAgent是一种统一的智能超分辨率系统，能够将低分辨率图像提升至4K分辨率。该系统通过三个核心组件实现：配置模块、感知代理和恢复代理，能够根据不同的使用场景定制处理流程。4KAgent特别适用于处理严重失真的图像，并能显著提升人脸细节，适合肖像和自拍照片。通过在多个成像领域的严格评估，4KAgent在图像质量和保真度方面均表现出色，推动了低级视觉任务的创新。'}}}, {'id': 'https://huggingface.co/papers/2507.07095', 'title': 'Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data', 'url': 'https://huggingface.co/papers/2507.07095', 'abstract': 'A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes.', 'score': 39, 'issue_id': 4738, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'd8bd0a82b6576b80', 'authors': ['Ke Fan', 'Shunlin Lu', 'Minyue Dai', 'Runyi Yu', 'Lixing Xiao', 'Zhiyang Dou', 'Junting Dong', 'Lizhuang Ma', 'Jingbo Wang'], 'affiliations': ['CUHK, Shenzhen', 'East China Normal University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07095.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#benchmark', '#dataset', '#robotics', '#games'], 'emoji': '🤖', 'ru': {'title': 'Революция в генерации движений: от текста к реальности', 'desc': 'Исследователи представили новый набор данных MotionMillion и систему оценки MotionMillion-Eval для улучшения генерации движений по текстовому описанию с нулевым обучением. MotionMillion содержит более 2 миллионов высококачественных последовательностей движений, что делает его крупнейшим набором данных о движениях человека на сегодняшний день. Авторы также разработали масштабируемую архитектуру модели с 7 миллиардами параметров для решения этой задачи. Результаты демонстрируют сильную генерализацию на движения вне обучающей выборки и сложные составные движения, что является значительным шагом к генерации движений человека с нулевым обучением.'}, 'en': {'title': 'Revolutionizing Zero-Shot Motion Generation with MotionMillion', 'desc': 'This paper presents a new dataset and evaluation framework aimed at enhancing zero-shot text-to-motion generation. The authors introduce MotionMillion, the largest dataset of human motion sequences, which includes over 2 million high-quality motions. They also propose MotionMillion-Eval, a comprehensive benchmark for assessing the performance of zero-shot motion generation models. By utilizing a scalable model architecture with 7 billion parameters, the study demonstrates improved generalization capabilities for generating complex human motions from textual descriptions.'}, 'zh': {'title': '推动零-shot人类动作生成的新纪元', 'desc': '本文提出了一种新的数据集和评估框架，以改善零-shot文本到动作生成的能力。我们开发了MotionMillion，这是迄今为止最大的高质量人类动作数据集，包含超过2000小时和200万条动作序列。通过引入MotionMillion-Eval，我们建立了一个全面的基准来评估零-shot动作生成的效果。我们的模型在7B参数的可扩展架构下表现出色，能够有效地生成复杂的动作序列，标志着零-shot人类动作生成的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2507.06448', 'title': 'Perception-Aware Policy Optimization for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2507.06448', 'abstract': 'Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.', 'score': 32, 'issue_id': 4739, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '371bd96113b91da3', 'authors': ['Zhenhailong Wang', 'Xuehang Guo', 'Sofia Stoica', 'Haiyang Xu', 'Hongru Wang', 'Hyeonjeong Ha', 'Xiusi Chen', 'Yangyi Chen', 'Ming Yan', 'Fei Huang', 'Heng Ji'], 'affiliations': ['Alibaba Group', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.06448.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#rl', '#rlhf', '#optimization'], 'emoji': '👁️', 'ru': {'title': 'Улучшение восприятия в мультимодальном обучении с подкреплением', 'desc': 'Статья представляет метод Perception-Aware Policy Optimization (PAPO) для улучшения обучения с подкреплением в задачах мультимодального рассуждения. PAPO интегрирует неявную функцию потерь восприятия в целевую функцию GRPO, что позволяет модели одновременно учиться воспринимать и рассуждать. Этот подход значительно улучшает общую производительность на различных мультимодальных бенчмарках, особенно в задачах с высокой зависимостью от зрения. Авторы также выявили и решили проблему взлома функции потерь с помощью двойной энтропийной функции потерь.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Perception-Aware Learning', 'desc': 'Perception-Aware Policy Optimization (PAPO) is a novel approach that enhances reinforcement learning by integrating implicit perception loss to improve visual reasoning in multimodal tasks. It addresses the limitations of existing methods that primarily focus on textual data, leading to errors in visual input perception. By introducing a KL divergence term to the GRPO objective, PAPO significantly boosts performance on multimodal benchmarks, especially in vision-dependent tasks. This method not only reduces perception errors but also establishes a new framework for visually grounded reasoning in reinforcement learning.'}, 'zh': {'title': '提升视觉推理的感知优化策略', 'desc': '感知意识策略优化（PAPO）通过整合隐式感知损失，增强了强化学习在多模态推理中的可验证奖励。该方法改善了视觉感知和推理能力，特别是在处理视觉输入时显著减少了错误。PAPO是对GRPO的有效扩展，能够在没有额外数据或外部奖励模型的情况下，利用内部监督信号进行学习。实验结果显示，PAPO在多模态基准测试中提高了4.4%的性能，尤其在视觉依赖性高的任务中，提升接近8.0%。'}}}, {'id': 'https://huggingface.co/papers/2507.06920', 'title': 'Rethinking Verification for LLM Code Generation: From Generation to\n  Testing', 'url': 'https://huggingface.co/papers/2507.06920', 'abstract': 'A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.', 'score': 22, 'issue_id': 4738, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '93b419a1fb85f819', 'authors': ['Zihan Ma', 'Taolin Zhang', 'Maosong Cao', 'Wenwei Zhang', 'Minnan Luo', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['MOE KLINNS Lab, Xian Jiaotong University, China', 'School of Computer Science and Technology, Xian Jiaotong University, China', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.06920.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#dataset', '#training', '#games'], 'emoji': '🧪', 'ru': {'title': 'Человек и ИИ объединяются для создания надежных тестов кода', 'desc': 'Статья представляет новый метод SAGA для генерации тестовых случаев, объединяющий человеческий опыт и возможности больших языковых моделей. Авторы вводят многомерные метрики для оценки полноты набора тестов и создают специальный бенчмарк TCGBench. Эксперименты показывают, что SAGA достигает 90.62% обнаружения ошибок и 32.58% точности верификатора на TCGBench. Метод улучшает надежность оценки генерации кода большими языковыми моделями.'}, 'en': {'title': 'Enhancing Code Reliability with Human-LLM Collaboration', 'desc': 'This paper presents a method called SAGA that combines human expertise with large language models (LLMs) to improve the generation of test cases for code evaluation. The authors highlight that existing benchmarks often miss subtle errors due to their limited and similar test cases, which can lead to misleading performance metrics. By introducing multi-dimensional metrics and a new test-case generation benchmark (TCGBench), they rigorously assess the thoroughness of test suites. The results show that SAGA significantly enhances detection rates and verifier accuracy, indicating its potential to improve the reliability of LLMs in code generation tasks.'}, 'zh': {'title': '人机协作提升代码测试生成的可靠性', 'desc': '本文提出了一种人类与大型语言模型（LLM）协作的方法，以增强代码生成中的测试用例生成，提升代码评估基准的可靠性和检测率。研究发现，现有的评估套件通常只包含有限的同质测试用例，导致一些细微错误未被发现，从而影响了性能评估的准确性。为了解决这些问题，本文提出了多维度指标来量化测试套件的全面性，并引入了SAGA方法，结合人类编程专家与LLM的推理能力，显著提高了生成测试用例的覆盖率和质量。实验结果表明，SAGA在TCGBench上的检测率达到90.62%，验证器准确率为32.58%，显示了该方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.06457', 'title': 'A Systematic Analysis of Hybrid Linear Attention', 'url': 'https://huggingface.co/papers/2507.06457', 'abstract': 'Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers face quadratic complexity and memory issues with long sequences, prompting the adoption of linear attention mechanisms using fixed-size hidden states. However, linear models often suffer from limited recall performance, leading to hybrid architectures that combine linear and full attention layers. Despite extensive hybrid architecture research, the choice of linear attention component has not been deeply explored. We systematically evaluate various linear attention models across generations - vector recurrences to advanced gating mechanisms - both standalone and hybridized. To enable this comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six linear attention variants across five hybridization ratios. Benchmarking on standard language modeling and recall tasks reveals that superior standalone linear models do not necessarily excel in hybrids. While language modeling remains stable across linear-to-full attention ratios, recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. Our study highlights selective gating, hierarchical recurrence, and controlled forgetting as critical for effective hybrid models. We recommend architectures such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall efficiently. Our models are open-sourced at https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.', 'score': 16, 'issue_id': 4739, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '31144b92e85957ae', 'authors': ['Dustin Wang', 'Rui-Jie Zhu', 'Steven Abreu', 'Yong Shan', 'Taylor Kergan', 'Yuqi Pan', 'Yuhong Chou', 'Zheng Li', 'Ge Zhang', 'Wenhao Huang', 'Jason Eshraghian'], 'affiliations': ['ByteDance Seed', 'CASIA', 'M-A-P', 'PolyU', 'UC Santa Cruz', 'University of Groningen'], 'pdf_title_img': 'assets/pdf/title_img/2507.06457.jpg', 'data': {'categories': ['#dataset', '#training', '#open_source', '#benchmark', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Гибридные архитектуры внимания: оптимизация производительности и эффективности', 'desc': 'Исследование оценивает различные модели линейного внимания и их интеграцию с полным вниманием в трансформерах. Ключевыми механизмами для улучшения производительности запоминания определены селективное гейтирование и иерархическая рекуррентность. Авторы обучили и открыли исходный код 72 моделей с различными параметрами и вариантами линейного внимания. Результаты показывают, что лучшие автономные линейные модели не обязательно превосходят в гибридных архитектурах.'}, 'en': {'title': 'Enhancing Transformer Recall with Hybrid Linear Attention Models', 'desc': 'This research investigates different linear attention models and their combination with full attention in Transformers to improve recall performance. It identifies important mechanisms like selective gating and hierarchical recurrence that enhance the effectiveness of these hybrid models. The study systematically evaluates 72 models, revealing that the best standalone linear models do not always perform well in hybrid settings. The findings suggest optimal architectures and ratios for combining linear and full attention to achieve better recall in language tasks.'}, 'zh': {'title': '优化变换器的混合注意力模型', 'desc': '本研究评估了各种线性注意力模型及其与全注意力在变换器中的结合，识别出选择性门控和层次递归等关键机制，以提高回忆性能。变换器在处理长序列时面临二次复杂性和内存问题，因此采用了固定大小的隐藏状态的线性注意力机制。然而，线性模型通常在回忆性能上存在局限，导致出现结合线性和全注意力层的混合架构。我们的研究表明，选择性门控、层次递归和控制遗忘是有效混合模型的关键因素，并推荐在3:1到6:1的线性与全注意力比例下的架构。'}}}, {'id': 'https://huggingface.co/papers/2507.07017', 'title': 'First Return, Entropy-Eliciting Explore', 'url': 'https://huggingface.co/papers/2507.07017', 'abstract': "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.", 'score': 15, 'issue_id': 4740, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '93dc431bd34cdb14', 'authors': ['Tianyu Zheng', 'Tianshun Xing', 'Qingshui Gu', 'Taoran Liang', 'Xingwei Qu', 'Xin Zhou', 'Yizhi Li', 'Zhoufutu Wen', 'Chenghua Lin', 'Wenhao Huang', 'Qian Liu', 'Ge Zhang', 'Zejun Ma'], 'affiliations': ['ByteDance', 'M-A-P', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2507.07017.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'FR3E: Структурированное исследование для улучшения рассуждений ИИ', 'desc': 'FR3E - это новый метод для улучшения способностей рассуждения больших языковых моделей (LLM). Он использует структурированное исследование, выполняя целевые прогоны в точках с высокой неопределенностью. Это приводит к более стабильному обучению и точным ответам LLM. Эмпирические результаты на математических тестах показывают, что FR3E улучшает стабильность обучения, генерирует более длинные и связные ответы, а также увеличивает долю полностью правильных рассуждений.'}, 'en': {'title': 'Structured Exploration for Enhanced LLM Reasoning', 'desc': "FR3E is a framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by focusing on high-uncertainty decision points during their training. It utilizes targeted rollouts to provide structured exploration, which helps in generating semantically grounded feedback without needing extensive supervision. This approach leads to more stable training processes and improves the accuracy of the model's responses. Empirical tests demonstrate that FR3E results in longer, more coherent outputs and a higher rate of correct reasoning paths."}, 'zh': {'title': 'FR3E：提升LLM推理的结构化探索', 'desc': 'FR3E是一种增强大型语言模型（LLM）推理能力的方法。它通过在高不确定性决策点进行有针对性的探索，提供结构化的反馈，从而实现更稳定的训练和更准确的响应。该方法不依赖于密集的监督，而是通过构建语义上扎实的中间反馈来指导模型。实验结果表明，FR3E在数学推理基准测试中表现出更长、更连贯的响应，并提高了完全正确轨迹的比例。'}}}, {'id': 'https://huggingface.co/papers/2507.05687', 'title': 'AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs', 'url': 'https://huggingface.co/papers/2507.05687', 'abstract': 'Kernel development in deep learning requires optimizing computational units across hardware while balancing memory management, parallelism, and hardware-specific optimizations through extensive empirical tuning. Although domain-specific languages like Triton simplify GPU programming by abstracting low-level details, developers must still manually tune critical parameters such as tile sizes and memory access patterns through iterative experimentation, creating substantial barriers to optimal performance and wider adoption. In this work, we introduce AutoTriton, the first model dedicated to Triton programming powered by reinforcement learning (RL). AutoTriton performs supervised fine-tuning (SFT) to be equipped with essential Triton programming expertise using a high-quality data gathering pipeline, and conducts RL with Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based reward and an execution-based reward to further improve Triton programming ability, sequentially. Experiments across five evaluation channels of TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves performance comparable to mainstream large models, including Claude-4-Sonnet and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial role of each module within AutoTriton, including the SFT stage, the RL stage, and the reward design strategy. These findings underscore the promise of RL for automatically generating high-performance kernels, and since high-performance kernels are core components of AI systems, this breakthrough establishes an important foundation for building more efficient AI systems. The model and code will be available at https://github.com/AI9Stars/AutoTriton.', 'score': 13, 'issue_id': 4738, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '1e138b69433f5481', 'authors': ['Shangzhan Li', 'Zefan Wang', 'Ye He', 'Yuxuan Li', 'Qi Shi', 'Jianling Li', 'Yonggang Hu', 'Wanxiang Che', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Harbin Institute of Technology', 'OpenBMB', 'Tianjin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05687.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'AutoTriton: Революция в оптимизации ядер глубокого обучения с помощью RL', 'desc': 'AutoTriton - это первая модель, основанная на обучении с подкреплением (RL), для программирования на Triton. Она использует управляемую тонкую настройку (SFT) для приобретения экспертных знаний в Triton и применяет алгоритм Group Relative Policy Optimization (GRPO) для дальнейшего улучшения навыков программирования. Эксперименты показывают, что 8B-модель AutoTriton достигает производительности, сравнимой с ведущими большими моделями. Это исследование демонстрирует потенциал RL для автоматической генерации высокопроизводительных ядер, что важно для создания более эффективных систем искусственного интеллекта.'}, 'en': {'title': 'AutoTriton: Revolutionizing GPU Programming with Reinforcement Learning', 'desc': 'This paper presents AutoTriton, a novel model designed to enhance Triton programming for GPU optimization using reinforcement learning (RL). It addresses the challenges developers face in manually tuning parameters for performance by automating the process through supervised fine-tuning and RL techniques. The model employs a unique reward system that combines rule-based and execution-based rewards to improve kernel generation. Experimental results show that AutoTriton achieves performance on par with leading models, highlighting its potential to streamline the development of high-performance kernels essential for AI systems.'}, 'zh': {'title': '自动化Triton编程，提升深度学习性能！', 'desc': '本论文介绍了AutoTriton，这是一个基于强化学习的模型，旨在优化Triton编程。通过监督微调和群体相对策略优化算法，AutoTriton能够自动调整关键参数，从而提高GPU编程的性能。实验结果表明，AutoTriton的表现与主流大型模型相当，展示了强化学习在自动生成高性能内核方面的潜力。该研究为构建更高效的人工智能系统奠定了重要基础。'}}}, {'id': 'https://huggingface.co/papers/2507.06804', 'title': 'Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving', 'url': 'https://huggingface.co/papers/2507.06804', 'abstract': "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .", 'score': 10, 'issue_id': 4738, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '4c82f29e08bec1a6', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.06804.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#dataset', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Разделяй и властвуй: новый подход к автоматическому доказательству теорем', 'desc': 'Статья представляет новый подход к автоматическому доказательству теорем, разделяющий процессы рассуждения и формального доказательства. Авторы используют две отдельные модели: мощный Reasoner для генерации стратегических подцелей и эффективный Prover для их строгой верификации. Этот метод позволяет преодолеть ограничения существующих систем, которые часто отдают предпочтение поверхностным тактикам в ущерб глубоким рассуждениям. Новый подход успешно решил 5 сложных задач IMO после 2000 года, демонстрируя значительный прогресс в автоматизированном математическом рассуждении.'}, 'en': {'title': 'Decoupling Reasoning and Proving for Enhanced Theorem Proving', 'desc': 'This paper presents a new framework for Automated Theorem Proving (ATP) that separates the processes of reasoning and proving to enhance performance on complex mathematical problems. Current models struggle with formal proving due to their reliance on shallow tactics, which limits deep reasoning capabilities. The proposed solution involves using two specialized models: a Reasoner that generates strategic subgoals and a Prover that verifies these goals rigorously. By decoupling these functions, the framework achieves notable success on challenging IMO problems, marking a significant advancement in automated reasoning.'}, 'zh': {'title': '解耦推理与证明，提升自动定理证明性能', 'desc': '本文提出了一种新颖的框架，将推理与证明解耦，以提高自动定理证明（ATP）的性能。当前的最先进证明器将推理与证明紧密结合，导致深度推理受到抑制，而更倾向于浅层的策略。我们的方法使用两个独立的模型：一个强大的通用推理器生成多样的子目标引理，另一个高效的证明器对其进行严格验证。通过这种模块化设计，我们成功解决了五个2000年后国际数学奥林匹克（IMO）问题，展示了在极具挑战性的数学问题上实现自动推理的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2506.24044', 'title': 'A Survey on Vision-Language-Action Models for Autonomous Driving', 'url': 'https://huggingface.co/papers/2506.24044', 'abstract': "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at https://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.", 'score': 8, 'issue_id': 4738, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': 'ae24de54097b310f', 'authors': ['Sicong Jiang', 'Zilin Huang', 'Kangan Qian', 'Ziang Luo', 'Tianze Zhu', 'Yang Zhong', 'Yihong Tang', 'Menglin Kong', 'Yunlong Wang', 'Siwen Jiao', 'Hao Ye', 'Zihao Sheng', 'Xin Zhao', 'Tuopu Wen', 'Zheng Fu', 'Sikai Chen', 'Kun Jiang', 'Diange Yang', 'Seongjin Choi', 'Lijun Sun'], 'affiliations': ['McGill University, Canada', 'State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University, China', 'Tsinghua University, China', 'University of Minnesota-Twin Cities, USA', 'University of Wisconsin-Madison, USA', 'Xiaomi Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2506.24044.jpg', 'data': {'categories': ['#reasoning', '#agents', '#architecture', '#benchmark', '#survey', '#dataset', '#multimodal', '#alignment', '#interpretability'], 'emoji': '🚗', 'ru': {'title': 'VLA: Новый горизонт для интерпретируемых и социально-ориентированных беспилотных автомобилей', 'desc': 'Это обзор парадигм Vision-Language-Action (VLA) и их адаптации для автономного вождения. В статье рассматриваются архитектурные компоненты, эволюция моделей и наборы данных для VLA в контексте беспилотных автомобилей. Авторы анализируют более 20 репрезентативных моделей и обсуждают прогресс VLA в области автономного вождения. Также освещаются открытые проблемы, такие как надежность, эффективность в реальном времени и формальная верификация.'}, 'en': {'title': 'Driving the Future: Integrating Vision, Language, and Action in Autonomous Vehicles', 'desc': "This paper surveys the integration of Vision-Language-Action (VLA) paradigms in the context of autonomous driving. It discusses how multimodal large language models can enhance vehicles' abilities to understand visual inputs and natural language commands for decision-making. The authors analyze the evolution of VLA models, compare various architectures, and consolidate datasets relevant to this field. They also identify key challenges such as robustness and real-time efficiency, providing a roadmap for future research in VLA for autonomous driving."}, 'zh': {'title': '视觉-语言-行动：自动驾驶的未来之路', 'desc': '这篇调查论文全面概述了视觉-语言-行动（VLA）范式及其在自动驾驶中的应用，详细介绍了架构组件、模型演变、数据集和未来挑战。随着多模态大语言模型（MLLM）的快速发展，VLA范式将视觉感知、自然语言理解和控制整合在一个策略中。研究人员正在积极将这些方法适应于车辆领域，以实现能够理解高层指令、推理复杂交通场景并自主决策的自动驾驶汽车。论文还总结了现有数据集和基准，强调了共同测量驾驶安全性、准确性和解释质量的协议，并指出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2507.06853', 'title': 'DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2507.06853', 'abstract': 'DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.  \t\t\t\t\tAI-generated summary \t\t\t\t Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.', 'score': 4, 'issue_id': 4741, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '06caa3223981eb7b', 'authors': ['Liang Wang', 'Yu Rong', 'Tingyang Xu', 'Zhenyi Zhong', 'Zhiyuan Liu', 'Pengju Wang', 'Deli Zhao', 'Qiang Liu', 'Shu Wu', 'Liang Wang'], 'affiliations': ['College of Intelligence and Computing, Tianjin University', 'DAMO Academy, Alibaba Group', 'Hupan Lab', 'NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.06853.jpg', 'data': {'categories': ['#architecture', '#science', '#multimodal', '#data', '#diffusion', '#3d'], 'emoji': '🧪', 'ru': {'title': 'Революция в определении структуры молекул с помощью ИИ', 'desc': 'DiffSpectra - это генеративная модель для определения 2D и 3D структур молекул на основе мультимодальных спектральных данных. Она использует диффузионные модели с SE(3)-эквивариантной архитектурой и спектральный энкодер SpecFormer. DiffSpectra формулирует задачу как условную генерацию с помощью сети шумоподавления на основе трансформера. Модель показывает высокую точность в восстановлении точных структур молекул, значительно превосходя существующие методы.'}, 'en': {'title': 'Revolutionizing Molecular Structure Elucidation with Diffusion Models', 'desc': "DiffSpectra is a novel generative framework that utilizes diffusion models to infer both 2D and 3D molecular structures from various types of spectral data. It employs an SE(3)-equivariant architecture to effectively incorporate geometric and topological information, enhancing the accuracy of molecular structure elucidation. The model's conditioning is achieved through a transformer-based spectral encoder called SpecFormer, which captures complex dependencies within the spectral data. Experimental results show that DiffSpectra significantly outperforms traditional methods, achieving high accuracy in recovering molecular structures, thus addressing the limitations of previous approaches in the field."}, 'zh': {'title': 'DiffSpectra：多模态光谱下的分子结构推断新方法', 'desc': 'DiffSpectra 是一个使用扩散模型和 SE(3) 等变架构的生成框架，能够从多模态光谱数据中准确推断出二维和三维分子结构。该方法将结构阐明视为一个条件生成过程，利用 Diffusion Molecule Transformer 作为去噪网络，整合了拓扑和几何信息。SpecFormer 作为光谱编码器，捕捉了多模态光谱中的内部和外部依赖关系。实验结果表明，DiffSpectra 在结构阐明方面具有高准确性，展示了光谱条件扩散建模在分子结构阐明中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.05455', 'title': 'ModelCitizens: Representing Community Voices in Online Safety', 'url': 'https://huggingface.co/papers/2507.05455', 'abstract': 'A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens.', 'score': 3, 'issue_id': 4743, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'e6b75294b5b90953', 'authors': ['Ashima Suvarna', 'Christina Chance', 'Karolina Naranjo', 'Hamid Palangi', 'Sophie Hao', 'Thomas Hartvigsen', 'Saadia Gabriel'], 'affiliations': ['Google', 'New York University', 'University of California, Los Angeles', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2507.05455.jpg', 'data': {'categories': ['#data', '#open_source', '#ethics', '#dataset', '#training'], 'emoji': '🗨️', 'ru': {'title': 'Инклюзивная модерация контента: учет разнообразия и контекста', 'desc': 'Статья представляет новый датасет MODELCITIZENS для обнаружения токсичного языка, учитывающий разнообразные перспективы сообществ и контекст разговора. Авторы использовали генеративные языковые модели для создания контекстных сценариев. Существующие инструменты детекции токсичности показали низкую эффективность на этом датасете. Исследователи представили новые модели LLAMACITIZEN-8B и GEMMACITIZEN-12B, превосходящие существующие решения.'}, 'en': {'title': 'Empowering Toxicity Detection with Community Insights', 'desc': 'This paper presents MODELCITIZENS, a new dataset designed for detecting toxic language in social media, which includes 6.8K posts and 40K annotations reflecting diverse community perspectives. Traditional models often overlook the nuances of toxicity shaped by individual experiences and community norms, leading to inaccuracies. By incorporating conversational context through LLM-generated scenarios, the study demonstrates that existing tools struggle with this enriched dataset. The authors also introduce two new models, LLAMACITIZEN-8B and GEMMACITIZEN-12B, which significantly improve performance on toxicity detection tasks, emphasizing the need for community-informed approaches in AI moderation.'}, 'zh': {'title': '社区视角提升毒性语言检测准确性', 'desc': '这篇论文介绍了一个新的数据集MODELCITIZENS，用于检测有毒语言，包含6800条社交媒体帖子和4万条毒性注释，涵盖了多样的身份群体。现有的毒性检测模型通常基于单一的注释标准，忽视了社区规范和具体语境对毒性语言的影响。通过引入对话场景的增强，MODELCITIZENS能够更好地捕捉社交媒体中毒性语言的复杂性。研究表明，基于MODELCITIZENS微调的模型在毒性检测任务中表现优于现有的工具，强调了社区参与的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.06607', 'title': 'Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation', 'url': 'https://huggingface.co/papers/2507.06607', 'abstract': 'Gated Memory Units improve memory sharing in hybrid decoder architectures, enhancing efficiency and performance in language modeling tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.', 'score': 2, 'issue_id': 4757, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '203f70f6ea5f8c5c', 'authors': ['Liliang Ren', 'Congcong Chen', 'Haoran Xu', 'Young Jin Kim', 'Adam Atkinson', 'Zheng Zhan', 'Jiankai Sun', 'Baolin Peng', 'Liyuan Liu', 'Shuohang Wang', 'Hao Cheng', 'Jianfeng Gao', 'Weizhu Chen', 'Yelong Shen'], 'affiliations': ['Microsoft', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06607.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#long_context', '#reasoning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'GMU: Революция в обмене памятью для эффективных языковых моделей', 'desc': 'Статья представляет Gated Memory Unit (GMU) - механизм для эффективного обмена памятью между слоями в гибридных архитектурах декодеров. Авторы применяют GMU для создания SambaY - архитектуры декодер-гибрид-декодер, которая улучшает эффективность декодирования и производительность на длинных контекстах. Эксперименты показывают, что SambaY имеет значительно меньшие неустранимые потери по сравнению с базовой моделью YOCO, что указывает на лучшую масштабируемость производительности. Крупнейшая модель авторов, Phi4-mini-Flash-Reasoning, превосходит Phi4-mini-Reasoning на задачах рассуждений, обеспечивая при этом до 10 раз более высокую пропускную способность декодирования.'}, 'en': {'title': 'Unlocking Efficiency with Gated Memory Units in Language Models', 'desc': 'This paper presents Gated Memory Units (GMUs) as a novel mechanism to improve memory sharing in hybrid decoder architectures for language modeling. By integrating GMUs into the SambaY architecture, the authors enhance decoding efficiency and performance while maintaining linear pre-filling time complexity. The model demonstrates superior scalability and lower irreducible loss compared to existing architectures like YOCO, particularly in long-context tasks. Additionally, the enhanced model achieves significant improvements in reasoning tasks without the need for reinforcement learning, showcasing its effectiveness in handling large-scale computations.'}, 'zh': {'title': '门控记忆单元提升混合解码器效率', 'desc': '本文提出了一种新的机制，称为门控记忆单元（GMU），用于在混合解码器架构中提高记忆共享的效率。通过将GMU应用于SambaY架构，研究者们实现了跨解码器的记忆读取状态共享，从而显著提升了解码效率。实验结果表明，SambaY在长上下文性能上表现优异，并且在大规模计算环境下具有更好的性能可扩展性。该模型在推理任务上表现出色，解码吞吐量提高了10倍，且无需强化学习。'}}}, {'id': 'https://huggingface.co/papers/2507.06485', 'title': 'Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning', 'url': 'https://huggingface.co/papers/2507.06485', 'abstract': "Video-RTS enhances video reasoning efficiency and accuracy through pure RL training and adaptive test-time scaling, reducing data and computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reinforcement learning (RL)-based video reasoning with large language models (LLMs), data collection and finetuning remain significant challenges. These methods often rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annotations, making them costly and hard to scale. To address this, we present Video-RTS, a new approach to improve video reasoning capability with drastically improved data efficiency by combining data-efficient RL with a video-adaptive test-time scaling (TTS) strategy. Based on observations about the data scaling of RL samples, we skip the resource-intensive SFT step and employ efficient pure-RL training with output-based rewards, requiring no additional annotations or extensive fine-tuning. Furthermore, to utilize computational resources more efficiently, we introduce a sparse-to-dense video TTS strategy that improves inference by iteratively adding frames based on output consistency. We validate our approach on multiple video reasoning benchmarks, showing that Video-RTS surpasses existing video reasoning models by an average of 2.4% in accuracy using only 3.6% training samples. For example, Video-RTS achieves a 4.2% improvement on Video-Holmes, a recent and challenging video reasoning benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and adaptive video TTS offer complementary strengths, enabling Video-RTS's strong reasoning performance.", 'score': 2, 'issue_id': 4752, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '209daa331e7041bd', 'authors': ['Ziyang Wang', 'Jaehong Yoon', 'Shoubin Yu', 'Md Mohaiminul Islam', 'Gedas Bertasius', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.06485.jpg', 'data': {'categories': ['#training', '#optimization', '#video', '#rl', '#reasoning', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Эффективное рассуждение о видео с помощью RL и адаптивного масштабирования', 'desc': 'Video-RTS - это новый подход к улучшению способности рассуждать о видео с drastically повышенной эффективностью использования данных. Он сочетает в себе RL-обучение, эффективное с точки зрения данных, со стратегией масштабирования во время тестирования, адаптированной к видео. Video-RTS пропускает ресурсоемкий этап SFT и использует эффективное чистое RL-обучение с наградами на основе выходных данных. Подход был проверен на нескольких бенчмарках по рассуждению о видео, превзойдя существующие модели в среднем на 2.4% по точности, используя всего 3.6% обучающих примеров.'}, 'en': {'title': 'Efficient Video Reasoning with Pure RL and Adaptive Scaling', 'desc': 'Video-RTS is a novel approach that enhances video reasoning by utilizing pure reinforcement learning (RL) training and an adaptive test-time scaling (TTS) strategy. This method significantly reduces the need for large-scale supervised fine-tuning and extensive video data, making it more efficient and cost-effective. By focusing on output-based rewards and avoiding additional annotations, Video-RTS improves data efficiency and accuracy in video reasoning tasks. The results demonstrate that it outperforms existing models on various benchmarks, achieving higher accuracy with fewer training samples.'}, 'zh': {'title': '视频推理的新突破：高效与准确并存', 'desc': 'Video-RTS是一种新方法，通过纯强化学习（RL）训练和自适应测试时间缩放（TTS）策略，提高视频推理的效率和准确性。该方法避免了传统的大规模监督微调（SFT），减少了数据和计算成本。Video-RTS通过输出奖励进行高效的纯RL训练，无需额外的注释或广泛的微调。实验结果表明，Video-RTS在多个视频推理基准上超越了现有模型，显示出显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2507.06260', 'title': "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework", 'url': 'https://huggingface.co/papers/2507.06260', 'abstract': "Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework. Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. Based on this evaluation, we find that Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.", 'score': 2, 'issue_id': 4738, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'e5d9ffe28d4a0466', 'authors': ['Satyapriya Krishna', 'Ninareh Mehrabi', 'Abhinav Mohanty', 'Matteo Memelli', 'Vincent Ponzo', 'Payal Motwani', 'Rahul Gupta'], 'affiliations': ['Amazon Nova Responsible AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.06260.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#multimodal', '#alignment', '#security'], 'emoji': '🔬', 'ru': {'title': 'Nova Premier: мощная и безопасная мультимодальная модель ИИ от Amazon', 'desc': 'Amazon представила Nova Premier - мультимодальную фундаментальную модель, способную обрабатывать текст, изображения и видео с контекстным окном в миллион токенов. Модель прошла комплексную оценку безопасности по методологии Frontier Model Safety Framework в трех критических областях: ХБРЯ, кибероперации и автоматизированные ИИ-исследования. На основе проведенных тестов, включавших автоматизированные бенчмарки и экспертный анализ, модель была признана безопасной для публичного релиза. Amazon планирует продолжать совершенствовать процессы оценки и снижения рисков для передовых моделей ИИ.'}, 'en': {'title': 'Nova Premier: A Safe Multimodal Model for Complex Analysis', 'desc': "Nova Premier is a powerful multimodal foundation model developed by Amazon that can understand and analyze text, images, and videos all at once. It has a large context window of one million tokens, allowing it to handle extensive inputs like long documents and videos in a single prompt. The paper presents a thorough evaluation of Nova Premier's safety using the Frontier Model Safety Framework, focusing on high-risk areas such as CBRN and Offensive Cyber Operations. The findings indicate that Nova Premier meets safety standards for public use, and the team plans to continuously improve safety measures as new challenges arise."}, 'zh': {'title': 'Nova Premier：安全的多模态基础模型', 'desc': 'Nova Premier是亚马逊最强大的多模态基础模型，能够处理文本、图像和视频，具有一百万个标记的上下文窗口。这使得它能够在单个提示中分析大型代码库、400页文档和90分钟视频。我们首次全面评估了Nova Premier在前沿模型安全框架下的关键风险特征，重点关注化学、生物、放射性和核（CBRN）、进攻性网络操作和自动化人工智能研发等高风险领域。评估结果表明，Nova Premier符合2025年巴黎人工智能安全峰会的承诺，适合公开发布，并将继续增强安全评估和风险缓解流程。'}}}, {'id': 'https://huggingface.co/papers/2505.10251', 'title': 'SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning', 'url': 'https://huggingface.co/papers/2505.10251', 'abstract': "A hierarchical framework combining high-level task planning and low-level trajectory generation enables autonomous surgical procedures with high success rates in ex vivo experiments.  \t\t\t\t\tAI-generated summary \t\t\t\t Research on autonomous surgery has largely focused on simple task automation in controlled environments. However, real-world surgical applications demand dexterous manipulation over extended durations and generalization to the inherent variability of human tissue. These challenges remain difficult to address using existing logic-based or conventional end-to-end learning approaches. To address this gap, we propose a hierarchical framework for performing dexterous, long-horizon surgical steps. Our approach utilizes a high-level policy for task planning and a low-level policy for generating robot trajectories. The high-level planner plans in language space, generating task-level or corrective instructions that guide the robot through the long-horizon steps and correct for the low-level policy's errors. We validate our framework through ex vivo experiments on cholecystectomy, a commonly-practiced minimally invasive procedure, and conduct ablation studies to evaluate key components of the system. Our method achieves a 100\\% success rate across eight unseen ex vivo gallbladders, operating fully autonomously without human intervention. This work demonstrates step-level autonomy in a surgical procedure, marking a milestone toward clinical deployment of autonomous surgical systems.", 'score': 2, 'issue_id': 4749, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 мая', 'en': 'May 15', 'zh': '5月15日'}, 'hash': '2d7e1a44cbd9022b', 'authors': ['Ji Woong Kim', 'Juo-Tung Chen', 'Pascal Hansen', 'Lucy X. Shi', 'Antony Goldenberg', 'Samuel Schmidgall', 'Paul Maria Scheikl', 'Anton Deguet', 'Brandon M. White', 'De Ru Tsai', 'Richard Cha', 'Jeffrey Jopling', 'Chelsea Finn', 'Axel Krieger'], 'affiliations': ['Johns Hopkins University', 'Optosurgical', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.10251.jpg', 'data': {'categories': ['#robotics', '#agents', '#optimization', '#science', '#agi'], 'emoji': '🤖', 'ru': {'title': 'Автономная хирургия: от планирования к точным движениям', 'desc': 'Предложена иерархическая система для выполнения сложных хирургических операций роботом. Система сочетает высокоуровневое планирование задач и низкоуровневую генерацию траекторий движения. Высокоуровневый планировщик работает с языковыми инструкциями, направляя робота через длительные этапы операции. В экспериментах ex vivo по холецистэктомии система достигла 100% успеха без вмешательства человека.'}, 'en': {'title': 'Achieving Autonomous Surgery with Hierarchical Planning', 'desc': 'This paper presents a hierarchical framework designed for autonomous surgical procedures, combining high-level task planning with low-level trajectory generation. The high-level policy generates task instructions in language space, while the low-level policy focuses on executing these tasks through precise robot movements. This approach addresses the complexities of real-world surgeries, such as dexterous manipulation and variability in human tissue, which traditional methods struggle to manage. The framework was validated through successful ex vivo experiments on gallbladder removal, achieving a 100% success rate without human intervention, indicating a significant advancement towards autonomous surgical systems.'}, 'zh': {'title': '自主外科手术的分层框架', 'desc': '本研究提出了一种分层框架，结合了高层任务规划和低层轨迹生成，以实现自主外科手术。该框架能够在复杂的人体组织环境中进行灵活的操作，并且在长时间的手术过程中保持高成功率。通过在胆囊切除术的实验中验证，我们的方法在八个未见的样本上实现了100%的成功率，完全自主操作，无需人类干预。此项工作标志着自主外科系统向临床应用迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2507.07106', 'title': 'Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor', 'url': 'https://huggingface.co/papers/2507.07106', 'abstract': 'Text-to-image diffusion models enhance image-based question-answering by providing semantically rich and instruction-aware visual encodings, complementing CLIP and improving spatial and compositional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.', 'score': 1, 'issue_id': 4751, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'bfb7b389a6a985bd', 'authors': ['Vatsal Agarwal', 'Matthew Gwilliam', 'Gefen Kohavi', 'Eshan Verma', 'Daniel Ulbricht', 'Abhinav Shrivastava'], 'affiliations': ['Apple', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2507.07106.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#cv', '#benchmark', '#leakage', '#reasoning'], 'emoji': '🖼️', 'ru': {'title': 'Диффузионные модели как ключ к улучшению визуального ИИ', 'desc': 'Это исследование показывает, как модели диффузии текста в изображение могут улучшить возможности вопросно-ответных систем на основе изображений. Авторы обнаружили, что признаки диффузионных моделей семантически богаты и могут кодировать сильное выравнивание изображения и текста. Они предложили стратегию объединения признаков CLIP и условных диффузионных признаков для улучшения пространственных и композиционных рассуждений. Результаты демонстрируют перспективность диффузионных моделей для визуального понимания, особенно в задачах, ориентированных на зрение.'}, 'en': {'title': 'Enhancing Image Question-Answering with Diffusion Models', 'desc': 'This paper explores the use of text-to-image diffusion models as visual encoders for image-based question-answering tasks. Unlike CLIP, which captures broad visual information, diffusion models provide detailed semantic representations that enhance image-text alignment. The authors demonstrate that these models can focus on relevant image regions based on the input question, improving spatial and compositional reasoning. They also address a leakage issue where large language models can unintentionally access original diffusion prompts, proposing a fusion strategy that combines the strengths of both CLIP and diffusion features.'}, 'zh': {'title': '扩散模型提升图像问答能力', 'desc': '本文研究了文本到图像扩散模型在图像问答中的应用，旨在提升视觉编码的语义丰富性和指令感知能力。传统的CLIP视觉编码器虽然能够捕捉全局信息，但常常忽略与输入查询相关的细节。通过分析扩散模型的内部表示，发现其在语义上非常丰富，并且能够强有力地编码图像与文本的对齐。研究还提出了一种简单的融合策略，结合了CLIP和条件扩散特征，以提高视觉理解能力，特别是在需要空间和组合推理的任务中。'}}}, {'id': 'https://huggingface.co/papers/2507.06415', 'title': 'PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning', 'url': 'https://huggingface.co/papers/2507.06415', 'abstract': 'PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.', 'score': 1, 'issue_id': 4755, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'a3f686298107206f', 'authors': ['Zeming Chen', 'Angelika Romanou', 'Gail Weiss', 'Antoine Bosselut'], 'affiliations': ['Department of Computer and Communication Science EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2507.06415.jpg', 'data': {'categories': ['#long_context', '#training', '#small_models', '#reasoning', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'PERK: Эффективное обучение рассуждениям на длинном контексте', 'desc': 'PERK - это масштабируемый подход, использующий параметрически-эффективные адаптеры для улучшения рассуждений на основе длинного контекста. Он кодирует контексты в легковесную модель во время тестирования, достигая значительных улучшений производительности по сравнению с методами на основе промптов. PERK использует два вложенных цикла оптимизации: внутренний кодирует контексты в адаптер LoRA, а внешний учится использовать обновленный адаптер для точного восстановления и рассуждения. Эксперименты показывают, что PERK значительно превосходит стандартные базовые линии для длинного контекста, демонстрируя большую устойчивость к сложности рассуждений и экстраполяции длины.'}, 'en': {'title': 'Enhancing Long-Context Reasoning with Efficient Adapters', 'desc': 'PERK (Parameter Efficient Reasoning over Knowledge) is a novel approach that enhances long-context reasoning by using lightweight model adapters during test time. It effectively encodes extensive input contexts into a low-rank adapter, allowing for efficient memory usage and improved reasoning capabilities. The method employs two optimization loops: one for quickly adapting the model to the context and another for refining the reasoning process. Evaluations demonstrate that PERK significantly outperforms traditional prompt-based methods, achieving substantial performance gains across various long-context reasoning tasks.'}, 'zh': {'title': '高效长上下文推理的新方法', 'desc': 'PERK（参数高效知识推理）是一种可扩展的方法，通过在测试时使用轻量级模型适配器来增强长上下文推理。该方法通过梯度更新快速将上下文编码到低秩适配器中，作为基础模型的高效记忆模块。PERK在多个长上下文推理任务中的评估显示，其性能显著优于传统的基于提示的方法，尤其在小型模型（如GPT-2）上，性能提升可达90%。尽管在训练时内存消耗较大，但在推理时，PERK的效率优于基于提示的长上下文推理。'}}}, {'id': 'https://huggingface.co/papers/2507.01702', 'title': 'AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness', 'url': 'https://huggingface.co/papers/2507.01702', 'abstract': "AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.", 'score': 1, 'issue_id': 4745, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '6465ab2f7869c041', 'authors': ['Zixin Chen', 'Hongzhan Lin', 'Kaixin Li', 'Ziyang Luo', 'Zhen Ye', 'Guang Chen', 'Zhiyong Huang', 'Jing Ma'], 'affiliations': ['BUPT', 'HKBU', 'HKUST', 'NUS'], 'pdf_title_img': 'assets/pdf/title_img/2507.01702.jpg', 'data': {'categories': ['#agents', '#multimodal', '#benchmark', '#reasoning', '#interpretability', '#alignment'], 'emoji': '🕵️', 'ru': {'title': 'AdamMeme: Умный детектив в мире вредоносных мемов', 'desc': 'AdamMeme - это адаптивная система оценки мультимодальных языковых моделей в понимании вредоносных мемов. Она использует многоагентное сотрудничество и итеративные обновления для выявления слабых мест моделей. В отличие от статических наборов данных, AdamMeme динамически обновляет мемы, создавая сложные примеры. Эксперименты показывают, что система эффективно выявляет различия в производительности разных моделей и их конкретные недостатки.'}, 'en': {'title': 'Unveiling the Weaknesses of mLLMs in Understanding Harmful Memes', 'desc': 'AdamMeme is a new framework designed to evaluate how well multimodal Large Language Models (mLLMs) understand harmful memes. It uses an adaptive, agent-based approach that allows for continuous updates and collaboration among multiple agents. This method helps identify specific weaknesses in different mLLMs by testing them with evolving meme data. The framework provides a more dynamic and thorough assessment of mLLMs compared to traditional static benchmarks.'}, 'zh': {'title': '揭示多模态模型的有害性理解弱点', 'desc': 'AdamMeme是一个自适应的基于代理的框架，用于评估多模态大型语言模型对有害表情包的理解能力。该框架通过迭代更新和多代理协作，揭示了不同模型的特定弱点。现有的评估方法依赖于静态数据集，无法及时反映在线表情包的动态演变。通过挑战性样本的更新，AdamMeme能够全面评估模型在解读有害性方面的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2507.05566', 'title': 'SingLoRA: Low Rank Adaptation Using a Single Matrix', 'url': 'https://huggingface.co/papers/2507.05566', 'abstract': 'SingLoRA, a reformulated low-rank adaptation method, enhances parameter-efficient fine-tuning by learning a single low-rank matrix update, ensuring stable optimization and reduced parameter count.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SingLoRA, which reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SingLoRA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+ (90.2%) - while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SingLoRA significantly improves image fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively.', 'score': 60, 'issue_id': 4720, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'b4bac7e0cf74ddfa', 'authors': ['David Bensaïd', 'Noam Rotstein', 'Roy Velich', 'Daniel Bensaïd', 'Ron Kimmel'], 'affiliations': ['Technion - IIT Haifa, Israel', 'University Paris Dauphine Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.05566.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#architecture'], 'emoji': '🔢', 'ru': {'title': 'SingLoRA: стабильная и эффективная адаптация моделей', 'desc': 'SingLoRA - это новый метод адаптации с низким рангом для эффективной донастройки больших предобученных моделей. Он решает проблему нестабильности обучения, характерную для стандартного LoRA, путем использования разложения одной низкоранговой матрицы. SingLoRA обеспечивает более стабильную оптимизацию и уменьшает количество параметров примерно вдвое. Эксперименты показывают, что SingLoRA превосходит LoRA и другие методы в задачах обработки естественного языка и генерации изображений.'}, 'en': {'title': 'SingLoRA: Simplifying Fine-Tuning with Stable Low-Rank Updates', 'desc': 'SingLoRA is a new method for fine-tuning large machine learning models efficiently by using a single low-rank matrix update. This approach simplifies the training process by avoiding issues related to scale differences between multiple matrices, which can lead to unstable learning. By reformulating the adaptation process, SingLoRA reduces the number of parameters needed, making it more efficient while maintaining performance. Experiments show that SingLoRA outperforms previous methods in tasks like common sense reasoning and image generation, achieving higher accuracy and better image quality with fewer parameters.'}, 'zh': {'title': 'SingLoRA：稳定高效的低秩适应方法', 'desc': 'SingLoRA是一种重新构建的低秩适应方法，旨在通过学习单个低秩矩阵的更新来增强参数高效的微调。它通过将两个较小矩阵的乘积形成低秩矩阵更新，解决了传统LoRA方法中矩阵间尺度差异导致的不稳定训练问题。SingLoRA的设计消除了这些冲突，确保了优化过程的稳定性，并大幅减少了参数数量。实验结果表明，SingLoRA在多个任务上表现优异，尤其是在常识推理和图像生成方面，超越了传统的LoRA方法。'}}}, {'id': 'https://huggingface.co/papers/2507.06229', 'title': 'Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving', 'url': 'https://huggingface.co/papers/2507.06229', 'abstract': "As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.", 'score': 54, 'issue_id': 4733, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'fe7edacae46166de', 'authors': ['Xiangru Tang', 'Tianrui Qin', 'Tianhao Peng', 'Ziyang Zhou', 'Daniel Shao', 'Tingting Du', 'Xinming Wei', 'Peng Xia', 'Fang Wu', 'He Zhu', 'Ge Zhang', 'Jiaheng Liu', 'Xingyao Wang', 'Sirui Hong', 'Chenglin Wu', 'Hao Cheng', 'Chi Wang', 'Wangchunshu Zhou'], 'affiliations': ['All Hands AI', 'Bytedance', 'Google DeepMind', 'MetaGPT X', 'Microsoft Research', 'Nanjing University', 'OPPO', 'Stanford University', 'UNC Chapel Hill', 'UW-Madison', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06229.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум: Agent KB объединяет опыт языковых моделей', 'desc': 'Статья представляет Agent KB - иерархическую систему для обмена опытом между языковыми агентами. Используя процесс Reason-Retrieve-Refine, Agent KB создает общую базу знаний, позволяющую агентам учиться на опыте друг друга. Система улучшает показатели успешности на бенчмарке GAIA до 16.28 процентных пунктов. Agent KB также повышает эффективность исправления кода на SWE-bench для Claude-3 с 41.33% до 53.33%.'}, 'en': {'title': 'Empowering Agents with Shared Knowledge for Better Learning', 'desc': 'This paper presents Agent KB, a new framework designed to enhance the learning capabilities of language agents by allowing them to share and reuse experiences. It introduces a Reason-Retrieve-Refine pipeline that helps agents learn from both high-level strategies and detailed execution logs. By creating a shared knowledge base, Agent KB facilitates knowledge transfer between agents, overcoming the traditional limitation of isolated learning. The framework shows significant improvements in task success rates across various benchmarks, demonstrating its effectiveness in enabling agents to generalize successful strategies to new challenges.'}, 'zh': {'title': 'Agent KB：提升代理学习与经验共享的框架', 'desc': '本文介绍了一种名为Agent KB的层次化经验框架，旨在提升语言代理在复杂任务中的错误修正和经验重用能力。Agent KB通过一种新颖的推理-检索-精炼管道，解决了代理之间无法相互学习经验的核心限制。该框架捕捉高层策略和详细执行日志，创建了一个共享知识库，促进了跨代理的知识转移。在GAIA基准测试中，Agent KB显著提高了成功率，Claude-3和GPT-4在不同任务上的表现都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.06203', 'title': 'A Survey on Latent Reasoning', 'url': 'https://huggingface.co/papers/2507.06203', 'abstract': "Latent reasoning enhances large language models by performing multi-step inference in continuous hidden states, improving efficiency and expressiveness beyond token-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/.", 'score': 54, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '9b37c1970657d866', 'authors': ['Rui-Jie Zhu', 'Tianhao Peng', 'Tianhao Cheng', 'Xingwei Qu', 'Jinfa Huang', 'Dawei Zhu', 'Hao Wang', 'Kaiwen Xue', 'Xuanliang Zhang', 'Yong Shan', 'Tianle Cai', 'Taylor Kergan', 'Assel Kembay', 'Andrew Smith', 'Chenghua Lin', 'Binh Nguyen', 'Yuqi Pan', 'Yuhong Chou', 'Zefan Cai', 'Zhenhe Wu', 'Yongchi Zhao', 'Tianyu Liu', 'Jian Yang', 'Wangchunshu Zhou', 'Chujie Zheng', 'Chongxuan Li', 'Yuyin Zhou', 'Zhoujun Li', 'Zhaoxiang Zhang', 'Jiaheng Liu', 'Ge Zhang', 'Wenhao Huang', 'Jason Eshraghian'], 'affiliations': ['FDU', 'M-A-P', 'NJU', 'PKU', 'PolyU', 'RUC', 'UCSC', 'UW-Madison', 'UoM'], 'pdf_title_img': 'assets/pdf/title_img/2507.06203.jpg', 'data': {'categories': ['#reasoning', '#rl', '#survey', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Латентное рассуждение: новый горизонт для больших языковых моделей', 'desc': 'Это исследование посвящено латентному рассуждению в больших языковых моделях (LLM). Латентное рассуждение позволяет выполнять многошаговые логические выводы в непрерывном скрытом состоянии модели, что повышает эффективность и выразительность по сравнению с токеновым уровнем. В статье рассматриваются различные методологии латентного рассуждения, включая рекуррентность на основе активаций и распространение скрытого состояния. Также обсуждаются продвинутые парадигмы, такие как латентное рассуждение бесконечной глубины с использованием маскированных диффузионных моделей.'}, 'en': {'title': 'Unlocking LLM Potential with Latent Reasoning', 'desc': "This paper discusses how latent reasoning can improve large language models (LLMs) by allowing them to perform multi-step inference in their hidden states, rather than relying on token-level supervision. It highlights the limitations of traditional chain-of-thought reasoning, which, while effective, restricts the model's ability to express complex ideas. The authors review various methodologies for latent reasoning, such as activation-based recurrence and hidden state propagation, which enhance the model's reasoning capabilities. They also introduce advanced techniques like masked diffusion models that facilitate more sophisticated and reversible reasoning processes."}, 'zh': {'title': '潜在推理：超越标记的推理新境界', 'desc': '潜在推理通过在连续的隐藏状态中进行多步推理，增强了大型语言模型的能力，超越了基于标记的监督，提高了效率和表达能力。虽然链式推理（CoT）可以提高模型的可解释性和准确性，但其对自然语言推理的依赖限制了模型的表达带宽。潜在推理通过完全在模型的隐藏状态中进行推理，消除了对标记级监督的需求。本文综述了潜在推理的基础、方法和未来研究方向，旨在阐明这一新兴领域的概念框架。'}}}, {'id': 'https://huggingface.co/papers/2507.06165', 'title': 'OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion', 'url': 'https://huggingface.co/papers/2507.06165', 'abstract': 'OmniPart generates part-aware 3D objects with high semantic decoupling and robust structural cohesion using an autoregressive structure planning module and a spatially-conditioned rectified flow model.  \t\t\t\t\tAI-generated summary \t\t\t\t The creation of 3D assets with explicit, editable part structures is crucial for advancing interactive applications, yet most generative methods produce only monolithic shapes, limiting their utility. We introduce OmniPart, a novel framework for part-aware 3D object generation designed to achieve high semantic decoupling among components while maintaining robust structural cohesion. OmniPart uniquely decouples this complex task into two synergistic stages: (1) an autoregressive structure planning module generates a controllable, variable-length sequence of 3D part bounding boxes, critically guided by flexible 2D part masks that allow for intuitive control over part decomposition without requiring direct correspondences or semantic labels; and (2) a spatially-conditioned rectified flow model, efficiently adapted from a pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and consistently within the planned layout. Our approach supports user-defined part granularity, precise localization, and enables diverse downstream applications. Extensive experiments demonstrate that OmniPart achieves state-of-the-art performance, paving the way for more interpretable, editable, and versatile 3D content.', 'score': 41, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '3acb1f1fb28acbf6', 'authors': ['Yunhan Yang', 'Yufan Zhou', 'Yuan-Chen Guo', 'Zi-Xin Zou', 'Yukun Huang', 'Ying-Tian Liu', 'Hao Xu', 'Ding Liang', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['Harbin Institute of Technology, China', 'The University of Hong Kong, China', 'VAST, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.06165.jpg', 'data': {'categories': ['#optimization', '#3d', '#games'], 'emoji': '🧩', 'ru': {'title': 'OmniPart: Интеллектуальная генерация редактируемых 3D-объектов', 'desc': 'OmniPart - это новая система для генерации 3D-объектов с явной структурой частей. Она использует авторегрессивный модуль планирования структуры и пространственно-обусловленную модель выпрямленного потока. OmniPart обеспечивает высокую семантическую независимость компонентов при сохранении структурной целостности. Система поддерживает контроль гранулярности частей и позволяет создавать редактируемый 3D-контент для различных приложений.'}, 'en': {'title': 'Revolutionizing 3D Object Generation with Part Awareness', 'desc': 'OmniPart is a framework designed to generate 3D objects that are aware of their individual parts, allowing for better control and editing. It uses an autoregressive structure planning module to create a sequence of bounding boxes for each part, guided by 2D masks for intuitive control. Additionally, it employs a spatially-conditioned rectified flow model to synthesize all parts together in a coherent manner. This method enhances the usability of 3D assets for interactive applications by providing high semantic decoupling and structural integrity.'}, 'zh': {'title': 'OmniPart：可编辑的三维对象生成新方法', 'desc': 'OmniPart 是一个新颖的框架，用于生成具有明确可编辑部件结构的三维对象。它通过自回归结构规划模块和空间条件修正流模型，实现了部件之间的高语义解耦和稳健的结构凝聚力。该方法将复杂任务分为两个协同阶段，首先生成可控的三维部件边界框序列，然后同时合成所有三维部件。实验表明，OmniPart 在性能上达到了最先进水平，为更可解释、可编辑和多功能的三维内容铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2507.05240', 'title': 'StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling', 'url': 'https://huggingface.co/papers/2507.05240', 'abstract': 'StreamVLN, a streaming VLN framework, uses a hybrid slow-fast context modeling strategy to balance fine-grained visual understanding, long-term context modeling, and computational efficiency in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: https://streamvln.github.io/{https://streamvln.github.io/}.', 'score': 35, 'issue_id': 4718, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '89c364a9ab7df29e', 'authors': ['Meng Wei', 'Chenyang Wan', 'Xiqian Yu', 'Tai Wang', 'Yuqiang Yang', 'Xiaohan Mao', 'Chenming Zhu', 'Wenzhe Cai', 'Hanqing Wang', 'Yilun Chen', 'Xihui Liu', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05240.jpg', 'data': {'categories': ['#video', '#agents', '#multimodal', '#games', '#benchmark', '#long_context'], 'emoji': '🧭', 'ru': {'title': 'StreamVLN: Эффективная навигация ИИ в реальном мире', 'desc': 'StreamVLN - это новая система для навигации с использованием зрения и языка (VLN) в реальном мире. Она применяет гибридную стратегию медленно-быстрого моделирования контекста для баланса между детальным пониманием визуальной информации, долгосрочным моделированием контекста и вычислительной эффективностью. StreamVLN использует быстрый диалоговый контекст для оперативной генерации действий и медленно обновляемый контекст памяти для сжатия исторических визуальных состояний. Эксперименты показывают, что StreamVLN достигает современного уровня производительности с стабильно низкой задержкой.'}, 'en': {'title': 'StreamVLN: Efficient Navigation with Hybrid Context Modeling', 'desc': 'StreamVLN is a new framework designed for Vision-and-Language Navigation (VLN) that effectively processes continuous visual data while following language instructions. It uses a hybrid slow-fast context modeling approach to balance detailed visual understanding with long-term context and computational efficiency. The fast-streaming dialogue context allows for quick action responses, while the slow-updating memory context efficiently manages historical visual information. This innovative design leads to improved performance in real-world applications, achieving state-of-the-art results with low latency and robust operation.'}, 'zh': {'title': 'StreamVLN：高效的流式视觉与语言导航', 'desc': 'StreamVLN是一个流式视觉与语言导航框架，采用混合的慢速-快速上下文建模策略，以平衡细粒度视觉理解、长期上下文建模和计算效率。该框架支持多模态推理，能够处理交错的视觉、语言和动作输入。快速流式对话上下文通过滑动窗口实现响应式动作生成，而慢速更新的记忆上下文则利用3D感知的标记修剪策略压缩历史视觉状态。实验结果表明，StreamVLN在VLN-CE基准测试中表现出色，具备稳定的低延迟，确保在实际应用中的鲁棒性和效率。'}}}, {'id': 'https://huggingface.co/papers/2507.04103', 'title': 'How to Train Your LLM Web Agent: A Statistical Diagnosis', 'url': 'https://huggingface.co/papers/2507.04103', 'abstract': 'A study on compute allocation for post-training LLM-based web agents finds that combining supervised fine-tuning with on-policy reinforcement learning improves performance and reduces computational costs compared to either method alone.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models.', 'score': 35, 'issue_id': 4716, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 июля', 'en': 'July 5', 'zh': '7月5日'}, 'hash': '64d72d8a50ee925a', 'authors': ['Dheeraj Vattikonda', 'Santhoshi Ravichandran', 'Emiliano Penaloza', 'Hadi Nekoei', 'Megh Thakkar', 'Thibault Le Sellier de Chezelles', 'Nicolas Gontier', 'Miguel Muñoz-Mármol', 'Sahar Omidi Shayegan', 'Stefania Raimondo', 'Xue Liu', 'Alexandre Drouin', 'Laurent Charlin', 'Alexandre Piché', 'Alexandre Lacoste', 'Massimo Caccia'], 'affiliations': ['HEC Montréal', 'McGill University', 'MilaQuebec AI Institute', 'Polytechnique Montréal', 'ServiceNow Research', 'Univeristé de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2507.04103.jpg', 'data': {'categories': ['#agents', '#optimization', '#rl', '#training', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Эффективное пост-обучение LLM-агентов: синергия SFT и RL', 'desc': 'Исследование посвящено оптимизации вычислительных ресурсов для пост-обучения веб-агентов на основе больших языковых моделей (LLM). Авторы применяют двухэтапный подход, сочетающий обучение с учителем (SFT) и обучение с подкреплением (RL). Результаты показывают, что такая комбинация превосходит каждый метод по отдельности на бенчмарках WorkArena и MiniWob++. Этот подход также позволяет сократить вычислительные затраты на 45% по сравнению с чистым SFT при сохранении пиковой производительности.'}, 'en': {'title': 'Optimizing LLM Training: Combining SFT and RL for Efficiency', 'desc': 'This paper investigates how to allocate computing resources effectively for post-training large language model (LLM)-based web agents. It demonstrates that integrating supervised fine-tuning (SFT) with on-policy reinforcement learning (RL) leads to better performance and lower computational costs than using either method separately. The study employs a two-stage training process, where a smaller model learns from a larger one, and it carefully samples hyperparameters to optimize the training process. The findings indicate that this combined approach not only enhances performance but also significantly reduces the computational resources needed, making it a viable alternative to closed-source systems.'}, 'zh': {'title': '结合微调与强化学习，提升LLM代理性能', 'desc': '本研究探讨了基于大语言模型（LLM）的网络代理的计算分配问题。我们提出了一种结合监督微调（SFT）和在线强化学习（RL）的方法，发现这种组合在性能和计算成本上均优于单独使用任一方法。通过对1,370种配置进行采样并使用自助法估计有效超参数，我们的结果表明，这种策略在MiniWob++上仅需55%的计算资源即可达到纯SFT的最佳性能。此方法有效缩小了与闭源模型的差距，推动了计算与性能的帕累托前沿。'}}}, {'id': 'https://huggingface.co/papers/2507.06181', 'title': 'CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization', 'url': 'https://huggingface.co/papers/2507.06181', 'abstract': "CriticLean, a reinforcement learning framework with CriticLeanGPT and CriticLeanBench, enhances semantic evaluation in automated theorem proving by actively learning to distinguish correct from incorrect formalizations.  \t\t\t\t\tAI-generated summary \t\t\t\t Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning.", 'score': 34, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'ad86fa3f7ff162ee', 'authors': ['Zhongyuan Peng', 'Yifan Yao', 'Kaijing Ma', 'Shuyue Guo', 'Yizhe Li', 'Yichi Zhang', 'Chenchen Zhang', 'Yifan Zhang', 'Zhouliang Yu', 'Luming Li', 'Minghao Liu', 'Yihang Xia', 'Jiawei Shen', 'Yuchen Wu', 'Yixin Cao', 'Zhaoxiang Zhang', 'Wenhao Huang', 'Jiaheng Liu', 'Ge Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.06181.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#reasoning', '#rl', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Критик учится различать правильные и неправильные формализации теорем', 'desc': 'CriticLean - это фреймворк обучения с подкреплением для улучшения семантической оценки в автоматическом доказательстве теорем. Он включает в себя CriticLeanGPT - модель, обученную различать правильные и неправильные формализации, и CriticLeanBench - набор данных для оценки таких моделей. Фреймворк позволяет активно обучать критика, переводя его роль из пассивного валидатора в активный обучающийся компонент. Результаты показывают, что оптимизация фазы критики крайне важна для получения надежных формализаций в математических рассуждениях.'}, 'en': {'title': 'Empowering Theorem Proving with Active Semantic Evaluation', 'desc': 'CriticLean is a reinforcement learning framework designed to improve the evaluation of formalizations in automated theorem proving. It introduces CriticLeanGPT, a model that actively learns to assess the semantic accuracy of mathematical statements translated into formal code. The framework also includes CriticLeanBench, a benchmark for measuring the effectiveness of models in distinguishing correct from incorrect formalizations. By optimizing the critic phase, CriticLean aims to enhance the reliability of formalizations and contribute to advancements in formal mathematical reasoning.'}, 'zh': {'title': '优化评判阶段，提升自动定理证明的可靠性', 'desc': 'CriticLean是一个强化学习框架，旨在提高自动定理证明中的语义评估。它通过CriticLeanGPT和CriticLeanBench，主动学习区分正确和错误的形式化表达。CriticLeanGPT经过监督微调和强化学习训练，能够严格评估Lean 4形式化的语义准确性。我们的研究表明，优化评判阶段对于生成可靠的形式化表达至关重要。'}}}, {'id': 'https://huggingface.co/papers/2507.03112', 'title': 'RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents', 'url': 'https://huggingface.co/papers/2507.03112', 'abstract': "An end-to-end reinforcement learning framework using simulated user emotion rewards enhances emotional intelligence in large language models while maintaining cognitive skills.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.", 'score': 27, 'issue_id': 4715, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'c1368a26272d7e57', 'authors': ['Peisong Wang', 'Ruotian Ma', 'Bang Zhang', 'Xingyu Chen', 'Zhiwei He', 'Kang Luo', 'Qingsong Lv', 'Qingxuan Jiang', 'Zheng Xie', 'Shanyi Wang', 'Yuan Li', 'Fanghua Ye', 'Jian Li', 'Yifan Yang', 'Zhaopeng Tu', 'Xiaolong Li'], 'affiliations': ['Hunyuan AI Digital Human, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2507.03112.jpg', 'data': {'categories': ['#reasoning', '#rl', '#alignment', '#agents', '#rlhf', '#training'], 'emoji': '🤖💕', 'ru': {'title': 'Эмоциональный интеллект ИИ: обучение с подкреплением открывает новые горизонты', 'desc': 'Статья представляет RLVER - первую систему обучения с подкреплением для развития эмоционального интеллекта у больших языковых моделей (LLM). Система использует симулированных пользователей для генерации эмоциональных наград в процессе диалога. Применение RLVER к модели Qwen2.5-7B-Instruct значительно повысило её показатели эмоционального интеллекта при сохранении когнитивных навыков. Эксперименты показали, что RLVER последовательно улучшает различные диалоговые способности модели.'}, 'en': {'title': 'Enhancing Emotional Intelligence in Language Models with RLVER', 'desc': 'This paper presents RLVER, a novel reinforcement learning framework designed to enhance emotional intelligence in large language models (LLMs) by using simulated user emotion rewards. The framework employs reinforcement learning from verifiable rewards (RLVR) to train LLMs in dialogue settings, focusing on developing empathetic abilities. By fine-tuning the Qwen2.5-7B-Instruct model with Proximal Policy Optimization (PPO), the authors demonstrate significant improvements in emotional understanding while maintaining cognitive skills. The findings indicate that RLVER effectively boosts dialogue capabilities and suggests that moderate training environments can lead to better outcomes than more challenging ones.'}, 'zh': {'title': '情感智能与认知能力的完美结合', 'desc': '本文提出了一种端到端的强化学习框架RLVER，旨在通过模拟用户的情感奖励来提升大型语言模型的情感智能。尽管大型语言模型在逻辑推理方面表现出色，但它们的情感智能仍然不足。RLVER利用可验证的情感奖励，指导模型学习更高层次的同理心能力。实验结果表明，RLVER显著提高了对话能力，并在保持数学和编码能力的同时，提升了模型的情感理解能力。'}}}, {'id': 'https://huggingface.co/papers/2507.05675', 'title': 'MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos', 'url': 'https://huggingface.co/papers/2507.05675', 'abstract': 'MedGen, a model trained on the large-scale MedVideoCap-55K dataset, achieves top performance in medical video generation by balancing visual quality and medical accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen', 'score': 23, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '06c0aadc1cba572e', 'authors': ['Rongsheng Wang', 'Junying Chen', 'Ke Ji', 'Zhenyang Cai', 'Shunian Chen', 'Yunjin Yang', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2507.05675.jpg', 'data': {'categories': ['#science', '#video', '#healthcare', '#open_source', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'MedGen: Новый стандарт в генерации медицинских видео с помощью ИИ', 'desc': 'Модель MedGen, обученная на крупномасштабном наборе данных MedVideoCap-55K, достигает высоких результатов в генерации медицинских видео, балансируя между визуальным качеством и медицинской точностью. MedVideoCap-55K - это первый масштабный, разнообразный набор данных с подписями для генерации медицинских видео, содержащий более 55 000 отобранных клипов из реальных медицинских сценариев. MedGen превосходит открытые модели и соперничает с коммерческими системами по визуальному качеству и медицинской точности на различных бенчмарках. Авторы надеются, что их датасет и модель послужат ценным ресурсом и катализатором дальнейших исследований в области генерации медицинских видео.'}, 'en': {'title': 'Revolutionizing Medical Video Generation with MedGen', 'desc': 'MedGen is a cutting-edge model designed for generating medical videos, trained on the extensive MedVideoCap-55K dataset. This dataset is the first of its kind, featuring over 55,000 high-quality video clips that accurately depict real-world medical scenarios. MedGen excels in producing videos that not only look visually appealing but also maintain strict adherence to medical accuracy, addressing a significant challenge in the field. By providing this innovative model and dataset, the authors aim to advance research in medical video generation and improve applications in clinical training and education.'}, 'zh': {'title': '医学视频生成的新突破', 'desc': 'MedGen是一个基于大规模MedVideoCap-55K数据集训练的模型，专注于医学视频生成。该模型在视觉质量和医学准确性之间取得了良好的平衡，表现出色。医学视频在临床培训、教育和模拟中至关重要，因此需要高视觉保真度和严格的医学准确性。MedVideoCap-55K数据集是首个大规模、多样化且富含字幕的医学视频生成数据集，为训练医学视频生成模型提供了坚实基础。'}}}, {'id': 'https://huggingface.co/papers/2507.06219', 'title': 'Is Diversity All You Need for Scalable Robotic Manipulation?', 'url': 'https://huggingface.co/papers/2507.06219', 'abstract': 'Investigation into data diversity in robotic manipulation reveals that task diversity is crucial, multi-embodiment data is optional, and expert diversity can be confounding, leading to a distribution debiasing method for improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.', 'score': 17, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'd4781dc7e2730cb8', 'authors': ['Modi Shi', 'Li Chen', 'Jin Chen', 'Yuxiang Lu', 'Chiming Liu', 'Guanghui Ren', 'Ping Luo', 'Di Huang', 'Maoqing Yao', 'Hongyang Li'], 'affiliations': ['AgiBot', 'Beihang University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.06219.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#robotics', '#data', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'Ключ к эффективному обучению роботов: разнообразие задач важнее разнообразия платформ', 'desc': 'Исследование разнообразия данных в робототехнической манипуляции показало, что разнообразие задач имеет решающее значение, в то время как использование данных от нескольких роботов необязательно. Разнообразие экспертов может вносить путаницу, что привело к разработке метода дебиасинга распределения для улучшения производительности. Модели, обученные на высококачественных данных от одного робота, могут эффективно переноситься на другие платформы. Предложенный метод GO-1-Pro, снижающий неоднозначность скорости, позволяет достичь значительного прироста производительности в 15%.'}, 'en': {'title': 'Diversity in Data: Key to Better Robot Learning', 'desc': 'This paper explores the importance of data diversity in robotic manipulation, focusing on three key aspects: task diversity, embodiment diversity, and expert diversity. It finds that having a variety of tasks is more beneficial than simply increasing the number of demonstrations for each task. The study also shows that using data from a single robot can be just as effective as using data from multiple robots for training. Additionally, it highlights that differences in how experts demonstrate tasks can complicate learning, leading to the development of a new method to reduce confusion caused by these variations, resulting in significant performance improvements.'}, 'zh': {'title': '任务多样性是机器人操作的关键', 'desc': '本研究探讨了数据多样性在机器人操作中的重要性，发现任务多样性是关键，而多种机器人形态的数据是可选的。通过对不同机器人平台的广泛实验，我们发现任务多样性比每个任务的演示数量更为重要，有助于从多样的预训练任务转移到新的下游场景。我们还提出了一种分布去偏方法，以减少速度模糊，从而显著提高性能。整体而言，这些发现为有效扩展机器人操作数据集提供了新的视角和实用指导。'}}}, {'id': 'https://huggingface.co/papers/2507.04569', 'title': 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts', 'url': 'https://huggingface.co/papers/2507.04569', 'abstract': 'Nile-Chat models, using Branch-Train-MiX strategy, outperform existing multilingual and Arabic LLMs on Egyptian dialect benchmarks in both Arabic and Latin scripts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for Egyptian dialect, uniquely designed to understand and generate texts written in both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we introduce a novel language adaptation approach by leveraging the Branch-Train-MiX strategy to merge script-specialized experts, into a single MoE model. Our Nile-Chat models significantly outperform leading multilingual and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced Egyptian evaluation benchmarks, which span both understanding and generative tasks. Notably, our 12B model yields a 14.4% performance gain over Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly available. We believe this work presents a comprehensive methodology for adapting LLMs to dual-script languages, addressing an often overlooked aspect in modern LLM development.', 'score': 15, 'issue_id': 4722, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '40595bd58ae7eb1b', 'authors': ['Guokan Shang', 'Hadi Abdine', 'Ahmad Chamma', 'Amr Mohamed', 'Mohamed Anwar', 'Abdelaziz Bounhar', 'Omar El Herraoui', 'Preslav Nakov', 'Michalis Vazirgiannis', 'Eric Xing'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2507.04569.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multilingual', '#training', '#low_resource'], 'emoji': '🇪🇬', 'ru': {'title': 'Nile-Chat: прорыв в обработке египетского диалекта с двойной системой письма', 'desc': 'Исследователи представили серию моделей Nile-Chat для египетского диалекта, способных работать как с арабской, так и с латинской письменностью. Они разработали новый подход к адаптации языковых моделей, названный Branch-Train-MiX, который объединяет специализированные по письменности экспертные модели в единую модель смеси экспертов (MoE). Модели Nile-Chat значительно превосходят существующие многоязычные и арабские языковые модели на новых бенчмарках для египетского диалекта. Авторы считают, что их работа предлагает комплексную методологию адаптации больших языковых моделей к языкам с двойной системой письма.'}, 'en': {'title': 'Nile-Chat: Bridging Scripts for Egyptian Dialect Mastery', 'desc': 'The paper presents Nile-Chat, a series of large language models (LLMs) specifically designed for the Egyptian dialect, capable of processing both Arabic and Latin scripts. It introduces the Branch-Train-MiX strategy, which combines specialized models into a single mixture of experts (MoE) framework, enhancing language adaptation. The Nile-Chat models demonstrate superior performance on newly established benchmarks compared to existing multilingual and Arabic LLMs, achieving significant improvements in both understanding and generation tasks. This work highlights a novel approach to adapting LLMs for dual-script languages, filling a gap in current LLM research.'}, 'zh': {'title': 'Nile-Chat：双脚本语言的突破性适应', 'desc': 'Nile-Chat模型采用了Branch-Train-MiX策略，在埃及方言基准测试中，超越了现有的多语言和阿拉伯大型语言模型（LLMs）。我们推出了Nile-Chat-4B、3x4B-A6B和12B，这些模型专门设计用于理解和生成阿拉伯和拉丁文书写的文本。特别是Nile-Chat-3x4B-A6B，通过合并脚本专门化的专家，提出了一种新颖的语言适应方法，形成了一个单一的混合专家（MoE）模型。我们的模型在理解和生成任务上显著优于领先的多语言和阿拉伯LLMs，展示了如何将LLMs适应双脚本语言的全面方法。'}}}, {'id': 'https://huggingface.co/papers/2507.06138', 'title': 'Coding Triangle: How Does Large Language Model Understand Code?', 'url': 'https://huggingface.co/papers/2507.06138', 'abstract': 'The Code Triangle framework evaluates large language models across editorial analysis, code implementation, and test case generation, revealing limitations in diversity and robustness compared to human programmers and suggesting enhancements through human-generated content and model mixtures.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models.', 'score': 14, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '1eb6710e161b9c74', 'authors': ['Taolin Zhang', 'Zihan Ma', 'Maosong Cao', 'Junnan Liu', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06138.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#training', '#benchmark'], 'emoji': '🔺', 'ru': {'title': 'Треугольник кода: новый взгляд на оценку языковых моделей в программировании', 'desc': 'Исследователи представили фреймворк Code Triangle для оценки больших языковых моделей в программировании. Фреймворк анализирует три аспекта: редакционный анализ, реализацию кода и генерацию тестовых случаев. Эксперименты показали, что решения языковых моделей часто уступают решениям человека-программиста в разнообразии и надежности. Авторы предлагают улучшить модели, используя контент, созданный людьми, и комбинируя различные модели.'}, 'en': {'title': 'Enhancing LLMs: Bridging the Gap with Human Insight', 'desc': 'The Code Triangle framework assesses large language models (LLMs) by examining their performance in three key areas: editorial analysis, code implementation, and test case generation. The study finds that while LLMs can create coherent outputs, they often lack the diversity and robustness seen in human programming. It highlights a gap between how models understand coding tasks and the expertise of human programmers, primarily due to biases in training data. The paper suggests that integrating human-generated content and using a mix of models can significantly improve the capabilities and reliability of LLMs in coding tasks.'}, 'zh': {'title': '提升大型语言模型的编码能力', 'desc': '本文介绍了Code Triangle框架，用于评估大型语言模型（LLMs）在编辑分析、代码实现和测试用例生成三个方面的表现。研究发现，尽管LLMs在这些维度上能够形成自洽的系统，但其解决方案的多样性和鲁棒性往往不及人类程序员。我们指出模型认知与人类专业知识之间存在显著的分布差异，模型错误往往因训练数据偏差和有限的推理转移而聚集。通过引入人类生成的内容和模型混合，能够显著提升LLMs的性能和鲁棒性，为开发更强大的编码模型提供了潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2507.05791', 'title': 'GTA1: GUI Test-time Scaling Agent', 'url': 'https://huggingface.co/papers/2507.05791', 'abstract': 'GTA1 addresses task planning ambiguity and visual grounding in GUI interactions using test-time scaling and reinforcement learning, achieving state-of-the-art performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets.   This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements.   Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here.', 'score': 14, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'e20f930b7b567221', 'authors': ['Yan Yang', 'Dongxu Li', 'Yutong Dai', 'Yuhao Yang', 'Ziyang Luo', 'Zirui Zhao', 'Zhiyuan Hu', 'Junzhe Huang', 'Amrita Saha', 'Zeyuan Chen', 'Ran Xu', 'Liyuan Pan', 'Caiming Xiong', 'Junnan Li'], 'affiliations': ['Salesforce AI Research', 'The Australian National University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.05791.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#games', '#rl', '#agents', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'GTA1: Умный агент для автономного управления графическими интерфейсами', 'desc': 'Статья представляет GTA1 - агента для взаимодействия с графическим интерфейсом, решающего проблемы планирования задач и визуальной привязки действий. Метод использует масштабирование во время тестирования для выбора оптимальных действий и обучение с подкреплением для точного взаимодействия с визуальными элементами. GTA1 достигает наилучших результатов на нескольких бенчмарках, значительно улучшая точность и успешность выполнения задач. Авторы открыли исходный код и модели для дальнейших исследований.'}, 'en': {'title': 'GTA1: Mastering GUI Interactions with Smart Planning and Learning', 'desc': 'The paper presents GTA1, a novel approach to enhance task planning and visual grounding in graphical user interface (GUI) interactions using reinforcement learning. It addresses the challenges of ambiguity in action proposals by employing a test-time scaling method that samples multiple candidates and selects the best one through a judge model. Additionally, it improves the accuracy of grounding actions to visual elements by leveraging reinforcement learning, which aligns objectives with successful interactions. The results demonstrate that GTA1 achieves state-of-the-art performance on various benchmarks, showcasing its effectiveness in autonomous GUI task execution.'}, 'zh': {'title': 'GTA1：提升GUI交互的智能决策与视觉定位', 'desc': 'GTA1是一种图形用户界面（GUI）代理，旨在解决任务规划中的模糊性和视觉定位问题。它通过测试时缩放和强化学习的方法，优化了在复杂界面中与视觉元素的交互。该方法通过采样多个候选动作提案，并利用评判模型选择最合适的提案，从而提高决策质量。实验结果表明，GTA1在多个基准测试中达到了最先进的性能，展示了其在任务成功率和准确性方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2507.05169', 'title': 'Critiques of World Models', 'url': 'https://huggingface.co/papers/2507.05169', 'abstract': 'World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.', 'score': 12, 'issue_id': 4729, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'a6ec55259ef20f8f', 'authors': ['Eric Xing', 'Mingkai Deng', 'Jinyu Hou', 'Zhiting Hu'], 'affiliations': ['Halıcıoglu Data Science Institute, UC San Diego', 'Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence', 'School of Computer Science, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05169.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#agents', '#agi'], 'emoji': '🌍', 'ru': {'title': 'Новый взгляд на мировые модели для ИИ общего назначения', 'desc': 'Статья посвящена концепции Мировой Модели в контексте искусственного интеллекта. Авторы предлагают новую архитектуру для универсальной мировой модели, основанную на иерархических, многоуровневых и смешанных непрерывно-дискретных представлениях. Они утверждают, что основная цель мировой модели - моделирование всех возможных действий в реальном мире для целенаправленных рассуждений и действий. Статья также представляет генеративную систему обучения с самоконтролем и перспективу создания физической, агентной и вложенной (PAN) системы ИИ общего назначения.'}, 'en': {'title': 'Building Smarter Agents with Advanced World Models', 'desc': "This paper discusses the concept of a 'world model', which is a representation of the environment that artificial agents use to understand and interact with the world. It critiques existing approaches to building and evaluating these models, emphasizing the need for a system that can simulate all possible actions in a realistic manner. The authors propose a new architecture that combines hierarchical structures and mixed representations to enhance the model's capabilities. Ultimately, they envision a Physical, Agentic, and Nested (PAN) AGI system that leverages this advanced world model for better reasoning and decision-making."}, 'zh': {'title': '构建智能代理的世界模型', 'desc': '世界模型是生物体在真实环境中体验和行动的算法替代品，近年来因开发具有人工智能的虚拟代理的需求而受到关注。本文探讨了世界模型的定义、构建、使用和评估等问题，并批评了几种关于世界建模的理论。我们认为，世界模型的主要目标是模拟现实世界中所有可操作的可能性，以便进行有目的的推理和行动。基于这些批评，我们提出了一种新的通用世界模型架构，采用分层、多级和混合连续/离散表示，并结合生成和自我监督学习框架，展望基于此模型的物理、代理和嵌套的人工通用智能系统。'}}}, {'id': 'https://huggingface.co/papers/2507.06223', 'title': 'Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers', 'url': 'https://huggingface.co/papers/2507.06223', 'abstract': 'E\\textsuperscript{2}R-FLOPs evaluates LLM-based rerankers by measuring relevance and throughput per PetaFLOP, providing a hardware-agnostic metric for efficiency and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E2R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.', 'score': 11, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '6073d3ee8c07d225', 'authors': ['Zhiyuan Peng', 'Ting-ruen Wei', 'Tingyu Song', 'Yilun Zhao', 'Yi Fang'], 'affiliations': ['Independent Researcher, Beijing, China', 'Santa Clara University, Santa Clara, CA', 'Yale University, New Haven, CT'], 'pdf_title_img': 'assets/pdf/title_img/2507.06223.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#interpretability', '#architecture', '#inference'], 'emoji': '🔬', 'ru': {'title': 'E2R-FLOPs: Новый взгляд на эффективность LLM-ранжировщиков', 'desc': 'Статья представляет новую метрику E2R-FLOPs для оценки эффективности ранжировщиков на основе больших языковых моделей (LLM). Метрика измеряет релевантность и пропускную способность на ПетаФЛОП, предоставляя аппаратно-независимый способ оценки эффективности и результативности. Авторы разработали интерпретируемый оценщик ФЛОП для расчета вычислительной сложности LLM-ранжировщиков без необходимости проведения экспериментов. На основе предложенной метрики были проведены комплексные эксперименты для оценки различных LLM-ранжировщиков, изучая компромисс между эффективностью и результативностью.'}, 'en': {'title': 'E2R-FLOPs: A New Standard for Evaluating LLM Efficiency', 'desc': 'The paper introduces E2R-FLOPs, a new metric for evaluating the efficiency of Large Language Model (LLM)-based rerankers in information retrieval. It measures relevance and throughput per PetaFLOP, providing a hardware-agnostic way to assess performance. Traditional metrics like latency and token counts are limited by hardware dependencies and do not adequately reflect model size, making comparisons challenging. By using E2R-FLOPs, researchers can better understand the trade-offs between efficiency and effectiveness in LLM-based reranking tasks.'}, 'zh': {'title': 'E2R-FLOPs：高效评估LLM重排序器的工具', 'desc': 'E2R-FLOPs 是一种评估基于大型语言模型（LLM）的重排序器的新方法，通过每 PetaFLOP 的相关性和吞吐量来衡量其效率和有效性。这种方法解决了现有评估指标依赖于硬件和运行时间选择的问题，使得评估更加通用和易于理解。我们还构建了一个可解释的 FLOPs 估算器，可以在不进行实验的情况下估算 LLM 重排序器的 FLOPs。通过这些新指标，我们对多种不同架构的 LLM 重排序器进行了全面实验，研究了效率与有效性之间的权衡。'}}}, {'id': 'https://huggingface.co/papers/2507.05101', 'title': 'PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs', 'url': 'https://huggingface.co/papers/2507.05101', 'abstract': "Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.", 'score': 10, 'issue_id': 4715, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'c987593bed9476c8', 'authors': ['Xinzhe Zheng', 'Hao Du', 'Fanding Xu', 'Jinzhe Li', 'Zhiyuan Liu', 'Wenkang Wang', 'Tao Chen', 'Wanli Ouyang', 'Stan Z. Li', 'Yan Lu', 'Nanqing Dong', 'Yang Zhang'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong', 'Westlake University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05101.jpg', 'data': {'categories': ['#benchmark', '#data', '#open_source', '#dataset', '#graphs', '#leakage'], 'emoji': '🧬', 'ru': {'title': 'PRING: новый стандарт оценки предсказания белковых взаимодействий на уровне графов', 'desc': 'Статья представляет PRING - новый комплексный бенчмарк для оценки предсказания взаимодействий белок-белок (PPI) с точки зрения графов. PRING включает высококачественный набор данных PPI-сетей для нескольких видов, содержащий 21,484 белка и 186,818 взаимодействий. Бенчмарк предлагает две парадигмы оценки: задачи, ориентированные на топологию сети, и задачи, ориентированные на функции белков. Эксперименты показали, что современные модели PPI имеют ограничения в восстановлении структурных и функциональных свойств PPI-сетей.'}, 'en': {'title': 'Revolutionizing PPI Prediction with PRING: A Graph-Level Benchmark', 'desc': 'This paper introduces PRING, a new benchmark for evaluating protein-protein interaction (PPI) prediction models from a graph-level perspective. Unlike previous benchmarks that focused on pairwise evaluations, PRING assesses the ability of models to reconstruct meaningful PPI networks, which is essential for biological research. The benchmark includes a comprehensive dataset of 21,484 proteins and 186,818 interactions, addressing issues like data redundancy and leakage. The evaluation framework consists of topology-oriented and function-oriented tasks, revealing limitations in current PPI models and guiding future improvements in PPI prediction.'}, 'zh': {'title': 'PRING：蛋白质相互作用预测的新基准', 'desc': '本论文介绍了一种新的基准测试工具PRING，用于评估蛋白质-蛋白质相互作用（PPI）预测模型的能力。与以往的评估方法不同，PRING从图级别的角度出发，关注模型重建生物学意义的PPI网络。该基准数据集包含21,484个蛋白质和186,818个相互作用，旨在解决数据冗余和泄漏问题。通过拓扑导向和功能导向的评估任务，PRING帮助研究人员更好地理解PPI网络的结构和功能。'}}}, {'id': 'https://huggingface.co/papers/2507.03698', 'title': 'SAMed-2: Selective Memory Enhanced Medical Segment Anything Model', 'url': 'https://huggingface.co/papers/2507.03698', 'abstract': 'SAMed-2, an adaptation of SAM-2 for medical image segmentation, incorporates a temporal adapter and confidence-driven memory to improve performance across diverse medical datasets and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent "segment anything" efforts show promise by learning from large-scale data, but adapting such models directly to medical images remains challenging due to the complexity of medical data, noisy annotations, and continual learning requirements across diverse modalities and anatomical structures. In this work, we propose SAMed-2, a new foundation model for medical image segmentation built upon the SAM-2 architecture. Specifically, we introduce a temporal adapter into the image encoder to capture image correlations and a confidence-driven memory mechanism to store high-certainty features for later retrieval. This memory-based strategy counters the pervasive noise in large-scale medical datasets and mitigates catastrophic forgetting when encountering new tasks or modalities. To train and evaluate SAMed-2, we curate MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21 medical segmentation tasks. Our experiments on both internal benchmarks and 10 external datasets demonstrate superior performance over state-of-the-art baselines in multi-task scenarios. The code is available at: https://github.com/ZhilingYan/Medical-SAM-Bench.', 'score': 10, 'issue_id': 4715, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': '6eb9d67bc0e6c585', 'authors': ['Zhiling Yan', 'Sifan Song', 'Dingjie Song', 'Yiwei Li', 'Rong Zhou', 'Weixiang Sun', 'Zhennong Chen', 'Sekeun Kim', 'Hui Ren', 'Tianming Liu', 'Quanzheng Li', 'Xiang Li', 'Lifang He', 'Lichao Sun'], 'affiliations': ['Lehigh University, Bethlehem, PA, USA', 'Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA', 'University of Georgia, Athens, GA, USA', 'University of Notre Dame, Notre Dame, IN, USA'], 'pdf_title_img': 'assets/pdf/title_img/2507.03698.jpg', 'data': {'categories': ['#cv', '#benchmark', '#healthcare', '#data', '#dataset', '#training'], 'emoji': '🏥', 'ru': {'title': 'SAMed-2: Универсальный сегментатор для медицинских изображений', 'desc': 'SAMed-2 - это новая модель для сегментации медицинских изображений, основанная на архитектуре SAM-2. Она включает в себя временной адаптер для захвата корреляций между изображениями и механизм памяти, управляемый уверенностью, для хранения высокодостоверных признаков. Модель обучена на наборе данных MedBank-100k, охватывающем 7 модальностей визуализации и 21 задачу медицинской сегментации. Эксперименты показали превосходную производительность SAMed-2 по сравнению с современными базовыми моделями в многозадачных сценариях.'}, 'en': {'title': 'Enhancing Medical Image Segmentation with SAMed-2', 'desc': 'SAMed-2 is a new model designed for medical image segmentation, enhancing the original SAM-2 framework. It introduces a temporal adapter to the image encoder, which helps in understanding relationships between images over time. Additionally, a confidence-driven memory mechanism is implemented to retain important features, addressing issues like noisy data and preventing loss of knowledge when learning new tasks. The model is trained on a large dataset called MedBank-100k, showing improved performance in various medical imaging tasks compared to existing methods.'}, 'zh': {'title': '医学图像分割的新突破：SAMed-2', 'desc': 'SAMed-2是针对医学图像分割的SAM-2模型的改进版本。它引入了时间适配器和基于信心的记忆机制，以提高在不同医学数据集和任务中的表现。时间适配器帮助捕捉图像之间的相关性，而记忆机制则存储高置信度特征，以应对医学数据中的噪声和避免灾难性遗忘。通过构建MedBank-100k数据集并进行实验，SAMed-2在多任务场景中表现优于现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2507.05920', 'title': 'High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.05920', 'abstract': "MGPO, an end-to-end reinforcement learning framework, enhances large multi-modal models' ability to focus on key visual regions without requiring additional grounding annotations, improving performance on both in-distribution and out-of-distribution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art large multi-modal models (LMMs) face challenges when processing high-resolution images, as these inputs are converted into enormous visual tokens, many of which are irrelevant to the downstream task. In this paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an end-to-end reinforcement learning (RL) framework that enables LMMs to iteratively focus on key visual regions by automatically cropping sub-images, based on model-predicted grounding coordinates within a multi-turn conversation framework. Compared to supervised fine-tuning (SFT), which requires costly additional grounding annotations, our approach highlights that LMMs can emerge robust grounding abilities during the RL training process, leveraging only a binary reward function derived from the correctness of the final answer. Additionally, we observe that LMMs struggle to autonomously trigger visual grounding during the rollout process. To address this cold start problem, we design a multi-turn conversational template and restrict policy loss computation to model outputs generated across multiple dialogue rounds, thereby promoting stable optimization. Extensive experiments demonstrate that, when trained on standard visual-question-short answering data without grounding annotations, MGPO effectively elicits stronger grounding capabilities compared to GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and 5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench. Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at https://github.com/EvolvingLMMs-Lab/MGPO.", 'score': 9, 'issue_id': 4717, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'be1d59008c501b7d', 'authors': ['Xinyu Huang', 'Yuhao Dong', 'Weiwei Tian', 'Bo Li', 'Rui Feng', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05920.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#rl', '#games'], 'emoji': '🔍', 'ru': {'title': 'Фокусировка внимания мультимодальных моделей без дополнительной разметки', 'desc': 'MGPO - это фреймворк обучения с подкреплением для улучшения способности мультимодальных моделей фокусироваться на ключевых визуальных областях. Он использует итеративное обрезание подизображений на основе предсказанных моделью координат в рамках многоэтапного диалога. MGPO не требует дополнительных аннотаций для привязки к изображению, что отличает его от обучения с учителем. Эксперименты показывают, что MGPO превосходит базовые модели на стандартных и out-of-distribution бенчмарках.'}, 'en': {'title': 'Empowering LMMs with Reinforcement Learning for Visual Grounding', 'desc': "The paper introduces MGPO, a novel reinforcement learning framework designed to enhance the performance of large multi-modal models (LMMs) by enabling them to focus on important visual areas without needing extra grounding annotations. MGPO uses a multi-turn conversational approach to iteratively crop sub-images based on predicted grounding coordinates, improving the model's ability to ground visual information effectively. Unlike traditional supervised fine-tuning methods, MGPO relies on a simple binary reward system derived from the accuracy of the final output, allowing LMMs to develop grounding skills during training. The results show significant improvements in both in-distribution and out-of-distribution benchmarks, demonstrating MGPO's effectiveness in enhancing visual grounding capabilities in LMMs."}, 'zh': {'title': 'MGPO：无需标注的强化学习聚焦关键视觉区域', 'desc': 'MGPO是一种端到端的强化学习框架，旨在提高大型多模态模型在处理高分辨率图像时的关键视觉区域聚焦能力，而无需额外的标注。该方法通过在多轮对话框架中自动裁剪子图像，利用模型预测的定位坐标，帮助模型逐步聚焦于重要区域。与传统的监督微调方法相比，MGPO仅依赖于最终答案的正确性来提供二元奖励，从而在训练过程中培养出更强的定位能力。实验结果表明，MGPO在没有标注的情况下，能够显著提升模型在标准视觉问答任务上的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.05963', 'title': 'Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation', 'url': 'https://huggingface.co/papers/2507.05963', 'abstract': 'Tora2 enhances motion-guided video generation by introducing a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss, enabling simultaneous multi-entity customization and advanced motion control.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://github.com/alibaba/Tora .', 'score': 8, 'issue_id': 4716, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '0230e47a99a4f7d8', 'authors': ['Zhenghao Zhang', 'Junchao Liao', 'Xiangyu Meng', 'Long Qin', 'Weizhi Wang'], 'affiliations': ['Alibaba Group China'], 'pdf_title_img': 'assets/pdf/title_img/2507.05963.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Tora2: Революция в персонализированной генерации видео с управлением движением', 'desc': 'Tora2 - это усовершенствованная версия модели Tora для генерации видео на основе движения. Она вводит несколько улучшений, включая отдельный экстрактор персонализации для нескольких сущностей, механизм гейтированного самовнимания и контрастную функцию потерь. Эти инновации позволяют одновременно настраивать внешний вид и движение нескольких объектов в генерируемом видео. Tora2 демонстрирует конкурентоспособные результаты по сравнению с современными методами кастомизации, обеспечивая при этом расширенные возможности контроля движения.'}, 'en': {'title': 'Tora2: Revolutionizing Multi-Entity Video Customization', 'desc': 'Tora2 is an advanced model for generating videos that can be customized for multiple entities at the same time. It uses a decoupled personalization extractor to create detailed embeddings that capture the unique features of each entity. The model also incorporates a gated self-attention mechanism to effectively combine different types of information, such as motion and text descriptions, which helps improve the alignment during training. Additionally, Tora2 employs a contrastive loss to ensure that the generated motion is consistent with the personalized features, making it a significant step forward in motion-guided video generation.'}, 'zh': {'title': 'Tora2：多实体个性化与运动控制的突破', 'desc': 'Tora2 是一种增强的运动引导视频生成模型，采用了分离的个性化提取器和门控自注意力机制。它能够同时对多个实体进行个性化定制，并实现更高级的运动控制。通过引入对比损失，Tora2 优化了运动动态和实体一致性，提升了多模态条件下的对齐效果。实验结果表明，Tora2 在个性化定制方面的表现与最先进的方法相当，同时提供了更强的运动控制能力。'}}}, {'id': 'https://huggingface.co/papers/2507.04723', 'title': 'LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework', 'url': 'https://huggingface.co/papers/2507.04723', 'abstract': "Long-context processing has become a fundamental capability for large language models~(LLMs). To assess model's long-context performance, numerous long-context evaluation benchmarks have been proposed. However, variations in evaluation settings across these benchmarks lead to inconsistent results, making it difficult to draw reliable comparisons. Besides, the high computational cost of long-context evaluation poses a significant barrier for the community to conduct comprehensive assessments of long-context models. In this paper, we propose LOOM-Scope, a comprehensive and efficient framework for long-context evaluation. LOOM-Scope standardizes evaluation settings across diverse benchmarks, supports deployment of efficient long-context inference acceleration methods, and introduces a holistic yet lightweight benchmark suite to evaluate models comprehensively. Homepage: https://loomscope.github.io", 'score': 8, 'issue_id': 4718, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '14c1c5cb4e9b0c49', 'authors': ['Zecheng Tang', 'Haitian Wang', 'Quantong Qiu', 'Baibei Ji', 'Ruoxi Sun', 'Keyan Zhou', 'Juntao Li', 'Min Zhang'], 'affiliations': ['Key Laboratory of Data Intelligence and Advanced Computing, Soochow University', 'Soochow University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.04723.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#inference'], 'emoji': '🔬', 'ru': {'title': 'LOOM-Scope: стандартизация и оптимизация оценки языковых моделей для длинных контекстов', 'desc': 'LOOM-Scope - это комплексная и эффективная система для оценки способностей языковых моделей обрабатывать длинные контексты. Она стандартизирует настройки оценки для различных бенчмарков, что позволяет проводить более надежные сравнения моделей. LOOM-Scope поддерживает внедрение методов ускорения вывода для длинных контекстов, снижая вычислительные затраты. Кроме того, система предлагает целостный, но легковесный набор бенчмарков для всесторонней оценки языковых моделей.'}, 'en': {'title': 'Standardizing Long-Context Evaluation for Better Comparisons', 'desc': 'This paper introduces LOOM-Scope, a new framework designed to improve the evaluation of long-context performance in large language models (LLMs). It addresses the inconsistencies in results caused by varying evaluation settings across existing benchmarks. LOOM-Scope not only standardizes these settings but also incorporates efficient inference acceleration methods to reduce computational costs. Additionally, it provides a lightweight benchmark suite that allows for comprehensive assessments of long-context models, facilitating better comparisons within the research community.'}, 'zh': {'title': 'LOOM-Scope：高效的长文本评估框架', 'desc': '长文本处理已成为大型语言模型（LLMs）的基本能力。为了评估模型在长文本上的表现，提出了许多长文本评估基准。然而，这些基准的评估设置差异导致结果不一致，使得可靠比较变得困难。此外，长文本评估的高计算成本也成为社区进行全面评估的重大障碍。本文提出了LOOM-Scope，一个全面且高效的长文本评估框架，标准化了不同基准的评估设置，并支持高效的长文本推理加速方法。'}}}, {'id': 'https://huggingface.co/papers/2507.06204', 'title': 'Differential Mamba', 'url': 'https://huggingface.co/papers/2507.06204', 'abstract': 'A novel differential mechanism for Mamba, a selective state-space layer architecture, improves retrieval capabilities and performance by addressing overallocation issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available.', 'score': 7, 'issue_id': 4720, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '69109b333dac5628', 'authors': ['Nadav Schneider', 'Itamar Zimerman', 'Eliya Nachmani'], 'affiliations': ['Ben-Gurion University', 'IBM Research', 'School of Electrical and Computer Engineering, Ben Gurion University of the Negev', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2507.06204.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#optimization', '#long_context', '#hallucinations', '#architecture'], 'emoji': '🐍', 'ru': {'title': 'Дифференциальная Mamba: точнее, эффективнее, умнее', 'desc': 'Статья представляет новый дифференциальный механизм для архитектуры Mamba, основанной на селективных слоях пространства состояний. Этот механизм решает проблему избыточного выделения внимания нерелевантному контексту, характерную для моделей последовательностей. Авторы демонстрируют улучшение способностей извлечения информации и общей производительности модели по сравнению с ванильной версией Mamba. Проведены обширные эксперименты и абляционные исследования для обоснования предложенного подхода.'}, 'en': {'title': 'Enhancing Mamba: A New Approach to Mitigate Overallocation in Language Models', 'desc': 'This paper presents a new differential mechanism for the Mamba architecture, which uses selective state-space layers to enhance performance in language modeling. The authors identify that traditional sequence models, like Transformers, often focus too much on irrelevant information, leading to poor results. By adapting differential design techniques to Mamba, they show that careful modifications can significantly improve retrieval capabilities and overall performance. The results from their experiments indicate that this new approach effectively reduces the overallocation problem, making Mamba models more robust and efficient.'}, 'zh': {'title': '提升Mamba架构性能的新机制', 'desc': '本文提出了一种新颖的差分机制，用于Mamba架构，这是一种选择性状态空间层的设计，旨在改善检索能力和性能。传统的序列模型如Transformer和RNN常常对无关上下文过度关注，导致中间表示噪声增多，从而影响大语言模型的能力。我们发现，简单地将差分设计应用于Mamba并不足够，需要进行仔细的架构修改。通过实验证明，我们的新机制有效缓解了Mamba模型中的过度分配问题，提升了其检索能力和整体性能。'}}}, {'id': 'https://huggingface.co/papers/2507.05578', 'title': 'The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation', 'url': 'https://huggingface.co/papers/2507.05578', 'abstract': 'The paper reviews recent studies on memorization in Large Language Models, exploring factors that influence memorization, detection methodologies, and mitigation strategies, while addressing privacy and ethical implications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they also exhibit memorization of their training data. This phenomenon raises critical questions about model behavior, privacy risks, and the boundary between learning and memorization. Addressing these concerns, this paper synthesizes recent studies and investigates the landscape of memorization, the factors influencing it, and methods for its detection and mitigation. We explore key drivers, including training data duplication, training dynamics, and fine-tuning procedures that influence data memorization. In addition, we examine methodologies such as prefix-based extraction, membership inference, and adversarial prompting, assessing their effectiveness in detecting and measuring memorized content. Beyond technical analysis, we also explore the broader implications of memorization, including the legal and ethical implications. Finally, we discuss mitigation strategies, including data cleaning, differential privacy, and post-training unlearning, while highlighting open challenges in balancing the minimization of harmful memorization with utility. This paper provides a comprehensive overview of the current state of research on LLM memorization across technical, privacy, and performance dimensions, identifying critical directions for future work.', 'score': 4, 'issue_id': 4715, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '9fd6f105854c8570', 'authors': ['Alexander Xiong', 'Xuandong Zhao', 'Aneesh Pappu', 'Dawn Song'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.05578.jpg', 'data': {'categories': ['#healthcare', '#hallucinations', '#survey', '#data', '#training', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Запоминание в LLM: от технических аспектов до этических проблем', 'desc': 'Статья рассматривает недавние исследования запоминания в больших языковых моделях (LLM). Авторы изучают факторы, влияющие на запоминание, методологии его обнаружения и стратегии смягчения последствий. Рассматриваются такие аспекты, как дублирование обучающих данных, динамика обучения и процедуры тонкой настройки. Также обсуждаются правовые и этические последствия запоминания в LLM.'}, 'en': {'title': 'Understanding and Mitigating Memorization in Large Language Models', 'desc': 'This paper reviews how Large Language Models (LLMs) memorize information from their training data, which can lead to privacy concerns. It discusses factors that contribute to this memorization, such as data duplication and training methods. The paper also evaluates various techniques for detecting memorized data, like membership inference and adversarial prompting. Finally, it suggests strategies to reduce harmful memorization while maintaining model performance, highlighting the need for further research in this area.'}, 'zh': {'title': '大型语言模型的记忆现象与挑战', 'desc': '这篇论文回顾了关于大型语言模型（LLM）记忆现象的最新研究，探讨了影响记忆的因素、检测方法和缓解策略，同时关注隐私和伦理问题。研究表明，LLM在执行任务时会记住训练数据，这引发了关于模型行为和隐私风险的关键问题。论文分析了训练数据重复、训练动态和微调过程等关键驱动因素，并评估了前缀提取、成员推断和对抗性提示等检测方法的有效性。最后，论文讨论了数据清理、差分隐私和后训练遗忘等缓解策略，强调在减少有害记忆与保持模型效用之间的挑战。'}}}, {'id': 'https://huggingface.co/papers/2507.04610', 'title': 'any4: Learned 4-bit Numeric Representation for LLMs', 'url': 'https://huggingface.co/papers/2507.04610', 'abstract': 'any4 is a learned 4-bit weight quantization method for LLMs that achieves high accuracy without preprocessing and uses a GPU-efficient lookup table strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present any4, a learned 4-bit weight quantization solution for large language models (LLMs) providing arbitrary numeric representations without requiring pre-processing of weights or activations. any4 yields higher accuracy compared to other related 4-bit numeric representation types: int4, fp4 and nf4, as evaluated on a range of model sizes, generations and families (Llama 2, Llama 3, Mistral and Mixtral). While any4 does not require preprocessing of weights or activations, it is also competitive with orthogonal techniques that require such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3 and any2 and show competitiveness at lower bits. Additionally, we show that we can calibrate using a single curated diverse sample rather than hundreds of samples from a dataset as done in most quantization approaches. We also open source tinygemm, a latency optimized GPU matrix multiplication library for LLMs, that implements any4 using a GPU-efficient lookup table strategy along with other common quantization methods. We open source our code at https://github.com/facebookresearch/any4 .', 'score': 4, 'issue_id': 4715, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '677d34e801c63489', 'authors': ['Mostafa Elhoushi', 'Jeff Johnson'], 'affiliations': ['FAIR at Meta'], 'pdf_title_img': 'assets/pdf/title_img/2507.04610.jpg', 'data': {'categories': ['#open_source', '#optimization', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'any4: Эффективная квантизация LLM без компромиссов', 'desc': 'Статья представляет any4 - метод обучаемой 4-битной квантизации весов для больших языковых моделей (LLM). Этот метод обеспечивает высокую точность без предварительной обработки весов или активаций, превосходя другие 4-битные представления. any4 использует эффективную для GPU стратегию поиска по таблице и конкурентоспособен с методами, требующими предобработки. Исследователи также экспериментировали с any3 и any2, показав их эффективность при меньшем количестве битов.'}, 'en': {'title': 'any4: Efficient 4-Bit Weight Quantization for High-Accuracy LLMs', 'desc': 'The paper introduces any4, a novel method for quantizing weights in large language models (LLMs) to 4 bits, which maintains high accuracy without the need for preprocessing. This method outperforms existing 4-bit representations like int4, fp4, and nf4 across various model sizes and families. Additionally, any4 allows for calibration using just one diverse sample, contrasting with traditional methods that require many samples. The authors also provide an open-source GPU-optimized library, tinygemm, to implement this quantization technique efficiently.'}, 'zh': {'title': 'any4：高效的4位权重量化方法', 'desc': 'any4是一种针对大型语言模型（LLMs）的学习型4位权重量化方法，能够在不需要预处理的情况下实现高精度的数值表示。与其他4位数值表示方法（如int4、fp4和nf4）相比，any4在多种模型规模和类型上表现出更高的准确性。该方法还可以使用单个多样化样本进行校准，而不是像大多数量化方法那样需要数百个样本。此外，我们开源了tinygemm，这是一个针对LLMs优化的GPU矩阵乘法库，采用了高效的查找表策略来实现any4。'}}}, {'id': 'https://huggingface.co/papers/2507.06230', 'title': 'Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion', 'url': 'https://huggingface.co/papers/2507.06230', 'abstract': 'SceneDINO achieves state-of-the-art segmentation accuracy in unsupervised semantic scene completion by leveraging self-supervised representation learning and 2D unsupervised scene understanding techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding.', 'score': 3, 'issue_id': 4725, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '12d51a9f4c4400e9', 'authors': ['Aleksandar Jevtić', 'Christoph Reich', 'Felix Wimbauer', 'Oliver Hahn', 'Christian Rupprecht', 'Stefan Roth', 'Daniel Cremers'], 'affiliations': ['ELIZA', 'MCML', 'TU Darmstadt', 'TU Munich', 'University of Oxford', 'hessian.AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.06230.jpg', 'data': {'categories': ['#optimization', '#cv', '#transfer_learning', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'Несупервизорное 3D понимание сцены по одному изображению', 'desc': 'SceneDINO - это новый метод для несупервизорного семантического завершения сцены (SSC). Он использует самоконтролируемое обучение представлений и техники несупервизорного понимания 2D сцен для SSC. SceneDINO обучается исключительно на самоконтроле согласованности нескольких ракурсов, без использования семантической или геометрической разметки. Метод достигает современного уровня точности сегментации в несупервизорном понимании 3D и 2D сцен.'}, 'en': {'title': 'Unsupervised 3D Scene Understanding with SceneDINO', 'desc': 'SceneDINO is a novel method for unsupervised semantic scene completion (SSC) that excels in accurately inferring 3D geometry and semantics from single images. It utilizes self-supervised representation learning and 2D scene understanding techniques, avoiding the need for expensive ground-truth annotations. The model leverages multi-view consistency self-supervision to train, allowing it to generate 3D features and semantics without any labeled data. SceneDINO achieves state-of-the-art segmentation accuracy, demonstrating its effectiveness in both 3D and 2D scene understanding tasks.'}, 'zh': {'title': '无监督语义场景补全的新突破', 'desc': 'SceneDINO是一种无监督的语义场景补全方法，能够从单张图像中推断出场景的3D几何形状和语义信息。该方法利用自监督表示学习和2D无监督场景理解技术，避免了对昂贵的真实标注的依赖。通过多视图一致性的自我监督训练，SceneDINO在无监督的情况下实现了最先进的分割精度。该方法的3D特征蒸馏技术使得我们能够获得无监督的3D语义，为单图像3D场景理解奠定了坚实的基础。'}}}, {'id': 'https://huggingface.co/papers/2507.05201', 'title': 'MedGemma Technical Report', 'url': 'https://huggingface.co/papers/2507.05201', 'abstract': "MedGemma, a collection of medical vision-language foundation models, demonstrates advanced understanding and reasoning in healthcare applications, improving performance across various tasks and maintaining general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.", 'score': 3, 'issue_id': 4728, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '7f5f922ddf886fbb', 'authors': ['Andrew Sellergren', 'Sahar Kazemzadeh', 'Tiam Jaroensri', 'Atilla Kiraly', 'Madeleine Traverse', 'Timo Kohlberger', 'Shawn Xu', 'Fayaz Jamil', 'Cían Hughes', 'Charles Lau', 'Justin Chen', 'Fereshteh Mahvar', 'Liron Yatziv', 'Tiffany Chen', 'Bram Sterling', 'Stefanie Anna Baby', 'Susanna Maria Baby', 'Jeremy Lai', 'Samuel Schmidgall', 'Lu Yang', 'Kejia Chen', 'Per Bjornsson', 'Shashir Reddy', 'Ryan Brush', 'Kenneth Philbrick', 'Howard Hu', 'Howard Yang', 'Richa Tiwari', 'Sunny Jansen', 'Preeti Singh', 'Yun Liu', 'Shekoofeh Azizi', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre Ramé', 'Morgane Riviere', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Dmitry Lepikhin', 'Vlad Feinberg', 'Sebastian Borgeaud', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'Léonard Hussenot', 'Armand Joulin', 'Olivier Bachem', 'Yossi Matias', 'Katherine Chou', 'Avinatan Hassidim', 'Kavi Goel', 'Clement Farabet', 'Joelle Barral', 'Tris Warkentin', 'Jonathon Shlens', 'David Fleet', 'Victor Cotruta', 'Omar Sanseviero', 'Gus Martins', 'Phoebe Kirk', 'Anand Rao', 'Shravya Shetty', 'David F. Steiner', 'Can Kirmizibayrak', 'Rory Pilgrim', 'Daniel Golden', 'Lin Yang'], 'affiliations': ['Google DeepMind', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.05201.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#multimodal', '#science', '#dataset', '#training', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'MedGemma: Революция в медицинском ИИ', 'desc': 'MedGemma - это набор медицинских моделей компьютерного зрения и обработки естественного языка, основанных на Gemma 3. Модели демонстрируют продвинутое понимание и рассуждение в медицинских приложениях, значительно превосходя производительность аналогичных генеративных моделей. MedGemma улучшает результаты в задачах мультимодального вопросно-ответного анализа, классификации рентгеновских снимков грудной клетки и агентных оценках. Модели сохраняют общие возможности базовых моделей Gemma 3, предоставляя сильную основу для медицинских исследований и разработки приложений.'}, 'en': {'title': 'MedGemma: Revolutionizing Healthcare AI with Vision-Language Models', 'desc': "MedGemma is a set of advanced medical vision-language foundation models designed to enhance AI applications in healthcare. It excels in understanding and reasoning with medical images and text, outperforming similar models and nearing the effectiveness of specialized systems. The models show significant improvements in various tasks, such as medical question answering and chest X-ray classification, while also reducing errors in electronic health record retrieval. Additionally, MedSigLIP, a vision encoder, boosts MedGemma's capabilities, making it a valuable resource for accelerating medical research and application development."}, 'zh': {'title': 'MedGemma：加速医疗AI应用的基础模型', 'desc': 'MedGemma是一个医疗视觉-语言基础模型的集合，展示了在医疗应用中的高级理解和推理能力。它在多个任务上表现出色，并保持了Gemma 3基础模型的通用能力。MedGemma在医学多模态问答、胸部X光发现分类等任务上显著提高了性能，尤其是在处理分布外任务时。通过微调，MedGemma在特定领域的表现进一步提升，错误率降低了50%，并与现有的专业方法相媲美。'}}}, {'id': 'https://huggingface.co/papers/2507.03728', 'title': 'FAROS: Fair Graph Generation via Attribute Switching Mechanisms', 'url': 'https://huggingface.co/papers/2507.03728', 'abstract': "FAROS is a framework that enhances fairness in graph diffusion models by strategically switching node attributes during generation to balance accuracy and fairness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in graph diffusion models (GDMs) have enabled the synthesis of realistic network structures, yet ensuring fairness in the generated data remains a critical challenge. Existing solutions attempt to mitigate bias by re-training the GDMs with ad-hoc fairness constraints. Conversely, with this work, we propose FAROS, a novel FAir graph geneRatiOn framework leveraging attribute Switching mechanisms and directly running in the generation process of the pre-trained GDM. Technically, our approach works by altering nodes' sensitive attributes during the generation. To this end, FAROS calculates the optimal fraction of switching nodes, and selects the diffusion step to perform the switch by setting tailored multi-criteria constraints to preserve the node-topology profile from the original distribution (a proxy for accuracy) while ensuring the edge independence on the sensitive attributes for the generated graph (a proxy for fairness). Our experiments on benchmark datasets for link prediction demonstrate that the proposed approach effectively reduces fairness discrepancies while maintaining comparable (or even higher) accuracy performance to other similar baselines. Noteworthy, FAROS is also able to strike a better accuracy-fairness trade-off than other competitors in some of the tested settings under the Pareto optimality concept, demonstrating the effectiveness of the imposed multi-criteria constraints.", 'score': 1, 'issue_id': 4723, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': 'a0d6f2a3eb887939', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#graphs', '#ethics', '#benchmark', '#multimodal', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'Справедливая генерация графов без ущерба для точности', 'desc': 'FAROS - это фреймворк, который улучшает справедливость в графовых диффузионных моделях путем стратегического переключения атрибутов узлов во время генерации. Он рассчитывает оптимальную долю переключаемых узлов и выбирает шаг диффузии для переключения, устанавливая специальные многокритериальные ограничения. Это позволяет сохранить профиль топологии узлов из исходного распределения, обеспечивая при этом независимость ребер от чувствительных атрибутов в сгенерированном графе. Эксперименты показывают, что FAROS эффективно снижает неравенство, сохраняя при этом сопоставимую или даже более высокую точность по сравнению с другими базовыми методами.'}, 'en': {'title': 'Balancing Fairness and Accuracy in Graph Generation with FAROS', 'desc': 'FAROS is a framework designed to improve fairness in graph diffusion models (GDMs) by modifying node attributes during the generation process. It addresses the challenge of bias in generated data by strategically switching sensitive attributes of nodes while preserving the overall network structure. The framework calculates the optimal number of nodes to switch and selects the appropriate diffusion steps to maintain accuracy and fairness. Experimental results show that FAROS not only reduces fairness discrepancies but also achieves competitive accuracy compared to existing methods, highlighting its effectiveness in balancing these two important aspects.'}, 'zh': {'title': 'FAROS：平衡准确性与公平性的图生成框架', 'desc': 'FAROS是一个框架，旨在通过在生成过程中战略性地切换节点属性来增强图扩散模型的公平性。该方法在生成预训练图扩散模型时，动态调整节点的敏感属性，以平衡生成数据的准确性和公平性。FAROS通过计算最佳切换节点的比例，并设置多标准约束，确保生成图的边缘独立性和节点拓扑特征的保留。实验结果表明，FAROS在减少公平性差异的同时，能够保持与其他基线相当或更高的准确性。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2507.06137', 'title': 'NeoBabel: A Multilingual Open Tower for Visual Generation', 'url': 'https://huggingface.co/papers/2507.06137', 'abstract': 'NeoBabel, a multilingual image generation framework, achieves state-of-the-art performance across six languages while maintaining efficiency and cultural alignment, outperforming existing multilingual models.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.', 'score': 0, 'issue_id': 4733, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '15b4ba4c927c0d0b', 'authors': ['Mohammad Mahdi Derakhshani', 'Dheeraj Varghese', 'Marzieh Fadaee', 'Cees G. M. Snoek'], 'affiliations': ['Cohere Labs', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2507.06137.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#alignment', '#benchmark', '#open_source'], 'emoji': '🌍', 'ru': {'title': 'NeoBabel: прорыв в мультиязычной генерации изображений', 'desc': 'NeoBabel - это новая мультиязычная система генерации изображений, поддерживающая шесть языков. Она достигает наилучших результатов в многоязычных тестах, сохраняя при этом высокую производительность на английском языке. Модель обучена с использованием комбинации крупномасштабного мультиязычного предобучения и высокоточной настройки инструкций. NeoBabel демонстрирует, что многоязычность не является компромиссом, а катализатором повышения надежности, эффективности и культурной точности в генеративном ИИ.'}, 'en': {'title': 'Empowering Multilingual Image Generation with NeoBabel', 'desc': 'NeoBabel is a multilingual image generation framework that excels in creating images from text in six different languages, achieving top performance while being efficient and culturally relevant. Unlike previous models that relied on translation, which often led to inaccuracies and inefficiencies, NeoBabel uses a unique training approach that combines multilingual pretraining with high-resolution instruction tuning. It has been evaluated using new multilingual benchmarks and shows superior performance compared to existing models, particularly in multilingual tasks. The framework is designed to be smaller and more efficient, proving that supporting multiple languages can enhance the overall quality and robustness of AI-generated content.'}, 'zh': {'title': 'NeoBabel：多语言生成的未来', 'desc': 'NeoBabel是一个多语言图像生成框架，能够在六种语言中实现最先进的性能，同时保持高效性和文化一致性，超越了现有的多语言模型。该模型通过大规模的多语言预训练和高分辨率的指令调优进行训练，支持英语、中文、荷兰语、法语、印地语和波斯语。NeoBabel在多语言基准测试中表现优异，尤其在英语任务上与领先模型相当，同时在多语言基准上超出它们。我们的研究表明，多语言能力不仅不是一种权衡，而是生成AI中提高鲁棒性、高效性和文化忠实度的催化剂。'}}}, {'id': 'https://huggingface.co/papers/2507.05411', 'title': 'AXLearn: Modular Large Model Training on Heterogeneous Infrastructure', 'url': 'https://huggingface.co/papers/2507.05411', 'abstract': "AXLearn is a modular deep learning system designed for scalable training on heterogeneous hardware, maintaining performance and modularity through efficient code integration methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We design and implement AXLearn, a production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-the-art deep learning systems, AXLearn has a unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn.", 'score': 0, 'issue_id': 4722, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'b9bc54d8cca9de71', 'authors': ['Mark Lee', 'Tom Gunter', 'Chang Lan', 'John Peebles', 'Hanzhi Zhou', 'Kelvin Zou', 'Sneha Bangalore', 'Chung-Cheng Chiu', 'Nan Du', 'Xianzhi Du', 'Philipp Dufter', 'Ruixuan Hou', 'Haoshuo Huang', 'Dongseong Hwang', 'Xiang Kong', 'Jinhao Lei', 'Tao Lei', 'Meng Li', 'Li Li', 'Jiarui Lu', 'Zhiyun Lu', 'Yiping Ma', 'David Qiu', 'Vivek Rathod', 'Senyu Tong', 'Zhucheng Tu', 'Jianyu Wang', 'Yongqiang Wang', 'Zirui Wang', 'Floris Weers', 'Sam Wiseman', 'Guoli Yin', 'Bowen Zhang', 'Xiyou Zhou', 'Danyang Zhuo', 'Cheng Leong', 'Ruoming Pang'], 'affiliations': ['Apple', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05411.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Модульность и эффективность в глубоком обучении', 'desc': 'AXLearn - это модульная система глубокого обучения, разработанная для масштабируемого обучения на гетерогенном оборудовании. Система фокусируется на модульности и поддержке разнородной аппаратной инфраструктуры, сохраняя при этом высокую производительность. AXLearn использует строгую инкапсуляцию между программными компонентами, что позволяет быстро разрабатывать и экспериментировать с моделями. Авторы вводят метод количественной оценки модульности через сложность в строках кода, демонстрируя постоянную сложность при масштабировании компонентов системы.'}, 'en': {'title': 'Modular Deep Learning for Scalable Performance', 'desc': 'AXLearn is a deep learning system that allows for efficient training of large models on various types of hardware. It emphasizes modularity, meaning different parts of the system can be easily combined and reused, which speeds up the development process. The system uses a unique way to measure modularity, showing that it keeps complexity low even as more components are added. This design enables quick integration of new features while maintaining high performance, making AXLearn a competitive choice among existing deep learning frameworks.'}, 'zh': {'title': 'AXLearn：高效模块化的深度学习系统', 'desc': 'AXLearn是一个模块化的深度学习系统，旨在支持在异构硬件上进行可扩展的训练。它通过高效的代码集成方法，保持了性能和模块化的特点。AXLearn的内部接口遵循严格的封装原则，使得不同组件可以快速组装，便于在异构计算基础设施上进行模型开发和实验。我们还提出了一种新的量化模块化的方法，通过代码行复杂度（LoC复杂度）来展示系统在扩展组件时保持恒定复杂度的能力。'}}}, {'id': 'https://huggingface.co/papers/2507.09862', 'title': 'SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation', 'url': 'https://huggingface.co/papers/2507.09862', 'abstract': 'A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/', 'score': 39, 'issue_id': 4814, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '67b82ab227be6ce9', 'authors': ['Youliang Zhang', 'Zhaoyang Li', 'Duomin Wang', 'Jiahe Zhang', 'Deyu Zhou', 'Zixin Yin', 'Xili Dai', 'Gang Yu', 'Xiu Li'], 'affiliations': ['StepFun', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.09862.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SpeakerVid-5M: Революция в создании интерактивных виртуальных людей', 'desc': 'Представлен крупномасштабный набор данных SpeakerVid-5M для генерации аудиовизуальных диадических интерактивных виртуальных людей. Датасет содержит более 5,2 миллиона видеоклипов с портретами людей, охватывающих различные типы взаимодействий. Структура набора данных организована по двум ключевым измерениям: типу взаимодействия и качеству данных. Авторы также предоставляют базовую модель видеочата на основе авторегрессии и набор метрик для оценки будущих работ в этой области.'}, 'en': {'title': 'Unlocking Interactive Virtual Humans with SpeakerVid-5M', 'desc': 'The paper introduces the SpeakerVid-5M dataset, a large-scale resource designed for generating audio-visual dyadic interactive virtual humans. It consists of over 8,743 hours of video data, featuring more than 5.2 million clips that capture various interaction types, such as monadic and dyadic conversations. The dataset is organized into two main categories: interaction type and data quality, allowing for effective pre-training and fine-tuning of models. Additionally, the authors present a baseline video chat model and a benchmark called VidChatBench to facilitate future research in this area.'}, 'zh': {'title': 'SpeakerVid-5M：音视频互动虚拟人的新里程碑', 'desc': '本文介绍了一个名为SpeakerVid-5M的大规模数据集，旨在生成音视频双向互动的虚拟人。该数据集包含超过8743小时的高质量视频片段，涵盖多种互动类型，如单向对话和双向对话。数据集按照互动类型和数据质量两个维度进行结构化，支持多种2D虚拟人任务。我们还提供了基于自回归模型的视频聊天基线，并设立了VidChatBench作为未来研究的基准。'}}}, {'id': 'https://huggingface.co/papers/2507.10532', 'title': 'Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination', 'url': 'https://huggingface.co/papers/2507.10532', 'abstract': 'Research on enhancing LLM reasoning through RL reveals that accurate reward signals are crucial for performance improvement, and current benchmarks may be unreliable due to data contamination.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.', 'score': 37, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '39054d8e3e70cc7b', 'authors': ['Mingqi Wu', 'Zhihao Zhang', 'Qiaole Dong', 'Zhiheng Xi', 'Jun Zhao', 'Senjie Jin', 'Xiaoran Fan', 'Yuhao Zhou', 'Yanwei Fu', 'Qin Liu', 'Songyang Zhang', 'Qi Zhang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2507.10532.jpg', 'data': {'categories': ['#leakage', '#dataset', '#synthetic', '#benchmark', '#rl', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Чистые данные - ключ к надежному обучению с подкреплением для LLM', 'desc': 'Исследование показывает, что для улучшения способностей больших языковых моделей (LLM) к рассуждению с помощью обучения с подкреплением критически важны точные сигналы вознаграждения. Обнаружено, что существующие бенчмарки могут быть ненадежными из-за загрязнения данных. Авторы предлагают новый чистый набор данных RandomCalculation для оценки методов обучения с подкреплением. Результаты подтверждают, что только точные сигналы вознаграждения последовательно улучшают производительность моделей.'}, 'en': {'title': 'Accurate Rewards: The Key to Enhancing LLM Reasoning', 'desc': 'This paper investigates how to improve the reasoning abilities of large language models (LLMs) using reinforcement learning (RL). It highlights the importance of accurate reward signals for enhancing performance, noting that current benchmarks may be flawed due to data contamination. The authors introduce a new dataset called RandomCalculation, which consists of synthetic arithmetic problems, to provide a cleaner evaluation environment. Their findings suggest that only precise reward signals lead to consistent improvements in reasoning, emphasizing the need for reliable benchmarks across different model families.'}, 'zh': {'title': '准确奖励信号是提升推理能力的关键', 'desc': '本研究探讨了通过强化学习（RL）提升大型语言模型（LLM）推理能力的重要性，强调准确的奖励信号对性能提升至关重要。当前的基准测试可能因数据污染而不可靠，导致研究结果的可信度下降。尽管Qwen2.5模型在数学推理方面表现出色，但其在大规模网络语料库上的预训练使其易受污染影响。为了解决这一问题，我们引入了一种生成器，创建了完全合成的算术问题数据集，证明只有准确的奖励信号才能持续提升模型性能。'}}}, {'id': 'https://huggingface.co/papers/2507.10524', 'title': 'Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation', 'url': 'https://huggingface.co/papers/2507.10524', 'abstract': 'Mixture-of-Recursions (MoR) achieves parameter and computational efficiency in large language models through shared layers and adaptive recursion depths, improving performance metrics and throughput.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.', 'score': 26, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': 'f28faa5fbfe7a41c', 'authors': ['Sangmin Bae', 'Yujin Kim', 'Reza Bayat', 'Sungnyun Kim', 'Jiyoun Ha', 'Tal Schuster', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Aaron Courville', 'Se-Young Yun'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Google Research', 'KAIST AI', 'Mila', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2507.10524.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Эффективность больших языковых моделей через рекурсию и разделение параметров', 'desc': 'Статья представляет Mixture-of-Recursions (MoR) - новый подход к повышению эффективности больших языковых моделей. MoR объединяет разделение параметров и адаптивные вычисления в рамках рекурсивного трансформера. Метод использует общий стек слоев и динамически назначает разную глубину рекурсии для отдельных токенов. MoR демонстрирует улучшение метрик производительности и пропускной способности по сравнению с базовыми моделями при меньшем размере модели.'}, 'en': {'title': 'Efficient Language Modeling with Mixture-of-Recursions', 'desc': 'Mixture-of-Recursions (MoR) is a novel framework designed to enhance the efficiency of large language models by integrating parameter sharing and adaptive recursion depths. It utilizes a shared stack of layers across recursion steps, which reduces the number of parameters needed while maintaining performance. Additionally, MoR employs lightweight routers to assign different recursion depths to individual tokens, optimizing attention computation and memory access. This approach not only improves validation perplexity and few-shot accuracy but also increases throughput, making it a cost-effective solution for high-quality language modeling.'}, 'zh': {'title': '高效递归，提升语言模型性能', 'desc': 'Mixture-of-Recursions（MoR）是一种新颖的框架，通过共享层和自适应递归深度，在大型语言模型中实现参数和计算效率的提升。MoR在递归步骤中重用共享的层堆栈，从而提高参数效率，同时通过轻量级路由器动态分配不同的递归深度，实现自适应计算。该方法使得模型能够在特定递归深度中仅关注活跃的token，从而优化内存访问效率。实验结果表明，MoR在多个模型规模下显著降低了验证困惑度，并提高了少量样本的准确性，展示了在不增加成本的情况下实现大型模型质量的有效路径。'}}}, {'id': 'https://huggingface.co/papers/2507.10548', 'title': 'EmbRACE-3K: Embodied Reasoning and Action in Complex Environments', 'url': 'https://huggingface.co/papers/2507.10548', 'abstract': "A new dataset, EmRACE-3K, evaluates vision-language models in embodied settings, showing limitations in spatial reasoning and long-horizon planning, and demonstrates improvements through supervised and reinforcement learning fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.", 'score': 24, 'issue_id': 4817, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '046b6dd86b975bd1', 'authors': ['Mingxian Lin', 'Wei Huang', 'Yitang Li', 'Chengjie Jiang', 'Kui Wu', 'Fangwei Zhong', 'Shengju Qian', 'Xin Wang', 'Xiaojuan Qi'], 'affiliations': ['Beijing Normal University', 'LIGHTSPEED', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10548.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#multimodal', '#benchmark', '#rl', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'EmRACE-3K: Новый рубеж в обучении воплощенного ИИ', 'desc': 'Статья представляет новый набор данных EmRACE-3K для оценки моделей компьютерного зрения и обработки естественного языка в воплощенных средах. Исследование выявляет ограничения современных моделей в пространственных рассуждениях и долгосрочном планировании. Авторы демонстрируют улучшения производительности моделей через дообучение с учителем и обучение с подкреплением. EmRACE-3K содержит более 3000 задач, управляемых естественным языком, в разнообразных фотореалистичных средах.'}, 'en': {'title': 'Enhancing Embodied Reasoning with EmRACE-3K Dataset', 'desc': 'The paper introduces EmRACE-3K, a new dataset designed to evaluate vision-language models (VLMs) in embodied settings, where agents interact with environments in real-time. It highlights the limitations of current state-of-the-art models in spatial reasoning and long-horizon planning, particularly in open-environment interactions. The dataset includes over 3,000 tasks that require navigation, object manipulation, and multi-stage goal execution, providing a benchmark for assessing embodied reasoning capabilities. Fine-tuning a model using supervised and reinforcement learning on this dataset shows significant improvements, demonstrating its potential to enhance VLM performance in interactive scenarios.'}, 'zh': {'title': 'EmRACE-3K：提升视觉-语言模型的具身推理能力', 'desc': '本论文介绍了一个新的数据集EmRACE-3K，用于评估视觉-语言模型在具身环境中的表现。研究发现，当前的模型在空间推理和长远规划方面存在明显的局限性，尤其是在需要实时互动的场景中。通过使用EmRACE-3K数据集，研究者们建立了一个基准，评估模型在探索、动态空间语义推理和多阶段目标执行等方面的能力。最后，通过监督学习和强化学习的微调，显著提升了模型在这些挑战中的表现，证明了该数据集在发展具身推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.10541', 'title': 'REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once', 'url': 'https://huggingface.co/papers/2507.10541', 'abstract': 'REST evaluates large reasoning models under simultaneous multi-context pressure, revealing performance differences not apparent in single-question tests and highlighting the importance of contextual priority allocation and cognitive load management.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the "overthinking trap" is a critical factor contributing to the performance degradation; (2) the models trained with "long2short" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation.', 'score': 21, 'issue_id': 4819, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '6e4271f0c17f7ec8', 'authors': ['Zhuoshi Pan', 'Qizhi Pei', 'Yu Li', 'Qiyao Sun', 'Zinan Tang', 'H. Vicky Zhao', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab', 'Renmin University of China', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10541.jpg', 'data': {'categories': ['#benchmark', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'REST: стресс-тест для искусственного интеллекта', 'desc': 'REST - это новый метод оценки больших языковых моделей, который тестирует их способность решать несколько задач одновременно. Он выявляет различия в производительности моделей, которые не видны при стандартном тестировании с одиночными вопросами. REST оценивает такие важные способности моделей, как распределение приоритетов между задачами и управление когнитивной нагрузкой. Результаты показывают, что даже современные модели значительно хуже справляются с задачами в условиях стресс-тестирования по сравнению с обычными тестами.'}, 'en': {'title': 'REST: Stress Testing for Real-World Reasoning', 'desc': 'The paper introduces REST, a new framework for evaluating Large Reasoning Models (LRMs) under simultaneous multi-context conditions. Unlike traditional methods that test models on single questions, REST exposes them to multiple problems at once, revealing their performance under stress. This approach highlights important capabilities such as contextual priority allocation and cognitive load management, which are crucial for real-world applications. The findings show that even top models struggle under this pressure, indicating that current evaluation methods may not accurately reflect their true reasoning abilities.'}, 'zh': {'title': 'REST：多上下文压力下的推理评估新方法', 'desc': '这篇论文介绍了一种新的评估框架REST，用于同时测试大型推理模型（LRMs）。传统的评估方法主要集中在单一问题的顺序测试，导致模型在多上下文压力下的表现未被充分评估。REST通过同时暴露多个问题，揭示了模型在上下文优先分配和认知负荷管理方面的能力差异。研究结果表明，即使是最先进的模型在压力测试下也会显著下降，强调了这种新评估方法在真实世界应用中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.04404', 'title': 'LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers', 'url': 'https://huggingface.co/papers/2507.04404', 'abstract': 'A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.', 'score': 17, 'issue_id': 4815, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '67be09e9b6db6fc6', 'authors': ['Jingze Zhu', 'Yongliang Wu', 'Wenbo Zhu', 'Jiawang Cao', 'Yanqiang Zheng', 'Jiawei Chen', 'Xu Yang', 'Bernt Schiele', 'Jonas Fischer', 'Xinting Hu'], 'affiliations': ['Growth, Xiaomi Corporation', 'Max Planck Institute for Informatics', 'Opus AI Research', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2507.04404.jpg', 'data': {'categories': ['#training', '#hallucinations', '#benchmark', '#architecture', '#interpretability'], 'emoji': '🎯', 'ru': {'title': 'Повышение фактической точности языковых моделей через токен-ориентированное контрастное декодирование', 'desc': 'Статья представляет новый метод контрастного декодирования для улучшения фактической точности больших языковых моделей (LLM). Метод основан на избирательном подавлении внимания к определенным типам токенов на соответствующих глубинах трансформера. Авторы обнаружили, что знаки препинания получают доминирующее внимание на ранних слоях, а концептуальные токены управляют семантическими рассуждениями на промежуточных слоях. Эксперименты показывают, что предложенный метод последовательно улучшает фактическую точность различных LLM на нескольких эталонных наборах данных.'}, 'en': {'title': 'Enhancing Factual Accuracy in LLMs with Layer-Localized Attention', 'desc': 'This paper presents a new method called token-aware, layer-localized contrastive decoding to enhance the factual accuracy of large language models (LLMs). The approach focuses on managing attention to different types of tokens at specific layers of the model, which helps in reducing factual errors during text generation. By analyzing how punctuation and conceptual tokens are processed in different layers, the method effectively suppresses attention to these tokens when necessary. The results show that this technique improves the factuality of generated text without requiring any additional training or changes to the model architecture.'}, 'zh': {'title': '提升大型语言模型事实性的对比解码方法', 'desc': '本文提出了一种基于令牌感知和层局部对比解码的方法，以提高大型语言模型的事实准确性。该方法通过在不同层次上选择性地抑制特定类型令牌的注意力，来改善生成的事实内容。研究发现，标点符号在早期层中占主导地位，而概念令牌则在中间层中主导语义推理。通过这种方式，我们的方法在不需要额外训练或模型修改的情况下，显著提高了多个大型语言模型的事实性表现。'}}}, {'id': 'https://huggingface.co/papers/2507.09104', 'title': 'CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards', 'url': 'https://huggingface.co/papers/2507.09104', 'abstract': 'CompassJudger-2, a generalist judge model, achieves superior performance across multiple benchmarks through task-driven data curation, verifiable rewards, and a refined learning objective with margin policy gradient loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.', 'score': 15, 'issue_id': 4816, 'pub_date': '2025-07-12', 'pub_date_card': {'ru': '12 июля', 'en': 'July 12', 'zh': '7月12日'}, 'hash': '40a4c36902a53203', 'authors': ['Taolin Zhang', 'Maosong Cao', 'Alexander Lam', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.09104.jpg', 'data': {'categories': ['#data', '#architecture', '#training', '#benchmark', '#dataset', '#agi', '#reasoning', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'CompassJudger-2: универсальный судья для ИИ-моделей', 'desc': 'CompassJudger-2 - это новая модель-судья общего назначения для оценки больших языковых моделей. Она использует многодоменную стратегию курации данных и обучение с подкреплением для развития надежных способностей к суждению. Модель применяет усовершенствованную целевую функцию с градиентным спуском по политике с маржой. CompassJudger-2 демонстрирует превосходные результаты на различных тестовых наборах для оценки моделей-судей.'}, 'en': {'title': 'Revolutionizing Model Evaluation with CompassJudger-2', 'desc': 'CompassJudger-2 is a versatile judge model designed to evaluate large language models more effectively. It improves upon previous models by using a task-driven approach to curate diverse data, ensuring robust evaluations across different domains. The model employs verifiable rewards and a refined learning objective with margin policy gradient loss to enhance its judgment capabilities. With its superior performance on various benchmarks, CompassJudger-2 sets new standards for accuracy and consistency in model evaluation.'}, 'zh': {'title': 'CompassJudger-2：通用评判模型的突破', 'desc': 'CompassJudger-2是一种通用评判模型，通过任务驱动的数据整理、可验证的奖励和改进的学习目标（边际策略梯度损失）在多个基准测试中表现优异。该模型克服了现有评判模型的专业化狭窄和鲁棒性不足的问题，能够进行全面的评估。我们的方法通过可验证的奖励来监督判断任务，并通过拒绝采样促进内在的批判性推理，从而增强判断能力。CompassJudger-2在多个评判和奖励基准测试中取得了优异的结果，展示了与更大模型的竞争性判断准确性。'}}}, {'id': 'https://huggingface.co/papers/2507.10065', 'title': 'MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second', 'url': 'https://huggingface.co/papers/2507.10065', 'abstract': 'MoVieS synthesizes 4D dynamic novel views from monocular videos using Gaussian primitives, enabling unified modeling of appearance, geometry, and motion with minimal task-specific supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.', 'score': 11, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '0083d2c68182097f', 'authors': ['Chenguo Lin', 'Yuchen Lin', 'Panwang Pan', 'Yifan Yu', 'Honglei Yan', 'Katerina Fragkiadaki', 'Yadong Mu'], 'affiliations': ['ByteDance', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10065.jpg', 'data': {'categories': ['#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Единая модель для синтеза видов, реконструкции и отслеживания в 3D', 'desc': 'MoVieS - это новая модель, которая синтезирует динамические виды из монокулярных видео за одну секунду. Она использует сетки гауссовых примитивов для представления динамичных 3D-сцен, явно моделируя их движение во времени. Этот подход позволяет объединить моделирование внешнего вида, геометрии и движения в единой обучаемой системе. MoVieS поддерживает широкий спектр задач без дополнительного обучения, включая оценку потока сцены и сегментацию движущихся объектов.'}, 'en': {'title': 'Revolutionizing 4D View Synthesis with MoVieS', 'desc': 'MoVieS is a machine learning model that creates 4D views from single videos quickly and efficiently. It uses Gaussian primitives to represent dynamic 3D scenes, allowing it to capture changes in motion over time. This model combines the understanding of appearance, geometry, and motion into one framework, which simplifies the process of view synthesis and 3D reconstruction. MoVieS can be trained on large datasets with little specific guidance, making it versatile for various applications like scene flow estimation and moving object segmentation.'}, 'zh': {'title': 'MoVieS：统一建模动态场景的创新方法', 'desc': 'MoVieS是一种新型的前馈模型，可以从单目视频中合成4D动态新视图。它使用像素对齐的高斯原语来表示动态3D场景，并明确监督其随时间变化的运动。这种方法首次实现了外观、几何和运动的统一建模，支持在单一学习框架内进行视图合成、重建和3D点跟踪。MoVieS能够在多样化的数据集上进行大规模训练，且对特定任务的监督依赖最小，支持多种零样本应用。'}}}, {'id': 'https://huggingface.co/papers/2507.08924', 'title': 'From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation', 'url': 'https://huggingface.co/papers/2507.08924', 'abstract': 'Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, are introduced to evaluate Large Language Models across academic and industrial domains in Korea.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.', 'score': 5, 'issue_id': 4819, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'f75ad1f8a8d2cda6', 'authors': ['Seokhee Hong', 'Sunkyoung Kim', 'Guijin Son', 'Soyeon Kim', 'Yeonjung Hong', 'Jinsik Lee'], 'affiliations': ['LG AI Research', 'OnelineAI'], 'pdf_title_img': 'assets/pdf/title_img/2507.08924.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#science', '#open_source'], 'emoji': '🇰🇷', 'ru': {'title': 'Новые корейские бенчмарки для оценки LLM в реальных сценариях', 'desc': 'В статье представлены два новых эталонных теста для оценки больших языковых моделей (LLM) на корейском языке: KMMLU-Redux и KMMLU-Pro. KMMLU-Redux основан на вопросах из корейских национальных технических квалификационных экзаменов, а KMMLU-Pro - на экзаменах для получения профессиональных лицензий. Эти тесты охватывают как академические, так и промышленные области знаний в Корее. Эксперименты показали, что данные бенчмарки комплексно отражают профессиональные знания в корейской индустрии.'}, 'en': {'title': 'Evaluating LLMs with Korean Expert Benchmarks', 'desc': 'This paper presents two new benchmarks, KMMLU-Redux and KMMLU-Pro, designed to assess Large Language Models (LLMs) in both academic and industrial contexts in Korea. KMMLU-Redux is an improved version of the original KMMLU, featuring questions from the Korean National Technical Qualification exams with errors corrected for better accuracy. KMMLU-Pro focuses on the Korean National Professional Licensure exams, ensuring that it captures essential professional knowledge. The authors provide evidence that these benchmarks effectively cover the necessary industrial knowledge in Korea and make the dataset publicly available for further research.'}, 'zh': {'title': '评估大型语言模型的新基准', 'desc': '本文介绍了两个用于评估大型语言模型（LLMs）的韩国专家级基准：KMMLU-Redux和KMMLU-Pro。这些基准不仅涵盖学术领域，还包括工业领域，以有效评估LLMs在实际场景中的适用性。KMMLU-Redux是从现有的KMMLU重建而来，包含韩国国家技术资格考试的问题，并去除了关键错误以提高可靠性。KMMLU-Pro则基于韩国国家职业执照考试，反映了韩国的专业知识。'}}}, {'id': 'https://huggingface.co/papers/2507.08267', 'title': 'A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.08267', 'abstract': "A combination of extended supervised fine-tuning and reinforcement learning from online inference enhances the mathematical reasoning capabilities of large language models, achieving top-tier performance on benchmarks like the AI Mathematical Olympiad.  \t\t\t\t\tAI-generated summary \t\t\t\t Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.", 'score': 5, 'issue_id': 4820, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '2aec98226e1c5d62', 'authors': ['Hiroshi Yoshihara', 'Taiki Yamaguchi', 'Yuichi Inoue'], 'affiliations': ['Aillis Inc., Tokyo, Japan', 'Department of Health Policy and Public Health, Graduate School of Pharmaceutical Sciences, The University of Tokyo, Tokyo, Japan', 'Rist Inc., Kyoto, Japan', 'Sakana AI, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2507.08267.jpg', 'data': {'categories': ['#open_source', '#math', '#optimization', '#reasoning', '#training', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Симбиоз SFT и RL: путь к совершенству в математическом мышлении ИИ', 'desc': 'Статья представляет эффективный метод обучения больших языковых моделей (LLM) для улучшения их математических способностей. Авторы комбинируют расширенное обучение с учителем (SFT) и обучение с подкреплением (RL) от онлайн-вывода. Этот подход позволяет достичь высокой точности и эффективности модели в решении математических задач. Результаты подтверждены высокими показателями на сложных бенчмарках, включая AI Mathematical Olympiad.'}, 'en': {'title': 'Boosting Mathematical Reasoning in LLMs with Smart Training Techniques', 'desc': "This paper presents a novel approach to enhance the mathematical reasoning abilities of large language models (LLMs) by combining extended supervised fine-tuning (SFT) with reinforcement learning from online inference (GRPO). The authors argue that these two training methods complement each other, with SFT initially maximizing the model's accuracy, followed by GRPO which improves efficiency in generating solutions. Their experiments demonstrate that extending the SFT phase significantly contributes to performance improvements, while GRPO focuses on optimizing the length of the generated solutions. The proposed methodology achieves outstanding results on benchmarks, including a high ranking in the AI Mathematical Olympiad, and the authors plan to share their framework to support further research in this area."}, 'zh': {'title': '提升大型语言模型的数学推理能力', 'desc': '本文探讨了如何通过扩展的监督微调和在线推理的强化学习相结合，提升大型语言模型的数学推理能力。研究表明，延长监督微调阶段可以显著提高模型的准确性，而随后引入的在线推理强化学习则能优化解决方案的长度和效率。实验结果显示，延长监督微调至多10个周期对于性能突破至关重要。该方法在AI数学奥林匹克等基准测试中表现优异，为开发高效且准确的数学推理模型提供了实用的蓝图。'}}}, {'id': 'https://huggingface.co/papers/2507.04218', 'title': 'DreamPoster: A Unified Framework for Image-Conditioned Generative Poster\n  Design', 'url': 'https://huggingface.co/papers/2507.04218', 'abstract': "DreamPoster generates high-quality posters from images and text prompts using a progressive training strategy and Seedream3.0 model, outperforming existing methods in usability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\\%, compared to GPT-4o (47.56\\%) and SeedEdit3.0 (25.96\\%). DreamPoster will be online in Jimeng and other Bytedance Apps.", 'score': 5, 'issue_id': 4824, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '1445577894611bf7', 'authors': ['Xiwei Hu', 'Haokun Chen', 'Zhongqi Qi', 'Hui Zhang', 'Dexiang Hong', 'Jie Shao', 'Xinglong Wu'], 'affiliations': ['Fudan University', 'Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.04218.jpg', 'data': {'categories': ['#training', '#synthetic', '#benchmark', '#dataset', '#cv', '#data'], 'emoji': '🎨', 'ru': {'title': 'DreamPoster: Интеллектуальное создание постеров с помощью ИИ', 'desc': 'DreamPoster - это система генерации изображений на основе текста, которая создает высококачественные постеры из пользовательских изображений и текстовых запросов. Она использует модель Seedream3.0 и прогрессивную стратегию обучения для обработки различных типов генерации постеров. DreamPoster применяет систематический конвейер аннотации данных для точной разметки текстового содержания и типографической иерархии в изображениях постеров. Согласно оценкам на тестовых наборах данных, DreamPoster превосходит существующие методы, достигая высокого уровня применимости в 88.55%.'}, 'en': {'title': 'Transforming Ideas into Stunning Posters with DreamPoster!', 'desc': 'DreamPoster is a novel framework for generating high-quality posters from images and text prompts, leveraging the advanced Seedream3.0 model. It employs a systematic data annotation pipeline to accurately capture textual content and typographic hierarchy, ensuring fidelity in the generated outputs. The model utilizes a progressive training strategy that enhances its ability to perform multiple tasks while maintaining high-quality results. Evaluations show that DreamPoster significantly outperforms existing methods in usability, achieving an impressive usability rate of 88.55%.'}, 'zh': {'title': 'DreamPoster：智能生成高质量海报的创新框架', 'desc': 'DreamPoster 是一个文本到图像生成框架，可以从用户提供的图像和文本提示中智能合成高质量的海报。它基于 Seedream3.0 模型，能够统一处理不同类型的海报生成。为了构建数据集，我们提出了一种系统的数据注释流程，精确标注海报图像中的文本内容和排版层次信息。通过逐步训练策略，DreamPoster 能够分层获取多任务生成能力，同时保持高质量的生成效果。'}}}, {'id': 'https://huggingface.co/papers/2507.09074', 'title': 'Favicon Trojans: Executable Steganography Via Ico Alpha Channel\n  Exploitation', 'url': 'https://huggingface.co/papers/2507.09074', 'abstract': 'This paper presents a novel method of executable steganography using the alpha transparency layer of ICO image files to embed and deliver self-decompressing JavaScript payloads within web browsers. By targeting the least significant bit (LSB) of non-transparent alpha layer image values, the proposed method successfully conceals compressed JavaScript code inside a favicon image without affecting visual fidelity. Global web traffic loads 294 billion favicons daily and consume 0.9 petabytes of network bandwidth. A proof-of-concept implementation demonstrates that a 64x64 ICO image can embed up to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold compression. On page load, a browser fetches the favicon as part of standard behavior, allowing an embedded loader script to extract and execute the payload entirely in memory using native JavaScript APIs and canvas pixel access. This creates a two-stage covert channel requiring no additional network or user requests. Testing across multiple browsers in both desktop and mobile environments confirms successful and silent execution of the embedded script. We evaluate the threat model, relate it to polymorphic phishing attacks that evade favicon-based detection, and analyze evasion of content security policies and antivirus scanners. We map nine example MITRE ATT&CK Framework objectives to single line JavaScript to execute arbitrarily in ICO files. Existing steganalysis and sanitization defenses are discussed, highlighting limitations in detecting or neutralizing alpha-channel exploits. The results demonstrate a stealthy and reusable attack surface that blurs traditional boundaries between static images and executable content. Because modern browsers report silent errors when developers specifically fail to load ICO files, this attack surface offers an interesting example of required web behaviors that in turn compromise security.', 'score': 2, 'issue_id': 4823, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '4a098df2d28ece2f', 'authors': ['David Noever', 'Forrest McKee'], 'affiliations': ['PeopleTec, Inc., Huntsville, Alabama, USA'], 'pdf_title_img': 'assets/pdf/title_img/2507.09074.jpg', 'data': {'categories': ['#multimodal', '#audio', '#dataset', '#data', '#video', '#healthcare', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Невидимый JavaScript: Стеганография в фавиконках', 'desc': 'Статья представляет новый метод исполняемой стеганографии, использующий альфа-канал прозрачности ICO-файлов для встраивания JavaScript-кода в веб-браузерах. Метод скрывает сжатый JavaScript в фавиконке, не влияя на её визуальное качество. Доказательство концепции показывает, что 64x64 ICO-изображение может вместить до 512 байт несжатого или 0.8 килобайт сжатого кода. Этот подход создает двухэтапный скрытый канал, не требующий дополнительных сетевых запросов, и успешно обходит системы безопасности и антивирусные сканеры.'}, 'en': {'title': 'Stealthy JavaScript Payloads Hidden in Favicons', 'desc': 'This paper introduces a new technique for executable steganography that uses the alpha transparency layer of ICO image files to hide JavaScript payloads. By manipulating the least significant bit of the alpha values, the method embeds compressed JavaScript code within favicon images without altering their appearance. The approach takes advantage of the fact that browsers automatically load favicons, allowing the hidden script to be executed in memory without additional network requests. The study also evaluates the implications for security, showing how this method can evade detection by existing defenses and contribute to sophisticated phishing attacks.'}, 'zh': {'title': '利用ICO图像实现隐蔽的JavaScript攻击', 'desc': '本文提出了一种新颖的可执行隐写术方法，利用ICO图像文件的透明度层嵌入和传递自解压的JavaScript负载。该方法通过针对非透明alpha层图像值的最低有效位（LSB），成功地在favicon图像中隐藏压缩的JavaScript代码，而不影响视觉效果。研究表明，浏览器在加载页面时会自动获取favicon，从而使嵌入的加载脚本能够在内存中提取和执行负载，形成一个无需额外网络请求的隐蔽通道。我们还评估了该方法的威胁模型，并分析了其对现有安全防护措施的规避能力。'}}}, {'id': 'https://huggingface.co/papers/2507.09751', 'title': 'Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded\n  Interpretations', 'url': 'https://huggingface.co/papers/2507.09751', 'abstract': "A method integrates large language models into formal semantics for paraconsistent logic, preserving logical soundness and completeness while leveraging LLM knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.", 'score': 0, 'issue_id': 4833, 'pub_date': '2025-07-13', 'pub_date_card': {'ru': '13 июля', 'en': 'July 13', 'zh': '7月13日'}, 'hash': '401b05470cb2bb76', 'authors': ['Bradley P. Allen', 'Prateek Chhikara', 'Thomas Macaulay Ferguson', 'Filip Ilievski', 'Paul Groth'], 'affiliations': ['Rensselaer Polytechnic Institute, Troy, NY, US', 'University of Amsterdam, Amsterdam, NL', 'University of Southern California, CA, US', 'Vrije Universiteit Amsterdam, Amsterdam, NL'], 'pdf_title_img': 'assets/pdf/title_img/2507.09751.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#data', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Логическая непротиворечивость встречает мощь языковых моделей', 'desc': 'Статья представляет метод интеграции больших языковых моделей (LLM) в формальную семантику для паранепротиворечивой логики. Этот подход позволяет сохранить логическую непротиворечивость и полноту, одновременно используя знания LLM. Авторы предлагают теоретическую основу для нейро-символических рассуждений, которая использует знания LLM, сохраняя при этом свойства непротиворечивости и полноты базовой логики. Экспериментальные результаты подтверждают эффективность метода на основе оценки с использованием наборов данных, созданных из нескольких эталонных тестов на фактическую точность.'}, 'en': {'title': 'Harnessing LLMs for Consistent Paraconsistent Logic', 'desc': "This paper presents a novel method that combines large language models (LLMs) with formal semantics in the context of paraconsistent logic. The approach aims to utilize the extensive knowledge embedded in LLMs while ensuring that the logical principles of soundness and completeness are maintained. By integrating LLMs into the interpretation function of paraconsistent logic, the authors demonstrate how to effectively manage the inherent inconsistencies of LLM outputs. Experimental results show the method's viability through evaluations on datasets derived from factuality benchmarks, establishing a new framework for neuro-symbolic reasoning."}, 'zh': {'title': '将大型语言模型与不一致逻辑相结合的创新方法', 'desc': '本文提出了一种将大型语言模型（LLM）整合到形式语义学中的方法，专注于处理不一致逻辑。该方法在保持逻辑的健全性和完备性的同时，利用LLM的广泛知识进行形式推理。通过对多个短文本事实基准数据集的实验评估，验证了该方法的可行性。与以往的研究不同，我们的方法为神经符号推理提供了理论框架，能够有效利用LLM的知识。'}}}, {'id': 'https://huggingface.co/papers/2507.01955', 'title': 'How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks', 'url': 'https://huggingface.co/papers/2507.01955', 'abstract': 'Multimodal foundation models, despite being primarily trained on image-text tasks, demonstrate respectable performance across various vision tasks when adapted through prompt chaining, though they fall short compared to specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.', 'score': 9, 'issue_id': 4676, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '650bb4b7601b21e0', 'authors': ['Rahul Ramachandran', 'Ali Garjani', 'Roman Bachmann', 'Andrei Atanov', 'Oğuzhan Fatih Kar', 'Amir Zamir'], 'affiliations': ['Swiss Federal Institute of Technology Lausanne (EPFL)'], 'pdf_title_img': 'assets/pdf/title_img/2507.01955.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#multimodal', '#games', '#cv', '#reasoning', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Универсальность мультимодальных моделей в компьютерном зрении: потенциал и ограничения', 'desc': 'Статья исследует эффективность мультимодальных фундаментальных моделей в различных задачах компьютерного зрения. Авторы разработали стандартизированную систему оценки, используя цепочки промптов для адаптации моделей к задачам, которые обычно требуют специализированных выходных данных. Результаты показывают, что хотя эти модели уступают специализированным решениям, они демонстрируют достойную производительность в качестве универсальных инструментов. Исследование также выявило, что модели лучше справляются с семантическими задачами, чем с геометрическими, а GPT-4o показала наилучшие результаты среди нерассуждающих моделей.'}, 'en': {'title': 'Benchmarking Multimodal Models: Generalists in Vision Tasks', 'desc': 'This paper evaluates the performance of multimodal foundation models on various computer vision tasks, such as semantic segmentation and object detection. Despite their training primarily on image-text tasks, these models show respectable generalist capabilities but do not match the performance of specialized models. The authors introduce a benchmarking framework that translates vision tasks into text-promptable formats to address challenges related to model accessibility and output limitations. Findings indicate that while these models excel in semantic tasks, they struggle with geometric tasks, and the best-performing model, GPT-4o, leads in several categories despite some quirks in image generation.'}, 'zh': {'title': '多模态模型的视觉任务表现评估', 'desc': '多模态基础模型主要在图像-文本任务上训练，但在适应性提示链的帮助下，在各种视觉任务上表现出色。本文对多种流行的多模态基础模型在标准计算机视觉任务上的表现进行了基准测试，发现这些模型在语义任务上表现优于几何任务。尽管它们在任何任务上都未能接近专业模型的水平，但作为通用模型，它们的表现仍然令人瞩目。我们提出了一种标准化的基准框架，通过将标准视觉任务转换为可通过提示链和API兼容的任务来解决模型适应性的问题。'}}}, {'id': 'https://huggingface.co/papers/2507.02608', 'title': 'Lost in Latent Space: An Empirical Study of Latent Diffusion Models for\n  Physics Emulation', 'url': 'https://huggingface.co/papers/2507.02608', 'abstract': 'The use of latent space diffusion models for faster and accurate emulation of dynamical systems is viable, offering robustness to high compression rates and improved prediction diversity compared to non-generative approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t The steep computational cost of diffusion models at inference hinders their use as fast physics emulators. In the context of image and video generation, this computational drawback has been addressed by generating in the latent space of an autoencoder instead of the pixel space. In this work, we investigate whether a similar strategy can be effectively applied to the emulation of dynamical systems and at what cost. We find that the accuracy of latent-space emulation is surprisingly robust to a wide range of compression rates (up to 1000x). We also show that diffusion-based emulators are consistently more accurate than non-generative counterparts and compensate for uncertainty in their predictions with greater diversity. Finally, we cover practical design choices, spanning from architectures to optimizers, that we found critical to train latent-space emulators.', 'score': 5, 'issue_id': 4683, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '4fe1b404e88f405f', 'authors': ['François Rozet', 'Ruben Ohana', 'Michael McCabe', 'Gilles Louppe', 'François Lanusse', 'Shirley Ho'], 'affiliations': ['Flatiron Institute', 'New York University', 'Polymathic AI', 'Princeton University', 'University of Liège', 'Université Paris-Saclay, Université Paris Cité, CEA, CNRS, AIM'], 'pdf_title_img': 'assets/pdf/title_img/2507.02608.jpg', 'data': {'categories': ['#diffusion', '#inference', '#data', '#architecture', '#training', '#optimization'], 'emoji': '🌀', 'ru': {'title': 'Латентная диффузия для быстрой и точной эмуляции физических систем', 'desc': 'Статья исследует применение моделей диффузии в латентном пространстве для эмуляции динамических систем. Авторы обнаружили, что точность эмуляции в латентном пространстве устойчива к высоким степеням сжатия данных. Эмуляторы на основе диффузии оказались более точными, чем негенеративные подходы, и обеспечивают большее разнообразие предсказаний. В работе также рассматриваются практические аспекты проектирования таких моделей, включая архитектуры и оптимизаторы.'}, 'en': {'title': 'Efficient and Accurate Emulation of Dynamical Systems with Latent Space Diffusion Models', 'desc': 'This paper explores the use of latent space diffusion models to emulate dynamical systems more efficiently and accurately. By operating in the latent space of an autoencoder, the models can significantly reduce computational costs while maintaining high accuracy, even with compression rates up to 1000 times. The study demonstrates that these diffusion-based emulators outperform traditional non-generative methods, providing better prediction diversity and handling uncertainty more effectively. Additionally, the authors discuss important design considerations for training these latent-space emulators, including architecture and optimization strategies.'}, 'zh': {'title': '潜在空间扩散模型：动态系统模拟的新突破', 'desc': '本论文探讨了在潜在空间中使用扩散模型来快速且准确地模拟动态系统的可行性。研究表明，潜在空间的模拟在高压缩率下（最高可达1000倍）仍然保持良好的准确性。与非生成方法相比，基于扩散的模拟器在预测准确性上表现更佳，并且能够通过更大的多样性来补偿预测的不确定性。最后，论文还讨论了在训练潜在空间模拟器时，架构和优化器等设计选择的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.01853', 'title': 'Eka-Eval : A Comprehensive Evaluation Framework for Large Language\n  Models in Indian Languages', 'url': 'https://huggingface.co/papers/2507.01853', 'abstract': 'EKA-EVAL is a comprehensive multilingual evaluation framework for large language models, supporting diverse benchmarks and features for efficient distributed inference and GPU usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.', 'score': 4, 'issue_id': 4672, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': 'a397a0d71f721623', 'authors': ['Samridhi Raj Sinha', 'Rajvee Sheth', 'Abhishek Upperwal', 'Mayank Singh'], 'affiliations': ['Indian Institute of Technology Gandhinagar', 'LINGO Research Group', 'NMIMS', 'Soket AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01853.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#open_source', '#benchmark'], 'emoji': '🌏', 'ru': {'title': 'EKA-EVAL: Универсальная платформа для оценки многоязычных языковых моделей', 'desc': 'EKA-EVAL - это комплексная многоязычная система оценки больших языковых моделей (LLM), поддерживающая разнообразные бенчмарки. Она включает более 35 тестов, в том числе 10 наборов данных для индийских языков, охватывающих такие категории, как рассуждение, математика, использование инструментов и понимание длинного контекста. EKA-EVAL предлагает встроенную поддержку распределенного вывода, квантизации и использования нескольких GPU. Эта система позиционируется как первый комплексный инструмент оценки, адаптированный как для глобальных, так и для индийских LLM.'}, 'en': {'title': 'Empowering Multilingual Evaluation for Large Language Models', 'desc': 'EKA-EVAL is a multilingual evaluation framework designed for large language models (LLMs), focusing on diverse linguistic needs, particularly in regions like India. It includes over 35 benchmarks, with specific datasets for Indic languages, covering various tasks such as reasoning and reading comprehension. The framework supports efficient distributed inference and multi-GPU usage, making it suitable for production environments. EKA-EVAL aims to lower the barriers for multilingual benchmarking and is part of a larger initiative to expand its benchmark offerings.'}, 'zh': {'title': 'EKA-EVAL：多语言模型评估的新标准', 'desc': 'EKA-EVAL是一个全面的多语言评估框架，专为大型语言模型设计。它支持多种基准测试和功能，能够高效地进行分布式推理和GPU使用。该框架整合了超过35个基准，包括10个特定于印度的数据库，涵盖推理、数学、工具使用、长文本理解和阅读理解等类别。EKA-EVAL是首个为全球和印度语言模型量身定制的端到端可扩展评估套件，显著降低了多语言基准测试的门槛。'}}}, {'id': 'https://huggingface.co/papers/2507.00769', 'title': 'LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative\n  Writing', 'url': 'https://huggingface.co/papers/2507.00769', 'abstract': 'LitBench introduces a standardized benchmark for evaluating creative writing generated by language models, using human-labeled story comparisons and training reward models to assess and validate automated evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.', 'score': 1, 'issue_id': 4683, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '38c5e5a52a8decf8', 'authors': ['Daniel Fein', 'Sebastian Russo', 'Violet Xiang', 'Kabir Jolly', 'Rafael Rafailov', 'Nick Haber'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.00769.jpg', 'data': {'categories': ['#story_generation', '#dataset', '#alignment', '#benchmark', '#optimization'], 'emoji': '📝', 'ru': {'title': 'LitBench: новый стандарт оценки генеративного творческого письма', 'desc': 'LitBench представляет собой стандартизированный бенчмарк для оценки творческого письма, сгенерированного языковыми моделями. Он использует размеченные людьми сравнения историй и обучает модели вознаграждения для оценки и валидации автоматизированных методов оценки. Бенчмарк включает тестовый набор из 2480 сравнений историй и обучающий корпус из 43827 пар с метками человеческих предпочтений. LitBench позволяет сравнивать различные методы оценки, включая готовые языковые модели и специально обученные модели вознаграждения.'}, 'en': {'title': 'LitBench: A New Standard for Evaluating Creative Writing in AI', 'desc': 'LitBench is a new benchmark designed to evaluate creative writing produced by language models. It addresses the challenge of assessing open-ended narratives, which often lack clear correct answers. The benchmark includes a dataset of human-labeled story comparisons and uses reward models to improve automated evaluation methods. Results show that trained reward models outperform off-the-shelf language models in aligning with human preferences for creative writing.'}, 'zh': {'title': 'LitBench：创意写作评估的新标准', 'desc': 'LitBench是一个标准化的基准，用于评估语言模型生成的创意写作。由于开放式叙事缺乏明确的真相，评估创意写作变得具有挑战性。该基准包含2,480个去偏见的人类标注故事比较和43,827对人类偏好标签的训练语料库。通过LitBench，我们能够评估零-shot语言模型的表现，并训练奖励模型，以提高创意写作的自动评估方法的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2507.07104', 'title': 'Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models', 'url': 'https://huggingface.co/papers/2507.07104', 'abstract': 'The VLV auto-encoder framework uses pretrained vision and text models to create a cost-effective and data-efficient captioning system.  \t\t\t\t\tAI-generated summary \t\t\t\t Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.', 'score': 32, 'issue_id': 4841, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '36745eebc71db54e', 'authors': ['Tiezheng Zhang', 'Yitong Li', 'Yu-cheng Chou', 'Jieneng Chen', 'Alan Yuille', 'Chen Wei', 'Junfei Xiao'], 'affiliations': ['Johns Hopkins University', 'Rice University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07104.jpg', 'data': {'categories': ['#data', '#optimization', '#architecture', '#multimodal', '#transfer_learning', '#dataset', '#training'], 'emoji': '🔬', 'ru': {'title': 'Эффективное создание подписей к изображениям без огромных датасетов', 'desc': 'Статья представляет новый фреймворк VLV auto-encoder для создания эффективной системы генерации подписей к изображениям. Он использует предобученные модели компьютерного зрения и обработки текста, что значительно снижает затраты на обучение и требования к данным. Ключевым элементом является создание информационного узкого места путем регуляризации пространства языковых представлений. Результаты показывают, что модель достигает качества ведущих систем вроде GPT-4 и Gemini 2.0 при затратах менее $1000 на обучение.'}, 'en': {'title': 'Efficient Captioning with VLV: Less Data, More Insight!', 'desc': 'The VLV auto-encoder framework innovatively combines pretrained vision and text models to enhance image captioning efficiency. By utilizing a vision encoder and a Text-to-Image diffusion model, it reduces the need for extensive image-text pair datasets, which are typically costly and time-consuming to compile. The framework introduces an information bottleneck that regularizes language representations, allowing for effective knowledge distillation from the diffusion model. Ultimately, this approach enables the creation of a state-of-the-art captioning system that is both cost-effective and data-efficient, achieving high-quality outputs with minimal resources.'}, 'zh': {'title': '高效图像描述生成的新方法', 'desc': 'VLV自编码器框架利用预训练的视觉和文本模型，创建了一种成本效益高且数据高效的图像描述系统。该框架通过冻结预训练的文本到图像扩散模型的解码器，建立了信息瓶颈，从而规范化语言表示空间。VLV管道有效地从文本条件的扩散模型中提取知识，展示了高质量重建的全面语义理解。通过微调预训练的大型语言模型，将中间语言表示解码为详细描述，我们构建了一个与领先模型相媲美的最先进图像描述生成器。'}}}, {'id': 'https://huggingface.co/papers/2507.11407', 'title': 'EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and\n  Reasoning Modes', 'url': 'https://huggingface.co/papers/2507.11407', 'abstract': 'EXAONE 4.0 integrates non-reasoning and reasoning modes, supports multilingualism, and offers models optimized for high performance and on-device use, demonstrating superior performance compared to open-weight models.  \t\t\t\t\tAI-generated summary \t\t\t\t This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.', 'score': 20, 'issue_id': 4844, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '9cfc5fdcccb88d4a', 'authors': ['LG AI Research', ':', 'Kyunghoon Bae', 'Eunbi Choi', 'Kibong Choi', 'Stanley Jungkyu Choi', 'Yemuk Choi', 'Kyubeen Han', 'Seokhee Hong', 'Junwon Hwang', 'Taewan Hwang', 'Joonwon Jang', 'Hyojin Jeon', 'Kijeong Jeon', 'Gerrard Jeongwon Jo', 'Hyunjik Jo', 'Jiyeon Jung', 'Euisoon Kim', 'Hyosang Kim', 'Jihoon Kim', 'Joonkee Kim', 'Seonghwan Kim', 'Soyeon Kim', 'Sunkyoung Kim', 'Yireun Kim', 'Yongil Kim', 'Youchul Kim', 'Edward Hwayoung Lee', 'Gwangho Lee', 'Haeju Lee', 'Honglak Lee', 'Jinsik Lee', 'Kyungmin Lee', 'Sangha Park', 'Young Min Paik', 'Yongmin Park', 'Youngyong Park', 'Sanghyun Seo', 'Sihoon Yang', 'Heuiyeen Yeen', 'Sihyuk Yi', 'Hyeongu Yun'], 'affiliations': ['LG AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.11407.jpg', 'data': {'categories': ['#open_source', '#small_models', '#agents', '#multilingual', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'EXAONE 4.0: Интеграция рассуждений и многоязычности в ИИ нового поколения', 'desc': 'EXAONE 4.0 - это новая версия языковой модели, объединяющая режимы рассуждения и нерассуждения. Модель поддерживает многоязычность, включая английский, корейский и испанский языки. EXAONE 4.0 представлена в двух версиях: 32B для высокой производительности и 1.2B для использования на устройствах. Согласно отчету, модель демонстрирует превосходную производительность по сравнению с открытыми моделями аналогичного класса.'}, 'en': {'title': 'EXAONE 4.0: Bridging Usability and Advanced Reasoning in AI', 'desc': 'EXAONE 4.0 is a machine learning model that combines two operational modes: Non-reasoning and Reasoning, enhancing its usability and reasoning capabilities. It supports multiple languages, including English, Korean, and Spanish, making it versatile for diverse applications. The model comes in two sizes, a mid-size 32B for high performance and a small-size 1.2B for on-device use, catering to different computational needs. EXAONE 4.0 outperforms many open-weight models and remains competitive with advanced models, making it a valuable resource for researchers.'}, 'zh': {'title': 'EXAONE 4.0：智能代理时代的多语言高性能模型', 'desc': 'EXAONE 4.0 是一款集成了非推理模式和推理模式的机器学习模型，旨在结合 EXAONE 3.5 的优良可用性和 EXAONE Deep 的高级推理能力。该模型支持多语言，除了英语和韩语外，还新增了西班牙语的支持，推动了智能代理时代的发展。EXAONE 4.0 系列包括两种型号：中型 32B 模型优化了高性能，小型 1.2B 模型则专为设备端应用设计。与同类开放权重模型相比，EXAONE 4.0 展现出卓越的性能，甚至在前沿模型中也保持竞争力。'}}}, {'id': 'https://huggingface.co/papers/2507.09404', 'title': 'Scaling Laws for Optimal Data Mixtures', 'url': 'https://huggingface.co/papers/2507.09404', 'abstract': 'Scaling laws predict optimal data mixtures for large foundation models, improving performance across different domains and scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size N trained with D tokens and a specific domain weight vector h. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget (N,D), providing a principled alternative to costly trial-and-error methods.', 'score': 16, 'issue_id': 4841, 'pub_date': '2025-07-12', 'pub_date_card': {'ru': '12 июля', 'en': 'July 12', 'zh': '7月12日'}, 'hash': 'eac44b64250a5d2b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#data', '#optimization', '#training', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Законы масштабирования для оптимизации смеси данных в больших моделях', 'desc': 'Статья предлагает систематический метод определения оптимальной смеси данных для обучения больших фундаментальных моделей с использованием законов масштабирования. Авторы демонстрируют универсальность этих законов на примере обучения больших языковых моделей, мультимодальных моделей и моделей компьютерного зрения. Метод позволяет точно предсказывать потери модели в зависимости от ее размера, объема данных и весов доменов. Это дает возможность определить оптимальные веса доменов для целевой задачи при заданном бюджете обучения, что является принципиальной альтернативой затратным методам проб и ошибок.'}, 'en': {'title': 'Optimizing Data Mixtures for Better Model Performance', 'desc': 'This paper introduces a systematic method for determining the optimal data mixture for training large foundation models, which are models that learn from diverse data sources. It highlights the importance of the data mixture, or the proportion of data from different domains, in enhancing model performance. The authors utilize scaling laws to predict how well a model will perform based on its size and the specific weights assigned to different domains. Their approach is validated across various model types, showing that it can effectively guide the training process without the need for extensive trial-and-error.'}, 'zh': {'title': '优化数据混合，提升模型性能的科学方法', 'desc': '本文提出了一种系统的方法，通过缩放法则来确定大型基础模型的最佳数据混合比例。数据混合比例对模型性能至关重要，传统的选择方法依赖于反复试验，这在大规模预训练中变得不切实际。我们的方法能够准确预测在特定领域权重下，模型的损失，并在大型语言模型、原生多模态模型和大型视觉模型的预训练中验证了其普适性。通过少量小规模训练，我们可以估计参数，并推断在更大规模和未见领域权重下的性能，从而为任何目标领域提供最佳领域权重的原则性替代方案。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2507.10787', 'title': 'Can Multimodal Foundation Models Understand Schematic Diagrams? An\n  Empirical Study on Information-Seeking QA over Scientific Papers', 'url': 'https://huggingface.co/papers/2507.10787', 'abstract': "A benchmark evaluates multimodal models' ability to interpret scientific schematic diagrams and answer related questions, revealing performance gaps and insights for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature.", 'score': 8, 'issue_id': 4838, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '0dbd3a8f3aa7f42f', 'authors': ['Yilun Zhao', 'Chengye Wang', 'Chuhan Li', 'Arman Cohan'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10787.jpg', 'data': {'categories': ['#multimodal', '#science', '#interpretability', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'MISS-QA: новый рубеж в оценке мультимодальных моделей для научной литературы', 'desc': 'Статья представляет MISS-QA - первый бенчмарк для оценки способности моделей интерпретировать схематические диаграммы в научной литературе. Бенчмарк содержит 1500 примеров с экспертными аннотациями из 465 научных статей. Авторы оценили производительность 18 передовых мультимодальных моделей, включая o4-mini, Gemini-2.5-Flash и Qwen2.5-VL. Анализ выявил значительный разрыв в производительности между этими моделями и экспертами-людьми, а также предоставил ценные выводы для улучшения понимания моделями мультимодальной научной литературы.'}, 'en': {'title': 'Bridging the Gap: Evaluating Multimodal Models in Scientific Diagram Interpretation', 'desc': "This paper presents MISS-QA, a benchmark designed to evaluate how well multimodal models can interpret schematic diagrams in scientific literature. It includes 1,500 expert-annotated examples from 465 scientific papers, focusing on models' abilities to answer questions related to these diagrams. The study assesses 18 advanced multimodal foundation models, revealing a notable performance gap compared to human experts. Additionally, the paper provides insights into the strengths and weaknesses of these models, suggesting areas for improvement in understanding multimodal scientific content."}, 'zh': {'title': '评估多模态模型解读科学示意图的能力', 'desc': '本文介绍了MISS-QA，这是第一个专门用于评估模型解读科学示意图能力的基准。MISS-QA包含1500个专家注释的示例，涵盖465篇科学论文。模型的任务是解读示意图，并根据论文的整体背景回答相关问题。我们评估了18个前沿的多模态基础模型，发现这些模型与人类专家之间存在显著的性能差距，并提供了改进模型理解多模态科学文献的关键见解。'}}}, {'id': 'https://huggingface.co/papers/2507.09075', 'title': 'OpenCodeReasoning-II: A Simple Test Time Scaling Approach via\n  Self-Critique', 'url': 'https://huggingface.co/papers/2507.09075', 'abstract': 'Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a dataset consists of 2.5M question-solution-critique triples (approx. 35K unique programming questions), making it nearly twice the size of the previous largest publicly available code reasoning dataset. In this work, we employ a two-stage supervised fine-tuning strategy. The first stage focuses on fine-tuning for code generation, while the second stage involves the joint training of models for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct models achieve performance in code generation that either exceeds or equals the best prior open-weight distilled models. Notably, the integration of our code generation and critique models leads to significant improvements in competitive coding performance. Furthermore, we present an extension of the LiveCodeBench benchmark to specifically support the C++ programming language, thereby facilitating more comprehensive LLM evaluation using this benchmark.', 'score': 4, 'issue_id': 4840, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '1f6af7d1be9df3d3', 'authors': ['Wasi Uddin Ahmad', 'Somshubra Majumdar', 'Aleksander Ficek', 'Sean Narenthiran', 'Mehrzad Samadi', 'Jocelyn Huang', 'Siddhartha Jain', 'Vahid Noroozi', 'Boris Ginsburg'], 'affiliations': ['NVIDIA Santa Clara, CA 95051, USA'], 'pdf_title_img': 'assets/pdf/title_img/2507.09075.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#training', '#optimization', '#plp'], 'emoji': '🖥️', 'ru': {'title': 'Большие данные для умных кодеров: новый подход к обучению ИИ программированию', 'desc': 'Статья представляет OpenCodeReasoning-II - новый набор данных для обучения языковых моделей генерации и анализу кода. Он содержит 2.5 миллиона триплетов вопрос-решение-критика по 35 тысячам уникальных задач программирования. Авторы применяют двухэтапное обучение: сначала для генерации кода, затем совместно для генерации и критики. Полученные модели Qwen2.5-Instruct показывают высокие результаты в генерации кода и улучшают производительность в соревновательном программировании.'}, 'en': {'title': 'Empowering Code Generation with OpenCodeReasoning-II', 'desc': 'This paper presents OpenCodeReasoning-II, a large dataset designed to enhance code generation and critique using Large Language Models (LLMs). It contains 2.5 million question-solution-critique triples, making it the largest publicly available dataset for code reasoning. The authors employ a two-stage supervised fine-tuning approach, first optimizing for code generation and then jointly training for both code generation and critique. The resulting models demonstrate superior performance in coding tasks, and the paper also introduces an updated benchmark for evaluating LLMs specifically in C++ programming.'}, 'zh': {'title': '提升代码生成与批评的双重能力', 'desc': '本文介绍了OpenCodeReasoning-II数据集，该数据集包含250万对问题-解决方案-批评的三元组，几乎是之前最大公开代码推理数据集的两倍。我们采用了两阶段的监督微调策略，第一阶段专注于代码生成，第二阶段则联合训练代码生成和批评模型。经过微调的Qwen2.5-Instruct模型在代码生成方面的表现超过或等于之前最佳的开放权重蒸馏模型。通过将代码生成和批评模型结合，我们显著提高了竞争性编码的表现，并扩展了LiveCodeBench基准，以支持C++编程语言的评估。'}}}, {'id': 'https://huggingface.co/papers/2507.08616', 'title': 'AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs', 'url': 'https://huggingface.co/papers/2507.08616', 'abstract': "AgentsNet is a new benchmark for evaluating multi-agent systems' ability to self-organize, communicate, and solve problems collaboratively across varying network sizes.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-language models (LLMs) have demonstrated powerful problem-solving capabilities, in particular when organized in multi-agent systems. However, the advent of such systems also raises several questions on the ability of a complex network of agents to effectively self-organize and collaborate. While measuring performance on standard reasoning benchmarks indicates how well multi-agent systems can solve reasoning tasks, it is unclear whether these systems are able to leverage their topology effectively. Here, we propose AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration from classical problems in distributed systems and graph theory, AgentsNet measures the ability of multi-agent systems to collaboratively form strategies for problem-solving, self-organization, and effective communication given a network topology. We evaluate a variety of baseline methods on AgentsNet including homogeneous networks of agents which first have to agree on basic protocols for organization and communication. We find that some frontier LLMs are already demonstrating strong performance for small networks but begin to fall off once the size of the network scales. While existing multi-agent benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size and can scale with new generations of LLMs. As such, we also probe frontier models in a setup with up to 100 agents.", 'score': 3, 'issue_id': 4844, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'b3f11371fe8138de', 'authors': ['Florian Grötschla', 'Luis Müller', 'Jan Tönshoff', 'Mikhail Galkin', 'Bryan Perozzi'], 'affiliations': ['ETH Zurich', 'Google Research', 'RWTH Aachen University'], 'pdf_title_img': 'assets/pdf/title_img/2507.08616.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#agents', '#reasoning', '#games'], 'emoji': '🕸️', 'ru': {'title': 'AgentsNet: Оценка коллективного интеллекта в масштабируемых мультиагентных системах', 'desc': 'AgentsNet - это новый эталонный тест для оценки способности мультиагентных систем к самоорганизации, коммуникации и совместному решению задач в сетях различного размера. Он измеряет способность систем эффективно использовать свою топологию, формировать стратегии и устанавливать протоколы взаимодействия. Тест основан на классических проблемах распределенных систем и теории графов. Результаты показывают, что современные языковые модели хорошо справляются с небольшими сетями, но их эффективность снижается при увеличении масштаба.'}, 'en': {'title': 'AgentsNet: Scaling Multi-Agent Collaboration and Communication', 'desc': 'AgentsNet is a benchmark designed to assess how well multi-agent systems can self-organize, communicate, and collaboratively solve problems across different network sizes. It focuses on the ability of agents to leverage their network topology for effective problem-solving strategies. The benchmark evaluates various baseline methods, including homogeneous networks, to see how agents agree on communication protocols. Results show that while some advanced large-language models perform well with small networks, their effectiveness decreases as the network size increases, highlighting the need for scalable evaluation methods.'}, 'zh': {'title': 'AgentsNet：多智能体系统的新基准', 'desc': 'AgentsNet是一个新的基准，用于评估多智能体系统在自我组织、沟通和协作解决问题方面的能力。该基准灵感来源于分布式系统和图论的经典问题，旨在测量多智能体系统在特定网络拓扑下的协作策略形成能力。研究发现，尽管一些前沿的大型语言模型在小型网络中表现良好，但当网络规模扩大时，其性能会下降。AgentsNet的规模几乎没有限制，可以与新一代大型语言模型一起扩展，支持多达100个智能体的设置。'}}}, {'id': 'https://huggingface.co/papers/2507.08333', 'title': 'Token-based Audio Inpainting via Discrete Diffusion', 'url': 'https://huggingface.co/papers/2507.08333', 'abstract': 'A discrete diffusion model for audio inpainting using tokenized audio representations achieves competitive performance for reconstructing long gaps in corrupted audio recordings.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio inpainting refers to the task of reconstructing missing segments in corrupted audio recordings. While prior approaches-including waveform and spectrogram-based diffusion models-have shown promising results for short gaps, they often degrade in quality when gaps exceed 100 milliseconds (ms). In this work, we introduce a novel inpainting method based on discrete diffusion modeling, which operates over tokenized audio representations produced by a pre-trained audio tokenizer. Our approach models the generative process directly in the discrete latent space, enabling stable and semantically coherent reconstruction of missing audio. We evaluate the method on the MusicNet dataset using both objective and perceptual metrics across gap durations up to 300 ms. We further evaluated our approach on the MTG dataset, extending the gap duration to 500 ms. Experimental results demonstrate that our method achieves competitive or superior performance compared to existing baselines, particularly for longer gaps, offering a robust solution for restoring degraded musical recordings. Audio examples of our proposed method can be found at https://iftach21.github.io/', 'score': 3, 'issue_id': 4852, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'd4c158f8a9ae8f27', 'authors': ['Tali Dror', 'Iftach Shoham', 'Moshe Buchris', 'Oren Gal', 'Haim Permuter', 'Gilad Katz', 'Eliya Nachmani'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.08333.jpg', 'data': {'categories': ['#audio', '#diffusion'], 'emoji': '🎵', 'ru': {'title': 'Восстановление аудио с помощью дискретной диффузии в токенизированном пространстве', 'desc': 'Статья представляет новый метод восстановления пропущенных сегментов в поврежденных аудиозаписях, основанный на дискретной диффузионной модели. Модель работает с токенизированными аудио-представлениями, полученными с помощью предварительно обученного аудио-токенизатора. Этот подход позволяет моделировать генеративный процесс непосредственно в дискретном латентном пространстве, обеспечивая стабильную и семантически согласованную реконструкцию отсутствующего аудио. Эксперименты показывают, что метод достигает конкурентоспособных или превосходящих результатов по сравнению с существующими базовыми моделями, особенно для длительных пропусков до 500 мс.'}, 'en': {'title': 'Revolutionizing Audio Inpainting with Discrete Diffusion Models', 'desc': 'This paper presents a new method for audio inpainting, which is the process of filling in missing parts of audio recordings. The authors propose a discrete diffusion model that works with tokenized audio representations, allowing for better reconstruction of longer gaps in audio, specifically those exceeding 100 milliseconds. By operating in a discrete latent space, the model ensures that the reconstructed audio is both stable and semantically coherent. The results show that this method outperforms existing techniques, especially for gaps up to 500 milliseconds, making it a strong candidate for restoring damaged musical recordings.'}, 'zh': {'title': '离散扩散模型：音频修复的新突破', 'desc': '本文提出了一种基于离散扩散模型的音频修复方法，旨在重建受损音频录音中的缺失部分。与之前的波形和谱图扩散模型相比，该方法在处理超过100毫秒的长缺口时表现更为出色。我们的方法利用预训练的音频标记器生成的标记化音频表示，在离散潜在空间中直接建模生成过程，从而实现稳定且语义一致的音频重建。实验结果表明，该方法在MusicNet和MTG数据集上均表现出竞争力，尤其是在长缺口的修复任务中。'}}}, {'id': 'https://huggingface.co/papers/2507.09411', 'title': 'LLMalMorph: On The Feasibility of Generating Variant Malware using\n  Large-Language-Models', 'url': 'https://huggingface.co/papers/2507.09411', 'abstract': 'A semi-automated framework uses Large Language Models to generate malware variants, demonstrating reduced detection rates and notable attack success against ML classifiers.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have transformed software development and automated code generation. Motivated by these advancements, this paper explores the feasibility of LLMs in modifying malware source code to generate variants. We introduce LLMalMorph, a semi-automated framework that leverages semantical and syntactical code comprehension by LLMs to generate new malware variants. LLMalMorph extracts function-level information from the malware source code and employs custom-engineered prompts coupled with strategically defined code transformations to guide the LLM in generating variants without resource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse Windows malware samples of varying types, complexity and functionality and generated 618 variants. Our thorough experiments demonstrate that it is possible to reduce the detection rates of antivirus engines of these malware variants to some extent while preserving malware functionalities. In addition, despite not optimizing against any Machine Learning (ML)-based malware detectors, several variants also achieved notable attack success rates against an ML-based malware classifier. We also discuss the limitations of current LLM capabilities in generating malware variants from source code and assess where this emerging technology stands in the broader context of malware variant generation.', 'score': 2, 'issue_id': 4838, 'pub_date': '2025-07-12', 'pub_date_card': {'ru': '12 июля', 'en': 'July 12', 'zh': '7月12日'}, 'hash': '19537aed07d51388', 'authors': ['Md Ajwad Akil', 'Adrian Shuai Li', 'Imtiaz Karim', 'Arun Iyengar', 'Ashish Kundu', 'Vinny Parla', 'Elisa Bertino'], 'affiliations': ['Cisco Research', 'Cisco Systems, Inc', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2507.09411.jpg', 'data': {'categories': ['#dataset', '#security', '#agents', '#data'], 'emoji': '🦠', 'ru': {'title': 'LLM на службе киберпреступности: новый подход к созданию вредоносного ПО', 'desc': 'Исследователи разработали полуавтоматизированную систему LLMalMorph, использующую большие языковые модели (LLM) для создания вариантов вредоносного ПО. Система извлекает информацию на уровне функций из исходного кода вредоносного ПО и применяет специально разработанные промпты для генерации новых вариантов. Эксперименты показали, что созданные варианты имеют более низкие показатели обнаружения антивирусными движками, сохраняя при этом функциональность вредоносного ПО. Некоторые варианты также продемонстрировали успешные атаки на классификатор вредоносного ПО на основе машинного обучения.'}, 'en': {'title': 'Harnessing LLMs for Evolving Malware Variants', 'desc': 'This paper presents LLMalMorph, a semi-automated framework that utilizes Large Language Models (LLMs) to create new variants of malware by modifying existing source code. The framework employs semantic and syntactic understanding to extract function-level information and generate variants through custom prompts and code transformations. Experiments with 10 different Windows malware samples resulted in 618 variants, which showed reduced detection rates by antivirus engines while maintaining their malicious functionalities. The study highlights the effectiveness of LLMs in generating malware variants and discusses their limitations in the context of evolving malware detection technologies.'}, 'zh': {'title': '利用LLMs生成恶意软件变种的创新框架', 'desc': '这篇论文介绍了一种半自动化框架，利用大型语言模型（LLMs）生成恶意软件变种。该框架名为LLMalMorph，通过对恶意软件源代码的语义和语法理解，生成新的恶意软件变种。研究表明，使用LLMalMorph生成的变种可以在一定程度上降低杀毒引擎的检测率，同时保持恶意软件的功能。此外，尽管没有针对机器学习（ML）恶意软件检测器进行优化，某些变种在攻击成功率上也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2507.11336', 'title': 'UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New\n  Benchmarks', 'url': 'https://huggingface.co/papers/2507.11336', 'abstract': 'UGC-VideoCap introduces a new benchmark and model for detailed omnimodal captioning of user-generated videos, emphasizing audio-visual integration and using a novel training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio visual content. However, existing video captioning benchmarks and models remain predominantly visual centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of omni datasets and lightweight, capable models hampers progress in fine grained, multimodal video understanding. To address these challenges, we introduce UGC-VideoCap, a new benchmark and model framework specifically designed for detailed omnimodal captioning of short form user-generated videos. Unlike prior datasets, UGC-VideoCap emphasizes balanced integration of audio and visual modalities, featuring 1000 TikTok videos annotated through a structured three stage human-in-the-loop pipeline covering audio only, visual only, and joint audio visual semantics. The benchmark also includes 4000 carefully crafted QA pairs probing both unimodal and cross modal understanding. Alongside the dataset, we propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine tuning followed by Group Relative Policy Optimization (GRPO), our approach enables efficient adaptation from limited data while maintaining competitive performance. Together, our benchmark and model offer a high-quality foundation and a data-efficient solution for advancing omnimodal video captioning in unconstrained real-world UGC settings.', 'score': 1, 'issue_id': 4850, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '4df1238a684dfbaf', 'authors': ['Peiran Wu', 'Yunze Liu', 'Zhengdong Zhu', 'Enmin Zhou', 'Shawn Shen'], 'affiliations': ['Memories.ai', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2507.11336.jpg', 'data': {'categories': ['#optimization', '#dataset', '#multimodal', '#video', '#training', '#transfer_learning', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Омнимодальное описание пользовательских видео: новый рубеж в понимании мультимодального контента', 'desc': 'UGC-VideoCap представляет новый эталонный набор данных и модель для детального омнимодального описания пользовательских видео, уделяя особое внимание аудиовизуальной интеграции. Модель использует новую стратегию обучения, включающую контролируемую точную настройку и групповую относительную оптимизацию политики (GRPO). Набор данных содержит 1000 видео TikTok с аннотациями, охватывающими аудио, визуальные и совместные аудиовизуальные семантики, а также 4000 пар вопросов и ответов. Этот подход обеспечивает эффективную адаптацию модели при ограниченных данных, сохраняя конкурентоспособную производительность.'}, 'en': {'title': 'Revolutionizing Video Captioning with Omnimodal Insights', 'desc': 'UGC-VideoCap presents a new benchmark and model aimed at enhancing the captioning of user-generated videos by integrating both audio and visual elements. Traditional models have focused mainly on visual data, neglecting the important role of audio in understanding context and intent. This new framework includes a dataset of 1000 TikTok videos with annotations for audio, visual, and combined modalities, along with 4000 QA pairs for testing both unimodal and cross-modal comprehension. The UGC-VideoCaptioner model employs a two-stage training strategy to efficiently learn from limited data while achieving strong performance in generating detailed captions.'}, 'zh': {'title': '全模态视频字幕生成的新基准与模型', 'desc': 'UGC-VideoCap是一个新的基准和模型，专注于用户生成视频的详细全模态字幕生成，强调音频与视觉的整合。现有的视频字幕生成模型主要集中于视觉内容，忽视了音频在场景动态、说话者意图和叙事背景中的重要作用。UGC-VideoCap包含1000个经过注释的TikTok视频，采用三阶段的人机协作流程，确保音频和视觉模态的平衡整合。我们还提出了UGC-VideoCaptioner(3B)模型，采用新颖的两阶段训练策略，能够在有限数据下高效适应，同时保持竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.07186', 'title': 'Planted in Pretraining, Swayed by Finetuning: A Case Study on the\n  Origins of Cognitive Biases in LLMs', 'url': 'https://huggingface.co/papers/2507.07186', 'abstract': 'Research identifies pretraining as the primary source of cognitive biases in large language models, distinguishing its influence from finetuning and training randomness.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans. Prior work has found that these biases vary across models and can be amplified by instruction tuning. However, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity. We propose a two-step causal experimental approach to disentangle these factors. First, we finetune models multiple times using different random seeds to study how training randomness affects over 30 cognitive biases. Second, we introduce cross-tuning -- swapping instruction datasets between models to isolate bias sources. This swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent. Our findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data. These insights suggest that understanding biases in finetuned models requires considering their pretraining origins beyond finetuning effects. This perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs.', 'score': 1, 'issue_id': 4843, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'd971fa9f03ad8239', 'authors': ['Itay Itzhak', 'Yonatan Belinkov', 'Gabriel Stanovsky'], 'affiliations': ['Technion Israel Institute of Technology', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2507.07186.jpg', 'data': {'categories': ['#hallucinations', '#training', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Предобучение - ключ к когнитивным искажениям в ИИ', 'desc': 'Исследование показывает, что когнитивные искажения в больших языковых моделях (LLM) в основном формируются на этапе предварительного обучения, а не во время тонкой настройки или из-за случайности в процессе обучения. Учёные использовали метод кросс-настройки, меняя наборы данных для инструкций между моделями, чтобы выделить источники искажений. Результаты демонстрируют, что модели с одинаковой предварительно обученной основой имеют более схожие паттерны искажений, чем модели, разделяющие только данные для тонкой настройки. Это понимание может помочь в разработке стратегий оценки и снижения искажений в LLM.'}, 'en': {'title': 'Pretraining: The Key to Understanding Bias in Language Models', 'desc': 'This paper investigates the origins of cognitive biases in large language models (LLMs), focusing on the role of pretraining. It distinguishes the effects of pretraining from those of finetuning and random training noise. The authors employ a two-step experimental approach, including finetuning with different random seeds and cross-tuning with varied instruction datasets. Their findings indicate that pretraining is the primary factor influencing bias patterns, suggesting that understanding these biases requires a deeper look at the pretraining phase.'}, 'zh': {'title': '预训练是认知偏见的根源', 'desc': '本研究发现，预训练是大型语言模型中认知偏见的主要来源，区别于微调和训练随机性。研究表明，这些偏见在不同模型之间存在差异，并且可以通过指令微调加以放大。我们采用了两步因果实验方法，首先通过不同随机种子多次微调模型，研究训练随机性对认知偏见的影响。其次，通过交叉微调的方法，交换模型之间的指令数据集，以隔离偏见来源，结果显示预训练对偏见的形成起着决定性作用。'}}}, {'id': 'https://huggingface.co/papers/2507.10571', 'title': 'Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification\n  System with Trust-Aware Orchestration and RAG-Based Reasoning', 'url': 'https://huggingface.co/papers/2507.10571', 'abstract': 'A modular Agentic AI framework integrates multimodal agents with a reasoning orchestrator and RAG module to improve trust and accuracy in zero-shot visual classification tasks like apple leaf disease diagnosis.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust', 'score': 1, 'issue_id': 4849, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '22fa300e7776b459', 'authors': ['Konstantinos I. Roumeliotis', 'Ranjan Sapkota', 'Manoj Karkee', 'Nikolaos D. Tselikas'], 'affiliations': ['Cornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA', 'University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2507.10571.jpg', 'data': {'categories': ['#reasoning', '#rag', '#open_source', '#multimodal', '#interpretability', '#agents', '#benchmark'], 'emoji': '🍎', 'ru': {'title': 'Доверительная мультиагентная система для визуальной классификации без обучения', 'desc': 'Статья представляет новую модульную архитектуру агентного ИИ для задач визуальной классификации без предварительного обучения. Система интегрирует мультимодальных агентов с оркестратором рассуждений и модулем RAG для повышения доверия и точности. Применение к диагностике болезней листьев яблони показало значительное улучшение точности до 85,63% в режиме zero-shot. Архитектура разделяет восприятие и мета-рассуждения, что делает ее масштабируемой и интерпретируемой.'}, 'en': {'title': 'Enhancing Trust and Accuracy in AI with Modular Frameworks', 'desc': 'This paper presents a modular Agentic AI framework designed to enhance trust and accuracy in zero-shot visual classification tasks, specifically for diagnosing apple leaf diseases. The framework combines multimodal agents with a reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module to improve performance without the need for fine-tuning. By implementing trust-calibrated orchestration and confidence calibration metrics, the system achieves significant accuracy improvements in zero-shot settings. The proposed architecture separates perception from reasoning, making it scalable and interpretable for various applications in trust-critical fields.'}, 'zh': {'title': '模块化Agentic AI框架：提升视觉分类信任与准确性', 'desc': '本论文提出了一种模块化的Agentic AI框架，旨在提高零-shot视觉分类任务中的信任度和准确性，例如苹果叶病的诊断。该框架结合了多模态智能体、推理协调器和增强检索生成（RAG）模块，以应对在没有微调的情况下如何信任这些智能体的挑战。通过信心校准指标，协调器调节不同智能体之间的信任，最终在零-shot设置中实现了77.94%的准确率提升。该系统的设计使得视觉感知与元推理分离，具有可扩展性和可解释性，适用于诊断、生物学等信任关键领域。'}}}, {'id': 'https://huggingface.co/papers/2507.04127', 'title': 'BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question\n  Answering', 'url': 'https://huggingface.co/papers/2507.04127', 'abstract': 'BYOKG-RAG combines LLMs with specialized graph retrieval tools to enhance KGQA, improving generalization and performance over custom knowledge graphs.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge graph question answering (KGQA) presents significant challenges due to the structural and semantic variations across input graphs. Existing works rely on Large Language Model (LLM) agents for graph traversal and retrieval; an approach that is sensitive to traversal initialization, as it is prone to entity linking errors and may not generalize well to custom ("bring-your-own") KGs. We introduce BYOKG-RAG, a framework that enhances KGQA by synergistically combining LLMs with specialized graph retrieval tools. In BYOKG-RAG, LLMs generate critical graph artifacts (question entities, candidate answers, reasoning paths, and OpenCypher queries), and graph tools link these artifacts to the KG and retrieve relevant graph context. The retrieved context enables the LLM to iteratively refine its graph linking and retrieval, before final answer generation. By retrieving context from different graph tools, BYOKG-RAG offers a more general and robust solution for QA over custom KGs. Through experiments on five benchmarks spanning diverse KG types, we demonstrate that BYOKG-RAG outperforms the second-best graph retrieval method by 4.5% points while showing better generalization to custom KGs. BYOKG-RAG framework is open-sourced at https://github.com/awslabs/graphrag-toolkit.', 'score': 0, 'issue_id': 4855, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 июля', 'en': 'July 5', 'zh': '7月5日'}, 'hash': 'e4b4d056f4c6d790', 'authors': ['Costas Mavromatis', 'Soji Adeshina', 'Vassilis N. Ioannidis', 'Zhen Han', 'Qi Zhu', 'Ian Robinson', 'Bryan Thompson', 'Huzefa Rangwala', 'George Karypis'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2507.04127.jpg', 'data': {'categories': ['#rag', '#multimodal', '#graphs', '#benchmark', '#open_source'], 'emoji': '🕸️', 'ru': {'title': 'BYOKG-RAG: Синергия LLM и графовых инструментов для улучшения KGQA', 'desc': 'BYOKG-RAG - это новый фреймворк для улучшения вопросно-ответных систем на основе графов знаний (KGQA). Он объединяет большие языковые модели (LLM) со специализированными инструментами извлечения информации из графов. BYOKG-RAG генерирует ключевые артефакты графа с помощью LLM и использует графовые инструменты для связывания этих артефактов с графом знаний. Этот подход показывает лучшую обобщаемость и производительность на пользовательских графах знаний по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing KGQA with BYOKG-RAG: A Synergistic Approach', 'desc': 'The paper introduces BYOKG-RAG, a novel framework that enhances Knowledge Graph Question Answering (KGQA) by integrating Large Language Models (LLMs) with specialized graph retrieval tools. This approach addresses the challenges of structural and semantic variations in custom knowledge graphs, which can lead to errors in entity linking and poor generalization. BYOKG-RAG allows LLMs to generate essential graph components and uses graph tools to link these components to the knowledge graph, improving the retrieval of relevant context. Experimental results show that BYOKG-RAG significantly outperforms existing methods, achieving better accuracy and generalization across various knowledge graph types.'}, 'zh': {'title': 'BYOKG-RAG：提升知识图谱问答的强大工具', 'desc': 'BYOKG-RAG是一个结合大型语言模型（LLM）和专门图检索工具的框架，旨在提升知识图谱问答（KGQA）的性能和泛化能力。该框架通过生成关键的图形构件（如问题实体、候选答案、推理路径和OpenCypher查询），并利用图工具将这些构件链接到知识图谱中，从而改善图的链接和检索过程。通过从不同的图工具中检索上下文，BYOKG-RAG提供了一个更通用和稳健的解决方案，适用于自定义知识图谱的问答。实验结果表明，BYOKG-RAG在五个基准测试中表现优于第二好的图检索方法，提升了4.5个百分点，并且在自定义知识图谱上具有更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.07966', 'title': 'Scaling RL to Long Videos', 'url': 'https://huggingface.co/papers/2507.07966', 'abstract': 'A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).', 'score': 112, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '4ab23da398f0e8d8', 'authors': ['Yukang Chen', 'Wei Huang', 'Baifeng Shi', 'Qinghao Hu', 'Hanrong Ye', 'Ligeng Zhu', 'Zhijian Liu', 'Pavlo Molchanov', 'Jan Kautz', 'Xiaojuan Qi', 'Sifei Liu', 'Hongxu Yin', 'Yao Lu', 'Song Han'], 'affiliations': ['HKU', 'MIT', 'NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.07966.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#multimodal', '#rl', '#open_source', '#training', '#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'Революция в понимании длинных видео с помощью ИИ', 'desc': 'Представлена полноценная система для масштабирования моделей визуально-языкового понимания на длинные видео с использованием обучения с подкреплением. Система включает большой датасет LongVideo-Reason с 52 тысячами пар вопрос-ответ по длинным видео, двухэтапный процесс обучения с цепочкой рассуждений и RL, а также специализированную инфраструктуру обучения MR-SP. Модель LongVILA-R1-7B демонстрирует высокую производительность на различных задачах рассуждения по длинным видео. Предложенный подход позволяет эффективно масштабировать визуально-языковые модели для работы с длинными видео.'}, 'en': {'title': 'Scaling Vision-Language Models for Long Video Reasoning', 'desc': "This paper presents a framework designed to enhance vision-language models (VLMs) for reasoning tasks involving long videos. It introduces a large dataset called LongVideo-Reason, which contains 52,000 question-answer pairs related to long videos, facilitating high-quality reasoning across various domains. The framework employs a two-stage training process that combines chain-of-thought supervised fine-tuning with reinforcement learning, optimizing the model's performance. Additionally, it features a specialized training infrastructure, Multi-modal Reinforcement Sequence Parallelism, which significantly accelerates the training process for long video reasoning tasks."}, 'zh': {'title': '长视频推理的新突破', 'desc': '本文提出了一种框架，用于通过强化学习将视觉-语言模型（VLMs）扩展到长视频推理。我们整合了三个关键组件：一个包含52K长视频问答对的大规模数据集LongVideo-Reason，一个两阶段的训练流程，以及一个名为多模态强化序列并行（MR-SP）的训练基础设施。实验结果表明，LongVILA-R1-7B在长视频问答基准上表现优异，并在时间推理、目标和目的推理、空间推理等方面超越了其他模型。我们的系统在长视频强化学习训练中实现了高达2.1倍的加速，标志着在VLMs中进行长视频推理的坚实一步。'}}}, {'id': 'https://huggingface.co/papers/2507.05964', 'title': 'T-LoRA: Single Image Diffusion Model Customization Without Overfitting', 'url': 'https://huggingface.co/papers/2507.05964', 'abstract': 'T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA.', 'score': 94, 'issue_id': 4766, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': '8df9f241b664bc80', 'authors': ['Vera Soboleva', 'Aibek Alanov', 'Andrey Kuznetsov', 'Konstantin Sobolev'], 'affiliations': ['AIRI, HSE University', 'AIRI, MSU', 'AIRI, Sber, Innopolis', 'HSE University, AIRI'], 'pdf_title_img': 'assets/pdf/title_img/2507.05964.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#low_resource'], 'emoji': '🎨', 'ru': {'title': 'T-LoRA: Точная персонализация диффузионных моделей на одном изображении', 'desc': 'T-LoRA - это новая техника для персонализации диффузионных моделей, использующая адаптацию низкого ранга, зависящую от временного шага. Она решает проблему переобучения при ограниченном наборе данных, улучшая точность воспроизведения концепций и соответствие тексту. T-LoRA применяет динамическую стратегию дообучения и ортогональную инициализацию для обеспечения независимости компонентов адаптера. Эксперименты показывают превосходство T-LoRA над стандартными методами персонализации диффузионных моделей в условиях ограниченных данных и ресурсов.'}, 'en': {'title': 'Personalizing Diffusion Models with T-LoRA: Dynamic Fine-Tuning for Better Results', 'desc': "T-LoRA is a framework designed to improve the personalization of diffusion models, particularly when only a limited amount of data is available. It introduces a dynamic fine-tuning strategy that adapts to different diffusion timesteps, addressing the issue of overfitting that often occurs with higher timesteps. Additionally, T-LoRA employs orthogonal initialization to maintain independence among adapter components, enhancing the model's ability to generate accurate outputs. Through extensive testing, T-LoRA demonstrates superior performance in balancing concept fidelity and text alignment compared to traditional methods."}, 'zh': {'title': 'T-LoRA：扩散模型个性化的新突破', 'desc': 'T-LoRA是一种时间步依赖的低秩适应框架，旨在通过动态微调策略和正交初始化来增强扩散模型的个性化。该方法解决了在样本有限的情况下，扩散模型微调常常出现的过拟合问题，从而提高了概念的保真度和文本对齐能力。T-LoRA的创新之处在于其动态微调策略和权重参数化技术，使得适配器组件之间保持独立。实验结果表明，T-LoRA在数据有限和资源受限的情况下，优于标准的LoRA和其他个性化技术。'}}}, {'id': 'https://huggingface.co/papers/2507.07999', 'title': 'Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology', 'url': 'https://huggingface.co/papers/2507.07999', 'abstract': 'TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human "thinking with images". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.', 'score': 40, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'e52c2296896d713c', 'authors': ['Haochen Wang', 'Xiangtai Li', 'Zilong Huang', 'Anran Wang', 'Jiacong Wang', 'Tao Zhang', 'Jiani Zheng', 'Sule Bai', 'Zijian Kang', 'Jiashi Feng', 'Zhuochen Wang', 'Zhaoxiang Zhang'], 'affiliations': ['ByteDance', 'NLPR, MAIS, CASIA', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2507.07999.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#interpretability', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Отслеживаемые доказательства - ключ к улучшению визуального ИИ', 'desc': 'TreeBench - это диагностический бенчмарк для оценки визуального обоснованного рассуждения, основанный на трех принципах: восприятие сложных сцен, отслеживаемые доказательства и рассуждения второго порядка. Он состоит из 405 сложных пар вопросов и ответов по изображениям, с которыми даже самые продвинутые модели справляются с трудом. TreeVGR - это парадигма обучения, использующая обучение с подкреплением для совместного контроля локализации и рассуждений. Инициализированная на основе Qwen2.5-VL-7B, она улучшает результаты на нескольких бенчмарках, доказывая важность отслеживаемости для развития визуально-обоснованных рассуждений.'}, 'en': {'title': 'Enhancing Visual Grounded Reasoning with TreeBench and TreeVGR', 'desc': 'This paper introduces TreeBench, a benchmark designed to evaluate visual grounded reasoning by focusing on subtle target detection, traceable evidence, and second-order reasoning. It highlights the need for a comprehensive assessment tool as existing models struggle with complex visual tasks, achieving less than 60% accuracy on the benchmark. TreeVGR is proposed as an enhancement that uses reinforcement learning to improve joint localization and reasoning, demonstrating significant performance gains over existing models. The research emphasizes the importance of traceability in developing advanced visual reasoning capabilities.'}, 'zh': {'title': '提升视觉推理的可追溯性', 'desc': '本文提出了TreeBench，一个用于评估视觉基础推理的基准，侧重于复杂场景中微妙目标的检测、可追溯证据和二阶推理。TreeVGR则通过强化学习增强了这一过程，实现了定位和推理的联合训练。研究表明，现有的先进模型在TreeBench基准上表现不佳，准确率未超过60%。通过引入可追溯性，TreeVGR显著提升了模型在视觉基础推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.07984', 'title': 'OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding', 'url': 'https://huggingface.co/papers/2507.07984', 'abstract': 'OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/', 'score': 34, 'issue_id': 4765, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '044eb90ac618689f', 'authors': ['JingLi Lin', 'Chenming Zhu', 'Runsen Xu', 'Xiaohan Mao', 'Xihui Liu', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.07984.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#dataset', '#reasoning', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Новый вызов для ИИ: рассуждения в пространстве и времени', 'desc': 'OST-Bench - это новый бенчмарк для оценки мультимодальных больших языковых моделей в задачах онлайн пространственно-временного рассуждения. Он включает 1,4 тыс. сцен и 10 тыс. пар вопросов-ответов, собранных из реальных 3D-сканов помещений. Эксперименты показали, что современные модели испытывают трудности с комплексным пространственным рассуждением и долговременной памятью. Бенчмарк выявляет ключевые проблемы, которые необходимо решить для улучшения воплощенных рассуждений в реальном мире.'}, 'en': {'title': 'Evaluating MLLMs in Real-World Spatio-Temporal Reasoning', 'desc': 'OST-Bench is a new benchmark that tests multimodal large language models (MLLMs) on their ability to understand and reason about space and time while interacting with real-world environments. Unlike traditional benchmarks that use fixed inputs, OST-Bench evaluates models in an online setting where they must process information as they explore scenes. The study reveals that current MLLMs struggle with complex spatial reasoning and long-term memory tasks, especially as the amount of information increases. By identifying common errors, the research highlights key areas for improvement in online embodied perception and reasoning.'}, 'zh': {'title': '在线时空推理的新挑战', 'desc': 'OST-Bench是一个评估多模态大型语言模型（MLLMs）在在线时空推理任务中的基准。它强调了在动态场景中处理复杂空间线索和长期记忆的挑战。通过对1.4千个场景和1万对问答的评估，发现现有模型在复杂时空推理任务中表现不佳，尤其是在探索范围扩大和记忆增长时准确率下降。该基准旨在推动在线具身推理的研究与发展，提供了数据集和代码以供进一步探索。'}}}, {'id': 'https://huggingface.co/papers/2507.07990', 'title': 'Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs', 'url': 'https://huggingface.co/papers/2507.07990', 'abstract': 'A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm.', 'score': 31, 'issue_id': 4766, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'eff3cb8ac467d2a7', 'authors': ['Jeongseok Hyun', 'Sukjun Hwang', 'Su Ho Han', 'Taeoh Kim', 'Inwoong Lee', 'Dongyoon Wee', 'Joon-Young Lee', 'Seon Joo Kim', 'Minho Shim'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'NAVER Cloud', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07990.jpg', 'data': {'categories': ['#video', '#optimization', '#training', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение видео-LLM без потери качества', 'desc': 'Метод пространственно-временного объединения токенов (STTM) повышает эффективность видео-LLM, используя избыточность данных. STTM преобразует каждый кадр в многоуровневые пространственные токены с помощью квадродерева, а затем выполняет направленное попарное объединение по временной оси. Этот подход превосходит существующие методы сокращения токенов на шести эталонных тестах видео-QA. STTM достигает двукратного ускорения с минимальной потерей точности при 50% бюджете токенов.'}, 'en': {'title': 'Boosting Video LLM Efficiency with Smart Token Merging', 'desc': 'This paper introduces a new method called Spatio-Temporal Token Merging (STTM) to enhance the efficiency of video large language models (LLMs). The method reduces the number of tokens used in video processing by merging them based on local spatial and temporal redundancies, which helps to maintain performance while speeding up computations. STTM achieves significant speed improvements, allowing for up to three times faster processing with minimal accuracy loss. Additionally, it is designed to be query-agnostic, enabling the reuse of cached information across different questions about the same video.'}, 'zh': {'title': '时空令牌合并，提升视频模型效率！', 'desc': '本文提出了一种时空令牌合并方法（STTM），旨在提高视频大语言模型（LLM）的效率。该方法通过利用视频数据中的局部空间和时间冗余，显著减少计算量，同时保持较高的准确性。STTM首先将每帧图像转化为多粒度的空间令牌，然后在时间维度上进行有针对性的配对合并。实验结果表明，STTM在多个视频问答基准测试中表现优于现有的令牌减少方法，能够实现显著的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2507.07998', 'title': 'PyVision: Agentic Vision with Dynamic Tooling', 'url': 'https://huggingface.co/papers/2507.07998', 'abstract': 'PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.', 'score': 27, 'issue_id': 4761, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'ab8504b49800fe67', 'authors': ['Shitian Zhao', 'Haoquan Zhang', 'Shaoheng Lin', 'Ming Li', 'Qilong Wu', 'Kaipeng Zhang', 'Chen Wei'], 'affiliations': ['CUHK', 'NUS', 'Rice University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.07998.jpg', 'data': {'categories': ['#reasoning', '#agents', '#interpretability', '#cv', '#benchmark'], 'emoji': '🔧', 'ru': {'title': 'PyVision: LLM создают инструменты для визуального анализа', 'desc': 'PyVision - это интерактивный фреймворк, позволяющий большим языковым моделям (LLM) автономно создавать и совершенствовать инструменты на Python для визуального анализа. Фреймворк использует многоэтапный подход, где модель генерирует, выполняет и улучшает инструменты под конкретную задачу. PyVision значительно повышает производительность LLM на различных бенчмарках, например, улучшая результаты GPT-4.1 на 7.8% в V* и Claude-4.0-Sonnet на 31.1% в VLMsAreBlind-mini. Это представляет собой шаг к более гибкому и интерпретируемому решению задач в области компьютерного зрения.'}, 'en': {'title': 'Empowering LLMs with Dynamic Tool Creation for Visual Reasoning', 'desc': 'PyVision is an innovative framework that empowers large language models (LLMs) to autonomously create and improve Python tools for visual reasoning tasks. Unlike previous methods that relied on fixed workflows, PyVision allows for dynamic tool generation and execution, enhancing flexibility in problem-solving. The framework has been evaluated across various benchmarks, showing significant performance improvements, such as a 7.8% increase for GPT-4.1 and a 31.1% boost for Claude-4.0-Sonnet. This advancement signifies a shift towards more agentic capabilities in visual reasoning, where models can not only utilize existing tools but also invent new ones.'}, 'zh': {'title': '动态工具，智能推理的新纪元', 'desc': 'PyVision是一个交互式框架，允许大型语言模型（LLMs）自主创建和改进基于Python的视觉推理工具。与以往的静态工具集不同，PyVision支持多轮交互，使模型能够根据具体任务灵活生成和执行工具。研究表明，PyVision在多个基准测试中显著提高了性能，例如GPT-4.1在V*上提升了7.8%。这些结果表明，动态工具的使用使模型不仅能够使用工具，还能发明新工具，推动视觉推理的进步。'}}}, {'id': 'https://huggingface.co/papers/2507.07982', 'title': 'Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling', 'url': 'https://huggingface.co/papers/2507.07982', 'abstract': "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.", 'score': 27, 'issue_id': 4762, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'fbe6e1954d8e9c30', 'authors': ['Haoyu Wu', 'Diankun Wu', 'Tianyu He', 'Junliang Guo', 'Yang Ye', 'Yueqi Duan', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.07982.jpg', 'data': {'categories': ['#alignment', '#video', '#diffusion', '#3d'], 'emoji': '🧊', 'ru': {'title': 'Geometry Forcing: внедрение 3D-геометрии в видео-диффузионные модели', 'desc': 'Статья представляет метод Geometry Forcing для улучшения видео-диффузионных моделей. Авторы предлагают выравнивать промежуточные представления модели с признаками из предобученной геометрической модели-основы. Метод включает два вида выравнивания: угловое для согласованности направлений и масштабное для сохранения информации о масштабе. Эксперименты показывают, что Geometry Forcing значительно улучшает визуальное качество и 3D-согласованность генерируемых видео.'}, 'en': {'title': 'Bridging 2D Videos to 3D Understanding with Geometry Forcing', 'desc': 'This paper addresses the limitations of video diffusion models that do not effectively capture the 3D structure of the world from 2D video data. The authors introduce a technique called Geometry Forcing, which helps these models learn geometric representations by aligning their intermediate features with those from a pretrained geometric foundation model. They propose two alignment objectives: Angular Alignment, which ensures directional consistency, and Scale Alignment, which maintains scale information. The results show that Geometry Forcing significantly enhances the visual quality and 3D consistency of generated videos compared to existing methods.'}, 'zh': {'title': '提升视频模型的几何感知能力', 'desc': '本论文提出了一种名为几何强制（Geometry Forcing）的方法，旨在改善视频扩散模型在学习表示时对三维几何结构的捕捉能力。我们发现，仅使用原始视频数据训练的模型往往无法有效捕捉到有意义的几何信息。通过将模型的中间表示与预训练的几何基础模型的特征对齐，我们引入了两个互补的对齐目标：角度对齐和尺度对齐，以增强模型的几何感知能力。实验结果表明，几何强制方法在视频生成任务中显著提高了视觉质量和三维一致性。'}}}, {'id': 'https://huggingface.co/papers/2507.07136', 'title': 'LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS', 'url': 'https://huggingface.co/papers/2507.07136', 'abstract': 'LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 times speedup and a 47 times boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.', 'score': 25, 'issue_id': 4763, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'f35bfc7d12aabb90', 'authors': ['Wanhua Li', 'Yujie Zhao', 'Minghan Qin', 'Yang Liu', 'Yuanhao Cai', 'Chuang Gan', 'Hanspeter Pfister'], 'affiliations': ['Harvard University', 'Johns Hopkins University', 'MIT-IBM Watson AI Lab', 'Tsinghua University', 'UMass Amherst', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.07136.jpg', 'data': {'categories': ['#3d', '#optimization', '#inference', '#data'], 'emoji': '🚀', 'ru': {'title': 'LangSplatV2: Революция в скорости и точности 3D текстовых запросов', 'desc': 'LangSplatV2 - это усовершенствованная версия системы для трехмерных текстовых запросов. Она заменяет тяжеловесный декодер на разреженное поле коэффициентов и использует оптимизацию CUDA. Это позволяет достичь скорости 476.2 кадров в секунду для сплаттинга высокоразмерных признаков и 384.6 кадров в секунду для текстовых запросов в трехмерном пространстве. LangSplatV2 демонстрирует значительное ускорение и повышение точности по сравнению с предыдущей версией.'}, 'en': {'title': 'Speeding Up 3D Text Querying with LangSplatV2', 'desc': 'LangSplatV2 is a machine learning model that improves the speed and accuracy of 3D text querying by replacing the traditional heavyweight decoder with a more efficient sparse coefficient field. It achieves impressive performance metrics, processing high-dimensional features at 476.2 frames per second (FPS) and 3D text queries at 384.6 FPS, marking a significant speedup compared to its predecessor, LangSplat. The model utilizes Gaussian Splatting to effectively integrate 2D language features into a 3D context, enhancing the precision of language interactions in complex scenes. Despite these advancements, LangSplatV2 still struggles to reach real-time inference speeds, which limits its practical applications in dynamic environments.'}, 'zh': {'title': 'LangSplatV2：提升 3D 文本查询速度与准确性', 'desc': 'LangSplatV2 是一种新型的 3D 文本查询方法，通过用稀疏系数场替代传统的重型解码器，显著提高了查询速度和准确性。该方法在高分辨率图像上实现了每秒 476.2 帧的高维特征喷溅和每秒 384.6 帧的 3D 开放词汇文本查询，速度提升达 42 倍，准确性也有显著提高。LangSplatV2 利用高斯喷溅技术将 2D CLIP 语言特征嵌入 3D，学习精确的 3D 语言场，适用于复杂场景中的语言交互应用。尽管 LangSplatV2 显著提升了性能，但在实时推理方面仍有待改进。'}}}, {'id': 'https://huggingface.co/papers/2507.07996', 'title': 'Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs', 'url': 'https://huggingface.co/papers/2507.07996', 'abstract': 'A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For >60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation.', 'score': 23, 'issue_id': 4765, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'c861c1e4c9288d18', 'authors': ['Ziyue Li', 'Yang Li', 'Tianyi Zhou'], 'affiliations': ['Department of Computer Science, University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2507.07996.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Динамическая адаптация архитектуры нейросети для каждой задачи', 'desc': 'Метод цепочки слоев (CoLa), основанный на предобученной большой языковой модели, позволяет динамически адаптировать архитектуру, повышая эффективность и точность для различных задач. Это достигается путем выборочной манипуляции слоями и оптимизации с помощью поиска Монте-Карло по дереву. CoLa позволяет пропускать, повторять или переупорядочивать слои, создавая оптимальную архитектуру для каждого входного образца. Анализ показал, что для большинства образцов можно найти более короткие или более точные архитектуры по сравнению с исходной моделью.'}, 'en': {'title': 'Dynamic Layer Adaptation for Enhanced Model Efficiency', 'desc': 'This paper introduces a novel method called chain-of-layers (CoLa) that utilizes a pretrained large language model (LLM) to dynamically adapt its architecture for various tasks. By manipulating individual layers, the model can skip, repeat, or rearrange them, allowing for a more efficient and tailored approach to processing different inputs. The authors employ Monte Carlo Tree Search (MCTS) to optimize the selection of layers for each sample, enhancing both accuracy and efficiency. The findings demonstrate that CoLa can significantly improve performance and reduce inference time compared to traditional static models, highlighting the potential for adaptive architectures in machine learning.'}, 'zh': {'title': '动态架构适应，提升模型效率与准确性', 'desc': '本文提出了一种基于预训练大语言模型的层链（CoLa）方法，能够动态调整模型架构以提高效率和准确性。通过选择性地操作层和使用蒙特卡洛树搜索优化，CoLa可以为每个测试样本构建更优的模型。研究表明，预训练模型的层可以作为独立模块进行操作，从而实现更灵活的架构适应不同输入。我们的实验结果显示，CoLa在提高推理效率和性能方面具有显著优势，尤其是在处理不同样本时。'}}}, {'id': 'https://huggingface.co/papers/2507.07202', 'title': 'A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality', 'url': 'https://huggingface.co/papers/2507.07202', 'abstract': 'Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled "long-form videos". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.', 'score': 21, 'issue_id': 4762, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'c7dc5888e8a06c13', 'authors': ['Mohamed Elmoghany', 'Ryan Rossi', 'Seunghyun Yoon', 'Subhojyoti Mukherjee', 'Eslam Bakr', 'Puneet Mathur', 'Gang Wu', 'Viet Dac Lai', 'Nedim Lipka', 'Ruiyi Zhang', 'Varun Manjunatha', 'Chien Nguyen', 'Daksh Dangi', 'Abel Salinas', 'Mohammad Taesiri', 'Hongjie Chen', 'Xiaolei Huang', 'Joe Barrow', 'Nesreen Ahmed', 'Hoda Eldardiry', 'Namyong Park', 'Yu Wang', 'Jaemin Cho', 'Anh Totti Nguyen', 'Zhengzhong Tu', 'Thien Nguyen', 'Dinesh Manocha', 'Mohamed Elhoseiny', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research', 'Auburn University', 'Cisco', 'Dolby Labs', 'Independent Researcher', 'KAUST', 'Meta AI', 'Pattern Data', 'Texas A&M University', 'UNC Chapel Hill', 'University of Maryland, College Park', 'University of Memphis', 'University of Oregon', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2507.07202.jpg', 'data': {'categories': ['#survey', '#architecture', '#video', '#training'], 'emoji': '🎬', 'ru': {'title': 'Прорыв в генерации длинных видео: анализ ключевых компонентов и стратегий', 'desc': 'Эта статья посвящена анализу современных методов генерации видео с помощью машинного обучения. Авторы изучили 32 научные работы, чтобы выявить ключевые архитектурные компоненты и стратегии обучения, позволяющие создавать длительные видео с несколькими персонажами и связным сюжетом. В статье отмечается, что существующие модели генеративного ИИ способны создавать видео длительностью только 5-16 секунд, при этом более длинные видео страдают от проблем с согласованностью персонажей и сцен. Авторы представляют новую таксономию существующих методов и сравнительные таблицы, классифицирующие работы по их архитектурным особенностям и характеристикам производительности.'}, 'en': {'title': 'Unlocking the Future of Long-Form Video Generation', 'desc': 'This paper reviews the current state of video generative models, highlighting their limitations in producing long-form videos that exceed 16 seconds. It identifies issues such as character consistency and motion coherence, particularly in videos featuring multiple subjects. The authors analyze 32 existing studies to pinpoint effective architectural components and training strategies that enhance video quality. Additionally, they propose a new taxonomy to classify these methods based on their designs and performance metrics, aiming to guide future research in this area.'}, 'zh': {'title': '提升视频生成的连贯性与多样性', 'desc': '尽管视频生成模型取得了显著进展，但现有的最先进方法只能生成持续5到16秒的视频，通常被称为“长视频”。超过16秒的视频在角色外观和场景布局的一致性方面存在困难，尤其是多角色长视频在角色一致性和运动连贯性方面仍然存在问题。虽然一些方法可以生成长达150秒的视频，但它们往往面临帧冗余和时间多样性不足的问题。我们对32篇视频生成论文进行了全面研究，识别出关键的架构组件和训练策略，并构建了一个新的现有方法分类法。'}}}, {'id': 'https://huggingface.co/papers/2507.06543', 'title': 'Token Bottleneck: One Token to Remember Dynamics', 'url': 'https://huggingface.co/papers/2507.06543', 'abstract': 'ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.', 'score': 16, 'issue_id': 4765, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '5dcb21845afc4bb6', 'authors': ['Taekyung Kim', 'Dongyoon Han', 'Byeongho Heo', 'Jeongeun Park', 'Sangdoo Yun'], 'affiliations': ['Korea University', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.06543.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#cv', '#robotics', '#video'], 'emoji': '🤖', 'ru': {'title': 'ToBo: Компактные временные представления для динамического зрения', 'desc': 'ToBo - это метод самоконтролируемого обучения для создания компактных визуальных представлений с учетом временных зависимостей. Он применяется для задач последовательного понимания сцен, таких как визуальное отслеживание и роботизированные манипуляции. ToBo сжимает сцену в компактный токен и предсказывает последующую сцену, используя минимальные фрагменты в качестве подсказок. Метод превосходит базовые подходы как в симулированных, так и в реальных средах.'}, 'en': {'title': 'Compact and Temporal: Revolutionizing Scene Understanding with ToBo', 'desc': 'ToBo is a self-supervised learning method that focuses on creating compact visual representations for understanding dynamic scenes. It works by encoding a scene into a small bottleneck token and then predicting the next scene using minimal visual hints. This approach helps the model learn temporal relationships between scenes, which is crucial for tasks like visual tracking and robotic manipulation. The effectiveness of ToBo is demonstrated through experiments in both simulated and real-world environments, showing its ability to outperform existing methods.'}, 'zh': {'title': 'ToBo：紧凑的时间感知视觉表示', 'desc': 'ToBo是一种自监督学习方法，旨在为顺序场景理解任务创建紧凑且具有时间感知的视觉表示。该方法通过将场景压缩为瓶颈标记，并利用最小的补丁作为提示来预测后续场景，从而提高了视觉跟踪和机器人操作等任务的性能。ToBo的设计鼓励模型捕捉时间动态，使其能够理解场景之间的动态过渡。大量实验表明，ToBo在模拟和真实环境中的表现优于基线方法，证明了其在实际应用中的有效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.07955', 'title': 'Dynamic Chunking for End-to-End Hierarchical Sequence Modeling', 'url': 'https://huggingface.co/papers/2507.07955', 'abstract': "Hierarchical networks replace traditional tokenization pipelines by dynamically learning segmentation strategies, achieving better performance and scalability across various languages and modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite incredible progress in language models (LMs) in recent years, largely resulting from moving away from specialized models designed for specific tasks to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. We introduce a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content -- and context -- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching a token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement in data efficiency over baselines), showing the potential of true end-to-end models that learn and scale better from unprocessed data.", 'score': 11, 'issue_id': 4779, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'e7521540f9f39040', 'authors': ['Sukjun Hwang', 'Brandon Wang', 'Albert Gu'], 'affiliations': ['Carnegie Mellon University', 'Cartesia AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.07955.jpg', 'data': {'categories': ['#architecture', '#data', '#transfer_learning', '#long_context', '#multilingual', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Иерархические нейросети: конец эры токенизации', 'desc': 'Статья представляет новый подход к обработке естественного языка, заменяющий традиционную токенизацию иерархическими нейронными сетями (H-Net). Эти сети динамически обучаются стратегиям сегментации текста, что позволяет достичь лучшей производительности и масштабируемости для различных языков и модальностей. H-Net превосходит сильные трансформерные языковые модели, работающие с токенами BPE, при сопоставимых вычислительных ресурсах и объемах данных. Особенно значительное улучшение наблюдается для языков и модальностей со слабыми эвристиками токенизации, таких как китайский язык, программный код и последовательности ДНК.'}, 'en': {'title': 'Dynamic Segmentation for Enhanced Language Modeling', 'desc': 'This paper presents Hierarchical Networks (H-Nets) that improve upon traditional tokenization methods by dynamically learning how to segment data based on its content and context. By integrating this dynamic chunking mechanism into a hierarchical structure, H-Nets can operate as a single end-to-end model, eliminating the need for separate tokenization and detokenization processes. The results show that H-Nets outperform conventional Transformer models, especially in languages and modalities where tokenization is less effective, such as Chinese and DNA sequences. This approach not only enhances performance but also increases data efficiency, demonstrating the advantages of learning directly from raw data without relying on predefined tokenization strategies.'}, 'zh': {'title': '层次网络：打破标记化的界限', 'desc': '这篇论文介绍了一种新的层次网络（H-Net），它通过动态学习分割策略来替代传统的标记化流程，从而在多种语言和模态中实现更好的性能和可扩展性。H-Net能够在没有显式标记化的情况下，自动学习内容和上下文相关的分割策略，并与模型的其余部分共同训练。研究表明，H-Net在字节级别的单层次结构上超越了强大的Transformer语言模型，并且通过多层次的迭代进一步提升了性能。特别是在标记化启发式较弱的语言（如中文）中，H-Net显示出显著的改进，展示了从未处理数据中学习和扩展的潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.07484', 'title': 'Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2507.07484', 'abstract': "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.  \t\t\t\t\tAI-generated summary \t\t\t\t Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.", 'score': 9, 'issue_id': 4763, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'c35c3791aec951d3', 'authors': ['Kaiqu Liang', 'Haimin Hu', 'Xuandong Zhao', 'Dawn Song', 'Thomas L. Griffiths', 'Jaime Fernández Fisac'], 'affiliations': ['Princeton University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.07484.jpg', 'data': {'categories': ['#alignment', '#ethics', '#benchmark', '#hallucinations', '#rlhf', '#training'], 'emoji': '🤖', 'ru': {'title': 'Проблема правдивости в LLM: анализ машинного буллшита', 'desc': 'В статье рассматривается концепция "машинного буллшита", когда LLMs генерируют утверждения без учета их истинности. Исследователи вводят новый индекс буллшита, чтобы количественно оценить безразличие LLMs к правде, и предлагают таксономию, анализирующую четыре формы буллшита: пустая риторика, увиливание, слова-лазейки и непроверенные утверждения. Эксперименты показывают, что обучение с подкреплением от обратной связи человека (RLHF) и использование цепочки рассуждений (CoT) усиливают некоторые формы буллшита. Результаты подчеркивают проблемы в согласовании AI и предлагают новые пути к более правдивому поведению LLM.'}, 'en': {'title': 'Quantifying Machine Bullshit: A New Framework for LLM Truthfulness', 'desc': "This paper introduces the concept of 'machine bullshit' to describe how large language models (LLMs) can generate statements without regard for their truthfulness. It presents a new framework that includes the Bullshit Index, a metric designed to quantify this indifference to truth. The authors analyze four types of machine bullshit: empty rhetoric, paltering, weasel words, and unverified claims, and evaluate these through various datasets. The findings indicate that techniques like reinforcement learning from human feedback (RLHF) and chain-of-thought prompting can worsen the generation of certain types of bullshit, particularly in political contexts."}, 'zh': {'title': '揭示机器胡说的真相', 'desc': '本文提出了一个新的框架来量化和分析大型语言模型（LLM）在生成内容时对真相的漠视，称之为“机器胡说”。我们引入了“胡说指数”，这是一个新的指标，用于量化LLM对真相的无动于衷，并分析了四种胡说的定性形式：空洞修辞、模棱两可、狡猾用词和未经验证的声明。研究表明，使用人类反馈的强化学习（RLHF）会显著加剧胡说现象，而推理时的思维链（CoT）提示则特别放大了空洞修辞和模棱两可的表现。我们的发现揭示了AI对齐中的系统性挑战，并为实现更真实的LLM行为提供了新的见解。'}}}, {'id': 'https://huggingface.co/papers/2507.07574', 'title': 'Beyond the Linear Separability Ceiling', 'url': 'https://huggingface.co/papers/2507.07574', 'abstract': 'The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this "linear reasoning bottleneck" by introducing the Linear Separability Ceiling (LSC), the performance of a simple linear classifier on a VLM\'s visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language model\'s reasoning pathways. We demonstrate this is a solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as a methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides a new lens for VLM analysis, showing that robust reasoning is a matter of targeted alignment, not simply improved representation learning.', 'score': 4, 'issue_id': 4767, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': 'facb529d60e86e84', 'authors': ['Enrico Vompa', 'Tanel Tammet', 'Mohit Vaishnav'], 'affiliations': ['Applied Artificial Intelligence Group, Tallinn University of Technology, Estonia'], 'pdf_title_img': 'assets/pdf/title_img/2507.07574.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#training', '#cv', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Преодоление линейного барьера в рассуждениях визуально-языковых моделей', 'desc': "Исследование выявляет линейное ограничение в рассуждениях визуально-языковых моделей (VLM) и предлагает метрику 'Потолок линейной разделимости' (LSC) для его оценки. Авторы обнаружили, что это ограничение широко распространено и связано не с плохим восприятием, а с недостатками в путях рассуждений языковой модели. Решение проблемы требует целенаправленной настройки, а не просто улучшения качества представлений. Исследование показывает, что VLM содержат мощные, но неактивные пути рассуждений, которые можно активировать для семантических концепций, в то время как сложные реляционные рассуждения требуют адаптации основных весов модели."}, 'en': {'title': 'Unlocking Reasoning in Visual-Language Models', 'desc': "This paper explores a limitation in Visual-Language Models (VLMs) known as the linear reasoning bottleneck, which affects their performance on abstract reasoning tasks. The authors introduce the Linear Separability Ceiling (LSC) as a metric to evaluate how well a simple linear classifier can perform on the visual embeddings produced by VLMs. They find that the bottleneck arises not from the models' perception abilities but from issues in the reasoning pathways of the language model. The study suggests that improving alignment, rather than just enhancing representation learning, is key to overcoming this bottleneck, with different strategies needed for various types of reasoning tasks."}, 'zh': {'title': '解决视觉语言模型的线性推理瓶颈', 'desc': '本研究发现视觉语言模型（VLMs）在抽象推理任务中存在线性推理瓶颈，并提出了线性可分性上限（Linear Separability Ceiling，LSC）作为评估该瓶颈的指标。研究表明，这一瓶颈普遍存在，主要源于语言模型推理路径的失败，而非感知能力的不足。解决这一问题需要针对性地进行对齐，而不是单纯提高表示学习的质量。对于语义概念，激活现有路径即可，而复杂的关系推理则需要调整核心模型权重。'}}}, {'id': 'https://huggingface.co/papers/2507.05241', 'title': "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?", 'url': 'https://huggingface.co/papers/2507.05241', 'abstract': "The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.", 'score': 3, 'issue_id': 4765, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '6f1b7fc6f47b4db5', 'authors': ['Jingyi Chai', 'Shuo Tang', 'Rui Ye', 'Yuwen Du', 'Xinyu Zhu', 'Mengcheng Zhou', 'Yanfeng Wang', 'Weinan E', 'Yuzhi Zhang', 'Linfeng Zhang', 'Siheng Chen'], 'affiliations': ['DP Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05241.jpg', 'data': {'categories': ['#agents', '#agi', '#training', '#science', '#reasoning', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'X-Master: ИИ-агент нового поколения для научных открытий', 'desc': "Статья представляет X-Master - агента искусственного интеллекта, способного эмулировать работу исследователей-людей. X-Master использует внешние инструменты и библиотеки Python для улучшения процесса рассуждений. Авторы также разработали X-Masters - систему, объединяющую несколько агентов для повышения широты и глубины анализа. X-Masters достиг рекордного результата в 32.1% в тесте Humanity's Last Exam, превзойдя предыдущие достижения OpenAI и Google."}, 'en': {'title': 'Empowering AI for Scientific Breakthroughs with X-Master', 'desc': "This paper discusses the development of a new AI agent called X-Master, which is designed to enhance scientific discovery by mimicking human researchers. X-Master utilizes external tools and Python libraries to improve its reasoning capabilities, allowing it to tackle complex tasks more effectively. The authors introduce a novel workflow called X-Masters, which enhances the agent's reasoning breadth and depth. Their approach has achieved a new record score of 32.1% on the Humanity's Last Exam, outperforming previous benchmarks set by other leading AI models."}, 'zh': {'title': '利用AI加速科学发现的新时代', 'desc': '本研究探讨了如何利用人工智能代理加速科学发现，提出了人类最后考试（HLE）作为评估科学AI代理的标准。我们构建了通用代理的基础架构，并通过X-Master工具增强推理能力，模拟人类研究者的灵活性。X-Master能够与外部工具互动，利用内置的Python库和定制工具来增强推理过程。我们的开源解决方案X-Masters在HLE上取得了32.1%的新纪录，超越了OpenAI和谷歌的深度研究，首次突破30%的门槛。'}}}, {'id': 'https://huggingface.co/papers/2507.07867', 'title': 'Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders', 'url': 'https://huggingface.co/papers/2507.07867', 'abstract': 'A Re-Bottleneck framework modifies pre-trained autoencoders to enforce specific latent structures, improving performance in diverse downstream applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a "Re-Bottleneck", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework\'s effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training.', 'score': 2, 'issue_id': 4774, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '72ee0206287ff7a0', 'authors': ['Dimitrios Bralios', 'Jonah Casebeer', 'Paris Smaragdis'], 'affiliations': ['Adobe Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.07867.jpg', 'data': {'categories': ['#audio', '#training', '#optimization'], 'emoji': '🎛️', 'ru': {'title': 'Гибкая настройка латентного пространства аудиомоделей', 'desc': "Предложен новый подход 'Re-Bottleneck' для модификации предобученных автоэнкодеров с целью улучшения структуры скрытого пространства. Метод вводит внутреннее сжатие, обучаемое исключительно на основе потерь в латентном пространстве. Эксперименты показали эффективность подхода для упорядочивания латентных каналов, выравнивания с семантическими эмбеддингами и введения эквивариантности. Фреймворк позволяет гибко адаптировать представления нейронных аудиомоделей под различные задачи с минимальным дообучением."}, 'en': {'title': 'Enhancing Autoencoders with Re-Bottleneck for Tailored Latent Structures', 'desc': "This paper presents a new framework called 'Re-Bottleneck' that modifies pre-trained autoencoders to enhance their latent structures for better performance in various applications. Traditional autoencoders focus on reconstructing input data accurately but often overlook the specific latent structures needed for different tasks. The Re-Bottleneck approach introduces an additional bottleneck that is trained to optimize latent space representations based on user-defined criteria. Through experiments, the authors show that this method can enforce order in latent channels, align them with semantic meanings, and ensure that changes in input lead to predictable transformations in the latent space."}, 'zh': {'title': 'Re-Bottleneck框架：优化潜在结构以提升性能', 'desc': '本文提出了一种名为“Re-Bottleneck”的框架，旨在通过修改预训练自编码器的瓶颈部分，来增强潜在结构的特定性。这种方法通过在潜在空间损失上进行训练，来引入用户定义的结构，从而提高在不同下游应用中的性能。我们在三个实验中验证了该框架的有效性，包括对潜在通道的排序、与语义嵌入的对齐以及引入等变性。最终，Re-Bottleneck框架为神经音频模型的表示提供了一种灵活高效的定制方式，能够满足不同应用的多样化需求。'}}}, {'id': 'https://huggingface.co/papers/2507.07129', 'title': 'Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate', 'url': 'https://huggingface.co/papers/2507.07129', 'abstract': 'A novel approach to scaling large language models through modular composition and layer-wise growth using fixed embeddings enhances performance and flexibility.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing paradigm for scaling large language models (LLMs) involves monolithic, end-to-end training, a resource-intensive process that lacks flexibility. This paper explores an alternative, constructive approach to model development, built upon the foundation of non-trainable, deterministic input embeddings. In prior [1], we established that high-level semantic reasoning can emerge in Transformers using frozen embeddings derived from the visual structure of Unicode glyphs. Here, we demonstrate that this fixed representational substrate acts as a universal "docking port," enabling two powerful and efficient scaling paradigms: seamless modular composition and progressive layer-wise growth.   First, we show that specialist models trained on disparate datasets (e.g., Russian and Chinese text) can be merged into a single, more capable Mixture-of-Experts (MoE) model, post-training, with zero architectural modification. This is achieved by simply averaging their output logits. The resulting MoE model exhibits immediate performance improvements on reasoning benchmarks like MMLU, surpassing its constituent experts without catastrophic forgetting. Second, we introduce a layer-wise constructive training methodology, where a deep Transformer is "grown" by progressively stacking and training one layer at a time. This method demonstrates stable convergence and a clear correlation between model depth and the emergence of complex reasoning abilities, such as those required for SQuAD.   Our findings suggest a paradigm shift from monolithic optimization towards a more biological or constructive model of AI development, where complexity is built incrementally and modules can be composed freely. This opens new avenues for resource-efficient scaling, continual learning, and a more democratized ecosystem for building powerful AI systems. We release all code and models to facilitate further research.', 'score': 2, 'issue_id': 4773, 'pub_date': '2025-07-08', 'pub_date_card': {'ru': '8 июля', 'en': 'July 8', 'zh': '7月8日'}, 'hash': 'a5414a289d5b5913', 'authors': ['A. Bochkov'], 'affiliations': ['Moscow Institute of Physics and Technology (MIPT), Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.07129.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#optimization', '#training', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Конструктор ИИ: сборка языковых моделей по кирпичикам', 'desc': "Статья предлагает новый подход к масштабированию больших языковых моделей (LLM) с использованием модульной композиции и послойного роста на основе фиксированных эмбеддингов. Авторы демонстрируют, что специализированные модели, обученные на разных наборах данных, могут быть объединены в единую модель Mixture-of-Experts без архитектурных изменений. Также представлена методология послойного конструктивного обучения, где глубокий трансформер 'выращивается' путем последовательного добавления и обучения слоев. Результаты указывают на возможность перехода от монолитной оптимизации к более биологичной или конструктивной модели развития ИИ."}, 'en': {'title': 'Modular Growth: A New Era for Language Models', 'desc': 'This paper presents a new method for scaling large language models (LLMs) that focuses on modular composition and layer-wise growth instead of traditional end-to-end training. By using fixed, non-trainable embeddings, the authors show that different specialized models can be combined into a more powerful Mixture-of-Experts (MoE) model without changing their architecture. Additionally, they introduce a technique for gradually adding layers to a Transformer model, which leads to better performance and stability during training. This approach promotes a more flexible and efficient way to develop AI systems, allowing for continual learning and easier integration of new capabilities.'}, 'zh': {'title': '模块化组合与逐层增长：大型语言模型的新方法', 'desc': '本文提出了一种新方法，通过模块化组合和逐层增长来扩展大型语言模型（LLMs），使用固定的嵌入增强了模型的性能和灵活性。与传统的整体训练方法不同，这种方法利用不可训练的确定性输入嵌入，允许在不修改架构的情况下将不同数据集训练的专家模型合并为一个更强大的混合专家模型。我们还介绍了一种逐层构建的训练方法，通过逐步堆叠和训练每一层，展示了模型深度与复杂推理能力之间的明确关联。我们的研究表明，AI开发可以从单一优化转向更具生物学特征的构建模型，促进资源高效扩展和持续学习。'}}}, {'id': 'https://huggingface.co/papers/2507.04886', 'title': 'Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations', 'url': 'https://huggingface.co/papers/2507.04886', 'abstract': 'Transformer models equipped with fixed, visually derived embeddings outperform those with trainable embeddings on a reasoning benchmark, challenging the traditional role of embeddings in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational "meaning vectors." This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to "representational interference" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer\'s compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research.', 'score': 2, 'issue_id': 4773, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'fc80c81b93b3402d', 'authors': ['A. Bochkov'], 'affiliations': ['Moscow Institute of Physics and Technology (MIPT), Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.04886.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление роли эмбеддингов в языковых моделях', 'desc': 'Исследование показывает, что модели трансформеров с фиксированными визуальными эмбеддингами превосходят модели с обучаемыми эмбеддингами в задачах рассуждения. Авторы использовали замороженный слой эмбеддингов, основанный на визуальной структуре символов Unicode. Несмотря на отсутствие обучаемых семантических эмбеддингов, модели сходились и генерировали связный текст. Результаты указывают на то, что высокоуровневая семантика является emergent-свойством архитектуры трансформера, а не содержится в самих эмбеддингах.'}, 'en': {'title': 'Rethinking Embeddings: Structure Over Semantics in Transformers', 'desc': "This paper investigates the role of embeddings in large language models (LLMs) by using fixed, visually derived embeddings instead of trainable ones. The authors demonstrate that their Transformer models, which utilize precomputed visual embeddings from Unicode glyphs, outperform traditional models with trainable embeddings on reasoning tasks. They argue that the conventional view of embeddings as essential meaning vectors is flawed, as high-level semantics emerge from the Transformer's architecture and the scale of data rather than from the embeddings themselves. This research suggests a shift in understanding embeddings as structural components rather than semantic containers, paving the way for new approaches in model design."}, 'zh': {'title': '嵌入的角色重塑：从语义容器到结构原语', 'desc': '本论文探讨了在大型语言模型（LLMs）中，嵌入层的传统角色。研究表明，使用固定的视觉派生嵌入的Transformer模型在推理基准测试中表现优于使用可训练嵌入的模型。我们提出的模型在训练过程中嵌入层保持不变，使用来自Unicode字形的视觉结构向量，而非数据生成的向量。结果表明，高级语义并非嵌入的固有特性，而是Transformer的组合架构和数据规模的涌现属性。'}}}, {'id': 'https://huggingface.co/papers/2507.03724', 'title': 'MemOS: A Memory OS for AI System', 'url': 'https://huggingface.co/papers/2507.03724', 'abstract': 'MemOS is proposed as a memory operating system for Large Language Models to enhance memory management, enabling efficient storage and retrieval, and facilitating continual learning and personalized modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.', 'score': 77, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': '5a64c779be945671', 'authors': ['Zhiyu Li', 'Shichao Song', 'Chenyang Xi', 'Hanyu Wang', 'Chen Tang', 'Simin Niu', 'Ding Chen', 'Jiawei Yang', 'Chunyu Li', 'Qingchen Yu', 'Jihao Zhao', 'Yezhaohui Wang', 'Peng Liu', 'Zehao Lin', 'Pengyuan Wang', 'Jiahao Huo', 'Tianyi Chen', 'Kai Chen', 'Kehang Li', 'Zhen Tao', 'Junpeng Ren', 'Huayi Lai', 'Hao Wu', 'Bo Tang', 'Zhenren Wang', 'Zhaoxin Fan', 'Ningyu Zhang', 'Linfeng Zhang', 'Junchi Yan', 'Mingchuan Yang', 'Tong Xu', 'Wei Xu', 'Huajun Chen', 'Haofeng Wang', 'Hongkang Yang', 'Wentao Zhang', 'Zhi-Qin John Xu', 'Siheng Chen', 'Feiyu Xiong'], 'affiliations': ['Beihang University', 'Institute for Advanced Algorithms Research, Shanghai', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Peking University', 'Renmin University of China', 'Research Institute of China Telecom', 'Shanghai Jiao Tong University', 'Tongji University', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.03724.jpg', 'data': {'categories': ['#training', '#agi', '#rag', '#long_context', '#optimization', '#data'], 'emoji': '🧠', 'ru': {'title': 'MemOS: операционная система памяти для более умных и адаптивных языковых моделей', 'desc': 'MemOS - это операционная система памяти для больших языковых моделей (LLM), предложенная для улучшения управления памятью. Она позволяет эффективно хранить и извлекать информацию, а также способствует непрерывному обучению и персонализированному моделированию. MemOS вводит концепцию MemCube как базовой единицы памяти, содержащей как контент, так и метаданные. Система объединяет представление, планирование и эволюцию различных типов памяти, обеспечивая гибкость и эффективность работы LLM.'}, 'en': {'title': 'MemOS: Revolutionizing Memory Management for LLMs', 'desc': 'MemOS is a proposed memory operating system designed to improve memory management in Large Language Models (LLMs). It addresses the limitations of existing models that rely on static parameters and short-term context by introducing a structured memory layer that enhances storage and retrieval capabilities. The system utilizes MemCubes, which encapsulate memory content along with metadata, allowing for flexible transitions between different memory types. This approach not only increases computational efficiency but also supports continual learning and personalized modeling by managing knowledge across various temporal scales.'}, 'zh': {'title': 'MemOS：为大型语言模型提供智能内存管理', 'desc': 'MemOS是一种为大型语言模型（LLMs）设计的内存操作系统，旨在改善内存管理。它通过统一表示、调度和演变不同类型的内存，支持高效的存储和检索。MemOS引入了MemCube作为基本单元，封装了内存内容和元数据，允许灵活的内存类型转换。该系统为LLMs提供了可控性、可塑性和可演化性，促进了持续学习和个性化建模。'}}}, {'id': 'https://huggingface.co/papers/2507.00994', 'title': 'Should We Still Pretrain Encoders with Masked Language Modeling?', 'url': 'https://huggingface.co/papers/2507.00994', 'abstract': 'Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.', 'score': 56, 'issue_id': 4700, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': '3445ad02ac25de31', 'authors': ['Hippolyte Gisserot-Boukhlef', 'Nicolas Boizard', 'Manuel Faysse', 'Duarte M. Alves', 'Emmanuel Malherbe', 'André F. T. Martins', 'Céline Hudelot', 'Pierre Colombo'], 'affiliations': ['Artefact Research Center', 'Diabolocom', 'Equall', 'Illuin Technology', 'Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicações', 'MICS, CentraleSupélec, Université Paris-Saclay', 'Unbabel'], 'pdf_title_img': 'assets/pdf/title_img/2507.00994.jpg', 'data': {'categories': ['#survey', '#optimization', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация предобучения энкодеров: синергия CLM и MLM', 'desc': 'Исследование сравнивает эффективность методов предобучения энкодеров на основе маскированного языкового моделирования (MLM) и каузального языкового моделирования (CLM). Авторы проводят масштабные эксперименты с моделями разного размера, оценивая их производительность на задачах представления текста. Результаты показывают, что MLM в целом дает лучшие результаты, но CLM более эффективен с точки зрения данных и стабильности дообучения. Предлагается двухфазная стратегия обучения, сочетающая CLM и MLM, которая достигает оптимальной производительности при фиксированном вычислительном бюджете.'}, 'en': {'title': 'Unlocking Text Representation: CLM Meets MLM for Optimal Performance', 'desc': 'This paper investigates the effectiveness of Causal Language Modeling (CLM) compared to traditional Masked Language Modeling (MLM) for training text representations in natural language processing (NLP). The authors conduct extensive experiments with various model sizes and training strategies, revealing that while MLM generally performs better, CLM models are more efficient with data and offer greater stability during fine-tuning. They propose a biphasic training approach that first uses CLM and then MLM, which optimizes performance within a limited computational budget. Additionally, leveraging existing pretrained CLM models can significantly reduce the resources needed to achieve high-quality encoder models.'}, 'zh': {'title': '双相训练策略：CLM与MLM的最佳结合', 'desc': '本文探讨了文本表示学习在自然语言处理中的重要性。研究表明，使用因果语言模型（CLM）预训练的解码器模型可以有效地作为编码器，且在文本表示基准测试中常常超越传统的编码器。尽管使用掩码语言模型（MLM）训练通常在文本表示任务中表现更好，但CLM训练的模型在数据效率和微调稳定性方面表现更佳。我们提出了一种双相训练策略，先应用CLM再应用MLM，在固定的计算预算下实现最佳性能，并且在使用现有的预训练CLM模型初始化时，进一步降低了训练顶级编码器模型的计算负担。'}}}, {'id': 'https://huggingface.co/papers/2507.05163', 'title': '4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous\n  Capture', 'url': 'https://huggingface.co/papers/2507.05163', 'abstract': 'A high-speed 4D capturing system using low FPS cameras with asynchronous capture and video-diffusion-based artifact correction enhances reconstruction quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.', 'score': 32, 'issue_id': 4693, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'e1f4c8e83495db53', 'authors': ['Yutian Chen', 'Shi Guo', 'Tianshuo Yang', 'Lihe Ding', 'Xiuyuan Yu', 'Jinwei Gu', 'Tianfan Xue'], 'affiliations': ['NVIDIA', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.05163.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Высокоскоростная 4D-съемка обычными камерами', 'desc': 'Предлагается система высокоскоростной 4D-съемки с использованием камер с низкой частотой кадров и асинхронным захватом. Система повышает эффективную частоту кадров до 100-200 FPS путем смещения времени начала съемки для разных камер. Для устранения артефактов, вызванных реконструкцией по малому числу ракурсов, применяется генеративная модель на основе видео-диффузии. Экспериментальные результаты показывают значительное улучшение качества высокоскоростной 4D-реконструкции по сравнению с синхронной съемкой.'}, 'en': {'title': 'Revolutionizing 4D Capture: High-Speed Reconstruction with Low FPS Cameras', 'desc': 'This paper presents a novel system for capturing high-speed 4D scenes using low frame rate cameras. It introduces an asynchronous capture technique that effectively increases the frame rate by staggering the start times of multiple cameras, achieving rates of 100-200 FPS from a base of 25 FPS. Additionally, the authors propose a video-diffusion-based generative model to correct artifacts in the sparse 4D reconstruction, ensuring better detail and temporal consistency. Experimental results show that this approach significantly improves the quality of high-speed 4D reconstructions compared to traditional synchronous methods.'}, 'zh': {'title': '低帧率相机实现高速度4D重建的创新方案', 'desc': '本研究提出了一种高速度的4D捕捉系统，利用低帧率相机进行异步捕捉和视频扩散基础的伪影修正，从而提高重建质量。传统的4D捕捉系统通常帧率低于30 FPS，直接从低帧率输入进行高速度运动的4D重建会导致不理想的结果。我们的方法通过异步捕捉方案，将相机的启动时间错开，提升了有效帧率，达到100-200 FPS的效果。处理方面，我们提出了一种基于视频扩散的生成模型，修复4D稀疏视图重建中产生的伪影，显著改善了重建的细节和时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2507.04447', 'title': 'DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive\n  World Knowledge', 'url': 'https://huggingface.co/papers/2507.04447', 'abstract': 'DreamVLA improves robot manipulation through a VLA framework that incorporates world knowledge, dynamic-region guidance, and a diffusion-based transformer to ensure clear, disentangled representations for action planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.', 'score': 29, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '8fbe1a8248768baa', 'authors': ['Wenyao Zhang', 'Hongsi Liu', 'Zekun Qi', 'Yunnan Wang', 'XinQiang Yu', 'Jiazhao Zhang', 'Runpei Dong', 'Jiawei He', 'He Wang', 'Zhizheng Zhang', 'Li Yi', 'Wenjun Zeng', 'Xin Jin'], 'affiliations': ['EIT', 'Galbot', 'PKU', 'SJTU', 'THU', 'UIUC', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2507.04447.jpg', 'data': {'categories': ['#robotics', '#reasoning', '#training', '#optimization', '#multimodal', '#diffusion', '#agents', '#games'], 'emoji': '🤖', 'ru': {'title': 'DreamVLA: Интеллектуальное планирование действий робота на основе комплексного анализа окружающего мира', 'desc': 'DreamVLA - это новая система управления роботами, использующая комплексный подход к прогнозированию и планированию действий. Она объединяет зрение, язык и действие (VLA) с прогнозированием знаний о мире для улучшения манипуляций робота. Система использует механизм внимания для разделения динамической, пространственной и семантической информации. DreamVLA применяет диффузионный трансформер для моделирования распределения будущих действий робота.'}, 'en': {'title': 'Enhancing Robot Manipulation with DreamVLA: Clear Action Planning through World Knowledge', 'desc': 'DreamVLA is a new framework designed to enhance robot manipulation by integrating world knowledge with a vision-language-action (VLA) model. It addresses the limitations of existing methods by providing a clear and organized representation of dynamic, spatial, and semantic information, which is crucial for effective action planning. The framework uses a dynamic-region-guided approach to predict world knowledge, allowing robots to form abstract reasoning chains before executing actions. Additionally, a diffusion-based transformer is employed to ensure that action representations remain distinct and free from interference during training, leading to improved performance in both real-world and simulated tasks.'}, 'zh': {'title': 'DreamVLA：提升机器人操作的智能框架', 'desc': 'DreamVLA 是一种新颖的视觉-语言-动作（VLA）框架，旨在通过整合全面的世界知识来改善机器人操作。它引入了动态区域引导的世界知识预测，结合空间和语义线索，为动作规划提供了紧凑而全面的表示。该框架通过块状结构注意机制，减少动态、空间和语义信息之间的干扰，确保每个表示的清晰和分离。此外，DreamVLA 采用扩散式变换器来建模未来动作的条件分布，从而提高了机器人任务的成功率。'}}}, {'id': 'https://huggingface.co/papers/2507.05197', 'title': 'Pre-Trained Policy Discriminators are General Reward Models', 'url': 'https://huggingface.co/papers/2507.05197', 'abstract': 'A scalable reward modeling method, Policy Discriminative Learning (POLAR), enhances reward model performance and generalizes robustly in reinforcement learning through policy comparison.  \t\t\t\t\tAI-generated summary \t\t\t\t We offer a novel perspective on reward modeling by formulating it as a policy discriminator, which quantifies the difference between two policies to generate a reward signal, guiding the training policy towards a target policy with desired behaviors. Based on this conceptual insight, we propose a scalable pre-training method named Policy Discriminative Learning (POLAR), which trains a reward model (RM) to discern identical policies and discriminate different ones. Unlike traditional reward modeling methods relying on absolute preferences, POLAR captures the relative difference between one policy and an arbitrary target policy, which is a scalable, high-level optimization objective suitable for modeling generic ranking relationships. Leveraging the POLAR pre-training paradigm, we present a series of RMs with parameter scales from 1.8B to 7B. Empirical results show that POLAR substantially outperforms traditional non-pre-trained methods, significantly enhancing RM performance. For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA baselines. POLAR also shows robust generalization capabilities in RLHF using Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36% to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover, scaling experiments reveal a clear power-law relationship between computation and performance, supported by linear correlation coefficients approaching 0.99. The impressive performance, strong generalization, and scaling properties suggest that POLAR is a promising direction for developing general and strong reward models.', 'score': 27, 'issue_id': 4694, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '88d62db0ed894120', 'authors': ['Shihan Dou', 'Shichun Liu', 'Yuming Yang', 'Yicheng Zou', 'Yunhua Zhou', 'Shuhao Xing', 'Chenhao Huang', 'Qiming Ge', 'Demin Song', 'Haijun Lv', 'Songyang Gao', 'Chengqi Lv', 'Enyu Zhou', 'Honglin Guo', 'Zhiheng Xi', 'Wenwei Zhang', 'Qipeng Guo', 'Qi Zhang', 'Xipeng Qiu', 'Xuanjing Huang', 'Tao Gui', 'Kai Chen'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.05197.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl'], 'emoji': '🎯', 'ru': {'title': 'POLAR: Революция в моделировании вознаграждений для обучения с подкреплением', 'desc': 'В статье представлен новый метод моделирования вознаграждений в обучении с подкреплением, названный Policy Discriminative Learning (POLAR). POLAR обучает модель вознаграждения различать идентичные политики и дискриминировать различные, что позволяет захватывать относительную разницу между политиками. Эмпирические результаты показывают, что POLAR значительно превосходит традиционные методы, улучшая точность предпочтений и производительность политик в различных задачах. Метод демонстрирует надежные возможности обобщения и масштабирования, что делает его перспективным направлением для разработки сильных и общих моделей вознаграждения.'}, 'en': {'title': 'POLAR: Revolutionizing Reward Modeling through Policy Comparison', 'desc': 'The paper introduces Policy Discriminative Learning (POLAR), a new method for reward modeling in reinforcement learning that focuses on comparing policies rather than relying on absolute preferences. By treating reward modeling as a policy discriminator, POLAR effectively generates reward signals that guide the training policy towards a target policy with desired behaviors. This approach allows for scalable pre-training of reward models, which can discern between similar and different policies, enhancing their performance significantly. Empirical results demonstrate that POLAR outperforms traditional methods, showing improved accuracy and robust generalization in various tasks.'}, 'zh': {'title': 'POLAR：提升奖励模型性能的新方法', 'desc': 'POLAR是一种可扩展的奖励建模方法，通过策略比较来增强奖励模型的性能并在强化学习中实现稳健的泛化。它将奖励建模视为策略鉴别器，量化两个策略之间的差异，以生成奖励信号，指导训练策略朝向具有期望行为的目标策略。与传统的绝对偏好方法不同，POLAR捕捉一个策略与任意目标策略之间的相对差异，适合于建模通用的排名关系。实验结果表明，POLAR显著优于传统的非预训练方法，提升了奖励模型的表现，展示了其在强化学习中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.03483', 'title': 'BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning\n  Dataset', 'url': 'https://huggingface.co/papers/2507.03483', 'abstract': "A large-scale dataset and verification tool are introduced for assessing and improving cross-disciplinary reasoning capabilities in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce BMMR, a large-scale bilingual, multimodal, multi-disciplinary reasoning dataset for the community to develop and evaluate large multimodal models (LMMs). BMMR comprises 110k college-level questions spanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice, fill-in-the-blank, and open-ended QA-and sourced from both print and digital media such as books, exams, and quizzes. All data are curated and filtered via a human-in-the-loop and scalable framework, and each instance is paired with a high-quality reasoning path. The dataset is organized into two parts: BMMR-Eval that comprises 20,458 high-quality instances to comprehensively assess LMMs' knowledge and reasoning across multiple disciplines in both Chinese and English; and BMMR-Train that contains 88,991 instances to support further research and development, extending the current focus on mathematical reasoning to diverse disciplines and domains. In addition, we propose the process-based multi-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained evaluation of reasoning paths. Extensive experiments on 24 models reveal that (i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom on BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs only on specific subjects; (iii) open-source models still trail their proprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap. Additionally, we conduct reasoning-chain analyses using BMMR-Verifier and other in-depth studies, uncovering the challenges LMMs currently face in multidisciplinary reasoning. We will release the data, and we hope our work can offer insights and contributions to the community.", 'score': 20, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': 'a916ca78a2bd6196', 'authors': ['Zhiheng Xi', 'Guanyu Li', 'Yutao Fan', 'Honglin Guo', 'Yufang Liu', 'Xiaoran Fan', 'Jiaqi Liu', 'Jingchao Ding', 'Wangmeng Zuo', 'Zhenfei Yin', 'Lei Bai', 'Tao Ji', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['East China Normal University', 'Fudan University', 'Harbin Institute of Technology', 'Oxford', 'Shanghai AI Laboratory', 'University of Sydney', 'Yimudata'], 'pdf_title_img': 'assets/pdf/title_img/2507.03483.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#dataset', '#benchmark', '#open_source', '#data'], 'emoji': '🧠', 'ru': {'title': 'Новый инструмент для оценки мультидисциплинарных рассуждений ИИ', 'desc': 'Статья представляет BMMR - масштабный двуязычный мультимодальный датасет для оценки рассуждений в различных дисциплинах. Он содержит 110 тысяч вопросов университетского уровня по 300 предметам, включая тесты, вопросы с открытым ответом и заполнение пропусков. Авторы также предлагают BMMR-Verifier для точной оценки цепочек рассуждений моделей. Эксперименты показывают, что даже современные модели оставляют значительный простор для улучшений в мультидисциплинарных рассуждениях.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with BMMR Dataset', 'desc': "This paper presents BMMR, a comprehensive dataset designed to enhance the reasoning abilities of large multimodal models (LMMs) across various disciplines. It includes 110,000 college-level questions from 300 subjects, formatted in multiple-choice, fill-in-the-blank, and open-ended styles, sourced from both print and digital media. The dataset is divided into BMMR-Eval for evaluation and BMMR-Train for training, with a focus on improving reasoning in diverse domains beyond just mathematics. Additionally, the authors introduce BMMR-Verifier, a tool for detailed assessment of reasoning paths, revealing significant gaps in current models' performance and highlighting the need for further research in multidisciplinary reasoning."}, 'zh': {'title': '推动多模态模型的跨学科推理能力', 'desc': '本文介绍了BMMR，一个大规模的双语、多模态、多学科推理数据集，旨在帮助开发和评估大型多模态模型（LMMs）。该数据集包含110,000个大学水平的问题，涵盖300个联合国教科文组织定义的学科，问题形式多样，包括选择题、填空题和开放式问答。数据经过人工筛选和过滤，并为每个实例配备高质量的推理路径，分为BMMR-Eval和BMMR-Train两部分，以支持多学科知识和推理的评估与研究。我们还提出了基于过程的多学科验证器（BMMR-Verifier），用于对推理路径进行准确和细致的评估。'}}}, {'id': 'https://huggingface.co/papers/2507.02029', 'title': 'RoboBrain 2.0 Technical Report', 'url': 'https://huggingface.co/papers/2507.02029', 'abstract': 'RoboBrain 2.0, a vision-language foundation model, achieves top performance in embodied tasks through its heterogeneous architecture and multi-stage training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.', 'score': 18, 'issue_id': 4698, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '5d72b9df64404714', 'authors': ['BAAI RoboBrain Team', 'Mingyu Cao', 'Huajie Tan', 'Yuheng Ji', 'Minglan Lin', 'Zhiyu Li', 'Zhou Cao', 'Pengwei Wang', 'Enshen Zhou', 'Yi Han', 'Yingbo Tang', 'Xiangqi Xu', 'Wei Guo', 'Yaoxu Lyu', 'Yijie Xu', 'Jiayu Shi', 'Mengfei Du', 'Cheng Chi', 'Mengdi Zhao', 'Xiaoshuai Hao', 'Junkai Zhao', 'Xiaojie Zhang', 'Sh/anyu Rong', 'Huaihai Lyu', 'Zhengliang Cai', 'Yankai Fu', 'Ning Chen', 'Bolun Zhang', 'Lingfeng Zhang', 'Shuyi Zhang', 'Xi Feng', 'Songjing Wang', 'Xiaodan Liu', 'Yance Jiao', 'Mengsi Lyu', 'Zhuo Chen', 'Chenrui He', 'Yulong Ao', 'Xue Sun', 'Zheqi He', 'Jingshu Zheng', 'Xi Yang', 'Donghai Shi', 'Kunchang Xie', 'Bochao Zhang', 'Shaokai Nie', 'Chunlei Men', 'Yonghua Lin', 'Zhongyuan Wang', 'Tiejun Huang', 'Shanghang Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.02029.jpg', 'data': {'categories': ['#agi', '#reasoning', '#architecture', '#benchmark', '#training', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'RoboBrain 2.0: Универсальный ИИ для воплощенных задач', 'desc': 'RoboBrain 2.0 - это новая модель искусственного интеллекта для воплощенных задач, объединяющая восприятие, рассуждение и планирование в физических средах. Модель имеет гетерогенную архитектуру с визуальным энкодером и языковой моделью, доступна в вариантах 7B и 32B параметров. RoboBrain 2.0 демонстрирует ведущие результаты на широком спектре пространственных и временных тестов, превосходя предыдущие открытые и проприетарные модели. Модель поддерживает ключевые возможности воплощенного ИИ, включая пространственное понимание и временное принятие решений.'}, 'en': {'title': 'RoboBrain 2.0: Unifying Vision and Language for Advanced Embodied AI', 'desc': 'RoboBrain 2.0 is a cutting-edge vision-language foundation model that integrates perception, reasoning, and planning for complex tasks in physical environments. It features a heterogeneous architecture with both a vision encoder and a language model, available in two sizes: a lightweight 7B model and a powerful 32B model. The 32B variant excels in various embodied reasoning tasks, outperforming previous models in spatial and temporal benchmarks. This model aims to enhance embodied AI research and facilitate the development of generalist embodied agents, with resources available for further exploration.'}, 'zh': {'title': 'RoboBrain 2.0：推动实体AI的未来', 'desc': 'RoboBrain 2.0 是一种视觉-语言基础模型，旨在统一感知、推理和规划，以应对复杂的实体任务。它有两个版本：轻量级的7B模型和全规模的32B模型，采用异构架构，结合了视觉编码器和语言模型。尽管体积小，RoboBrain 2.0 在多种实体推理任务中表现出色，特别是在空间和时间基准测试中，32B版本的性能领先于之前的开源和专有模型。该模型支持关键的现实世界实体AI能力，如空间理解和时间决策，推动了实体AI研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2507.03253', 'title': 'RefineX: Learning to Refine Pre-training Data at Scale from\n  Expert-Guided Programs', 'url': 'https://huggingface.co/papers/2507.03253', 'abstract': 'RefineX is a scalable framework for improving the quality of large language model pre-training data through programmatic editing, yielding better performance than alternative methods across various downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The foundational capabilities of large language models (LLMs) are deeply influenced by the quality of their pre-training corpora. However, enhancing data quality at scale remains a significant challenge, primarily due to the trade-off between refinement effectiveness and processing efficiency. While rule-based filtering remains the dominant paradigm, it typically operates at the document level and lacks the granularity needed to refine specific content within documents. Inspired by emerging work such as ProX, we propose RefineX, a novel framework for large-scale, surgical refinement of pre-training data through programmatic editing tasks. RefineX enables efficient and fine-grained data refinement while reliably preserving the diversity and naturalness of raw text. The core strength of RefineX lies in distilling high-quality, expert-guided end-to-end refinement results into minimal edit-based deletion programs. This high-precision distillation pipeline is used to train an efficient and reliable refine model that can systematically improve every instance in the corpus at scale. We evaluate RefineX across from-scratch pre-training at multiple model scales and find that it consistently outperforms models trained on raw, filtered, or alternatively refined data across diverse downstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on lighteval tasks, and achieves comparable performance using significantly fewer training tokens. Further analysis shows that RefineX reliably enhances text quality with both high efficiency and precision, outperforming prior approaches such as end-to-end generation and Prox-C. These results position RefineX as a scalable, effective, and reliable solution for optimizing pre-training data in modern LLM pipelines.', 'score': 14, 'issue_id': 4693, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': '6f3d1aa17a4188e7', 'authors': ['Baolong Bi', 'Shenghua Liu', 'Xingzhang Ren', 'Dayiheng Liu', 'Junyang Lin', 'Yiwei Wang', 'Lingrui Mei', 'Junfeng Fang', 'Jiafeng Guo', 'Xueqi Cheng'], 'affiliations': ['Alibaba Group', 'Institute of Computing Technology, Chinese Academy of Sciences', 'National University of Singapore', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2507.03253.jpg', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': '✂️', 'ru': {'title': 'RefineX: хирургическая точность в улучшении данных для ИИ', 'desc': 'RefineX - это масштабируемый фреймворк для улучшения качества данных предобучения больших языковых моделей путем программного редактирования. Он позволяет эффективно и точечно улучшать качество данных, сохраняя при этом разнообразие и естественность исходного текста. RefineX обучает модель уточнения, которая может систематически улучшать каждый экземпляр в корпусе в масштабе. Эксперименты показывают, что модели, обученные на данных, улучшенных с помощью RefineX, превосходят модели, обученные на необработанных, отфильтрованных или альтернативно улучшенных данных по различным задачам.'}, 'en': {'title': 'RefineX: Precision Editing for Superior Language Model Training', 'desc': 'RefineX is a new framework designed to enhance the quality of pre-training data for large language models (LLMs) through targeted programmatic editing. It addresses the challenge of improving data quality at scale by allowing for precise modifications rather than broad document-level changes. This method preserves the diversity and naturalness of the text while ensuring efficient processing. Evaluations show that models trained with RefineX consistently outperform those trained on raw or traditionally refined data across various tasks, demonstrating its effectiveness in optimizing pre-training data.'}, 'zh': {'title': 'RefineX：提升预训练数据质量的可扩展框架', 'desc': 'RefineX是一个可扩展的框架，旨在通过程序化编辑提高大型语言模型预训练数据的质量。该框架解决了数据质量提升与处理效率之间的权衡问题，能够进行高效且细致的数据精炼。RefineX通过最小化编辑的删除程序，提炼出高质量的专家指导的端到端精炼结果，从而系统性地改善语料库中的每个实例。实验表明，RefineX在多个下游任务中表现优于使用原始、过滤或其他精炼数据训练的模型。'}}}, {'id': 'https://huggingface.co/papers/2507.04009', 'title': 'Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM\n  Fine-Tuning Data from Unstructured Documents', 'url': 'https://huggingface.co/papers/2507.04009', 'abstract': 'A unified framework called Easy Dataset synthesizes fine-tuning data from unstructured documents using a GUI and LLMs, improving domain-specific performance of LLMs while maintaining general knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.', 'score': 13, 'issue_id': 4698, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 июля', 'en': 'July 5', 'zh': '7月5日'}, 'hash': 'fd1930ff40937fac', 'authors': ['Ziyang Miao', 'Qiyu Sun', 'Jingyuan Wang', 'Yuchen Gong', 'Yaowei Zheng', 'Shiqi Li', 'Richong Zhang'], 'affiliations': ['Independent Researcher', 'School of Computer Science and Engineering, Beihang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.04009.jpg', 'data': {'categories': ['#synthetic', '#data', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Синтез данных для дообучения LLM с помощью Easy Dataset', 'desc': 'Easy Dataset - это унифицированный фреймворк для синтеза данных для дообучения больших языковых моделей (LLM) из неструктурированных документов с помощью графического интерфейса. Он позволяет пользователям настраивать модели извлечения текста и стратегии разбиения для преобразования необработанных документов в связные текстовые фрагменты. Затем Easy Dataset использует подход к промптингу на основе персон для генерации разнообразных пар вопрос-ответ с помощью общедоступных LLM. Эксперименты показывают, что дообучение LLM на синтезированном наборе данных значительно улучшает производительность в конкретной предметной области, сохраняя при этом общие знания.'}, 'en': {'title': 'Transforming Unstructured Data into Domain-Specific Knowledge', 'desc': "The paper introduces Easy Dataset, a framework designed to create fine-tuning data from unstructured documents using a user-friendly graphical interface and large language models (LLMs). It addresses the challenge of obtaining high-quality domain-specific data by allowing users to configure text extraction and chunking methods easily. The framework employs a persona-driven prompting technique to generate varied question-answer pairs, enhancing the dataset's richness. Experiments demonstrate that fine-tuning LLMs with this synthesized data significantly boosts their performance in specific domains while retaining their general knowledge capabilities."}, 'zh': {'title': '轻松合成数据，提升模型表现', 'desc': 'Easy Dataset是一个统一框架，旨在从非结构化文档中合成微调数据，以提高大语言模型（LLMs）在特定领域的表现，同时保持其通用知识。该框架通过直观的图形用户界面（GUI）使用户能够轻松配置文本提取模型和分块策略，将原始文档转化为连贯的文本块。接着，Easy Dataset利用基于角色的提示方法，生成多样化的问题-答案对，使用公开可用的LLMs。实验结果表明，在合成数据集上微调LLMs显著提高了其在特定领域的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.03745', 'title': 'StreamDiT: Real-Time Streaming Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2507.03745', 'abstract': 'A streaming video generation model named StreamDiT, based on transformer-based diffusion models, enables real-time video generation with high content consistency and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href="https://cumulo-autumn.github.io/StreamDiT/">this https URL.</a>', 'score': 13, 'issue_id': 4696, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': '8ca0a425a1ddc352', 'authors': ['Akio Kodaira', 'Tingbo Hou', 'Ji Hou', 'Masayoshi Tomizuka', 'Yue Zhao'], 'affiliations': ['Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.03745.jpg', 'data': {'categories': ['#diffusion', '#video', '#games'], 'emoji': '🎬', 'ru': {'title': 'StreamDiT: Реальновременная генерация видео с помощью ИИ', 'desc': 'StreamDiT - это модель генерации потокового видео, основанная на трансформерных диффузионных моделях. Она позволяет генерировать видео в реальном времени с высокой согласованностью контента и визуальным качеством. Обучение StreamDiT основано на методе flow matching с использованием движущегося буфера и смешанного обучения с различными схемами разбиения буферизованных кадров. После дистилляции модель достигает производительности реального времени 16 кадров в секунду на одном GPU при разрешении 512p.'}, 'en': {'title': 'Real-Time Video Generation with StreamDiT', 'desc': 'StreamDiT is a novel streaming video generation model that leverages transformer-based diffusion techniques to create high-quality videos in real-time. Unlike traditional models that generate short clips offline, StreamDiT can produce continuous video streams, making it suitable for interactive applications. The model employs flow matching with a moving buffer and mixed training strategies to enhance both content consistency and visual fidelity. With 4 billion parameters, StreamDiT achieves impressive performance, generating videos at 16 frames per second while maintaining a resolution of 512p.'}, 'zh': {'title': '实时视频生成的新突破：StreamDiT', 'desc': '本文提出了一种名为StreamDiT的流媒体视频生成模型，基于变换器的扩散模型，能够实现实时视频生成，同时保持高内容一致性和视觉质量。通过引入移动缓冲区，StreamDiT的训练采用流匹配的方法，设计了不同分区方案的混合训练，以提升生成视频的质量。该模型使用了具有不同时间嵌入和窗口注意力的adaLN DiT架构，并通过多步蒸馏方法优化性能。最终，经过蒸馏的模型在单个GPU上以16帧每秒的速度实现了实时性能，能够生成512p分辨率的视频流。'}}}, {'id': 'https://huggingface.co/papers/2507.05108', 'title': 'Reviving Cultural Heritage: A Novel Approach for Comprehensive\n  Historical Document Restoration', 'url': 'https://huggingface.co/papers/2507.05108', 'abstract': "Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83\\% to 84.05\\%, with further enhancement to 94.25\\% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.", 'score': 10, 'issue_id': 4698, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '1fe680b6a13d9932', 'authors': ['Yuyi Zhang', 'Peirong Zhang', 'Zhenhua Yang', 'Pengyu Yan', 'Yongxin Shi', 'Pengwei Liu', 'Fengjun Guo', 'Lianwen Jin'], 'affiliations': ['INTSIG-SCUT Joint Lab on Document Analysis and Recognition', 'Intsig Information Co., Ltd.', 'SCUT-Zhuhai Institute of Modern Industrial Innovation', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2507.05108.jpg', 'data': {'categories': ['#architecture', '#cv', '#multimodal', '#dataset', '#data'], 'emoji': '📜', 'ru': {'title': 'AutoHDR: Возрождение истории через искусственный интеллект', 'desc': 'Эта статья представляет новый подход к автоматизированной реставрации исторических документов под названием AutoHDR. Авторы создали полностраничный набор данных FPHDR, содержащий реальные и синтетические изображения с различными уровнями повреждений. AutoHDR использует трехэтапный процесс, включающий локализацию повреждений с помощью OCR, предсказание контекста текста с использованием vision-language моделей и авторегрессивное восстановление внешнего вида. Метод значительно улучшает точность OCR для сильно поврежденных документов, достигая 84.05% без вмешательства человека и 94.25% при человеко-машинном взаимодействии.'}, 'en': {'title': 'Revolutionizing Historical Document Restoration with AutoHDR', 'desc': 'This paper introduces a new approach to restoring historical documents that have been damaged over time. It presents a dataset called FPHDR, which includes thousands of images with detailed annotations for various damage levels. The proposed method, AutoHDR, uses a three-stage process that combines damage detection, text prediction, and appearance restoration, mimicking the work of historians. The results show a significant improvement in OCR accuracy, demonstrating the effectiveness of this automated solution in preserving cultural heritage.'}, 'zh': {'title': '自动化历史文献修复的创新之路', 'desc': '历史文献是宝贵的文化遗产，但由于撕裂、水侵蚀和氧化等原因，经历了严重的退化。现有的历史文献修复方法主要集中在单一模态或有限规模的修复，无法满足实际需求。为了解决这个问题，我们提出了一个全页历史文献修复数据集（FPHDR）和一种新颖的自动化修复解决方案（AutoHDR）。AutoHDR通过三个阶段模拟历史学家的修复工作流程，显著提高了严重损坏文档的OCR准确率，推动了自动化历史文献修复的进步。'}}}, {'id': 'https://huggingface.co/papers/2507.02659', 'title': 'OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding', 'url': 'https://huggingface.co/papers/2507.02659', 'abstract': "OmniDraft, a unified framework, addresses cross-vocabulary mismatch and improves decoding speed by allowing a single draft model to interact dynamically with diverse target models in online settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the ``one drafter for all'' paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.", 'score': 10, 'issue_id': 4694, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '356734d41c5a5e65', 'authors': ['Ramchalam Kinattinkara Ramakrishnan', 'Zhaocong Yuan', 'Shaojie Zhuo', 'Chen Feng', 'Yicheng Lin', 'Chenzheng Su', 'Xiaopeng Zhang'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.02659.jpg', 'data': {'categories': ['#optimization', '#games', '#inference', '#training', '#reasoning', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Один черновик для всех: ускорение и адаптация языковых моделей', 'desc': 'OmniDraft - это унифицированная система для ускорения работы языковых моделей. Она решает проблему несоответствия словарей между черновой и целевой моделями, используя онлайн-кэш n-грамм и гибридную дистилляцию. OmniDraft позволяет одной черновой модели работать с различными целевыми моделями и адаптироваться к пользовательским данным. Система особенно подходит для LLM-приложений на устройствах, где важны эффективность и кастомизация.'}, 'en': {'title': 'One Draft Model for All Target Models!', 'desc': 'OmniDraft is a new framework designed to solve the problem of cross-vocabulary mismatch between draft models and target models in machine learning applications. It allows a single draft model to work with different target models dynamically, improving decoding speed and efficiency. The framework uses an online n-gram cache and hybrid distillation fine-tuning to adapt to user data and enhance performance. OmniDraft is particularly beneficial for on-device large language model (LLM) applications, where it can significantly reduce latency and improve user customization.'}, 'zh': {'title': '一个草稿模型，适配所有目标模型', 'desc': 'OmniDraft是一个统一框架，旨在解决跨词汇不匹配问题，并通过允许单一草稿模型与多种目标模型动态交互来提高解码速度。该框架特别适用于在线部署环境，能够使单一草稿模型与任何目标模型兼容，并根据用户数据动态调整。通过引入在线n-gram缓存和混合蒸馏微调，OmniDraft有效解决了草稿模型与目标模型之间的词汇不匹配问题。该方法在数学推理、编码和文本生成任务中表现出色，显著提高了解码效率。'}}}, {'id': 'https://huggingface.co/papers/2507.03683', 'title': 'On the rankability of visual embeddings', 'url': 'https://huggingface.co/papers/2507.03683', 'abstract': "Visual embedding models often capture continuous, ordinal attributes along specific axes, enabling effective image ranking with minimal supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term _rank axes_. We define a model as _rankable_ for an attribute if projecting embeddings onto such an axis preserves the attribute's order. Across 7 popular encoders and 9 datasets with attributes like age, crowd count, head pose, aesthetics, and recency, we find that many embeddings are inherently rankable. Surprisingly, a small number of samples, or even just two extreme examples, often suffice to recover meaningful rank axes, without full-scale supervision. These findings open up new use cases for image ranking in vector databases and motivate further study into the structure and learning of rankable embeddings. Our code is available at https://github.com/aktsonthalia/rankable-vision-embeddings.", 'score': 9, 'issue_id': 4707, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': 'ddaccaf820821b2e', 'authors': ['Ankit Sonthalia', 'Arnas Uselis', 'Seong Joon Oh'], 'affiliations': ['Tübingen AI Center, Universität Tübingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2507.03683.jpg', 'data': {'categories': ['#cv', '#dataset'], 'emoji': '📊', 'ru': {'title': 'Ранжирование изображений с минимальным надзором благодаря скрытой структуре встраиваний', 'desc': "Исследование показывает, что визуальные модели встраивания часто захватывают непрерывные, порядковые атрибуты вдоль линейных направлений, называемых осями ранжирования. Модель считается 'ранжируемой' для атрибута, если проекция встраиваний на такую ось сохраняет порядок атрибута. Анализ 7 популярных энкодеров и 9 наборов данных выявил, что многие встраивания по своей природе ранжируемы. Удивительно, но небольшого количества образцов, или даже двух крайних примеров, часто достаточно для восстановления значимых осей ранжирования без полномасштабного обучения с учителем."}, 'en': {'title': 'Unlocking Image Ranking with Minimal Supervision', 'desc': "This paper investigates how visual embedding models can represent continuous and ordinal attributes along specific directions, referred to as 'rank axes'. It defines a model as 'rankable' if projecting its embeddings onto these axes maintains the order of the attributes. The study reveals that many popular encoders can produce rankable embeddings across various datasets, and interestingly, only a few samples are needed to identify these rank axes effectively. This research highlights the potential for improved image ranking in vector databases and encourages further exploration of rankable embeddings."}, 'zh': {'title': '视觉嵌入模型的排名能力探索', 'desc': '本文研究了视觉嵌入模型是否能够沿着线性方向捕捉连续的、有序的属性，这些方向被称为_排名轴_。我们定义一个模型为_可排名_，如果将嵌入投影到这样的轴上能够保持属性的顺序。通过对7种流行编码器和9个数据集的分析，发现许多嵌入本质上是可排名的。令人惊讶的是，少量样本，甚至仅仅两个极端例子，通常就足以恢复有意义的排名轴，而无需全面监督。'}}}, {'id': 'https://huggingface.co/papers/2507.04952', 'title': 'ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code\n  Generation Evaluation', 'url': 'https://huggingface.co/papers/2507.04952', 'abstract': 'ArtifactsBench, a novel benchmark and evaluation framework, automates the assessment of visual code generation quality using temporal screenshots and a multimodal language model judge.  \t\t\t\t\tAI-generated summary \t\t\t\t The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.', 'score': 8, 'issue_id': 4695, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '8eed78adf2fbda3a', 'authors': ['Chenchen Zhang', 'Yuhang Li', 'Can Xu', 'Jiaheng Liu', 'Ao Liu', 'Shihui Hu', 'Dengpeng Wu', 'Guanhua Huang', 'Kejiao Li', 'Qi Yi', 'Ruibin Xiong', 'Haotian Zhu', 'Yuanxing Zhang', 'Yuhao Jiang', 'Yue Zhang', 'Zenan Xu', 'Bohui Zhai', 'Guoxiang He', 'Hebin Li', 'Jie Zhao', 'Le Zhang', 'Lingyun Tan', 'Pengyu Guo', 'Xianshu Pang', 'Yang Ruan', 'Zhifeng Zhang', 'Zhonghu Wang', 'Ziyan Xu', 'Zuopu Yin', 'Wiggin Zhou', 'Chayse Zhou', 'Fengzong Lian'], 'affiliations': ['Tencent Hunyuan Team'], 'pdf_title_img': 'assets/pdf/title_img/2507.04952.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'Автоматизированная оценка визуального кода с помощью мультимодальных языковых моделей', 'desc': 'ArtifactsBench - это новый фреймворк для автоматизированной оценки качества генерации визуального кода. Он использует временные скриншоты и мультимодальную языковую модель в качестве судьи для оценки. Фреймворк включает в себя бенчмарк из 1825 разнообразных задач и оценивает более 30 ведущих языковых моделей. ArtifactsBench достигает 94.4% согласованности ранжирования с золотым стандартом WebDev Arena и более 90% попарного согласия с экспертами-людьми.'}, 'en': {'title': 'Automating Quality Assessment in Visual Code Generation', 'desc': 'ArtifactsBench is a new framework designed to evaluate the quality of visual code generation by using temporal screenshots and a multimodal language model as a judge. It addresses the limitations of existing benchmarks that only focus on algorithmic correctness, ignoring the visual and interactive aspects crucial for user experiences. By programmatically rendering artifacts and capturing their dynamic behavior, ArtifactsBench provides a comprehensive assessment through a detailed checklist. The framework has been tested on 1,825 tasks and shows high consistency with human evaluations, making it a valuable tool for improving generative models in web development.'}, 'zh': {'title': 'ArtifactsBench：自动化视觉代码生成评估的新标准', 'desc': 'ArtifactsBench是一个新颖的基准和评估框架，旨在自动评估视觉代码生成的质量。它通过时间截图和多模态语言模型评判，捕捉生成的视觉工件的动态行为。该框架使用细致的任务清单来确保评估的全面性和可重复性，并构建了一个包含1825个多样化任务的新基准。我们的自动评估与人类专家的评分高度一致，标志着ArtifactsBench成为可靠的自动化评估人类感知质量的首个框架。'}}}, {'id': 'https://huggingface.co/papers/2506.21884', 'title': 'UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields', 'url': 'https://huggingface.co/papers/2506.21884', 'abstract': 'A framework combining NeRF with spectral unmixing yields accurate material segmentation and editing through hyperspectral synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF.', 'score': 7, 'issue_id': 4707, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': 'f4e1f24bc4c740b2', 'authors': ['Fabian Perez', 'Sara Rojas', 'Carlos Hinojosa', 'Hoover Rueda-Chacón', 'Bernard Ghanem'], 'affiliations': ['KAUST', 'Universidad Industrial de Santander'], 'pdf_title_img': 'assets/pdf/title_img/2506.21884.jpg', 'data': {'categories': ['#robotics', '#cv', '#3d'], 'emoji': '🌈', 'ru': {'title': 'Гиперспектральный NeRF для точной сегментации и редактирования материалов', 'desc': 'Статья представляет UnMix-NeRF - фреймворк, объединяющий нейронные радиальные поля (NeRF) со спектральным разложением для синтеза гиперспектральных изображений и сегментации материалов. Метод моделирует спектральное отражение через диффузные и зеркальные компоненты, используя словарь глобальных конечных элементов для представления чистых сигнатур материалов. UnMix-NeRF позволяет выполнять несупервизорную кластеризацию материалов и редактирование сцены путем изменения словаря конечных элементов. Эксперименты показывают превосходство метода в спектральной реконструкции и сегментации материалов по сравнению с существующими подходами.'}, 'en': {'title': 'UnMix-NeRF: Revolutionizing Material Segmentation with Spectral Insights', 'desc': 'This paper presents UnMix-NeRF, a novel framework that enhances Neural Radiance Fields (NeRF) by incorporating spectral unmixing for improved material segmentation and editing. Traditional NeRF methods rely on RGB data, which limits their ability to accurately perceive material properties essential for various applications like robotics and augmented reality. UnMix-NeRF addresses this by modeling spectral reflectance through diffuse and specular components, using a learned dictionary of global endmembers to represent pure material signatures. The framework allows for unsupervised material clustering and scene editing by modifying endmember dictionaries, resulting in superior performance in spectral reconstruction and material segmentation compared to existing techniques.'}, 'zh': {'title': '结合NeRF与光谱解混合，实现精准材料分割与编辑', 'desc': '本文提出了一种名为UnMix-NeRF的框架，将神经辐射场（NeRF）与光谱解混合相结合，实现了准确的材料分割和编辑。该方法通过联合超光谱新视图合成和无监督材料分割，克服了传统NeRF方法仅依赖RGB数据的局限性。UnMix-NeRF通过学习的全局端元字典表示纯材料特征，并利用每个点的丰度捕捉材料分布，从而实现材料的无监督聚类。实验结果表明，该方法在光谱重建和材料分割方面优于现有技术，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.05257', 'title': 'Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions', 'url': 'https://huggingface.co/papers/2507.05257', 'abstract': 'Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.', 'score': 6, 'issue_id': 4711, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'bdaecd0c6c4258a2', 'authors': ['Yuanzhe Hu', 'Yu Wang', 'Julian McAuley'], 'affiliations': ['UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2507.05257.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#agents', '#benchmark', '#rag', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Новый бенчмарк для оценки памяти LLM-агентов', 'desc': 'Статья представляет новый бенчмарк MemoryAgentBench для оценки агентов с механизмами памяти на основе больших языковых моделей (LLM). Авторы выделяют четыре ключевые компетенции для таких агентов: точное извлечение информации, обучение во время тестирования, понимание долгосрочных зависимостей и разрешение конфликтов. Бенчмарк объединяет переработанные существующие наборы данных с новыми, охватывая все четыре компетенции. Эмпирические результаты показывают, что современные методы пока не способны полностью овладеть всеми четырьмя компетенциями.'}, 'en': {'title': 'Enhancing Memory in Language Models: Introducing MemoryAgentBench', 'desc': "This paper addresses the gap in evaluating memory capabilities of Large Language Model (LLM) agents, which are crucial for tasks involving long-term information management. It introduces the concept of 'memory agents' and identifies four key competencies: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. The authors present MemoryAgentBench, a new benchmark that combines existing datasets with new ones to comprehensively assess these competencies in memory agents. The findings indicate that current memory methods are inadequate, highlighting the necessity for improved memory mechanisms in LLMs."}, 'zh': {'title': '记忆代理的新基准评估', 'desc': '本文探讨了大型语言模型（LLM）代理的记忆机制，提出了记忆代理的概念。我们识别出四个记忆代理的核心能力：准确检索、测试时学习、长程理解和冲突解决。现有的数据集无法全面评估这些能力，因此我们推出了MemoryAgentBench，一个专门为记忆代理设计的新基准。通过结合现有数据集和新构建的数据集，我们为评估记忆质量提供了一个系统且具有挑战性的测试平台。'}}}, {'id': 'https://huggingface.co/papers/2507.04590', 'title': 'VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents', 'url': 'https://huggingface.co/papers/2507.04590', 'abstract': 'A unified framework VLM2Vec-V2 is proposed for learning embeddings across diverse visual forms such as videos and documents, demonstrating strong performance on new tasks and improving upon existing benchmarks for images.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.', 'score': 5, 'issue_id': 4694, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': 'a417297c3b4c5459', 'authors': ['Rui Meng', 'Ziyan Jiang', 'Ye Liu', 'Mingyi Su', 'Xinyi Yang', 'Yuepeng Fu', 'Can Qin', 'Zeyuan Chen', 'Ran Xu', 'Caiming Xiong', 'Yingbo Zhou', 'Wenhu Chen', 'Semih Yavuz'], 'affiliations': ['Salesforce Research', 'Tsinghua University', 'UC Santa Barbara', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2507.04590.jpg', 'data': {'categories': ['#games', '#rag', '#survey', '#benchmark', '#transfer_learning', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Единая модель эмбеддингов для всех визуальных форматов', 'desc': 'VLM2Vec-V2 - это унифицированная система для создания эмбеддингов различных визуальных форматов, включая видео и документы. Модель демонстрирует высокую эффективность на новых задачах и превосходит существующие бенчмарки для изображений. VLM2Vec-V2 обучена на расширенном наборе данных MMEB-V2, который включает задачи поиска визуальных документов, поиска видео, временной привязки, классификации видео и ответов на вопросы по видео. Эксперименты показывают, что модель обобщается на различные мультимодальные задачи и закладывает основу для более масштабируемого и адаптивного обучения представлений.'}, 'en': {'title': 'Unified Embeddings for All Visual Forms!', 'desc': 'The paper introduces VLM2Vec-V2, a new framework designed to learn embeddings for various visual forms, including videos and documents. This model enhances the capabilities of existing multimodal embedding models, which have primarily focused on natural images. By establishing a comprehensive benchmark called MMEB-V2, the authors evaluate the model on new tasks such as video retrieval and visual document retrieval. The results show that VLM2Vec-V2 not only excels in these new tasks but also outperforms previous models on traditional image benchmarks, demonstrating its versatility and effectiveness in real-world applications.'}, 'zh': {'title': '统一多模态嵌入学习的新框架', 'desc': '本文提出了一种统一框架VLM2Vec-V2，用于学习多种视觉形式（如视频和文档）的嵌入。该模型在新任务上表现出色，并在图像的现有基准上有所提升。我们引入了MMEB-V2基准，扩展了五种新任务类型，包括视觉文档检索和视频分类等。通过广泛的实验，VLM2Vec-V2展示了其在多模态嵌入学习中的强大能力，为未来的研究和实际应用奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2507.03607', 'title': 'VLAI: A RoBERTa-Based Model for Automated Vulnerability Severity\n  Classification', 'url': 'https://huggingface.co/papers/2507.03607', 'abstract': 'A transformer-based model predicts software vulnerability severity levels directly from text, enhancing triage efficiency and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents VLAI, a transformer-based model that predicts software vulnerability severity levels directly from text descriptions. Built on RoBERTa, VLAI is fine-tuned on over 600,000 real-world vulnerabilities and achieves over 82% accuracy in predicting severity categories, enabling faster and more consistent triage ahead of manual CVSS scoring. The model and dataset are open-source and integrated into the Vulnerability-Lookup service.', 'score': 5, 'issue_id': 4698, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': '7fd058ff4a7cb43a', 'authors': ['Cédric Bonhomme', 'Alexandre Dulaunoy'], 'affiliations': ['Computer Incident Response Center Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2507.03607.jpg', 'data': {'categories': ['#security', '#architecture', '#dataset', '#data', '#training', '#open_source'], 'emoji': '🛡️', 'ru': {'title': 'Искусственный интеллект на страже кибербезопасности: автоматическая оценка уязвимостей ПО', 'desc': 'Представлена модель VLAI на основе трансформера, которая предсказывает уровни серьезности уязвимостей программного обеспечения непосредственно из текстовых описаний. VLAI построена на архитектуре RoBERTa и обучена на более чем 600 000 реальных уязвимостей. Модель достигает точности более 82% в предсказании категорий серьезности, что позволяет проводить более быструю и последовательную сортировку перед ручной оценкой CVSS. VLAI и набор данных являются открытыми и интегрированы в сервис Vulnerability-Lookup.'}, 'en': {'title': 'Transforming Vulnerability Assessment with VLAI', 'desc': 'This paper introduces VLAI, a transformer-based model designed to predict the severity levels of software vulnerabilities from textual descriptions. Utilizing the RoBERTa architecture, VLAI has been fine-tuned on a large dataset of over 600,000 real-world vulnerabilities, achieving an impressive accuracy of over 82% in classifying severity categories. This model significantly improves the efficiency and consistency of vulnerability triage processes, allowing for quicker assessments before manual Common Vulnerability Scoring System (CVSS) evaluations. Additionally, both the model and the dataset are made open-source and are integrated into the Vulnerability-Lookup service for broader accessibility.'}, 'zh': {'title': '智能预测软件漏洞严重性', 'desc': '本文介绍了一种基于变换器的模型VLAI，该模型能够直接从文本描述中预测软件漏洞的严重性等级。VLAI基于RoBERTa模型，经过对超过60万个真实漏洞的微调，达到了超过82%的严重性分类准确率。这一模型的应用可以提高漏洞分类的效率和一致性，帮助在手动CVSS评分之前进行更快速的评估。该模型和数据集都是开源的，并已集成到漏洞查询服务中。'}}}, {'id': 'https://huggingface.co/papers/2507.04036', 'title': 'PresentAgent: Multimodal Agent for Presentation Video Generation', 'url': 'https://huggingface.co/papers/2507.04036', 'abstract': 'A multimodal agent transforms documents into detailed presentation videos with audio, evaluated using a comprehensive framework involving vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.', 'score': 4, 'issue_id': 4693, 'pub_date': '2025-07-05', 'pub_date_card': {'ru': '5 июля', 'en': 'July 5', 'zh': '7月5日'}, 'hash': '79b10f5eed3bd7e4', 'authors': ['Jingwei Shi', 'Zeyu Zhang', 'Biao Wu', 'Yanjie Liang', 'Meng Fang', 'Ling Chen', 'Yang Zhao'], 'affiliations': ['AI Geeks, Australia', 'Australian Artificial Intelligence Institute, Australia', 'La Trobe University, Australia', 'University of Liverpool, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.04036.jpg', 'data': {'categories': ['#cv', '#multimodal', '#agents', '#optimization', '#dataset', '#benchmark', '#games', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'Искусственный интеллект создает презентации на уровне человека', 'desc': 'Статья представляет PresentAgent - мультимодального агента, преобразующего длинные документы в видеопрезентации с озвучкой. Система использует модульный конвейер для сегментации документа, создания слайдов, генерации речи и компоновки видео. Для оценки качества выходных данных авторы разработали фреймворк PresentEval на основе визуально-языковых моделей. Эксперименты показали, что PresentAgent приближается к уровню человека по всем метрикам оценки.'}, 'en': {'title': 'Transforming Text into Engaging Videos with PresentAgent', 'desc': 'PresentAgent is a multimodal agent designed to convert long documents into engaging presentation videos with synchronized audio. Unlike traditional methods that only create static slides or text summaries, this approach generates dynamic visual and spoken content that resembles human presentations. It utilizes a modular pipeline for document segmentation, slide rendering, and narration generation, ensuring high-quality audio-visual alignment. The effectiveness of PresentAgent is evaluated using PresentEval, a framework that assesses video quality based on content fidelity, visual clarity, and audience comprehension, demonstrating its potential to enhance the accessibility of information.'}, 'zh': {'title': '将文档转化为生动演示的智能体', 'desc': '本文介绍了一种名为PresentAgent的多模态智能体，它能够将长篇文档转化为带有旁白的演示视频。与现有方法仅能生成静态幻灯片或文本摘要不同，我们的方法能够生成与人类演示风格相似的同步视觉和语音内容。PresentAgent采用模块化流程，系统地对输入文档进行分段，规划和渲染幻灯片风格的视觉框架，并利用大型语言模型和文本转语音模型生成上下文相关的旁白。我们还提出了PresentEval评估框架，通过视觉-语言模型对视频进行全面评分，验证了PresentAgent在内容真实性、视觉清晰度和观众理解力等方面接近人类水平的质量。'}}}, {'id': 'https://huggingface.co/papers/2507.03033', 'title': 'Preserving Privacy, Increasing Accessibility, and Reducing Cost: An\n  On-Device Artificial Intelligence Model for Medical Transcription and Note\n  Generation', 'url': 'https://huggingface.co/papers/2507.03033', 'abstract': 'A fine-tuned Llama 3.2 1B model using PEFT with LoRA in the browser improves medical transcription accuracy and reduces privacy and computational concerns.  \t\t\t\t\tAI-generated summary \t\t\t\t Background: Clinical documentation represents a significant burden for healthcare providers, with physicians spending up to 2 hours daily on administrative tasks. Recent advances in large language models (LLMs) offer promising solutions, but privacy concerns and computational requirements limit their adoption in healthcare settings. Objective: To develop and evaluate a privacy-preserving, on-device medical transcription system using a fine-tuned Llama 3.2 1B model capable of generating structured medical notes from medical transcriptions while maintaining complete data sovereignty entirely in the browser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical transcription-to-structured note pairs. The model was evaluated against the base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140 modified ACI benchmark cases. Evaluation employed both statistical metrics (ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple clinical quality dimensions. Results: The fine-tuned OnDevice model demonstrated substantial improvements over the base model. On the ACI benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1 improved from 0.832 to 0.866. Clinical quality assessments showed marked reduction in major hallucinations (from 85 to 35 cases) and enhanced factual correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed on the internal evaluation dataset, with composite scores increasing from 3.13 to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical transcription yields clinically meaningful improvements while enabling complete on-device browser deployment. This approach addresses key barriers to AI adoption in healthcare: privacy preservation, cost reduction, and accessibility for resource-constrained environments.', 'score': 4, 'issue_id': 4708, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '0ea33e70360e744c', 'authors': ['Johnson Thomas', 'Ayush Mudgal', 'Wendao Liu', 'Nisten Tahiraj', 'Zeeshaan Mohammed', 'Dhruv Diddi'], 'affiliations': ['Department of Endocrinology, Mercy Hospital, Springfield, Missouri, USA', 'Solo Tech', 'Starfishdata.ai', 'alignmentlab.ai'], 'pdf_title_img': 'assets/pdf/title_img/2507.03033.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#synthetic', '#inference', '#training', '#low_resource', '#healthcare'], 'emoji': '🏥', 'ru': {'title': 'Эффективная и безопасная медицинская транскрипция с помощью ИИ в браузере', 'desc': 'В статье описывается разработка и оценка системы медицинской транскрипции на основе модели Llama 3.2 1B, дообученной с помощью техники PEFT с LoRA. Система работает полностью в браузере, обеспечивая конфиденциальность данных и снижая вычислительные требования. Результаты показывают значительное улучшение точности транскрипции по сравнению с базовой моделью, что измерялось с помощью метрик ROUGE, BERTScore и оценок клинического качества. Такой подход решает ключевые проблемы внедрения ИИ в здравоохранении: сохранение конфиденциальности, снижение затрат и доступность для ресурсно-ограниченных сред.'}, 'en': {'title': 'Enhancing Medical Transcription with Privacy-Preserving AI', 'desc': 'This paper presents a method to enhance medical transcription accuracy using a fine-tuned Llama 3.2 1B model with Parameter-Efficient Fine-Tuning (PEFT) and LoRA. The model operates entirely in the browser, ensuring data privacy and reducing computational demands, which are critical in healthcare settings. Evaluation results show significant improvements in transcription quality, with reductions in errors and hallucinations, as well as better factual correctness. This approach not only improves clinical documentation efficiency but also addresses privacy and accessibility challenges in AI adoption for healthcare.'}, 'zh': {'title': '提升医疗转录准确性，保护隐私与计算效率', 'desc': '本研究开发了一种基于Llama 3.2 1B模型的医疗转录系统，采用了参数高效微调（PEFT）和LoRA技术。该系统能够在浏览器中生成结构化的医疗笔记，同时确保数据隐私和安全。通过对1500对合成医疗转录和结构化笔记的微调，模型在临床质量评估中显示出显著的改进。研究结果表明，该模型在减少错误和提高准确性方面表现优异，解决了医疗领域中AI应用的隐私和计算成本问题。'}}}, {'id': 'https://huggingface.co/papers/2507.05259', 'title': 'Beyond Simple Edits: X-Planner for Complex Instruction-Based Image\n  Editing', 'url': 'https://huggingface.co/papers/2507.05259', 'abstract': 'X-Planner, a planning system utilizing a multimodal large language model, decomposes complex text-guided image editing instructions into precise sub-instructions, ensuring localized, identity-preserving edits and achieving top performance on established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.', 'score': 3, 'issue_id': 4696, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '2db1a2de292203d6', 'authors': ['Chun-Hsiao Yeh', 'Yilin Wang', 'Nanxuan Zhao', 'Richard Zhang', 'Yuheng Li', 'Yi Ma', 'Krishna Kumar Singh'], 'affiliations': ['Adobe', 'HKU', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2507.05259.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'X-Planner: умное планирование для точного редактирования изображений', 'desc': 'X-Planner - это система планирования на основе мультимодальной большой языковой модели для редактирования изображений по текстовым инструкциям. Она разбивает сложные указания на точные подинструкции, обеспечивая локализованные правки с сохранением идентичности объектов. X-Planner автоматически генерирует типы правок и маски сегментации для каждой подинструкции. Система достигает наилучших результатов на существующих бенчмарках и новом тесте сложного редактирования.'}, 'en': {'title': 'X-Planner: Simplifying Complex Image Edits with Precision', 'desc': 'X-Planner is a planning system that uses a multimodal large language model to improve text-guided image editing. It breaks down complex editing instructions into simpler sub-instructions, which helps in making precise edits while preserving the identity of the images. This system reduces the need for manual masks and minimizes unintended changes by generating accurate edit types and segmentation masks automatically. X-Planner has shown to achieve top performance on established benchmarks, demonstrating its effectiveness in handling intricate editing tasks.'}, 'zh': {'title': 'X-Planner：精准分解复杂指令的图像编辑系统', 'desc': 'X-Planner 是一个利用多模态大语言模型的规划系统，能够将复杂的文本引导图像编辑指令分解为精确的子指令。这种方法确保了编辑的局部性和身份保留，避免了不必要的编辑错误。X-Planner 通过链式思维推理，系统地将复杂指令简化为更清晰的子指令，并自动生成编辑类型和分割掩码，减少了人工干预。该系统在现有基准测试和新引入的复杂编辑基准上都取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2507.03336', 'title': 'Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky', 'url': 'https://huggingface.co/papers/2507.03336', 'abstract': "DiaFORGE is a disambiguation framework that enhances large language models' ability to invoke enterprise APIs accurately through dialogue synthesis, supervised fine-tuning, and real-world evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.", 'score': 3, 'issue_id': 4694, 'pub_date': '2025-07-04', 'pub_date_card': {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'}, 'hash': 'a22f17539601dde1', 'authors': ['Ashutosh Hathidara', 'Julien Yu', 'Sebastian Schreiber'], 'affiliations': ['SAP Labs'], 'pdf_title_img': 'assets/pdf/title_img/2507.03336.jpg', 'data': {'categories': ['#optimization', '#open_source', '#dataset', '#training', '#data', '#alignment', '#benchmark', '#agents'], 'emoji': '🔧', 'ru': {'title': 'DiaFORGE: точные вызовы API через синтез диалогов и дообучение LLM', 'desc': 'DiaFORGE - это фреймворк для улучшения способности больших языковых моделей (LLM) точно вызывать корпоративные API через синтез диалогов и дообучение с учителем. Он включает трехэтапный процесс: синтез диалогов, дообучение моделей и оценку готовности к реальному использованию. На тестовом наборе DiaBENCH модели, обученные с помощью DiaFORGE, повышают успешность вызова инструментов на 27 процентных пунктов по сравнению с GPT-4 и на 49 пунктов по сравнению с Claude-3.5-Sonnet. Авторы также выпустили открытый корпус из 5000 корпоративных API-спецификаций с проверенными диалогами для дальнейших исследований.'}, 'en': {'title': 'Empowering LLMs to Accurately Invoke APIs with DiaFORGE', 'desc': "DiaFORGE is a framework designed to improve how large language models (LLMs) interact with enterprise APIs by resolving ambiguities in user requests. It consists of a three-stage process that includes generating multi-turn dialogues to help the model differentiate between similar tools, fine-tuning the model with supervised learning using reasoning traces, and evaluating the model's performance in real-world scenarios. The results show that models trained with DiaFORGE significantly outperform existing models like GPT-4o and Claude-3.5-Sonnet in successfully invoking tools. Additionally, DiaFORGE provides a valuable resource by releasing a corpus of enterprise API specifications and validated dialogues to aid future research."}, 'zh': {'title': '提升API调用准确性的对话框架', 'desc': 'DiaFORGE是一个消歧义框架，旨在提高大型语言模型在对话中准确调用企业API的能力。该框架包括三个阶段：首先合成以角色为驱动的多轮对话，帮助助手区分相似工具；其次对开源模型进行监督微调，利用3B到70B参数的推理轨迹；最后通过动态评估套件测试模型在真实环境中的表现。通过DiaFORGE训练的模型在工具调用成功率上比GPT-4o提高了27个百分点，比Claude-3.5-Sonnet提高了49个百分点。'}}}, {'id': 'https://huggingface.co/papers/2507.04642', 'title': 'R1-RE: Cross-Domain Relationship Extraction with RLVR', 'url': 'https://huggingface.co/papers/2507.04642', 'abstract': "R1-RE, a reinforcement learning with verifiable reward framework, enhances out-of-domain robustness in relationship extraction by leveraging small language models' reasoning abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Relationship extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.", 'score': 2, 'issue_id': 4712, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '26a997cc2412dd19', 'authors': ['Runpeng Dai', 'Tong Zheng', 'Run Yang', 'Hongtu Zhu'], 'affiliations': ['BiliBili', 'University of Maryland, College Park', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2507.04642.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#small_models', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение обобщения в извлечении отношений через обучение с подкреплением', 'desc': 'Статья представляет R1-RE - новый фреймворк обучения с подкреплением для задачи извлечения отношений. R1-RE использует способности малых языковых моделей к рассуждению, что значительно улучшает обобщение вне домена. Метод переосмысливает извлечение отношений как задачу рассуждения, основанную на инструкциях по аннотации. Эксперименты показывают, что R1-RE-7B достигает точности вне домена около 70%, сравнимой с ведущими проприетарными моделями.'}, 'en': {'title': 'Enhancing Relationship Extraction with Reinforcement Learning and Reasoning', 'desc': 'The paper introduces R1-RE, a novel framework that applies reinforcement learning with verifiable rewards to improve relationship extraction (RE) in natural language processing. Traditional RE methods struggle with generalizing to new, unseen data, but R1-RE reframes the task as a reasoning challenge, similar to how human annotators work. By utilizing the reasoning capabilities of small language models, this approach enhances robustness against out-of-domain scenarios. The results show that R1-RE achieves competitive accuracy on benchmark datasets, providing valuable insights into the training dynamics of reinforcement learning in RE tasks.'}, 'zh': {'title': '提升关系提取的域外鲁棒性', 'desc': 'R1-RE是一种强化学习框架，旨在提高关系提取任务在域外的鲁棒性。该方法通过利用小型语言模型的推理能力，将关系提取重新定义为一个推理任务，而不是传统的监督学习问题。通过引入可验证奖励的强化学习机制，R1-RE显著提升了模型在未见数据上的表现。我们的实验表明，R1-RE-7B模型在Sem-2010和MDKG数据集上达到了约70%的域外准确率，表现与领先的专有模型相当。'}}}, {'id': 'https://huggingface.co/papers/2507.04562', 'title': 'Evaluating LLMs on Real-World Forecasting Against Human Superforecasters', 'url': 'https://huggingface.co/papers/2507.04562', 'abstract': 'State-of-the-art large language models are evaluated on forecasting questions and show lower accuracy compared to human superforecasters.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against human superforecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of superforecasters.', 'score': 2, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '27146571110c725d', 'authors': ['Janna Lu'], 'affiliations': ['Department of Economics, George Mason University, Fairfax, VA 22030'], 'pdf_title_img': 'assets/pdf/title_img/2507.04562.jpg', 'data': {'categories': ['#benchmark', '#reasoning'], 'emoji': '🔮', 'ru': {'title': 'Искусственный интеллект vs суперпрогнозисты: битва предсказателей будущего', 'desc': 'Современные большие языковые модели (LLM) были протестированы на задачах прогнозирования будущих событий. Результаты показали, что LLM достигают более высоких оценок Брайера, чем обычные люди, но все еще значительно уступают группе суперпрогнозистов. Исследование проводилось на 464 вопросах прогнозирования с платформы Metaculus. Это подчеркивает, что способности LLM к прогнозированию все еще требуют улучшения.'}, 'en': {'title': 'LLMs Lag Behind Humans in Forecasting Accuracy', 'desc': 'This paper evaluates the forecasting abilities of state-of-the-art large language models (LLMs) against human superforecasters. Despite LLMs showing impressive performance in various tasks, their accuracy in predicting future events is still lacking. The study uses Brier scores to compare the predictions of LLMs on 464 questions from Metaculus with those made by human experts. The results indicate that while LLMs may achieve scores that seem better than average human predictions, they still fall short when compared to the best human forecasters.'}, 'zh': {'title': '大型语言模型在预测中的局限性', 'desc': '本研究评估了最新的大型语言模型（LLMs）在预测问题上的表现，发现其准确性低于人类超级预测者。尽管LLMs在多种任务中展现出卓越的能力，但它们在预测未来事件方面的能力仍然缺乏深入研究。通过对Metaculus的464个预测问题进行评估，研究发现前沿模型的Brier分数表面上超过了人类群体，但仍显著低于超级预测者的表现。该研究揭示了当前LLMs在预测任务中的局限性，强调了人类预测者的优势。'}}}, {'id': 'https://huggingface.co/papers/2507.04376', 'title': 'MOD-X: A Modular Open Decentralized eXchange Framework proposal for\n  Heterogeneous Interoperable Artificial Agents', 'url': 'https://huggingface.co/papers/2507.04376', 'abstract': "As Artificial Intelligence systems evolve from monolithic models to ecosystems of specialized agents, the need for standardized communication protocols becomes increasingly critical. This paper introduces MOD-X (Modular Open Decentralized eXchange), a novel architectural framework proposal for agent interoperability that addresses key limitations of existing protocols. Unlike current approaches, MOD-X proposes a layered architecture with a Universal Message Bus, thorough state management, translation capabilities, and blockchain-based security mechanisms. We present MOD-X's architecture, compare it with existing protocols, and demonstrate its application through a worked example how it enables integration between heterogeneous specialist agents (agents with different architectures, vendors, capabilities, and knowledge representations--including rule-based systems, neural networks, symbolic reasoning engines, and legacy software with agent wrappers). MOD-X's key innovations include a publish-subscribe communication model, semantic capability discovery, and dynamic workflow orchestration--providing a framework that bridges theoretical formalism with practical implementation. This architecture addresses the growing need for truly decentralized, interoperable agent ecosystems that can scale effectively without the need for central coordination.", 'score': 2, 'issue_id': 4696, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': 'a7ed25fb7089c612', 'authors': ['Georgios Ioannides', 'Christos Constantinou', 'Vinija Jain', 'Aman Chadha', 'Aaron Elkins'], 'affiliations': ['Amazon GenAI, USA', 'James Silberrad Brown Center for Artificial Intelligence, Carnegie Mellon University', 'James Silberrad Brown Center for Artificial Intelligence, USA', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2507.04376.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#agi', '#agents', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'MOD-X: Новый стандарт для взаимодействия ИИ-агентов', 'desc': 'Статья представляет MOD-X - новую архитектурную концепцию для обеспечения взаимодействия между ИИ-агентами. MOD-X предлагает многоуровневую архитектуру с универсальной шиной сообщений, управлением состоянием и механизмами безопасности на основе блокчейна. Ключевые инновации включают модель публикации-подписки, семантическое обнаружение возможностей и динамическую оркестровку рабочих процессов. Эта архитектура отвечает растущей потребности в действительно децентрализованных, совместимых экосистемах агентов, способных эффективно масштабироваться без необходимости централизованной координации.'}, 'en': {'title': 'MOD-X: Bridging AI Agents for Seamless Collaboration', 'desc': 'This paper presents MOD-X, a new framework designed to improve communication between different AI agents. It introduces a layered architecture that includes a Universal Message Bus and blockchain security, allowing diverse agents to work together seamlessly. MOD-X enhances interoperability by using a publish-subscribe model and dynamic workflow orchestration, making it easier for agents with different capabilities to collaborate. The framework aims to create decentralized ecosystems of specialized agents that can scale without central control.'}, 'zh': {'title': 'MOD-X：构建智能代理的互操作性新框架', 'desc': '随着人工智能系统从单一模型演变为专门化代理的生态系统，标准化通信协议的需求变得越来越重要。本文提出了MOD-X（模块化开放去中心化交换），这是一个新的架构框架，旨在解决现有协议的关键限制。MOD-X采用分层架构，配备通用消息总线、全面的状态管理、翻译能力和基于区块链的安全机制。其创新之处在于发布-订阅通信模型、语义能力发现和动态工作流编排，提供了一个连接理论形式与实际实施的框架。'}}}, {'id': 'https://huggingface.co/papers/2507.04285', 'title': 'SeqTex: Generate Mesh Textures in Video Sequence', 'url': 'https://huggingface.co/papers/2507.04285', 'abstract': 'SeqTex leverages pretrained video foundation models to directly generate high-fidelity UV texture maps through a sequence generation approach, enhancing 3D texture generation with superior consistency and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.', 'score': 1, 'issue_id': 4701, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': 'be8c2e50f0eeca2f', 'authors': ['Ze Yuan', 'Xin Yu', 'Yangtian Sun', 'Yuan-Chen Guo', 'Yan-Pei Cao', 'Ding Liang', 'Xiaojuan Qi'], 'affiliations': ['HKU', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2507.04285.jpg', 'data': {'categories': ['#architecture', '#3d', '#synthetic', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'SeqTex: прямая генерация UV-текстур с помощью видео-моделей', 'desc': 'SeqTex - это новый метод генерации текстур для 3D-моделей, использующий предобученные видео-модели. Он генерирует UV-развертки текстур напрямую, без промежуточных этапов, что улучшает согласованность и выравнивание текстур. SeqTex переформулирует задачу как проблему генерации последовательностей, что позволяет модели изучать совместное распределение многоракурсных рендеров и UV-текстур. Метод достигает наилучших результатов в генерации 3D-текстур по изображениям и текстовым описаниям.'}, 'en': {'title': 'SeqTex: Direct UV Texture Generation with Video Model Power', 'desc': 'SeqTex is a new framework that uses pretrained video models to create high-quality UV texture maps directly, improving the process of 3D texture generation. Traditional methods often struggle with limited datasets and generate textures in two stages, which can lead to errors and inconsistencies. By treating the generation of UV textures as a sequence problem, SeqTex learns to connect multi-view images and UV textures more effectively. This approach, along with innovative design features, allows SeqTex to produce detailed and consistent textures without needing additional processing steps.'}, 'zh': {'title': 'SeqTex：直接生成高保真 UV 纹理图的创新框架', 'desc': 'SeqTex 是一个新颖的端到端框架，利用预训练的视频基础模型直接生成高保真 UV 纹理图。与以往方法不同，SeqTex 将任务重新定义为序列生成问题，从而学习多视图渲染和 UV 纹理的联合分布。这种设计有效地将视频基础模型中的一致性图像空间先验转移到 UV 领域。通过多种架构创新，SeqTex 在生成高质量 3D 纹理时实现了更好的一致性和对齐，且无需后处理。'}}}, {'id': 'https://huggingface.co/papers/2507.01951', 'title': 'Test-Time Scaling with Reflective Generative Model', 'url': 'https://huggingface.co/papers/2507.01951', 'abstract': "MetaStone-S1, a reflective generative model using a self-supervised process reward model, achieves efficient reasoning and scalable performance with fewer parameters compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3's performance via the self-supervised process reward model (SPRM). Through sharing the backbone network and using task-specific heads for next token prediction and process scoring respectively, SPRM successfully integrates the policy model and process reward model(PRM) into a unified interface without extra process annotation, reducing over 99% PRM parameters for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable for test time scaling (TTS), and we provide three reasoning effort modes (low, medium, and high), based on the controllable thinking length. Moreover, we empirically establish a scaling law that reveals the relationship between total thinking computation and TTS performance. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", 'score': 67, 'issue_id': 4792, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '46a6ab7d22e05401', 'authors': ['Zixiao Wang', 'Yuxin Wang', 'Xiaorui Wang', 'Mengting Xing', 'Jie Gao', 'Jianjun Xu', 'Guangcan Liu', 'Chenhui Jin', 'Zhuo Wang', 'Shengzhuo Zhang', 'Hongtao Xie'], 'affiliations': ['MetaStone-AI', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2507.01951.jpg', 'data': {'categories': ['#training', '#open_source', '#architecture', '#rl', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с меньшими ресурсами: MetaStone-S1 переопределяет генеративные модели', 'desc': 'MetaStone-S1 - это рефлексивная генеративная модель, использующая самоконтролируемую модель вознаграждения процесса (SPRM). Она достигает эффективного рассуждения и масштабируемой производительности с меньшим количеством параметров по сравнению с существующими моделями. SPRM объединяет модель политики и модель вознаграждения процесса в единый интерфейс, сокращая более 99% параметров для эффективного рассуждения. MetaStone-S1 предлагает три режима усилий рассуждения и устанавливает закон масштабирования, связывающий общие вычисления мышления и производительность TTS.'}, 'en': {'title': 'Efficient Reasoning with Fewer Parameters: Introducing MetaStone-S1', 'desc': 'MetaStone-S1 is a new generative model that uses a self-supervised process reward model (SPRM) to enhance reasoning capabilities while maintaining a smaller parameter size. By integrating the policy model and process reward model into a single framework, it significantly reduces the number of parameters needed for effective reasoning. The model offers different reasoning effort modes, allowing users to control the depth of thinking during tasks. Experiments show that MetaStone-S1 performs comparably to larger models while being more efficient, and it is available for the research community to explore.'}, 'zh': {'title': 'MetaStone-S1：高效推理的新一代生成模型', 'desc': 'MetaStone-S1是一种反思生成模型，采用自监督过程奖励模型（SPRM），在参数更少的情况下实现高效推理和可扩展性能。该模型通过共享主干网络，并使用特定任务的头部进行下一个标记预测和过程评分，成功将策略模型和过程奖励模型整合为统一接口，减少了99%以上的参数。MetaStone-S1适合测试时间扩展（TTS），并提供低、中、高三种推理努力模式，基于可控的思考长度。实验表明，MetaStone-S1在仅32B参数的情况下，性能与OpenAI-o3-mini系列相当。'}}}, {'id': 'https://huggingface.co/papers/2507.08776', 'title': 'CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering', 'url': 'https://huggingface.co/papers/2507.08776', 'abstract': 'A neural rendering method uses compressed light-field tokens to efficiently represent scenes and render novel views with varying compute budgets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes a neural rendering approach that represents a scene as "compressed light-field tokens (CLiFTs)", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser\'\' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.', 'score': 41, 'issue_id': 4792, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '9361d4738ec618fe', 'authors': ['Zhengqing Wang', 'Yuefan Wu', 'Jiacheng Chen', 'Fuyang Zhang', 'Yasutaka Furukawa'], 'affiliations': ['Simon Fraser University', 'Wayve'], 'pdf_title_img': 'assets/pdf/title_img/2507.08776.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset'], 'emoji': '🎥', 'ru': {'title': 'Эффективный нейрорендеринг с помощью сжатых токенов светового поля', 'desc': 'Эта статья представляет нейронный метод рендеринга, использующий сжатые токены светового поля (CLiFTs) для эффективного представления сцен. Метод позволяет рендерить новые ракурсы с различными вычислительными бюджетами, сохраняя богатую информацию о внешнем виде и геометрии сцены. Система использует мультиракурсное кодирование, кластеризацию в латентном пространстве и адаптивный рендерер для достижения компромисса между размером данных, качеством рендеринга и скоростью. Эксперименты на наборах данных RealEstate10K и DL3DV подтверждают эффективность подхода, демонстрируя значительное сокращение данных при сопоставимом качестве рендеринга.'}, 'en': {'title': 'Efficient Scene Representation with Compressed Light-Field Tokens', 'desc': 'This paper introduces a novel neural rendering technique that utilizes compressed light-field tokens (CLiFTs) to efficiently depict scenes and generate new views. By employing a multi-view encoder, the method tokenizes images based on their camera positions, allowing for effective representation of both appearance and geometry. The approach leverages latent-space K-means to select key rays as cluster centroids, which are then condensed into CLiFTs for streamlined rendering. The system adapts to different compute budgets by varying the number of tokens used, demonstrating significant data reduction while maintaining high rendering quality across various datasets.'}, 'zh': {'title': '高效神经渲染：压缩光场标记的应用', 'desc': '这篇论文提出了一种神经渲染方法，使用压缩光场标记（CLiFTs）来高效表示场景并渲染新视图。CLiFT通过压缩标记实现计算效率，同时能够根据需要调整标记数量以表示场景或渲染新视图。具体来说，给定一组图像，多视角编码器将图像与相机姿态进行标记。通过潜在空间K均值选择一组减少的光线作为聚类中心，最终构建出CLiFTs。'}}}, {'id': 'https://huggingface.co/papers/2507.08800', 'title': 'NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models', 'url': 'https://huggingface.co/papers/2507.08800', 'abstract': 'NeuralOS uses a combination of RNNs and diffusion-based rendering to simulate OS GUIs by predicting screen frames from user inputs, demonstrating realistic GUI rendering and state transitions.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.', 'score': 33, 'issue_id': 4792, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '97c49e854df6a19d', 'authors': ['Luke Rivard', 'Sun Sun', 'Hongyu Guo', 'Wenhu Chen', 'Yuntian Deng'], 'affiliations': ['National Research Council Canada', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2507.08800.jpg', 'data': {'categories': ['#games', '#diffusion', '#dataset', '#agents', '#multimodal', '#cv'], 'emoji': '🖥️', 'ru': {'title': 'Нейронная симуляция GUI: шаг к адаптивным интерфейсам будущего', 'desc': 'NeuralOS - это нейронная система, симулирующая графические интерфейсы операционных систем путем предсказания кадров экрана в ответ на действия пользователя. Она объединяет рекуррентную нейронную сеть (RNN) для отслеживания состояния компьютера и нейронный рендерер на основе диффузии для генерации изображений экрана. Модель обучена на большом наборе данных записей Ubuntu XFCE, включающем как случайные, так и реалистичные взаимодействия. Эксперименты показывают, что NeuralOS успешно визуализирует реалистичные GUI-последовательности и точно предсказывает переходы состояний, хотя моделирование детальных клавиатурных взаимодействий остается сложной задачей.'}, 'en': {'title': 'NeuralOS: Predicting GUIs with RNNs and Diffusion Rendering', 'desc': "NeuralOS is a neural framework designed to simulate operating system graphical user interfaces (GUIs) by predicting screen frames based on user inputs like mouse movements and keyboard events. It utilizes a recurrent neural network (RNN) to keep track of the computer's state and a diffusion-based renderer to create realistic screen images. The model is trained on a comprehensive dataset of Ubuntu XFCE recordings, which include both random and realistic user interactions. While it excels at rendering GUI sequences and predicting state transitions, accurately modeling detailed keyboard interactions remains a challenge, marking a significant advancement in generative neural interfaces for human-computer interaction."}, 'zh': {'title': 'NeuralOS：未来人机交互的智能界面', 'desc': 'NeuralOS 是一个神经网络框架，能够通过预测用户输入（如鼠标移动、点击和键盘事件）来模拟操作系统的图形用户界面（GUI）。它结合了递归神经网络（RNN）和基于扩散的神经渲染器，能够生成屏幕图像并跟踪计算机状态。该模型在大规模的 Ubuntu XFCE 录制数据集上进行训练，包含随机生成的交互和 AI 代理生成的真实交互。尽管精确建模细粒度的键盘交互仍然具有挑战性，NeuralOS 为未来人机交互系统创建完全自适应的生成神经接口迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2507.05255', 'title': 'Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning', 'url': 'https://huggingface.co/papers/2507.05255', 'abstract': 'A two-stage paradigm involving cold-start fine-tuning and multimodal reinforcement learning enhances visual reasoning in large language models, achieving top performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.', 'score': 30, 'issue_id': 4800, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '4cfe77eb19ff1906', 'authors': ['Yana Wei', 'Liang Zhao', 'Jianjian Sun', 'Kangheng Lin', 'Jisheng Yin', 'Jingcheng Hu', 'Yinmin Zhang', 'En Yu', 'Haoran Lv', 'Zejia Weng', 'Jia Wang', 'Chunrui Han', 'Yuang Peng', 'Qi Han', 'Zheng Ge', 'Xiangyu Zhang', 'Daxin Jiang', 'Vishal M. Patel'], 'affiliations': ['BUPT', 'HUST', 'Johns Hopkins University', 'StepFun', 'THU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2507.05255.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#transfer_learning', '#open_source', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Двухэтапное обучение раскрывает потенциал визуального мышления ИИ', 'desc': 'Статья представляет новый двухэтапный подход к улучшению визуального мышления мультимодальных языковых моделей (MLLM). Первый этап включает масштабное языковое дообучение, а второй - мультимодальное обучение с подкреплением. Исследователи обнаружили, что перенос поведения возникает на ранних стадиях из-за лингвистических ментальных образов. Разработанная модель Open-Vision-Reasoner (OVR) достигла наилучших результатов на ряде тестов визуального мышления.'}, 'en': {'title': 'Unlocking Visual Reasoning with Two-Stage Learning', 'desc': 'This paper presents a novel approach to enhance visual reasoning in Multimodal Large Language Models (MLLMs) through a two-stage process. The first stage involves cold-start fine-tuning, which helps the model quickly learn visual behaviors from linguistic data. The second stage employs multimodal reinforcement learning (RL) to refine and optimize these behaviors over many iterations. The resulting model, Open-Vision-Reasoner (OVR), achieves top scores on various reasoning benchmarks, demonstrating the effectiveness of this method in improving visual reasoning capabilities.'}, 'zh': {'title': '提升视觉推理的两阶段方法', 'desc': '本文提出了一种两阶段的范式，通过冷启动微调和多模态强化学习来增强大型语言模型的视觉推理能力。研究表明，冷启动阶段能够快速转移行为，而强化学习则能够识别和放大有效的模式。我们的模型Open-Vision-Reasoner在多个推理基准上表现出色，超越了以往的开源成果。我们还发布了模型、数据和训练动态，以促进更强大且行为对齐的多模态推理器的发展。'}}}, {'id': 'https://huggingface.co/papers/2507.08799', 'title': 'KV Cache Steering for Inducing Reasoning in Small Language Models', 'url': 'https://huggingface.co/papers/2507.08799', 'abstract': 'Cache steering improves reasoning in language models through a single intervention in the key-value cache, enhancing both reasoning structure and task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.', 'score': 23, 'issue_id': 4796, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '368d77b32f2df891', 'authors': ['Max Belitsky', 'Dawid J. Kopiczko', 'Michael Dorkenwald', 'M. Jehanzeb Mirza', 'Cees G. M. Snoek', 'Yuki M. Asano'], 'affiliations': ['CSAIL MIT', 'FunAI Lab University of Technology Nuremberg', 'VIS Lab University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2507.08799.jpg', 'data': {'categories': ['#small_models', '#optimization', '#benchmark', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений в ИИ через одноразовое вмешательство в кэш', 'desc': 'В статье представлен метод кэш-управления для улучшения рассуждений в языковых моделях. Этот легковесный подход использует одноразовое вмешательство в кэш ключ-значение для изменения поведения модели. Метод применяется для индукции рассуждений по цепочке мыслей в небольших языковых моделях, используя следы рассуждений, сгенерированные GPT-4. Экспериментальные оценки показывают улучшение как качественной структуры рассуждений модели, так и количественной производительности задач.'}, 'en': {'title': 'Cache Steering: Enhancing Reasoning in Language Models Efficiently', 'desc': 'This paper introduces cache steering, a novel technique that enhances the reasoning capabilities of language models by modifying the key-value cache in a single step. By using this method, the authors demonstrate how to encourage chain-of-thought reasoning in smaller language models without the need for extensive fine-tuning or altering prompts. The approach utilizes reasoning traces generated by GPT-4o to create steering vectors that guide the model towards more structured and multi-step reasoning. Experimental results show that cache steering not only improves the quality of reasoning but also boosts performance on various reasoning tasks, offering a more efficient and stable alternative to previous methods.'}, 'zh': {'title': '缓存引导：提升语言模型推理的有效方法', 'desc': '本文提出了一种名为缓存引导的方法，通过对关键值缓存进行一次性干预，隐式地引导语言模型的推理。该方法旨在提高小型语言模型的链式推理能力，利用GPT-4生成的推理轨迹构建引导向量，从而使模型行为向更明确的多步骤推理转变，而无需进行微调或修改提示。实验结果表明，缓存引导不仅改善了模型推理的定性结构，还提高了定量任务性能。与需要持续干预的先前激活引导技术相比，我们的一次性缓存引导在超参数稳定性、推理时间效率和集成便利性方面具有显著优势，成为一种更稳健和实用的受控生成解决方案。'}}}, {'id': 'https://huggingface.co/papers/2507.05397', 'title': 'Neural-Driven Image Editing', 'url': 'https://huggingface.co/papers/2507.05397', 'abstract': 'LoongX uses multimodal neurophysiological signals and diffusion models for hands-free image editing, achieving performance comparable to text-driven methods and outperforming them when combined with speech.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional image editing typically relies on manual prompting, making it labor-intensive and inaccessible to individuals with limited motor control or language abilities. Leveraging recent advances in brain-computer interfaces (BCIs) and generative models, we propose LoongX, a hands-free image editing approach driven by multimodal neurophysiological signals. LoongX utilizes state-of-the-art diffusion models trained on a comprehensive dataset of 23,928 image editing pairs, each paired with synchronized electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography (PPG), and head motion signals that capture user intent. To effectively address the heterogeneity of these signals, LoongX integrates two key modules. The cross-scale state space (CS3) module encodes informative modality-specific features. The dynamic gated fusion (DGF) module further aggregates these features into a unified latent space, which is then aligned with edit semantics via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train the encoders using contrastive learning to align cognitive states with semantic intentions from embedded natural language. Extensive experiments demonstrate that LoongX achieves performance comparable to text-driven methods (CLIP-I: 0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results highlight the promise of neural-driven generative models in enabling accessible, intuitive image editing and open new directions for cognitive-driven creative technologies. Datasets and code will be released to support future work and foster progress in this emerging area.', 'score': 23, 'issue_id': 4796, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '37985f3d38738096', 'authors': ['Pengfei Zhou', 'Jie Xia', 'Xiaopeng Peng', 'Wangbo Zhao', 'Zilong Ye', 'Zekai Li', 'Suorong Yang', 'Jiadong Pan', 'Yuanxiang Chen', 'Ziqiao Wang', 'Kai Wang', 'Qian Zheng', 'Xiaojun Chang', 'Gang Pan', 'Shurong Dong', 'Kaipeng Zhang', 'Yang You'], 'affiliations': ['MBZUAI', 'NJU', 'NUS', 'RIT', 'SII', 'Shanghai AI Lab', 'USTC', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.05397.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#multimodal', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Мысленное редактирование изображений становится реальностью', 'desc': 'LoongX - это новый подход к редактированию изображений без использования рук, основанный на мультимодальных нейрофизиологических сигналах и диффузионных моделях. Система использует современные методы машинного обучения, включая нейронные сети с пространством состояний и динамическое слияние признаков, для обработки сигналов ЭЭГ, фНИРС, ФПГ и движений головы. LoongX достигает производительности, сравнимой с методами на основе текста, и превосходит их при комбинировании с речью. Этот подход открывает новые возможности для интуитивного редактирования изображений и развития когнитивно-управляемых творческих технологий.'}, 'en': {'title': 'Hands-Free Image Editing with Brain Signals!', 'desc': 'LoongX is a novel hands-free image editing system that utilizes multimodal neurophysiological signals, such as EEG and fNIRS, combined with advanced diffusion models. This approach allows users with limited motor control or language abilities to edit images intuitively, without manual input. By integrating a cross-scale state space module and a dynamic gated fusion module, LoongX effectively processes diverse signals to capture user intent and align it with image editing semantics. Experimental results show that LoongX performs comparably to traditional text-driven methods and even surpasses them when incorporating speech signals, demonstrating its potential for accessible creative technologies.'}, 'zh': {'title': 'LoongX：无障碍图像编辑的新方式', 'desc': 'LoongX是一种基于多模态神经生理信号的免手动图像编辑方法，利用扩散模型实现图像编辑。该方法结合了脑机接口（BCI）和生成模型的最新进展，能够帮助运动能力或语言能力有限的用户进行图像编辑。LoongX通过跨尺度状态空间（CS3）模块和动态门控融合（DGF）模块有效处理不同类型的信号，并将其整合到统一的潜在空间中。实验结果表明，LoongX在性能上与基于文本的方法相当，并在结合语音时表现更佳，展示了神经驱动生成模型在图像编辑中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.08801', 'title': 'Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective', 'url': 'https://huggingface.co/papers/2507.08801', 'abstract': 'Lumos-1 is an autoregressive video generator that uses a modified LLM architecture with MM-RoPE and AR-DF to address spatiotemporal correlation and frame-wise loss imbalance, achieving competitive performance with fewer resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.', 'score': 18, 'issue_id': 4792, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'd53fd2bb25db02a0', 'authors': ['Hangjie Yuan', 'Weihua Chen', 'Jun Cen', 'Hu Yu', 'Jingyun Liang', 'Shuning Chang', 'Zhihui Lin', 'Tao Feng', 'Pengwei Liu', 'Jiazheng Xing', 'Hao Luo', 'Jiasheng Tang', 'Fan Wang', 'Yi Yang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.08801.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#video', '#optimization', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео с помощью модифицированных языковых моделей', 'desc': 'Lumos-1 - это авторегрессионный генератор видео, использующий модифицированную архитектуру языковой модели (LLM). Он применяет технологии MM-RoPE и AR-DF для решения проблем пространственно-временной корреляции и дисбаланса покадровых потерь. Модель достигает конкурентоспособной производительности, используя меньше вычислительных ресурсов. Lumos-1 сохраняет архитектуру LLM с минимальными модификациями, что отличает его от существующих подходов к генерации видео.'}, 'en': {'title': 'Lumos-1: Efficient Video Generation with LLM Architecture', 'desc': "Lumos-1 is an innovative autoregressive video generator that enhances the traditional large language model (LLM) architecture to effectively handle video data. It introduces a new method called MM-RoPE, which improves the model's ability to understand spatiotemporal correlations while addressing issues of frame-wise loss imbalance. The model employs a token dependency strategy that respects both intra-frame and inter-frame relationships, ensuring coherent video generation. By utilizing efficient training techniques, Lumos-1 achieves competitive performance with fewer computational resources compared to existing models."}, 'zh': {'title': 'Lumos-1：高效自回归视频生成的新突破', 'desc': 'Lumos-1是一种自回归视频生成器，采用了经过修改的LLM架构，结合了MM-RoPE和AR-DF技术，以解决时空相关性和帧间损失不平衡的问题。该模型在保持LLM架构的基础上，利用3D RoPE来增强时空相关性，并提出了一种新的RoPE方案MM-RoPE，以支持多模态时空数据建模。Lumos-1还采用了一种令牌依赖策略，确保帧内双向性和帧间时间因果关系，从而解决了空间信息冗余导致的帧间损失不平衡问题。通过高效的训练技术，Lumos-1在仅使用48个GPU的情况下，达到了与现有模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.06261', 'title': 'Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities', 'url': 'https://huggingface.co/papers/2507.06261', 'abstract': 'The Gemini 2.X model family offers varying levels of capability and efficiency for complex problem-solving, including multimodal understanding and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.', 'score': 18, 'issue_id': 4800, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '96d7881be5226186', 'authors': ['Gheorghe Comanici', 'Eric Bieber', 'Mike Schaekermann', 'Ice Pasupat', 'Noveen Sachdeva', 'Inderjit Dhillon', 'Marcel Blistein', 'Ori Ram', 'Dan Zhang', 'Evan Rosen', 'Luke Marris', 'Sam Petulla', 'Colin Gaffney', 'Asaf Aharoni', 'Nathan Lintz', 'Tiago Cardal Pais', 'Henrik Jacobsson', 'Idan Szpektor', 'Nan-Jiang Jiang', 'Krishna Haridasan', 'Ahmed Omran', 'Nikunj Saunshi', 'Dara Bahri', 'Gaurav Mishra', 'Eric Chu', 'Toby Boyd', 'Brad Hekman', 'Aaron Parisi', 'Chaoyi Zhang', 'Kornraphop Kawintiranon', 'Tania Bedrax-Weiss', 'Oliver Wang', 'Ya Xu', 'Ollie Purkiss', 'Uri Mendlovic', 'Ilaï Deutel', 'Nam Nguyen', 'Adam Langley', 'Flip Korn', 'Lucia Rossazza', 'Alexandre Ramé', 'Sagar Waghmare', 'Helen Miller', 'Vaishakh Keshava', 'Ying Jian', 'Xiaofan Zhang', 'Raluca Ada Popa', 'Kedar Dhamdhere', 'Blaž Bratanič', 'Kyuyeun Kim', 'Terry Koo', 'Ferran Alet', 'Yi-ting Chen', 'Arsha Nagrani', 'Hannah Muckenhirn', 'Zhiyuan Zhang', 'Corbin Quick', 'Filip Pavetić', 'Duc Dung Nguyen', 'Joao Carreira', 'Michael Elabd', 'Haroon Qureshi', 'Fabian Mentzer', 'Yao-Yuan Yang', 'Danielle Eisenbud', 'Anmol Gulati', 'Ellie Talius', 'Eric Ni', 'Sahra Ghalebikesabi', 'Edouard Yvinec', 'Alaa Saade', 'Thatcher Ulrich', 'Lorenzo Blanco', 'Dan A. Calian', 'Muhuan Huang', 'Aäron van den Oord', 'Naman Goyal', 'Terry Chen', 'Praynaa Rawlani', 'Christian Schallhart', 'Swachhand Lokhande', 'Xianghong Luo', 'Jyn Shan', 'Ceslee Montgomery', 'Victoria Krakovna', 'Federico Piccinini', 'Omer Barak', 'Jingyu Cui', 'Yiling Jia', 'Mikhail Dektiarev', 'Alexey Kolganov', 'Shiyu Huang', 'Zhe Chen', 'Xingyu Wang', 'Jessica Austin', 'Peter de Boursac', 'Evgeny Sluzhaev', 'Frank Ding', 'Huijian Li', 'Surya Bhupatiraju', 'Mohit Agarwal', 'Sławek Kwasiborski', 'Paramjit Sandhu', 'Patrick Siegler', 'Ahmet Iscen', 'Eyal Ben-David', 'Shiraz Butt', 'Miltos Allamanis', 'Seth Benjamin', 'Robert Busa-Fekete', 'Felix Hernandez-Campos', 'Sasha Goldshtein', 'Matt Dibb', 'Weiyang Zhang', 'Annie Marsden', 'Carey Radebaugh', 'Stephen Roller', 'Abhishek Nayyar', 'Jacob Austin', 'Tayfun Terzi', 'Bhargav Kanagal Shamanna', 'Pete Shaw', 'Aayush Singh', 'Florian Luisier', 'Artur Mendonça', 'Vaibhav Aggarwal', 'Larisa Markeeva', 'Claudio Fantacci', 'Sergey Brin', 'HyunJeong Choe', 'Guanyu Wang', 'Hartwig Adam', 'Avigail Dabush', 'Tatsuya Kiyono', 'Eyal Marcus', 'Jeremy Cole', 'Theophane Weber', 'Hongrae Lee', 'Ronny Huang', 'Alex Muzio', 'Leandro Kieliger', 'Maigo Le', 'Courtney Biles', 'Long Le', 'Archit Sharma', 'Chengrun Yang', 'Avery Lamp', 'Dave Dopson', 'Nate Hurley', 'Katrina Xinyi Xu', 'Zhihao Shan', 'Shuang Song', 'Jiewen Tan', 'Alexandre Senges', 'George Zhang', 'Chong You', 'Yennie Jun', 'David Raposo', 'Susanna Ricco', 'Xuan Yang', 'Weijie Chen', 'Prakhar Gupta', 'Arthur Szlam', 'Kevin Villela', 'Chun-Sung Ferng', 'Daniel Kasenberg', 'Chen Liang', 'Rui Zhu', 'Arunachalam Narayanaswamy', 'Florence Perot', 'Paul Pucciarelli', 'Anna Shekhawat', 'Alexey Stern', 'Rishikesh Ingale', 'Stefani Karp', 'Sanaz Bahargam', 'Adrian Goedeckemeyer', 'Jie Han', 'Sicheng Li', 'Andrea Tacchetti', 'Dian Yu', 'Abhishek Chakladar', 'Zhiying Zhang', 'Mona El Mahdy', 'Xu Gao', 'Dale Johnson', 'Samrat Phatale', 'AJ Piergiovanni', 'Hyeontaek Lim', 'Clement Farabet', 'Carl Lebsack', 'Theo Guidroz', 'John Blitzer', 'Nico Duduta', 'David Madras', 'Steve Li', 'Daniel von Dincklage', 'Xin Li', 'Mahdis Mahdieh', 'George Tucker', 'Ganesh Jawahar', 'Owen Xiao', 'Danny Tarlow', 'Robert Geirhos', 'Noam Velan', 'Daniel Vlasic', 'Kalesha Bullard', 'SK Park', 'Nishesh Gupta', 'Kellie Webster', 'Ayal Hitron', 'Jieming Mao', 'Julian Eisenschlos', 'Laurel Prince', "Nina D'Souza", 'Kelvin Zheng', 'Sara Nasso', 'Gabriela Botea', 'Carl Doersch', 'Caglar Unlu', 'Chris Alberti', 'Alexey Svyatkovskiy', 'Ankita Goel', 'Krzysztof Choromanski', 'Pan-Pan Jiang', 'Richard Nguyen', 'Four Flynn', 'Daria Ćurko', 'Peter Chen', 'Nicholas Roth', 'Kieran Milan', 'Caleb Habtegebriel', 'Shashi Narayan', 'Michael Moffitt', 'Jake Marcus', 'Thomas Anthony', 'Brendan McMahan', 'Gowoon Cheon', 'Ruibo Liu', 'Megan Barnes', 'Lukasz Lew', 'Rebeca Santamaria-Fernandez', 'Mayank Upadhyay', 'Arjun Akula', 'Arnar Mar Hrafnkelsson', 'Alvaro Caceres', 'Andrew Bunner', 'Michal Sokolik', 'Subha Puttagunta', 'Lawrence Moore', 'Berivan Isik', 'Jay Hartford', 'Lawrence Chan', 'Pradeep Shenoy', 'Dan Holtmann-Rice', 'Jane Park', 'Fabio Viola', 'Alex Salcianu', 'Sujeevan Rajayogam', 'Ian Stewart-Binks', 'Zelin Wu', 'Richard Everett', 'Xi Xiong', 'Pierre-Antoine Manzagol', 'Gary Leung', 'Carl Saroufim', 'Bo Pang', 'Dawid Wegner', 'George Papamakarios', 'Jennimaria Palomaki', 'Helena Pankov', 'Guangda Lai', 'Guilherme Tubone', 'Shubin Zhao', 'Theofilos Strinopoulos', 'Seth Neel', 'Mingqiu Wang', 'Joe Kelley', 'Li Li', 'Pingmei Xu', 'Anitha Vijayakumar', "Andrea D'olimpio", 'Omer Levy', 'Massimo Nicosia', 'Grigory Rozhdestvenskiy', 'Ni Lao', 'Sirui Xie', 'Yash Katariya', 'Jon Simon', 'Sanjiv Kumar', 'Florian Hartmann', 'Michael Kilgore', 'Jinhyuk Lee', 'Aroma Mahendru', 'Roman Ring', 'Tom Hennigan', 'Fiona Lang', 'Colin Cherry', 'David Steiner', 'Dawsen Hwang', 'Ray Smith', 'Pidong Wang', 'Jeremy Chen', 'Ming-Hsuan Yang', 'Sam Kwei', 'Philippe Schlattner', 'Donnie Kim', 'Ganesh Poomal Girirajan', 'Nikola Momchev', 'Ayushi Agarwal', 'Xingyi Zhou', 'Ilkin Safarli', 'Zachary Garrett', 'AJ Pierigiovanni', 'Sarthak Jauhari', 'Alif Raditya Rochman', 'Shikhar Vashishth', 'Quan Yuan', 'Christof Angermueller', 'Jon Blanton', 'Xinying Song', 'Nitesh Bharadwaj Gundavarapu', 'Thi Avrahami', 'Maxine Deines', 'Subhrajit Roy', 'Manish Gupta', 'Christopher Semturs', 'Shobha Vasudevan', 'Aditya Srikanth Veerubhotla', 'Shriya Sharma', 'Josh Jacob', 'Zhen Yang', 'Andreas Terzis', 'Dan Karliner', 'Auriel Wright', 'Tania Rojas-Esponda', 'Ashley Brown', 'Abhijit Guha Roy', 'Pawan Dogra', 'Andrei Kapishnikov', 'Peter Young', 'Wendy Kan', 'Vinodh Kumar Rajendran', 'Maria Ivanova', 'Salil Deshmukh', 'Chia-Hua Ho', 'Mike Kwong', 'Stav Ginzburg', 'Annie Louis', 'KP Sawhney', 'Slav Petrov', 'Jing Xie', 'Yunfei Bai', 'Georgi Stoyanov', 'Alex Fabrikant', 'Rajesh Jayaram', 'Yuqi Li', 'Joe Heyward', 'Justin Gilmer', 'Yaqing Wang', 'Radu Soricut', 'Luyang Liu', 'Qingnan Duan', 'Jamie Hayes', "Maura O'Brien", 'Gaurav Singh Tomar', 'Sivan Eiger', 'Bahar Fatemi', 'Jeffrey Hui', 'Catarina Barros', 'Adaeze Chukwuka', 'Alena Butryna', 'Saksham Thakur', 'Austin Huang', 'Zhufeng Pan', 'Haotian Tang', 'Serkan Cabi', 'Tulsee Doshi', 'Michiel Bakker', 'Sumit Bagri', 'Ruy Ley-Wild', 'Adam Lelkes', 'Jennie Lees', 'Patrick Kane', 'David Greene', 'Shimu Wu', 'Jörg Bornschein', 'Gabriela Surita', 'Sarah Hodkinson', 'Fangtao Li', 'Chris Hidey', 'Sébastien Pereira', 'Sean Ammirati', 'Phillip Lippe', 'Adam Kraft', 'Pu Han', 'Sebastian Gerlach', 'Zifeng Wang', 'Liviu Panait', 'Feng Han', 'Brian Farris', 'Yingying Bi', 'Hannah DeBalsi', 'Miaosen Wang', 'Gladys Tyen', 'James Cohan', 'Susan Zhang', 'Jarred Barber', 'Da-Woon Chung', 'Jaeyoun Kim', 'Markus Kunesch', 'Steven Pecht', 'Nami Akazawa', 'Abe Friesen', 'James Lyon', 'Ali Eslami', 'Junru Wu', 'Jie Tan', 'Yue Song', 'Ravi Kumar', 'Chris Welty', 'Ilia Akolzin', 'Gena Gibson', 'Sean Augenstein', 'Arjun Pillai', 'Nancy Yuen', 'Du Phan', 'Xin Wang', 'Iain Barr', 'Heiga Zen', 'Nan Hua', 'Casper Liu', 'Jilei Jerry Wang', 'Tanuj Bhatia', 'Hao Xu', 'Oded Elyada', 'Pushmeet Kohli', 'Mirek Olšák', 'Ke Chen', 'Azalia Mirhoseini', 'Noam Shazeer', 'Shoshana Jakobovits', 'Maggie Tran', 'Nolan Ramsden', 'Tarun Bharti', 'Fred Alcober', 'Yunjie Li', 'Shilpa Shetty', 'Jing Chen', 'Dmitry Kalashnikov', 'Megha Nawhal', 'Sercan Arik', 'Hanwen Chen', 'Michiel Blokzijl', 'Shubham Gupta', 'James Rubin', 'Rigel Swavely', 'Sophie Bridgers', 'Ian Gemp', 'Chen Su', 'Arun Suggala', 'Juliette Pluto', 'Mary Cassin', 'Alain Vaucher', 'Kaiyang Ji', 'Jiahao Cai', 'Andrew Audibert', 'Animesh Sinha', 'David Tian', 'Efrat Farkash', 'Amy Hua', 'Jilin Chen', 'Duc-Hieu Tran', 'Edward Loper', 'Nicole Brichtova', 'Lara McConnaughey', 'Ballie Sandhu', 'Robert Leland', 'Doug DeCarlo', 'Andrew Over', 'James Huang', 'Xing Wu', 'Connie Fan', 'Eric Li', 'Yun Lei', 'Deepak Sharma', 'Cosmin Paduraru', 'Luo Yu', 'Matko Bošnjak', 'Phuong Dao', 'Min Choi', 'Sneha Kudugunta', 'Jakub Adamek', 'Carlos Guía', 'Ali Khodaei', 'Jie Feng', 'Wenjun Zeng', 'David Welling', 'Sandeep Tata', 'Christina Butterfield', 'Andrey Vlasov', 'Seliem El-Sayed', 'Swaroop Mishra', 'Tara Sainath', 'Shentao Yang', 'RJ Skerry-Ryan', 'Jeremy Shar', 'Robert Berry', 'Arunkumar Rajendran', 'Arun Kandoor', 'Andrea Burns', 'Deepali Jain', 'Tom Stone', 'Wonpyo Park', 'Shibo Wang', 'Albin Cassirer', 'Guohui Wang', 'Hayato Kobayashi', 'Sergey Rogulenko', 'Vineetha Govindaraj', 'Mikołaj Rybiński', 'Nadav Olmert', 'Colin Evans', 'Po-Sen Huang', 'Kelvin Xu', 'Premal Shah', 'Terry Thurk', 'Caitlin Sikora', 'Mu Cai', 'Jin Xie', 'Elahe Dabir', 'Saloni Shah', 'Norbert Kalb', 'Carrie Zhang', 'Shruthi Prabhakara', 'Amit Sabne', 'Artiom Myaskovsky', 'Vikas Raunak', 'Blanca Huergo', 'Behnam Neyshabur', 'Jon Clark', 'Ye Zhang', 'Shankar Krishnan', 'Eden Cohen', 'Dinesh Tewari', 'James Lottes', 'Yumeya Yamamori', 'Hui Elena Li', 'Mohamed Elhawaty', 'Ada Maksutaj Oflazer', 'Adrià Recasens', 'Sheryl Luo', 'Duy Nguyen', 'Taylor Bos', 'Kalyan Andra', 'Ana Salazar', 'Ed Chi', 'Jeongwoo Ko', 'Matt Ginsberg', 'Anders Andreassen', 'Anian Ruoss', 'Todor Davchev', 'Elnaz Davoodi', 'Chenxi Liu', 'Min Kim', 'Santiago Ontanon', 'Chi Ming To', 'Dawei Jia', 'Rosemary Ke', 'Jing Wang', 'Anna Korsun', 'Moran Ambar', 'Ilya Kornakov', 'Irene Giannoumis', 'Toni Creswell', 'Denny Zhou', 'Yi Su', 'Ishaan Watts', 'Aleksandr Zaks', 'Evgenii Eltyshev', 'Ziqiang Feng', 'Sidharth Mudgal', 'Alex Kaskasoli', 'Juliette Love', 'Kingshuk Dasgupta', 'Sam Shleifer', 'Richard Green', 'Sungyong Seo', 'Chansoo Lee', 'Dale Webster', 'Prakash Shroff', 'Ganna Raboshchuk', 'Isabel Leal', 'James Manyika', 'Sofia Erell', 'Daniel Murphy', 'Zhisheng Xiao', 'Anton Bulyenov', 'Julian Walker', 'Mark Collier', 'Matej Kastelic', 'Nelson George', 'Sushant Prakash', 'Sailesh Sidhwani', 'Alexey Frolov', 'Steven Hansen', 'Petko Georgiev', 'Tiberiu Sosea', 'Chris Apps', 'Aishwarya Kamath', 'David Reid', 'Emma Cooney', 'Charlotte Magister', 'Oriana Riva', 'Alec Go', 'Pu-Chin Chen', 'Sebastian Krause', 'Nir Levine', 'Marco Fornoni', 'Ilya Figotin', 'Nick Roy', 'Parsa Mahmoudieh', 'Vladimir Magay', 'Mukundan Madhavan', 'Jin Miao', 'Jianmo Ni', 'Yasuhisa Fujii', 'Ian Chou', 'George Scrivener', 'Zak Tsai', 'Siobhan Mcloughlin', 'Jeremy Selier', 'Sandra Lefdal', 'Jeffrey Zhao', 'Abhijit Karmarkar', 'Kushal Chauhan', 'Shivanker Goel', 'Zhaoyi Zhang', 'Vihan Jain', 'Parisa Haghani', 'Mostafa Dehghani', 'Jacob Scott', 'Erin Farnese', 'Anastasija Ilić', 'Steven Baker', 'Julia Pawar', 'Li Zhong', 'Josh Camp', 'Yoel Zeldes', 'Shravya Shetty', 'Anand Iyer', 'Vít Listík', 'Jiaxian Guo', 'Luming Tang', 'Mark Geller', 'Simon Bucher', 'Yifan Ding', 'Hongzhi Shi', 'Carrie Muir', 'Dominik Grewe', 'Ramy Eskander', 'Octavio Ponce', 'Boqing Gong', 'Derek Gasaway', 'Samira Khan', 'Umang Gupta', 'Angelos Filos', 'Weicheng Kuo', 'Klemen Kloboves', 'Jennifer Beattie', 'Christian Wright', 'Leon Li', 'Alicia Jin', 'Sandeep Mariserla', 'Miteyan Patel', 'Jens Heitkaemper', 'Dilip Krishnan', 'Vivek Sharma', 'David Bieber', 'Christian Frank', 'John Lambert', 'Paul Caron', 'Martin Polacek', 'Mai Giménez', 'Himadri Choudhury', 'Xing Yu', 'Sasan Tavakkol', 'Arun Ahuja', 'Franz Och', 'Rodolphe Jenatton', 'Wojtek Skut', 'Bryan Richter', 'David Gaddy', 'Andy Ly', 'Misha Bilenko', 'Megh Umekar', 'Ethan Liang', 'Martin Sevenich', 'Mandar Joshi', 'Hassan Mansoor', 'Rebecca Lin', 'Sumit Sanghai', 'Abhimanyu Singh', 'Xiaowei Li', 'Sudheendra Vijayanarasimhan', 'Zaheer Abbas', 'Yonatan Bitton', 'Hansa Srinivasan', 'Manish Reddy Vuyyuru', 'Alexander Frömmgen', 'Yanhua Sun', 'Ralph Leith', 'Alfonso Castaño', 'DJ Strouse', 'Le Yan', 'Austin Kyker', 'Satish Kambala', 'Mary Jasarevic', 'Thibault Sellam', 'Chao Jia', 'Alexander Pritzel', 'Raghavender R', 'Huizhong Chen', 'Natalie Clay', 'Sudeep Gandhe', 'Sean Kirmani', 'Sayna Ebrahimi', 'Hannah Kirkwood', 'Jonathan Mallinson', 'Chao Wang', 'Adnan Ozturel', 'Kuo Lin', 'Shyam Upadhyay', 'Vincent Cohen-Addad', 'Sean Purser-haskell', 'Yichong Xu', 'Ebrahim Songhori', 'Babi Seal', 'Alberto Magni', 'Almog Gueta', 'Tingting Zou', 'Guru Guruganesh', 'Thais Kagohara', 'Hung Nguyen', 'Khalid Salama', 'Alejandro Cruzado Ruiz', 'Justin Frye', 'Zhenkai Zhu', 'Matthias Lochbrunner', 'Simon Osindero', 'Wentao Yuan', 'Lisa Lee', 'Aman Prasad', 'Lam Nguyen Thiet', 'Daniele Calandriello', 'Victor Stone', 'Qixuan Feng', 'Han Ke', 'Maria Voitovich', 'Geta Sampemane', 'Lewis Chiang', 'Ling Wu', 'Alexander Bykovsky', 'Matt Young', 'Luke Vilnis', 'Ishita Dasgupta', 'Aditya Chawla', 'Qin Cao', 'Bowen Liang', 'Daniel Toyama', 'Szabolcs Payrits', 'Anca Stefanoiu', 'Dimitrios Vytiniotis', 'Ankesh Anand', 'Tianxiao Shen', 'Blagoj Mitrevski', 'Michael Tschannen', 'Sreenivas Gollapudi', 'Aishwarya P S', 'José Leal', 'Zhe Shen', 'Han Fu', 'Wei Wang', 'Arvind Kannan', 'Doron Kukliansky', 'Sergey Yaroshenko', 'Svetlana Grant', 'Umesh Telang', 'David Wood', 'Alexandra Chronopoulou', 'Alexandru Ţifrea', 'Tao Zhou', "Tony Tu\\'ân Nguy\\~ên", 'Muge Ersoy', 'Anima Singh', 'Meiyan Xie', 'Emanuel Taropa', 'Woohyun Han', 'Eirikur Agustsson', 'Andrei Sozanschi', 'Hui Peng', 'Alex Chen', 'Yoel Drori', 'Efren Robles', 'Yang Gao', 'Xerxes Dotiwalla', 'Ying Chen', 'Anudhyan Boral', 'Alexei Bendebury', 'John Nham', 'Chris Tar', 'Luis Castro', 'Jiepu Jiang', 'Canoee Liu', 'Felix Halim', 'Jinoo Baek', 'Andy Wan', 'Jeremiah Liu', 'Yuan Cao', 'Shengyang Dai', 'Trilok Acharya', 'Ruoxi Sun', 'Fuzhao Xue', 'Saket Joshi', 'Morgane Lustman', 'Yongqin Xian', 'Rishabh Joshi', 'Deep Karkhanis', 'Nora Kassner', 'Jamie Hall', 'Xiangzhuo Ding', 'Gan Song', 'Gang Li', 'Chen Zhu', 'Yana Kulizhskaya', 'Bin Ni', 'Alexey Vlaskin', 'Solomon Demmessie', 'Lucio Dery', 'Salah Zaiem', 'Yanping Huang', 'Cindy Fan', 'Felix Gimeno', 'Ananth Balashankar', 'Koji Kojima', 'Hagai Taitelbaum', 'Maya Meng', 'Dero Gharibian', 'Sahil Singla', 'Wei Chen', 'Ambrose Slone', 'Guanjie Chen', 'Sujee Rajayogam', 'Max Schumacher', 'Suyog Kotecha', 'Rory Blevins', 'Qifei Wang', 'Mor Hazan Taege', 'Alex Morris', 'Xin Liu', 'Fayaz Jamil', 'Richard Zhang', 'Pratik Joshi', 'Ben Ingram', 'Tyler Liechty', 'Ahmed Eleryan', 'Scott Baird', 'Alex Grills', 'Gagan Bansal', 'Shan Han', 'Kiran Yalasangi', 'Shawn Xu', 'Majd Al Merey', 'Isabel Gao', 'Felix Weissenberger', 'Igor Karpov', 'Robert Riachi', 'Ankit Anand', 'Gautam Prasad', 'Kay Lamerigts', 'Reid Hayes', 'Jamie Rogers', 'Mandy Guo', 'Ashish Shenoy', 'Qiong Q Hu', 'Kyle He', 'Yuchen Liu', 'Polina Zablotskaia', 'Sagar Gubbi', 'Yifan Chang', 'Jay Pavagadhi', 'Kristian Kjems', 'Archita Vadali', 'Diego Machado', 'Yeqing Li', 'Renshen Wang', 'Dipankar Ghosh', 'Aahil Mehta', 'Dana Alon', 'George Polovets', 'Alessio Tonioni', 'Nate Kushman', "Joel D'sa", 'Lin Zhuo', 'Allen Wu', 'Rohin Shah', 'John Youssef', 'Jiayu Ye', 'Justin Snyder', 'Karel Lenc', 'Senaka Buthpitiya', 'Matthew Tung', 'Jichuan Chang', 'Tao Chen', 'David Saxton', 'Jenny Lee', 'Lydia Lihui Zhang', 'James Qin', 'Prabakar Radhakrishnan', 'Maxwell Chen', 'Piotr Ambroszczyk', 'Metin Toksoz-Exley', 'Yan Zhong', 'Nitzan Katz', "Brendan O'Donoghue", 'Tamara von Glehn', 'Adi Gerzi Rosenthal', 'Aga Świetlik', 'Xiaokai Zhao', 'Nick Fernando', 'Jinliang Wei', 'Jieru Mei', 'Sergei Vassilvitskii', 'Diego Cedillo', 'Pranjal Awasthi', 'Hui Zheng', 'Koray Kavukcuoglu', 'Itay Laish', 'Joseph Pagadora', 'Marc Brockschmidt', 'Christopher A. Choquette-Choo', 'Arunkumar Byravan', 'Yifeng Lu', 'Xu Chen', 'Mia Chen', 'Kenton Lee', 'Rama Pasumarthi', 'Sijal Bhatnagar', 'Aditya Shah', 'Qiyin Wu', 'Zhuoyuan Chen', 'Zack Nado', 'Bartek Perz', 'Zixuan Jiang', 'David Kao', 'Ganesh Mallya', 'Nino Vieillard', 'Lantao Mei', 'Sertan Girgin', 'Mandy Jordan', 'Yeongil Ko', 'Alekh Agarwal', 'Yaxin Liu', 'Yasemin Altun', 'Raoul de Liedekerke', 'Anastasios Kementsietsidis', 'Daiyi Peng', 'Dangyi Liu', 'Utku Evci', 'Peter Humphreys', 'Austin Tarango', 'Xiang Deng', 'Yoad Lewenberg', 'Kevin Aydin', 'Chengda Wu', 'Bhavishya Mittal', 'Tsendsuren Munkhdalai', 'Kleopatra Chatziprimou', 'Rodrigo Benenson', 'Uri First', 'Xiao Ma', 'Jinning Li', 'Armand Joulin', 'Hamish Tomlinson', 'Tingnan Zhang', 'Milad Nasr', 'Zhi Hong', 'Michaël Sander', 'Lisa Anne Hendricks', 'Anuj Sharma', 'Andrew Bolt', 'Eszter Vértes', 'Jiri Simsa', 'Tomer Levinboim', 'Olcan Sercinoglu', 'Divyansh Shukla', 'Austin Wu', 'Craig Swanson', 'Danny Vainstein', 'Fan Bu', 'Bo Wang', 'Ryan Julian', 'Charles Yoon', 'Sergei Lebedev', 'Antonious Girgis', 'Bernd Bandemer', 'David Du', 'Todd Wang', 'Xi Chen', 'Ying Xiao', 'Peggy Lu', 'Natalie Ha', 'Vlad Ionescu', 'Simon Rowe', 'Josip Matak', 'Federico Lebron', 'Andreas Steiner', 'Lalit Jain', 'Manaal Faruqui', 'Nicolas Lacasse', 'Georgie Evans', 'Neesha Subramaniam', 'Dean Reich', 'Giulia Vezzani', 'Aditya Pandey', 'Joe Stanton', 'Tianhao Zhou', 'Liam McCafferty', 'Henry Griffiths', 'Verena Rieser', 'Soheil Hassas Yeganeh', 'Eleftheria Briakou', 'Lu Huang', 'Zichuan Wei', 'Liangchen Luo', 'Erik Jue', 'Gabby Wang', 'Victor Cotruta', 'Myriam Khan', 'Jongbin Park', 'Qiuchen Guo', 'Peiran Li', 'Rong Rong', 'Diego Antognini', 'Anastasia Petrushkina', 'Chetan Tekur', 'Eli Collins', 'Parul Bhatia', 'Chester Kwak', 'Wenhu Chen', 'Arvind Neelakantan', 'Immanuel Odisho', 'Sheng Peng', 'Vincent Nallatamby', 'Vaibhav Tulsyan', 'Fabian Pedregosa', 'Peng Xu', 'Raymond Lin', 'Yulong Wang', 'Emma Wang', 'Sholto Douglas', 'Reut Tsarfaty', 'Elena Gribovskaya', 'Renga Aravamudhan', 'Manu Agarwal', 'Mara Finkelstein', 'Qiao Zhang', 'Elizabeth Cole', 'Phil Crone', 'Sarmishta Velury', 'Anil Das', 'Chris Sauer', 'Luyao Xu', 'Danfeng Qin', 'Chenjie Gu', 'Dror Marcus', 'CJ Zheng', 'Wouter Van Gansbeke', 'Sobhan Miryoosefi', 'Haitian Sun', 'YaGuang Li', 'Charlie Chen', 'Jae Yoo', 'Pavel Dubov', 'Alex Tomala', 'Adams Yu', 'Paweł Wesołowski', 'Alok Gunjan', 'Eddie Cao', 'Jiaming Luo', 'Nikhil Sethi', 'Arkadiusz Socala', 'Laura Graesser', 'Tomas Kocisky', 'Arturo BC', 'Minmin Chen', 'Edward Lee', 'Sophie Wang', 'Weize Kong', 'Qiantong Xu', 'Nilesh Tripuraneni', 'Yiming Li', 'Xinxin Yu', 'Allen Porter', 'Paul Voigtlaender', 'Biao Zhang', 'Arpi Vezer', 'Sarah York', 'Qing Wei', 'Geoffrey Cideron', 'Mark Kurzeja', 'Seungyeon Kim', 'Benny Li', 'Angéline Pouget', 'Hyo Lee', 'Kaspar Daugaard', 'Yang Li', 'Dave Uthus', 'Aditya Siddhant', 'Paul Cavallaro', 'Sriram Ganapathy', 'Maulik Shah', 'Rolf Jagerman', 'Jeff Stanway', 'Piermaria Mendolicchio', 'Li Xiao', 'Kayi Lee', 'Tara Thompson', 'Shubham Milind Phal', 'Jason Chase', 'Sun Jae Lee', 'Adrian N Reyes', 'Disha Shrivastava', 'Zhen Qin', 'Roykrong Sukkerd', 'Seth Odoom', 'Lior Madmoni', 'John Aslanides', 'Jonathan Herzig', 'Elena Pochernina', 'Sheng Zhang', 'Parker Barnes', 'Daisuke Ikeda', 'Qiujia Li', 'Shuo-yiin Chang', 'Shakir Mohamed', 'Jim Sproch', 'Richard Powell', 'Bidisha Samanta', 'Domagoj Ćevid', 'Anton Kovsharov', 'Shrestha Basu Mallick', 'Srinivas Tadepalli', 'Anne Zheng', 'Kareem Ayoub', 'Andreas Noever', 'Christian Reisswig', 'Zhuo Xu', 'Junhyuk Oh', 'Martin Matysiak', 'Tim Blyth', 'Shereen Ashraf', 'Julien Amelot', 'Boone Severson', 'Michele Bevilacqua', 'Motoki Sano', 'Ethan Dyer', 'Ofir Roval', 'Anu Sinha', 'Yin Zhong', 'Sagi Perel', 'Tea Sabolić', 'Johannes Mauerer', 'Willi Gierke', 'Mauro Verzetti', 'Rodrigo Cabrera', 'Alvin Abdagic', 'Steven Hemingray', 'Austin Stone', 'Jong Lee', 'Farooq Ahmad', 'Karthik Raman', 'Lior Shani', 'Jonathan Lai', 'Orhan Firat', 'Nathan Waters', 'Eric Ge', 'Mo Shomrat', 'Himanshu Gupta', 'Rajeev Aggarwal', 'Tom Hudson', 'Bill Jia', 'Simon Baumgartner', 'Palak Jain', 'Joe Kovac', 'Junehyuk Jung', 'Ante Žužul', 'Will Truong', 'Morteza Zadimoghaddam', 'Songyou Peng', 'Marco Liang', 'Rachel Sterneck', 'Balaji Lakshminarayanan', 'Machel Reid', 'Oliver Woodman', 'Tong Zhou', 'Jianling Wang', 'Vincent Coriou', 'Arjun Narayanan', 'Jay Hoover', 'Yenai Ma', 'Apoorv Jindal', 'Clayton Sanford', 'Doug Reid', 'Swaroop Ramaswamy', 'Alex Kurakin', 'Roland Zimmermann', 'Yana Lunts', 'Dragos Dena', 'Zalán Borsos', 'Vered Cohen', 'Shujian Zhang', 'Will Grathwohl', 'Robert Dadashi', 'Morgan Redshaw', 'Joshua Kessinger', 'Julian Odell', 'Silvano Bonacina', 'Zihang Dai', 'Grace Chen', 'Ayush Dubey', 'Pablo Sprechmann', 'Mantas Pajarskas', 'Wenxuan Zhou', 'Niharika Ahuja', 'Tara Thomas', 'Martin Nikoltchev', 'Matija Kecman', 'Bharath Mankalale', 'Andrey Ryabtsev', 'Jennifer She', 'Christian Walder', 'Jiaming Shen', 'Lu Li', 'Carolina Parada', 'Sheena Panthaplackel', 'Okwan Kwon', 'Matt Lawlor', 'Utsav Prabhu', 'Yannick Schroecker', "Marc'aurelio Ranzato", 'Pete Blois', 'Iurii Kemaev', 'Ting Yu', 'Dmitry Lepikhin', 'Hao Xiong', 'Sahand Sharifzadeh', 'Oleaser Johnson', 'Jeremiah Willcock', 'Rui Yao', 'Greg Farquhar', 'Sujoy Basu', 'Hidetoshi Shimokawa', 'Nina Anderson', 'Haiguang Li', 'Khiem Pham', 'Yizhong Liang', 'Sebastian Borgeaud', 'Alexandre Moufarek', 'Hideto Kazawa', 'Blair Kutzman', 'Marcin Sieniek', 'Sara Smoot', 'Ruth Wang', 'Natalie Axelsson', 'Nova Fallen', 'Prasha Sundaram', 'Yuexiang Zhai', 'Varun Godbole', 'Petros Maniatis', 'Alek Wang', 'Ilia Shumailov', 'Santhosh Thangaraj', 'Remi Crocker', 'Nikita Gupta', 'Gang Wu', 'Phil Chen', 'Gellért Weisz', 'Celine Smith', 'Mojtaba Seyedhosseini', 'Boya Fang', 'Xiyang Luo', 'Roey Yogev', 'Zeynep Cankara', 'Andrew Hard', 'Helen Ran', 'Rahul Sukthankar', 'George Necula', 'Gaël Liu', 'Honglong Cai', 'Praseem Banzal', 'Daniel Keysers', 'Sanjay Ghemawat', 'Connie Tao', 'Emma Dunleavy', 'Aditi Chaudhary', 'Wei Li', 'Maciej Mikuła', 'Chen-Yu Lee', 'Tiziana Refice', 'Krishna Somandepalli', 'Alexandre Fréchette', 'Dan Bahir', 'John Karro', 'Keith Rush', 'Sarah Perrin', 'Bill Rosgen', 'Xiaomeng Yang', 'Clara Huiyi Hu', 'Mahmoud Alnahlawi', 'Justin Mao-Jones', 'Roopal Garg', 'Hoang Nguyen', 'Bat-Orgil Batsaikhan', 'Iñaki Iturrate', 'Anselm Levskaya', 'Avi Singh', 'Ashyana Kachra', 'Tony Lu', 'Denis Petek', 'Zheng Xu', 'Mark Graham', 'Lukas Zilka', 'Yael Karov', 'Marija Kostelac', 'Fangyu Liu', 'Yaohui Guo', 'Weiyue Wang', 'Bernd Bohnet', 'Emily Pitler', 'Tony Bruguier', 'Keisuke Kinoshita', 'Chrysovalantis Anastasiou', 'Nilpa Jha', 'Ting Liu', 'Jerome Connor', 'Phil Wallis', 'Philip Pham', 'Eric Bailey', 'Shixin Li', 'Heng-Tze Cheng', 'Sally Ma', 'Haiqiong Li', 'Akanksha Maurya', 'Kate Olszewska', 'Manfred Warmuth', 'Christy Koh', 'Dominik Paulus', 'Siddhartha Reddy Jonnalagadda', 'Enrique Piqueras', 'Ali Elqursh', 'Geoff Brown', 'Hadar Shemtov', 'Loren Maggiore', 'Fei Xia', 'Ryan Foley', 'Beka Westberg', 'George van den Driessche', 'Livio Baldini Soares', 'Arjun Kar', 'Michael Quinn', 'Siqi Zuo', 'Jialin Wu', 'Kyle Kastner', 'Anna Bortsova', 'Aijun Bai', 'Ales Mikhalap', 'Luowei Zhou', 'Jennifer Brennan', 'Vinay Ramasesh', 'Honglei Zhuang', 'John Maggs', 'Johan Schalkwyk', 'Yuntao Xu', 'Hui Huang', 'Andrew Howard', 'Sasha Brown', 'Linting Xue', 'Gloria Shen', 'Brian Albert', 'Neha Jha', 'Daniel Zheng', 'Varvara Krayvanova', 'Spurthi Amba Hombaiah', 'Olivier Lacombe', 'Gautam Vasudevan', 'Dan Graur', 'Tian Xie', 'Meet Gandhi', 'Bangju Wang', 'Dustin Zelle', 'Harman Singh', 'Dahun Kim', 'Sébastien Cevey', 'Victor Ungureanu', 'Natasha Noy', 'Fei Liu', 'Annie Xie', 'Fangxiaoyu Feng', 'Katerina Tsihlas', 'Daniel Formoso', 'Neera Vats', 'Quentin Wellens', 'Yinan Wang', 'Niket Kumar Bhumihar', 'Samrat Ghosh', 'Matt Hoffman', 'Tom Lieber', 'Oran Lang', 'Kush Bhatia', 'Tom Paine', 'Aroonalok Pyne', 'Ronny Votel', 'Madeleine Clare Elish', 'Benoit Schillings', 'Alex Panagopoulos', 'Haichuan Yang', 'Adam Raveret', 'Zohar Yahav', 'Shuang Liu', 'Dalia El Badawy', 'Nishant Agrawal', 'Mohammed Badawi', 'Mahdi Mirzazadeh', 'Carla Bromberg', 'Fan Ye', 'Chang Liu', 'Tatiana Sholokhova', 'George-Cristian Muraru', 'Gargi Balasubramaniam', 'Jonathan Malmaud', 'Alen Carin', 'Danilo Martins', 'Irina Jurenka', 'Pankil Botadra', 'Dave Lacey', 'Richa Singh', 'Mariano Schain', 'Dan Zheng', 'Isabelle Guyon', 'Victor Lavrenko', 'Seungji Lee', 'Xiang Zhou', 'Demis Hassabis', 'Jeshwanth Challagundla', 'Derek Cheng', 'Nikhil Mehta', 'Matthew Mauger', 'Michela Paganini', 'Pushkar Mishra', 'Kate Lee', 'Zhang Li', 'Lexi Baugher', 'Ondrej Skopek', 'Max Chang', 'Amir Zait', 'Gaurav Menghani', 'Lizzetth Bellot', 'Guangxing Han', 'Jean-Michel Sarr', 'Sharat Chikkerur', 'Himanshu Sahni', 'Rohan Anil', 'Arun Narayanan', 'Chandu Thekkath', 'Daniele Pighin', 'Hana Strejček', 'Marko Velic', 'Fred Bertsch', 'Manuel Tragut', 'Keran Rong', 'Alicia Parrish', 'Kai Bailey', 'Jiho Park', 'Isabela Albuquerque', 'Abhishek Bapna', 'Rajesh Venkataraman', 'Alec Kosik', 'Johannes Griesser', 'Zhiwei Deng', 'Alek Andreev', 'Qingyun Dou', 'Kevin Hui', 'Fanny Wei', 'Xiaobin Yu', 'Lei Shu', 'Avia Aharon', 'David Barker', 'Badih Ghazi', 'Sebastian Flennerhag', 'Chris Breaux', 'Yuchuan Liu', 'Matthew Bilotti', 'Josh Woodward', 'Uri Alon', 'Stephanie Winkler', 'Tzu-Kuo Huang', 'Kostas Andriopoulos', 'João Gabriel Oliveira', 'Penporn Koanantakool', 'Berkin Akin', 'Michael Wunder', 'Cicero Nogueira dos Santos', 'Mohammad Hossein Bateni', 'Lin Yang', 'Dan Horgan', 'Beer Changpinyo', 'Keyvan Amiri', 'Min Ma', 'Dayeong Lee', 'Lihao Liang', 'Anirudh Baddepudi', 'Tejasi Latkar', 'Raia Hadsell', 'Jun Xu', 'Hairong Mu', 'Michael Han', 'Aedan Pope', 'Snchit Grover', 'Frank Kim', 'Ankit Bhagatwala', 'Guan Sun', 'Yamini Bansal', 'Amir Globerson', 'Alireza Nazari', 'Samira Daruki', 'Hagen Soltau', 'Jane Labanowski', 'Laurent El Shafey', 'Matt Harvey', 'Yanif Ahmad', 'Elan Rosenfeld', 'William Kong', 'Etienne Pot', 'Yi-Xuan Tan', 'Aurora Wei', 'Victoria Langston', 'Marcel Prasetya', 'Petar Veličković', 'Richard Killam', 'Robin Strudel', 'Darren Ni', 'Zhenhai Zhu', 'Aaron Archer', 'Kavya Kopparapu', 'Lynn Nguyen', 'Emilio Parisotto', 'Hussain Masoom', 'Sravanti Addepalli', 'Jordan Grimstad', 'Hexiang Hu', 'Joss Moore', 'Avinatan Hassidim', 'Le Hou', 'Mukund Raghavachari', 'Jared Lichtarge', 'Adam R. Brown', 'Hilal Dib', 'Natalia Ponomareva', 'Justin Fu', 'Yujing Zhang', 'Altaf Rahman', 'Joana Iljazi', 'Edouard Leurent', 'Gabriel Dulac-Arnold', 'Cosmo Du', 'Chulayuth Asawaroengchai', 'Larry Jin', 'Ela Gruzewska', 'Ziwei Ji', 'Benigno Uria', 'Daniel De Freitas', 'Paul Barham', 'Lauren Beltrone', 'Víctor Campos', 'Jun Yan', 'Neel Kovelamudi', 'Arthur Nguyen', 'Elinor Davies', 'Zhichun Wu', 'Zoltan Egyed', 'Kristina Toutanova', 'Nithya Attaluri', 'Hongliang Fei', 'Peter Stys', 'Siddhartha Brahma', 'Martin Izzard', 'Siva Velusamy', 'Scott Lundberg', 'Vincent Zhuang', 'Kevin Sequeira', 'Adam Santoro', 'Ehsan Amid', 'Ophir Aharoni', 'Shuai Ye', 'Mukund Sundararajan', 'Lijun Yu', 'Yu-Cheng Ling', 'Stephen Spencer', 'Hugo Song', 'Josip Djolonga', 'Christo Kirov', 'Sonal Gupta', 'Alessandro Bissacco', 'Clemens Meyer', 'Mukul Bhutani', 'Andrew Dai', 'Weiyi Wang', 'Siqi Liu', 'Ashwin Sreevatsa', 'Qijun Tan', 'Maria Wang', 'Lucy Kim', 'Yicheng Wang', 'Alex Irpan', 'Yang Xiao', 'Stanislav Fort', 'Yifan He', 'Alex Gurney', 'Bryan Gale', 'Yue Ma', 'Monica Roy', 'Viorica Patraucean', 'Taylan Bilal', 'Golnaz Ghiasi', 'Anahita Hosseini', 'Melvin Johnson', 'Zhuowan Li', 'Yi Tay', 'Benjamin Beyret', 'Katie Millican', 'Josef Broder', 'Mayank Lunayach', 'Danny Swisher', 'Eugen Vušak', 'David Parkinson', 'MH Tessler', 'Adi Mayrav Gilady', 'Richard Song', 'Allan Dafoe', 'Yves Raimond', 'Masa Yamaguchi', 'Itay Karo', 'Elizabeth Nielsen', 'Kevin Kilgour', 'Mike Dusenberry', 'Rajiv Mathews', 'Jiho Choi', 'Siyuan Qiao', 'Harsh Mehta', 'Sahitya Potluri', 'Chris Knutsen', 'Jialu Liu', 'Tat Tan', 'Kuntal Sengupta', 'Keerthana Gopalakrishnan', 'Abodunrinwa Toki', 'Mencher Chiang', 'Mike Burrows', 'Grace Vesom', 'Zafarali Ahmed', 'Ilia Labzovsky', 'Siddharth Vashishtha', 'Preeti Singh', 'Ankur Sharma', 'Ada Ma', 'Jinyu Xie', 'Pranav Talluri', 'Hannah Forbes-Pollard', 'Aarush Selvan', 'Joel Wee', 'Loic Matthey', 'Tom Funkhouser', 'Parthasarathy Gopavarapu', 'Lev Proleev', 'Cheng Li', 'Matt Thomas', 'Kashyap Kolipaka', 'Zhipeng Jia', 'Ashwin Kakarla', 'Srinivas Sunkara', 'Joan Puigcerver', 'Suraj Satishkumar Sheth', 'Emily Graves', 'Chen Wang', 'Sadh MNM Khan', 'Kai Kang', 'Shyamal Buch', 'Fred Zhang', 'Omkar Savant', 'David Soergel', 'Kevin Lee', 'Linda Friso', 'Xuanyi Dong', 'Rahul Arya', 'Shreyas Chandrakaladharan', 'Connor Schenck', 'Greg Billock', 'Tejas Iyer', 'Anton Bakalov', 'Leslie Baker', 'Alex Ruiz', 'Angad Chandorkar', 'Trieu Trinh', 'Matt Miecnikowski', 'Yanqi Zhou', 'Yangsibo Huang', 'Jiazhong Nie', 'Ali Shah', 'Ashish Thapliyal', 'Sam Haves', 'Lun Wang', 'Uri Shaham', 'Patrick Morris-Suzuki', 'Soroush Radpour', 'Leonard Berrada', 'Thomas Strohmann', 'Chaochao Yan', 'Jingwei Shen', 'Sonam Goenka', 'Tris Warkentin', 'Petar Dević', 'Dan Belov', 'Albert Webson', 'Madhavi Yenugula', 'Puranjay Datta', 'Jerry Chang', 'Nimesh Ghelani', 'Aviral Kumar', 'Vincent Perot', 'Jessica Lo', 'Yang Song', 'Herman Schmit', 'Jianmin Chen', 'Vasilisa Bashlovkina', 'Xiaoyue Pan', 'Diana Mincu', 'Paul Roit', 'Isabel Edkins', 'Andy Davis', 'Yujia Li', 'Ben Horn', 'Xinjian Li', 'Pradeep Kumar S', 'Eric Doi', 'Wanzheng Zhu', 'Sri Gayatri Sundara Padmanabhan', 'Siddharth Verma', 'Jasmine Liu', 'Heng Chen', 'Mihajlo Velimirović', 'Malcolm Reynolds', 'Priyanka Agrawal', 'Nick Sukhanov', 'Abhinit Modi', 'Siddharth Goyal', 'John Palowitch', 'Nima Khajehnouri', 'Wing Lowe', 'David Klinghoffer', 'Sharon Silver', 'Vinh Tran', 'Candice Schumann', 'Francesco Piccinno', 'Xi Liu', 'Mario Lučić', 'Xiaochen Yang', 'Sandeep Kumar', 'Ajay Kannan', 'Ragha Kotikalapudi', 'Mudit Bansal', 'Fabian Fuchs', 'Mohammad Javad Hosseini', 'Abdelrahman Abdelhamed', 'Dawn Bloxwich', 'Tianhe Yu', 'Ruoxin Sang', 'Gregory Thornton', 'Karan Gill', 'Yuchi Liu', 'Virat Shejwalkar', 'Jason Lin', 'Zhipeng Yan', 'Kehang Han', 'Thomas Buschmann', 'Michael Pliskin', 'Zhi Xing', 'Susheel Tatineni', 'Junlin Zhang', 'Sissie Hsiao', 'Gavin Buttimore', 'Marcus Wu', 'Zefei Li', 'Geza Kovacs', 'Legg Yeung', 'Tao Huang', 'Aaron Cohen', 'Bethanie Brownfield', 'Averi Nowak', 'Mikel Rodriguez', 'Tianze Shi', 'Hado van Hasselt', 'Kevin Cen', 'Deepanway Ghoshal', 'Kushal Majmundar', 'Weiren Yu', 'Warren Weilun Chen', 'Danila Sinopalnikov', 'Hao Zhang', 'Vlado Galić', 'Di Lu', 'Zeyu Zheng', 'Maggie Song', 'Gary Wang', 'Gui Citovsky', 'Swapnil Gawde', 'Isaac Galatzer-Levy', 'David Silver', 'Ivana Balazevic', 'Dipanjan Das', 'Kingshuk Majumder', 'Yale Cong', 'Praneet Dutta', 'Dustin Tran', 'Hui Wan', 'Junwei Yuan', 'Daniel Eppens', 'Alanna Walton', 'Been Kim', 'Harry Ragan', 'James Cobon-Kerr', 'Lu Liu', 'Weijun Wang', 'Bryce Petrini', 'Jack Rae', 'Rakesh Shivanna', 'Yan Xiong', 'Chace Lee', 'Pauline Coquinot', 'Yiming Gu', 'Lisa Patel', 'Blake Hechtman', 'Aviel Boag', 'Orion Jankowski', 'Alex Wertheim', 'Alex Lee', 'Paul Covington', 'Hila Noga', 'Sam Sobell', 'Shanthal Vasanth', 'William Bono', 'Chirag Nagpal', 'Wei Fan', 'Xavier Garcia', 'Kedar Soparkar', 'Aybuke Turker', 'Nathan Howard', 'Sachit Menon', 'Yuankai Chen', 'Vikas Verma', 'Vladimir Pchelin', 'Harish Rajamani', 'Valentin Dalibard', 'Ana Ramalho', 'Yang Guo', 'Kartikeya Badola', 'Seojin Bang', 'Nathalie Rauschmayr', 'Julia Proskurnia', 'Sudeep Dasari', 'Xinyun Chen', 'Mikhail Sushkov', 'Anja Hauth', 'Pauline Sho', 'Abhinav Singh', 'Bilva Chandra', 'Allie Culp', 'Max Dylla', 'Olivier Bachem', 'James Besley', 'Heri Zhao', 'Timothy Lillicrap', 'Wei Wei', 'Wael Al Jishi', 'Ning Niu', 'Alban Rrustemi', 'Raphaël Lopez Kaufman', 'Ryan Poplin', 'Jewel Zhao', 'Minh Truong', 'Shikhar Bharadwaj', 'Ester Hlavnova', 'Eli Stickgold', 'Cordelia Schmid', 'Georgi Stephanov', 'Zhaoqi Leng', 'Frederick Liu', 'Léonard Hussenot', 'Shenil Dodhia', 'Juliana Vicente Franco', 'Lesley Katzen', 'Abhanshu Sharma', 'Sarah Cogan', 'Zuguang Yang', 'Aniket Ray', 'Sergi Caelles', 'Shen Yan', 'Ravin Kumar', 'Daniel Gillick', 'Renee Wong', 'Joshua Ainslie', 'Jonathan Hoech', 'Séb Arnold', 'Dan Abolafia', 'Anca Dragan', 'Ben Hora', 'Grace Hu', 'Alexey Guseynov', 'Yang Lu', 'Chas Leichner', 'Jinmeng Rao', 'Abhimanyu Goyal', 'Nagabhushan Baddi', 'Daniel Hernandez Diaz', 'Tim McConnell', 'Max Bain', 'Jake Abernethy', 'Qiqi Yan', 'Rylan Schaeffer', 'Paul Vicol', 'Will Thompson', 'Montse Gonzalez Arenas', 'Mathias Bellaiche', 'Pablo Barrio', 'Stefan Zinke', 'Riccardo Patana', 'Pulkit Mehta', 'JK Kearns', 'Avraham Ruderman', 'Scott Pollom', "David D'Ambrosio", 'Cath Hope', 'Yang Yu', 'Andrea Gesmundo', 'Kuang-Huei Lee', 'Aviv Rosenberg', 'Yiqian Zhou', 'Yaoyiran Li', 'Drew Garmon', 'Yonghui Wu', 'Safeen Huda', 'Gil Fidel', 'Martin Baeuml', 'Jian Li', 'Phoebe Kirk', 'Rhys May', 'Tao Tu', 'Sara Mc Carthy', 'Toshiyuki Fukuzawa', 'Miranda Aperghis', 'Chih-Kuan Yeh', 'Toshihiro Yoshino', 'Bo Li', 'Austin Myers', 'Kaisheng Yao', 'Ben Limonchik', 'Changwan Ryu', 'Rohun Saxena', 'Alex Goldin', 'Ruizhe Zhao', 'Rocky Rhodes', 'Tao Zhu', 'Divya Tyam', 'Heidi Howard', 'Nathan Byrd', 'Hongxu Ma', 'Yan Wu', 'Ryan Mullins', 'Qingze Wang', 'Aida Amini', 'Sebastien Baur', 'Yiran Mao', 'Subhashini Venugopalan', 'Will Song', 'Wen Ding', 'Paul Collins', 'Sashank Reddi', 'Megan Shum', 'Andrei Rusu', 'Luisa Zintgraf', 'Kelvin Chan', 'Sheela Goenka', 'Mathieu Blondel', 'Michael Collins', 'Renke Pan', 'Marissa Giustina', 'Nikolai Chinaev', 'Christian Schuler', 'Ce Zheng', 'Jonas Valfridsson', 'Alyssa Loo', 'Alex Yakubovich', 'Jamie Smith', 'Tao Jiang', 'Rich Munoz', 'Gabriel Barcik', 'Rishabh Bansal', 'Mingyao Yang', 'Yilun Du', 'Pablo Duque', 'Mary Phuong', 'Alexandra Belias', 'Kunal Lad', 'Zeyu Liu', 'Tal Schuster', 'Karthik Duddu', 'Jieru Hu', 'Paige Kunkle', 'Matthew Watson', 'Jackson Tolins', 'Josh Smith', 'Denis Teplyashin', 'Garrett Bingham', 'Marvin Ritter', 'Marco Andreetto', 'Divya Pitta', 'Mohak Patel', 'Shashank Viswanadha', 'Trevor Strohman', 'Catalin Ionescu', 'Jincheng Luo', 'Yogesh Kalley', 'Jeremy Wiesner', 'Dan Deutsch', 'Derek Lockhart', 'Peter Choy', 'Rumen Dangovski', 'Chawin Sitawarin', 'Cat Graves', 'Tanya Lando', 'Joost van Amersfoort', 'Ndidi Elue', 'Zhouyuan Huo', 'Pooya Moradi', 'Jean Tarbouriech', 'Henryk Michalewski', 'Wenting Ye', 'Eunyoung Kim', 'Alex Druinsky', 'Florent Altché', 'Xinyi Chen', 'Artur Dwornik', 'Da-Cheng Juan', 'Rivka Moroshko', 'Horia Toma', 'Jarrod Kahn', 'Hai Qian', 'Maximilian Sieb', 'Irene Cai', 'Roman Goldenberg', 'Praneeth Netrapalli', 'Sindhu Raghuram', 'Yuan Gong', 'Lijie Fan', 'Evan Palmer', 'Yossi Matias', 'Valentin Gabeur', 'Shreya Pathak', 'Tom Ouyang', 'Don Metzler', 'Geoff Bacon', 'Srinivasan Venkatachary', 'Sridhar Thiagarajan', 'Alex Cullum', 'Eran Ofek', 'Vytenis Sakenas', 'Mohamed Hammad', 'Cesar Magalhaes', 'Mayank Daswani', 'Oscar Chang', 'Ashok Popat', 'Ruichao Li', 'Komal Jalan', 'Yanhan Hou', 'Josh Lipschultz', 'Antoine He', 'Wenhao Jia', 'Pier Giuseppe Sessa', 'Prateek Kolhar', 'William Wong', 'Sumeet Singh', 'Lukas Haas', 'Jay Whang', 'Hanna Klimczak-Plucińska', 'Georges Rotival', 'Grace Chung', 'Yiqing Hua', 'Anfal Siddiqui', 'Nicolas Serrano', 'Dongkai Chen', 'Billy Porter', 'Libin Bai', 'Keshav Shivam', 'Sho Arora', 'Partha Talukdar', 'Tom Cobley', 'Sangnie Bhardwaj', 'Evgeny Gladchenko', 'Simon Green', 'Kelvin Guu', 'Felix Fischer', 'Xiao Wu', 'Eric Wang', 'Achintya Singhal', 'Tatiana Matejovicova', 'James Martens', 'Hongji Li', 'Roma Patel', 'Elizabeth Kemp', 'Jiaqi Pan', 'Lily Wang', 'Blake JianHang Chen', 'Jean-Baptiste Alayrac', 'Navneet Potti', 'Erika Gemzer', 'Eugene Ie', 'Kay McKinney', 'Takaaki Saeki', 'Edward Chou', 'Pascal Lamblin', 'SQ Mah', 'Zach Fisher', 'Martin Chadwick', 'Jon Stritar', 'Obaid Sarvana', 'Andrew Hogue', 'Artem Shtefan', 'Hadi Hashemi', 'Yang Xu', 'Jindong Gu', 'Sharad Vikram', 'Chung-Ching Chang', 'Sabela Ramos', 'Logan Kilpatrick', 'Weijuan Xi', 'Jenny Brennan', 'Yinghao Sun', 'Abhishek Jindal', 'Ionel Gog', 'Dawn Chen', 'Felix Wu', 'Jason Lee', 'Sudhindra Kopalle', 'Srinadh Bhojanapalli', 'Oriol Vinyals', 'Natan Potikha', 'Burcu Karagol Ayan', 'Yuan Yuan', 'Michael Riley', 'Piotr Stanczyk', 'Sergey Kishchenko', 'Bing Wang', 'Dan Garrette', 'Antoine Yang', 'Vlad Feinberg', 'CJ Carey', 'Javad Azizi', 'Viral Shah', 'Erica Moreira', 'Chongyang Shi', 'Josh Feldman', 'Elizabeth Salesky', 'Thomas Lampe', 'Aneesh Pappu', 'Duhyeon Kim', 'Jonas Adler', 'Avi Caciularu', 'Brian Walker', 'Yunhan Xu', 'Yochai Blau', 'Dylan Scandinaro', 'Terry Huang', 'Sam El-Husseini', 'Abhishek Sinha', 'Lijie Ren', 'Taylor Tobin', 'Patrik Sundberg', 'Tim Sohn', 'Vikas Yadav', 'Mimi Ly', 'Emily Xue', 'Jing Xiong', 'Afzal Shama Soudagar', 'Sneha Mondal', 'Nikhil Khadke', 'Qingchun Ren', 'Ben Vargas', 'Stan Bileschi', 'Sarah Chakera', 'Cindy Wang', 'Boyu Wang', 'Yoni Halpern', 'Joe Jiang', 'Vikas Sindhwani', 'Petre Petrov', 'Pranavaraj Ponnuramu', 'Sanket Vaibhav Mehta', 'Yu Watanabe', 'Betty Chan', 'Matheus Wisniewski', 'Trang Pham', 'Jingwei Zhang', 'Conglong Li', 'Dario de Cesare', 'Art Khurshudov', 'Alex Vasiloff', 'Melissa Tan', 'Zoe Ashwood', 'Bobak Shahriari', 'Maryam Majzoubi', 'Garrett Tanzer', 'Olga Kozlova', 'Robin Alazard', 'James Lee-Thorp', 'Nguyet Minh Phu', 'Isaac Tian', 'Junwhan Ahn', 'Andy Crawford', 'Lauren Lax', 'Yuan Shangguan', 'Iftekhar Naim', 'David Ross', 'Oleksandr Ferludin', 'Tongfei Guo', 'Andrea Banino', 'Hubert Soyer', 'Xiaoen Ju', 'Dominika Rogozińska', 'Ishaan Malhi', 'Marcella Valentine', 'Daniel Balle', 'Apoorv Kulshreshtha', 'Maciej Kula', 'Yiwen Song', 'Sophia Austin', 'John Schultz', 'Roy Hirsch', 'Arthur Douillard', 'Apoorv Reddy', 'Michael Fink', 'Summer Yue', 'Khyatti Gupta', 'Adam Zhang', 'Norman Rink', 'Daniel McDuff', 'Lei Meng', 'András György', 'Yasaman Razeghi', 'Ricky Liang', 'Kazuki Osawa', 'Aviel Atias', 'Matan Eyal', 'Tyrone Hill', 'Nikolai Grigorev', 'Zhengdong Wang', 'Nitish Kulkarni', 'Rachel Soh', 'Ivan Lobov', 'Zachary Charles', 'Sid Lall', 'Kazuma Hashimoto', 'Ido Kessler', 'Victor Gomes', 'Zelda Mariet', 'Danny Driess', 'Alessandro Agostini', 'Canfer Akbulut', 'Jingcao Hu', 'Marissa Ikonomidis', 'Emily Caveness', 'Kartik Audhkhasi', 'Saurabh Agrawal', 'Ioana Bica', 'Evan Senter', 'Jayaram Mudigonda', 'Kelly Chen', 'Jingchen Ye', 'Xuanhui Wang', 'James Svensson', 'Philipp Fränken', 'Josh Newlan', 'Li Lao', 'Eva Schnider', 'Sami Alabed', 'Joseph Kready', 'Jesse Emond', 'Afief Halumi', 'Tim Zaman', 'Chengxi Ye', 'Naina Raisinghani', 'Vilobh Meshram', 'Bo Chang', 'Ankit Singh Rawat', 'Axel Stjerngren', 'Sergey Levi', 'Rui Wang', 'Xiangzhu Long', 'Mitchelle Rasquinha', 'Steven Hand', 'Aditi Mavalankar', 'Lauren Agubuzu', 'Sudeshna Roy', 'Junquan Chen', 'Jarek Wilkiewicz', 'Hao Zhou', 'Michal Jastrzebski', 'Qiong Hu', 'Agustin Dal Lago', 'Ramya Sree Boppana', 'Wei-Jen Ko', 'Jennifer Prendki', 'Yao Su', 'Zhi Li', 'Eliza Rutherford', 'Girish Ramchandra Rao', 'Ramona Comanescu', 'Adrià Puigdomènech', 'Qihang Chen', 'Dessie Petrova', 'Christine Chan', 'Vedrana Milutinovic', 'Felipe Tiengo Ferreira', 'Chin-Yi Cheng', 'Ming Zhang', 'Tapomay Dey', 'Sherry Yang', 'Ramesh Sampath', 'Quoc Le', 'Howard Zhou', 'Chu-Cheng Lin', 'Hoi Lam', 'Christine Kaeser-Chen', 'Kai Hui', 'Dean Hirsch', 'Tom Eccles', 'Basil Mustafa', 'Shruti Rijhwani', 'Morgane Rivière', 'Yuanzhong Xu', 'Junjie Wang', 'Xinyang Geng', 'Xiance Si', 'Arjun Khare', 'Cheolmin Kim', 'Vahab Mirrokni', 'Kamyu Lee', 'Khuslen Baatarsukh', 'Nathaniel Braun', 'Lisa Wang', 'Pallavi LV', 'Richard Tanburn', 'Yonghao Zhu', 'Fangda Li', 'Setareh Ariafar', 'Dan Goldberg', 'Ken Burke', 'Daniil Mirylenka', 'Meiqi Guo', 'Olaf Ronneberger', 'Hadas Natalie Vogel', 'Liqun Cheng', 'Nishita Shetty', 'Johnson Jia', 'Thomas Jimma', 'Corey Fry', 'Ted Xiao', 'Martin Sundermeyer', 'Ryan Burnell', 'Yannis Assael', 'Mario Pinto', 'JD Chen', 'Rohit Sathyanarayana', 'Donghyun Cho', 'Jing Lu', 'Rishabh Agarwal', 'Sugato Basu', 'Lucas Gonzalez', 'Dhruv Shah', 'Meng Wei', 'Dre Mahaarachchi', 'Rohan Agrawal', 'Tero Rissa', 'Yani Donchev', 'Ramiro Leal-Cavazos', 'Adrian Hutter', 'Markus Mircea', 'Alon Jacovi', 'Faruk Ahmed', 'Jiageng Zhang', 'Shuguang Hu', 'Bo-Juen Chen', 'Jonni Kanerva', 'Guillaume Desjardins', 'Andrew Lee', 'Nikos Parotsidis', 'Asier Mujika', 'Tobias Weyand', 'Jasper Snoek', 'Jo Chick', 'Kai Chen', 'Paul Chang', 'Ethan Mahintorabi', 'Zi Wang', 'Tolly Powell', 'Orgad Keller', 'Abhirut Gupta', 'Claire Sha', 'Kanav Garg', 'Nicolas Heess', 'Ágoston Weisz', 'Cassidy Hardin', 'Bartek Wydrowski', 'Ben Coleman', 'Karina Zainullina', 'Pankaj Joshi', 'Alessandro Epasto', 'Terry Spitz', 'Binbin Xiong', 'Kai Zhao', 'Arseniy Klimovskiy', 'Ivy Zheng', 'Johan Ferret', 'Itay Yona', 'Waleed Khawaja', 'Jean-Baptiste Lespiau', 'Maxim Krikun', 'Siamak Shakeri', 'Timothee Cour', 'Bonnie Li', 'Igor Krivokon', 'Dan Suh', 'Alex Hofer', 'Jad Al Abdallah', 'Nikita Putikhin', 'Oscar Akerlund', 'Silvio Lattanzi', 'Anurag Kumar', 'Shane Settle', 'Himanshu Srivastava', 'Folawiyo Campbell-Ajala', 'Edouard Rosseel', 'Mihai Dorin Istin', 'Nishanth Dikkala', 'Anand Rao', 'Nick Young', 'Kate Lin', 'Dhruva Bhaswar', 'Yiming Wang', 'Jaume Sanchez Elias', 'Kritika Muralidharan', 'James Keeling', 'Dayou Du', 'Siddharth Gopal', 'Gregory Dibb', 'Charles Blundell', 'Manolis Delakis', 'Jacky Liang', 'Marco Tulio Ribeiro', 'Georgi Karadzhov', 'Guillermo Garrido', 'Ankur Bapna', 'Jiawei Cao', 'Adam Sadovsky', 'Pouya Tafti', 'Arthur Guez', 'Coline Devin', 'Yixian Di', 'Jinwei Xing', 'Chuqiao Joyce Xu', 'Hanzhao Lin', 'Chun-Te Chu', 'Sameera Ponda', 'Wesley Helmholz', 'Fan Yang', 'Yue Gao', 'Sara Javanmardi', 'Wael Farhan', 'Alex Ramirez', 'Ricardo Figueira', 'Khe Chai Sim', 'Yuval Bahat', 'Ashwin Vaswani', 'Liangzhe Yuan', 'Gufeng Zhang', 'Leland Rechis', 'Hanjun Dai', 'Tayo Oguntebi', 'Alexandra Cordell', 'Eugénie Rives', 'Kaan Tekelioglu', 'Naveen Kumar', 'Bing Zhang', 'Aurick Zhou', 'Nikolay Savinov', 'Andrew Leach', 'Alex Tudor', 'Sanjay Ganapathy', 'Yanyan Zheng', 'Mirko Rossini', 'Vera Axelrod', 'Arnaud Autef', 'Yukun Zhu', 'Zheng Zheng', 'Mingda Zhang', 'Baochen Sun', 'Jie Ren', 'Nenad Tomasev', 'Nithish Kannen', 'Amer Sinha', 'Charles Chen', "Louis O'Bryan", 'Alex Pak', 'Aditya Kusupati', 'Weel Yang', 'Deepak Ramachandran', 'Patrick Griffin', 'Seokhwan Kim', 'Philipp Neubeck', 'Craig Schiff', 'Tammo Spalink', 'Mingyang Ling', 'Arun Nair', 'Ga-Young Joung', 'Linda Deng', 'Avishkar Bhoopchand', 'Lora Aroyo', 'Tom Duerig', 'Jordan Griffith', 'Gabe Barth-Maron', 'Jake Ades', 'Alex Haig', 'Ankur Taly', 'Yunting Song', 'Paul Michel', 'Dave Orr', 'Dean Weesner', 'Corentin Tallec', 'Carrie Grimes Bostock', 'Paul Niemczyk', 'Andy Twigg', 'Mudit Verma', 'Rohith Vallu', 'Henry Wang', 'Marco Gelmi', 'Kiranbir Sodhia', 'Aleksandr Chuklin', 'Omer Goldman', 'Jasmine George', 'Liang Bai', 'Kelvin Zhang', 'Petar Sirkovic', 'Efrat Nehoran', 'Golan Pundak', 'Jiaqi Mu', 'Alice Chen', 'Alex Greve', 'Paulo Zacchello', 'David Amos', 'Heming Ge', 'Eric Noland', 'Colton Bishop', 'Jeffrey Dudek', 'Youhei Namiki', 'Elena Buchatskaya', 'Jing Li', 'Dorsa Sadigh', 'Masha Samsikova', 'Dan Malkin', 'Damien Vincent', 'Robert David', 'Rob Willoughby', 'Phoenix Meadowlark', 'Shawn Gao', 'Yan Li', 'Raj Apte', 'Amit Jhindal', 'Stein Xudong Lin', 'Alex Polozov', 'Zhicheng Wang', 'Tomas Mery', 'Anirudh GP', 'Varun Yerram', 'Sage Stevens', 'Tianqi Liu', 'Noah Fiedel', 'Charles Sutton', 'Matthew Johnson', 'Xiaodan Song', 'Kate Baumli', 'Nir Shabat', 'Muqthar Mohammad', 'Hao Liu', 'Marco Selvi', 'Yichao Zhou', 'Mehdi Hafezi Manshadi', 'Chu-ling Ko', 'Anthony Chen', 'Michael Bendersky', 'Jorge Gonzalez Mendez', 'Nisarg Kothari', 'Amir Zandieh', 'Yiling Huang', 'Daniel Andor', 'Ellie Pavlick', 'Idan Brusilovsky', 'Jitendra Harlalka', 'Sally Goldman', 'Andrew Lampinen', 'Guowang Li', 'Asahi Ushio', 'Somit Gupta', 'Lei Zhang', 'Chuyuan Kelly Fu', 'Madhavi Sewak', 'Timo Denk', 'Jed Borovik', 'Brendan Jou', 'Avital Zipori', 'Prateek Jain', 'Junwen Bai', 'Thang Luong', 'Jonathan Tompson', 'Alice Li', 'Li Liu', 'George Powell', 'Jiajun Shen', 'Alex Feng', 'Grishma Chole', 'Da Yu', 'Yinlam Chow', 'Tongxin Yin', 'Eric Malmi', 'Kefan Xiao', 'Yash Pande', 'Shachi Paul', 'Niccolò Dal Santo', 'Adil Dostmohamed', 'Sergio Guadarrama', 'Aaron Phillips', 'Thanumalayan Sankaranarayana Pillai', 'Gal Yona', 'Amin Ghafouri', 'Preethi Lahoti', 'Benjamin Lee', 'Dhruv Madeka', 'Eren Sezener', 'Simon Tokumine', 'Adrian Collister', 'Nicola De Cao', 'Richard Shin', 'Uday Kalra', 'Parker Beak', 'Emily Nottage', 'Ryo Nakashima', 'Ivan Jurin', 'Vikash Sehwag', 'Meenu Gaba', 'Junhao Zeng', 'Kevin R. McKee', 'Fernando Pereira', 'Tamar Yakar', 'Amayika Panda', 'Arka Dhar', 'Peilin Zhong', 'Daniel Sohn', 'Mark Brand', 'Lars Lowe Sjoesund', 'Viral Carpenter', 'Sharon Lin', 'Shantanu Thakoor', 'Marcus Wainwright', 'Ashwin Chaugule', 'Pranesh Srinivasan', 'Muye Zhu', 'Bernett Orlando', 'Jack Weber', 'Ayzaan Wahid', 'Gilles Baechler', 'Apurv Suman', 'Jovana Mitrović', 'Gabe Taubman', 'Honglin Yu', 'Helen King', 'Josh Dillon', 'Cathy Yip', 'Dhriti Varma', 'Tomas Izo', 'Levent Bolelli', 'Borja De Balle Pigem', 'Julia Di Trapani', 'Fotis Iliopoulos', 'Adam Paszke', 'Nishant Ranka', 'Joe Zou', 'Francesco Pongetti', 'Jed McGiffin', 'Alex Siegman', 'Rich Galt', 'Ross Hemsley', 'Goran Žužić', 'Victor Carbune', 'Tao Li', 'Myle Ott', 'Félix de Chaumont Quitry', 'David Vilar Torres', 'Yuri Chervonyi', 'Tomy Tsai', 'Prem Eruvbetine', 'Samuel Yang', 'Matthew Denton', 'Jake Walker', 'Slavica Andačić', 'Idan Heimlich Shtacher', 'Vittal Premachandran', 'Harshal Tushar Lehri', 'Cip Baetu', 'Damion Yates', 'Lampros Lamprou', 'Mariko Iinuma', 'Ioana Mihailescu', 'Ben Albrecht', 'Shachi Dave', 'Susie Sargsyan', 'Bryan Perozzi', 'Lucas Manning', 'Chiyuan Zhang', 'Denis Vnukov', 'Igor Mordatch', 'Raia Hadsell Wolfgang Macherey', 'Ryan Kappedal', 'Jim Stephan', 'Aditya Tripathi', 'Klaus Macherey', 'Jun Qian', 'Abhishek Bhowmick', 'Shekoofeh Azizi', 'Rémi Leblond', 'Shiva Mohan Reddy Garlapati', 'Timothy Knight', 'Matthew Wiethoff', 'Wei-Chih Hung', 'Anelia Angelova', 'Georgios Evangelopoulos', 'Pawel Janus', 'Dimitris Paparas', 'Matthew Rahtz', 'Ken Caluwaerts', 'Vivek Sampathkumar', 'Daniel Jarrett', 'Shadi Noghabi', 'Antoine Miech', 'Chak Yeung', 'Geoff Clark', 'Henry Prior', 'Fei Zheng', 'Jean Pouget-Abadie', 'Indro Bhattacharya', 'Kalpesh Krishna', 'Will Bishop', 'Zhe Yuan', 'Yunxiao Deng', 'Ashutosh Sathe', 'Kacper Krasowiak', 'Ciprian Chelba', 'Cho-Jui Hsieh', 'Kiran Vodrahalli', 'Buhuang Liu', 'Thomas Köppe', 'Amr Khalifa', 'Lubo Litchev', 'Pichi Charoenpanit', 'Reed Roberts', 'Sachin Yadav', 'Yasumasa Onoe', 'Desi Ivanov', 'Megha Mohabey', 'Vighnesh Birodkar', 'Nemanja Rakićević', 'Pierre Sermanet', 'Vaibhav Mehta', 'Krishan Subudhi', 'Travis Choma', 'Will Ng', 'Luheng He', 'Kathie Wang', 'Tasos Kementsietsidis', 'Shane Gu', 'Mansi Gupta', 'Andrew Nystrom', 'Mehran Kazemi', 'Timothy Chung', 'Nacho Cano', 'Nikhil Dhawan', 'Yufei Wang', 'Jiawei Xia', 'Trevor Yacovone', 'Eric Jia', 'Mingqing Chen', 'Simeon Ivanov', 'Ashrith Sheshan', 'Sid Dalmia', 'Paweł Stradomski', 'Pengcheng Yin', 'Salem Haykal', 'Congchao Wang', 'Dennis Duan', 'Neslihan Bulut', 'Greg Kochanski', 'Liam MacDermed', 'Namrata Godbole', 'Shitao Weng', 'Jingjing Chen', 'Rachana Fellinger', 'Ramin Mehran', 'Daniel Suo', 'Hisham Husain', 'Tong He', 'Kaushal Patel', 'Joshua Howland', 'Randall Parker', 'Kelvin Nguyen', 'Sharath Maddineni', 'Chris Rawles', 'Mina Khan', 'Shlomi Cohen-Ganor', 'Amol Mandhane', 'Xinyi Wu', 'Chenkai Kuang', 'Iulia Comşa', 'Ramya Ganeshan', 'Hanie Sedghi', 'Adam Bloniarz', 'Nuo Wang Pierse', 'Anton Briukhov', 'Petr Mitrichev', 'Anita Gergely', 'Serena Zhan', 'Allan Zhou', 'Nikita Saxena', 'Eva Lu', 'Josef Dean', 'Ashish Gupta', 'Nicolas Perez-Nieves', 'Renjie Wu', 'Cory McLean', 'Wei Liang', 'Disha Jindal', 'Anton Tsitsulin', 'Wenhao Yu', 'Kaiz Alarakyia', 'Tom Schaul', 'Piyush Patil', 'Peter Sung', 'Elijah Peake', 'Hongkun Yu', 'Feryal Behbahani', 'JD Co-Reyes', 'Alan Ansell', 'Sean Sun', 'Clara Barbu', 'Jonathan Lee', 'Seb Noury', 'James Allingham', 'Bilal Piot', 'Mohit Sharma', 'Christopher Yew', 'Ivan Korotkov', 'Bibo Xu', 'Demetra Brady', 'Goran Petrovic', 'Shibl Mourad', 'Claire Cui', 'Aditya Gupta', 'Parker Schuh', 'Saarthak Khanna', 'Anna Goldie', 'Abhinav Arora', 'Vadim Zubov', 'Amy Stuart', 'Mark Epstein', 'Yun Zhu', 'Jianqiao Liu', 'Yury Stuken', 'Ziyue Wang', 'Karolis Misiunas', 'Dee Guo', 'Ashleah Gill', 'Ale Hartman', 'Zaid Nabulsi', 'Aurko Roy', 'Aleksandra Faust', 'Jason Riesa', 'Ben Withbroe', 'Mengchao Wang', 'Marco Tagliasacchi', 'Andreea Marzoca', 'James Noraky', 'Serge Toropov', 'Malika Mehrotra', 'Bahram Raad', 'Sanja Deur', 'Steve Xu', 'Marianne Monteiro', 'Zhongru Wu', 'Yi Luan', 'Sam Ritter', 'Nick Li', 'Håvard Garnes', 'Yanzhang He', 'Martin Zlocha', 'Jifan Zhu', 'Matteo Hessel', 'Will Wu', 'Spandana Raj Babbula', 'Chizu Kawamoto', 'Yuanzhen Li', 'Mehadi Hassen', 'Yan Wang', 'Brian Wieder', 'James Freedman', 'Yin Zhang', 'Xinyi Bai', 'Tianli Yu', 'David Reitter', 'XiangHai Sheng', 'Mateo Wirth', 'Aditya Kini', 'Dima Damen', 'Mingcen Gao', 'Rachel Hornung', 'Michael Voznesensky', 'Brian Roark', 'Adhi Kuncoro', 'Yuxiang Zhou', 'Rushin Shah', 'Anthony Brohan', 'Kuangyuan Chen', 'James Wendt', 'David Rim', 'Paul Kishan Rubenstein', 'Jonathan Halcrow', 'Michelle Liu', 'Ty Geri', 'Yunhsuan Sung', 'Jane Shapiro', 'Shaan Bijwadia', 'Chris Duvarney', 'Christina Sorokin', 'Paul Natsev', 'Reeve Ingle', 'Pramod Gupta', 'Young Maeng', 'Ndaba Ndebele', 'Kexin Zhu', 'Valentin Anklin', 'Katherine Lee', 'Yuan Liu', 'Yaroslav Akulov', 'Shaleen Gupta', 'Guolong Su', 'Flavien Prost', 'Tianlin Liu', 'Vitaly Kovalev', 'Pol Moreno', 'Martin Scholz', 'Sam Redmond', 'Zongwei Zhou', 'Alex Castro-Ros', 'André Susano Pinto', 'Dia Kharrat', 'Michal Yarom', 'Rachel Saputro', 'Jannis Bulian', 'Ben Caine', 'Ji Liu', 'Abbas Abdolmaleki', 'Shariq Iqbal', 'Tautvydas Misiunas', 'Mikhail Sirotenko', 'Shefali Garg', 'Guy Bensky', 'Huan Gui', 'Xuezhi Wang', 'Raphael Koster', 'Mike Bernico', 'Da Huang', 'Romal Thoppilan', 'Trevor Cohn', 'Ben Golan', 'Wenlei Zhou', 'Andrew Rosenberg', 'Markus Freitag', 'Tynan Gangwani', 'Vincent Tsang', 'Anand Shukla', 'Xiaoqi Ren', 'Minh Giang', 'Chi Zou', 'Andre Elisseeff', 'Charline Le Lan', 'Dheeru Dua', 'Shuba Lall', 'Pranav Shyam', 'Frankie Garcia', 'Sarah Nguyen', 'Michael Guzman', 'AJ Maschinot', 'Marcello Maggioni', 'Ming-Wei Chang', 'Karol Gregor', 'Lotte Weerts', 'Kumaran Venkatesan', 'Bogdan Damoc', 'Leon Liu', 'Jan Wassenberg', 'Lewis Ho', 'Becca Roelofs', 'Majid Hadian', 'François-Xavier Aubet', 'Yu Liang', 'Sami Lachgar', 'Danny Karmon', 'Yong Cheng', 'Amelio Vázquez-Reina', 'Angie Chen', 'Zhuyun Dai', 'Andy Brock', 'Shubham Agrawal', 'Chenxi Pang', 'Peter Garst', 'Mariella Sanchez-Vargas', 'Ivor Rendulic', 'Aditya Ayyar', 'Andrija Ražnatović', 'Olivia Ma', 'Roopali Vij', 'Neha Sharma', 'Ashwin Balakrishna', 'Bingyuan Liu', 'Ian Mackinnon', 'Sorin Baltateanu', 'Petra Poklukar', 'Gabriel Ibagon', 'Colin Ji', 'Hongyang Jiao', 'Isaac Noble', 'Wojciech Stokowiec', 'Zhihao Li', 'Jeff Dean', 'David Lindner', 'Mark Omernick', 'Kristen Chiafullo', 'Mason Dimarco', 'Vitor Rodrigues', 'Vittorio Selo', 'Garrett Honke', 'Xintian Cindy Wu', 'Wei He', 'Adam Hillier', 'Anhad Mohananey', 'Vihari Piratla', 'Chang Ye', 'Chase Malik', 'Sebastian Riedel', 'Samuel Albanie', 'Zi Yang', 'Kenny Vassigh', 'Maria Bauza', 'Sheng Li', 'Yiqing Tao', 'Nevan Wichers', 'Andrii Maksai', 'Abe Ittycheriah', 'Ross Mcilroy', 'Bryan Seybold', 'Noah Goodman', 'Romina Datta', 'Steven M. Hernandez', 'Tian Shi', 'Yony Kochinski', 'Anna Bulanova', 'Ken Franko', 'Mikita Sazanovich', 'Nicholas FitzGerald', 'Praneeth Kacham', 'Shubha Srinivas Raghvendra', 'Vincent Hellendoorn', 'Alexander Grushetsky', 'Julian Salazar', 'Angeliki Lazaridou', 'Jason Chang', 'Jan-Thorsten Peter', 'Sushant Kafle', 'Yann Dauphin', 'Abhishek Rao', 'Filippo Graziano', 'Izhak Shafran', 'Yuguo Liao', 'Tianli Ding', 'Geng Yan', 'Grace Chu', 'Zhao Fu', 'Vincent Roulet', 'Gabriel Rasskin', 'Duncan Williams', 'Shahar Drath', 'Alex Mossin', 'Raphael Hoffmann', 'Jordi Orbay', 'Francesco Bertolini', 'Hila Sheftel', 'Justin Chiu', 'Siyang Xue', 'Yuheng Kuang', 'Ferjad Naeem', 'Swaroop Nath', 'Nana Nti', 'Phil Culliton', 'Kashyap Krishnakumar', 'Michael Isard', 'Pei Sun', 'Ayan Chakrabarti', 'Nathan Clement', 'Regev Cohen', 'Arissa Wongpanich', 'GS Oh', 'Ashwin Murthy', 'Hao Zheng', 'Jessica Hamrick', 'Oskar Bunyan', 'Suhas Ganesh', 'Nitish Gupta', 'Roy Frostig', 'John Wieting', 'Yury Malkov', 'Pierre Marcenac', 'Zhixin Lucas Lai', 'Xiaodan Tang', 'Mohammad Saleh', 'Fedir Zubach', 'Chinmay Kulkarni', 'Huanjie Zhou', 'Vicky Zayats', 'Nan Ding', 'Anshuman Tripathi', 'Arijit Pramanik', 'Patrik Zochbauer', 'Harish Ganapathy', 'Vedant Misra', 'Zach Behrman', 'Hugo Vallet', 'Mingyang Zhang', 'Mukund Sridhar', 'Ye Jin', 'Mohammad Babaeizadeh', 'Siim Põder', 'Megha Goel', 'Divya Jain', 'Tajwar Nasir', 'Shubham Mittal', 'Tim Dozat', 'Diego Ardila', 'Aliaksei Severyn', 'Fabio Pardo', 'Sammy Jerome', 'Siyang Qin', 'Louis Rouillard', 'Amir Yazdanbakhsh', 'Zizhao Zhang', 'Shivani Agrawal', 'Kaushik Shivakumar', 'Caden Lu', 'Praveen Kallakuri', 'Rachita Chhaparia', 'Kanishka Rao', 'Charles Kwong', 'Asya Fadeeva', 'Shitij Nigam', 'Yan Virin', 'Yuan Zhang', 'Balaji Venkatraman', 'Beliz Gunel', 'Marc Wilson', 'Huiyu Wang', 'Abhinav Gupta', 'Xiaowei Xu', 'Adrien Ali Taïga', 'Kareem Mohamed', 'Doug Fritz', 'Daniel Rodriguez', 'Zoubin Ghahramani', 'Harry Askham', 'Lior Belenki', 'James Zhao', 'Rahul Gupta', 'Krzysztof Jastrzębski', 'Takahiro Kosakai', 'Kaan Katircioglu', 'Jon Schneider', 'Rina Panigrahy', 'Konstantinos Bousmalis', 'Peter Grabowski', 'Prajit Ramachandran', 'Chaitra Hegde', 'Mihaela Rosca', 'Angelo Scorza Scarpati', 'Kyriakos Axiotis', 'Ying Xu', 'Zach Gleicher', 'Assaf Hurwitz Michaely', 'Mandar Sharma', 'Sanil Jain', 'Christoph Hirnschall', 'Tal Marian', 'Xuhui Jia', 'Kevin Mather', 'Kilol Gupta', 'Linhai Qiu', 'Nigamaa Nayakanti', 'Lucian Ionita', 'Steven Zheng', 'Lucia Loher', 'Kurt Shuster', 'Igor Petrovski', 'Roshan Sharma', 'Rahma Chaabouni', 'Angel Yeh', 'James An', 'Arushi Gupta', 'Steven Schwarcz', 'Seher Ellis', 'Sam Conway-Rahman', 'Javier Snaider', 'Alex Zhai', 'James Atwood', 'Daniel Golovin', 'Liqian Peng', 'Te I', 'Vivian Xia', 'Salvatore Scellato', 'Mahan Malihi', 'Arthur Bražinskas', 'Vlad-Doru Ion', 'Younghoon Jun', 'James Swirhun', 'Soroosh Mariooryad', 'Jiao Sun', 'Steve Chien', 'Rey Coaguila', 'Ariel Brand', 'Yi Gao', 'Tom Kwiatkowski', 'Roee Aharoni', 'Cheng-Chun Lee', 'Mislav Žanić', 'Yichi Zhang', 'Dan Ethier', 'Vitaly Nikolaev', 'Pranav Nair', 'Yoav Ben Shalom', 'Hen Fitoussi', 'Jai Gupta', 'Hongbin Liu', 'Dee Cattle', 'Tolga Bolukbasi', 'Ben Murdoch', 'Fantine Huot', 'Yin Li', 'Chris Hahn'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2507.06261.jpg', 'data': {'categories': ['#long_context', '#agents', '#benchmark', '#architecture', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Gemini 2.X: новый рубеж в мультимодальном ИИ и рассуждениях', 'desc': 'В статье представлено семейство моделей Gemini 2.X, включая Gemini 2.5 Pro и Gemini 2.5 Flash. Gemini 2.5 Pro демонстрирует наилучшие результаты в кодировании и рассуждениях, а также обладает мультимодальными возможностями, включая обработку до 3 часов видеоконтента. Модели Gemini 2.5 Flash, Gemini 2.0 Flash и Flash-Lite предлагают различные уровни производительности и эффективности. Семейство Gemini 2.X охватывает весь фронт Парето соотношения возможностей и стоимости моделей для решения сложных задач.'}, 'en': {'title': 'Unlocking New Frontiers in AI Problem Solving with Gemini 2.X', 'desc': 'The Gemini 2.X model family introduces advanced AI models designed for complex problem-solving tasks. The Gemini 2.5 Pro model stands out with state-of-the-art performance in coding and reasoning, capable of processing extensive video content. Additionally, the models offer a range of capabilities, balancing efficiency and performance, making them suitable for various applications. Overall, the Gemini 2.X series enables users to leverage multimodal understanding and reasoning to enhance agentic workflows.'}, 'zh': {'title': 'Gemini 2.X：多模态理解与推理的最佳选择', 'desc': 'Gemini 2.X模型系列提供了不同层次的能力和效率，适用于复杂问题的解决，包括多模态理解和推理。Gemini 2.5 Pro是目前最强大的模型，在编码和推理基准测试中达到了最先进的性能。该模型不仅具备出色的编码和推理能力，还能处理多达3小时的视频内容，展现了其在多模态理解方面的优势。整体而言，Gemini 2.X系列模型在能力与成本之间达成了最佳平衡，帮助用户探索复杂问题解决的可能性。'}}}, {'id': 'https://huggingface.co/papers/2507.08794', 'title': 'One Token to Fool LLM-as-a-Judge', 'url': 'https://huggingface.co/papers/2507.08794', 'abstract': 'Generative reward models using LLMs are vulnerable to superficial manipulations but can be improved with data augmentation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let\'s solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM.', 'score': 12, 'issue_id': 4793, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'c5951de5379e6612', 'authors': ['Yulai Zhao', 'Haolin Liu', 'Dian Yu', 'S. Y. Kung', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Princeton University', 'Tencent AI Lab', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2507.08794.jpg', 'data': {'categories': ['#training', '#rl', '#data', '#dataset', '#optimization', '#hallucinations', '#synthetic', '#rlhf'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление генеративных моделей вознаграждения против поверхностных манипуляций', 'desc': 'Статья посвящена уязвимостям генеративных моделей вознаграждения, использующих большие языковые модели (LLM) для оценки качества ответов. Авторы обнаружили, что такие модели могут быть обмануты поверхностными манипуляциями, такими как добавление символов или фраз-заполнителей. Для решения этой проблемы предложена стратегия аугментации данных, позволяющая создать более надежную модель оценки. Исследование подчеркивает необходимость разработки более надежных методов оценки на основе LLM.'}, 'en': {'title': 'Strengthening LLMs: Combatting Superficial Manipulations in Reward Models', 'desc': 'This paper discusses the vulnerabilities of generative reward models, which use large language models (LLMs) to assess the quality of answers in reinforcement learning scenarios. The authors reveal that these models can be easily tricked by superficial changes in the input, such as adding non-word symbols or specific phrases, leading to incorrect evaluations. To address this issue, they propose a data augmentation strategy that enhances the robustness of the generative reward models against such manipulations. The study emphasizes the importance of developing more reliable evaluation methods for LLMs in reinforcement learning applications.'}, 'zh': {'title': '提升生成奖励模型的鲁棒性', 'desc': '生成奖励模型（LLMs作为评判者）在使用大型语言模型评估答案质量时，容易受到表面操控的影响。尽管这种比较任务看似简单，但我们发现生成奖励模型在面对非单词符号或推理开头词时，常常会产生错误的正向奖励。这种脆弱性在不同的LLM、数据集和提示格式中普遍存在，威胁到依赖生成奖励模型的核心算法范式。为了解决这个问题，我们提出了一种简单有效的数据增强策略，训练出一种具有显著改进鲁棒性的生成奖励模型。'}}}, {'id': 'https://huggingface.co/papers/2507.08772', 'title': 'From One to More: Contextual Part Latents for 3D Generation', 'url': 'https://huggingface.co/papers/2507.08772', 'abstract': "A part-aware diffusion framework, CoPart, enhances 3D generation by decomposing objects into contextual parts, improving complexity handling, relationship modeling, and part-level conditioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.", 'score': 10, 'issue_id': 4792, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '13e80f68dc4965b1', 'authors': ['Shaocong Dong', 'Lihe Ding', 'Xiao Chen', 'Yaokun Li', 'Yuxin Wang', 'Yucheng Wang', 'Qi Wang', 'Jaehyeok Kim', 'Chenjian Gao', 'Zhanpeng Huang', 'Zibin Wang', 'Tianfan Xue', 'Dan Xu'], 'affiliations': ['CUHK', 'HKUST', 'SenseTime Research', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.08772.jpg', 'data': {'categories': ['#games', '#3d', '#diffusion', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Разделяй и властвуй: новый подход к генерации 3D-объектов', 'desc': 'CoPart - это новая система для генерации трехмерных объектов, основанная на диффузионных моделях. Она разбивает объекты на отдельные части, что позволяет лучше моделировать сложные геометрические формы и взаимосвязи между компонентами. CoPart дает возможность более точного контроля над генерацией на уровне отдельных частей объекта. Для обучения системы был создан новый набор данных Partverse с аннотированными трехмерными моделями и их сегментацией на части.'}, 'en': {'title': 'Revolutionizing 3D Generation with Part-Aware Diffusion', 'desc': "The paper introduces CoPart, a part-aware diffusion framework designed to improve 3D object generation by breaking down objects into contextual parts. This approach addresses limitations in existing models, such as the inability to capture complex geometries and the lack of part independence in holistic representations. CoPart enhances the modeling of relationships between parts and allows for fine-grained control over part-level conditioning. Additionally, the authors present a new dataset, Partverse, to support large-scale training and demonstrate CoPart's effectiveness in tasks like part-level editing and scene composition."}, 'zh': {'title': '部件意识的3D生成新框架', 'desc': 'CoPart是一个关注部件的扩散框架，旨在通过将3D对象分解为上下文相关的部件来增强3D生成。该方法解决了传统方法在处理复杂多部件几何形状时的局限性，能够更好地建模部件之间的关系。通过部件分解，CoPart降低了编码复杂性，并支持精细的部件级条件控制。实验结果表明，CoPart在部件级编辑、关节对象生成和场景组合方面表现出色，具有前所未有的可控性。'}}}, {'id': 'https://huggingface.co/papers/2507.08441', 'title': 'Vision Foundation Models as Effective Visual Tokenizers for\n  Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2507.08441', 'abstract': "A novel image tokenizer built on pre-trained vision foundation models improves image reconstruction, generation quality, and token efficiency, enhancing autoregressive generation and class-conditional synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.", 'score': 9, 'issue_id': 4797, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '700a23d0c8771ece', 'authors': ['Anlin Zheng', 'Xin Wen', 'Xuanyang Zhang', 'Chuofan Ma', 'Tiancai Wang', 'Gang Yu', 'Xiangyu Zhang', 'Xiaojuan Qi'], 'affiliations': ['Dexmal', 'MEGVII Technology', 'StepFun', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.08441.jpg', 'data': {'categories': ['#open_source', '#dataset', '#optimization', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'VFMTok: Революция в токенизации изображений с помощью фундаментальных моделей зрения', 'desc': 'Исследователи представили новый токенизатор изображений VFMTok, основанный на предобученных фундаментальных моделях компьютерного зрения. VFMTok использует замороженную фундаментальную модель в качестве энкодера и включает адаптивное квантование регионов и семантическую цель реконструкции. Этот подход значительно улучшает качество реконструкции и генерации изображений, а также эффективность токенизации. VFMTok также ускоряет сходимость авторегрессионных моделей и позволяет осуществлять высококачественный условный синтез изображений без использования classifier-free guidance.'}, 'en': {'title': 'Enhancing Image Generation with VFMTok: A New Tokenizer Approach', 'desc': 'This paper introduces VFMTok, a new image tokenizer that utilizes pre-trained vision foundation models to improve image reconstruction and generation. By employing a frozen vision model as its encoder, VFMTok incorporates a region-adaptive quantization framework to minimize redundancy in feature representation. Additionally, it uses a semantic reconstruction objective to ensure that the outputs maintain semantic accuracy aligned with the original model. The results show significant enhancements in token efficiency and autoregressive generation performance, achieving a gFID of 2.07 on ImageNet and tripling model convergence speed.'}, 'zh': {'title': '基于视觉模型的高效图像标记器', 'desc': '本文提出了一种新颖的图像标记器，基于预训练的视觉基础模型，旨在提高图像重建和生成的质量以及标记效率。我们使用冻结的视觉基础模型作为标记器的编码器，并引入了区域自适应量化框架和语义重建目标，以减少冗余并保持语义一致性。通过这些设计，VFMTok在图像重建和生成质量上取得了显著提升，同时加速了自回归生成的收敛速度。该方法在ImageNet基准测试中实现了2.07的gFID，并支持高保真度的类别条件合成。'}}}, {'id': 'https://huggingface.co/papers/2507.06952', 'title': 'What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models', 'url': 'https://huggingface.co/papers/2507.06952', 'abstract': "Foundation models, despite excelling in training tasks, often fail to generalize to new tasks due to task-specific heuristics rather than capturing underlying world models.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.", 'score': 4, 'issue_id': 4795, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': 'd203424a90614c2b', 'authors': ['Keyon Vafa', 'Peter G. Chang', 'Ashesh Rambachan', 'Sendhil Mullainathan'], 'affiliations': ['Harvard University', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2507.06952.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#synthetic', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Фундаментальные модели: успех в обучении, провал в обобщении', 'desc': 'Фундаментальные модели, несмотря на их успехи в обучении, часто не могут обобщаться на новые задачи из-за использования эвристик, специфичных для конкретных задач, вместо понимания базовых моделей мира. Авторы разработали метод оценки фундаментальных моделей, который исследует их адаптацию к синтетическим датасетам, сгенерированным на основе некоторой постулированной модели мира. Этот метод, названный зондом индуктивного смещения, измеряет, насколько индуктивное смещение модели соответствует модели мира. Исследования показали, что фундаментальные модели, обученные на орбитальных траекториях, не смогли применить законы Ньютона при адаптации к новым физическим задачам.'}, 'en': {'title': 'Unveiling the Limits of Foundation Models: Task-Specific Heuristics vs. General Understanding', 'desc': 'This paper discusses the limitations of foundation models in machine learning, particularly their inability to generalize to new tasks. The authors introduce a method called the inductive bias probe, which evaluates how well these models adapt to synthetic datasets based on a theoretical world model. Their findings indicate that while foundation models perform well on training tasks, they often rely on task-specific heuristics instead of understanding the underlying principles. Specifically, models trained on orbital trajectories struggle to apply Newtonian mechanics in new physics tasks, highlighting the need for better generalization capabilities.'}, 'zh': {'title': '基础模型的归纳偏差探测与泛化能力', 'desc': '基础模型在训练任务中表现出色，但在新任务上常常无法泛化，这是因为它们依赖于特定任务的启发式方法，而不是捕捉到更深层次的世界模型。我们提出了一种评估基础模型的新技术，旨在检查它们如何适应从假设的世界模型生成的合成数据集。该技术测量基础模型的归纳偏差是否与世界模型一致，因此我们称之为归纳偏差探测器。我们的研究发现，尽管基础模型在训练任务中表现良好，但在适应新任务时，它们往往未能发展出对底层世界模型的归纳偏差。'}}}, {'id': 'https://huggingface.co/papers/2507.08771', 'title': 'BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity', 'url': 'https://huggingface.co/papers/2507.08771', 'abstract': 'To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).', 'score': 3, 'issue_id': 4794, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '27bac3ede0d76a2a', 'authors': ['Chenyang Song', 'Weilin Zhao', 'Xu Han', 'Chaojun Xiao', 'Yingfa Chen', 'Yuxuan Li', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.08771.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#low_resource', '#open_source', '#training'], 'emoji': '🚀', 'ru': {'title': 'BlockFFN: Эффективная архитектура смеси экспертов для ускорения больших языковых моделей', 'desc': 'Статья представляет новую архитектуру смеси экспертов (MoE) под названием BlockFFN для снижения вычислительной нагрузки больших языковых моделей (LLM). BlockFFN использует дифференцируемую и гибкую маршрутизацию, объединяющую активацию ReLU и RMSNorm. Авторы разработали цели обучения, учитывающие разреженность на уровне токенов и чанков, что делает модель более подходящей для ускорения. Экспериментальные результаты показывают превосходство BlockFFN над другими базовыми MoE, достигая более 80% разреженности на уровне токенов и 70% на уровне 8-токенных чанков.'}, 'en': {'title': 'BlockFFN: Efficient Sparsity for Faster Language Models', 'desc': 'This paper presents a new architecture called BlockFFN that improves the efficiency of mixture-of-experts (MoE) models by addressing issues with routing and sparsity. The authors introduce a router that uses ReLU activation and RMSNorm, allowing for more flexible and differentiable routing of parameters. They also propose training objectives that enhance both token-level and chunk-level sparsity, making the model more suitable for low-resource environments. Experimental results show that BlockFFN outperforms existing MoE models, achieving significant speedups on end-side devices while maintaining high levels of sparsity.'}, 'zh': {'title': '提升稀疏激活模型的性能与加速', 'desc': '为了减轻大型语言模型的计算负担，稀疏激活架构（如专家混合模型MoE）受到越来越多的关注。然而，传统MoE的非可微和不灵活的路由方式会影响模型性能。此外，虽然每个token只激活少量参数，但这些稀疏激活架构在块级稀疏性上表现较低，这使得在低资源条件下加速变得困难。为了解决这些问题，我们提出了一种新型的MoE架构BlockFFN，并设计了高效的训练和部署技术。'}}}, {'id': 'https://huggingface.co/papers/2507.07151', 'title': 'Robust Multimodal Large Language Models Against Modality Conflict', 'url': 'https://huggingface.co/papers/2507.07151', 'abstract': 'Investigation of modality conflict in multimodal large language models reveals its role in causing hallucinations, with reinforcement learning emerging as the most effective mitigation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.', 'score': 2, 'issue_id': 4792, 'pub_date': '2025-07-09', 'pub_date_card': {'ru': '9 июля', 'en': 'July 9', 'zh': '7月9日'}, 'hash': '2c4bd34981d368dd', 'authors': ['Zongmeng Zhang', 'Wengang Zhou', 'Jie Zhao', 'Houqiang Li'], 'affiliations': ['Department of Electronic Engineering and Information Science, University of Science and Technology of China', 'Huawei Technologies Co., Ltd.', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.07151.jpg', 'data': {'categories': ['#training', '#interpretability', '#dataset', '#rl', '#hallucinations', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных нейросетях', 'desc': 'Исследование посвящено проблеме галлюцинаций в мультимодальных больших языковых моделях (MLLM) из-за конфликта модальностей. Авторы создали датасет MMMC для симуляции этого явления в задачах компьютерного зрения и обработки естественного языка. Были предложены три метода для уменьшения галлюцинаций: инженерия промптов, обучение с учителем и обучение с подкреплением. Эксперименты показали, что обучение с подкреплением наиболее эффективно в снижении галлюцинаций при конфликте модальностей.'}, 'en': {'title': 'Tackling Hallucinations in MLLMs: The Power of Reinforcement Learning', 'desc': 'This paper explores how multimodal large language models (MLLMs) can experience hallucinations due to conflicts between different types of input data, known as modality conflict. The authors define modality conflict and create a dataset called Multimodal Modality Conflict (MMMC) to study this issue in vision-language tasks. They propose three strategies to reduce hallucinations: prompt engineering, supervised fine-tuning, and reinforcement learning. Among these, reinforcement learning is found to be the most effective method for addressing hallucinations caused by modality conflict, while supervised fine-tuning also shows reliable results.'}, 'zh': {'title': '揭示模态冲突，减轻幻觉的有效策略', 'desc': '这篇论文研究了多模态大型语言模型（MLLMs）中的模态冲突现象，发现它是导致幻觉的一个重要原因。与以往研究不同，本文关注的是来自不同模态的输入之间的内在冲突，这种冲突使得MLLMs面临困境并直接导致幻觉。我们正式定义了模态冲突，并构建了一个名为多模态模态冲突（MMMC）的数据集，以模拟这一现象。通过实验，我们发现基于强化学习的方法在减轻模态冲突引起的幻觉方面表现最佳，而监督微调方法则展现出良好且稳定的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.04517', 'title': 'DOTResize: Reducing LLM Width via Discrete Optimal Transport-based\n  Neuron Merging', 'url': 'https://huggingface.co/papers/2507.04517', 'abstract': 'DOTResize, a novel Transformer compression method based on Discrete Optimal Transport, reduces model size by combining similar neurons without discarding information, leading to better performance than pruning techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Model compression offers a promising path to reducing the cost and inaccessibility of large pre-trained models, without significantly compromising their impressive performance. Large Transformer models, including large language models (LLMs), often contain computational redundancy, which can serve as a target for new model compression methods. In this work, we specifically target neuron-level redundancies in model layers by combining groups of similar neurons into fewer neurons. We frame this width reduction as a Discrete Optimal Transport problem, and propose DOTResize, a novel Transformer compression method that uses optimal transport theory to transform and compress model weights. To ensure applicability within the Transformer architecture, we motivate and incorporate entropic regularization and matrix factorization into the transportation maps produced by our method. Unlike pruning-based approaches which discard neurons based on importance measures, DOTResize re-projects the entire neuron width, allowing the retention and redistribution of useful signal across the reduced layer. Empirical results show that compared to simple or state-of-the-art neuron width-pruning techniques, DOTResize can outperform these methods across multiple LLM families and sizes, while achieving measurable reductions in real-world computational cost.', 'score': 1, 'issue_id': 4811, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '8ee5e3aed98a5c25', 'authors': ['Neha Verma', 'Kenton Murray', 'Kevin Duh'], 'affiliations': ['Center for Language and Speech Processing', 'Human Language Technology Center of Excellence', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2507.04517.jpg', 'data': {'categories': ['#inference', '#training', '#architecture', '#optimization'], 'emoji': '🗜️', 'ru': {'title': 'Эффективное сжатие трансформеров без потери качества', 'desc': 'DOTResize - это новый метод сжатия трансформеров, основанный на дискретном оптимальном транспорте. Он уменьшает размер модели, объединяя схожие нейроны без потери информации. Метод использует теорию оптимального транспорта для трансформации и сжатия весов модели. В отличие от методов прунинга, DOTResize перепроецирует всю ширину нейронной сети, сохраняя и перераспределяя полезный сигнал в уменьшенном слое.'}, 'en': {'title': 'Efficient Compression with DOTResize: Merging Neurons for Better Performance', 'desc': 'DOTResize is a new method for compressing Transformer models by addressing neuron-level redundancies. It combines similar neurons into fewer ones without losing important information, which helps maintain performance. This approach uses Discrete Optimal Transport theory to optimize and compress model weights effectively. Compared to traditional pruning methods that remove neurons, DOTResize redistributes the useful signals, leading to better efficiency and lower computational costs.'}, 'zh': {'title': 'DOTResize：高效的Transformer压缩方法', 'desc': 'DOTResize是一种基于离散最优传输的Transformer压缩方法，通过将相似的神经元组合在一起，减少模型大小而不丢失信息，从而提高性能。该方法专注于模型层中的神经元级冗余，将相似神经元组合成更少的神经元。我们将这种宽度减少视为一个离散最优传输问题，并提出了DOTResize，通过最优传输理论来转换和压缩模型权重。与基于剪枝的方法不同，DOTResize重新投影整个神经元宽度，保留并重新分配有用信号，从而在多个大型语言模型中实现了更好的性能和计算成本的降低。'}}}, {'id': 'https://huggingface.co/papers/2507.13563', 'title': 'A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges\n  in Russian Speech Generative Models', 'url': 'https://huggingface.co/papers/2507.13563', 'abstract': 'Balalaika, a large Russian speech dataset with detailed annotations, improves performance in speech synthesis and enhancement tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, a novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations.', 'score': 41, 'issue_id': 4921, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '8737d8bc0b245da4', 'authors': ['Kirill Borodin', 'Nikita Vasiliev', 'Vasiliy Kudryavtsev', 'Maxim Maslov', 'Mikhail Gorodnichev', 'Oleg Rogov', 'Grach Mkrtchian'], 'affiliations': ['Artificial Intelligence Research Institute', 'Moscow Technical University of Communication and Informatics'], 'pdf_title_img': 'assets/pdf/title_img/2507.13563.jpg', 'data': {'categories': ['#data', '#low_resource', '#dataset', '#audio', '#synthetic'], 'emoji': '🎻', 'ru': {'title': 'Balalaika: мощный инструмент для улучшения русского речевого ИИ', 'desc': 'Статья представляет новый набор данных для русской речи под названием Balalaika. Этот датасет содержит более 2000 часов высококачественной речи с подробными аннотациями, включая пунктуацию и ударения. Модели, обученные на Balalaika, показывают значительно лучшие результаты в задачах синтеза и улучшения речи по сравнению с существующими датасетами. Авторы описывают процесс создания датасета, методологию аннотирования и результаты сравнительных оценок.'}, 'en': {'title': 'Balalaika: Elevating Russian Speech Synthesis with Quality Data', 'desc': 'The paper presents Balalaika, a large dataset designed to improve Russian speech synthesis and enhancement. It contains over 2,000 hours of high-quality speech recordings with detailed annotations, addressing specific challenges in Russian phonetics. The authors demonstrate that models trained on this dataset achieve superior performance compared to those trained on existing datasets. The paper also outlines the methods used for dataset construction and annotation, along with comparative evaluation results.'}, 'zh': {'title': 'Balalaika：提升俄罗斯语音合成的新数据集', 'desc': 'Balalaika是一个大型的俄罗斯语语音数据集，包含详细的注释，旨在提高语音合成和增强任务的性能。该数据集包含超过2000小时的高质量录音，提供了全面的文本注释，包括标点符号和重音标记。研究表明，使用Balalaika训练的模型在语音合成和增强任务中显著优于现有数据集训练的模型。本文详细介绍了数据集的构建流程、注释方法以及比较评估的结果。'}}}, {'id': 'https://huggingface.co/papers/2507.11097', 'title': 'The Devil behind the mask: An emergent safety vulnerability of Diffusion\n  LLMs', 'url': 'https://huggingface.co/papers/2507.11097', 'abstract': 'DIJA is a framework that exploits safety weaknesses in diffusion-based large language models by constructing adversarial prompts, demonstrating significant vulnerabilities in their alignment mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.', 'score': 39, 'issue_id': 4917, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': 'a104a398841ff4ff', 'authors': ['Zichen Wen', 'Jiashu Qu', 'Dongrui Liu', 'Zhiyuan Liu', 'Ruixi Wu', 'Yicun Yang', 'Xiangqi Jin', 'Haoyun Xu', 'Xuyang Liu', 'Weijia Li', 'Chaochao Lu', 'Jing Shao', 'Conghui He', 'Linfeng Zhang'], 'affiliations': ['EPIC Lab, Shanghai Jiao Tong University', 'Shanghai AI Laboratory', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2507.11097.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#alignment', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Уязвимости диффузионных языковых моделей: новый фронт в безопасности ИИ', 'desc': 'DIJA - это фреймворк для создания состязательных промптов, который использует уязвимости в механизмах безопасности диффузионных языковых моделей. Он демонстрирует, что существующие методы выравнивания не защищают эти модели от вредоносных запросов с маскированным вводом. DIJA конструирует промпты, чередуя маскированный и обычный текст, что позволяет обойти стандартные механизмы безопасности. Эксперименты показывают, что DIJA значительно превосходит существующие методы взлома языковых моделей, раскрывая новую поверхность угроз.'}, 'en': {'title': 'Unmasking Vulnerabilities in Diffusion-Based Language Models with DIJA', 'desc': 'DIJA is a novel framework that identifies and exploits safety vulnerabilities in diffusion-based large language models (dLLMs) through the use of adversarial prompts. It reveals that existing alignment mechanisms are inadequate in preventing harmful outputs when faced with context-aware, masked-input prompts. By leveraging the unique features of dLLMs, such as bidirectional modeling and parallel decoding, DIJA demonstrates how these models can produce unsafe content despite alignment efforts. The framework significantly outperforms previous methods, highlighting the critical need for improved safety measures in the design of dLLMs.'}, 'zh': {'title': '揭示扩散型大语言模型的安全脆弱性', 'desc': 'DIJA是一个框架，利用扩散型大语言模型中的安全弱点，通过构造对抗性提示，展示了其对齐机制的显著脆弱性。尽管扩散型大语言模型在代码生成和文本填充方面表现出色，但我们发现现有的对齐机制无法有效防护针对上下文的对抗性提示。DIJA通过构建对抗性交错掩码文本提示，利用了扩散型大语言模型的文本生成机制，导致标准对齐机制失效。我们的实验表明，DIJA在揭示扩散型大语言模型架构中的新威胁方面，显著优于现有的越狱方法。'}}}, {'id': 'https://huggingface.co/papers/2507.14137', 'title': 'Franca: Nested Matryoshka Clustering for Scalable Visual Representation\n  Learning', 'url': 'https://huggingface.co/papers/2507.14137', 'abstract': 'Franca, an open-source vision foundation model, achieves high performance using a transparent training pipeline and novel clustering and disentanglement techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.', 'score': 14, 'issue_id': 4918, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '9435de131fc3d757', 'authors': ['Shashanka Venkataramanan', 'Valentinos Pariza', 'Mohammadreza Salehi', 'Lukas Knobel', 'Spyros Gidaris', 'Elias Ramzi', 'Andrei Bursuc', 'Yuki M. Asano'], 'affiliations': ['Fundamental AI Lab, UTN', 'VIS Lab, UvA', 'valeo.ai, Paris'], 'pdf_title_img': 'assets/pdf/title_img/2507.14137.jpg', 'data': {'categories': ['#training', '#cv', '#optimization', '#dataset', '#architecture', '#open_source'], 'emoji': '👁️', 'ru': {'title': 'Открытая модель компьютерного зрения нового поколения', 'desc': 'Franca - это первая полностью открытая модель компьютерного зрения, которая соответствует или превосходит производительность современных проприетарных моделей. Она использует прозрачный конвейер обучения и общедоступные данные, включая ImageNet-21K и подмножество ReLAION-2B. Авторы представили новый многоголовый кластеризующий проектор на основе вложенных матрешечных представлений, который позволяет постепенно уточнять признаки в более детальные кластеры без увеличения размера модели. Кроме того, они предложили стратегию позиционного разделения, которая явно удаляет позиционные смещения из плотных представлений, улучшая кодирование семантического содержания.'}, 'en': {'title': 'Franca: The Open-Source Vision Model Redefining Performance Standards', 'desc': 'Franca is an open-source vision foundation model that achieves competitive performance with leading proprietary models by utilizing a transparent training pipeline. It employs innovative clustering and disentanglement techniques to enhance the quality of feature representations. The model addresses limitations in self-supervised learning (SSL) clustering methods by introducing a multi-head clustering projector that refines features into detailed clusters efficiently. Additionally, a positional disentanglement strategy is implemented to eliminate biases, resulting in improved semantic encoding and better performance on various benchmarks.'}, 'zh': {'title': 'Franca：开源视觉模型的新标准', 'desc': 'Franca是一个开源的视觉基础模型，采用透明的训练流程和新颖的聚类与解缠技术，取得了高性能。它是第一个完全开源的视觉基础模型，性能与许多最先进的专有模型相当，甚至在某些情况下超越它们。Franca使用公开可用的数据进行训练，并引入了一种基于嵌套马特ryoshka表示的多头聚类投影器，以提高聚类的精确度。通过消除密集表示中的位置偏差，Franca在多个下游基准测试中表现出一致的提升，展示了更清晰特征空间的实用性。'}}}, {'id': 'https://huggingface.co/papers/2507.12566', 'title': 'Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2507.12566', 'abstract': 'Mono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and improved pre-training strategies to enhance visual learning and reduce computational costs while maintaining competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.', 'score': 9, 'issue_id': 4917, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'fdd7f0b217aa40b2', 'authors': ['Gen Luo', 'Wenhan Dou', 'Wenhao Li', 'Zhaokai Wang', 'Xue Yang', 'Changyao Tian', 'Hao Li', 'Weiyun Wang', 'Wenhai Wang', 'Xizhou Zhu', 'Yu Qiao', 'Jifeng Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12566.jpg', 'data': {'categories': ['#multimodal', '#training', '#architecture', '#agi', '#optimization', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Mono-InternVL: Монолитная мультимодальная модель с улучшенным визуальным обучением', 'desc': 'Mono-InternVL - это усовершенствованная монолитная мультимодальная большая языковая модель (MLLM), которая объединяет визуальное кодирование и языковое декодирование в единую модель. Она использует визуальных экспертов и улучшенные стратегии предобучения для повышения визуального обучения и снижения вычислительных затрат. Mono-InternVL применяет архитектуру смеси экспертов и инновационное эндогенное визуальное предобучение (EViP) для максимизации своих визуальных возможностей. Модель демонстрирует конкурентоспособную производительность по сравнению с существующими MLLM, значительно улучшая результаты на различных бенчмарках.'}, 'en': {'title': 'Revolutionizing Visual Learning with Mono-InternVL', 'desc': 'The paper introduces Mono-InternVL, a monolithic Multimodal Large Language Model (MLLM) that combines visual and language processing into one model. It addresses issues like unstable optimization and catastrophic forgetting by embedding a new visual parameter space into a pre-trained language model, allowing for stable learning from noisy data. The model incorporates a multimodal mixture-of-experts architecture and an innovative Endogenous Visual Pre-training (EViP) strategy to enhance visual capabilities. Additionally, Mono-InternVL-1.5 is presented as a more efficient version that reduces training costs while maintaining competitive performance across multiple benchmarks.'}, 'zh': {'title': 'Mono-InternVL：高效的单体多模态大语言模型', 'desc': 'Mono-InternVL是一种先进的单体多模态大语言模型，结合了视觉专家和改进的预训练策略，以增强视觉学习并降低计算成本，同时保持竞争力的性能。该模型通过将新的视觉参数空间嵌入到预训练的语言模型中，解决了不稳定优化和灾难性遗忘的问题。Mono-InternVL-1.5在此基础上进一步优化，采用了改进的内生视觉预训练（EViP++），引入了额外的视觉注意力专家，并高效地重新组织了预训练过程。实验结果表明，Mono-InternVL在多个基准测试中表现优于现有的单体多模态大语言模型，且在推理时显著降低了延迟。'}}}, {'id': 'https://huggingface.co/papers/2507.13984', 'title': 'CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models', 'url': 'https://huggingface.co/papers/2507.13984', 'abstract': 'CSD-VAR, a Visual Autoregressive Modeling approach, enhances content-style decomposition by introducing scale-aware optimization, SVD-based rectification, and augmented K-V memory, outperforming diffusion models in content preservation and stylization.  \t\t\t\t\tAI-generated summary \t\t\t\t Disentangling content and style from a single image, known as content-style decomposition (CSD), enables recontextualization of extracted content and stylization of extracted styles, offering greater creative flexibility in visual synthesis. While recent personalization methods have explored the decomposition of explicit content style, they remain tailored for diffusion models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a promising alternative with a next-scale prediction paradigm, achieving performance comparable to that of diffusion models. In this paper, we explore VAR as a generative framework for CSD, leveraging its scale-wise generation process for improved disentanglement. To this end, we propose CSD-VAR, a novel method that introduces three key innovations: (1) a scale-aware alternating optimization strategy that aligns content and style representation with their respective scales to enhance separation, (2) an SVD-based rectification method to mitigate content leakage into style representations, and (3) an Augmented Key-Value (K-V) memory enhancing content identity preservation. To benchmark this task, we introduce CSD-100, a dataset specifically designed for content-style decomposition, featuring diverse subjects rendered in various artistic styles. Experiments demonstrate that CSD-VAR outperforms prior approaches, achieving superior content preservation and stylization fidelity.', 'score': 7, 'issue_id': 4921, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '0b3986b86b2e5406', 'authors': ['Quang-Binh Nguyen', 'Minh Luu', 'Quang Nguyen', 'Anh Tran', 'Khoi Nguyen'], 'affiliations': ['MovianAI', 'Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.13984.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#synthetic', '#cv'], 'emoji': '🎨', 'ru': {'title': 'CSD-VAR: Революция в разделении контента и стиля изображений', 'desc': 'CSD-VAR - это новый подход к визуальному авторегрессионному моделированию для декомпозиции контента и стиля изображений. Метод вводит три ключевые инновации: оптимизацию с учетом масштаба, SVD-ректификацию и расширенную память ключ-значение. CSD-VAR превосходит диффузионные модели в сохранении контента и стилизации. Для оценки метода авторы создали специальный датасет CSD-100 с разнообразными сюжетами в различных художественных стилях.'}, 'en': {'title': 'CSD-VAR: Mastering Content and Style Separation in Visual Synthesis', 'desc': "CSD-VAR is a new method for separating content and style in images, which is important for creative tasks in visual synthesis. It uses a Visual Autoregressive Modeling approach that improves how content and style are handled by focusing on their different scales. The method includes innovations like a scale-aware optimization strategy, an SVD-based technique to prevent content from leaking into style, and an enhanced memory system to keep the content's identity intact. Experiments show that CSD-VAR performs better than previous methods, especially in maintaining the quality of both content and style."}, 'zh': {'title': 'CSD-VAR：提升内容与风格分解的新方法', 'desc': 'CSD-VAR是一种视觉自回归建模方法，旨在通过引入规模感知优化、基于SVD的修正和增强的K-V记忆来提升内容与风格的分解效果。内容风格分解（CSD）允许从单一图像中提取内容和风格，从而实现更大的创作灵活性。与传统的扩散模型相比，CSD-VAR在内容保留和风格化方面表现更佳。我们还提出了CSD-100数据集，以便于评估内容风格分解的效果。'}}}, {'id': 'https://huggingface.co/papers/2507.13158', 'title': 'Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities', 'url': 'https://huggingface.co/papers/2507.13158', 'abstract': 'A review of advancements in aligning large language models using inverse reinforcement learning, emphasizing challenges and opportunities in neural reward modeling and sparse-reward reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.', 'score': 6, 'issue_id': 4923, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '2f87de19df04c9a4', 'authors': ['Hao Sun', 'Mihaela van der Schaar'], 'affiliations': ['Department of Applied Mathematics and Theoretical Physics University of Cambridge Cambridge, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.13158.jpg', 'data': {'categories': ['#rlhf', '#training', '#rl', '#survey', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Выравнивание языковых моделей через призму обратного обучения с подкреплением', 'desc': 'Статья представляет обзор последних достижений в области выравнивания больших языковых моделей (LLM) с использованием обратного обучения с подкреплением (IRL). Авторы подчеркивают важность создания нейронных моделей вознаграждения на основе данных о поведении человека. Рассматриваются ключевые проблемы и возможности применения IRL для выравнивания LLM, а также практические аспекты, включая наборы данных, метрики оценки и эффективные методы обучения. Статья также анализирует связь с исследованиями в области обучения с подкреплением с разреженным вознаграждением.'}, 'en': {'title': 'Aligning Language Models: Challenges and Opportunities in Reinforcement Learning', 'desc': 'This paper reviews the progress in aligning large language models (LLMs) using inverse reinforcement learning (IRL), focusing on the unique challenges and opportunities in this area. It emphasizes the importance of developing neural reward models based on human data to improve the alignment of LLMs, which is crucial for creating more reliable AI systems. The authors discuss the differences between traditional reinforcement learning methods and those specifically tailored for LLM alignment, providing insights into practical aspects like datasets and evaluation metrics. By synthesizing existing research, the paper aims to highlight unresolved issues and suggest future research directions in the field of LLM alignment through reinforcement learning techniques.'}, 'zh': {'title': '逆强化学习助力大型语言模型对齐的未来', 'desc': '本文回顾了使用逆强化学习（IRL）对大型语言模型（LLM）进行对齐的最新进展，强调了神经奖励建模和稀疏奖励强化学习中的挑战与机遇。在大型语言模型的时代，对齐问题变得尤为重要，影响着机器智能的可靠性和可控性。文章介绍了强化学习的基本概念，并探讨了IRL在LLM对齐中的应用，特别是如何从人类数据中构建神经奖励模型。最后，文章总结了当前研究中的关键挑战和未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2507.10605', 'title': 'RedOne: Revealing Domain-specific LLM Post-Training in Social Networking\n  Services', 'url': 'https://huggingface.co/papers/2507.10605', 'abstract': 'RedOne, a domain-specific LLM, enhances performance across multiple SNS tasks through a three-stage training strategy, improving generalization and reducing harmful content exposure.  \t\t\t\t\tAI-generated summary \t\t\t\t As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.', 'score': 4, 'issue_id': 4914, 'pub_date': '2025-07-13', 'pub_date_card': {'ru': '13 июля', 'en': 'July 13', 'zh': '7月13日'}, 'hash': '0f03049bcdca7ad4', 'authors': ['Fei Zhao', 'Chonggang Lu', 'Yue Wang', 'Zheyong Xie', 'Ziyan Liu', 'Haofu Qian', 'JianZhao Huang', 'Fangcheng Shi', 'Zijie Meng', 'Hongcheng Guo', 'Mingqian He', 'Xinze Lyu', 'Yiming Lu', 'Ziyang Xiang', 'Zheyu Ye', 'Chengqiang Lu', 'Zhe Xu', 'Yi Wu', 'Yao Hu', 'Yan Gao', 'Jun Fan', 'Xiaolong Jiang', 'Weiting Liu', 'Boyang Wang', 'Shaosheng Cao'], 'affiliations': ['NLP Team, Xiaohongshu Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2507.10605.jpg', 'data': {'categories': ['#training', '#alignment', '#optimization', '#dataset', '#multilingual', '#science'], 'emoji': '🚀', 'ru': {'title': 'RedOne: универсальная языковая модель для революции в социальных сетях', 'desc': 'RedOne - это специализированная языковая модель для социальных сетей, разработанная с использованием трехэтапной стратегии обучения. Модель демонстрирует улучшение производительности в среднем на 14.02% по 8 основным задачам SNS по сравнению с базовыми моделями. RedOne также показывает хорошие результаты в обнаружении вредоносного контента и повышении кликабельности в поисковой выдаче. Модель обладает отличной обобщающей способностью для различных задач SNS и перспективна для применения в реальных сценариях.'}, 'en': {'title': 'RedOne: Elevating SNS Performance with Domain-Specific LLMs', 'desc': 'RedOne is a specialized large language model (LLM) designed to improve performance on various social networking service (SNS) tasks. It employs a three-stage training strategy that includes continued pretraining, supervised fine-tuning, and preference optimization, which helps it generalize better across different tasks. The model shows significant improvements, achieving up to 14.02% better performance on major SNS tasks and reducing harmful content exposure by 11.23%. Overall, RedOne demonstrates its effectiveness as a domain-specific LLM, enhancing interaction quality and content management in real-world SNS applications.'}, 'zh': {'title': 'RedOne：社交网络服务的强大语言模型', 'desc': 'RedOne是一种特定领域的大型语言模型（LLM），通过三阶段的训练策略提升了社交网络服务（SNS）任务的表现。该模型的训练包括持续预训练、监督微调和偏好优化，使用了大规模的真实世界数据集。实验结果显示，RedOne在8个主要SNS任务上平均提升了14.02%的性能，并在有害内容检测中减少了11.23%的曝光率。这些成果表明RedOne在多任务上具有良好的泛化能力，适用于实际应用场景。'}}}, {'id': 'https://huggingface.co/papers/2507.12455', 'title': 'Mitigating Object Hallucinations via Sentence-Level Early Intervention', 'url': 'https://huggingface.co/papers/2507.12455', 'abstract': 'SENTINEL reduces hallucinations in multimodal large language models by iteratively generating and validating sentence-level outputs using in-domain preference learning and context-aware preference loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose **SENTINEL** (**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90\\% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at https://github.com/pspdada/SENTINEL.', 'score': 3, 'issue_id': 4922, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'f80b51f3876eeca6', 'authors': ['Shangpin Peng', 'Senqiao Yang', 'Li Jiang', 'Zhuotao Tian'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2507.12455.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#open_source', '#benchmark', '#data', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Защита от галлюцинаций в мультимодальных ИИ-моделях', 'desc': 'SENTINEL - это фреймворк для снижения галлюцинаций в мультимодальных больших языковых моделях. Он использует итеративную генерацию и валидацию выходных данных на уровне предложений с помощью обучения предпочтениям в предметной области. SENTINEL применяет контекстно-зависимую функцию потерь предпочтений для акцентирования дискриминативного обучения на уровне предложений. Экспериментальные результаты показывают, что SENTINEL может снизить галлюцинации более чем на 90% по сравнению с исходной моделью.'}, 'en': {'title': 'SENTINEL: Early Intervention to Combat Hallucinations in MLLMs', 'desc': "SENTINEL is a framework designed to reduce hallucinations in multimodal large language models (MLLMs) by focusing on early intervention during text generation. It utilizes in-domain preference learning to create high-quality preference pairs without needing human annotations, validating outputs through cross-checking with open-vocabulary detectors. By classifying sentences as hallucinated or non-hallucinated, SENTINEL builds context-aware preference data that enhances the model's ability to distinguish between accurate and fabricated content. The approach significantly decreases hallucinations by over 90% and outperforms existing methods, showcasing its effectiveness in improving MLLM performance."}, 'zh': {'title': 'SENTINEL：消除多模态模型中的幻觉', 'desc': 'SENTINEL 是一种新框架，旨在减少多模态大语言模型中的幻觉现象。它通过迭代生成和验证句子级输出，利用领域内偏好学习和上下文感知偏好损失来实现这一目标。研究发现，幻觉主要在文本生成的早期阶段出现，并会在后续输出中传播。通过构建上下文一致的正样本和幻觉负样本，SENTINEL 显著提高了模型的准确性，减少了超过90%的幻觉现象。'}}}, {'id': 'https://huggingface.co/papers/2507.13302', 'title': 'The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations', 'url': 'https://huggingface.co/papers/2507.13302', 'abstract': 'GEA, a public arena that includes energy consumption data, shows that users often prefer smaller, more energy-efficient language models over larger, more complex ones.  \t\t\t\t\tAI-generated summary \t\t\t\t The evaluation of large language models is a complex task, in which several approaches have been proposed. The most common is the use of automated benchmarks in which LLMs have to answer multiple-choice questions of different topics. However, this method has certain limitations, being the most concerning, the poor correlation with the humans. An alternative approach, is to have humans evaluate the LLMs. This poses scalability issues as there is a large and growing number of models to evaluate making it impractical (and costly) to run traditional studies based on recruiting a number of evaluators and having them rank the responses of the models. An alternative approach is the use of public arenas, such as the popular LM arena, on which any user can freely evaluate models on any question and rank the responses of two models. The results are then elaborated into a model ranking. An increasingly important aspect of LLMs is their energy consumption and, therefore, evaluating how energy awareness influences the decisions of humans in selecting a model is of interest. In this paper, we present GEA, the Generative Energy Arena, an arena that incorporates information on the energy consumption of the model in the evaluation process. Preliminary results obtained with GEA are also presented, showing that for most questions, when users are aware of the energy consumption, they favor smaller and more energy efficient models. This suggests that for most user interactions, the extra cost and energy incurred by the more complex and top-performing models do not provide an increase in the perceived quality of the responses that justifies their use.', 'score': 1, 'issue_id': 4919, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': 'ae45d08b563bfedc', 'authors': ['Carlos Arriaga', 'Gonzalo Martínez', 'Eneko Sendin', 'Javier Conde', 'Pedro Reviriego'], 'affiliations': ['ETSI de Telecomunicación, Universidad Politécnica de Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.13302.jpg', 'data': {'categories': ['#ethics', '#dataset', '#benchmark', '#small_models', '#optimization'], 'emoji': '🌿', 'ru': {'title': 'Эффективность важнее размера: пользователи выбирают энергоэкономичные языковые модели', 'desc': 'В статье представлена GEA (Generative Energy Arena) - публичная арена для оценки языковых моделей, учитывающая их энергопотребление. Результаты показывают, что пользователи часто предпочитают меньшие и более энергоэффективные модели более крупным и сложным. Это свидетельствует о том, что для большинства пользовательских взаимодействий дополнительные затраты и энергия, потребляемые более сложными моделями, не оправданы с точки зрения воспринимаемого качества ответов. GEA предлагает альтернативный подход к оценке языковых моделей, учитывающий важный аспект энергопотребления.'}, 'en': {'title': 'Choose Efficiency: Users Prefer Smaller Language Models!', 'desc': 'The paper introduces GEA, the Generative Energy Arena, which allows users to evaluate language models while considering their energy consumption. It highlights the limitations of traditional evaluation methods, such as automated benchmarks and human evaluations, particularly in terms of scalability and correlation with human judgment. The findings suggest that users tend to prefer smaller, more energy-efficient models over larger ones when they are aware of energy costs. This indicates that the perceived quality of responses does not always justify the higher energy usage of more complex models.'}, 'zh': {'title': '选择更小更高效的语言模型', 'desc': '本文介绍了生成能量竞技场（GEA），这是一个包含能量消耗数据的公共评估平台。研究表明，用户在选择语言模型时，往往更倾向于选择较小且能效更高的模型，而不是更大更复杂的模型。传统的评估方法存在局限性，尤其是与人类评估结果的相关性较差。通过GEA，用户可以在了解模型能耗的情况下进行评估，初步结果显示，用户在意识到能耗后，倾向于选择能效更高的模型。'}}}, {'id': 'https://huggingface.co/papers/2507.13391', 'title': 'Quantitative Risk Management in Volatile Markets with an Expectile-Based\n  Framework for the FTSE Index', 'url': 'https://huggingface.co/papers/2507.13391', 'abstract': 'This research presents a framework for quantitative risk management in volatile markets, specifically focusing on expectile-based methodologies applied to the FTSE 100 index. Traditional risk measures such as Value-at-Risk (VaR) have demonstrated significant limitations during periods of market stress, as evidenced during the 2008 financial crisis and subsequent volatile periods. This study develops an advanced expectile-based framework that addresses the shortcomings of conventional quantile-based approaches by providing greater sensitivity to tail losses and improved stability in extreme market conditions. The research employs a dataset spanning two decades of FTSE 100 returns, incorporating periods of high volatility, market crashes, and recovery phases. Our methodology introduces novel mathematical formulations for expectile regression models, enhanced threshold determination techniques using time series analysis, and robust backtesting procedures. The empirical results demonstrate that expectile-based Value-at-Risk (EVaR) consistently outperforms traditional VaR measures across various confidence levels and market conditions. The framework exhibits superior performance during volatile periods, with reduced model risk and enhanced predictive accuracy. Furthermore, the study establishes practical implementation guidelines for financial institutions and provides evidence-based recommendations for regulatory compliance and portfolio management. The findings contribute significantly to the literature on financial risk management and offer practical tools for practitioners dealing with volatile market environments.', 'score': 1, 'issue_id': 4919, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '08b61c33b30fa00a', 'authors': ['Abiodun Finbarrs Oketunji'], 'affiliations': ['University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.13391.jpg', 'data': {'categories': ['#dataset', '#math'], 'emoji': '📊', 'ru': {'title': 'Экспектиль-базированное управление рисками: новый стандарт для волатильных рынков', 'desc': 'Это исследование представляет новый подход к управлению рисками на волатильных рынках, основанный на методологии экспектилей, применяемой к индексу FTSE 100. Разработанная система решает проблемы традиционных мер риска, таких как Value-at-Risk (VaR), обеспечивая большую чувствительность к экстремальным потерям и улучшенную стабильность в условиях рыночного стресса. Эмпирические результаты показывают, что экспектиль-базированный VaR (EVaR) стабильно превосходит традиционные меры VaR при различных уровнях доверия и рыночных условиях. Исследование вносит значительный вклад в литературу по управлению финансовыми рисками и предлагает практические инструменты для работы в волатильной рыночной среде.'}, 'en': {'title': 'Enhancing Risk Management with Expectile-Based Approaches', 'desc': 'This research introduces a new framework for managing financial risk in unstable markets, focusing on expectile-based methods for the FTSE 100 index. Unlike traditional Value-at-Risk (VaR), which struggles during market stress, the expectile-based approach offers better sensitivity to extreme losses and stability in turbulent times. The study uses two decades of data to develop advanced expectile regression models and robust backtesting techniques, showing that expectile-based Value-at-Risk (EVaR) outperforms traditional measures. The findings provide valuable insights and practical guidelines for financial institutions to enhance risk management and comply with regulations in volatile environments.'}, 'zh': {'title': '基于期望值的风险管理新框架', 'desc': '本研究提出了一种用于波动市场的定量风险管理框架，特别关注基于期望值的方法，应用于FTSE 100指数。传统的风险度量方法如风险价值（VaR）在市场压力期间表现出显著的局限性，尤其是在2008年金融危机及其后的波动时期。该研究开发了一种先进的基于期望值的框架，解决了传统分位数方法的不足，提供了对尾部损失的更高敏感性和在极端市场条件下的更好稳定性。实证结果表明，基于期望值的风险价值（EVaR）在各种置信水平和市场条件下始终优于传统的VaR度量。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (21)', '#agents (39)', '#agi (13)', '#alignment (22)', '#architecture (68)', '#audio (7)', '#benchmark (114)', '#cv (43)', '#data (39)', '#dataset (93)', '#diffusion (33)', '#ethics (7)', '#games (31)', '#graphs (5)', '#hallucinations (14)', '#healthcare (12)', '#inference (20)', '#interpretability (21)', '#leakage (3)', '#long_context (18)', '#low_resource (12)', '#machine_translation (2)', '#math (6)', '#multilingual (10)', '#multimodal (75)', '#open_source (59)', '#optimization (112)', '#plp (3)', '#rag (10)', '#reasoning (82)', '#rl (45)', '#rlhf (11)', '#robotics (9)', '#science (17)', '#security (8)', '#small_models (10)', '#story_generation (1)', '#survey (13)', '#synthetic (16)', '#training (126)', '#transfer_learning (17)', '#video (29)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-22 12:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-22 12:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-22 12:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    