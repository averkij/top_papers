
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 46 papers. July 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ˜ÑĞ»ÑŒ 2025</span> | <span id="title-articles-count">46 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-06.html">â¬…ï¸ <span id="prev-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-08.html">â¡ï¸ <span id="next-date">08.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ˜ÑĞ»ÑŒ 2025', 'en': 'July 2025', 'zh': '7æœˆ2025å¹´'};
        let feedDateNext = {'ru': '08.2025', 'en': '08/2025', 'zh': '8æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '06.2025', 'en': '06/2025', 'zh': '6æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.01949', 'title': 'Kwai Keye-VL Technical Report', 'url': 'https://huggingface.co/papers/2507.01949', 'abstract': "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.", 'score': 85, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': 'ca23195c7fa1bb87', 'authors': ['Kwai Keye Team', 'Biao Yang', 'Bin Wen', 'Changyi Liu', 'Chenglong Chu', 'Chengru Song', 'Chongling Rao', 'Chuan Yi', 'Da Li', 'Dunju Zang', 'Fan Yang', 'Guorui Zhou', 'Hao Peng', 'Haojie Ding', 'Jiaming Huang', 'Jiangxia Cao', 'Jiankang Chen', 'Jingyun Hua', 'Jin Ouyang', 'Kaibing Chen', 'Kaiyu Jiang', 'Kaiyu Tang', 'Kun Gai', 'Shengnan Zhang', 'Siyang Mao', 'Sui Huang', 'Tianke Zhang', 'Tingting Gao', 'Wei Chen', 'Wei Yuan', 'Xiangyu Wu', 'Xiao Hu', 'Xingyu Lu', 'Yang Zhou', 'Yi-Fan Zhang', 'Yiping Yang', 'Yulong Chen', 'Zhenhua Wu', 'Zhenyu Li', 'Zhixin Ling', 'Ziming Li', 'Dehua Ma', 'Di Xu', 'Haixuan Gao', 'Hang Li', 'Jiawei Guo', 'Jing Wang', 'Lejian Ren', 'Muhao Wei', 'Qianqian Wang', 'Qigen Hu', 'Shiyao Wang', 'Tao Yu', 'Xinchen Luo', 'Yan Li', 'Yiming Liang', 'Yuhang Hu', 'Zeyi Lu', 'Zhuoran Yang', 'Zixing Zhang'], 'affiliations': ['Kuaishou Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.01949.jpg', 'data': {'categories': ['#reasoning', '#rl', '#video', '#dataset', '#benchmark', '#training', '#multimodal', '#alignment'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Kwai Keye-VL: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Kwai Keye-VL - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ 600 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Keye-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Short-Video Understanding with Keye-VL', 'desc': 'This paper presents Kwai Keye-VL, a multimodal large language model designed to improve understanding of short-form videos, which are increasingly popular. Keye-VL is built on a vast dataset of over 600 billion tokens, focusing on video content, and employs a unique training strategy that includes a four-stage pre-training and a two-phase post-training process. The model enhances its reasoning abilities through a novel data mixture that encourages different modes of thinking, followed by reinforcement learning to refine its outputs. Evaluations demonstrate that Keye-VL outperforms existing models on video benchmarks while maintaining strong performance on general image tasks.'}, 'zh': {'title': 'çŸ­è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šKwai Keye-VL', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKwai Keye-VLçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºçŸ­è§†é¢‘ç†è§£ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰80äº¿ä¸ªå‚æ•°ï¼Œæ—¨åœ¨æå‡å¯¹åŠ¨æ€ã€ä¿¡æ¯å¯†é›†å‹çŸ­è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨è§†è§‰-è¯­è¨€èƒ½åŠ›ã€‚Keye-VLçš„å¼€å‘åŸºäºä¸¤ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šä¸€ä¸ªè¶…è¿‡6000äº¿ä¸ªæ ‡è®°çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œç‰¹åˆ«å¼ºè°ƒè§†é¢‘å†…å®¹ï¼Œä»¥åŠä¸€ç§åˆ›æ–°çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬å››é˜¶æ®µçš„é¢„è®­ç»ƒå’Œä¸¤é˜¶æ®µçš„åè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå¯¹é½æ­¥éª¤ï¼ŒKeye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨ä¸€èˆ¬å›¾åƒä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01945', 'title': 'LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory', 'url': 'https://huggingface.co/papers/2507.01945', 'abstract': 'Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.', 'score': 54, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': 'cf167e3958c2df99', 'authors': ['Nan Chen', 'Mengqi Huang', 'Yihao Meng', 'Zhendong Mao'], 'affiliations': ['Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.01945.jpg', 'data': {'categories': ['#video', '#open_source', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'LongAnimation: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LongAnimation. Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğµ. LongAnimation Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ SketchDiT Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (DGLM) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ°Ğº Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… (14 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²), Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… (Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ 500 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²) Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ĞºĞ¾Ğ»Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ¼.'}, 'en': {'title': 'Achieving Long-Term Color Consistency in Animation Colorization', 'desc': 'This paper presents LongAnimation, a framework designed to automate the colorization of long animations, addressing the high labor costs associated with traditional methods. It critiques existing approaches that focus on short-term colorization and local feature fusion, which often overlook the importance of global color consistency. The proposed method utilizes a Dynamic Global-Local Memory (DGLM) to dynamically integrate global features with current generation data, ensuring a cohesive color palette throughout the animation. Additionally, a Color Consistency Reward is introduced to enhance the smoothness of transitions between video segments, demonstrating effectiveness in both short-term and long-term animation colorization tasks.'}, 'zh': {'title': 'åŠ¨æ€å…¨å±€-å±€éƒ¨èŒƒå¼å®ç°åŠ¨ç”»è‰²å½©ä¸€è‡´æ€§', 'desc': 'åŠ¨ç”»ä¸Šè‰²æ˜¯åŠ¨ç”»äº§ä¸šç”Ÿäº§ä¸­çš„é‡è¦ç¯èŠ‚ï¼Œé•¿æ—¶é—´åŠ¨ç”»çš„ä¸Šè‰²æˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼ŒåŸºäºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è‡ªåŠ¨åŒ–é•¿æ—¶é—´åŠ¨ç”»ä¸Šè‰²å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨çŸ­æœŸä¸Šè‰²ï¼Œé‡‡ç”¨å±€éƒ¨èŒƒå¼æ¥å®ç°å±€éƒ¨ç‰‡æ®µä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œä½†å¿½è§†äº†å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´é•¿æœŸè‰²å½©ä¸€è‡´æ€§ä¸è¶³ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŠ¨æ€å…¨å±€-å±€éƒ¨èŒƒå¼ï¼Œé€šè¿‡åŠ¨æ€æå–ä¸å½“å‰ç”Ÿæˆç›¸å…³çš„å…¨å±€è‰²å½©ä¸€è‡´ç‰¹å¾ï¼Œæå‡ºäº†LongAnimationæ¡†æ¶ï¼Œæœ‰æ•ˆç»´æŠ¤äº†çŸ­æœŸå’Œé•¿æœŸçš„è‰²å½©ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01634', 'title': 'Depth Anything at Any Condition', 'url': 'https://huggingface.co/papers/2507.01634', 'abstract': 'We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.   Project Page: https://ghost233lism.github.io/depthanything-AC-page   Code: https://github.com/HVision-NKU/DepthAnythingAC', 'score': 28, 'issue_id': 4615, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': '48dee9247e2393f0', 'authors': ['Boyuan Sun', 'Modi Jin', 'Bowen Yin', 'Qibin Hou'], 'affiliations': ['VCIP, School of Computer Science, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01634.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#data', '#training', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»ÑĞ±Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…', 'desc': 'DepthAnything-AC - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ»Ğ¸ÑÑŒ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ÑƒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Mastering Depth Estimation in Any Environment', 'desc': 'DepthAnything-AC is a monocular depth estimation model designed to perform well in various challenging environmental conditions. It addresses the limitations of previous models that struggle with issues like lighting changes and weather effects. The model uses an unsupervised consistency regularization approach, allowing it to learn effectively from a small amount of unlabeled data. Additionally, it incorporates a Spatial Distance Constraint to improve the accuracy of depth estimation by focusing on the relationships between different image patches.'}, 'zh': {'title': 'åœ¨ä»»ä½•æ¡ä»¶ä¸‹çš„æ·±åº¦ä¼°è®¡æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDepth Anything at Any Conditionï¼ˆDepthAnything-ACï¼‰çš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒæ¡ä»¶ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ·±åº¦ä¼°è®¡ã€‚ä»¥å¾€çš„æ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨ä¸€èˆ¬åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚çš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¦‚å…‰ç…§å˜åŒ–å’Œæ¶åŠ£å¤©æ°”ä¸‹ï¼Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºå’Œä»å—æŸå›¾åƒç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾çš„å›°éš¾ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£ä¸€è‡´æ€§æ­£åˆ™åŒ–å¾®è°ƒæ–¹æ³•ï¼Œä»…éœ€å°‘é‡æœªæ ‡è®°æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDepthAnything-ACåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†é›¶æ ·æœ¬èƒ½åŠ›ï¼ŒåŒ…æ‹¬çœŸå®ä¸–ç•Œçš„æ¶åŠ£å¤©æ°”åŸºå‡†å’ŒåˆæˆæŸååŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01925', 'title': 'A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective', 'url': 'https://huggingface.co/papers/2507.01925', 'abstract': 'The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.', 'score': 13, 'issue_id': 4618, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': '28708b74dd1e7612', 'authors': ['Yifan Zhong', 'Fengshuo Bai', 'Shaofei Cai', 'Xuchuan Huang', 'Zhang Chen', 'Xiaowei Zhang', 'Yuanfei Wang', 'Shaoyang Guo', 'Tianrui Guan', 'Ka Nam Lui', 'Zhiquan Qi', 'Yitao Liang', 'Yuanpei Chen', 'Yaodong Yang'], 'affiliations': ['Institute for AI, Peking University', 'PKU-PsiBot Joint Lab', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01925.jpg', 'data': {'categories': ['#survey', '#reasoning', '#multimodal', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (VLA) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº VLA, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ğ´, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ VLA.'}, 'en': {'title': 'Unifying Vision-Language-Action Models through Action Tokenization', 'desc': 'This paper discusses the progress of vision-language-action (VLA) models, which integrate visual and linguistic inputs to perform actions in the physical world. It identifies a common framework among these models, where a series of VLA modules process inputs to generate action tokens that convey actionable information. The authors categorize these action tokens into various types, such as language descriptions and trajectories, highlighting the importance of how they are formulated. The survey aims to clarify the role of action tokens in VLA development, assess their strengths and weaknesses, and suggest future research directions to enhance the effectiveness of VLA models.'}, 'zh': {'title': 'ç»Ÿä¸€è¡ŒåŠ¨æ ‡è®°ï¼Œæ¨åŠ¨VLAæ¨¡å‹å‘å±•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„è¿›å±•ã€‚å°½ç®¡å½“å‰çš„VLAæ¨¡å‹æ–¹æ³•å¤šæ ·ï¼Œä½†å¯ä»¥ç»Ÿä¸€åœ¨ä¸€ä¸ªæ¡†æ¶ä¸‹ï¼Œå¤„ç†è§†è§‰å’Œè¯­è¨€è¾“å…¥ï¼Œç”Ÿæˆé€æ­¥ç¼–ç çš„è¡ŒåŠ¨æ ‡è®°ã€‚æ–‡ç« è¿˜æŒ‡å‡ºï¼ŒVLAæ¨¡å‹çš„ä¸»è¦è®¾è®¡é€‰æ‹©åœ¨äºè¡ŒåŠ¨æ ‡è®°çš„å½¢å¼åŒ–æ–¹å¼ï¼ŒåŒ…æ‹¬è¯­è¨€æè¿°ã€ä»£ç ã€å¯ç”¨æ€§ã€è½¨è¿¹ã€ç›®æ ‡çŠ¶æ€ç­‰ã€‚é€šè¿‡å¯¹ç°æœ‰VLAç ”ç©¶çš„åˆ†ç±»å’Œè§£è¯»ï¼Œæœ¬æ–‡æ—¨åœ¨è¯†åˆ«æ”¹è¿›é¢†åŸŸï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01953', 'title': 'FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model', 'url': 'https://huggingface.co/papers/2507.01953', 'abstract': 'We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.', 'score': 10, 'issue_id': 4617, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': 'de2dfee54bea9af5', 'authors': ['Yukang Cao', 'Chenyang Si', 'Jinghao Wang', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.01953.jpg', 'data': {'categories': ['#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'FreeMorph: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'FreeMorph - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, FreeMorph Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑˆĞ°Ğ³Ğ¸ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FreeMorph Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 10-50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ€Ñ„Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Morphing with Tuning-Free Efficiency', 'desc': 'FreeMorph is a novel method for image morphing that does not require tuning, making it efficient and effective for images with different meanings or layouts. It overcomes limitations of previous techniques that needed fine-tuning of diffusion models, which could lead to time delays and inconsistencies. The method introduces a guidance-aware spherical interpolation to enhance the quality of transitions and reduce identity loss, while also employing a step-oriented variation trend for smoother morphing. Evaluations show that FreeMorph is significantly faster and achieves superior results compared to existing methods, setting a new benchmark in the field.'}, 'zh': {'title': 'FreeMorphï¼šæ— éœ€è°ƒä¼˜çš„é«˜æ•ˆå›¾åƒå˜å½¢æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†FreeMorphï¼Œè¿™æ˜¯ä¸€ç§é¦–ä¸ªæ— éœ€è°ƒä¼˜çš„å›¾åƒå˜å½¢æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†å…·æœ‰ä¸åŒè¯­ä¹‰æˆ–å¸ƒå±€çš„è¾“å…¥ã€‚ä¸ä¾èµ–äºå¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFreeMorphæ— éœ€é’ˆå¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜ä¿çœŸåœ°å®ç°å›¾åƒå˜å½¢ã€‚å°½ç®¡è°ƒä¼˜è‡ªç”±çš„æ–¹æ³•åœ¨æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†ç”±äºå¤šæ­¥å»å™ªè¿‡ç¨‹çš„éçº¿æ€§ç‰¹æ€§å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åå·®ï¼Œä¿æŒé«˜è´¨é‡ç»“æœä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æŒ‡å¯¼æ„ŸçŸ¥çš„çƒé¢æ’å€¼è®¾è®¡å’Œæ­¥éª¤å¯¼å‘çš„å˜åŒ–è¶‹åŠ¿ï¼ŒæˆåŠŸè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½¿FreeMorphåœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•å¿«10åˆ°50å€ï¼Œå¹¶å»ºç«‹äº†å›¾åƒå˜å½¢çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01957', 'title': 'Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation', 'url': 'https://huggingface.co/papers/2507.01957', 'abstract': 'We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256times256 res.) and 1024 to 48 (512times512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4times lower latency than previous parallelized autoregressive models.', 'score': 7, 'issue_id': 4621, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': 'b2594c8c1eebcb0c', 'authors': ['Zhuoyang Zhang', 'Luke J. Huang', 'Chengyue Wu', 'Shang Yang', 'Kelly Peng', 'Yao Lu', 'Song Han'], 'affiliations': ['First Intelligence', 'MIT', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2507.01957.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Locality-aware Parallel Decoding (LPD) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 3.4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Accelerating Image Generation with Locality-aware Parallel Decoding', 'desc': 'This paper introduces Locality-aware Parallel Decoding (LPD) to improve the speed of autoregressive image generation. Traditional methods face high latency due to memory constraints when predicting the next patch of an image. The authors propose two innovative techniques: a flexible architecture for parallelized autoregressive modeling and a locality-aware generation ordering that optimizes the order of patch generation. These advancements significantly reduce the number of generation steps and latency while maintaining high image quality, outperforming previous models.'}, 'zh': {'title': 'åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§å±€éƒ¨æ„ŸçŸ¥å¹¶è¡Œè§£ç ï¼ˆLPDï¼‰æ–¹æ³•ï¼Œä»¥åŠ é€Ÿè‡ªå›å½’å›¾åƒç”Ÿæˆã€‚ä¼ ç»Ÿçš„è‡ªå›å½’å›¾åƒç”Ÿæˆä¾èµ–äºä¸‹ä¸€ä¸ªè¡¥ä¸çš„é¢„æµ‹ï¼Œè¿™ä¸€è¿‡ç¨‹å—å†…å­˜é™åˆ¶ï¼Œå¯¼è‡´å»¶è¿Ÿè¾ƒé«˜ã€‚æˆ‘ä»¬å¼•å…¥äº†çµæ´»çš„å¹¶è¡Œè‡ªå›å½’å»ºæ¨¡å’Œå±€éƒ¨æ„ŸçŸ¥ç”Ÿæˆé¡ºåºä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼Œä»¥å®ç°é«˜å¹¶è¡Œæ€§å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆæ­¥éª¤ä»256å‡å°‘åˆ°20ï¼Œå¹¶åœ¨ä¸å½±å“è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23552', 'title': 'JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching', 'url': 'https://huggingface.co/papers/2506.23552', 'abstract': 'The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web', 'score': 3, 'issue_id': 4615, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': '69157bacab4dea7d', 'authors': ['Mingi Kwon', 'Joonghyuk Shin', 'Jaeseok Jung', 'Jaesik Park', 'Youngjung Uh'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23552.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#architecture'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°', 'desc': 'JAM-Flow - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ»Ğ¸Ñ†ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ flow matching Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Multi-Modal Diffusion Transformer (MM-DiT) Ñ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Motion-DiT Ğ¸ Audio-DiT. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ. JAM-Flow Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·.'}, 'en': {'title': 'Unified Synthesis of Speech and Facial Motion with JAM-Flow', 'desc': 'This paper presents JAM-Flow, a new framework that combines facial motion and speech synthesis into one model. It uses advanced techniques like flow matching and a Multi-Modal Diffusion Transformer (MM-DiT) to allow for effective interaction between audio and visual data. The model includes specialized components for handling both motion and audio, ensuring that each modality retains its unique characteristics while working together. By training with a unique inpainting-style objective, JAM-Flow can generate synchronized talking heads from various inputs, making it a significant advancement in multi-modal generative modeling.'}, 'zh': {'title': 'ç»Ÿä¸€é¢éƒ¨è¿åŠ¨ä¸è¯­éŸ³çš„ç”Ÿæˆæ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºJAM-Flowçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶åˆæˆé¢éƒ¨è¿åŠ¨å’Œè¯­éŸ³ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æµåŒ¹é…å’Œæ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼ˆMM-DiTï¼‰æ¶æ„ï¼Œé›†æˆäº†ä¸“é—¨çš„è¿åŠ¨å’ŒéŸ³é¢‘æ¨¡å—ã€‚é€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„åŠ›å±‚ï¼Œè¿™äº›æ¨¡å—å®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ï¼ŒåŒæ—¶ä¿ç•™äº†å„è‡ªæ¨¡æ€çš„ä¼˜åŠ¿ã€‚JAM-Flowæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°æ–‡æœ¬é©±åŠ¨çš„åŒæ­¥äººå¤´ç”Ÿæˆå’ŒéŸ³é¢‘é©±åŠ¨çš„åŠ¨ç”»ç­‰ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22868', 'title': 'STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing', 'url': 'https://huggingface.co/papers/2506.22868', 'abstract': 'STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous text-guided video editing methods often suffer from temporal inconsistency, motion distortion, and-most notably-limited domain transformation. We attribute these limitations to insufficient modeling of spatiotemporal pixel relevance during the editing process. To address this, we propose STR-Match, a training-free video editing algorithm that produces visually appealing and spatiotemporally coherent videos through latent optimization guided by our novel STR score. The score captures spatiotemporal pixel relevance across adjacent frames by leveraging 2D spatial attention and 1D temporal modules in text-to-video (T2V) diffusion models, without the overhead of computationally expensive 3D attention mechanisms. Integrated into a latent optimization framework with a latent mask, STR-Match generates temporally consistent and visually faithful videos, maintaining strong performance even under significant domain transformations while preserving key visual attributes of the source. Extensive experiments demonstrate that STR-Match consistently outperforms existing methods in both visual quality and spatiotemporal consistency.', 'score': 3, 'issue_id': 4621, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': '94371810be905c93', 'authors': ['Junsung Lee', 'Junoh Kang', 'Bohyung Han'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2506.22868.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'STR-Match: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'STR-Match - ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ STR. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 2D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ 1D Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. STR-Match Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°.'}, 'en': {'title': 'Enhancing Video Editing with STR-Match: Coherence Meets Quality', 'desc': 'STR-Match is a novel video editing algorithm that enhances the quality and coherence of AI-generated videos. It utilizes a unique STR score to assess the relevance of pixels across time and space, ensuring that the edited videos maintain visual appeal and temporal consistency. By employing 2D spatial attention and 1D temporal attention, STR-Match avoids the complexity of 3D attention mechanisms while still achieving impressive results. The method is training-free and effectively handles significant domain transformations, outperforming existing techniques in visual quality and spatiotemporal coherence.'}, 'zh': {'title': 'STR-Matchï¼šæ—¶ç©ºä¸€è‡´çš„è§†è§‰è§†é¢‘ç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'STR-Matchæ˜¯ä¸€ç§æ— è®­ç»ƒçš„è§†é¢‘ç¼–è¾‘ç®—æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººä¸”æ—¶ç©ºä¸€è‡´çš„è§†é¢‘ã€‚å®ƒé€šè¿‡å¼•å…¥æ–°é¢–çš„STRè¯„åˆ†ï¼Œåˆ©ç”¨2Dç©ºé—´æ³¨æ„åŠ›å’Œ1Dæ—¶é—´æ¨¡å—æ¥æ•æ‰ç›¸é‚»å¸§ä¹‹é—´çš„åƒç´ ç›¸å…³æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSTR-Matché¿å…äº†è®¡ç®—å¼€é”€å¤§çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—çš„é¢†åŸŸè½¬æ¢ä¸‹ä¿æŒè§†é¢‘çš„å…³é”®è§†è§‰ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTR-Matchåœ¨è§†è§‰è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01544', 'title': 'MARVIS: Modality Adaptive Reasoning over VISualizations', 'url': 'https://huggingface.co/papers/2507.01544', 'abstract': 'Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16\\% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis', 'score': 1, 'issue_id': 4624, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 Ğ¸ÑĞ»Ñ', 'en': 'July 2', 'zh': '7æœˆ2æ—¥'}, 'hash': '547bee35865d6ddd', 'authors': ['Benjamin Feuer', 'Lennart Purucker', 'Oussama Elachqar', 'Chinmay Hegde'], 'affiliations': ['NYU', 'Oumi.AI', 'University of Freiburg'], 'pdf_title_img': 'assets/pdf/title_img/2507.01544.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#training', '#open_source', '#dataset', '#science'], 'emoji': 'ğŸ”®', 'ru': {'title': 'MARVIS: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'MARVIS - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ VLM Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸. MARVIS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Gemini Ğ½Ğ° 16% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ.'}, 'en': {'title': 'Unlocking Versatility in Vision-Language Models with MARVIS', 'desc': 'This paper introduces MARVIS, a novel method that enhances small vision-language models by allowing them to predict various data modalities without the need for training. MARVIS converts latent embeddings into visual representations, enabling the models to apply their reasoning capabilities effectively across different domains. The approach demonstrates competitive performance in areas like vision, audio, and biological data, outperforming existing models like Gemini by 16% on average. Importantly, MARVIS maintains privacy by not requiring any domain-specific training or exposing personally identifiable information.'}, 'zh': {'title': 'MARVISï¼šå°å‹æ¨¡å‹çš„å¤šæ¨¡æ€é¢„æµ‹æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMARVISçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§æ•°æ®æ¨¡æ€ä¸Šçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚MARVISé€šè¿‡å°†æ½œåœ¨åµŒå…¥ç©ºé—´è½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´å’Œç»†ç²’åº¦æ¨ç†èƒ½åŠ›ï¼ŒæˆåŠŸè§£è¯»å’Œåˆ©ç”¨è¿™äº›è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æ— éœ€ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨è§†è§‰ã€éŸ³é¢‘ã€ç”Ÿç‰©å’Œè¡¨æ ¼æ•°æ®ç­‰é¢†åŸŸä¸­å®ç°ç«äº‰æ€§çš„æ€§èƒ½ã€‚MARVISåœ¨ä¸æš´éœ²ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å•ä¸ª3Bå‚æ•°æ¨¡å‹çš„è¡¨ç°è¶…è¶Šäº†Geminiï¼Œæ¥è¿‘ä¸“ä¸šæ–¹æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01006', 'title': 'GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.01006', 'abstract': 'A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.', 'score': 135, 'issue_id': 4595, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': '174c869b64ed9ae7', 'authors': ['Wenyi Hong', 'Wenmeng Yu', 'Xiaotao Gu', 'Guo Wang', 'Guobing Gan', 'Haomiao Tang', 'Jiale Cheng', 'Ji Qi', 'Junhui Ji', 'Lihang Pan', 'Shuaiqi Duan', 'Weihan Wang', 'Yan Wang', 'Yean Cheng', 'Zehai He', 'Zhe Su', 'Zhen Yang', 'Ziyang Pan', 'Aohan Zeng', 'Baoxu Wang', 'Boyan Shi', 'Changyu Pang', 'Chenhui Zhang', 'Da Yin', 'Fan Yang', 'Guoqing Chen', 'Jiazheng Xu', 'Jiali Chen', 'Jing Chen', 'Jinhao Chen', 'Jinghao Lin', 'Jinjiang Wang', 'Junjie Chen', 'Leqi Lei', 'Leyi Pan', 'Mingzhi Zhang', 'Qinkai Zheng', 'Sheng Yang', 'Shi Zhong', 'Shiyu Huang', 'Shuyuan Zhao', 'Siyan Xue', 'Shangqin Tu', 'Shengbiao Meng', 'Tianshu Zhang', 'Tianwei Luo', 'Tianxiang Hao', 'Tianle Gong', 'Wenkai Li', 'Wei Jia', 'Xin Lyu', 'Xuancheng Huang', 'Yanling Wang', 'Yadong Xue', 'Yanfeng Wang', 'Yifan An', 'Yifan Du', 'Yiming Shi', 'Yiheng Huang', 'Yilin Niu', 'Yuan Wang', 'Yuanchang Yue', 'Yuchen Li', 'Yutao Zhang', 'Yuxuan Zhang', 'Zhanxiao Du', 'Zhenyu Hou', 'Zhao Xue', 'Zhengxiao Du', 'Zihan Wang', 'Peng Zhang', 'Debing Liu', 'Bin Xu', 'Juanzi Li', 'Minlie Huang', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01006.jpg', 'data': {'categories': ['#open_source', '#rl', '#architecture', '#reasoning', '#multimodal', '#benchmark', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GLM-4.1V-Thinking - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 28 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. GLM-4.1V-9B-Thinking Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4 Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Unlocking Multimodal Reasoning with GLM-4.1V-Thinking', 'desc': 'GLM-4.1V-Thinking is a vision-language model that enhances multimodal reasoning through extensive pre-training and reinforcement learning. The model is built on a strong vision foundation, which is crucial for achieving high performance across various tasks. By employing Reinforcement Learning with Curriculum Sampling, it maximizes its capabilities in areas like STEM problem solving and video understanding. The model has been open-sourced and shows superior performance compared to other models of similar size, making it a significant contribution to the field of AI.'}, 'zh': {'title': 'GLM-4.1V-Thinkingï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°é«˜åº¦', 'desc': 'GLM-4.1V-Thinking æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡é€šç”¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡è¯¾ç¨‹é‡‡æ ·çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨28ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šåŒç±»æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ–‡æ¡£ç†è§£å’ŒSTEMæ¨ç†ç­‰å¤æ‚ä»»åŠ¡ä¸­çš„ç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.01001', 'title': 'SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks', 'url': 'https://huggingface.co/papers/2507.01001', 'abstract': "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.", 'score': 32, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': 'f3c20682e2dcf410', 'authors': ['Yilun Zhao', 'Kaiyan Zhang', 'Tiansheng Hu', 'Sihong Wu', 'Ronan Le Bras', 'Taira Anderson', 'Jonathan Bragg', 'Joseph Chee Chang', 'Jesse Dodge', 'Matt Latzke', 'Yixin Liu', 'Charles McGrady', 'Xiangru Tang', 'Zihang Wang', 'Chen Zhao', 'Hannaneh Hajishirzi', 'Doug Downey', 'Arman Cohan'], 'affiliations': ['Allen Institute for AI', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2507.01001.jpg', 'data': {'categories': ['#open_source', '#science', '#benchmark', '#survey', '#dataset'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹', 'desc': 'SciArena - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 23 Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 13 000 Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ² Ğ¾Ñ‚ Ğ´Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑƒÑ‡Ğ°ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SciArena-Eval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Scientific Model Evaluation through Community Collaboration', 'desc': 'SciArena is a collaborative platform designed to evaluate foundation models specifically for tasks related to scientific literature. It utilizes community voting to rank models, moving away from traditional benchmarks and fostering direct engagement from researchers. The platform supports a variety of models and has gathered extensive voting data, demonstrating strong agreement among participants in their evaluations. Additionally, SciArena introduces a meta-evaluation benchmark, SciArena-Eval, to enhance automated evaluation systems by comparing model assessments with human judgments.'}, 'zh': {'title': 'SciArenaï¼šç§‘å­¦æ–‡çŒ®ä»»åŠ¡çš„ç¤¾åŒºè¯„ä¼°å¹³å°', 'desc': 'SciArenaæ˜¯ä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„å¹³å°ï¼Œç”¨äºè¯„ä¼°åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„ç§‘å­¦æ–‡çŒ®ç†è§£åŸºå‡†ä¸åŒï¼ŒSciArenaé€šè¿‡ç¤¾åŒºæŠ•ç¥¨çš„æ–¹å¼ç›´æ¥å‚ä¸ç ”ç©¶è€…ï¼Œåˆ©ç”¨é›†ä½“æ™ºæ…§å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚è¯¥å¹³å°æ”¯æŒ23ä¸ªå¼€æºå’Œä¸“æœ‰çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶æ”¶é›†äº†æ¥è‡ªä¸åŒç§‘å­¦é¢†åŸŸçš„ç ”ç©¶è€…çš„è¶…è¿‡13,000ä¸ªæŠ•ç¥¨ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†SciArena-Evalï¼Œä¸€ä¸ªåŸºäºæ”¶é›†çš„åå¥½æ•°æ®çš„å…ƒè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨ä¿ƒè¿›æ–‡çŒ®ä»»åŠ¡çš„è‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿçš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23115', 'title': 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings', 'url': 'https://huggingface.co/papers/2506.23115', 'abstract': 'MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB.', 'score': 30, 'issue_id': 4592, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': 'd7fecdae218ccf8e', 'authors': ['Haonan Chen', 'Hong Liu', 'Yuping Luo', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Microsoft Corporation', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23115.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#training', '#optimization', '#multimodal', '#alignment', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MoCa: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'MoCa - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'MoCa: Enhancing Multimodal Embedding with Bidirectional Attention', 'desc': 'MoCa is a two-stage framework designed to improve pre-trained causal vision-language models for better multimodal embedding. It addresses limitations in current models, such as the inefficiency of causal attention and the need for high-quality labeled data. The first stage focuses on modality-aware continual pre-training, which enhances understanding by denoising both text and image inputs. The second stage employs heterogeneous contrastive fine-tuning, using diverse multimodal data to improve model generalization and alignment, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'MoCaï¼šåŒå‘å¤šæ¨¡æ€åµŒå…¥çš„åˆ›æ–°æ¡†æ¶', 'desc': 'MoCaæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„å› æœè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€åµŒå…¥ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡å¼•å…¥åŒå‘æ³¨æ„åŠ›æœºåˆ¶ã€åˆ©ç”¨æœªæ ‡è®°æ•°æ®è¿›è¡Œæ‰©å±•ä»¥åŠå¤šæ ·åŒ–çš„è®­ç»ƒç›®æ ‡æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è”åˆé‡å»ºç›®æ ‡æ¥æé«˜æ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„å»å™ªèƒ½åŠ›ï¼Œå¢å¼ºåŒå‘ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¼‚æ„å¯¹æ¯”å¾®è°ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹é½æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00432', 'title': 'Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.00432', 'abstract': 'Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.', 'score': 29, 'issue_id': 4592, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': 'c4a7e4dd11865858', 'authors': ['Maggie Huan', 'Yuetai Li', 'Tuney Zheng', 'Xiaoyu Xu', 'Seungone Kim', 'Minxin Du', 'Radha Poovendran', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University', 'M-A-P', 'The Hong Kong Polytechnic University', 'University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.00432.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#transfer_learning', '#optimization', '#math'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL), Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT). ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ SFT Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº RL ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3-14B Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SFT.'}, 'en': {'title': 'Rethinking Training: Reinforcement Learning for Better Generalization', 'desc': 'This paper investigates the effectiveness of different training methods for reasoning models, particularly in the context of mathematical problem-solving. It finds that models trained with reinforcement learning (RL) outperform those fine-tuned with supervised learning (SFT) when applied to a variety of tasks beyond mathematics. The study reveals that while SFT models excel in math, they struggle to generalize their skills to other domains due to significant representation drift. In contrast, RL-tuned models maintain their general problem-solving abilities, suggesting a need to reconsider current training approaches for reasoning models.'}, 'zh': {'title': 'é‡æ–°æ€è€ƒæ¨ç†æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è°ƒä¼˜æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨å…¶ä»–é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶è®¸å¤šæ¨¡å‹åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„è¿ç§»èƒ½åŠ›å´è¾ƒå·®ã€‚é€šè¿‡å¯¹Qwen3-14Bæ¨¡å‹çš„å®éªŒï¼Œå‘ç°RLè°ƒä¼˜æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä¸­ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒSFTæ¨¡å‹åˆ™å®¹æ˜“é—å¿˜å…¶é€šç”¨èƒ½åŠ›ã€‚ç»“æœæç¤ºæˆ‘ä»¬éœ€è¦é‡æ–°å®¡è§†ç°æœ‰çš„è®­ç»ƒæ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹SFTæ•°æ®çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.19852', 'title': 'Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation', 'url': 'https://huggingface.co/papers/2506.19852', 'abstract': 'Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with O(n log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n^2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup over the original dense attention. With minimal tuning, it enables video generation up to 4times longer while reducing training costs by up to 4.4times compared to direct fine-tuning and accelerating inference by up to 3.7times compared to dense attention inference.', 'score': 24, 'issue_id': 4594, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ½Ñ', 'en': 'June 24', 'zh': '6æœˆ24æ—¥'}, 'hash': 'c195d9c32370fcf1', 'authors': ['Xingyang Li', 'Muyang Li', 'Tianle Cai', 'Haocheng Xi', 'Shuo Yang', 'Yujun Lin', 'Lvmin Zhang', 'Songlin Yang', 'Jinbo Hu', 'Kelly Peng', 'Maneesh Agrawala', 'Ion Stoica', 'Kurt Keutzer', 'Song Han'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.19852.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#architecture', '#inference', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ' Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ Ğ°Ğ´Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ O(n log n) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼."}, 'en': {'title': 'Radial Attention: Efficient Video Generation with Sparse Attention', 'desc': 'This paper introduces Radial Attention, a new sparse attention mechanism designed to enhance the efficiency of video generation in diffusion models. It leverages the concept of Spatiotemporal Energy Decay, which explains how attention scores decrease as the distance between tokens increases, similar to how signals weaken over distance. By using a static attention mask that focuses on nearby tokens and reduces the attention window with temporal distance, Radial Attention achieves a computational complexity of O(n log n), making it much faster than traditional O(n^2) dense attention. The results show that this method not only speeds up training and inference but also maintains high video quality, allowing for longer video generation with reduced costs.'}, 'zh': {'title': 'å¾„å‘æ³¨æ„åŠ›ï¼šé«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æœºåˆ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¾„å‘æ³¨æ„åŠ›ï¼ˆRadial Attentionï¼‰çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚æˆ‘ä»¬å‘ç°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨ä¸€ç§ç°è±¡ï¼Œç§°ä¸ºæ—¶ç©ºèƒ½é‡è¡°å‡ï¼Œéšç€ç©ºé—´å’Œæ—¶é—´è·ç¦»çš„å¢åŠ ï¼Œæ³¨æ„åŠ›å¾—åˆ†ä¼šå‡å°ã€‚å¾„å‘æ³¨æ„åŠ›é€šè¿‡å°†èƒ½é‡è¡°å‡è½¬åŒ–ä¸ºæŒ‡æ•°è¡°å‡çš„è®¡ç®—å¯†åº¦ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œå¤æ‚åº¦ä¸ºO(n log n)ï¼Œè¿œä¼˜äºä¼ ç»Ÿçš„O(n^2)å¯†é›†æ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾„å‘æ³¨æ„åŠ›åœ¨å¤šä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ä¿æŒäº†è§†é¢‘è´¨é‡ï¼Œå¹¶å®ç°äº†è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.20639', 'title': 'DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.20639', 'abstract': "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are compelling alternatives to autoregressive (AR) models because their denoising models operate over the entire sequence. The global planning and iterative refinement features of dLLMs are particularly useful for code generation. However, current training and inference mechanisms for dLLMs in coding are still under-explored. To demystify the decoding behavior of dLLMs and unlock their potential for coding, we systematically investigate their denoising processes and reinforcement learning (RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code. Using this model as a testbed, we analyze its decoding behavior, revealing how it differs from that of AR models: (1) dLLMs can decide how causal their generation should be without relying on semi-AR decoding, and (2) increasing the sampling temperature diversifies not only token choices but also their generation order. This diversity creates a rich search space for RL rollouts. For RL training, to reduce the variance of token log-likelihood estimates and maintain training efficiency, we propose coupled-GRPO, a novel sampling scheme that constructs complementary mask noise for completions used in training. In our experiments, coupled-GRPO significantly improves DiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and reduces reliance on AR causal during decoding. Our work provides deeper insight into the machinery of dLLM generation and offers an effective, diffusion-native RL training framework. https://github.com/apple/ml-diffucoder.", 'score': 15, 'issue_id': 4593, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 Ğ¸ÑĞ½Ñ', 'en': 'June 25', 'zh': '6æœˆ25æ—¥'}, 'hash': '20d886d0a4cd5bb6', 'authors': ['Shansan Gong', 'Ruixiang Zhang', 'Huangjie Zheng', 'Jiatao Gu', 'Navdeep Jaitly', 'Lingpeng Kong', 'Yizhe Zhang'], 'affiliations': ['Apple', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.20639.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#optimization', '#dataset', '#diffusion'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (dLLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² dLLM Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ coupled-GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Unlocking Code Generation with Diffusion Models', 'desc': 'This paper explores the use of diffusion large language models (dLLMs) for code generation, highlighting their unique denoising processes compared to traditional autoregressive models. The authors introduce a novel reinforcement learning sampling scheme called coupled-GRPO, which enhances the training efficiency and performance of the dLLM named DiffuCoder. By analyzing the decoding behavior of DiffuCoder, the study reveals that dLLMs can flexibly adjust their generation strategies and improve diversity in output. The findings demonstrate that dLLMs have significant potential for coding tasks, providing a new framework for effective code generation.'}, 'zh': {'title': 'æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼šä»£ç ç”Ÿæˆçš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„å»å™ªè¿‡ç¨‹ã€‚dLLMsä¸è‡ªå›å½’æ¨¡å‹ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿›è¡Œå»å™ªï¼Œå…·æœ‰å…¨çƒè§„åˆ’å’Œè¿­ä»£ä¼˜åŒ–çš„ç‰¹ç‚¹ï¼Œç‰¹åˆ«é€‚åˆä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ é‡‡æ ·æ–¹æ¡ˆcoupled-GRPOï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡å°‘æ ‡è®°å¯¹æ•°ä¼°è®¡çš„æ–¹å·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œcoupled-GRPOæ˜¾è‘—æå‡äº†DiffuCoderåœ¨ä»£ç ç”ŸæˆåŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‡å°‘äº†å¯¹è‡ªå›å½’å› æœè§£ç çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21277', 'title': 'HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context', 'url': 'https://huggingface.co/papers/2506.21277', 'abstract': 'A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models.', 'score': 10, 'issue_id': 4592, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': '15a38ef84e7820fa', 'authors': ['Qize Yang', 'Shimin Yao', 'Weixuan Chen', 'Shenghao Fu', 'Detao Bai', 'Jiaxing Zhao', 'Boyuan Sun', 'Bowen Yin', 'Xihan Wei', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.21277.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#games', '#rl', '#multimodal', '#survey'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğµ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IntentBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° IntentBench Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with Reinforcement Learning', 'desc': 'This paper presents a reinforcement learning approach to improve multimodal reasoning in large language models. It addresses two main challenges: insufficient understanding of global context and the shortcut problem, where models fail to consider important multimodal cues. By implementing context, format, accuracy, and logical rewards, the model enhances its reasoning capabilities and interprets multimodal inputs more effectively. The proposed method outperforms existing models on the IntentBench benchmark, showcasing its ability to understand complex human intentions and emotions.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ä¸Šä¸‹æ–‡ç†è§£å’Œæ·å¾„é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰å¤šæ¨¡æ€æ¨ç†æ¨¡å‹å­˜åœ¨å…¨çƒä¸Šä¸‹æ–‡ç†è§£ä¸è¶³å’Œæ·å¾„é—®é¢˜ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹é”™è¯¯è§£è¯»å¤šæ¨¡æ€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼ºè°ƒæ¨¡å‹éœ€è¦åœ¨å¤šæ¨¡æ€è¾“å…¥ä¸­æ¸…æ™°ç†è§£å…¨çƒä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å¥–åŠ±ã€æ ¼å¼å¥–åŠ±å’Œå‡†ç¡®æ€§å¥–åŠ±æ¥ç¡®ä¿å¯¹å¤šæ¨¡æ€ä¿¡æ¯çš„å‡†ç¡®è§£è¯»ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨IntentBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†åœ¨ç†è§£å¤æ‚äººç±»æ„å›¾å’Œæƒ…æ„Ÿæ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00951', 'title': 'Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact', 'url': 'https://huggingface.co/papers/2507.00951', 'abstract': 'The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.', 'score': 8, 'issue_id': 4593, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': '056b6a5007ee5fc2', 'authors': ['Rizwan Qureshi', 'Ranjan Sapkota', 'Abbas Shah', 'Amgad Muneer', 'Anas Zafar', 'Ashmal Vayani', 'Maged Shoman', 'Abdelrahman B. M. Eldaly', 'Kai Zhang', 'Ferhat Sadak', 'Shaina Raza', 'Xinqi Fan', 'Ravid Shwartz-Ziv', 'Hong Yan', 'Vinjia Jain', 'Aman Chadha', 'Manoj Karkee', 'Jia Wu', 'Philip Torr', 'Seyedali Mirjalili'], 'affiliations': ['Amazon Research (Work done outside Amazon)', 'Center for Data Science, New York University, NYU, NY, USA', 'Center for research in Computer Vision, University of Central Florida, Orlando, FL, USA', 'Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley, Brisbane, QLD 4006, Australia', 'Cornell University, Department of Biological and Environmental Engineering, Ithaca, NY 14853, USA', 'Department of Electrical Engineering, City University of Hong Kong, SAR China', 'Department of Electronics Engineering, Mehran University of Engineering & Technology, Jamshoro, Sindh, Pakistan', 'Department of Engineering Science, University of Oxford, UK', 'Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX, USA', 'Department of Mechanical Engineering, Bartin University, Bartin Turkey', 'Intelligent Transportation Systems, University of Tennessee, Oakridge, TN, USA', 'Manchester Metropolitan University, Manchester, UK', 'Meta Research (Work done outside Meta)', 'University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary', 'Vector Institute, Toronto Canada'], 'pdf_title_img': 'assets/pdf/title_img/2507.00951.jpg', 'data': {'categories': ['#agents', '#agi', '#rl', '#architecture', '#multimodal', '#rag', '#ethics', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑƒÑ‚ÑŒ Ğº AGI: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° (AGI). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ¾Ğ»ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ AGI.'}, 'en': {'title': 'Towards True Intelligence: Integrating Memory, Reasoning, and Adaptation for AGI', 'desc': 'This paper explores the interdisciplinary approach to achieving Artificial General Intelligence (AGI) by integrating concepts from various fields such as cognitive neuroscience and psychology. It emphasizes the importance of modular reasoning, persistent memory, and multi-agent coordination in developing intelligent systems. The authors propose that true intelligence is not just about scaling models but involves the orchestration of memory and reasoning capabilities. They also highlight the potential of neurosymbolic systems and reinforcement learning to create more adaptive and flexible AI agents.'}, 'zh': {'title': 'è·¨å­¦ç§‘æ¨åŠ¨äººå·¥é€šç”¨æ™ºèƒ½çš„å®ç°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å®ç°äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è·¨å­¦ç§‘æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ¨¡å—åŒ–æ¨ç†ã€æŒä¹…è®°å¿†å’Œå¤šæ™ºèƒ½ä½“åè°ƒçš„é‡è¦æ€§ã€‚è®ºæ–‡åˆ†æäº†é€šç”¨æ™ºèƒ½çš„æ¶æ„å’Œè®¤çŸ¥åŸºç¡€ï¼Œæå‡ºäº†ç»“åˆæ£€ç´¢ã€è§„åˆ’å’ŒåŠ¨æ€å·¥å…·ä½¿ç”¨çš„Agentic RAGæ¡†æ¶ï¼Œä»¥å®ç°æ›´çµæ´»çš„è¡Œä¸ºã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†ä¿¡æ¯å‹ç¼©ã€æµ‹è¯•æ—¶é€‚åº”å’Œæ— è®­ç»ƒæ–¹æ³•ç­‰æ³›åŒ–ç­–ç•¥ï¼Œä½œä¸ºå®ç°çµæ´»ã€é¢†åŸŸæ— å…³æ™ºèƒ½çš„å…³é”®è·¯å¾„ã€‚æœ€åï¼Œè®ºæ–‡æŒ‡å‡ºäº†åœ¨å®ç°AGIè¿‡ç¨‹ä¸­é¢ä¸´çš„ç§‘å­¦ã€æŠ€æœ¯å’Œä¼¦ç†æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00339', 'title': 'Training for X-Ray Vision: Amodal Segmentation, Amodal Content\n  Completion, and View-Invariant Object Representation from Multi-Camera Video', 'url': 'https://huggingface.co/papers/2507.00339', 'abstract': 'Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,', 'score': 8, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': 'a0d52ad7a093d23d', 'authors': ['Alexander Moore', 'Amar Saini', 'Kylie Cancilla', 'Doug Poland', 'Carmen Carrano'], 'affiliations': ['Lawrence Livermore National Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2507.00339.jpg', 'data': {'categories': ['#dataset', '#cv'], 'emoji': 'ğŸ“¹', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'MOVi-MC-AC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 5,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼ Ğ² ÑĞ²Ğ¾ĞµĞ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. MOVi-MC-AC Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Amodal Segmentation with Multi-Camera Insights', 'desc': 'This paper presents MOVi-MC-AC, a groundbreaking dataset for amodal segmentation and content completion in complex scenes. It introduces the concept of using multiple cameras to capture a scene, allowing for better tracking and identification of objects from different perspectives. The dataset includes approximately 5.8 million labeled object instances, providing a rich resource for training deep learning models in computer vision. By offering ground-truth amodal content, it addresses limitations in previous datasets and enhances the understanding of occluded objects in cluttered environments.'}, 'zh': {'title': 'å¤šæ‘„åƒå¤´ä¸‹çš„æ— æ¨¡æ€åˆ†å‰²æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†MOVi-MC-ACæ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„æ— æ¨¡æ€åˆ†å‰²å’Œæ— æ¨¡æ€å†…å®¹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¨¡æ‹Ÿäº†å¤šæ‘„åƒå¤´è§†é¢‘ä¸­çš„æ‚ä¹±åœºæ™¯ï¼Œæä¾›äº†å¯¹è±¡çš„é®æŒ¡æ©ç å’Œç‰¹å¾ã€‚é€šè¿‡åœ¨å¤šä¸ªç‹¬ç‰¹æ‘„åƒå¤´è§†è§’ä¹‹é—´è¯†åˆ«å’Œè·Ÿè¸ªå¯¹è±¡ï¼ŒMOVi-MC-ACä¸ºæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚æ•°æ®é›†ä¸­åŒ…å«çº¦580ä¸‡ä¸ªå¯¹è±¡å®ä¾‹ï¼Œå¹¶æä¾›äº†çœŸå®çš„æ— æ¨¡æ€å†…å®¹æ ‡ç­¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21545', 'title': 'Data Efficacy for Language Model Training', 'url': 'https://huggingface.co/papers/2506.21545', 'abstract': 'DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.  \t\t\t\t\tAI-generated summary \t\t\t\t Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.', 'score': 6, 'issue_id': 4596, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'b19a54a5dd4e35c8', 'authors': ['Yalun Dai', 'Yangyu Huang', 'Xin Zhang', 'Wenshan Wu', 'Chong Li', 'Wenhui Lu', 'Shijie Cao', 'Li Dong', 'Scarlett Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.21545.jpg', 'data': {'categories': ['#optimization', '#training', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DELT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LQS, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FO Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DELT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Maximizing Language Model Performance through Data Efficacy', 'desc': 'The paper introduces DELT, a new approach to improve language model performance by focusing on data efficacy, which is about how well training data is organized. It consists of three main components: Data Scoring, Data Selection, and Data Ordering, which work together to enhance model training without needing more data or larger models. A key innovation is the Learnability-Quality Scoring (LQS), which evaluates data samples based on their learnability and quality. The results show that using DELT can significantly boost performance, especially when combining LQS with Folding Ordering, while also achieving data efficiency.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ï¼šDELT', 'desc': 'DELTæ˜¯ä¸€ç§æé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚å®ƒåŒ…æ‹¬æ•°æ®è¯„åˆ†ã€é€‰æ‹©å’Œæ’åºä¸‰ä¸ªéƒ¨åˆ†ï¼Œæ—¨åœ¨ä¼˜åŒ–è®­ç»ƒæ•°æ®çš„ç»„ç»‡æ–¹å¼ã€‚é€šè¿‡è®¾è®¡å­¦ä¹ è´¨é‡è¯„åˆ†ï¼ˆLQSï¼‰å’ŒæŠ˜å æ’åºï¼ˆFOï¼‰ï¼ŒDELTèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ•°æ®è§„æ¨¡æˆ–æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•°æ®æœ‰æ•ˆæ€§ä¸æ•°æ®æ•ˆç‡å¯ä»¥ç»“åˆä½¿ç”¨ï¼Œä»è€Œä¸ºè¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00162', 'title': 'FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion', 'url': 'https://huggingface.co/papers/2507.00162', 'abstract': 'Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences.', 'score': 4, 'issue_id': 4600, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': '2e49553aab1de98a', 'authors': ['Yu Lu', 'Yi Yang'], 'affiliations': ['ReLER, CCAI, Zhejiang University, Hangzhou, 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00162.jpg', 'data': {'categories': ['#games', '#optimization', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'FreeLong: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FreeLong Ğ¸ FreeLong++, Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. FreeLong++ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²ĞµÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Long Video Generation with FreeLong Framework', 'desc': 'This paper introduces FreeLong, a novel framework aimed at improving the generation of longer videos from text prompts while maintaining high visual quality and temporal consistency. The authors identify a problem called high-frequency distortion, which occurs when short-video generation models are applied to longer sequences, leading to degraded video quality. FreeLong addresses this by blending low-frequency features that capture overall video semantics with high-frequency features that preserve fine details. The enhanced version, FreeLong++, expands this concept into a multi-branch architecture, allowing for better frequency fusion and improved performance in generating longer videos with coherent transitions and controllable elements.'}, 'zh': {'title': 'é•¿è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ€è¿‘è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çŸ­è§†é¢‘æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°æ›´é•¿çš„è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ—¶é—´ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦çš„ä¸‹é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°å°†çŸ­è§†é¢‘ç”Ÿæˆæ¨¡å‹åº”ç”¨äºè¾ƒé•¿åºåˆ—ä¼šå¯¼è‡´æ˜æ˜¾çš„è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FreeLongæ¡†æ¶ï¼Œé€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­å¹³è¡¡é•¿è§†é¢‘ç‰¹å¾çš„é¢‘ç‡åˆ†å¸ƒï¼Œç»“åˆå…¨å±€ä½é¢‘ç‰¹å¾å’Œå±€éƒ¨é«˜é¢‘ç‰¹å¾ï¼Œä»è€Œä¿æŒç»†èŠ‚å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23329', 'title': 'IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as\n  Agentic Inverse Rendering', 'url': 'https://huggingface.co/papers/2506.23329', 'abstract': 'Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This "understanding-by-creating" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating.', 'score': 4, 'issue_id': 4604, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': 'a1469fc6d15316f8', 'authors': ['Parker Liu', 'Chenxin Li', 'Zhengxin Li', 'Yipeng Wu', 'Wuyang Li', 'Zhiqin Yang', 'Zhenyuan Zhang', 'Yunlong Lin', 'Sirui Han', 'Brandon Y. Feng'], 'affiliations': ['CUHK', 'EPFL', 'HKUST', 'MIT', 'TJU', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2506.23329.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#cv', '#interpretability'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞµÑ‘ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IR3D-Bench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ, IR3D-Bench Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ' Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… ÑƒĞ¼ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½."}, 'en': {'title': 'Understanding Scenes by Creating, Not Just Recognizing', 'desc': "This paper introduces IR3D-Bench, a new benchmark designed to test the understanding capabilities of Vision-Language Models (VLMs) through active creation rather than just recognition. It employs the analysis-by-synthesis approach, where Vision-Language Agents (VLAs) use programming and rendering tools to reconstruct the 3D structure of images. This method emphasizes 'understanding-by-creating', which assesses the generative abilities of VLAs beyond traditional descriptive tasks. The study also presents metrics for evaluating various aspects of the generated scenes, revealing current limitations in visual precision among state-of-the-art VLMs."}, 'zh': {'title': 'é€šè¿‡åˆ›é€ ç†è§£åœºæ™¯çš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†IR3D-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£åœºæ™¯æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„è¢«åŠ¨è¯†åˆ«ä¸åŒï¼ŒIR3D-Benchè¦æ±‚è§†è§‰è¯­è¨€ä»£ç†ï¼ˆVLAsï¼‰é€šè¿‡ç¼–ç¨‹å’Œæ¸²æŸ“å·¥å…·ä¸»åŠ¨åˆ›å»ºè¾“å…¥å›¾åƒçš„3Dç»“æ„ã€‚è¯¥æ–¹æ³•åŸºäºåˆ†æ-åˆæˆèŒƒå¼ï¼Œå¼ºè°ƒé€šè¿‡åˆ›é€ æ¥ç†è§£ï¼Œè€Œä¸ä»…ä»…æ˜¯æè¿°ã€‚åˆæ­¥å®éªŒæ˜¾ç¤ºï¼Œå°½ç®¡å½“å‰çš„VLMåœ¨å·¥å…·ä½¿ç”¨ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§†è§‰ç²¾åº¦æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23009', 'title': 'MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.23009', 'abstract': 'Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance.', 'score': 4, 'issue_id': 4610, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': 'd5d1bf85d7b72633', 'authors': ['Jian Chen', 'Wenye Ma', 'Penghang Liu', 'Wei Wang', 'Tengwei Song', 'Ming Li', 'Chenguang Wang', 'Ruiyi Zhang', 'Changyou Chen'], 'affiliations': ['Duke University', 'King Abdullah University of Science and Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23009.jpg', 'data': {'categories': ['#training', '#benchmark', '#synthetic', '#games', '#dataset', '#multimodal'], 'emoji': 'ğŸ¼', 'ru': {'title': 'MusiXQA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MusiXQA - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²ÑĞµĞ¾Ğ±ÑŠĞµĞ¼Ğ»ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»Ğ¸ÑÑ‚Ğ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ»Ğ¸ÑÑ‚Ñ‹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Phi-3-MusiX, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GPT.'}, 'en': {'title': 'Unlocking Music Sheet Understanding for MLLMs', 'desc': 'This paper introduces MusiXQA, a new dataset designed to improve the understanding of music sheets by Multimodal Large Language Models (MLLMs). The dataset includes high-quality synthetic music sheets with detailed annotations, allowing MLLMs to perform various visual question-answering tasks related to music notation. The authors demonstrate that current MLLMs struggle with music sheet interpretation, highlighting the need for specialized training. They also present Phi-3-MusiX, an MLLM fine-tuned on MusiXQA, which shows improved performance compared to existing models like GPT.'}, 'zh': {'title': 'ä¹è°±ç†è§£çš„æ–°çªç ´ï¼šMusiXQAä¸Phi-3-MusiX', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶å›¾åƒã€æ–‡æœ¬ä¸°å¯Œçš„æ–‡æ¡£å’Œå›¾å½¢è®¾è®¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¹è°±ç†è§£æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MusiXQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°å’Œæ¨åŠ¨MLLMsåœ¨ä¹è°±ç†è§£æ–¹é¢è¿›å±•çš„æ•°æ®é›†ã€‚MusiXQAåŒ…å«é«˜è´¨é‡çš„åˆæˆä¹è°±ï¼Œé…æœ‰ç»“æ„åŒ–æ³¨é‡Šï¼Œæ¶µç›–éŸ³ç¬¦éŸ³é«˜å’Œæ—¶å€¼ã€å’Œå¼¦ã€è°±å·ã€è°ƒå·/æ‹å·å’Œæ–‡æœ¬ï¼Œæ”¯æŒå¤šæ ·çš„è§†è§‰é—®ç­”ä»»åŠ¡ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„MLLMsåœ¨è¿™ä¸€é¢†åŸŸçš„æ˜¾è‘—å±€é™æ€§ï¼Œå¹¶å¼€å‘äº†Phi-3-MusiXï¼Œä¸€ä¸ªåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒçš„MLLMï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22960', 'title': 'Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images', 'url': 'https://huggingface.co/papers/2506.22960', 'abstract': 'PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.  \t\t\t\t\tAI-generated summary \t\t\t\t A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that "Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality." In response, California\'s Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.', 'score': 4, 'issue_id': 4593, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': 'c946e3ac9bf6133a', 'authors': ['Shreyas Dixit', 'Ashhar Aziz', 'Shashwat Bajpai', 'Vasu Sharma', 'Aman Chadha', 'Vinija Jain', 'Amitava Das'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'BITS Pilani Hyderabad, India', 'IIIT Delhi, India', 'Meta AI, USA', 'Stanford University, USA', 'VIIT Pune, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.22960.jpg', 'data': {'categories': ['#security', '#synthetic', '#data', '#open_source', '#multimodal', '#cv'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞµĞ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ğ¼Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'PECCAVI - ÑÑ‚Ğ¾ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ (NMP) Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. PECCAVI ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² NMP Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑˆÑƒĞ¼Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ğ¶Ğ¸Ğ½Ğ¸Ñ€Ğ¸Ğ½Ğ³Ñƒ. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ĞµÑ‰Ğ°ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'PECCAVI: Watermarking Resilience Against Visual Paraphrase Attacks', 'desc': 'PECCAVI is a novel image watermarking technique designed to withstand visual paraphrase attacks and various distortions. It utilizes Non-Melting Points (NMPs) to strategically embed watermarks, ensuring that the essential features of the image remain intact. The method employs multi-channel frequency domain watermarking and incorporates noisy burnishing to protect against reverse-engineering attempts. This approach is model-agnostic, making it applicable across different systems, and all resources will be made available to the public.'}, 'zh': {'title': 'PECCAVIï¼šæŠµå¾¡è§†è§‰æ”¹å†™çš„æ°´å°æ–°æŠ€æœ¯', 'desc': 'PECCAVIæ˜¯ä¸€ç§å¼ºå¤§çš„å›¾åƒæ°´å°æŠ€æœ¯ï¼Œèƒ½å¤ŸæŠµå¾¡è§†è§‰æ”¹å†™æ”»å‡»å’Œå¤±çœŸã€‚å®ƒåˆ©ç”¨éç†”åŒ–ç‚¹ï¼ˆNMPsï¼‰å’Œå¤šé€šé“é¢‘åŸŸæ°´å°æŠ€æœ¯ï¼Œå°†æ°´å°åµŒå…¥å›¾åƒçš„æ ¸å¿ƒè¯­ä¹‰åŒºåŸŸã€‚è¯¥æŠ€æœ¯è¿˜é‡‡ç”¨äº†å™ªå£°çƒ§ç¼æ–¹æ³•ï¼Œä»¥é˜²æ­¢é€†å‘å·¥ç¨‹æ”»å‡»ï¼Œå¢å¼ºæ°´å°çš„è€ä¹…æ€§ã€‚PECCAVIä¸ä¾èµ–äºç‰¹å®šæ¨¡å‹ï¼Œæ‰€æœ‰ç›¸å…³èµ„æºå’Œä»£ç å°†å¼€æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00606', 'title': 'Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies', 'url': 'https://huggingface.co/papers/2507.00606', 'abstract': 'Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.', 'score': 2, 'issue_id': 4607, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': 'a9828e151d8e7eb8', 'authors': ['Tao Xiong', 'Xavier Hu', 'Wenyan Fan', 'Shengyu Zhang'], 'affiliations': ['Dalian University of Technology, Dalian, LiaoNing, China', 'Independent, Hangzhou, ZheJiang, China', 'Zhejiang University, Hangzhou, ZheJiang, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.00606.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mixture of Reasoning (MoR). MoR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Autonomous Reasoning through Mixture of Reasoning', 'desc': "This paper presents a new framework called Mixture of Reasoning (MoR) that improves the performance of large language models (LLMs) by integrating various reasoning strategies. Unlike traditional methods that depend on specific prompts for each task, MoR allows LLMs to adapt their reasoning autonomously. The framework consists of two main phases: Thought Generation, which creates templates for reasoning chains, and SFT Dataset Construction, which pairs these templates with datasets for fine-tuning. Experimental results demonstrate that MoR enhances the model's performance significantly, making it a versatile solution for a wide range of tasks without the need for manual prompt design."}, 'zh': {'title': 'æ¨ç†æ··åˆï¼šæ— æç¤ºè‡ªé€‚åº”æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¾—ç›Šäºå…ˆè¿›çš„æç¤ºæŠ€æœ¯ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰ï¼Œä½†å®ƒä»¬å¯¹æ‰‹åŠ¨è®¾è®¡çš„ç‰¹å®šä»»åŠ¡æç¤ºçš„ä¾èµ–é™åˆ¶äº†é€‚åº”æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬æå‡ºäº†æ¨ç†æ··åˆï¼ˆMoRï¼‰æ¡†æ¶ï¼Œå°†å¤šæ ·çš„æ¨ç†ç­–ç•¥åµŒå…¥LLMsä¸­ï¼Œå®ç°è‡ªä¸»ã€ä»»åŠ¡è‡ªé€‚åº”çš„æ¨ç†ï¼Œè€Œæ— éœ€å¤–éƒ¨æç¤ºå·¥ç¨‹ã€‚MoRåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šæ€ç»´ç”Ÿæˆï¼Œä½¿ç”¨åƒGPT-4oè¿™æ ·çš„æ¨¡å‹åˆ›å»ºæ¨ç†é“¾æ¨¡æ¿ï¼Œä»¥åŠSFTæ•°æ®é›†æ„å»ºï¼Œå°†æ¨¡æ¿ä¸åŸºå‡†æ•°æ®é›†é…å¯¹è¿›è¡Œç›‘ç£å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒMoRæ˜¾è‘—æå‡äº†æ€§èƒ½ï¼ŒMoR150åœ¨ä½¿ç”¨CoTæç¤ºæ—¶è¾¾åˆ°äº†0.730ï¼ˆæé«˜2.2%ï¼‰ï¼Œä¸åŸºçº¿ç›¸æ¯”æé«˜äº†13.5%ï¼Œæä¾›äº†ä¸€ç§å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å®ç°è·¨å¤šæ ·ä»»åŠ¡çš„ç¨³å¥æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.00476', 'title': 'FreNBRDF: A Frequency-Rectified Neural Material Representation', 'url': 'https://huggingface.co/papers/2507.00476', 'abstract': 'Accurate material modeling is crucial for achieving photorealistic rendering, bridging the gap between computer-generated imagery and real-world photographs. While traditional approaches rely on tabulated BRDF data, recent work has shifted towards implicit neural representations, which offer compact and flexible frameworks for a range of tasks. However, their behavior in the frequency domain remains poorly understood. To address this, we introduce FreNBRDF, a frequency-rectified neural material representation. By leveraging spherical harmonics, we integrate frequency-domain considerations into neural BRDF modeling. We propose a novel frequency-rectified loss, derived from a frequency analysis of neural materials, and incorporate it into a generalizable and adaptive reconstruction and editing pipeline. This framework enhances fidelity, adaptability, and efficiency. Extensive experiments demonstrate that \\ours improves the accuracy and robustness of material appearance reconstruction and editing compared to state-of-the-art baselines, enabling more structured and interpretable downstream tasks and applications.', 'score': 1, 'issue_id': 4608, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 Ğ¸ÑĞ»Ñ', 'en': 'July 1', 'zh': '7æœˆ1æ—¥'}, 'hash': 'c539d168bcfbab61', 'authors': ['Chenliang Zhou', 'Zheyuan Hu', 'Cengiz Oztireli'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.00476.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ BRDF Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FreNBRDF - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ BRDF. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ loss Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FreNBRDF Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Material Modeling with Frequency-Rectified Neural Representations', 'desc': 'This paper presents FreNBRDF, a new method for modeling materials in computer graphics using neural networks. It focuses on improving the accuracy of material representation by incorporating frequency-domain analysis through spherical harmonics. The authors introduce a frequency-rectified loss function that enhances the training of neural BRDF models, making them more adaptable and efficient. Experimental results show that this approach significantly outperforms existing methods in reconstructing and editing material appearances, leading to better photorealistic rendering.'}, 'zh': {'title': 'é¢‘ç‡æ ¡æ­£ï¼Œæå‡ææ–™å»ºæ¨¡çš„å‡†ç¡®æ€§', 'desc': 'å‡†ç¡®çš„ææ–™å»ºæ¨¡å¯¹äºå®ç°é€¼çœŸçš„æ¸²æŸ“è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿç¼©å°è®¡ç®—æœºç”Ÿæˆå›¾åƒä¸çœŸå®ç…§ç‰‡ä¹‹é—´çš„å·®è·ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºè¡¨æ ¼åŒ–çš„BRDFæ•°æ®ï¼Œè€Œæœ€è¿‘çš„ç ”ç©¶åˆ™è½¬å‘éšå¼ç¥ç»è¡¨ç¤ºï¼Œæä¾›äº†ç´§å‡‘ä¸”çµæ´»çš„æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†FreNBRDFï¼Œè¿™æ˜¯ä¸€ç§é¢‘ç‡æ ¡æ­£çš„ç¥ç»ææ–™è¡¨ç¤ºï¼Œé€šè¿‡åˆ©ç”¨çƒè°å‡½æ•°å°†é¢‘åŸŸè€ƒè™‘æ•´åˆåˆ°ç¥ç»BRDFå»ºæ¨¡ä¸­ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ææ–™å¤–è§‚é‡å»ºå’Œç¼–è¾‘æ–¹é¢æé«˜äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œæ”¯æŒæ›´ç»“æ„åŒ–å’Œå¯è§£é‡Šçš„ä¸‹æ¸¸ä»»åŠ¡å’Œåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.24019', 'title': 'Ella: Embodied Social Agents with Lifelong Memory', 'url': 'https://huggingface.co/papers/2506.24019', 'abstract': "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella's capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.", 'score': 1, 'issue_id': 4612, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': 'e3bbc5a001cf1269', 'authors': ['Hongxin Zhang', 'Zheyuan Zhang', 'Zeyuan Wang', 'Zunzhe Zhang', 'Lixing Fang', 'Qinhong Zhou', 'Chuang Gan'], 'affiliations': ['Johns Hopkins University', 'Tsinghua University', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.24019.jpg', 'data': {'categories': ['#3d', '#agents', '#multimodal', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ğ»Ğ»Ğ°: Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ 3D-Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ­Ğ»Ğ»Ñƒ - Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¸Ñ€Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ­Ğ»Ğ»Ñ‹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ­Ğ»Ğ»Ğµ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ­Ğ»Ğ»Ñ‹ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ 3D-Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Ella: Lifelong Learning in a Social 3D World', 'desc': "The paper presents Ella, an embodied social agent designed for lifelong learning in a 3D open world. Ella utilizes a structured multimodal memory system that includes semantic memory for knowledge organization and episodic memory for capturing experiences. This system allows Ella to make informed decisions, plan activities, and build social relationships through interactions with other agents. The results demonstrate Ella's effectiveness in influencing and cooperating with peers, emphasizing the benefits of integrating structured memory with foundation models for enhanced embodied intelligence."}, 'zh': {'title': 'Ellaï¼šç»ˆèº«å­¦ä¹ çš„å…·èº«ç¤¾äº¤ä»£ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Ellaï¼Œä¸€ä¸ªèƒ½å¤Ÿåœ¨3Då¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œç»ˆèº«å­¦ä¹ çš„å…·èº«ç¤¾äº¤ä»£ç†ã€‚Ellaçš„æ ¸å¿ƒèƒ½åŠ›æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„é•¿æœŸå¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å­˜å‚¨ã€æ›´æ–°å’Œæ£€ç´¢ä¿¡æ¯ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä»¥åç§°ä¸ºä¸­å¿ƒçš„è¯­ä¹‰è®°å¿†å’Œæ•æ‰å¤šæ¨¡æ€ç»éªŒçš„æ—¶ç©ºæƒ…èŠ‚è®°å¿†ã€‚é€šè¿‡å°†è¿™ä¸€ç»ˆèº«è®°å¿†ç³»ç»Ÿä¸åŸºç¡€æ¨¡å‹ç»“åˆï¼ŒEllaèƒ½å¤Ÿåœ¨ä¸å…¶ä»–æ™ºèƒ½ç”Ÿç‰©å…±å­˜çš„ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ã€è§„åˆ’æ—¥å¸¸æ´»åŠ¨å’Œå»ºç«‹ç¤¾äº¤å…³ç³»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22973', 'title': 'Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions', 'url': 'https://huggingface.co/papers/2506.22973', 'abstract': "A novel lossy compression method using learnable confidence scores improves storage and computational efficiency in 3D Gaussian Splatting without sacrificing visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting", 'score': 0, 'issue_id': 4606, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': '20ec91d26e253f96', 'authors': ['AmirHossein Naghi Razlighi', 'Elaheh Badali Golezani', 'Shohreh Kasaei'], 'affiliations': ['Sharif University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.22973.jpg', 'data': {'categories': ['#3d', '#inference', '#open_source', '#optimization'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ Ğ´Ğ»Ñ 3D Gaussian Splatting, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±ĞµÑ‚Ğ°-Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ğ»Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ»ÑĞ±Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Gaussian Splatting Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ†ĞµĞ½Ñ‹.'}, 'en': {'title': 'Optimizing 3D Rendering with Learnable Confidence Scores', 'desc': 'This paper introduces a new method for compressing 3D Gaussian Splatting data using learnable confidence scores, which enhances both storage and computational efficiency. The method employs Beta distributions to model the confidence of each splat, allowing for the removal of less important splats without losing visual quality. By optimizing these confidence scores through reconstruction-aware losses, the approach effectively balances compression and fidelity. The technique is versatile and can be integrated with various Gaussian Splatting architectures, providing a new metric for scene quality assessment.'}, 'zh': {'title': 'æå‡3Dé«˜æ–¯ç‚¹äº‘å‹ç¼©æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æœ‰æŸå‹ç¼©æ–¹æ³•ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„ç½®ä¿¡åº¦è¯„åˆ†æ¥æé«˜3Dé«˜æ–¯ç‚¹äº‘çš„å­˜å‚¨å’Œè®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è§†è§‰è´¨é‡ã€‚3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“é€šå¸¸ä¼šäº§ç”Ÿå¤§é‡çš„ç‚¹ï¼Œå¯¼è‡´å­˜å‚¨å’Œè®¡ç®—å¼€é”€è¿‡å¤§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡å»ºæ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ¯ä¸ªç‚¹çš„ç½®ä¿¡åº¦ï¼Œä»è€Œåœ¨ä¿ç•™è§†è§‰ä¿çœŸåº¦çš„åŒæ—¶ï¼Œå»é™¤ä½ç½®ä¿¡åº¦çš„ç‚¹ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šæ¶æ„ï¼Œé€‚ç”¨äºä»»ä½•é«˜æ–¯ç‚¹äº‘å˜ä½“ï¼Œå¹¶ä¸”é€šè¿‡å¹³å‡ç½®ä¿¡åº¦å€¼æä¾›äº†ä¸€ç§æ–°çš„åœºæ™¯è´¨é‡è¯„ä¼°æŒ‡æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23044', 'title': 'Ovis-U1 Technical Report', 'url': 'https://huggingface.co/papers/2506.23044', 'abstract': 'Ovis-U1, a 3-billion-parameter model, combines multimodal understanding, text-to-image generation, and image editing, achieving state-of-the-art performance in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.', 'score': 48, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': 'caa82e446dde84a7', 'authors': ['Guo-Hua Wang', 'Shanshan Zhao', 'Xinjie Zhang', 'Liangfu Cao', 'Pengxin Zhan', 'Lunhao Duan', 'Shiyin Lu', 'Minghao Fu', 'Xiaohao Chen', 'Jianshan Zhao', 'Yang Li', 'Qing-Guo Chen'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.23044.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¦¾', 'ru': {'title': 'Ovis-U1: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ovis-U1 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ovis-U1 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Ovis-U1: Unifying Text and Image Mastery', 'desc': 'Ovis-U1 is a powerful machine learning model with 3 billion parameters that excels in understanding and generating images from text, as well as editing images. It uses a unique training method that combines language understanding and image generation, leading to improved performance over models that focus on just one of these tasks. The model features a diffusion-based visual decoder and a bidirectional token refiner, which enhance its capabilities in generating high-quality images. Ovis-U1 has achieved impressive scores on various benchmarks, outperforming other state-of-the-art models in multimodal tasks.'}, 'zh': {'title': 'Ovis-U1ï¼šå¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹çš„çªç ´', 'desc': 'Ovis-U1æ˜¯ä¸€ä¸ªæ‹¥æœ‰30äº¿å‚æ•°çš„ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆäº†å¤šæ¨¡æ€ç†è§£ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨åŸºäºæ‰©æ•£çš„è§†è§‰è§£ç å™¨å’ŒåŒå‘ä»¤ç‰Œç²¾ç‚¼å™¨ï¼Œä½¿å¾—å›¾åƒç”Ÿæˆä»»åŠ¡çš„è¡¨ç°ä¸é¢†å…ˆæ¨¡å‹å¦‚GPT-4oç›¸å½“ã€‚ä¸ä¸€äº›ä½¿ç”¨å›ºå®šå¤šè¯­è¨€æ¨¡å‹è¿›è¡Œç”Ÿæˆä»»åŠ¡çš„æ¨¡å‹ä¸åŒï¼ŒOvis-U1é‡‡ç”¨äº†ä¸€ç§æ–°çš„ç»Ÿä¸€è®­ç»ƒæ–¹æ³•ï¼Œä»è¯­è¨€æ¨¡å‹å¼€å§‹è®­ç»ƒã€‚é€šè¿‡å°†ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç»“åˆï¼ŒOvis-U1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.24119', 'title': 'SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.24119', 'abstract': 'Self-play in zero-sum games using SPIRAL enhances reasoning capabilities in language models through self-improvement and transfer learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.', 'score': 26, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': 'fbb4b5b047892d14', 'authors': ['Bo Liu', 'Leon Guertler', 'Simon Yu', 'Zichen Liu', 'Penghui Qi', 'Daniel Balcells', 'Mickel Liu', 'Cheston Tan', 'Weiyan Shi', 'Min Lin', 'Wee Sun Lee', 'Natasha Jaques'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR', 'National University of Singapore', 'Northeastern University', 'Plastic Labs', 'Sea AI Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.24119.jpg', 'data': {'categories': ['#rl', '#reasoning', '#games', '#transfer_learning', '#rlhf', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SPIRAL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñƒ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ ÑÑƒĞ¼Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²ĞµÑ€ÑĞ¸Ğ¹ ÑĞ°Ğ¼Ğ¸Ñ… ÑĞµĞ±Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ĞºÑƒÑ€Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ³Ñ€Ğµ ĞšÑƒĞ½-Ğ¿Ğ¾ĞºĞµÑ€ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B-Base Ğ½Ğ° 8.6% Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ 8.4% Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°: ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ, Ñ€Ğ°ÑÑ‡ĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ÑĞ¼.'}, 'en': {'title': 'Empowering AI Reasoning through Self-Play in Zero-Sum Games', 'desc': 'The paper presents SPIRAL, a self-play framework that enhances reasoning abilities in language models by allowing them to compete against improved versions of themselves in zero-sum games. This approach eliminates the need for human-generated problem-answer pairs and domain-specific rewards, enabling models to learn through an infinite curriculum of challenges. By implementing a multi-agent reinforcement learning system, SPIRAL stabilizes training and facilitates the development of reasoning skills that can be transferred across different tasks. The results show significant improvements in reasoning performance, demonstrating the effectiveness of self-play in fostering cognitive skills in AI models.'}, 'zh': {'title': 'è‡ªæˆ‘å¯¹å¼ˆï¼šæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPIRALçš„è‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›¶å’Œæ¸¸æˆæå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡å‹ä¸è‡ªèº«ä¸æ–­æ”¹è¿›çš„ç‰ˆæœ¬è¿›è¡Œå¤šè½®å¯¹å¼ˆï¼Œæ¶ˆé™¤äº†å¯¹äººå·¥ç›‘ç£çš„éœ€æ±‚ã€‚SPIRALèƒ½å¤Ÿç”Ÿæˆæ— é™çš„é€æ­¥æŒ‘æˆ˜é—®é¢˜ï¼Œä½¿æ¨¡å‹å¿…é¡»é€‚åº”æ›´å¼ºçš„å¯¹æ‰‹ï¼Œä»è€Œå®ç°è‡ªæˆ‘æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SPIRALè¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨æ•°å­¦å’Œä¸€èˆ¬æ¨ç†æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†é›¶å’Œæ¸¸æˆåœ¨è‡ªä¸»æ¨ç†å‘å±•ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23858', 'title': 'VMoBA: Mixture-of-Block Attention for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2506.23858', 'abstract': 'VMoBA, a novel sparse attention mechanism for Video Diffusion Models (VDMs), accelerates training and inference by addressing the quadratic complexity of full attention mechanisms while maintaining or improving video generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.', 'score': 25, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': '684efafe36e7bbc8', 'authors': ['Jianzong Wu', 'Liang Hou', 'Haotian Yang', 'Xin Tao', 'Ye Tian', 'Pengfei Wan', 'Di Zhang', 'Yunhai Tong'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23858.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#diffusion', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VMoBA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VMoBA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. VMoBA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ 1D-2D-3D, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention', 'desc': 'VMoBA is a new sparse attention mechanism designed to improve Video Diffusion Models (VDMs) by reducing the computational complexity associated with full attention mechanisms. It addresses the challenges of generating long-duration, high-resolution videos while maintaining quality. The method incorporates a layer-wise recurrent block partition scheme, global block selection, and threshold-based block selection to optimize attention patterns specific to video data. Experimental results show that VMoBA significantly speeds up training and inference times while achieving comparable or superior video generation quality.'}, 'zh': {'title': 'VMoBAï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ–°ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶', 'desc': 'VMoBAæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³å…¨æ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒè§†é¢‘å˜æ¢å™¨çš„æ³¨æ„åŠ›æ¨¡å¼è¿›è¡Œæ·±å…¥åˆ†æï¼ŒVMoBAèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘æ•°æ®çš„æ—¶ç©ºç‰¹æ€§ã€‚è¯¥æœºåˆ¶é€šè¿‡ä¸‰é¡¹å…³é”®æ”¹è¿›æå‡äº†åŸæœ‰çš„MoBAæ¡†æ¶ï¼ŒåŒ…æ‹¬åŠ¨æ€é€‚åº”æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å¼çš„åˆ†å±‚é€’å½’å—åˆ’åˆ†æ–¹æ¡ˆã€ä¼˜å…ˆé€‰æ‹©æœ€æ˜¾è‘—çš„æŸ¥è¯¢-é”®å—äº¤äº’çš„å…¨å±€å—é€‰æ‹©ï¼Œä»¥åŠåŸºäºé˜ˆå€¼çš„å—é€‰æ‹©æ¥åŠ¨æ€ç¡®å®šå…³æ³¨å—çš„æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVMoBAåœ¨é•¿åºåˆ—è®­ç»ƒä¸­æ˜¾è‘—åŠ é€ŸVDMsçš„è®­ç»ƒï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¸å…¨æ³¨æ„åŠ›æœºåˆ¶ç›¸å½“æˆ–æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.24123', 'title': 'Calligrapher: Freestyle Text Image Customization', 'url': 'https://huggingface.co/papers/2506.24123', 'abstract': "Calligrapher uses a diffusion-based framework with self-distillation and localized style injection to generate high-quality, stylistically consistent digital typography.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.", 'score': 24, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': 'ee3cd26fec757450', 'authors': ['Yue Ma', 'Qingyan Bai', 'Hao Ouyang', 'Ka Leong Cheng', 'Qiuyu Wang', 'Hongyu Liu', 'Zichen Liu', 'Haofan Wang', 'Jingye Chen', 'Yujun Shen', 'Qifeng Chen'], 'affiliations': ['Ant Group, China', 'Hong Kong University of Science and Technology, China', 'InstantX, Independent Research Team'], 'pdf_title_img': 'assets/pdf/title_img/2506.24123.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#diffusion', '#dataset'], 'emoji': 'âœ’ï¸', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ»Ğ»Ğ¸Ğ³Ñ€Ğ°Ñ„: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Calligrapher - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Calligrapher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¸Ñ„Ğ¾Ğ².'}, 'en': {'title': 'Empowering Digital Typography with Style Consistency', 'desc': "Calligrapher is a new framework that uses diffusion processes to create high-quality digital typography with a focus on style consistency. It addresses challenges in customizing typography by employing a self-distillation method that utilizes a pre-trained text-to-image model and a large language model to create a style-focused benchmark. Additionally, it features a localized style injection system that extracts style features from reference images using a trainable style encoder. The framework's in-context generation mechanism enhances the alignment of styles, allowing for precise and intricate typography suitable for various design applications."}, 'zh': {'title': 'Calligrapherï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ•°å­—æ’ç‰ˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'Calligrapher æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªè’¸é¦å’Œå±€éƒ¨é£æ ¼æ³¨å…¥æŠ€æœ¯ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸”é£æ ¼ä¸€è‡´çš„æ•°å­—æ’ç‰ˆã€‚è¯¥æ¡†æ¶è§£å†³äº†æ’ç‰ˆå®šåˆ¶ä¸­é£æ ¼æ§åˆ¶å’Œæ•°æ®ä¾èµ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ã€‚é¦–å…ˆï¼Œå¼€å‘äº†ä¸€ç§è‡ªè’¸é¦æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè‡ªåŠ¨æ„å»ºä»¥é£æ ¼ä¸ºä¸­å¿ƒçš„æ’ç‰ˆåŸºå‡†ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¯è®­ç»ƒçš„é£æ ¼ç¼–ç å™¨å¼•å…¥å±€éƒ¨é£æ ¼æ³¨å…¥æ¡†æ¶ï¼Œæå–å‚è€ƒå›¾åƒä¸­çš„å¼ºå¥é£æ ¼ç‰¹å¾ï¼Œä»è€Œæå‡ç›®æ ‡é£æ ¼çš„å¯¹é½ç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22832', 'title': 'Listener-Rewarded Thinking in VLMs for Image Preferences', 'url': 'https://huggingface.co/papers/2506.22832', 'abstract': 'A listener-augmented Group Relative Policy Optimization framework improves reward models by re-evaluating reasoning processes, leading to enhanced accuracy and out-of-distribution performance in aligning vision-language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a model\'s reasoning trace contradicts that of an independent, frozen vision-language model ("listener") evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasoner\'s chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.', 'score': 19, 'issue_id': 4575, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': '70be97df80ce7e08', 'authors': ['Alexander Gambashidze', 'Li Pengyi', 'Matvey Skripkin', 'Andrey Galichin', 'Anton Gusarov', 'Konstantin Sobolev', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['Artificial Intelligence Research Institute, Moscow, Russia', 'Skolkovo Institute of Science and Technology, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.22832.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#alignment', '#rl', '#reasoning', '#rlhf'], 'emoji': 'ğŸ‘‚', 'ru': {'title': 'Ğ¡Ğ»ÑƒÑˆĞ°Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸ÑÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ listener-augmented Group Relative Policy Optimization (GRPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-'ÑĞ»ÑƒÑˆĞ°Ñ‚ĞµĞ»Ñ' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageReward Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½ÑĞ°Ğ½ÑĞ°Ğ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Enhancing Model Reasoning with Listener-Augmented Rewards', 'desc': "This paper presents a new framework called listener-augmented Group Relative Policy Optimization (GRPO) to improve reward models for aligning vision-language models with human preferences. The framework addresses the issue of reasoning accuracy by incorporating a 'listener' model that re-evaluates the reasoning process of the main model, providing calibrated confidence scores. This approach not only enhances the accuracy of the model's outputs but also reduces contradictions in reasoning when compared to traditional methods. The results show significant improvements in both accuracy and out-of-distribution performance, demonstrating the effectiveness of listener-based rewards in training robust models."}, 'zh': {'title': 'å¢å¼ºæ¨ç†ï¼Œæå‡äººç±»åå¥½å¯¹é½çš„å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡é‡æ–°è¯„ä¼°æ¨ç†è¿‡ç¨‹æ¥æ”¹å–„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œæé«˜è§†è§‰-è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ç²¾åº¦å’Œåœ¨åˆ†å¸ƒå¤–çš„è¡¨ç°ã€‚å½“å‰çš„å¥–åŠ±æ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šå­˜åœ¨ä¸è¶³ï¼Œä¸”ç›‘ç£å¾®è°ƒå®¹æ˜“å¯¼è‡´è®°å¿†åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªâ€œå¬ä¼—â€æ¨¡å‹ï¼Œå®ƒç‹¬ç«‹äºæ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯¹æ¨ç†è¿‡ç¨‹è¿›è¡Œé‡æ–°è¯„ä¼°ï¼Œå¹¶æä¾›æ›´ä¸ºå‡†ç¡®çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„å¥–åŠ±æœºåˆ¶ä¸ä»…é¼“åŠ±æ¨ç†æ¨¡å‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜ä¿ƒä½¿å…¶ç”Ÿæˆå¯¹ç‹¬ç«‹æ¨¡å‹å…·æœ‰è¯´æœåŠ›çš„è§£é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17930', 'title': 'Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective', 'url': 'https://huggingface.co/papers/2506.17930', 'abstract': 'A novel prompt design paradigm, PromptQuine, shows that pruning random demonstrations into "gibberish" can improve large language model performance across various tasks, surpassing state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel prompt design paradigm that challenges conventional wisdom in large language model (LLM) prompting. While conventional wisdom prioritizes well-crafted instructions and demonstrations for in-context learning (ICL), we show that pruning random demonstrations into seemingly incoherent "gibberish" can remarkably improve performance across diverse tasks. Notably, the "gibberish" always matches or surpasses state-of-the-art automatic prompt optimization techniques, achieving substantial gains regardless of LLM alignment. Nevertheless, discovering an effective pruning strategy is non-trivial, as existing attribution methods and prompt compression algorithms fail to deliver robust results, let alone human intuition. In terms of this, we propose a self-discover prompt optimization framework, PromptQuine, an evolutionary search framework that automatically searches for the pruning strategy by itself using only low-data regimes. Much like the emergent complexity in nature--such as symbiosis and self-organization--arising in response to resource constraints, our framework evolves and refines unconventional yet highly effective prompts by leveraging only the tokens present within the context. We demonstrate its effectiveness across classification, multi-choice question answering, generation and math reasoning tasks across LLMs, while achieving decent runtime efficiency. We hope our findings can guide mechanistic studies on in-context learning, and provide a call to action, to pave the way for more open-ended search algorithms for more effective LLM prompting.', 'score': 16, 'issue_id': 4570, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ½Ñ', 'en': 'June 22', 'zh': '6æœˆ22æ—¥'}, 'hash': '346a389b3fbfb2bd', 'authors': ['Jianyu Wang', 'Zhiqiang Hu', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'MiroMind'], 'pdf_title_img': 'assets/pdf/title_img/2506.17930.jpg', 'data': {'categories': ['#optimization', '#alignment', '#multimodal', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ‘ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ğ° Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PromptQuine. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ 'Ğ±ĞµÑÑĞ¼Ñ‹ÑĞ»Ğ¸Ñ†Ñ‹' Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². PromptQuine Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."}, 'en': {'title': "Unlocking LLM Potential with 'Gibberish' Prompts!", 'desc': "This paper introduces PromptQuine, a new approach to designing prompts for large language models (LLMs). Instead of using well-structured instructions, it shows that transforming random examples into 'gibberish' can enhance model performance on various tasks. The authors propose an evolutionary search framework that autonomously finds effective pruning strategies, even with limited data. Their results indicate that this unconventional method can outperform traditional prompt optimization techniques, suggesting new directions for improving in-context learning."}, 'zh': {'title': 'èƒ¡è¨€ä¹±è¯­ï¼Œæå‡æ¨¡å‹è¡¨ç°çš„ç§˜å¯†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºè®¾è®¡èŒƒå¼ï¼Œç§°ä¸ºPromptQuineï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºæ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°†éšæœºç¤ºä¾‹ä¿®å‰ªæˆçœ‹ä¼¼æ— æ„ä¹‰çš„â€œèƒ¡è¨€ä¹±è¯­â€å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³æ–¹æ³•ã€‚å°½ç®¡å‘ç°æœ‰æ•ˆçš„ä¿®å‰ªç­–ç•¥å¹¶ä¸ç®€å•ï¼Œä½†PromptQuineæ¡†æ¶é€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼ï¼Œè‡ªåŠ¨æœç´¢ä¿®å‰ªç­–ç•¥ï¼Œåˆ©ç”¨ä½æ•°æ®ç¯å¢ƒè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¸Œæœ›èƒ½ä¸ºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æœºåˆ¶ç ”ç©¶æä¾›æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨æ›´å¼€æ”¾çš„æœç´¢ç®—æ³•çš„å‘å±•ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„LLMæç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23151', 'title': 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame\n  Optical Flow Estimation', 'url': 'https://huggingface.co/papers/2506.23151', 'abstract': 'MEMFOF is a memory-efficient multi-frame optical flow method that achieves state-of-the-art performance on high-resolution inputs with reduced GPU memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof.', 'score': 15, 'issue_id': 4578, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': 'ad8371e7e40b14b4', 'authors': ['Vladislav Bargatin', 'Egor Chistov', 'Alexander Yakovenko', 'Dmitriy Vatolin'], 'affiliations': ['Lomonosov Moscow State University, Moscow, Russia', 'MSU Institute for Artificial Intelligence, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.23151.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#cv', '#inference'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'MEMFOF - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ GPU Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Spring, Sintel Ğ¸ KITTI-2015. MEMFOF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ RAFT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 2,09 Ğ“Ğ‘ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 1080p, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ½Ğ° Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'MEMFOF: Efficient Optical Flow for High-Resolution Images', 'desc': 'MEMFOF is a novel optical flow estimation method designed to be memory-efficient while maintaining high accuracy for high-resolution images. It significantly reduces GPU memory usage during both training and inference, allowing for full 1080p processing without the need for downsampling. By optimizing design choices from existing architectures and incorporating multi-frame estimation, MEMFOF achieves state-of-the-art performance across various benchmarks. This method not only excels in accuracy but also enhances runtime efficiency, making it a robust solution for real-time applications in optical flow estimation.'}, 'zh': {'title': 'MEMFOFï¼šé«˜æ•ˆå…‰æµä¼°è®¡çš„æ–°é€‰æ‹©', 'desc': 'MEMFOFæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„å¤šå¸§å…‰æµä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡è¾“å…¥ä¸‹å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘GPUå†…å­˜ä½¿ç”¨ã€‚è¯¥æ–¹æ³•åœ¨1080pè¾“å…¥æ—¶ä»…éœ€2.09 GBçš„GPUå†…å­˜ï¼Œè®­ç»ƒæ—¶ä¸º28.5 GBï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åŸç”Ÿ1080pä¸‹è¿›è¡Œè®­ç»ƒï¼Œè€Œæ— éœ€è£å‰ªæˆ–ä¸‹é‡‡æ ·ã€‚MEMFOFé€šè¿‡æ•´åˆå‡å°‘çš„ç›¸å…³ä½“ç§¯å’Œé«˜åˆ†è¾¨ç‡è®­ç»ƒåè®®ï¼Œä¼˜åŒ–äº†RAFTç±»æ¶æ„çš„è®¾è®¡é€‰æ‹©ï¼Œä»è€Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸å…¶ä»–èµ„æºå¯†é›†å‹æ–¹æ³•ç›¸æ¯”ï¼ŒMEMFOFåœ¨å‡†ç¡®æ€§å’Œè¿è¡Œæ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶åœ¨é«˜åˆ†è¾¨ç‡å…‰æµä¼°è®¡ä¸­çš„ç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23542', 'title': 'Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric\n  Attention', 'url': 'https://huggingface.co/papers/2506.23542', 'abstract': 'A novel ToF depth denoising network uses motion-invariant graph fusion and adaptive filters to improve temporal stability and spatial sharpness, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at https://github.com/davidweidawang/GIGA-ToF{https://github.com/davidweidawang/GIGA-ToF}.', 'score': 12, 'issue_id': 4570, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 Ğ¸ÑĞ½Ñ', 'en': 'June 30', 'zh': '6æœˆ30æ—¥'}, 'hash': '9c9286ea4d796818', 'authors': ['Weida Wang', 'Changyong He', 'Jin Zeng', 'Di Qiu'], 'affiliations': ['Google', 'School of Computer Science and Technology, Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23542.jpg', 'data': {'categories': ['#cv', '#optimization', '#dataset', '#graphs', '#synthetic', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ToF: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Time-of-Flight (ToF), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹. Ğ¡ĞµÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing ToF Depth Images with Motion-Invariant Graph Fusion', 'desc': 'This paper presents a new method for denoising depth images captured by Time-of-Flight (ToF) sensors, which often suffer from noise. The proposed network utilizes motion-invariant graph fusion to improve both the temporal stability and spatial sharpness of the images. By leveraging the self-similarity of graph structures across frames, the method effectively addresses depth variations while maintaining consistency. The approach is formulated as a maximum a posterior problem, resulting in an interpretable network that outperforms existing methods on benchmark datasets.'}, 'zh': {'title': 'æå‡ToFæ·±åº¦å›¾åƒçš„å»å™ªæ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ—¶é—´é£è¡Œï¼ˆToFï¼‰æ·±åº¦å»å™ªç½‘ç»œï¼Œåˆ©ç”¨è¿åŠ¨ä¸å˜å›¾èåˆå’Œè‡ªé€‚åº”æ»¤æ³¢å™¨æ¥æé«˜æ—¶é—´ç¨³å®šæ€§å’Œç©ºé—´æ¸…æ™°åº¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å¸§å¤„ç†æˆ–å¤šå¸§å¤„ç†ï¼Œä½†æœªè€ƒè™‘ä¸åŒå¸§ä¸­å¯¹åº”åƒç´ çš„æ·±åº¦å˜åŒ–ï¼Œå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´å’Œç©ºé—´æ¨¡ç³Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å›¾ç»“æ„çš„æ—¶é—´è‡ªç›¸ä¼¼æ€§ï¼Œå®ç°è·¨å¸§å‡ ä½•æ³¨æ„åŠ›çš„å›¾èåˆï¼Œå¹¶ç»“åˆå›¾çš„å¹³æ»‘æ€§å…ˆéªŒå’ŒToFå™ªå£°åˆ†å¸ƒçš„æ•°æ®ä¿¡åº¦é¡¹ï¼Œæ„å»ºæœ€å¤§åéªŒå»å™ªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨åˆæˆDVToFæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®Kinectv2æ•°æ®é›†ä¸Šå±•ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.16500', 'title': 'SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity', 'url': 'https://huggingface.co/papers/2506.16500', 'abstract': 'SparseLoRA reduces computational cost and speeds up fine-tuning of LLMs by dynamically selecting a sparse subset of weights for loss and gradient computation.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning LLMs is both computationally and memory-intensive. While parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the number of trainable parameters and lower memory usage, they do not decrease computational cost. In some cases, they may even slow down fine-tuning. In this paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning through contextual sparsity. We propose a lightweight, training-free SVD sparsity estimator that dynamically selects a sparse subset of weights for loss and gradient computation. Also, we systematically analyze and address sensitivity across layers, tokens, and training steps. Our experimental results show that SparseLoRA reduces computational cost by up to 2.2 times and a measured speedup of up to 1.6 times while maintaining accuracy across various downstream tasks, including commonsense and arithmetic reasoning, code generation, and instruction following.', 'score': 11, 'issue_id': 4572, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 Ğ¸ÑĞ½Ñ', 'en': 'June 19', 'zh': '6æœˆ19æ—¥'}, 'hash': '8a6cd2aa2a56bf51', 'authors': ['Samir Khaki', 'Xiuyu Li', 'Junxian Guo', 'Ligeng Zhu', 'Chenfeng Xu', 'Konstantinos N. Plataniotis', 'Amir Yazdanbakhsh', 'Kurt Keutzer', 'Song Han', 'Zhijian Liu'], 'affiliations': ['Google DeepMind', 'MIT', 'UC Berkeley', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.16500.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'SparseLoRA: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'SparseLoRA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¼ SVD-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SparseLoRA ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 2,2 Ñ€Ğ°Ğ· Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¾ 1,6 Ñ€Ğ°Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'SparseLoRA: Speeding Up LLM Fine-Tuning with Smart Sparsity', 'desc': 'SparseLoRA is a novel method designed to enhance the efficiency of fine-tuning large language models (LLMs) by utilizing a sparse subset of weights for loss and gradient calculations. Unlike previous methods that only reduce the number of trainable parameters, SparseLoRA significantly cuts down on computational costs and speeds up the fine-tuning process. It employs a lightweight, training-free singular value decomposition (SVD) estimator to dynamically select which weights to use, ensuring optimal performance across different layers and training steps. Experimental results demonstrate that SparseLoRA can reduce computational costs by up to 2.2 times and improve speed by up to 1.6 times, all while maintaining high accuracy on various tasks.'}, 'zh': {'title': 'SparseLoRAï¼šåŠ é€Ÿå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¾®è°ƒçš„ç¨€ç–æ–¹æ³•', 'desc': 'SparseLoRAæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©ç¨€ç–æƒé‡å­é›†æ¥å‡å°‘å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒçš„è®¡ç®—æˆæœ¬å’ŒåŠ é€Ÿè¿‡ç¨‹ã€‚ä¸å…¶ä»–å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒSparseLoRAä¸ä»…é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œè¿˜æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è½»é‡çº§çš„è®­ç»ƒæ— å…³å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ç¨€ç–æ€§ä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶é€‰æ‹©éœ€è¦è®¡ç®—çš„æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseLoRAåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†æœ€å¤š2.2å€ï¼Œé€Ÿåº¦æå‡äº†æœ€å¤š1.6å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17417', 'title': 'Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in\n  Inference-time Scaling?', 'url': 'https://huggingface.co/papers/2506.17417', 'abstract': 'Inference-time techniques like decoding-time scaling and self-refinement enhance reasoning in vision-language models, with generation-based methods providing greater improvement than verification-based methods, despite RL-trained models not showing self-correction benefits.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.', 'score': 9, 'issue_id': 4570, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': '57576e8287f4515e', 'authors': ['Mingyuan Wu', 'Meitang Li', 'Jingcheng Yang', 'Jize Jiang', 'Kaizhuo Yan', 'Zhaoheng Li', 'Minjia Zhang', 'Klara Nahrstedt'], 'affiliations': ['University of Illinois Urbana Champaign', 'University of Michigan Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2506.17417.jpg', 'data': {'categories': ['#rl', '#reasoning', '#inference', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¿Ğ¾Ğ±ĞµĞ´Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ¼Ğ°Ğ¶Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñƒ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'Boosting VLM Reasoning with Inference-Time Techniques', 'desc': 'This paper explores how inference-time techniques can improve reasoning in vision-language models (VLMs). It highlights that methods based on generation, like decoding-time scaling, provide better performance than those based on verification. The study also reveals that reinforcement learning (RL) trained models do not exhibit the expected self-correction benefits. Ultimately, the findings suggest that RL-trained VLMs struggle with self-verification, which limits their reasoning capabilities.'}, 'zh': {'title': 'æ¨ç†æ—¶æŠ€æœ¯æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶æŠ€æœ¯å¦‚ä½•æå‡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œè§£ç æ—¶é—´ç¼©æ”¾å’Œè‡ªæˆ‘ä¿®æ­£ç­‰æŠ€æœ¯åœ¨æ²¡æœ‰å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹æ˜¾è‘—æ”¹å–„äº†æ¨¡å‹çš„è¡¨ç°ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹æœªèƒ½æ˜¾ç¤ºå‡ºè‡ªæˆ‘ä¿®æ­£çš„ä¼˜åŠ¿ï¼Œä½†åŸºäºç”Ÿæˆçš„æ–¹æ³•åœ¨æå‡æ€§èƒ½æ–¹é¢æ˜æ˜¾ä¼˜äºåŸºäºéªŒè¯çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLè®­ç»ƒçš„VLMåœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¸Šä»ç¼ºä¹å¼ºå¤§çš„è‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22598', 'title': 'RExBench: Can coding agents autonomously implement AI research\n  extensions?', 'url': 'https://huggingface.co/papers/2506.22598', 'abstract': 'RExBench evaluates the ability of LLM agents to autonomously implement research extensions, failing to achieve significant success without human hints.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.', 'score': 6, 'issue_id': 4582, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 Ğ¸ÑĞ½Ñ', 'en': 'June 27', 'zh': '6æœˆ27æ—¥'}, 'hash': '236eb718627b6f97', 'authors': ['Nicholas Edwards', 'Yukyung Lee', 'Yujun', 'Mao', 'Yulu Qin', 'Sebastian Schuster', 'Najoung Kim'], 'affiliations': ['Boston University', 'University College London', 'University of Vienna'], 'pdf_title_img': 'assets/pdf/title_img/2506.22598.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼', 'desc': 'RExBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 12 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ”Ğ°Ğ¶Ğµ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 40% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Evaluating LLM Agents: The Challenge of Autonomous Research Extensions', 'desc': 'RExBench is a benchmark designed to assess the capability of Large Language Model (LLM) agents in autonomously implementing research extensions. The benchmark includes 12 tasks that require agents to extend existing research papers and codebases, with expert-written instructions provided for guidance. Despite the potential of LLMs in software engineering, the study found that these agents struggled to successfully complete the tasks without significant human assistance. The results highlight the limitations of current LLM agents in handling complex research tasks independently, achieving less than 40% success even with hints.'}, 'zh': {'title': 'è¯„ä¼°LLMä»£ç†çš„ç ”ç©¶æ‰©å±•èƒ½åŠ›', 'desc': 'RExBenchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è‡ªä¸»å®ç°ç ”ç©¶æ‰©å±•èƒ½åŠ›çš„åŸºå‡†å·¥å…·ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„LLMä»£ç†åœ¨æ²¡æœ‰äººç±»æç¤ºçš„æƒ…å†µä¸‹ï¼Œæ— æ³•æˆåŠŸå®Œæˆå¤§å¤šæ•°ç ”ç©¶æ‰©å±•ä»»åŠ¡ã€‚å°½ç®¡åœ¨æä¾›é¢å¤–çš„äººç±»æç¤ºåï¼ŒæˆåŠŸç‡æœ‰æ‰€æé«˜ï¼Œä½†ä»ç„¶ä½äº40%ã€‚è¿™è¡¨æ˜å½“å‰çš„ä»£ç†åœ¨å¤„ç†ç°å®ç ”ç©¶æ‰©å±•ä»»åŠ¡æ—¶ï¼Œä»ç„¶éœ€è¦å¤§é‡çš„äººç±»æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23219', 'title': 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence\n  with Spatial Reasoning and Understanding', 'url': 'https://huggingface.co/papers/2506.23219', 'abstract': 'UrbanLLaVA, a multi-modal large language model, effectively processes urban datasets for various tasks, outperforming existing models in both single-modal and complex cross-modal scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce UrbanLLaVA, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In UrbanLLaVA, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of UrbanLLaVA across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that UrbanLLaVA outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via https://github.com/tsinghua-fib-lab/UrbanLLaVA.', 'score': 5, 'issue_id': 4573, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': '86191833a27a1741', 'authors': ['Jie Feng', 'Shengyuan Wang', 'Tianhui Liu', 'Yanxin Xi', 'Yong Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University, Beijing, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University, China', 'University of Helsinki, Finland'], 'pdf_title_img': 'assets/pdf/title_img/2506.23219.jpg', 'data': {'categories': ['#dataset', '#science', '#training', '#multimodal', '#benchmark', '#open_source'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'UrbanLLaVA: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'UrbanLLaVA - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UrbanLLaVA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'UrbanLLaVA: Revolutionizing Urban Data Processing with Multi-Modal AI', 'desc': 'UrbanLLaVA is a multi-modal large language model specifically designed to handle various urban datasets, excelling in both single-modal and complex cross-modal tasks. It addresses the limitations of existing models by providing a unified framework that processes multiple types of urban data simultaneously. The model is trained using a diverse urban instruction dataset and employs a multi-stage training approach to enhance spatial reasoning and domain knowledge. Experimental results indicate that UrbanLLaVA significantly outperforms other models in urban research, demonstrating strong generalization across different cities.'}, 'zh': {'title': 'UrbanLLaVAï¼šåŸå¸‚æ•°æ®å¤„ç†çš„æ–°çªç ´', 'desc': 'UrbanLLaVAæ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†åŸå¸‚æ•°æ®é›†ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å•æ¨¡æ€å’Œå¤æ‚è·¨æ¨¡æ€åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæå‡äº†ç©ºé—´æ¨ç†å’Œé¢†åŸŸçŸ¥è¯†çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUrbanLLaVAåœ¨å¤šä¸ªåŸå¸‚çš„ä»»åŠ¡ä¸­å‡ä¼˜äºå…¶ä»–å¼€æºå’Œä¸“æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22992', 'title': 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning', 'url': 'https://huggingface.co/papers/2506.22992', 'abstract': 'MARBLE is a challenging multimodal reasoning benchmark that highlights the limitations of existing multimodal language models in step-by-step reasoning and plan crafting under spatial and visual constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to process information from multiple modalities and to reason through it step-by-step remains a critical challenge in advancing artificial intelligence. However, existing reasoning benchmarks focus on text-only reasoning, or employ multimodal questions that can be answered by directly retrieving information from a non-text modality. Thus, complex reasoning remains poorly understood in multimodal domains. Here, we present MARBLE, a challenging multimodal reasoning benchmark that is designed to scrutinize multimodal language models (MLLMs) in their ability to carefully reason step-by-step through complex multimodal problems and environments. MARBLE is composed of two highly challenging tasks, M-Portal and M-Cube, that require the crafting and understanding of multistep plans under spatial, visual, and physical constraints. We find that current MLLMs perform poorly on MARBLE -- all the 12 advanced models obtain near-random performance on M-Portal and 0% accuracy on M-Cube. Only in simplified subtasks some models outperform the random baseline, indicating that complex reasoning is still a challenge for existing MLLMs. Moreover, we show that perception remains a bottleneck, where MLLMs occasionally fail to extract information from the visual inputs. By shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the development of the next generation of models with the ability to reason and plan across many, multimodal reasoning steps.', 'score': 5, 'issue_id': 4574, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': '19689e84c5482c65', 'authors': ['Yulun Jiang', 'Yekun Chai', 'Maria BrbiÄ‡', 'Michael Moor'], 'affiliations': ['EPFL', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2506.22992.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MARBLE: Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…', 'desc': 'MARBLE - ÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡: M-Portal Ğ¸ M-Cube, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ². Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° MARBLE, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° M-Portal Ğ¸ 0% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° M-Cube. MARBLE Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'MARBLE: Unveiling the Limits of Multimodal Reasoning in AI', 'desc': 'MARBLE is a new benchmark designed to test multimodal language models (MLLMs) on their ability to perform complex reasoning tasks that involve both visual and spatial elements. It includes two main tasks, M-Portal and M-Cube, which require models to create and understand multistep plans while adhering to various constraints. Current MLLMs struggle significantly with these tasks, showing near-random performance on M-Portal and failing completely on M-Cube, highlighting their limitations in complex reasoning. By identifying these challenges, MARBLE aims to encourage the development of more advanced models capable of effective multimodal reasoning and planning.'}, 'zh': {'title': 'MARBLEï¼šæ­ç¤ºå¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜', 'desc': 'MARBLEæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨æ­ç¤ºç°æœ‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨é€æ­¥æ¨ç†å’Œè®¡åˆ’åˆ¶å®šæ–¹é¢çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†åŒ…å«ä¸¤ä¸ªé«˜åº¦æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒM-Portalå’ŒM-Cubeï¼Œè¦æ±‚åœ¨ç©ºé—´ã€è§†è§‰å’Œç‰©ç†çº¦æŸä¸‹è¿›è¡Œå¤šæ­¥éª¤è®¡åˆ’çš„åˆ¶å®šå’Œç†è§£ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨MARBLEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ‰€æœ‰12ä¸ªå…ˆè¿›æ¨¡å‹åœ¨M-Portalä¸Šçš„è¡¨ç°æ¥è¿‘éšæœºï¼Œè€Œåœ¨M-Cubeä¸Šçš„å‡†ç¡®ç‡ä¸º0%ã€‚é€šè¿‡æ­ç¤ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼ŒMARBLEå¸Œæœ›æ¨åŠ¨ä¸‹ä¸€ä»£æ¨¡å‹çš„å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€æ¨ç†æ­¥éª¤ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œè®¡åˆ’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23394', 'title': 'Teaching a Language Model to Speak the Language of Tools', 'url': 'https://huggingface.co/papers/2506.23394', 'abstract': 'A methodology is presented to adapt language models for robust tool use across languages, specifically improving function-calling accuracy in Bulgarian.  \t\t\t\t\tAI-generated summary \t\t\t\t External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems.', 'score': 4, 'issue_id': 4570, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': '19b24559e749a260', 'authors': ['Simeon Emanuilov'], 'affiliations': ['Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski'], 'pdf_title_img': 'assets/pdf/title_img/2506.23394.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»Ğ³Ğ°Ñ€ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ BgGPT Ğ½Ğ° Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° TUCAN Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Empowering Multilingual Models for Effective Tool Use', 'desc': 'This paper presents a new method to enhance language models for better tool usage in various languages, focusing on Bulgarian. It addresses the challenges faced by multilingual models in accurately using functions and generating structured outputs, especially in lower-resource languages. The authors introduce TUCAN, a model that significantly improves function-calling accuracy by training on a specialized bilingual dataset. This approach not only boosts performance but also ensures that the outputs are clean and usable, paving the way for better multilingual applications.'}, 'zh': {'title': 'æå‡å¤šè¯­è¨€å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å·¥å…·ä½¿ç”¨çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ä¿åŠ åˆ©äºšè¯­çš„åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ã€‚å¤§å¤šæ•°å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­è¯­è¨€ä¸­ç¼ºä¹å¯é çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚ç ”ç©¶é€šè¿‡å¯¹BgGPTæ¨¡å‹ç³»åˆ—è¿›è¡ŒæŒç»­è®­ç»ƒï¼Œä½¿ç”¨ä¸€ä¸ªåŒ…å«10,035ä¸ªåŠŸèƒ½è°ƒç”¨ç¤ºä¾‹çš„åŒè¯­æ•°æ®é›†ï¼Œæ¥å¢å¼ºæ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œç ”ç©¶æ¨å‡ºçš„TUCANæ¨¡å‹åœ¨åŠŸèƒ½è°ƒç”¨å‡†ç¡®æ€§ä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†28.75%ï¼Œå¹¶ä¸”åœ¨å“åº”æ ¼å¼ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.21448', 'title': 'ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language\n  Models for Audio Generation and Editing', 'url': 'https://huggingface.co/papers/2506.21448', 'abstract': 'ThinkSound, a novel framework, uses Chain-of-Thought reasoning with a multimodal large language model to generate high-quality audio from videos, achieving state-of-the-art results in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While end-to-end video-to-audio generation has greatly improved, producing high-fidelity audio that authentically captures the nuances of visual content remains challenging. Like professionals in the creative industries, such generation requires sophisticated reasoning about items such as visual dynamics, acoustic environments, and temporal relationships. We present ThinkSound, a novel framework that leverages Chain-of-Thought (CoT) reasoning to enable stepwise, interactive audio generation and editing for videos. Our approach decomposes the process into three complementary stages: foundational foley generation that creates semantically coherent soundscapes, interactive object-centric refinement through precise user interactions, and targeted editing guided by natural language instructions. At each stage, a multimodal large language model generates contextually aligned CoT reasoning that guides a unified audio foundation model. Furthermore, we introduce AudioCoT, a comprehensive dataset with structured reasoning annotations that establishes connections between visual content, textual descriptions, and sound synthesis. Experiments demonstrate that ThinkSound achieves state-of-the-art performance in video-to-audio generation across both audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio benchmark. The demo page is available at https://ThinkSound-Project.github.io.', 'score': 4, 'issue_id': 4573, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 Ğ¸ÑĞ½Ñ', 'en': 'June 26', 'zh': '6æœˆ26æ—¥'}, 'hash': 'fefdbdbfb0394a3c', 'authors': ['Huadai Liu', 'Jialei Wang', 'Kaicheng Luo', 'Wen Wang', 'Qian Chen', 'Zhou Zhao', 'Wei Xue'], 'affiliations': ['Hong Kong University of Science and Technology (HKUST)', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21448.jpg', 'data': {'categories': ['#dataset', '#games', '#multimodal', '#benchmark', '#reasoning', '#audio'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°Ğ¹ ĞºĞ°Ğº Ğ·Ğ²ÑƒĞºĞ¾Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€: Ğ˜Ğ˜ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'ThinkSound - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ°, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AudioCoT Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ²ÑƒĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ThinkSound Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'ThinkSound: Revolutionizing Video-to-Audio Generation with Chain-of-Thought Reasoning', 'desc': 'ThinkSound is a new framework that enhances video-to-audio generation by using Chain-of-Thought reasoning with a multimodal large language model. It addresses the challenge of creating high-quality audio that accurately reflects the visual content by breaking the process into three stages: foundational foley generation, interactive object-centric refinement, and targeted editing. Each stage utilizes contextually aligned reasoning to guide the audio generation, ensuring that the soundscapes are semantically coherent and tailored to user inputs. The framework also introduces the AudioCoT dataset, which connects visual elements, text descriptions, and sound synthesis, leading to state-of-the-art performance in various benchmarks.'}, 'zh': {'title': 'ThinkSoundï¼šè§†é¢‘ç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘çš„æ–°æ–¹æ³•', 'desc': 'ThinkSoundæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é“¾å¼æ€ç»´æ¨ç†ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä»è§†é¢‘ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘ï¼Œè¾¾åˆ°äº†å„é¡¹åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœã€‚å°½ç®¡ç«¯åˆ°ç«¯çš„è§†é¢‘åˆ°éŸ³é¢‘ç”ŸæˆæŠ€æœ¯å·²æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†ç”ŸæˆçœŸå®æ•æ‰è§†è§‰å†…å®¹ç»†å¾®å·®åˆ«çš„é«˜ä¿çœŸéŸ³é¢‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªäº’è¡¥é˜¶æ®µï¼šåŸºç¡€éŸ³æ•ˆç”Ÿæˆã€äº¤äº’å¼å¯¹è±¡ä¸­å¿ƒç»†åŒ–å’ŒåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç›®æ ‡ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥AudioCoTæ•°æ®é›†ï¼ŒThinkSoundåœ¨è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆçš„å®éªŒä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨éŸ³é¢‘æŒ‡æ ‡å’Œé“¾å¼æ€ç»´æŒ‡æ ‡ä¸Šå‡å–å¾—äº†é¢†å…ˆæˆç»©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22694', 'title': 'VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs', 'url': 'https://huggingface.co/papers/2506.22694', 'abstract': 'A technique called VocabTrim improves drafter-based speculative decoding by reducing the vocabulary of the drafter language model, thus decreasing drafting latency in memory-bound environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.', 'score': 3, 'issue_id': 4573, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': 'a9c9ccf73356a002', 'authors': ['Raghavv Goel', 'Sudhanshu Agrawal', 'Mukul Gagrani', 'Junyoung Park', 'Yifan Zao', 'He Zhang', 'Tian Liu', 'Yiping Yang', 'Xin Yuan', 'Jiuyan Lu', 'Chris Lott', 'Mingu Lee'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.22694.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ VocabTrim, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ°. VocabTrim Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ head-Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ°, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¥Ğ¾Ñ‚Ñ ÑÑ‚Ğ¾ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 16% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.2-3B-Instruct Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Speed Up Drafting with VocabTrim!', 'desc': 'This paper presents VocabTrim, a technique designed to enhance drafter-based speculative decoding by optimizing the vocabulary used in the drafting process. By limiting the vocabulary to only the most frequently sampled tokens from the target model, VocabTrim reduces the inference overhead associated with drafting, particularly in memory-constrained environments. Although this approach may slightly lower the acceptance rate of generated sequences, it significantly accelerates drafting speed, especially on edge devices. The results demonstrate a notable 16% improvement in memory-bound speed-up for Llama-3 models on Spec-Bench, showcasing the effectiveness of this method.'}, 'zh': {'title': 'VocabTrimï¼šæå‡æ¨æµ‹è§£ç é€Ÿåº¦çš„å…³é”®æŠ€æœ¯', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVocabTrimçš„æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡å‡å°‘è‰æ‹Ÿè¯­è¨€æ¨¡å‹çš„è¯æ±‡é‡æ¥æ”¹å–„åŸºäºè‰æ‹Ÿçš„æ¨æµ‹è§£ç æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é™ä½åœ¨å†…å­˜å—é™ç¯å¢ƒä¸­çš„è‰æ‹Ÿå»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¤§è¯æ±‡é‡çš„ç›®æ ‡è¯­è¨€æ¨¡å‹ã€‚VocabTrimé€šè¿‡é‡æ„è‰æ‹Ÿè¯­è¨€æ¨¡å‹çš„å¤´éƒ¨ï¼Œä»…ä¿ç•™ç›®æ ‡æ¨¡å‹ä¸­æœ€å¸¸è¢«é‡‡æ ·çš„æœ‰é™è¯æ±‡ï¼Œä»è€Œæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚å°½ç®¡é™åˆ¶è¯æ±‡ä¼šç•¥å¾®é™ä½æ¥å—ç‡ï¼Œä½†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ˜¾è‘—æé«˜äº†å†…å­˜å—é™é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.23135', 'title': 'RoboScape: Physics-informed Embodied World Model', 'url': 'https://huggingface.co/papers/2506.23135', 'abstract': 'RoboScape is a unified physics-informed world model that enhances visual fidelity and physical plausibility in robotic video generation by integrating temporal depth prediction and keypoint dynamics learning.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.', 'score': 2, 'issue_id': 4574, 'pub_date': '2025-06-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ½Ñ', 'en': 'June 29', 'zh': '6æœˆ29æ—¥'}, 'hash': '421522bfdd825b96', 'authors': ['Yu Shang', 'Xin Zhang', 'Yinzhou Tang', 'Lei Jin', 'Chen Gao', 'Wei Wu', 'Yong Li'], 'affiliations': ['Manifold AI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.23135.jpg', 'data': {'categories': ['#optimization', '#science', '#open_source', '#robotics', '#video', '#games', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RoboScape: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'RoboScape - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'RoboScape: Realistic Robotic Video Generation with Physics Awareness', 'desc': 'RoboScape is a new model that improves how robots generate videos by making them look more realistic and physically accurate. It combines two important tasks: predicting depth over time to ensure 3D shapes are consistent, and learning how key points move to capture the physical properties of objects. This model helps robots create videos that are not only visually appealing but also reflect real-world physics, especially in complex interactions. The research shows that RoboScape can be used effectively for training robotic policies and evaluating their performance using the generated videos.'}, 'zh': {'title': 'RoboScapeï¼šæå‡æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„ç‰©ç†çœŸå®æ„Ÿ', 'desc': 'RoboScape æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡æ•´åˆæ—¶é—´æ·±åº¦é¢„æµ‹å’Œå…³é”®ç‚¹åŠ¨æ€å­¦ä¹ ï¼Œæå‡äº†æœºå™¨äººè§†é¢‘ç”Ÿæˆçš„è§†è§‰çœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§ã€‚å½“å‰çš„ä¸–ç•Œæ¨¡å‹åœ¨å»ºæ¨¡ä¸‰ç»´å‡ ä½•å’Œè¿åŠ¨åŠ¨æ€æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘åœ¨æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººåœºæ™¯ä¸­ä¸å¤ŸçœŸå®ã€‚æœ¬æ–‡æå‡ºçš„ RoboScape é€šè¿‡è”åˆå­¦ä¹  RGB è§†é¢‘ç”Ÿæˆå’Œç‰©ç†çŸ¥è¯†ï¼Œé‡‡ç”¨äº†ä¸¤ä¸ªå…³é”®çš„ç‰©ç†ä¿¡æ¯è”åˆè®­ç»ƒä»»åŠ¡ï¼Œå¢å¼ºäº†è§†é¢‘æ¸²æŸ“ä¸­çš„ä¸‰ç»´å‡ ä½•ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboScape åœ¨å¤šç§æœºå™¨äººåœºæ™¯ä¸­ç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ›´é«˜çš„è§†è§‰çœŸå®æ„Ÿå’Œç‰©ç†åˆç†æ€§ï¼Œå¹¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­éªŒè¯äº†å…¶å®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.22753', 'title': 'Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography', 'url': 'https://huggingface.co/papers/2506.22753', 'abstract': 'The proposed Degradation-Modeled Multipath Diffusion framework improves metalens image quality by using natural image priors and specific modules to balance detail, fidelity, and perceptual quality while addressing optical degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside pseudo data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: https://dmdiff.github.io/.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 Ğ¸ÑĞ½Ñ', 'en': 'June 28', 'zh': '6æœˆ28æ—¥'}, 'hash': 'a52ba639d6bd763f', 'authors': ['Jianing Zhang', 'Jiayi Zhu', 'Feiyu Ji', 'Xiaokang Yang', 'Xiaoyun Yuan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.22753.jpg', 'data': {'categories': ['#hallucinations', '#cv', '#data', '#inference', '#diffusion'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ· Ğ±ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Degradation-Modeled Multipath Diffusion Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ·. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ¸Ğ½Ğ·. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Metalens Imaging with Adaptive Degradation Modeling', 'desc': 'The Degradation-Modeled Multipath Diffusion framework enhances the image quality of metalenses by integrating natural image priors and specialized modules. It effectively addresses the challenges of optical degradation while balancing detail, fidelity, and perceptual quality. Unlike traditional methods that require extensive calibration or large datasets, this approach utilizes pretrained models and a tunable decoder for better control over the inference process. The introduction of a spatially varying degradation-aware attention module allows for adaptive modeling of complex degradations, leading to superior image reconstruction results.'}, 'zh': {'title': 'é™è§£å»ºæ¨¡æå‡é‡‘å±é€é•œå›¾åƒè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é™è§£å»ºæ¨¡çš„å¤šè·¯å¾„æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è‡ªç„¶å›¾åƒå…ˆéªŒæ¥æ”¹å–„é‡‘å±é€é•œçš„å›¾åƒè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å®šæ¨¡å—å¹³è¡¡ç»†èŠ‚ã€ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶è§£å†³å…‰å­¦é™è§£é—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºç²¾ç¡®çš„å…‰å­¦æ ¡å‡†æˆ–åºå¤§çš„é…å¯¹æ•°æ®é›†ï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§è‡ªç„¶å›¾åƒå…ˆéªŒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸå’Œæ¸…æ™°å›¾åƒé‡å»ºæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2506.17080', 'title': 'Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs', 'url': 'https://huggingface.co/papers/2506.17080', 'abstract': 'Tower+, a suite of fine-tuned language models, achieves strong performance in both translation and multilingual general-purpose text tasks through a novel training recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization.', 'score': 2, 'issue_id': 4578, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ½Ñ', 'en': 'June 20', 'zh': '6æœˆ20æ—¥'}, 'hash': 'f35759eeab0ef755', 'authors': ['Ricardo Rei', 'Nuno M. Guerreiro', 'JosÃ© Pombal', 'JoÃ£o Alves', 'Pedro Teixeirinha', 'Amin Farajian', 'AndrÃ© F. T. Martins'], 'affiliations': ['Instituto Superior TÃ©cnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de TelecomunicaÃ§Ãµes', 'MICS, CentraleSupÃ©lec, UniversitÃ© Paris-Saclay', 'Unbabel'], 'pdf_title_img': 'assets/pdf/title_img/2506.17080.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#optimization', '#dataset', '#training', '#machine_translation'], 'emoji': 'ğŸ—¼', 'ru': {'title': 'Tower+: Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Tower+ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Tower+ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Tower+ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Tower+: Bridging Translation and General-Purpose Language Tasks', 'desc': 'Tower+ is a suite of fine-tuned language models that excels in both translation and multilingual text tasks. The models are trained using a unique recipe that includes continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning, allowing them to maintain strong general-purpose capabilities while specializing in translation. By carefully curating data at each training stage, Tower+ achieves a balance between translation performance and general tasks like code generation and problem-solving. The results show that even smaller models can outperform larger existing models, demonstrating the effectiveness of this training approach.'}, 'zh': {'title': 'Tower+: ç¿»è¯‘ä¸å¤šè¯­è¨€èƒ½åŠ›çš„å®Œç¾å¹³è¡¡', 'desc': 'Tower+æ˜¯ä¸€å¥—ç»è¿‡ç²¾ç»†è°ƒä¼˜çš„è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç¿»è¯‘å’Œå¤šè¯­è¨€é€šç”¨æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€åå¥½ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒTower+å®ç°äº†ç¿»è¯‘ä¸“ä¸šåŒ–ä¸å¤šè¯­è¨€é€šç”¨èƒ½åŠ›ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µç²¾å¿ƒç”Ÿæˆå’Œæ•´ç†æ•°æ®ï¼Œä»¥å¢å¼ºç¿»è¯‘å’Œé€šç”¨ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªè§„æ¨¡ä¸Šå¼€å‘ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¸¸å¸¸è¶…è¶Šæ›´å¤§çš„é€šç”¨æ¨¡å‹ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (4)', '#agi (2)', '#alignment (4)', '#architecture (11)', '#audio (1)', '#benchmark (19)', '#cv (10)', '#data (4)', '#dataset (15)', '#diffusion (8)', '#ethics (1)', '#games (7)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (6)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (18)', '#open_source (10)', '#optimization (18)', '#plp', '#rag (1)', '#reasoning (12)', '#rl (9)', '#rlhf (2)', '#robotics (1)', '#science (5)', '#security (1)', '#small_models (2)', '#story_generation', '#survey (3)', '#synthetic (3)', '#training (20)', '#transfer_learning (2)', '#video (7)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-03 12:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-03 12:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-03 12:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    