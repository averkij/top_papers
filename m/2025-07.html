
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. July 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Июль 2025</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-06.html">⬅️ <span id="prev-date">06.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-08.html">➡️ <span id="next-date">08.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Июль 2025', 'en': 'July 2025', 'zh': '7月2025年'};
        let feedDateNext = {'ru': '08.2025', 'en': '08/2025', 'zh': '8月2025年'};
        let feedDatePrev = {'ru': '06.2025', 'en': '06/2025', 'zh': '6月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.17450', 'title': 'BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing', 'url': 'https://huggingface.co/papers/2506.17450', 'abstract': 'A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.', 'score': 45, 'issue_id': 4550, 'pub_date': '2025-06-20', 'pub_date_card': {'ru': '20 июня', 'en': 'June 20', 'zh': '6月20日'}, 'hash': 'b5bb4470d500be10', 'authors': ['Jiacheng Chen', 'Ramin Mehran', 'Xuhui Jia', 'Saining Xie', 'Sanghyun Woo'], 'affiliations': ['Google DeepMind', 'New York University', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17450.jpg', 'data': {'categories': ['#cv', '#diffusion', '#3d', '#training'], 'emoji': '🎭', 'ru': {'title': 'Генеративная композиция сцен с 3D-контролем', 'desc': 'BlenderFusion - это генеративная система визуальной композиции, использующая диффузионную модель для редактирования и составления сцен. Она работает по принципу разделения на слои, редактирования и композиции, преобразуя визуальные входные данные в редактируемые 3D-объекты. Система использует предобученную диффузионную модель, дообученную на видеокадрах с применением маскирования исходного изображения и симуляции дрожания объектов. BlenderFusion значительно превосходит существующие методы в сложных задачах композиционного редактирования сцен.'}, 'en': {'title': 'Revolutionizing Scene Editing with BlenderFusion', 'desc': 'BlenderFusion is a framework that allows users to create new scenes by rearranging objects, backgrounds, and camera angles. It uses a three-step process: first, it segments visual inputs into 3D elements, then it allows for editing these elements in Blender, and finally, it combines them into a complete scene using a generative compositor. The compositor is based on a diffusion model that processes both the original and edited scenes simultaneously, enhancing the editing process. Key techniques like source masking and simulated object jittering improve flexibility and control in scene composition, leading to better results than previous methods.'}, 'zh': {'title': '生成视觉合成的新方法', 'desc': 'BlenderFusion是一个生成视觉合成框架，能够通过重新组合对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程，首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D控制的编辑，最后使用生成合成器将它们融合成一个连贯的场景。该生成合成器扩展了预训练的扩散模型，能够并行处理原始场景和编辑后的场景。BlenderFusion在复杂的合成场景编辑任务中显著优于之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2506.21862', 'title': 'LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs', 'url': 'https://huggingface.co/papers/2506.21862', 'abstract': 'LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.', 'score': 29, 'issue_id': 4548, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': 'b9ad171aa3fb5bbf', 'authors': ['Boyuan Sun', 'Jiaxing Zhao', 'Xihan Wei', 'Qibin Hou'], 'affiliations': ['Tongyi Lab, Alibaba Group', 'VCIP, School of Computer Science, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21862.jpg', 'data': {'categories': ['#training', '#benchmark', '#multimodal', '#long_context', '#dataset', '#video'], 'emoji': '✂️', 'ru': {'title': 'Умное сжатие для умных видеомоделей', 'desc': 'LLaVA-Scissor - это стратегия сжатия токенов для видео мультимодальных больших языковых моделей. Она использует метод Семантически Связанных Компонентов (SCC) для эффективного сжатия токенов, сохраняя при этом семантическое покрытие. LLaVA-Scissor применяет двухэтапный подход к пространственно-временному сжатию токенов, используя SCC как в пространственной, так и во временной областях. Экспериментальные результаты показывают, что LLaVA-Scissor превосходит другие методы сжатия токенов в различных задачах понимания видео.'}, 'en': {'title': 'Efficient Video Understanding with Semantic Token Compression', 'desc': "LLaVA-Scissor is a novel token compression strategy specifically designed for video multimodal large language models. It utilizes Semantic Connected Components (SCC) to effectively group tokens into distinct semantic regions, which helps in reducing redundancy and maintaining semantic integrity. Unlike previous methods that rely on attention scores, LLaVA-Scissor compresses tokens in both spatial and temporal dimensions, ensuring comprehensive coverage of the video's content. Extensive evaluations demonstrate that this approach significantly outperforms existing token compression techniques, especially when retaining fewer tokens during video understanding tasks."}, 'zh': {'title': 'LLaVA-Scissor：高效的视频令牌压缩策略', 'desc': 'LLaVA-Scissor是一种针对视频多模态大语言模型的令牌压缩策略。它利用语义连通组件（SCC）方法，有效地将令牌分配到不同的语义区域，从而确保全面的语义覆盖。与以往基于注意力分数的压缩方法不同，LLaVA-Scissor能够减少令牌冗余，并在空间和时间域中进行两步压缩。实验结果表明，该方法在视频理解基准测试中表现优异，尤其是在低令牌保留比率下。'}}}, {'id': 'https://huggingface.co/papers/2506.21416', 'title': 'XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation', 'url': 'https://huggingface.co/papers/2506.21416', 'abstract': 'XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.', 'score': 23, 'issue_id': 4551, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '4c3c383901d9306f', 'authors': ['Bowen Chen', 'Mengyi Zhao', 'Haomiao Sun', 'Li Chen', 'Xu Wang', 'Kang Du', 'Xinglong Wu'], 'affiliations': ['Intelligent Creation Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.21416.jpg', 'data': {'categories': ['#diffusion', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над множеством объектов в генерации изображений', 'desc': 'XVerse - это новая модель для контролируемой генерации изображений с несколькими объектами. Она использует модуляцию текстового потока для точного и независимого управления отдельными объектами. XVerse преобразует референсные изображения в смещения для токен-специфичной модуляции, что позволяет контролировать характеристики объектов без нарушения латентного пространства. Это значительно улучшает возможности персонализированной генерации сложных сцен с высокой точностью.'}, 'en': {'title': 'XVerse: Mastering Multi-Subject Control in Image Generation', 'desc': 'XVerse is a new model that improves text-to-image generation by allowing users to control multiple subjects in an image with high precision. It uses a technique called token-specific text-stream modulation to manage the identity and attributes of each subject, such as pose and lighting, without losing image quality. Traditional methods often create unwanted artifacts or mix up attributes, but XVerse avoids these issues by transforming reference images into specific adjustments. This leads to better coherence and fidelity in generated images, making it easier to create complex scenes with distinct and editable subjects.'}, 'zh': {'title': 'XVerse：精确控制多对象图像生成的创新', 'desc': 'XVerse是一种增强文本到图像生成的模型，能够对多个对象进行精确和独立的控制。它通过特定的文本流调制，解决了在生成多对象图像时常见的编辑性和一致性问题。XVerse将参考图像转换为偏移量，从而实现对特定对象的控制，而不干扰图像的潜在特征。这一创新显著提高了个性化和复杂场景生成的能力。'}}}, {'id': 'https://huggingface.co/papers/2506.21356', 'title': 'ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2506.21356', 'abstract': "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce ShotBench, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct ShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new state-of-the-art performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.", 'score': 19, 'issue_id': 4551, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '5a54508ae68df265', 'authors': ['Hongbo Liu', 'Jingwen He', 'Yi Jin', 'Dian Zheng', 'Yuhao Dong', 'Fan Zhang', 'Ziqi Huang', 'Yinan He', 'Yangguang Li', 'Weichao Chen', 'Yu Qiao', 'Wanli Ouyang', 'Shengjie Zhao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21356.jpg', 'data': {'categories': ['#dataset', '#games', '#training', '#benchmark', '#open_source', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Новый бенчмарк для оценки понимания языка кино искусственным интеллектом', 'desc': 'Статья представляет новый набор данных ShotBench для оценки понимания кинематографического языка моделями искусственного интеллекта. Авторы также создали большой датасет ShotQA с 70 тысячами пар вопросов-ответов о кинематографии. На основе ShotQA была разработана модель ShotVL, превзошедшая существующие решения в понимании визуального языка кино. Эти инструменты призваны улучшить способности ИИ в анализе и генерации видеоконтента.'}, 'en': {'title': "Enhancing AI's Cinematic Language Comprehension with ShotBench and ShotQA", 'desc': "This paper introduces ShotBench and ShotQA datasets, along with the ShotVL model, to improve AI's ability to understand and generate cinematic language. Cinematography is a complex visual language that conveys stories and emotions, but current Vision-Language Models (VLMs) struggle with its nuances. The ShotBench benchmark evaluates VLMs on their comprehension of cinematic grammar, revealing significant limitations in their performance. By developing ShotQA and fine-tuning the ShotVL model, the authors achieve state-of-the-art results, providing valuable resources for advancing AI in cinematic understanding and generation."}, 'zh': {'title': '提升AI电影语言理解的突破性进展', 'desc': '本文介绍了ShotBench和ShotQA数据集以及ShotVL模型，旨在提升人工智能对电影语言的理解和生成能力。电影摄影是传达叙事、情感和美学质量的基本视觉语言，但现有的视觉-语言模型在理解细腻的电影语法方面仍存在不足。我们构建了ShotBench基准，包含3500多个专家注释的问答对，评估了24个领先的视觉-语言模型，结果显示它们在细粒度视觉线索和复杂空间推理方面表现不佳。通过构建ShotQA数据集并开发ShotVL模型，我们在ShotBench上取得了新的最佳性能，推动了AI在电影理解和生成领域的进步。'}}}, {'id': 'https://huggingface.co/papers/2506.20279', 'title': 'From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios', 'url': 'https://huggingface.co/papers/2506.20279', 'abstract': "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense prediction tasks hold significant importance of computer vision, aiming to learn pixel-wise annotated label for an input image. Despite advances in this field, existing methods primarily focus on idealized conditions, with limited generalization to real-world scenarios and facing the challenging scarcity of real-world data. To systematically study this problem, we first introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction tasks that correspond to urgent real-world applications, featuring unified evaluation across tasks. Then, we propose DenseDiT, which maximally exploits generative models' visual priors to perform diverse real-world dense prediction tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism and two lightweight branches that adaptively integrate multi-scale context, working with less than 0.1% additional parameters. Evaluations on DenseWorld reveal significant performance drops in existing general and specialized baselines, highlighting their limited real-world generalization. In contrast, DenseDiT achieves superior results using less than 0.01% training data of baselines, underscoring its practical value for real-world deployment. Our data, and checkpoints and codes are available at https://xcltql666.github.io/DenseDiTProj", 'score': 16, 'issue_id': 4550, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '8382f71877fe1997', 'authors': ['Changliang Xia', 'Chengyou Jia', 'Zhuohang Dang', 'Minnan Luo'], 'affiliations': ['Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.20279.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#cv', '#synthetic', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Эффективное плотное предсказание с минимумом данных', 'desc': 'DenseDiT - это подход к генеративным моделям для задач плотного предсказания в компьютерном зрении. Он достигает превосходных результатов на реальных данных, используя минимальное количество обучающих примеров. DenseDiT максимально использует визуальные прайоры генеративных моделей и включает механизм повторного использования параметров. Модель превосходит существующие методы, используя менее 0,01% обучающих данных по сравнению с базовыми моделями.'}, 'en': {'title': 'DenseDiT: Revolutionizing Dense Prediction with Minimal Data', 'desc': "DenseDiT is a generative model that excels in dense prediction tasks, which involve assigning labels to each pixel in an image. It addresses the challenge of limited training data by leveraging visual priors from generative models, allowing it to perform well in real-world scenarios. The model introduces DenseWorld, a benchmark for evaluating various dense prediction tasks, highlighting the shortcomings of existing methods in real-world applications. DenseDiT's innovative design, which includes a parameter-reuse mechanism and multi-scale context integration, enables it to achieve superior performance with significantly less training data compared to traditional approaches."}, 'zh': {'title': 'DenseDiT：用最少数据实现密集预测的突破', 'desc': 'DenseDiT是一种基于生成模型的方法，能够在真实世界的密集预测任务中以最少的训练数据实现优越的性能。密集预测任务在计算机视觉中非常重要，旨在为输入图像学习逐像素的标注标签。现有方法主要集中在理想条件下，缺乏对真实场景的广泛适应性，且面临真实数据稀缺的挑战。DenseDiT通过最大限度地利用生成模型的视觉先验，结合参数重用机制和轻量级分支，能够在多种真实世界的密集预测任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.21628', 'title': 'Ark: An Open-source Python-based Framework for Robot Learning', 'url': 'https://huggingface.co/papers/2506.21628', 'abstract': "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots.", 'score': 11, 'issue_id': 4555, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '19bc247d9dffd525', 'authors': ['Magnus Dierking', 'Christopher E. Mower', 'Sarthak Das', 'Huang Helong', 'Jiacheng Qiu', 'Cody Reading', 'Wei Chen', 'Huidong Liang', 'Huang Guowei', 'Jan Peters', 'Quan Xingyue', 'Jun Wang', 'Haitham Bou-Ammar'], 'affiliations': ['Huawei Noahs Ark', 'Imperial College London', 'Technical University of Darmstadt', 'University College London', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.21628.jpg', 'data': {'categories': ['#open_source', '#games', '#agents', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'ARK: Объединяя робототехнику и ИИ под крылом Python', 'desc': 'ARK - это открытая Python-фреймворк для робототехники, объединяющая современные алгоритмы имитационного обучения и взаимодействие симуляции с физическими роботами. Фреймворк предоставляет интерфейс в стиле Gym для сбора данных, их предобработки и обучения политик с использованием передовых алгоритмов имитационного обучения. ARK включает модули для управления, SLAM, планирования движения и визуализации, а также совместимость с ROS. Благодаря унификации практик робототехники и ИИ в рамках Python, ARK упрощает разработку и ускоряет исследования и коммерческое внедрение автономных роботов.'}, 'en': {'title': 'Simplifying Robotics with Python: ARK Framework', 'desc': 'ARK is an open-source framework that simplifies robotics development by integrating imitation-learning algorithms with Python, making it more accessible for developers. It provides a Gym-style interface for data collection, preprocessing, and training policies, allowing seamless transitions between simulation and real-world robots. The framework includes a client-server architecture for efficient communication and offers modules for various robotics tasks like control and motion planning. By bridging the gap between robotics and AI, ARK enhances rapid prototyping and deployment of autonomous systems.'}, 'zh': {'title': 'ARK：简化机器人开发的Python框架', 'desc': 'ARK是一个开源的Python优先框架，旨在简化机器人开发和部署。它集成了现代模仿学习算法和无缝的仿真与物理机器人交互，提供了一个类似Gym的环境接口，方便用户收集数据、预处理和训练策略。ARK还具有轻量级的客户端-服务器架构，支持网络通信，并提供C/C++绑定以确保实时性能。通过统一机器人和人工智能的实践，ARK降低了入门门槛，加速了自主机器人的研究和商业部署。'}}}, {'id': 'https://huggingface.co/papers/2505.21411', 'title': 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity', 'url': 'https://huggingface.co/papers/2505.21411', 'abstract': 'Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.', 'score': 11, 'issue_id': 4552, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e4bcbe7787b328fa', 'authors': ['Yehui Tang', 'Xiaosong Li', 'Fangcheng Liu', 'Wei Guo', 'Hang Zhou', 'Yaoyuan Wang', 'Kai Han', 'Xianzhi Yu', 'Jinpeng Li', 'Hui Zang', 'Fei Mi', 'Xiaojun Meng', 'Zhicheng Liu', 'Hanting Chen', 'Binfan Zheng', 'Can Chen', 'Youliang Yan', 'Ruiming Tang', 'Peifeng Qin', 'Xinghao Chen', 'Dacheng Tao', 'Yunhe Wang'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2505.21411.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#inference'], 'emoji': '🧠', 'ru': {'title': 'MoGE: Повышение эффективности больших языковых моделей через групповую экспертизу', 'desc': 'Статья представляет новый метод Mixture of Grouped Experts (MoGE) для улучшения балансировки нагрузки экспертов в больших языковых моделях. MoGE группирует экспертов при выборе, что обеспечивает более равномерное распределение вычислительной нагрузки между устройствами. На основе MoGE авторы создали модель Pangu Pro MoE с 72 миллиардами параметров, оптимизированную для NPU Ascend. Эксперименты показали, что MoGE повышает эффективность обучения и вывода модели, превосходя сопоставимые плотные модели по производительности.'}, 'en': {'title': 'Balancing Experts for Efficient Language Model Execution', 'desc': 'The paper introduces Mixture of Grouped Experts (MoGE), a novel approach to improve the efficiency of large language models by enhancing expert load balancing. MoGE ensures that an equal number of experts are activated for each input token, which addresses the issue of uneven expert activation seen in traditional Mixture of Experts (MoE) models. This architectural design allows for better distribution of computational load across multiple devices, significantly increasing throughput during inference. The results demonstrate that MoGE leads to superior performance and cost-effectiveness on Ascend NPUs, particularly with the Pangu Pro MoE model, which achieves impressive inference speeds and outperforms existing dense models.'}, 'zh': {'title': '混合分组专家：提升大型语言模型的效率与性能', 'desc': '混合分组专家（MoGE）是一种改进的模型架构，旨在提高大型语言模型的专家负载平衡和执行效率。通过将专家分组选择，MoGE确保每个输入令牌激活的专家数量相等，从而减少了系统在并行运行时的效率损失。我们在Ascend NPU上构建了Pangu Pro MoE，这是一种基于MoGE的稀疏模型，具有720亿个参数，显著提高了推理性能。实验结果表明，MoGE在模型训练和推理中都能实现更好的专家负载平衡和更高的执行效率。'}}}, {'id': 'https://huggingface.co/papers/2506.21656', 'title': 'Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs', 'url': 'https://huggingface.co/papers/2506.21656', 'abstract': 'SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.', 'score': 10, 'issue_id': 4548, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '8d063b13fc555964', 'authors': ['Yifan Shen', 'Yuanzhe Liu', 'Jingyuan Zhu', 'Xu Cao', 'Xiaofeng Zhang', 'Yixiao He', 'Wenming Ye', 'James Matthew Rehg', 'Ismini Lourentzou'], 'affiliations': ['Google', 'Shanghai Jiao Tong University', 'University of Illinois Urbana-Champaign', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.21656.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#multimodal', '#rlhf', '#cv', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Пространственное мышление ИИ выходит на новый уровень', 'desc': 'SpatialReasoner-R1 - это новая модель зрительно-языкового рассуждения, которая улучшает пространственное мышление с помощью мультимодельного поиска Монте-Карло по дереву и оптимизации прямых предпочтений. Модель генерирует длинные цепочки рассуждений и использует сегментированную оптимизацию предпочтений для улучшения визуальной и логической согласованности. SpatialReasoner-R1 достигает нового уровня производительности на бенчмарке SPATIALRGPT-Bench, превосходя базовые модели на 9.8% по средней точности. При этом модель сохраняет конкурентоспособность в общих задачах компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Elevating Spatial Reasoning with SpatialReasoner-R1', 'desc': "SpatialReasoner-R1 is a vision-language reasoning model that enhances spatial reasoning capabilities in AI. It employs Multi-Model Monte Carlo Tree Search (M3CTS) to create diverse reasoning paths that are logically consistent, improving the model's ability to handle complex spatial tasks. Additionally, it introduces fine-grained Direct Preference Optimization (fDPO), which refines the model's decision-making by focusing on specific segments of reasoning and using a spatial reward mechanism. The model achieves state-of-the-art performance on the SPATIALRGPT-Bench, significantly outperforming previous models in both spatial quality and quantity tasks."}, 'zh': {'title': '空间推理的新突破', 'desc': 'SpatialReasoner-R1是一种视觉-语言推理模型，旨在解决当前视觉-语言模型在细粒度空间推理方面的不足。该模型采用多模型蒙特卡洛树搜索（M3CTS）方法，生成多样且逻辑一致的长链思维推理轨迹，以构建高质量的空间推理监督。除此之外，SpatialReasoner-R1还引入了细粒度直接偏好优化（fDPO），通过空间奖励机制对候选响应进行评估，从而提高描述性基础和逻辑推理的准确性。实验结果表明，SpatialReasoner-R1在SPATIALRGPT-Bench上设定了新的最先进水平，平均准确率比最强基线提高了9.8%。'}}}, {'id': 'https://huggingface.co/papers/2506.22434', 'title': 'MiCo: Multi-image Contrast for Reinforcement Visual Reasoning', 'url': 'https://huggingface.co/papers/2506.22434', 'abstract': 'Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.', 'score': 9, 'issue_id': 4548, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': 'd7e89f248d4c331e', 'authors': ['Xi Chen', 'Mingkang Zhu', 'Shaoteng Liu', 'Xiaoyang Wu', 'Xiaogang Xu', 'Yu Liu', 'Xiang Bai', 'Hengshuang Zhao'], 'affiliations': ['CUHK', 'HKU', 'HUST', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.22434.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#cv', '#dataset', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Самообучение ИИ визуальным рассуждениям без участия человека', 'desc': 'Статья представляет метод самообучения моделей компьютерного зрения и обработки естественного языка (VLM) для улучшения их способности рассуждать о нескольких изображениях. Авторы используют триплеты изображений и обучение с подкреплением, чтобы научить модель сравнивать тонкие визуальные детали. Этот подход не требует размеченных человеком пар вопрос-ответ и позволяет генерировать цепочки рассуждений. Эксперименты показывают, что полученные навыки обобщаются на широкий спектр задач визуального анализа.'}, 'en': {'title': 'Empowering VLMs with Self-Supervised Image Triplet Learning', 'desc': 'This paper presents a method for enhancing the reasoning capabilities of Vision-Language Models (VLMs) using self-supervised learning with image triplets. The approach involves creating pairs of augmented images along with a distinct image, allowing the model to learn to compare and reason about visual differences. By training the model to determine whether images are the same or different, it develops a reasoning process that generalizes to various tasks without needing human-annotated data. The results demonstrate that this method significantly improves performance on multi-image reasoning benchmarks and general vision tasks.'}, 'zh': {'title': '自监督学习提升视觉语言模型推理能力', 'desc': '这篇论文探讨了如何通过使用图像三元组的自监督学习来增强视觉语言模型（VLM）在多图像任务上的推理能力，而无需人工标注的问题-答案对。研究者们构建了由同一图像的两个增强视图和一个相似但不同的图像组成的图像三元组。在训练过程中，模型被要求生成推理过程，以比较这些图像（即判断相同或不同）。实验表明，尽管模型仅在视觉比较任务上训练，但其学习到的推理能力能够有效地推广到各种问题上。'}}}, {'id': 'https://huggingface.co/papers/2506.22432', 'title': 'Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy', 'url': 'https://huggingface.co/papers/2506.22432', 'abstract': 'A novel framework integrates 3D proxy meshes and a decoupled video diffusion model to achieve precise and consistent video editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/', 'score': 9, 'issue_id': 4561, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '99840d39a4880400', 'authors': ['Yuhao Liu', 'Tengfei Wang', 'Fang Liu', 'Zhenwei Wang', 'Rynson W. H. Lau'], 'affiliations': ['City University of Hong Kong, Hong Kong SAR, China', 'Tencent, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.22432.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': '3D-прокси для точного и согласованного редактирования видео', 'desc': 'Статья представляет новую систему под названием Shape-for-Motion для точного и согласованного редактирования видео. Она использует 3D-прокси-сетки и раздельную модель видеодиффузии для достижения более точного контроля над редактированием. Система позволяет пользователям редактировать 3D-модель одного кадра, после чего изменения автоматически распространяются на остальные кадры. Shape-for-Motion поддерживает различные манипуляции, включая редактирование позы, вращение, масштабирование, перемещение, изменение текстуры и композицию объектов.'}, 'en': {'title': 'Shape-for-Motion: Precise Video Editing with 3D Proxies', 'desc': "This paper introduces Shape-for-Motion, a new framework that enhances video editing by using 3D proxy meshes. It allows users to edit a 3D representation of an object in a video, ensuring that changes are consistent across all frames. The framework employs a Dual-Propagation Strategy, which simplifies the editing process by automatically applying edits made to one frame's mesh to the meshes of other frames. This method supports various manipulations like pose editing and texture modification, leading to high-quality and controllable video outputs."}, 'zh': {'title': '精确一致的视频编辑新框架', 'desc': '本论文提出了一种新颖的框架，结合了3D代理网格和解耦视频扩散模型，以实现精确且一致的视频编辑。该框架通过将输入视频中的目标对象转换为时间一致的3D网格，使得用户可以直接在代理上进行编辑，并将编辑结果推断回视频帧。为了简化编辑过程，我们设计了一种新颖的双传播策略，允许用户在单帧的3D网格上进行编辑，并自动将这些编辑传播到其他帧的3D网格上。我们的框架支持多种精确且物理一致的操作，包括姿态编辑、旋转、缩放、平移、纹理修改和对象组合，标志着高质量、可控视频编辑工作流程的重要一步。'}}}, {'id': 'https://huggingface.co/papers/2506.22419', 'title': 'The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements', 'url': 'https://huggingface.co/papers/2506.22419', 'abstract': "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.", 'score': 8, 'issue_id': 4553, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '179a2fbf84ed5e98', 'authors': ['Bingchen Zhao', 'Despoina Magka', 'Minqi Jiang', 'Xian Li', 'Roberta Raileanu', 'Tatiana Shavrina', 'Jean-Christophe Gagnon-Audet', 'Kelvin Niu', 'Shagun Sodhani', 'Michael Shvartsman', 'Andrei Lupu', 'Alisia Lupidi', 'Edan Toledo', 'Karen Hambardzumyan', 'Martin Josifoski', 'Thomas Foster', 'Lucia Cipolina-Kun', 'Abhishek Charnalia', 'Derek Dunfield', 'Alexander H. Miller', 'Oisin Mac Aodha', 'Jakob Foerster', 'Yoram Bachrach'], 'affiliations': ['Meta', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.22419.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#science', '#agents', '#training', '#benchmark'], 'emoji': '🏃\u200d♂️', 'ru': {'title': 'ИИ пока не готов к научному спидраннингу', 'desc': 'Это исследование представляет Автоматизированный бенчмарк спидраннинга ЯБМ для оценки способности ИИ-агентов воспроизводить научные результаты. Бенчмарк основан на задачах спидраннинга NanoGPT, где агенты должны улучшить скорость обучения модели GPT-2. Результаты показывают, что даже современные языковые модели с рассуждениями испытывают трудности с реализацией известных улучшений. Это указывает на то, что способность ИИ автоматизировать воспроизведение научных результатов все еще ограничена.'}, 'en': {'title': 'Benchmarking LLMs: Can They Reproduce Scientific Results?', 'desc': "The paper introduces the Automated LLM Speedrunning Benchmark, which assesses the ability of large language models (LLMs) to reproduce scientific results. It utilizes tasks from the NanoGPT speedrun competition, where AI agents attempt to train a GPT-2 model as quickly as possible. Despite advancements in reasoning capabilities, recent LLMs struggle to replicate known improvements in the benchmark, even with detailed hints provided. This benchmark serves as a straightforward measure of an LLM's capacity for automating scientific reproduction, an essential skill for future autonomous research agents."}, 'zh': {'title': '评估LLM重现科学成果的自动化基准', 'desc': '本文介绍了一种自动化的LLM速度测试基准，旨在评估人工智能代理在科学研究中重现结果的能力。该基准利用NanoGPT速度测试任务，提供19个任务，帮助代理在最短时间内训练GPT-2模型。尽管提供了详细的提示，最新的推理LLM仍然难以重新实现已知的改进。这一基准为评估LLM在科学重现中的自动化能力提供了简单而有效的测量方法。'}}}, {'id': 'https://huggingface.co/papers/2506.21876', 'title': 'Do Vision-Language Models Have Internal World Models? Towards an Atomic\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.21876', 'abstract': "A benchmark framework evaluates the world modeling capabilities of Vision-Language Models, highlighting their limitations in perception and prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.", 'score': 8, 'issue_id': 4565, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': 'db21a8520a375546', 'authors': ['Qiyue Gao', 'Xinyu Pi', 'Kevin Liu', 'Junrong Chen', 'Ruolan Yang', 'Xinqi Huang', 'Xinyu Fang', 'Lu Sun', 'Gautham Kishore', 'Bo Ai', 'Stone Tao', 'Mengyang Liu', 'Jiaxi Yang', 'Chao-Jung Lai', 'Chuanyang Jin', 'Jiannan Xiang', 'Benhao Huang', 'Zeming Chen', 'David Danks', 'Hao Su', 'Tianmin Shu', 'Ziqiao Ma', 'Lianhui Qin', 'Zhiting Hu'], 'affiliations': ['Maitrix.org', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.21876.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Ограниченность VLM в моделировании мира', 'desc': 'Предложена система оценки способностей моделей компьютерного зрения и обработки естественного языка (VLM) к моделированию мира. Фреймворк включает оценку восприятия (визуального, пространственного, временного, количественного и движения) и прогнозирования (механистическое моделирование, транзитивный и композиционный вывод). Создан масштабный бенчмарк WM-ABench с 23 параметрами оценки в 6 симулированных средах. Тестирование 15 современных VLM выявило серьезные ограничения в базовых способностях моделирования мира, например, в различении траекторий движения.'}, 'en': {'title': 'Evaluating Vision-Language Models: Uncovering Limitations in World Modeling', 'desc': "This paper presents a benchmark framework to evaluate the world modeling capabilities of Vision-Language Models (VLMs). It identifies significant limitations in these models' abilities to perceive and predict various aspects of the world, such as motion and spatial relationships. The authors introduce WM-ABench, a comprehensive evaluation tool that tests VLMs across multiple dimensions in controlled environments. The findings reveal that current VLMs struggle with basic world modeling tasks, often performing poorly compared to human-level understanding."}, 'zh': {'title': '评估视觉-语言模型的世界建模能力', 'desc': '本文提出了一个基准框架，用于评估视觉-语言模型（VLMs）在世界建模能力方面的表现，揭示了它们在感知和预测方面的局限性。内部世界模型使智能体能够理解世界状态并预测变化，是高级推理的基础。我们设计了一个两阶段的评估框架，分别评估感知和预测能力，以系统性地评估VLMs的基本世界建模能力。通过对15个最新的商业和开源VLMs进行660次实验，我们发现这些模型在基本的世界建模能力上存在显著的不足。'}}}, {'id': 'https://huggingface.co/papers/2506.21458', 'title': 'Spatial Mental Modeling from Limited Views', 'url': 'https://huggingface.co/papers/2506.21458', 'abstract': 'A new benchmark, MindCube, shows that VLMs can improve their understanding of unseen spaces by forming internal spatial representations and reasoning over them.  \t\t\t\t\tAI-generated summary \t\t\t\t Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new MindCube benchmark with 21,154 questions across 3,268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using MindCube, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for "what-if" movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from a synergistic approach, "map-then-reason", that jointly trains the model to first generate a cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space.', 'score': 5, 'issue_id': 4563, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '2d558cf4535a9561', 'authors': ['Baiqiao Yin', 'Qineng Wang', 'Pingyue Zhang', 'Jianshu Zhang', 'Kangrui Wang', 'Zihan Wang', 'Jieyu Zhang', 'Keshigeyan Chandrasegaran', 'Han Liu', 'Ranjay Krishna', 'Saining Xie', 'Manling Li', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['New York University', 'Northwestern University', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.21458.jpg', 'data': {'categories': ['#games', '#rl', '#cv', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Пространственное мышление для ИИ: от карты к рассуждению', 'desc': "Статья представляет новый бенчмарк MindCube, который оценивает способность моделей компьютерного зрения и обработки естественного языка (VLM) формировать внутренние пространственные представления и рассуждать о невидимых пространствах. Авторы предлагают подход 'map-then-reason', который обучает модель сначала генерировать когнитивную карту, а затем рассуждать на ее основе. Этот метод значительно улучшил точность с 37.8% до 60.8%, а добавление обучения с подкреплением повысило производительность до 70.7%. Исследование подчеркивает важность формирования пространственных ментальных моделей для улучшения понимания ненаблюдаемого пространства в системах искусственного интеллекта."}, 'en': {'title': 'Enhancing VLMs with Spatial Mental Models for Better Scene Understanding', 'desc': "The paper introduces MindCube, a benchmark designed to evaluate how well Vision Language Models (VLMs) can create internal spatial representations to understand unseen environments. It highlights that current VLMs struggle with this task, often performing at near-random levels when faced with spatial reasoning questions. The authors propose a 'map-then-reason' approach, where models first generate cognitive maps of the space and then use these maps for reasoning, leading to significant performance improvements. By incorporating reinforcement learning, the models achieved even higher accuracy, demonstrating that structured spatial representations enhance the understanding of complex spatial scenarios."}, 'zh': {'title': '构建空间心理模型，提升视觉语言模型理解力', 'desc': '本论文介绍了一个新的基准测试MindCube，旨在评估视觉语言模型（VLMs）在理解未见空间方面的能力。研究发现，现有的VLMs在处理空间推理时表现接近随机，显示出其在构建内部空间表征方面的不足。通过MindCube，我们提出了三种方法来帮助VLMs更好地近似空间心理模型，其中“先构图再推理”的方法显著提高了模型的准确性。最终，通过结合强化学习，模型的性能从37.8%提升至70.7%，证明了构建和利用内部结构化空间表征的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.21355', 'title': 'SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning', 'url': 'https://huggingface.co/papers/2506.21355', 'abstract': 'Current multimodal large language models show moderate to poor performance in multimodal in-context learning for medical tasks, with sensitivity to example relevance and ordering.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, example ordering exhibits a recency bias, i.e., placing the most relevant example last can lead to substantial performance improvements by up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context.', 'score': 5, 'issue_id': 4561, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': '99f9cb46fa313c8b', 'authors': ['Melanie Rieff', 'Maya Varma', 'Ossian Rabow', 'Subathra Adithan', 'Julie Kim', 'Ken Chang', 'Hannah Lee', 'Nidhi Rohatgi', 'Christian Bluethgen', 'Mohamed S. Muneer', 'Jean-Benoit Delbrouck', 'Michael Moor'], 'affiliations': ['ETH Zurich', 'HOPPR', 'Jawaharlal Institute of Postgraduate Medical Education and Research', 'Lund University', 'Stanford University', 'UCSF', 'University Hospital Zurich', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2506.21355.jpg', 'data': {'categories': ['#games', '#healthcare', '#science', '#multimodal', '#interpretability', '#reasoning', '#benchmark'], 'emoji': '🩺', 'ru': {'title': 'Ограниченная способность языковых моделей к контекстному обучению в медицине', 'desc': 'Исследование показывает, что современные мультимодальные большие языковые модели (MLLM) демонстрируют умеренную или низкую способность к обучению медицинским задачам в контексте. Авторы представили новый бенчмарк SMMILE для оценки мультимодального обучения в контексте в медицинской сфере. Эксперименты выявили чувствительность моделей к релевантности и порядку примеров в контексте. Результаты указывают на критические ограничения MLLM при изучении мультимодальных медицинских задач из контекста.'}, 'en': {'title': 'Unlocking Multimodal Learning in Medicine: The SMMILE Challenge', 'desc': "This paper discusses the limitations of current multimodal large language models (MLLMs) in performing in-context learning (ICL) for medical tasks. The authors introduce SMMILE, a benchmark created with input from medical experts, which includes a variety of multimodal queries and examples to assess MLLMs' capabilities. The evaluation reveals that most models struggle with ICL, showing only slight improvements over zero-shot learning and being significantly affected by irrelevant examples and the order of presented information. The study emphasizes the need for better understanding and enhancement of MLLMs in the context of medical applications, particularly in how they learn from multimodal data."}, 'zh': {'title': '揭示多模态学习在医学中的局限性', 'desc': '当前的多模态大型语言模型在医学任务的多模态上下文学习中表现一般，且对示例的相关性和顺序敏感。尽管多模态上下文学习在医学领域具有重要潜力，但仍未得到充分探索。我们提出了SMMILE，这是首个由专家驱动的医学任务多模态上下文学习基准，涵盖了多种医学专业和成像方式。评估结果显示，大多数模型在医学任务的多模态上下文学习能力上表现不佳，且示例的顺序和相关性对性能影响显著。'}}}, {'id': 'https://huggingface.co/papers/2506.17859', 'title': 'In-Context Learning Strategies Emerge Rationally', 'url': 'https://huggingface.co/papers/2506.17859', 'abstract': "A hierarchical Bayesian framework explains in-context learning behavior by modeling it as a tradeoff between strategy loss and complexity, offering both explanatory power and predictive insights.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, where the prior matches the underlying task distribution. Adopting the normative lens of rational analysis, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next-token predictions throughout training -- without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transitioning from generalization to memorization as task diversity increases. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.", 'score': 5, 'issue_id': 4560, 'pub_date': '2025-06-21', 'pub_date_card': {'ru': '21 июня', 'en': 'June 21', 'zh': '6月21日'}, 'hash': '919b375c089d97fa', 'authors': ['Daniel Wurgaft', 'Ekdeep Singh Lubana', 'Core Francisco Park', 'Hidenori Tanaka', 'Gautam Reddy', 'Noah D. Goodman'], 'affiliations': ['CBS-NTT Program in Physics of Intelligence, Harvard University', 'Department of Computer Science, Stanford University', 'Department of Psychology, Stanford University', 'Joseph Henry Laboratories of Physics, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2506.17859.jpg', 'data': {'categories': ['#training', '#interpretability', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Байесовский взгляд на обучение в контексте: баланс между стратегией и сложностью', 'desc': 'Статья представляет иерархическую байесовскую модель для объяснения обучения в контексте (ICL) в нейронных сетях. Модель рассматривает ICL как компромисс между потерями стратегии и её сложностью. Авторы показывают, что их подход хорошо предсказывает поведение трансформеров на разных этапах обучения. Модель объясняет известные феномены ICL и даёт новые прогнозы, например о нелинейном переходе от обобщения к запоминанию при увеличении разнообразия задач.'}, 'en': {'title': 'Balancing Strategy Loss and Complexity in In-Context Learning', 'desc': 'This paper presents a hierarchical Bayesian framework to understand in-context learning (ICL) in machine learning models. It models ICL as a balance between the loss incurred by different strategies and their complexity. The authors propose that models learn various strategies based on their training on a mixture of tasks, using predictors that either memorize or generalize from the data. By analyzing these strategies through a Bayesian lens, the framework provides insights into how models adapt their learning behavior based on computational constraints and task diversity.'}, 'zh': {'title': '策略损失与复杂性的权衡解释上下文学习', 'desc': '这篇论文提出了一个层次贝叶斯框架，用于解释上下文学习行为，认为这是策略损失与复杂性之间的权衡。研究表明，当模型学习多种任务时，可以通过贝叶斯预测器来捕捉其学习策略，包括记忆型预测器和泛化型预测器。通过理性分析的视角，论文展示了如何在不依赖模型权重的情况下，几乎完美地预测Transformer的下一个标记。该框架强调了在候选策略中，模型对策略的偏好不仅取决于其对数据的解释能力，还与策略的复杂性有关。'}}}, {'id': 'https://huggingface.co/papers/2506.21594', 'title': 'Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training', 'url': 'https://huggingface.co/papers/2506.21594', 'abstract': 'Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Gazal-R1, a 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Built upon Qwen3 32B, our model demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains. We developed a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset of 107,033 synthetic medical reasoning examples that teaches structured clinical thinking, enhanced by advanced parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation (DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system that refines accuracy, format adherence, and reasoning quality. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12x larger. Beyond its strong empirical results, this work provides detailed insights into the challenges of training reasoning-capable models in specialized domains, including issues with reward hacking, training instability, and the fundamental tension between factual recall and detailed reasoning. Our methodology offers a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability.', 'score': 5, 'issue_id': 4555, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '02b5127629bc134b', 'authors': ['Ahmed M. Adly', 'Mostafa Samy', 'Amr Fawzy'], 'affiliations': ['TachyHealth, Riyadh 13316, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.21594.jpg', 'data': {'categories': ['#healthcare', '#rl', '#optimization', '#survey', '#training', '#interpretability', '#reasoning'], 'emoji': '🩺', 'ru': {'title': 'Прорыв в ИИ для медицины: Gazal-R1 - точность, эффективность, объяснимость', 'desc': 'Gazal-R1 - это языковая модель с 32 миллиардами параметров, достигающая наилучших результатов в медицинском рассуждении. Модель использует двухэтапный процесс обучения, включающий контролируемую тонкую настройку на синтетических медицинских примерах и обучение с подкреплением. Применяются передовые методы эффективной настройки параметров, такие как DoRA и rsLoRA. Gazal-R1 превосходит гораздо более крупные модели на медицинских тестах, обеспечивая при этом прозрачные пошаговые объяснения клинических решений.'}, 'en': {'title': 'Revolutionizing Medical Reasoning with Gazal-R1', 'desc': 'Gazal-R1 is a 32-billion-parameter language model designed for medical reasoning, achieving top performance through innovative training methods. It utilizes a two-stage training pipeline that includes supervised fine-tuning on a large dataset of synthetic medical examples and reinforcement learning with a multi-component reward system. This model demonstrates that mid-sized models can outperform larger ones in specialized tasks by employing advanced techniques like Weight-Decomposed Low-Rank Adaptation. Gazal-R1 not only excels in accuracy but also provides clear explanations for its clinical decisions, addressing challenges in training reasoning-capable models.'}, 'zh': {'title': 'Gazal-R1：医学推理的新标杆', 'desc': 'Gazal-R1是一种拥有320亿参数的语言模型，在医学推理方面表现出色。它通过战略性训练，包括先进的参数高效技术和强化学习，提供临床决策的详细解释。该模型采用了两阶段的训练流程，首先在精心策划的107,033个合成医学推理示例上进行监督微调，然后使用群体相对策略优化进行强化学习。Gazal-R1在医学基准测试中取得了优异的成绩，展示了中型模型在专业领域超越更大模型的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.19741', 'title': 'Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls', 'url': 'https://huggingface.co/papers/2506.19741', 'abstract': "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT", 'score': 4, 'issue_id': 4548, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '288a2c7ef1ba6865', 'authors': ['Yihong Luo', 'Shuchen Xue', 'Tianyang Hu', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'NUS', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.19741.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization'], 'emoji': '🎛️', 'ru': {'title': 'Эффективная адаптация генеративных моделей без переобучения', 'desc': 'Статья представляет новый метод Noise Consistency Training (NCT) для интеграции новых сигналов управления в предобученные одношаговые генераторы без необходимости переобучения. NCT использует адаптерный модуль и функцию потерь согласованности шума в пространстве шума генератора. Этот подход позволяет эффективно адаптировать модели к новым условиям, таким как структурные ограничения или семантические указания. Эксперименты показывают, что NCT превосходит существующие методы по качеству генерации и вычислительной эффективности.'}, 'en': {'title': 'Efficient Control in AI Content Generation with Noise Consistency Training', 'desc': 'This paper presents a new method called Noise Consistency Training (NCT) that enhances pre-trained one-step generators for content generation without the need for retraining. NCT efficiently integrates new control signals, such as structural or semantic guidelines, into the generator by using an adapter module and a noise consistency loss. This approach allows the generator to produce high-quality outputs while maintaining computational efficiency, outperforming traditional methods that require extensive modifications. The results show that NCT achieves superior controllable generation in a single forward pass, making it a significant advancement in the field of artificial intelligence-generated content.'}, 'zh': {'title': '噪声一致性训练：高效可控生成的新方法', 'desc': '本文提出了一种新颖的噪声一致性训练（NCT）方法，能够高效地将新的控制信号整合到预训练的一步生成器中，而无需重新训练。传统方法通常需要对基础模型进行昂贵的修改，而NCT通过引入适配模块和噪声一致性损失，在生成器的噪声空间中直接进行调整。该方法在生成质量和计算效率上超越了现有的多步和蒸馏方法，展示了其在可控生成方面的优越性。NCT的模块化设计使其在数据使用上更加高效，易于部署。'}}}, {'id': 'https://huggingface.co/papers/2506.18330', 'title': 'Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning', 'url': 'https://huggingface.co/papers/2506.18330', 'abstract': 'Confucius3-Math, a 14B parameter large language model, achieves state-of-the-art performance on mathematical reasoning tasks using reinforcement learning techniques and is optimized for education in China.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.', 'score': 4, 'issue_id': 4557, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': '7f069bbbc7caa337', 'authors': ['Lixin Wu', 'Na Cai', 'Qiao Cheng', 'Jiachen Wang', 'Yitao Duan'], 'affiliations': ['NetEase Youdao, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.18330.jpg', 'data': {'categories': ['#agi', '#open_source', '#reasoning', '#training', '#rl', '#math'], 'emoji': '🧮', 'ru': {'title': 'Confucius3-Math: Мощный ИИ для математического образования в Китае', 'desc': 'Confucius3-Math - это большая языковая модель с 14 миллиардами параметров, оптимизированная для математического обучения в Китае. Модель достигает передовых результатов в задачах математического мышления, превосходя многие более крупные модели. Она разработана с использованием методов обучения с подкреплением и включает три инновационные техники: целевую энтропийную регуляризацию, восстановление недавних примеров и взвешивание сложности для конкретной политики. Модель эффективно работает на одном потребительском GPU и открыта для использования.'}, 'en': {'title': 'Empowering Math Education with Advanced AI', 'desc': 'Confucius3-Math is a large language model with 14 billion parameters designed to excel in mathematical reasoning tasks, particularly for K-12 education in China. It utilizes reinforcement learning techniques to optimize its performance, achieving state-of-the-art results while being efficient enough to run on a single consumer-grade GPU. The model incorporates innovative methods such as Targeted Entropy Regularization and Policy-Specific Hardness Weighting to enhance training stability and data efficiency. By focusing on the national curriculum, Confucius3-Math aims to support mathematics learning for students and educators at a low cost.'}, 'zh': {'title': '强化学习助力数学教育的创新模型', 'desc': 'Confucius3-Math是一个拥有140亿参数的大型语言模型，专注于数学推理任务，采用强化学习技术进行优化。该模型在中国的教育领域表现出色，特别是针对K-12学生的数学学习。通过后期训练和创新技术，Confucius3-Math在解决主流数学问题时展现了高效性和低成本。我们分享了开发过程中的挑战和解决方案，展示了在特定领域构建强大推理模型的可行性。'}}}, {'id': 'https://huggingface.co/papers/2506.21718', 'title': 'Performance Prediction for Large Systems via Text-to-Text Regression', 'url': 'https://huggingface.co/papers/2506.21718', 'abstract': "A text-to-text regression model achieves high accuracy in predicting resource efficiency for Google's Borg system, surpassing tabular methods, and demonstrates adaptability and uncertainty quantification.  \t\t\t\t\tAI-generated summary \t\t\t\t In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes.", 'score': 2, 'issue_id': 4560, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'e8bc4fba805dea4f', 'authors': ['Yash Akhauri', 'Bryan Lewandowski', 'Cheng-Hsi Lin', 'Adrian N. Reyes', 'Grant C. Forbes', 'Arissa Wongpanich', 'Bangding Yang', 'Mohamed S. Abdelfattah', 'Sagi Perel', 'Xingyou Song'], 'affiliations': ['Cornell University', 'Google', 'North Carolina State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.21718.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#architecture', '#data'], 'emoji': '🤖', 'ru': {'title': 'Текстовая регрессия превосходит табличные методы в предсказании эффективности сложных систем', 'desc': 'Исследователи предложили использовать текстовую регрессию для предсказания эффективности ресурсов в системе планирования вычислительных кластеров Google Borg. Модель encoder-decoder с 60 миллионами параметров достигла почти идеальной корреляции 0.99 и в 100 раз меньшей среднеквадратичной ошибки по сравнению с табличными методами. Модель легко адаптируется к новым задачам, используя всего 500 примеров, и способна оценивать неопределенность. Этот подход открывает путь к созданию универсальных симуляторов реальных систем.'}, 'en': {'title': 'Revolutionizing Resource Efficiency Prediction with Text-to-Text Regression', 'desc': "This paper presents a text-to-text regression model that effectively predicts resource efficiency in Google's Borg system, outperforming traditional tabular regression methods. The model, which is a 60 million parameter encoder-decoder, achieves a high rank correlation of 0.99, indicating its accuracy in predicting outcomes. It demonstrates adaptability by requiring only 500 few-shot examples to learn new tasks and provides uncertainty quantification for complex data distributions. The research suggests that this approach could lead to the development of universal simulators for predicting real-world outcomes across various industries."}, 'zh': {'title': '文本到文本回归：提升资源效率预测的未来', 'desc': '本文提出了一种文本到文本的回归模型，用于预测谷歌Borg系统的资源效率，表现出比传统的表格回归方法更高的准确性。该模型使用了一个6000万参数的编码器-解码器架构，能够在复杂的数据环境中有效工作，尤其是在配置文件和系统日志等场景中。通过少量的示例，该模型能够快速适应新任务，并且能够量化预测的不确定性。研究结果表明，使用编码器和增加序列长度对模型性能至关重要，为现实世界结果的通用模拟器奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.22149', 'title': 'RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models', 'url': 'https://huggingface.co/papers/2506.22149', 'abstract': 'RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rise of imaging techniques such as optical coherence tomography (OCT) and advances in deep learning (DL) have enabled clinicians and researchers to streamline retinal disease staging. A popular DL approach is self-supervised learning (SSL), where models learn from vast amounts of unlabeled data, avoiding costly annotation. SSL has allowed the development of foundation models (FMs), large models that can be used for a variety of downstream tasks. However, existing FMs for OCT, trained solely on image data, lack a comprehensive and robust semantic understanding of images, as evidenced by their downstream performance (especially for complex tasks), and thus require supervised fine-tuning (which may be unfeasible) to better adapt to specific applications and populations. To address this, we propose RetFiner, an SSL vision-language refinement scheme that improves the representations of existing FMs and enables their efficient and direct adaptation to specific populations for improved downstream performance. Our method uses a diverse set of training objectives which take advantage of the rich supervisory signal found in textual data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM, showing significant improvements in linear probing performance on seven highly diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1 percentage points over their baselines, respectively. Our code and model weights are publicly available at https://github.com/ronnief1/RetFiner.', 'score': 1, 'issue_id': 4555, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '89abca6b475fc5b6', 'authors': ['Ronald Fecso', 'José Morano', 'Ursula Schmidt-Erfurth', 'Hrvoje Bogunović'], 'affiliations': ['Christian Doppler Lab for Artificial Intelligence in Retina, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria', 'Comprehensive Center for AI in Medicine, Medical University of Vienna, Austria', 'Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria', 'OPTIMA Lab, Dept. of Ophthalmology, Medical University of Vienna, Austria'], 'pdf_title_img': 'assets/pdf/title_img/2506.22149.jpg', 'data': {'categories': ['#cv', '#open_source', '#dataset', '#optimization', '#training', '#healthcare', '#data'], 'emoji': '👁️', 'ru': {'title': 'RetFiner: улучшение фундаментальных моделей ОКТ с помощью языковой информации', 'desc': 'RetFiner - это схема уточнения зрения и языка, которая улучшает самообучаемые фундаментальные модели для оптической когерентной томографии (ОКТ). Она использует текстовые данные для улучшения их производительности в задачах классификации заболеваний сетчатки. RetFiner применяет разнообразные цели обучения, использующие богатый сигнал обучения, найденный в текстовых данных. Метод был протестирован на моделях RETFound, UrFound и VisionFM, показав значительное улучшение производительности на семи разнообразных задачах классификации ОКТ.'}, 'en': {'title': 'Enhancing OCT Models with Vision-Language Refinement', 'desc': 'RetFiner is a novel vision-language refinement method designed to enhance self-supervised foundation models (FMs) for optical coherence tomography (OCT) by incorporating textual data. This approach addresses the limitations of existing FMs that rely solely on image data, which often struggle with complex retinal disease classification tasks. By utilizing a variety of training objectives that leverage rich textual supervisory signals, RetFiner improves the semantic understanding of the models. Our experiments demonstrate that RetFiner significantly boosts the performance of several retinal FMs across diverse classification tasks, achieving notable increases in accuracy.'}, 'zh': {'title': 'RetFiner：提升OCT模型性能的视觉-语言方案', 'desc': 'RetFiner是一种视觉-语言精细化方案，旨在通过利用文本数据来增强自监督基础模型在光学相干断层扫描（OCT）中的表现。该方法改善了现有模型的表示能力，使其能够更有效地适应特定人群，从而提高在视网膜疾病分类任务中的下游性能。通过使用多样化的训练目标，RetFiner充分利用了文本数据中的丰富监督信号。实验结果表明，RetFiner在多个OCT分类任务中显著提高了模型的性能，平均提升了5.8、3.9和2.1个百分点。'}}}, {'id': 'https://huggingface.co/papers/2506.21476', 'title': 'Global and Local Entailment Learning for Natural World Imagery', 'url': 'https://huggingface.co/papers/2506.21476', 'abstract': 'Radial Cross-Modal Embeddings enable explicit modeling of transitive entailment in vision-language models, leading to improved performance in hierarchical species classification and retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning the hierarchical structure of data in vision-language models is a significant challenge. Previous works have attempted to address this challenge by employing entailment learning. However, these approaches fail to model the transitive nature of entailment explicitly, which establishes the relationship between order and semantics within a representation space. In this work, we introduce Radial Cross-Modal Embeddings (RCME), a framework that enables the explicit modeling of transitivity-enforced entailment. Our proposed framework optimizes for the partial order of concepts within vision-language models. By leveraging our framework, we develop a hierarchical vision-language foundation model capable of representing the hierarchy in the Tree of Life. Our experiments on hierarchical species classification and hierarchical retrieval tasks demonstrate the enhanced performance of our models compared to the existing state-of-the-art models. Our code and models are open-sourced at https://vishu26.github.io/RCME/index.html.', 'score': 1, 'issue_id': 4560, 'pub_date': '2025-06-26', 'pub_date_card': {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'}, 'hash': 'd621e5f99b239efd', 'authors': ['Srikumar Sastry', 'Aayush Dhakal', 'Eric Xing', 'Subash Khanal', 'Nathan Jacobs'], 'affiliations': ['Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2506.21476.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#cv', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'Радиальные кросс-модальные эмбеддинги для моделирования иерархий в AI', 'desc': 'Статья представляет новый подход к моделированию иерархической структуры данных в мультимодальных моделях компьютерного зрения и обработки естественного языка. Авторы предлагают фреймворк Radial Cross-Modal Embeddings (RCME), который позволяет явно моделировать транзитивность в отношениях включения между концепциями. RCME оптимизирует частичный порядок концепций в пространстве представлений. Эксперименты показывают улучшение результатов в задачах иерархической классификации видов и иерархического поиска по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Vision-Language Models with Transitive Entailment', 'desc': 'This paper presents Radial Cross-Modal Embeddings (RCME), a new framework designed to improve vision-language models by explicitly modeling transitive entailment. Traditional methods struggle with the hierarchical structure of data, particularly in understanding the relationships between concepts. RCME addresses this by optimizing the partial order of concepts, allowing for better representation of hierarchical relationships, such as those found in the Tree of Life. The results show that models using RCME outperform existing state-of-the-art approaches in tasks like hierarchical species classification and retrieval.'}, 'zh': {'title': '显式建模传递蕴含，提升视觉-语言模型性能', 'desc': '本论文提出了一种新的框架，称为径向跨模态嵌入（RCME），用于在视觉-语言模型中显式建模传递蕴含关系。通过优化概念的部分顺序，RCME能够更好地处理数据的层次结构，特别是在物种分类和检索任务中。与之前的蕴含学习方法相比，RCME能够更准确地捕捉语义之间的关系。实验结果表明，使用RCME的模型在层次物种分类和检索任务中表现优于现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2506.19592', 'title': 'Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning', 'url': 'https://huggingface.co/papers/2506.19592', 'abstract': 'TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a multi-agent framework that integrates Large Language Models (LLMs) with symbolic planning to solve complex tasks without the need for manually defined environment models. TAPAS employs specialized LLM-based agents that collaboratively generate and adapt domain models, initial states, and goal specifications as needed using structured tool-calling mechanisms. Through this tool-based interaction, downstream agents can request modifications from upstream agents, enabling adaptation to novel attributes and constraints without manual domain redefinition. A ReAct (Reason+Act)-style execution agent, coupled with natural language plan translation, bridges the gap between dynamically generated plans and real-world robot capabilities. TAPAS demonstrates strong performance in benchmark planning domains and in the VirtualHome simulated real-world environment.', 'score': 1, 'issue_id': 4567, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '0e3b49a226f67c37', 'authors': ['Harisankar Babu', 'Philipp Schillinger', 'Tamim Asfour'], 'affiliations': ['Bosch Center for Artificial Intelligence, Renningen, Germany', 'Institute of Technology, Karlsruhe, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.19592.jpg', 'data': {'categories': ['#agi', '#multimodal', '#robotics', '#agents', '#reasoning', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Адаптивное планирование задач с помощью LLM и символических методов', 'desc': 'TAPAS - это фреймворк для многоагентных систем, объединяющий большие языковые модели (LLM) с символическим планированием для решения сложных задач. Он использует специализированных агентов на основе LLM для совместной генерации и адаптации моделей предметной области, начальных состояний и спецификаций целей. TAPAS позволяет агентам запрашивать модификации у других агентов, что обеспечивает адаптацию к новым атрибутам и ограничениям без ручного переопределения домена. Фреймворк демонстрирует высокую эффективность как в эталонных доменах планирования, так и в симулированной среде реального мира VirtualHome.'}, 'en': {'title': 'Dynamic Task Adaptation with TAPAS', 'desc': 'TAPAS is a framework that combines Large Language Models (LLMs) with symbolic planning to handle complex tasks efficiently. It allows agents to create and modify domain models, initial states, and goals dynamically, eliminating the need for pre-defined models. The system uses structured tool-calling mechanisms for agents to communicate and adapt to new requirements. TAPAS has shown impressive results in various planning scenarios and simulated real-world environments, showcasing its effectiveness in robotic applications.'}, 'zh': {'title': 'TAPAS：智能体协作，动态适应复杂任务', 'desc': 'TAPAS是一种多智能体框架，结合了大型语言模型（LLMs）和符号规划，能够动态适应和生成复杂任务的领域模型、初始状态和目标。该系统通过专门的LLM代理协作生成和调整所需的模型和目标，避免了手动定义环境模型的需求。通过工具调用机制，下游代理可以向上游代理请求修改，从而适应新的属性和约束。TAPAS在基准规划领域和VirtualHome模拟真实环境中表现出色，展示了其在实际机器人能力与动态生成计划之间的桥梁作用。'}}}, {'id': 'https://huggingface.co/papers/2506.15882', 'title': 'Fractional Reasoning via Latent Steering Vectors Improves Inference Time\n  Compute', 'url': 'https://huggingface.co/papers/2506.15882', 'abstract': 'Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.', 'score': 1, 'issue_id': 4567, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '40329d9b8146bbbc', 'authors': ['Sheng Liu', 'Tianlang Chen', 'Pan Lu', 'Haotian Ye', 'Yizheng Chen', 'Lei Xing', 'James Zou'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15882.jpg', 'data': {'categories': ['#inference', '#math', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Гибкие рассуждения для умных ИИ-моделей', 'desc': 'Fractional Reasoning - это новый подход к улучшению работы больших языковых моделей (LLM) во время вывода. Он позволяет динамически регулировать глубину рассуждений модели в зависимости от сложности задачи. Метод извлекает латентный вектор, связанный с более глубокими рассуждениями, и применяет его с настраиваемым коэффициентом масштабирования. Эксперименты показали, что Fractional Reasoning улучшает производительность на различных задачах рассуждения и для разных моделей.'}, 'en': {'title': 'Dynamic Depth for Enhanced Reasoning in Language Models', 'desc': 'Fractional Reasoning is a novel approach that allows large language models (LLMs) to adjust their reasoning depth dynamically during inference. This method recognizes that different tasks may require varying levels of reasoning complexity, rather than applying a one-size-fits-all strategy. By utilizing a latent steering vector and a tunable scaling factor, the model can enhance its output quality and correctness based on the specific demands of each input. Experiments show that this framework significantly boosts performance across multiple reasoning tasks and models, making it a versatile tool in the field of machine learning.'}, 'zh': {'title': '动态调整推理深度，提升模型表现', 'desc': 'Fractional Reasoning是一种动态调整推理深度的方法，旨在提升大型语言模型在各种任务中的表现。该方法允许在推理时连续控制推理强度，超越了固定指令提示的局限性。通过提取与更深层推理相关的潜在引导向量，并使用可调缩放因子重新应用，模型能够根据每个输入的复杂性定制推理过程。实验结果表明，Fractional Reasoning在GSM8K、MATH500和GPQA等多种推理任务中均能持续提高性能。'}}}, {'id': 'https://huggingface.co/papers/2506.22049', 'title': 'GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling', 'url': 'https://huggingface.co/papers/2506.22049', 'abstract': 'Gradient-Preserving Activation Scaling (GPAS) mitigates activation variance issues in Pre-LayerNorm Transformers and enhances training dynamics across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings.', 'score': 0, 'issue_id': 4560, 'pub_date': '2025-06-27', 'pub_date_card': {'ru': '27 июня', 'en': 'June 27', 'zh': '6月27日'}, 'hash': '89839b0e292e17a5', 'authors': ['Tianhao Chen', 'Xin Xu', 'Zijing Liu', 'Pengxiang Li', 'Xinyuan Song', 'Ajay Kumar Jaiswal', 'Fan Zhang', 'Jishan Hu', 'Yang Wang', 'Hao Chen', 'Shizhe Diao', 'Shiwei Liu', 'Yu Li', 'Yin Lu', 'Can Yang'], 'affiliations': ['Dalian University of Technology', 'Emory University', 'International Digital Economy Academy', 'NVIDIA', 'The Hong Kong University of Science and Technology', 'University of Oxford', 'University of Surrey', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.22049.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '📊', 'ru': {'title': 'GPAS: Улучшение обучения трансформеров без потери градиентов', 'desc': 'Статья представляет метод Gradient-Preserving Activation Scaling (GPAS) для улучшения обучения трансформеров с предварительной нормализацией слоев (Pre-LayerNorm). GPAS решает проблему экспоненциального роста дисперсии активаций в глубоких слоях, масштабируя промежуточные активации без изменения их градиентов. Эксперименты показывают, что GPAS улучшает производительность моделей разных размеров от 71 млн до 1 млрд параметров. Метод также демонстрирует потенциал для улучшения альтернативных архитектур, таких как Sandwich-LN и DeepNorm.'}, 'en': {'title': 'Enhancing Transformer Training with GPAS', 'desc': 'The paper introduces Gradient-Preserving Activation Scaling (GPAS), a technique designed to address activation variance issues in Pre-LayerNorm Transformers. Pre-LN Transformers, while effective for large models, experience increasing activation variance that hampers the learning ability of deeper layers. GPAS mitigates this by scaling down the activations without altering their gradients, preserving essential information and preventing gradient vanishing. The method has been tested across various model sizes and shows significant performance improvements, indicating its potential applicability to other architectures as well.'}, 'zh': {'title': '梯度保持激活缩放：提升变换器训练的关键', 'desc': '梯度保持激活缩放（GPAS）是一种新技术，旨在解决预层归一化变换器中的激活方差问题。它通过缩小中间激活值的大小，同时保持其梯度不变，从而避免了梯度消失的问题。GPAS在不同规模的模型中表现出一致的性能提升，证明了其在训练动态中的有效性。该方法不仅增强了预层归一化变换器，还在其他架构中显示出潜力，展现了其广泛的适用性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi (2)', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (9)', '#cv (10)', '#data (2)', '#dataset (5)', '#diffusion (4)', '#ethics', '#games (4)', '#graphs', '#hallucinations', '#healthcare (3)', '#inference (2)', '#interpretability (4)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual', '#multimodal (7)', '#open_source (5)', '#optimization (10)', '#plp', '#rag', '#reasoning (11)', '#rl (4)', '#rlhf (1)', '#robotics (2)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (15)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-01 03:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-01 03:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-01 03:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    