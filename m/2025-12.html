
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 41 papers. December 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">–î–µ–∫–∞–±—Ä—å 2025</span> | <span id="title-articles-count">41 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-11.html">‚¨ÖÔ∏è <span id="prev-date">11.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2026-01.html">‚û°Ô∏è <span id="next-date">01.2026</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">üìà <span id='top-day-label'>–î–µ–Ω—å</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '–î–µ–∫–∞–±—Ä—å 2025', 'en': 'December 2025', 'zh': '12Êúà2025Âπ¥'};
        let feedDateNext = {'ru': '01.2026', 'en': '01/2026', 'zh': '1Êúà2026Âπ¥'};
        let feedDatePrev = {'ru': '11.2025', 'en': '11/2025', 'zh': '11Êúà2025Âπ¥'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.09824', 'title': 'Composing Concepts from Images and Videos via Concept-prompt Binding', 'url': 'https://huggingface.co/papers/2512.09824', 'abstract': 'Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.', 'score': 15, 'issue_id': 6781, 'pub_date': '2025-12-10', 'pub_date_card': {'ru': '10 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 10', 'zh': '12Êúà10Êó•'}, 'hash': '809a434567c434bc', 'pdf_title_img': 'assets/pdf/title_img/2512.09824.jpg', 'data': {'categories': ['#cv', '#multimodal', '#architecture', '#video', '#diffusion'], 'emoji': 'üé®', 'ru': {'title': '–°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —á–µ—Ä–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–∏—Ñ—Ñ—É–∑–∏–∏', 'desc': 'Bind & Compose ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–π –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Diffusion Transformers. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç—ë–º —Å–≤—è–∑—ã–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ–∑–≤–æ–ª—è—è –≥–∏–±–∫–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –î–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫—Ä–æ—Å—Å-–∞—Ç—Ç–µ–Ω—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–≥–ª–æ—â–µ–Ω–∏—è –ª–∏—à–Ω–∏—Ö –¥–µ—Ç–∞–ª–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑–≤—è–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–≤–∏–∂–µ–Ω–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.09247', 'title': 'OmniPSD: Layered PSD Generation with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2512.09247', 'abstract': 'OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.', 'score': 3, 'issue_id': 6781, 'pub_date': '2025-12-10', 'pub_date_card': {'ru': '10 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 10', 'zh': '12Êúà10Êó•'}, 'hash': 'cf1c01fa0bd2895a', 'pdf_title_img': 'assets/pdf/title_img/2512.09247.jpg', 'data': {'categories': ['#open_source', '#cv', '#multimodal', '#architecture', '#diffusion', '#dataset'], 'emoji': 'üé®', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –¥–∏–∑–∞–π–Ω–æ–≤ —Å –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏', 'desc': 'OmniPSD ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Flux, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö PSD-—Ñ–∞–π–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º—ã–µ —Å–ª–æ–∏. –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ—Ç —Ü–µ–ª–µ–≤—ã–µ —Å–ª–æ–∏ –Ω–∞ –æ–¥–Ω–æ–º —Ö–æ–ª—Å—Ç–µ –∏ –∏–∑—É—á–∞–µ—Ç –∏—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ–∑–¥–∞–≤–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–∏. –î–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –ø–µ—Ä–µ–¥–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–ª–æ–µ–≤ –∏–∑ –ø–ª–æ—Å–∫–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å RGBA-VAE —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–∞, –ø–æ–∑–≤–æ–ª—è—è –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.09106', 'title': 'Learning Unmasking Policies for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2512.09106', 'abstract': "Reinforcement learning is used to train sampling procedures for masked discrete diffusion language models, improving token throughput and quality compared to heuristic strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.", 'score': 3, 'issue_id': 6781, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': 'bb2057653f823d81', 'pdf_title_img': 'assets/pdf/title_img/2512.09106.jpg', 'data': {'categories': ['#rl', '#training', '#architecture', '#optimization', '#transfer_learning', '#diffusion'], 'emoji': 'üéØ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ—Ä–º–∞–ª–∏–∑—É—é—Ç –∑–∞–¥–∞—á—É –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –∫–∞–∫ –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∏ –æ–±—É—á–∞—é—Ç –ª—ë–≥–∫—É—é –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ—à–µ–Ω–∏—è –æ –¥–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∏—Ö –≤ –ø–æ–ª–Ω–æ–º —Ä–µ–∂–∏–º–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è —Ö–æ—Ä–æ—à—É—é –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –û–¥–Ω–∞–∫–æ –≤—ã—è–≤–ª–µ–Ω—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.04753', 'title': 'EtCon: Edit-then-Consolidate for Reliable Knowledge Editing', 'url': 'https://huggingface.co/papers/2512.04753', 'abstract': "A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", 'score': 3, 'issue_id': 6781, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'ccb065e424ee2067', 'pdf_title_img': 'assets/pdf/title_img/2512.04753.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#alignment'], 'emoji': '‚úèÔ∏è', 'ru': {'title': '–û—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∫ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Edit-then-Consolidate –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã: –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã–µ —Ñ–∞–∫—Ç—ã —á–µ—Ä–µ–∑ —Ü–µ–ª–µ–≤—É—é –¥–æ–æ–±—É—á–∫—É —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥—Ä–µ–π—Ñ–∞ –ø–æ–ª–∏—Ç–∏–∫–∏, –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Targeted Proximal Supervised Fine-Tuning –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ Group Relative Policy Optimization –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π —Å –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.02892', 'title': 'Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules', 'url': 'https://huggingface.co/papers/2512.02892', 'abstract': "SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, Œ≥{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.", 'score': 3, 'issue_id': 6781, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '855bb2b6a35ff192', 'pdf_title_img': 'assets/pdf/title_img/2512.02892.jpg', 'data': {'categories': ['#inference', '#training', '#machine_translation', '#optimization', '#diffusion'], 'emoji': '‚ö°', 'ru': {'title': '–ë—ã—Å—Ç—Ä–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —É–≤–µ—Ä–µ–Ω–Ω—É—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É', 'desc': 'SchED ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –∏ —É—Å–∫–æ—Ä—è–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–ª–≥–æ—Ä–∏—Ç–º –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ª–æ–≥–∏—Ç-–º–∞—Ä–∂–∏–Ω—ã –ø–æ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –ø–ª–∞–≤–Ω—ã–π, –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –±—ã–ª –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –¥–≤—É—Ö —Å–µ–º–µ–π—Å—Ç–≤–∞—Ö –º–æ–¥–µ–ª–µ–π (Dream –∏ LLaDA) –Ω–∞ –¥–µ—Å—è—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –ø–æ–∫–∞–∑–∞–ª —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 3.8-4.0 —Ä–∞–∑–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ 99.8-100% –∫–∞—á–µ—Å—Ç–≤–∞. SchED –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–æ–≤–µ—Ä–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.07222', 'title': 'Pay Less Attention to Function Words for Free Robustness of Vision-Language Models', 'url': 'https://huggingface.co/papers/2512.07222', 'abstract': 'Function-word De-Attention (FDA) mitigates adversarial attacks on robust VLMs by differentially subtracting function-word cross-attention, improving robustness with minimal performance trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.', 'score': 1, 'issue_id': 6781, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': 'de0263c031804577', 'pdf_title_img': 'assets/pdf/title_img/2512.07222.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#security', '#architecture'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∑–∞—â–∏—Ç—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –º–µ—Ç–æ–¥—É Function-word De-Attention (FDA), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–º adversarial –∞—Ç–∞–∫–∞–º. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ (function words) –ø–æ–≤—ã—à–∞—é—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å VLM –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–æ–¥–∞–≤–ª—è—Ç—å –∏—Ö –≤–ª–∏—è–Ω–∏–µ –ø—É—Ç—ë–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤—ã—á–∏—Ç–∞–Ω–∏—è cross-attention —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ –æ–±—ã—á–Ω–æ–≥–æ cross-attention –≤ –≥–æ–ª–æ–≤–∞—Ö –≤–Ω–∏–º–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∞—Ç–∞–∫ (–Ω–∞ 18-90%) —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–∞–¥–µ–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∏—Å–∫–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å, –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–∂–µ –≤ zero-shot —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.04519', 'title': 'VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory', 'url': 'https://huggingface.co/papers/2512.04519', 'abstract': 'VideoSSM, a hybrid state-space memory model combining AR diffusion, achieves state-of-the-art temporal consistency and motion stability in long-video generation by coordinating short- and long-term context.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.', 'score': 1, 'issue_id': 6781, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'c841edb85058f835', 'pdf_title_img': 'assets/pdf/title_img/2512.04519.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#architecture', '#video', '#diffusion'], 'emoji': 'üé¨', 'ru': {'title': '–ì–∏–±—Ä–∏–¥–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ª—é–±–æ–π –¥–ª–∏–Ω—ã', 'desc': 'VideoSSM ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ state-space memory, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∫–∞–∫ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ü–µ–Ω—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –¥–µ—Ç–∞–ª–µ–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫, –¥—Ä–µ–π—Ñ–∞ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –≤–∏–¥–µ–æ –≤ —Ç–µ—á–µ–Ω–∏–µ –º–∏–Ω—É—Ç. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VideoSSM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.01453', 'title': 'Reinventing Clinical Dialogue: Agentic Paradigms for LLM Enabled Healthcare Communication', 'url': 'https://huggingface.co/papers/2512.01453', 'abstract': "The survey analyzes the cognitive architecture of medical AI systems, focusing on the shift from generative text prediction to agentic autonomy, and categorizes methods into four archetypes based on knowledge source and agency objective.  \t\t\t\t\tAI-generated summary \t\t\t\t Clinical dialogue represents a complex duality requiring both the empathetic fluency of natural conversation and the rigorous precision of evidence-based medicine. While Large Language Models possess unprecedented linguistic capabilities, their architectural reliance on reactive and stateless processing often favors probabilistic plausibility over factual veracity. This structural limitation has catalyzed a paradigm shift in medical AI from generative text prediction to agentic autonomy, where the model functions as a central reasoning engine capable of deliberate planning and persistent memory. Moving beyond existing reviews that primarily catalog downstream applications, this survey provides a first-principles analysis of the cognitive architecture underpinning this shift. We introduce a novel taxonomy structured along the orthogonal axes of knowledge source and agency objective to delineate the provenance of clinical knowledge against the system's operational scope. This framework facilitates a systematic analysis of the intrinsic trade-offs between creativity and reliability by categorizing methods into four archetypes: Latent Space Clinicians, Emergent Planners, Grounded Synthesizers, and Verifiable Workflow Automators. For each paradigm, we deconstruct the technical realization across the entire cognitive pipeline, encompassing strategic planning, memory management, action execution, collaboration, and evolution to reveal how distinct architectural choices balance the tension between autonomy and safety.", 'score': 0, 'issue_id': 6781, 'pub_date': '2025-12-01', 'pub_date_card': {'ru': '1 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 1', 'zh': '12Êúà1Êó•'}, 'hash': '2df8c54fe7f317e9', 'pdf_title_img': 'assets/pdf/title_img/2512.01453.jpg', 'data': {'categories': ['#healthcare', '#agents', '#survey', '#science', '#architecture', '#reasoning'], 'emoji': 'üè•', 'ru': {'title': '–û—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫ –¥—É–º–∞—é—â–µ–º—É –≤—Ä–∞—á–µ–±–Ω–æ–º—É –ø–æ–º–æ—â–Ω–∏–∫—É', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö AI —Å–∏—Å—Ç–µ–º, –æ—Ç—Å–ª–µ–∂–∏–≤–∞—è –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∫ –∞–≥–µ–Ω—Ç–Ω–æ–π –∞–≤—Ç–æ–Ω–æ–º–∏–∏. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–≤–æ–∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, —á–∞—Å—Ç–æ –æ—Ç–¥–∞—é—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞–¥ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏–∑-–∑–∞ —Å–≤–æ–µ–π —Ä–µ–∞–∫—Ç–∏–≤–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –ø–æ –¥–≤—É–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º ‚Äî –∏—Å—Ç–æ—á–Ω–∏–∫—É –∑–Ω–∞–Ω–∏–π –∏ —Ü–µ–ª—è–º –∞–≥–µ–Ω—Ç–∞ ‚Äî —Ä–∞–∑–¥–µ–ª—è—é—â–∞—è –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ —á–µ—Ç—ã—Ä–µ –∞—Ä—Ö–µ—Ç–∏–ø–∞: —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∫–ª–∏–Ω–∏—Ü–∏—Å—Ç–æ–≤, –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏, –∑–∞–∑–µ–º–ª–µ–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä—ã –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä—ã —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã –¥–µ—Ç–∞–ª—å–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞, –≤–∫–ª—é—á–∞—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é, –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –∏ —ç–≤–æ–ª—é—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2512.08765', 'title': 'Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance', 'url': 'https://huggingface.co/papers/2512.08765', 'abstract': "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", 'score': 102, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '35af41ed6b0c44fa', 'authors': ['Ruihang Chu', 'Yefei He', 'Zhekai Chen', 'Shiwei Zhang', 'Xiaogang Xu', 'Bin Xia', 'Dingdong Wang', 'Hongwei Yi', 'Xihui Liu', 'Hengshuang Zhao', 'Yu Liu', 'Yingya Zhang', 'Yujiu Yang'], 'affiliations': ['CUHK', 'HKU', 'Tongyi Lab, Alibaba Group', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08765.jpg', 'data': {'categories': ['#open_source', '#video', '#benchmark', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–¢–æ—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–µ–º –≤ –≤–∏–¥–µ–æ—Å–∏–Ω—Ç–µ–∑–µ —á–µ—Ä–µ–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Wan-Move ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º –≤ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –¥–≤–∏–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –ø–ª–æ—Ç–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ —Ç–æ—á–µ–∫ –∏ –∏—Ö –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –≥–¥–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç—Å—è –≤–¥–æ–ª—å –∫–∞–∂–¥–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–ª—É–∂–∏—Ç —É—Å–ª–æ–≤–∏–µ–º –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ image-to-video. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ MoveBench ‚Äî –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º, –∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Wan-Move –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08478', 'title': 'Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform', 'url': 'https://huggingface.co/papers/2512.08478', 'abstract': 'Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.', 'score': 69, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '5bdc819571b85c24', 'authors': ['Yuning Gong', 'Yifei Liu', 'Yifan Zhan', 'Muyao Niu', 'Xueying Li', 'Yuanjun Liao', 'Jiaming Chen', 'Yuanyuan Gao', 'Jiaqi Chen', 'Minming Chen', 'Li Zhou', 'Yuning Zhang', 'Wei Wang', 'Xiaoqing Hou', 'Huaxi Huang', 'Shixiang Tang', 'Le Ma', 'Dingwen Zhang', 'Xue Yang', 'Junchi Yan', 'Yanchi Zhang', 'Yinqiang Zheng', 'Xiao Sun', 'Zhihang Zhong'], 'affiliations': ['Northwestern Polytechnical University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Sichuan University', 'The University of Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08478.jpg', 'data': {'categories': ['#3d', '#multimodal', '#inference', '#open_source'], 'emoji': 'üé®', 'ru': {'title': '–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞', 'desc': 'Visionary ‚Äî —ç—Ç–æ –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ 3D Gaussian Splatting –∏ –º–µ—à–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —á–µ—Ä–µ–∑ WebGPU –∏ ONNX. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞, –ø–æ–∑–≤–æ–ª—è—é—â–µ–º—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä—è–º–æ –≤ –±—Ä–∞—É–∑–µ—Ä. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è MLP-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ 3DGS, 4DGS, –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –∞–≤–∞—Ç–∞—Ä—ã –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π TypeScript API –∏ –ø–ª–∞–≥–∏–Ω –¥–ª—è three.js –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –≤—Ö–æ–¥–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —É–Ω–∏—Ñ–∏—Ü–∏—Ä—É—è –≤—ã–≤–æ–¥ –∏ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –±—Ä–∞—É–∑–µ—Ä–µ.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.07951', 'title': 'Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality', 'url': 'https://huggingface.co/papers/2512.07951', 'abstract': "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", 'score': 43, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '2f09092cb73adc87', 'authors': ['Zekai Luo', 'Zongze Du', 'Zhouhang Zhu', 'Hao Zhong', 'Muzhi Zhu', 'Wen Wang', 'Yuling Xi', 'Chenchen Jing', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07951.jpg', 'data': {'categories': ['#open_source', '#video', '#dataset', '#multimodal'], 'emoji': 'üé¨', 'ru': {'title': '–í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ —Å–∏–Ω—Ç–µ–∑–µ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', 'desc': 'LivingSwap –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–º–µ–Ω—ã –ª–∏—Ü –≤ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–∞–¥—Ä—ã –∫–∞–∫ —Å–∏–≥–Ω–∞–ª—ã –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–æ –¥–ª–∏–Ω–Ω—ã–º –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Face2Face –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –ø–ª–∞–≤–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Ü–µ–ª–µ–≤—É—é –ª–∏—á–Ω–æ—Å—Ç—å —Å –≤—ã—Ä–∞–∂–µ–Ω–∏–µ–º –ª–∏—Ü–∞, –æ—Å–≤–µ—â–µ–Ω–∏–µ–º –∏ –¥–≤–∏–∂–µ–Ω–∏–µ–º –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.07802', 'title': 'OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory', 'url': 'https://huggingface.co/papers/2512.07802', 'abstract': 'OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.', 'score': 33, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '441bb46d54effdc9', 'authors': ['Zhaochong An', 'Menglin Jia', 'Haonan Qiu', 'Zijian Zhou', 'Xiaoke Huang', 'Zhiheng Liu', 'Weiming Ren', 'Kumara Kahatapitiya', 'Ding Liu', 'Sen He', 'Chenyang Zhang', 'Tao Xiang', 'Fanny Yang', 'Serge Belongie', 'Tian Xie'], 'affiliations': ['Meta AI', 'University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07802.jpg', 'data': {'categories': ['#open_source', '#training', '#video', '#synthetic', '#story_generation', '#long_context', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–≤—è–∑–Ω–æ–≥–æ –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ—Ä–∞—Å—Å–∫–∞–∑–∞', 'desc': 'OneStory ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–∫–∞–¥—Ä–æ–≤—ã—Ö –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª–∏ –≤—ã–±–æ—Ä–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∏–Ω—Ç–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ. –î–ª—è —Ç–æ—á–Ω–æ–π –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –≥–ª–æ–±–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –∏ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.06776', 'title': 'From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2512.06776', 'abstract': 'Adapting autoregressive models to block-wise diffusion enables parallel generation and retains pretrained knowledge, achieving state-of-the-art performance among 7B-class diffusion language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.', 'score': 20, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': '87da6ba7e0ddb824', 'authors': ['Yuchuan Tian', 'Yuchen Liang', 'Jiacheng Sun', 'Shuo Zhang', 'Guangwen Yang', 'Yingte Shu', 'Sibo Fang', 'Tianyu Guo', 'Kai Han', 'Chao Xu', 'Hanting Chen', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ['Huawei Technologies', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06776.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#architecture', '#small_models', '#diffusion'], 'emoji': '‚ö°', 'ru': {'title': '–û—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º—É: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –±–ª–æ—á–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –±–ª–æ—á–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π –±–ª–æ—á–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Å —Ä–∞–∑–º–µ—Ä–æ–º –±–ª–æ–∫–∞ –æ–¥–∏–Ω –∏ –≤–≤–æ–¥—è—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∫–∞—É–∑–∞–ª—å–Ω—É—é –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –ø–∞—Ä–∞–¥–∏–≥–º–∞–º–∏. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É –∞–¥–∞–ø—Ç–∞—Ü–∏–∏, –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –ø–æ—Ç–µ—Ä—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–ª–æ–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å NBDiff-7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ 7B-–∫–ª–∞—Å—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ –∑–Ω–∞–Ω–∏—è–º, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –∫–æ–¥—É.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.07843', 'title': 'ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models', 'url': 'https://huggingface.co/papers/2512.07843', 'abstract': "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.", 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 –Ω–æ—è–±—Ä—è', 'en': 'November 24', 'zh': '11Êúà24Êó•'}, 'hash': '48ba940c338c4183', 'authors': ['Long Lian', 'Sida Wang', 'Felix Juefei-Xu', 'Tsu-Jui Fu', 'Xiuyu Li', 'Adam Yala', 'Trevor Darrell', 'Alane Suhr', 'Yuandong Tian', 'Xi Victoria Lin'], 'affiliations': ['Meta Superintelligence Labs (MSL)', 'UC Berkeley', 'UCSF'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07843.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#inference', '#benchmark', '#rlhf', '#math'], 'emoji': '‚ö°', 'ru': {'title': '–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ LLM –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á', 'desc': 'ThreadWeaver ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–æ—Ç–æ–∫–∞–º –≤–º–µ—Å—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ supervised fine-tuning. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ trie-—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π. –ë–ª–∞–≥–æ–¥–∞—Ä—è reinforcement learning, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É –Ω–∞ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é, –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–æ 1.53x —Ä–∞–∑.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.06864', 'title': 'Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training', 'url': 'https://huggingface.co/papers/2512.06864', 'abstract': 'AutoQ-VIS achieves state-of-the-art results in unsupervised Video Instance Segmentation using quality-guided self-training to bridge the synthetic-to-real domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': '39be88a1bc10da6c', 'authors': ['Kaixuan Lu', 'Mehmet Onurcan Kaya', 'Dim P. Papadopoulos'], 'affiliations': ['Pioneer Centre for AI', 'Technical University of Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06864.jpg', 'data': {'categories': ['#open_source', '#training', '#video', '#synthetic', '#cv'], 'emoji': 'üé¨', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π', 'desc': 'AutoQ-VIS —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —á–µ–ª–æ–≤–µ–∫–∞. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Å–µ–≤–¥–æ-–º–µ—Ç–æ–∫ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –∏ —Ä–µ–∞–ª—å–Ω—ã–º –¥–æ–º–µ–Ω–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∑–∞–º–∫–Ω—É—Ç–æ–º —Ü–∏–∫–ª–µ: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Å–µ–≤–¥–æ-–º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É—è—Å—å –∫ —Ä–µ–∞–ª—å–Ω—ã–º –≤–∏–¥–µ–æ. –ü–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∞–º—ã—Ö –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º 52.6 AP‚ÇÖ‚ÇÄ, –ø—Ä–µ–≤—ã—Å–∏–≤ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –º–µ—Ç–æ–¥ –Ω–∞ 4.4% –±–µ–∑ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.06628', 'title': 'MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment', 'url': 'https://huggingface.co/papers/2512.06628', 'abstract': 'MIND-V generates long-horizon robotic manipulation videos by integrating semantic reasoning, domain-invariant representations, and physical plausibility through a hierarchical framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': 'b20d67c0287a6a3c', 'authors': ['Ruicheng Zhang', 'Mingyang Zhang', 'Jun Zhou', 'Zhangrui Guo', 'Xiaofan Liu', 'Zunnan Xu', 'Zhizhou Zhong', 'Puxin Yan', 'Haocheng Luo', 'Xiu Li'], 'affiliations': ['Central South University', 'China University of Geosciences', 'Hong Kong University of Science and Technology', 'Sun Yat-sen University', 'Tsinghua University', 'X Square Robot'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06628.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#multimodal', '#architecture', '#video', '#synthetic', '#robotics'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ç –º—ã—Å–ª–∏ –∫ –¥–≤–∏–∂–µ–Ω–∏—é: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π', 'desc': 'MIND-V ‚Äî —ç—Ç–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Ä–æ–±–æ—Ç–æ–º, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –Ω–µ—Ö–≤–∞—Ç–∫—É —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ vision-language –º–æ–¥–µ–ª—å, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ —É—Å–ª–æ–≤–Ω–æ–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GRPO –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –Ω–∞–≥—Ä–∞–¥–∞ Physical Foresight Coherence, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–∏—Ä—É —á–µ—Ä–µ–∑ V-JEPA –º–æ–¥–µ–ª—å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MIND-V –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ —Å–æ–∑–¥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.07921', 'title': 'DeepCode: Open Agentic Coding', 'url': 'https://huggingface.co/papers/2512.07921', 'abstract': 'DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '2209d884eee8a8b2', 'authors': ['Zongwei Li', 'Zhonghang Li', 'Zirui Guo', 'Xubin Ren', 'Chao Huang'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07921.jpg', 'data': {'categories': ['#agents', '#plp', '#optimization', '#benchmark', '#science', '#long_context', '#rag'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç –±—É–º–∞–≥–∏ –∫ –∫–æ–¥—É: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞', 'desc': 'DeepCode ‚Äî —ç—Ç–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ—Ç–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–∏: —Å–∂–∞—Ç–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å—Ö–µ–º—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –ø–∞–º—è—Ç–∏ –∫–æ–¥–∞, —É—Å–ª–æ–≤–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ retrieval-augmented generation –∏ –∑–∞–º–∫–Ω—É—Ç—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫. DeepCode –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ PaperBench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –∏ –¥–∞–∂–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —É—Ä–æ–≤–Ω—è PhD –≤ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç. –°–∏—Å—Ç–µ–º–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ production-grade –∫–æ–¥, —Å—Ä–∞–≤–Ω–∏–º—ã–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É —Å —Ä–∞–±–æ—Ç–æ–π —á–µ–ª–æ–≤–µ–∫–∞-—ç–∫—Å–ø–µ—Ä—Ç–∞.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08186', 'title': 'Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation', 'url': 'https://huggingface.co/papers/2512.08186', 'abstract': 'DualVLN integrates high-level reasoning and low-level action execution to improve vision-language navigation in dynamic environments, achieving robust real-time control and long-horizon planning.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '507c84e420909927', 'authors': ['Meng Wei', 'Chenyang Wan', 'Jiaqi Peng', 'Xiqian Yu', 'Yuqiang Yang', 'Delin Feng', 'Wenzhe Cai', 'Chenming Zhu', 'Tai Wang', 'Jiangmiao Pang', 'Xihui Liu'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08186.jpg', 'data': {'categories': ['#agents', '#reasoning', '#multimodal', '#architecture', '#interpretability', '#benchmark', '#robotics', '#diffusion'], 'emoji': 'üß≠', 'ru': {'title': '–î–≤–æ–π–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏: –º–µ–¥–ª–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –±—ã—Å—Ç—Ä–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ', 'desc': 'DualVLN –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –¥–≤—É—Ö—Å–∏—Å—Ç–µ–º–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –°–∏—Å—Ç–µ–º–∞ 2, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ —Ç–æ—á–∫–∏ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ 1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫—É—é –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –±–∞–∑–µ Diffusion Transformer –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —è–≤–Ω—ã–µ –ø–∏–∫—Å–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏ –∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç –°–∏—Å—Ç–µ–º—ã 2. –¢–∞–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö —Å –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏—è–º–∏ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤—Å–µ—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08153', 'title': 'TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models', 'url': 'https://huggingface.co/papers/2512.08153', 'abstract': 'TreeGRPO, a novel RL framework, enhances training efficiency for generative models by using a tree-structured denoising process, leading to faster training and better performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce TreeGRPO, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) High sample efficiency, achieving better performance under same training samples (2) Fine-grained credit assignment via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) Amortized computation where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves 2.4times faster training while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '38c97e4f2def5493', 'authors': ['Zheng Ding', 'Weirui Ye'], 'affiliations': ['MIT', 'UC San Diego'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08153.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#benchmark', '#alignment', '#diffusion'], 'emoji': 'üå≥', 'ru': {'title': '–î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'TreeGRPO –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—è –ø—Ä–æ—Ü–µ—Å—Å –¥–µ–Ωoisifying –∫–∞–∫ –ø–æ–∏—Å–∫ –≤ –¥–µ—Ä–µ–≤–µ —Ä–µ—à–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —à—É–º–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—è –≤—ã–±–æ—Ä–æ—á–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∞–º–æ—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TreeGRPO –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 2.4-–∫—Ä–∞—Ç–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08868', 'title': 'EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce', 'url': 'https://huggingface.co/papers/2512.08868', 'abstract': 'EcomBench is a benchmark that evaluates agent performance in real-world e-commerce environments through deep information retrieval, multi-step reasoning, and cross-source knowledge integration.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': 'ba4c5085fcb8fa21', 'authors': ['Rui Min', 'Zile Qiao', 'Ze Xu', 'Jiawen Zhai', 'Wenyu Gao', 'Xuanzhong Chen', 'Haozhen Sun', 'Zhen Zhang', 'Xinyu Wang', 'Hong Zhou', 'Wenbiao Yin', 'Xuan Zhou', 'Yong Jiang', 'Haicheng Liu', 'Liang Ding', 'Ling Zou', 'Yi R.', 'Fung', 'Yalong Li', 'Pengjun Xie'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08868.jpg', 'data': {'categories': ['#reasoning', '#agents', '#survey', '#benchmark', '#dataset'], 'emoji': 'üõí', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ', 'desc': 'EcomBench ‚Äî —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –≤–µ–¥—É—â–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –∏ –≤–∫–ª—é—á–∞–µ—Ç –∑–∞–¥–∞—á–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –û—Ü–µ–Ω–∫–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤: –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –î–∞—Ç–∞—Å–µ—Ç –±—ã–ª —Ç—â–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–≥–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π AI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08309', 'title': 'Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation', 'url': 'https://huggingface.co/papers/2512.08309', 'abstract': 'Terrain Diffusion uses diffusion models and a novel algorithm called InfiniteDiffusion to generate realistic, seamless, and boundless procedural worlds with constant-time random access.  \t\t\t\t\tAI-generated summary \t\t\t\t For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '19d0c940763ad27f', 'authors': ['Alexander Goslin'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08309.jpg', 'data': {'categories': ['#3d', '#diffusion', '#training', '#open_source'], 'emoji': 'üåç', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –º–∏—Ä–æ–≤', 'desc': 'Terrain Diffusion –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–º—É —à—É–º—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, —à—É–º—É –ü–µ—Ä–ª–∏–Ω–∞), –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –ª–∞–Ω–¥—à–∞—Ñ—Ç–æ–≤. –ö–ª—é—á–µ–≤—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º —è–≤–ª—è–µ—Ç—Å—è InfiniteDiffusion, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–µ—Å—à–æ–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –¥–æ—Å—Ç—É–ø–∞ –∫ –ª—é–±–æ–π —á–∞—Å—Ç–∏ –º–∏—Ä–∞. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–ª–∞–Ω–µ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏, –ø—Ä–∏ —ç—Ç–æ–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –õ–∞–ø–ª–∞—Å–∏–∞–Ω–∞ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–∞—Å—à—Ç–∞–±–∞—Ö –≤—Å–µ–π –ø–ª–∞–Ω–µ—Ç—ã. –§—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –∏ –º–µ—Ç–æ–¥—ã –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ª—ã–µ –ø–ª–∞–Ω–µ—Ç—ã –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –ø–∞–º—è—Ç–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.06531', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'url': 'https://huggingface.co/papers/2512.06531', 'abstract': 'Two novel deep learning architectures, SAETCN and SAS-Net, achieve high accuracy in classifying and segmenting brain tumors from MRI scans.  \t\t\t\t\tAI-generated summary \t\t\t\t Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 6', 'zh': '12Êúà6Êó•'}, 'hash': '8d34cff6bbc1e65a', 'authors': ['Sayan Das', 'Arghadip Biswas'], 'affiliations': ['IIIT Delhi', 'Jadavpur University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06531.jpg', 'data': {'categories': ['#science', '#healthcare', '#cv', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ—É–≤–∞–∂–∞—é—â–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–ø—É—Ö–æ–ª–µ–π –º–æ–∑–≥–∞', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –¥–≤–µ –Ω–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–ø—É—Ö–æ–ª–µ–π –º–æ–∑–≥–∞ –Ω–∞ –ú–†–¢-—Å–Ω–∏–º–∫–∞—Ö. SAETCN –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ—É–≤–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç—Ä—ë—Ö —Ç–∏–ø–æ–≤ –æ–ø—É—Ö–æ–ª–µ–π (–≥–ª–∏–æ–º–∞, –º–µ–Ω–∏–Ω–≥–∏–æ–º–∞, –≥–∏–ø–æ—Ñ–∏–∑–∞—Ä–Ω–∞—è) —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é 99.38%, –∞ SAS-Net –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –ø–∏–∫—Å–µ–ª–µ–π 99.23%. –≠—Ç–∏ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–ø–æ–ª–Ω–æ–π –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è—é—Ç –ø—Ä–æ—Ü–µ—Å—Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–¥–∏–æ–ª–æ–≥–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å –º–µ—Ö–∞–Ω–∏–∑–º–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08730', 'title': 'SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images', 'url': 'https://huggingface.co/papers/2512.08730', 'abstract': "A preliminary exploration of using SAM 3 for remote sensing open-vocabulary semantic segmentation demonstrates promising results through a mask fusion strategy and presence score filtering.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': 'd06af54aaee7980f', 'authors': ['Kaiyu Li', 'Shengqi Zhang', 'Yupeng Deng', 'Zhi Wang', 'Deyu Meng', 'Xiangyong Cao'], 'affiliations': ['Chinese Academy of Sciences', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08730.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#cv', '#science'], 'emoji': 'üõ∞Ô∏è', 'ru': {'title': 'SAM 3 –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–µ–º–ª–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Segment Anything Model 3 (SAM 3) –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä—ë–º –≤ –∑–∞–¥–∞—á–∞—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–ª–∏—è–Ω–∏—è –º–∞—Å–æ–∫, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—Ö–æ–¥—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–Ω–æ–≥–æ –≥–æ–ª–æ–≤–æ–∫ SAM 3, —á—Ç–æ–±—ã –ª—É—á—à–µ –æ—Ö–≤–∞—Ç–∏—Ç—å –∑–µ–º–µ–ª—å–Ω—ã–µ –ø–æ–∫—Ä—ã—Ç–∏—è. –î–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –æ—Ü–µ–Ω–∫–∞–º –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —Å—Ü–µ–Ω–µ, —É—á–∏—Ç—ã–≤–∞—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –º–µ–ª–∫–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª SAM 3 –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.08923', 'title': 'Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs', 'url': 'https://huggingface.co/papers/2512.08923', 'abstract': 'Two new benchmarks assess cross-modal inconsistency in multimodal large language models, showing significant variability and impact of visual characteristics on performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-12-09', 'pub_date_card': {'ru': '9 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 9', 'zh': '12Êúà9Êó•'}, 'hash': '10f41aa1ced2ff48', 'authors': ['Angela van Sprang', 'Laurens Samson', 'Ana Lucic', 'Erman Acar', 'Sennay Ghebreab', 'Yuki M. Asano'], 'affiliations': ['City of Amsterdam', 'University of Amsterdam', 'University of Technology Nuremberg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.08923.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#interpretability'], 'emoji': 'üîÄ', 'ru': {'title': '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –º–æ–¥–∞–ª—å–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ REST –∏ REST+ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫—Ä–æ—Å—Å–º–æ–¥–∞–ª—å–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ MLLMs –Ω–µ –º–æ–≥—É—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ö–æ—Ä–æ—à–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ 15 –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —Å—Ç–µ–ø–µ–Ω–∏ –º–æ–¥–∞–ª—å–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞, —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ü–µ–Ω–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –º–æ–¥–∞–ª—å–Ω—ã–º –∑–∞–∑–æ—Ä–æ–º –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, —á—Ç–æ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∫—Ä–æ—Å—Å–º–æ–¥–∞–ª—å–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏.'}, 'en': {'title': '', 'desc': ''}, 'zh': {'title': '', 'desc': ''}}}, {'id': 'https://huggingface.co/papers/2512.07461', 'title': 'Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.07461', 'abstract': "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", 'score': 63, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '338d051f90e86520', 'authors': ['Tong Wu', 'Yang Liu', 'Jun Bai', 'Zixia Jia', 'Shuyi Zhang', 'Ziyong Lin', 'Yanting Wang', 'Song-Chun Zhu', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence (BIGAI)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07461.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#benchmark', '#small_models'], 'emoji': '‚ö°', 'ru': {'title': '–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è', 'desc': 'NPR ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∞–º–æ–¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –æ—Ç –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∫ —Å—Ç—Ä–æ–≥–∏–º —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º, –∏ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º Parallel-Aware Policy Optimization –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫ –≤–µ—Ç–≤–ª–µ–Ω–∏—è –≤ –≥—Ä–∞—Ñ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. NPR Engine –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –∏ –ø–æ—Ç–æ–∫–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ù–∞ –≤–æ—Å—å–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ 24,5% –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –¥–æ 4,6x –ø—Ä–∏ 100% –ø–æ–¥–ª–∏–Ω–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏.'}, 'en': {'title': 'Empowering LLMs with Native Parallel Reasoning', 'desc': 'The Native Parallel Reasoner (NPR) is a framework designed to enhance Large Language Models (LLMs) by enabling them to perform parallel reasoning without the need for a teacher. It introduces a self-distilled training approach that allows models to evolve their reasoning capabilities independently. NPR also features a Parallel-Aware Policy Optimization (PAPO) algorithm that helps the model learn effective strategies for decision-making through experimentation. As a result, NPR achieves significant improvements in both performance and speed, setting a new benchmark for efficient reasoning in AI.'}, 'zh': {'title': 'NPRÔºöÂºÄÂêØÁúüÊ≠£ÁöÑÂπ∂Ë°åÊé®ÁêÜÊñ∞Êó∂‰ª£', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NPRÁöÑÊó†ÊïôÂ∏àÊ°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁîüÂπ∂Ë°åÊé®ÁêÜËÉΩÂäõ„ÄÇNPRÈÄöËøáËá™ÊàëËí∏È¶èËÆ≠ÁªÉ„ÄÅÂπ∂Ë°åÊÑüÁü•Á≠ñÁï•‰ºòÂåñÂíåÂº∫Â§ßÁöÑNPRÂºïÊìéÔºåÂÆûÁé∞‰∫Ü‰ªéÈ°∫Â∫èÊ®°ÊãüÂà∞ÂéüÁîüÂπ∂Ë°åËÆ§Áü•ÁöÑËΩ¨Âèò„ÄÇËØ•Ê°ÜÊû∂Âú®ÂÖ´‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫È´òËææ24.5%ÁöÑÊÄßËÉΩÊèêÂçáÂíå4.6ÂÄçÁöÑÊé®ÁêÜÈÄüÂ∫¶Âä†Âø´„ÄÇ‰∏é‰ª•ÂæÄÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåNPRÂÆûÁé∞‰∫Ü100%ÁöÑÁúüÊ≠£Âπ∂Ë°åÊâßË°åÔºåÊ†ëÁ´ã‰∫ÜËá™ÊàëËøõÂåñ„ÄÅÈ´òÊïà‰∏îÂèØÊâ©Â±ïÁöÑÊô∫ËÉΩÊé®ÁêÜÁöÑÊñ∞Ê†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07469', 'title': 'Unified Video Editing with Temporal Reasoner', 'url': 'https://huggingface.co/papers/2512.07469', 'abstract': 'VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.', 'score': 41, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': 'a3dca53d4b568421', 'authors': ['Xiangpeng Yang', 'Ji Xie', 'Yiyuan Yang', 'Yan Huang', 'Min Xu', 'Qiang Wu'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07469.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#architecture', '#video', '#diffusion', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–¶–µ–ø–æ—á–∫–∞ –∫–∞–¥—Ä–æ–≤: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –º–∞—Å–æ–∫ –≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–∏–¥–µ–æ', 'desc': 'VideoCoF –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Chain-of-Frames –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∫ —Ä–µ–≥–∏–æ–Ω–∞–º –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–∞—Å–æ–∫ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (–ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ–±–ª–∞—Å—Ç–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è), –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤–æ–µ –≤–∏–¥–µ–æ, —Å–ª–µ–¥—É—è –ø—Ä–æ—Ü–µ–¥—É—Ä–µ ¬´–≤–∏–¥–µ—Ç—å, —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å¬ª. –í–≤–µ–¥–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è RoPE, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –Ω–∞ –≤–∏–¥–µ–æ –¥–ª–∏–Ω–Ω–µ–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 50k –≤–∏–¥–µ–æ–ø–∞—Ä –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'VideoCoF: Precision Video Editing Through Reasoning Tokens', 'desc': 'VideoCoF is a novel approach to video editing that enhances precision and mapping of instructions to specific regions in a video. It utilizes a Chain-of-Frames method that incorporates reasoning tokens, allowing the model to predict edit regions without needing user-provided masks. This method improves the alignment of instructions to video content by enforcing a structured process of seeing, reasoning, and then editing. Additionally, the RoPE alignment strategy ensures that motion is consistent and can extend beyond the original video length, achieving state-of-the-art results with minimal data requirements.'}, 'zh': {'title': 'VideoCoFÔºöÁ≤æÂáÜËßÜÈ¢ëÁºñËæëÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'VideoCoFÊòØ‰∏ÄÁßçÈìæÂ∏ßÊñπÊ≥ïÔºåÈÄöËøá‰ΩøÁî®Êé®ÁêÜÊ†áËÆ∞Êù•ÊèêÈ´òËßÜÈ¢ëÁºñËæëÁöÑÁ≤æÁ°ÆÂ∫¶ÂíåÊåá‰ª§Âà∞Âå∫ÂüüÁöÑÊò†Â∞ÑÔºåËÄåÊó†ÈúÄÁî®Êà∑Êèê‰æõÁöÑÊé©Á†Å„ÄÇÁé∞ÊúâÁöÑËßÜÈ¢ëÁºñËæëÊñπÊ≥ïÈù¢‰∏¥ÁùÄÁ≤æÂ∫¶‰∏éÁªü‰∏ÄÊÄßÁöÑÊùÉË°°Ôºå‰∏ìÂÆ∂Ê®°Âûã‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°ÁöÑÂÖàÈ™åÁü•ËØÜÔºåËÄåÁªü‰∏ÄÁöÑÂ≠¶‰π†Ê®°ÂûãÂàôÁº∫‰πèÊòéÁ°ÆÁöÑÁ©∫Èó¥Á∫øÁ¥¢„ÄÇVideoCoFÈÄöËøáÂº∫Âà∂ËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÈ¶ñÂÖàÈ¢ÑÊµãÊé®ÁêÜÊ†áËÆ∞ÔºåÁÑ∂ÂêéÂÜçÁîüÊàêÁõÆÊ†áËßÜÈ¢ëÊ†áËÆ∞ÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçRoPEÂØπÈΩêÁ≠ñÁï•Ôºå‰ª•Á°Æ‰øùËøêÂä®ÂØπÈΩêÂπ∂ÊîØÊåÅË∂ÖÂá∫ËÆ≠ÁªÉÊó∂ÈïøÁöÑÈïøÂ∫¶Â§ñÊé®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07783', 'title': 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models', 'url': 'https://huggingface.co/papers/2512.07783', 'abstract': "A controlled experimental framework isolates and evaluates the contributions of pre-training, mid-training, and reinforcement learning in improving language model reasoning, demonstrating the necessity of each phase and the role of process-level rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", 'score': 29, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '93a20c7506c2655e', 'authors': ['Charlie Zhang', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University, Language Technologies Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07783.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#benchmark'], 'emoji': 'üß†', 'ru': {'title': '–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–∫–ª–∞–¥–∞ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –≤–∫–ª–∞–¥–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ —Å —è–≤–Ω—ã–º–∏ –∞—Ç–æ–º–∞—Ä–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ –∏ –ø–æ—à–∞–≥–æ–≤—ã–º–∏ —Ç—Ä–∞—Å—Å–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á—Ç–æ–±—ã –∏–∑–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–∞–µ—Ç –∏—Å—Ç–∏–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ ¬´–∑–∞–ø–∞—Å–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç–∏¬ª –ø–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ RL –Ω–∞—Ü–µ–ª–µ–Ω—ã –Ω–∞ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–Ω–∏–∂–∞—é—Ç –∏–≥—Ä—ã —Å —Å–∏—Å—Ç–µ–º–æ–π –∏ —É–ª—É—á—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Unlocking Language Model Reasoning: The Power of Training Phases', 'desc': 'This paper investigates how different training phases‚Äîpre-training, mid-training, and reinforcement learning (RL)‚Äîcontribute to the reasoning abilities of language models. It introduces a controlled experimental framework to isolate these contributions and evaluates models on their ability to generalize in complex reasoning tasks. The findings reveal that RL enhances capabilities only when pre-training is effective and that mid-training plays a crucial role in improving performance. Additionally, the study highlights the importance of process-level rewards in enhancing reasoning accuracy and reducing reward hacking.'}, 'zh': {'title': 'Êè≠Á§∫ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõÊèêÂçáÁöÑÂÖ≥ÈîÆÈò∂ÊÆµ', 'desc': 'Êú¨Á†îÁ©∂Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÂèóÊéßÂÆûÈ™åÊ°ÜÊû∂Ôºå‰ª•ËØÑ‰º∞È¢ÑËÆ≠ÁªÉ„ÄÅ‰∏≠ÊúüËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†Âú®ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ‰∏≠ÁöÑË¥°ÁåÆ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈ¢ÑËÆ≠ÁªÉ„ÄÅ‰∏≠ÊúüËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÂêÑÈò∂ÊÆµÈÉΩÊòØÂøÖË¶ÅÁöÑÔºå‰∏îËøáÁ®ãÁ∫ßÂ•ñÂä±Âú®ÂÖ∂‰∏≠ÂèëÊå•‰∫ÜÈáçË¶Å‰ΩúÁî®„ÄÇÈÄöËøáÂêàÊàêÊé®ÁêÜ‰ªªÂä°ÔºåÁ†îÁ©∂Âõ¢ÈòüËÉΩÂ§üÁ≥ªÁªüÂú∞ÂàÜÊûê‰∏çÂêåËÆ≠ÁªÉÈò∂ÊÆµÁöÑÂõ†ÊûúÂÖ≥Á≥ª„ÄÇÁªìÊûúÊòæÁ§∫Ôºå‰∏≠ÊúüËÆ≠ÁªÉÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÊÄßËÉΩÔºåËÄåÂº∫ÂåñÂ≠¶‰π†ÁöÑÊúâÊïàÊÄß‰æùËµñ‰∫éÈ¢ÑËÆ≠ÁªÉÁöÑÂÖÖÂàÜÊÄßÂíå‰ªªÂä°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06065', 'title': 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing', 'url': 'https://huggingface.co/papers/2512.06065', 'abstract': 'EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit', 'score': 23, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': 'fc437da58a265a3d', 'authors': ['Runjia Li', 'Moayed Haji-Ali', 'Ashkan Mirzaei', 'Chaoyang Wang', 'Arpit Sahni', 'Ivan Skorokhodov', 'Aliaksandr Siarohin', 'Tomas Jakab', 'Junlin Han', 'Sergey Tulyakov', 'Philip Torr', 'Willi Menapace'], 'affiliations': ['Rice University', 'Snap Research', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06065.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#inference', '#video', '#benchmark', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏', 'desc': 'EgoEdit ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–ª–µ–¥—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ –≤–∏–¥–µ–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –±—ã—Å—Ç—Ä–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã –∏ —á–∞—Å—Ç—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ä—É–∫ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, —á—Ç–æ —Å–æ–∑–¥–∞—ë—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—ã—á–Ω—ã–º–∏ –≤–∏–¥–µ–æ. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç EgoEditData –∏ –Ω–∞–±–æ—Ä –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ EgoEditBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä—É–∫ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –°–∏—Å—Ç–µ–º–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ –Ω–∞ –æ–¥–Ω–æ–º GPU –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Egocentric Video Editing in Real-Time', 'desc': 'EgoEdit is a novel video editing tool designed specifically for egocentric videos, which are recorded from a first-person perspective. It tackles the unique challenges of egomotion and hand-object interactions that traditional video editors struggle with. By utilizing a specially curated dataset called EgoEditData, it ensures that hand movements and interactions are accurately preserved during editing. The system operates in real-time on a single GPU, providing fast and reliable editing results that outperform existing methods in egocentric scenarios while maintaining competitive performance in general video editing tasks.'}, 'zh': {'title': 'ÂÆûÊó∂Ëá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁºñËæëÁöÑÂàõÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'EgoEdit ÊòØ‰∏Ä‰∏™ÂÆûÊó∂ÁöÑ„ÄÅÈÅµÂæ™Êåá‰ª§ÁöÑËá™Êàë‰∏≠ÂøÉËßÜÈ¢ëÁºñËæëÂô®Ôºå‰∏ìÈó®Ëß£ÂÜ≥Ëá™ÊàëËøêÂä®ÂíåÊâãÁâ©‰Ωì‰∫§‰∫íÁöÑÊåëÊàò„ÄÇÂÆÉÂú®Ëá™Êàë‰∏≠ÂøÉÁºñËæë‰ªªÂä°‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊñπÊ≥ïÔºåÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÁºñËæëÊïàÊûú„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü EgoEditData Êï∞ÊçÆÈõÜÔºå‰∏ìÊ≥®‰∫é‰∏∞ÂØåÁöÑÊâãÁâ©‰Ωì‰∫§‰∫íÔºåÂπ∂‰øùÁïôÊâãÈÉ®‰ø°ÊÅØ„ÄÇEgoEdit ËøòÊîØÊåÅÂú®Âçï‰∏™ GPU ‰∏äËøõË°åÂÆûÊó∂Êé®ÁêÜÔºåÁ°Æ‰øù‰∫Ü‰ΩéÂª∂ËøüÁöÑ‰∫§‰∫í‰ΩìÈ™å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07778', 'title': 'Distribution Matching Variational AutoEncoder', 'url': 'https://huggingface.co/papers/2512.07778', 'abstract': "DMVAE explicitly aligns the encoder's latent distribution with a reference distribution, improving modeling efficiency and image synthesis fidelity compared to conventional VAEs.  \t\t\t\t\tAI-generated summary \t\t\t\t Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", 'score': 20, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '36921e5514ed4873', 'authors': ['Sen Ye', 'Jianning Pei', 'Mengde Xu', 'Shuyang Gu', 'Chunyu Wang', 'Liwei Wang', 'Han Hu'], 'affiliations': ['Peking University', 'Tencent', 'UCAS'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07778.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#architecture', '#diffusion', '#cv'], 'emoji': 'üéØ', 'ru': {'title': '–Ø–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Distribution-Matching VAE (DMVAE) ‚Äî —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫, –∫–æ—Ç–æ—Ä–∞—è —è–≤–Ω–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö VAE, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ—è–≤–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—Ç —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≥–∞—É—Å—Å–æ–≤—ã–º –ø—Ä–∏–æ—Ä–æ–º, DMVAE –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–≤–Ω–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ self-supervised –æ–±—É—á–µ–Ω–∏—è, –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —à—É–º–∞ –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–æ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç, –∫–∞–∫–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç, —á—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ self-supervised feature, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ ImageNet –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç gFID = 3.2 –≤—Å–µ–≥–æ –∑–∞ 64 —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Aligning Latent Distributions for Better Image Synthesis', 'desc': "The paper introduces the Distribution-Matching VAE (DMVAE), which enhances the performance of visual generative models by explicitly aligning the encoder's latent distribution with a chosen reference distribution. This approach allows for greater flexibility compared to traditional VAEs, which typically rely on a fixed Gaussian prior. By using distribution matching constraints, DMVAE can adapt to various distributions, including those derived from self-supervised learning and diffusion processes. The results demonstrate that this method significantly improves image synthesis quality and modeling efficiency, achieving a notable gFID score on ImageNet with minimal training epochs."}, 'zh': {'title': 'ÂàÜÂ∏ÉÂåπÈÖçÔºåÊèêÂçáÂõæÂÉèÂêàÊàêË¥®Èáè', 'desc': 'DMVAEÔºàÂàÜÂ∏ÉÂåπÈÖçÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºâÈÄöËøáÂ∞ÜÁºñÁ†ÅÂô®ÁöÑÊΩúÂú®ÂàÜÂ∏É‰∏éÂèÇËÄÉÂàÜÂ∏ÉÊòæÂºèÂØπÈΩêÔºåÊèêÂçá‰∫ÜÂª∫Ê®°ÊïàÁéáÂíåÂõæÂÉèÂêàÊàêÁöÑ‰øùÁúüÂ∫¶„ÄÇ‰∏é‰º†ÁªüÁöÑÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâ‰∏çÂêåÔºåDMVAEÂÖÅËÆ∏‰∏é‰ªªÊÑèÂèÇËÄÉÂàÜÂ∏ÉÂØπÈΩêÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÈ´òÊñØÂÖàÈ™å„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÊàë‰ª¨ËÉΩÂ§üÁ≥ªÁªüÂú∞Á†îÁ©∂Âì™‰∫õÊΩúÂú®ÂàÜÂ∏ÉÊõ¥ÈÄÇÂêàÂª∫Ê®°ÔºåÂπ∂ÂèëÁé∞Ëá™ÁõëÁù£Â≠¶‰π†ÔºàSSLÔºâË°çÁîüÁöÑÂàÜÂ∏ÉÂú®ÈáçÂª∫‰øùÁúüÂ∫¶ÂíåÂª∫Ê®°ÊïàÁéá‰πãÈó¥ËææÂà∞‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊΩúÂú®ÂàÜÂ∏ÉÁªìÊûÑÊòØÂÆûÁé∞È´ò‰øùÁúüÂõæÂÉèÂêàÊàêÁöÑÂÖ≥ÈîÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06589', 'title': 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2512.06589', 'abstract': 'OmniSafeBench-MM is a comprehensive tool for evaluating multi-modal jailbreak attacks and defenses, covering various attack methods, defense strategies, and risk domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 6', 'zh': '12Êúà6Êó•'}, 'hash': 'ad55b75eeb1085ca', 'authors': ['Xiaojun Jia', 'Jie Liao', 'Qi Guo', 'Teng Ma', 'Simeng Qin', 'Ranjie Duan', 'Tianlin Li', 'Yihao Huang', 'Zhitao Zeng', 'Dongxian Wu', 'Yiming Li', 'Wenqi Ren', 'Xiaochun Cao', 'Yang Liu'], 'affiliations': ['Alibaba, China', 'BraneMatrix AI, China', 'ByteDance, China', 'Chongqing University, China', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Northeastern University, China', 'Sun Yat-sen University, China', 'Xian Jiaotong University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06589.jpg', 'data': {'categories': ['#security', '#open_source', '#multimodal', '#benchmark', '#alignment', '#dataset'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞—Ç–∞–∫ –∏ –∑–∞—â–∏—Ç—ã –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'OmniSafeBench-MM –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–¥ –∞—Ç–∞–∫–∞–º–∏ –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∑–∞—â–∏—Ç–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏. –ë–µ–Ω—á–º–∞—Ä–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç 13 –º–µ—Ç–æ–¥–æ–≤ –∞—Ç–∞–∫, 15 —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞—â–∏—Ç—ã –∏ –¥–∞—Ç–∞—Å–µ—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 9 –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Ä–∏—Å–∫–∞ —Å 50 –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏, –∏–∑–º–µ—Ä—è—é—â–∏–π –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ—Å—Ç—å –Ω–∞ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π —à–∫–∞–ª–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏ –∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏, –∞ —Ç–∞–∫–∂–µ —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 10 –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ 8 –∑–∞–∫—Ä—ã—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö LLM, —Å–æ–∑–¥–∞–≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Strengthening Multi-Modal Models Against Jailbreak Attacks', 'desc': 'OmniSafeBench-MM is a new tool designed to evaluate how well multi-modal large language models (MLLMs) can withstand jailbreak attacks. It includes a wide range of attack methods and defense strategies, making it a comprehensive resource for researchers. The tool assesses vulnerabilities across various risk domains and uses a detailed evaluation protocol to measure harmfulness, intent alignment, and response detail. By providing a standardized and reproducible framework, OmniSafeBench-MM aims to enhance the understanding and safety of MLLMs against potential threats.'}, 'zh': {'title': 'ÂÖ®Èù¢ËØÑ‰º∞Â§öÊ®°ÊÄÅË∂äÁã±ÊîªÂáª‰∏éÈò≤Âæ°ÁöÑÂ∑•ÂÖ∑', 'desc': 'OmniSafeBench-MM ÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ∑•ÂÖ∑ÔºåÁî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÁöÑË∂äÁã±ÊîªÂáªÂíåÈò≤Âæ°„ÄÇÂÆÉÊï¥Âêà‰∫Ü13Áßç‰ª£Ë°®ÊÄßÁöÑÊîªÂáªÊñπÊ≥ïÂíå15ÁßçÈò≤Âæ°Á≠ñÁï•ÔºåË¶ÜÁõñ‰∫Ü9‰∏™‰∏ªË¶ÅÈ£éÈô©È¢ÜÂüüÂíå50‰∏™ÁªÜÂàÜÁ±ªÂà´„ÄÇËØ•Â∑•ÂÖ∑Âª∫Á´ã‰∫Ü‰∏Ä‰∏™‰∏âÁª¥ËØÑ‰º∞ÂçèËÆÆÔºåÊµãÈáèÊúâÂÆ≥ÊÄß„ÄÅÊÑèÂõæ‰∏ÄËá¥ÊÄßÂíåÂìçÂ∫îÁªÜËäÇÊ∞¥Âπ≥Ôºå‰ª•‰æøËøõË°åÁªÜËá¥ÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁî®ÂàÜÊûê„ÄÇÈÄöËøáÁªü‰∏ÄÊï∞ÊçÆ„ÄÅÊñπÊ≥ïÂíåËØÑ‰º∞ÔºåOmniSafeBench-MM ‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07584', 'title': 'LongCat-Image Technical Report', 'url': 'https://huggingface.co/papers/2512.07584', 'abstract': 'LongCat-Image is a bilingual open-source foundation model for image generation that addresses multilingual text rendering, photorealism, and deployment efficiency through rigorous data curation, compact design, and comprehensive open-source support.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '19dd8c92e6e77b12', 'authors': ['Meituan LongCat Team', 'Hanghang Ma', 'Haoxian Tan', 'Jiale Huang', 'Junqiang Wu', 'Jun-Yan He', 'Lishuai Gao', 'Songlin Xiao', 'Xiaoming Wei', 'Xiaoqi Ma', 'Xunliang Cai', 'Yayong Guan', 'Jie Hu'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07584.jpg', 'data': {'categories': ['#data', '#open_source', '#multilingual', '#training', '#architecture', '#inference', '#small_models', '#diffusion', '#dataset', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –¥–≤—É—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏–¥–µ–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º', 'desc': 'LongCat-Image ‚Äî —ç—Ç–æ –¥–≤—É—è–∑—ã—á–Ω–∞—è (–∫–∏—Ç–∞–π—Å–∫–æ-–∞–Ω–≥–ª–∏–π—Å–∫–∞—è) –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–æ–≥—É—é –∫—É—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ reward models –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å—é —Å —è–¥—Ä–æ–º –≤—Å–µ–≥–æ –≤ 6B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ MoE –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ. –ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç–∫—Ä—ã—Ç—ã–π —ç–∫–æ—Å–∏—Å—Ç–µ–º —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–µ–π –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π.'}, 'en': {'title': 'LongCat-Image: Bridging Languages with Photorealistic Image Generation', 'desc': 'LongCat-Image is an innovative bilingual foundation model designed for image generation, focusing on both Chinese and English text rendering. It utilizes advanced data curation techniques throughout its training phases, leading to state-of-the-art performance in photorealism and text accuracy, especially for complex Chinese characters. The model is compact, with only 6 billion parameters, making it efficient for deployment while maintaining high-quality output. Additionally, it fosters community engagement by providing a comprehensive open-source ecosystem, including various model versions and training tools to support developers and researchers.'}, 'zh': {'title': 'ÈïøÁå´ÂõæÂÉèÔºöÂºÄÂàõÂèåËØ≠ÂõæÂÉèÁîüÊàêÊñ∞Ê†áÂáÜ', 'desc': 'LongCat-ImageÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂèåËØ≠Âü∫Á°ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÂõæÂÉèÁîüÊàêÔºåÊó®Âú®Ëß£ÂÜ≥Â§öËØ≠Ë®ÄÊñáÊú¨Ê∏≤Êüì„ÄÅÁúüÂÆûÊÑüÂíåÈÉ®ÁΩ≤ÊïàÁéáÁ≠âÊ†∏ÂøÉÊåëÊàò„ÄÇÈÄöËøá‰∏•Ê†ºÁöÑÊï∞ÊçÆÊï¥ÁêÜÂíåÁ¥ßÂáëÁöÑËÆæËÆ°ÔºåËØ•Ê®°ÂûãÂú®ÊñáÊú¨Ê∏≤ÊüìËÉΩÂäõÂíåÁúüÂÆûÊÑüÊñπÈù¢ËææÂà∞‰∫ÜÊñ∞ÁöÑË°å‰∏öÊ†áÂáÜÔºåÂ∞§ÂÖ∂Âú®‰∏≠ÊñáÂ≠óÁ¨¶ÁöÑÊ∏≤Êüì‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇÂÆÉÁöÑÊ†∏ÂøÉÊâ©Êï£Ê®°Âûã‰ªÖÊúâ6‰∫øÂèÇÊï∞ÔºåÊòæËëóÂ∞è‰∫éË°å‰∏öÂÜÖÂ∏∏ËßÅÁöÑ20‰∫øÂèÇÊï∞Ê®°ÂûãÔºåÁ°Æ‰øù‰∫Ü‰ΩéÊòæÂ≠ò‰ΩøÁî®ÂíåÂø´ÈÄüÊé®ÁêÜ„ÄÇLongCat-ImageËøòÂú®ÂõæÂÉèÁºñËæëÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂºÄÊ∫êÁîüÊÄÅÁ≥ªÁªüÔºåÊîØÊåÅÂºÄÂèëËÄÖÂíåÁ†îÁ©∂‰∫∫ÂëòÊé®Âä®ËßÜËßâÂÜÖÂÆπÂàõ‰ΩúÁöÑÂâçÊ≤ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.03244', 'title': 'SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.03244', 'abstract': 'A three-stage framework, SPARK, uses a generator and verifier to create synthetic training data for process reward models, enabling reference-free reinforcement learning that surpasses ground-truth methods in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.', 'score': 14, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': 'eced3f0c0df8f415', 'authors': ['Salman Rahman', 'Sruthi Gorantla', 'Arpit Gupta', 'Swastik Roy', 'Nanyun Peng', 'Yang Liu'], 'affiliations': ['Amazon AGI', 'UCLA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03244.jpg', 'data': {'categories': ['#rl', '#reasoning', '#data', '#training', '#optimization', '#benchmark', '#synthetic', '#rlhf', '#math'], 'emoji': '‚ö°', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ —ç—Ç–∞–ª–æ–Ω–æ–≤: –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –Ω–∞–∑–µ–º–Ω–æ–π –ø—Ä–∞–≤–¥—ã', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è SPARK ‚Äî —Ç—Ä—ë—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—ë—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å –ø–æ–º–æ—â—å—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –≤—ã—Ö–æ–¥ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –ù–∞ —Ç—Ä–µ—Ç—å–µ–º —ç—Ç–∞–ø–µ —ç—Ç–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø–æ–∑–≤–æ–ª—è—è –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –º–µ—Ç–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'SPARK: Revolutionizing Reinforcement Learning with Synthetic Data', 'desc': 'The paper introduces SPARK, a three-stage framework designed to enhance reinforcement learning by generating synthetic training data for process reward models (PRMs). In the first stage, a generator creates diverse solutions, while a verifier assesses these solutions using both self-consistency and meta-critique methods. The second stage utilizes the verification results to fine-tune PRMs, which then provide reward signals during training. The final stage demonstrates that this approach achieves superior performance in mathematical reasoning tasks compared to traditional ground-truth methods, enabling effective reinforcement learning without the need for expensive annotations.'}, 'zh': {'title': 'SPARKÔºöË∂ÖË∂äÁúüÂÆûÊï∞ÊçÆÁöÑÊó†ÂèÇËÄÉÂº∫ÂåñÂ≠¶‰π†', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SPARKÁöÑ‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆ‰ª•ÊîπËøõËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMÔºâ„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÁîüÊàêÂô®Ê®°Âûã‰∫ßÁîüÂ§öÊ†∑ÂåñÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÈ™åËØÅÂô®Ê®°ÂûãÈÄöËøáËá™‰∏ÄËá¥ÊÄßÂíåÂÖÉÊâπËØÑÂØπÂÖ∂ËøõË°åËØÑ‰º∞„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÂà©Áî®Ëøô‰∫õÈ™åËØÅËæìÂá∫‰Ωú‰∏∫ÂêàÊàêËÆ≠ÁªÉÊï∞ÊçÆÔºåÂæÆË∞ÉÁîüÊàêÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºå‰ª•Âú®ËÆ≠ÁªÉ‰∏≠Êèê‰æõÂ•ñÂä±‰ø°Âè∑„ÄÇÊúÄÁªàÔºåSPARKÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜË∂ÖËøáÂü∫‰∫éÁúüÂÆûÊï∞ÊçÆÁöÑÊñπÊ≥ïÁöÑË°®Áé∞ÔºåÂ±ïÁ§∫‰∫ÜÊó†ÂèÇËÄÉÂº∫ÂåñÂ≠¶‰π†ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06373', 'title': 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.06373', 'abstract': 'The VG-Refiner framework improves tool-integrated visual reasoning by introducing a two-stage mechanism to handle unreliable tool outputs and enhance accuracy in referring and grounding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 6', 'zh': '12Êúà6Êó•'}, 'hash': 'bdfd82bfcf4f8442', 'authors': ['Yuji Wang', 'Wenlong Liu', 'Jingxuan Niu', 'Haoji Zhang', 'Yansong Tang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06373.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#hallucinations', '#benchmark', '#cv'], 'emoji': 'üîÑ', 'ru': {'title': '–û—Ç –æ—à–∏–±–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∫ –≤–µ—Ä–Ω—ã–º –≤—ã–≤–æ–¥–∞–º: —É—Ç–æ—á–Ω–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ', 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ VG-Refiner, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º '–¥—É–º–∞–π-–ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏'. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º –≤ –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≥—Ä—É–Ω–¥–∏–Ω–≥–∞ –æ–±—ä–µ–∫—Ç–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É –∑–∞ —É—Ç–æ—á–Ω–µ–Ω–∏–µ (refinement reward), –æ–±—É—á–∞—è—Å—å –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –†–∞–±–æ—Ç–∞ –≤–≤–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —É—Ç–æ—á–Ω–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."}, 'en': {'title': 'Refining Visual Reasoning with VG-Refiner', 'desc': "The VG-Refiner framework enhances tool-integrated visual reasoning (TiVR) by implementing a two-stage mechanism that addresses unreliable outputs from visual tools. This framework specifically targets referring and grounding tasks, which often suffer from inaccuracies in tool predictions leading to misleading reasoning. By introducing a think-rethink process, VG-Refiner allows the model to critically evaluate tool feedback and apply corrections effectively. Additionally, it establishes new metrics for evaluating refinement capabilities, resulting in improved accuracy and correction performance while maintaining the pretrained model's general abilities."}, 'zh': {'title': 'VG-RefinerÔºöÊèêÂçáËßÜËßâÊé®ÁêÜÁöÑÂ∑•ÂÖ∑ÈõÜÊàêËÉΩÂäõ', 'desc': 'VG-RefinerÊ°ÜÊû∂ÈÄöËøáÂºïÂÖ•‰∏§Èò∂ÊÆµÊú∫Âà∂Êù•ÊîπÂñÑÂ∑•ÂÖ∑ÈõÜÊàêÁöÑËßÜËßâÊé®ÁêÜÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜ‰∏çÂèØÈù†ÁöÑÂ∑•ÂÖ∑ËæìÂá∫Êó∂„ÄÇËØ•Ê°ÜÊû∂‰∏ìÊ≥®‰∫éÊèêÈ´òÂºïÁî®ÂíåÂÆö‰Ωç‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄßÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ÂØπÈîôËØØÂ∑•ÂÖ∑ËæìÂá∫ÂèçÂ∫î‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇVG-RefinerÈÄöËøáÂàÜÊûêÂíåÂìçÂ∫îÂ∑•ÂÖ∑ÂèçÈ¶àÔºåÁªìÂêà‰øÆÊ≠£Â•ñÂä±Êú∫Âà∂ÔºåÈºìÂä±Ê®°ÂûãÊúâÊïàÁ∫†Ê≠£ÈîôËØØ„ÄÇÈÄöËøá‰ΩøÁî®Â∞ëÈáèÁâπÂÆö‰ªªÂä°Êï∞ÊçÆÔºåVG-RefinerÂú®ÂºïÁî®ÂíåÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÂíåÁ∫†Ê≠£ËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÈÄöÁî®ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07829', 'title': 'One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation', 'url': 'https://huggingface.co/papers/2512.07829', 'abstract': 'FAE, a framework using a feature auto-encoder and dual decoders, adapts pre-trained visual representations for generative models, achieving high performance in image generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '7fd0877bda27e798', 'authors': ['Yuan Gao', 'Chen Chen', 'Tianrong Chen', 'Jiatao Gu'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07829.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#cv', '#transfer_learning'], 'emoji': 'üé®', 'ru': {'title': '–ú–æ—Å—Ç –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ', 'desc': 'FAE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –¥–≤—É–º—è –¥–µcod–µ—Ä—ã. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–ª–µ–∑–Ω—ã–º–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏ –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–º–∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –¥–µ–∫–æ–¥–µ—Ä–∞: –ø–µ—Ä–≤—ã–π –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤—Ç–æ—Ä–æ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. FAE —É–Ω–∏–≤–µ—Ä—Å–∞–ª–µ–Ω –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤–∫–ª—é—á–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏, –¥–æ—Å—Ç–∏–≥–∞—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –±—ã—Å—Ç—Ä—ã–º –æ–±—É—á–µ–Ω–∏–µ–º.'}, 'en': {'title': 'FAE: Bridging Features and Generative Models for Superior Image Generation', 'desc': 'FAE (Feature Auto-Encoder) is a novel framework designed to adapt pre-trained visual representations for use in generative models, specifically targeting image generation tasks. It effectively bridges the gap between high-dimensional feature spaces and low-dimensional latent spaces, which are essential for generating high-quality images. By utilizing two separate decoders‚Äîone for reconstructing the original features and another for generating images‚ÄîFAE maintains crucial information while simplifying the adaptation process. The framework shows impressive results across various benchmarks, achieving state-of-the-art performance in both class-conditional and text-to-image generation tasks.'}, 'zh': {'title': 'FAEÔºöÈ´òÊïàÁöÑÂõæÂÉèÁîüÊàêÊ°ÜÊû∂', 'desc': 'FAEÔºàÁâπÂæÅËá™ÁºñÁ†ÅÂô®ÔºâÊòØ‰∏ÄÁßçÊ°ÜÊû∂ÔºåÂà©Áî®ÁâπÂæÅËá™ÁºñÁ†ÅÂô®ÂíåÂèåËß£Á†ÅÂô®ÔºåÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâË°®Á§∫ÈÄÇÂ∫î‰∫éÁîüÊàêÊ®°ÂûãÔºå‰ªéËÄåÂú®ÂõæÂÉèÁîüÊàê‰ªªÂä°‰∏≠ÂÆûÁé∞È´òÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂ∞ÜÈ´òÁª¥ÁâπÂæÅÊò†Â∞ÑÂà∞‰ΩéÁª¥ÊΩúÂú®Á©∫Èó¥ÔºåËß£ÂÜ≥‰∫ÜÁêÜËß£ÂØºÂêëÁâπÂæÅ‰∏éÁîüÊàêÂèãÂ•ΩÊΩúÂú®Á©∫Èó¥‰πãÈó¥ÁöÑÂü∫Êú¨‰∏çÂåπÈÖçÈóÆÈ¢ò„ÄÇFAEÁöÑÂÖ≥ÈîÆÂú®‰∫éÁªìÂêà‰∏§‰∏™Áã¨Á´ãÁöÑÊ∑±Â∫¶Ëß£Á†ÅÂô®Ôºå‰∏Ä‰∏™Áî®‰∫éÈáçÂª∫ÂéüÂßãÁâπÂæÅÁ©∫Èó¥ÔºåÂè¶‰∏Ä‰∏™ÂàôÂà©Áî®ÈáçÂª∫ÁöÑÁâπÂæÅËøõË°åÂõæÂÉèÁîüÊàê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFAEÂú®Â§öÁßçÁîüÊàêÊ®°Âûã‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂõæÂÉèÁîüÊàêÁöÑË¥®ÈáèÂíåÂ≠¶‰π†ÈÄüÂ∫¶‰∏ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.07805', 'title': 'Group Representational Position Encoding', 'url': 'https://huggingface.co/papers/2512.07805', 'abstract': 'GRAPE is a unified positional encoding framework that combines multiplicative rotations and additive logit biases, extending existing methods like RoPE and ALiBi.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,œâ,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'}, 'hash': '78d1417c6281ee3d', 'authors': ['Yifan Zhang', 'Zixiang Chen', 'Yifeng Liu', 'Zhen Qin', 'Huizhuo Yuan', 'Kangping Xu', 'Yang Yuan', 'Quanquan Gu', 'Andrew Chi-Chih Yao'], 'affiliations': ['IIIS, Tsinghua University', 'Princeton University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07805.jpg', 'data': {'categories': ['#open_source', '#long_context', '#architecture', '#math'], 'emoji': 'üåÄ', 'ru': {'title': '–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≥—Ä—É–ø–ø–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ GRAPE ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞: –º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω—ã–µ —Ä–æ—Ç–∞—Ü–∏–∏ –≤ –≥—Ä—É–ø–ø–µ SO(d) –∏ –∞–¥–¥–∏—Ç–∏–≤–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è –ª–æ–≥–∏—Ç–æ–≤ –∏–∑ —É–Ω–∏–ø–æ—Ç–µ–Ω—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–∞—è GRAPE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Ç—Ä–∏—á–Ω—É—é —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—É –¥–ª—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ –Ω–æ—Ä–º–∏—Ä—É—é—â–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π, –ø—Ä–∏ —ç—Ç–æ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ RoPE —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç–Ω—ã–º —Å–ª—É—á–∞–µ–º —ç—Ç–æ–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ê–¥–¥–∏—Ç–∏–≤–Ω–∞—è GRAPE –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–µ—Ç–æ–¥—ã ALiBi –∏ Forgetting Transformer –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ —Ä–∞–Ω–≥–∞-1, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ GRAPE –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ–¥–∏–Ω–æ–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–∏–∑–∞–π–Ω–∞ –¥–ª—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤ –º–æ–¥–µ–ª—è—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –æ–±–æ–±—â–∞—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã.'}, 'en': {'title': 'GRAPE: Unifying Positional Encoding for Enhanced Model Performance', 'desc': 'GRAPE is a new framework for positional encoding in machine learning that integrates two key methods: multiplicative rotations and additive logit biases. It uses mathematical structures from group theory to create a unified approach that enhances how models understand position in data. The framework allows for efficient representation of complex relationships between features, improving performance in tasks with long contexts. By encompassing existing methods like RoPE and ALiBi, GRAPE provides a flexible and powerful tool for developing advanced models.'}, 'zh': {'title': 'GRAPEÔºöÁªü‰∏ÄÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊ°ÜÊû∂', 'desc': 'GRAPEÊòØ‰∏ÄÁßçÁªü‰∏ÄÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊ°ÜÊû∂ÔºåÁªìÂêà‰∫Ü‰πòÊ≥ïÊóãËΩ¨ÂíåÂä†Ê≥ïÈÄªËæëÂÅèÁΩÆÔºåÊâ©Â±ï‰∫ÜÁé∞ÊúâÁöÑÊñπÊ≥ïÂ¶ÇRoPEÂíåALiBi„ÄÇÂÆÉÂ∞Ü‰∏§ÁßçÊú∫Âà∂ÁªìÂêàÂú®‰∏ÄËµ∑Ôºö‰πòÊ≥ïGRAPEÂíåÂä†Ê≥ïGRAPEÔºåÂàÜÂà´Âü∫‰∫éÁæ§‰Ωì‰ΩúÁî®„ÄÇ‰πòÊ≥ïGRAPEÈÄöËøáÂú®SO(d)‰∏≠ÁöÑÊóãËΩ¨ÂÆûÁé∞Áõ∏ÂØπÁöÑ„ÄÅÁªÑÂêàÁöÑ„ÄÅ‰øùÊåÅËåÉÊï∞ÁöÑÊò†Â∞ÑÔºåËÄåÂä†Ê≥ïGRAPEÂàôÈÄöËøá‰ΩéÁß©ÁöÑÂçïÂÖÉ‰ΩúÁî®ÁîüÊàêÂä†Ê≥ïÈÄªËæë„ÄÇGRAPE‰∏∫Èïø‰∏ä‰∏ãÊñáÊ®°Âûã‰∏≠ÁöÑ‰ΩçÁΩÆÂá†‰ΩïÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂéüÂàôÁöÑËÆæËÆ°Á©∫Èó¥ÔºåÊ∂µÁõñ‰∫ÜRoPEÂíåALiBi‰Ωú‰∏∫Áâπ‰æã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06963', 'title': 'VideoVLA: Video Generators Can Be Generalizable Robot Manipulators', 'url': 'https://huggingface.co/papers/2512.06963', 'abstract': "VideoVLA uses a multi-modal Diffusion Transformer to predict actions and visual outcomes from language and image inputs, enabling strong generalization in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': '4bad1a680c044c57', 'authors': ['Yichao Shen', 'Fangyun Wei', 'Zhiying Du', 'Yaobo Liang', 'Yan Lu', 'Jiaolong Yang', 'Nanning Zheng', 'Baining Guo'], 'affiliations': ['Fudan University', 'IAIR, Xian Jiaotong University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06963.jpg', 'data': {'categories': ['#agi', '#multimodal', '#architecture', '#video', '#robotics', '#diffusion'], 'emoji': 'ü§ñ', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–æ–µ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –∫–ª—é—á –∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–æ–±–æ—Ç-–º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤', 'desc': 'VideoVLA ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å Vision-Language-Action, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–∞–º–∏ —Ä–æ–±–æ—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π Diffusion Transformer –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏ –±—É–¥—É—â–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ö–ª—é—á–µ–≤–æ–π –∏–¥–µ–π —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å –Ω–∞–¥—ë–∂–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏ —É—Å–ø–µ—Ö–æ–º –∑–∞–¥–∞—á–∏, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏, –æ—Ç–∫—Ä—ã–≤–∞—è –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ –∏ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.'}, 'en': {'title': 'Empowering Robots with Visual Imagination for Action Prediction', 'desc': 'VideoVLA is a novel approach that utilizes a multi-modal Diffusion Transformer to enhance robotic manipulation by predicting actions and visual outcomes based on language and image inputs. This method addresses the limitations of existing Vision-Language-Action (VLA) models, which struggle to generalize to new tasks and environments. By leveraging pre-trained video generation models, VideoVLA effectively combines video, language, and action modalities to forecast both the actions a robot should take and the expected visual results. The findings indicate that this dual-prediction strategy not only improves task success but also enables robots to adapt to unfamiliar objects and skills, marking a significant advancement in robot learning.'}, 'zh': {'title': 'ËßÜÈ¢ëÁîüÊàê‰∏éÊú∫Âô®‰∫∫ÊìçÊéßÁöÑÁªìÂêà', 'desc': 'VideoVLAÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊâ©Êï£ÂèòÊç¢Âô®ÔºåËÉΩÂ§üÊ†πÊçÆËØ≠Ë®ÄÂíåÂõæÂÉèËæìÂÖ•È¢ÑÊµãÂä®‰ΩúÂíåËßÜËßâÁªìÊûúÔºå‰ªéËÄåÂú®Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÂÆûÁé∞Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂ∞ÜÂ§ßÂûãËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫VLAÊìçÊéßÂô®ÔºåÊé¢Á¥¢‰∫ÜËßÜÈ¢ëÁîüÊàê‰∏éÊú∫Âô®‰∫∫Êìç‰ΩúÁöÑÁªìÂêà„ÄÇÂÆûÈ™åË°®ÊòéÔºåÈ´òË¥®ÈáèÁöÑÊÉ≥Ë±°Êú™Êù•‰∏éÂèØÈù†ÁöÑÂä®‰ΩúÈ¢ÑÊµãÂíå‰ªªÂä°ÊàêÂäüÁéáÂØÜÂàáÁõ∏ÂÖ≥ÔºåÂº∫Ë∞É‰∫ÜËßÜËßâÊÉ≥Ë±°Âú®Êìç‰Ωú‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇVideoVLAÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂåÖÊã¨Ê®°‰ªøÂÖ∂‰ªñÂÆû‰æãÁöÑÊäÄËÉΩÂíåÂ§ÑÁêÜÊñ∞Áâ©‰ΩìÔºåÊé®Âä®‰∫ÜÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊñ∞ËåÉÂºè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06835', 'title': 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning', 'url': 'https://huggingface.co/papers/2512.06835', 'abstract': 'DoGe, a dual-decoupling framework, enhances vision-language models by separating context learning from problem solving, using a curriculum learning pipeline to improve reward signals and data diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': '0a8fcf1d1267bd3c', 'authors': ['Tingyu Li', 'Zheng Sun', 'Jingxuan Wei', 'Siyuan Li', 'Conghui He', 'Lijun Wu', 'Cheng Tan'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai JiaoTong University', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06835.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#multimodal', '#benchmark', '#synthetic', '#cv'], 'emoji': 'üß†', 'ru': {'title': '–†–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π: –¥–≤–æ–π–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'DoGe ‚Äî —ç—Ç–æ –¥–≤–æ–π–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ—Ç —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥ RL —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ ¬´–ú—ã—Å–ª–∏—Ç–µ–ª—å¬ª –∏ ¬´–†–µ—à–∞—Ç–µ–ª—å¬ª, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã reward hacking. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å curriculum learning —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º –∑–Ω–∞–Ω–∏–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–º –ø—É–ª–æ–º –±–∞–∑–æ–≤—ã—Ö –∑–∞–¥–∞—á. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∞–º–æ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—â–∏—Ö—Å—è –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Decoupling Context and Problem Solving for Enhanced Vision-Language Models', 'desc': 'The paper introduces DoGe, a dual-decoupling framework designed to improve vision-language models (VLMs) by separating the learning of context from the actual problem-solving process. This approach utilizes a curriculum learning pipeline to enhance the quality of reward signals and increase the diversity of training data, addressing challenges in specialized domains. By decoupling the learning into two components, Thinker and Solver, the framework allows for better quantification of rewards and a more effective reinforcement learning (RL) strategy. Experimental results demonstrate that DoGe significantly outperforms existing methods, paving the way for more robust self-evolving large vision-language models (LVLMs).'}, 'zh': {'title': 'Ëß£ËÄ¶Â≠¶‰π†ÔºåÊèêÂçáËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõ', 'desc': 'DoGeÊòØ‰∏ÄÁßçÂèåÈáçËß£ËÄ¶Ê°ÜÊû∂ÔºåÊó®Âú®Â¢ûÂº∫ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÂÆÉÈÄöËøáÂ∞Ü‰∏ä‰∏ãÊñáÂ≠¶‰π†‰∏éÈóÆÈ¢òËß£ÂÜ≥ÂàÜÂºÄÔºåÂà©Áî®ËØæÁ®ãÂ≠¶‰π†ÊµÅÁ®ãÊù•ÊîπÂñÑÂ•ñÂä±‰ø°Âè∑ÂíåÊï∞ÊçÆÂ§öÊ†∑ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖàÂºïÂØºÊ®°Âûã‰ªé‰∏ä‰∏ãÊñá‰∏≠Â≠¶‰π†ÔºåËÄå‰∏çÊòØÁõ¥Êé•Ëß£ÂÜ≥ÈóÆÈ¢òÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÂêàÊàêÊï∞ÊçÆÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDoGeÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÊñπÊ≥ïÔºå‰∏∫ÂÆûÁé∞Ëá™ÊàëËøõÂåñÁöÑÂ§ßÂûãËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõ‰∫ÜÂèØÊâ©Â±ïÁöÑË∑ØÂæÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06421', 'title': 'Rethinking Training Dynamics in Scale-wise Autoregressive Generation', 'url': 'https://huggingface.co/papers/2512.06421', 'abstract': 'Self-Autoregressive Refinement (SAR) improves the quality of autoregressive generative models by addressing exposure bias through Stagger-Scale Rollout and Contrastive Student-Forcing Loss, leading to consistent improvements with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 6', 'zh': '12Êúà6Êó•'}, 'hash': '4eb23fe19101d4d8', 'authors': ['Gengze Zhou', 'Chongjian Ge', 'Hao Tan', 'Feng Liu', 'Yicong Hong'], 'affiliations': ['Adobe Research', 'Australian Institute for Machine Learning, Adelaide University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06421.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#architecture'], 'emoji': 'üé®', 'ru': {'title': '–°–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–µ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–º —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Self-Autoregressive Refinement (SAR) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã exposure bias –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Ä–µ–∂–∏–º–µ coarse-to-fine. SAR –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –º–µ—Ö–∞–Ω–∏–∑–º Stagger-Scale Rollout, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª—ë–≥–∫–∏–µ –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è train-test —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, –∏ Contrastive Student-Forcing Loss –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∞–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SAR —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º, –∞ —Ç–∞–∫–∂–µ –¥–∏—Å–±–∞–ª–∞–Ω—Å –≤ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SAR –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–∞—ë—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.'}, 'en': {'title': 'Enhancing Autoregressive Models with Self-Autoregressive Refinement', 'desc': "Self-Autoregressive Refinement (SAR) enhances autoregressive generative models by tackling exposure bias, which occurs when models generate outputs based on their own previous predictions. It introduces a Stagger-Scale Rollout (SSR) technique that allows models to learn from their intermediate outputs, improving the alignment between training and testing phases. Additionally, the Contrastive Student-Forcing Loss (CSFL) provides better supervision for the model's self-generated contexts, leading to more stable training. Experimental results demonstrate that SAR significantly improves generation quality with minimal computational costs, making it a promising method for visual autoregressive generation."}, 'zh': {'title': 'Ëá™ÂõûÂΩíÁ≤æÁÇºÔºöÊèêÂçáÁîüÊàêÊ®°ÂûãË¥®ÈáèÁöÑÊúâÊïàÊñπÊ≥ï', 'desc': 'Ëá™ÂõûÂΩíÁ≤æÁÇºÔºàSARÔºâÈÄöËøáÂºïÂÖ•ÂàÜÂ±ÇËßÑÊ®°ÂõûÊªöÂíåÂØπÊØîÂ≠¶ÁîüÂº∫Âà∂ÊçüÂ§±ÔºåÊîπÂñÑ‰∫ÜËá™ÂõûÂΩíÁîüÊàêÊ®°ÂûãÁöÑË¥®ÈáèÔºåËß£ÂÜ≥‰∫ÜÊõùÂÖâÂÅèÂ∑ÆÈóÆÈ¢ò„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËΩªÈáèÁ∫ßÁöÑËá™ÂõûÂΩíÂõûÊªöÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üÊé•Ëß¶Âà∞Ëá™Ë∫´ÁöÑ‰∏≠Èó¥È¢ÑÊµãÔºå‰ªéËÄåÂØπÈΩêËÆ≠ÁªÉÂíåÊµãËØïÊ®°Âºè„ÄÇSARËøòÊèê‰æõ‰∫ÜË∂≥Â§üÁöÑÁõëÁù£Ôºå‰ª•Á°Æ‰øùËá™ÁîüÊàê‰∏ä‰∏ãÊñáÁöÑÁ®≥ÂÆöËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSARÂú®È¢ÑËÆ≠ÁªÉÁöÑËá™ÂõûÂΩíÊ®°Âûã‰∏äÂ∫îÁî®ÂêéÔºåÁîüÊàêË¥®ÈáèÊòæËëóÊèêÈ´òÔºåËÆ°ÁÆóÂºÄÈîÄÊûÅÂ∞è„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06558', 'title': 'Embodied Referring Expression Comprehension in Human-Robot Interaction', 'url': 'https://huggingface.co/papers/2512.06558', 'abstract': 'A large-scale dataset and multimodal model improve embodied interaction comprehension in robots by addressing perspective bias and enhancing multimodal signal integration.  \t\t\t\t\tAI-generated summary \t\t\t\t As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 6', 'zh': '12Êúà6Êó•'}, 'hash': '49b6bc2ca4f8291d', 'authors': ['Md Mofijul Islam', 'Alexi Gladstone', 'Sujan Sarker', 'Ganesh Nanduru', 'Md Fahim', 'Keyan Du', 'Aman Chadha', 'Tariq Iqbal'], 'affiliations': ['Amazon GenAI', 'Stanford University', 'University of Dhaka', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06558.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#robotics', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–æ–ø–ª–æ—â—ë–Ω–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Refer360, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –∑–∞–ø–∏—Å–∏ –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã—Ö –≤–µ—Ä–±–∞–ª—å–Ω—ã—Ö –∏ –Ω–µ–≤–µ—Ä–±–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ —Ä–æ–±–æ—Ç–∞–º–∏ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ MuRes ‚Äî –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –∏ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ —á–µ—Ç—ã—Ä—ë—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–ø–æ–ª–Ω–æ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ MuRes –∫ —ç—Ç–∏–º –º–æ–¥–µ–ª—è–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Ä–æ–±–æ—Ç–∞.'}, 'en': {'title': 'Enhancing Robot Comprehension with Multimodal Learning and Diverse Datasets', 'desc': 'This paper addresses the challenge of robots understanding human instructions through embodied interactions, which is essential for effective human-robot interaction (HRI). It introduces the Refer360 dataset, a comprehensive collection of verbal and nonverbal interactions captured from multiple perspectives in various environments. The authors also propose MuRes, a multimodal guided residual module that enhances the integration of different types of signals to improve comprehension of referring expressions. Experimental results show that incorporating MuRes significantly boosts the performance of existing multimodal models in understanding embodied interactions.'}, 'zh': {'title': 'ÊèêÂçáÊú∫Âô®‰∫∫ÁêÜËß£ËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ‰∏éÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫ÜRefer360Êï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÊó®Âú®ÊîπÂñÑÊú∫Âô®‰∫∫ÂØπ‰∫∫Á±ªÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇËØ•Êï∞ÊçÆÈõÜÈÄöËøáÂ§öËßíÂ∫¶Êî∂ÈõÜÂÆ§ÂÜÖÂíåÂÆ§Â§ñÁöÑËá™ÁÑ∂‰∫íÂä®ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊï∞ÊçÆÈõÜ‰∏≠Â≠òÂú®ÁöÑËßÜËßíÂÅèÂ∑ÆÂíåÈùûËØ≠Ë®ÄÊâãÂäøË¶ÜÁõñ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜMuResÊ®°ÂùóÔºåÂÆÉÈÄöËøáÊèêÂèñÁâπÂÆöÊ®°ÊÄÅÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂ¢ûÂº∫‰∫ÜÊú∫Âô®‰∫∫ÂØπÊåá‰ª£Ë°®ËææÁöÑÁêÜËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÁªìÂêàMuResÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®ÁêÜËß£‰∫∫Á±ª‰∫íÂä®ÊñπÈù¢Ë°®Áé∞Êõ¥‰Ω≥ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êú∫Âô®‰∫∫‰∏é‰∫∫Á±ªÁéØÂ¢É‰∏≠Â∫îÁî®ÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.06791', 'title': 'Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games', 'url': 'https://huggingface.co/papers/2512.06791', 'abstract': "The SGN condition provides a framework for certifying convergence of gradient-based learning in games by constructing a weighted block metric, enabling convergence under conditions where Euclidean geometry fails.  \t\t\t\t\tAI-generated summary \t\t\t\t Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 7', 'zh': '12Êúà7Êó•'}, 'hash': '31de0f6349469bd7', 'authors': ['Vedansh Sharma'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06791.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#games', '#math'], 'emoji': 'üéÆ', 'ru': {'title': '–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏–≥—Ä —á–µ—Ä–µ–∑ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –µ–≤–∫–ª–∏–¥–æ–≤—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ SGN (Small-Gain Nash) –¥–ª—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –∏–≥—Ä–∞—Ö, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –±–ª–æ—á–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ –ø—Å–µ–≤–¥–æ-–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ –µ–≤–∫–ª–∏–¥–æ–≤–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –ø–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ –ª–æ–∫–∞–ª—å–Ω–∞—è –∫—Ä–∏–≤–∏–∑–Ω–∞ –∏ –∫—Ä–æ—Å—Å-–∏–≥—Ä–æ–≤—ã–µ —Å–≤—è–∑–∏ –õ–∏–ø—à–∏—Ü–∞ –º–æ–≥—É—Ç –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É, –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Å–µ–≤–¥–æ-–≥—Ä–∞–¥–∏–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–∏–ª—å–Ω–æ –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–º –¥–∞–∂–µ –≤ –∏–≥—Ä–∞—Ö, –≥–¥–µ —ç—Ç–æ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –µ–≤–∫–ª–∏–¥–æ–≤–æ–º —Å–º—ã—Å–ª–µ, –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å —è–≤–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –Ω–∞ —Ä–∞–∑–º–µ—Ä —à–∞–≥–∞. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –∏–≥—Ä–∞—Ö –∏ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è –Ω–∞ –∑–µ—Ä–∫–∞–ª—å–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–æ-–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –º–∞—Ä–∫–æ–≤—Å–∫–∏—Ö –∏–≥—Ä–∞—Ö.'}, 'en': {'title': 'Certifying Convergence in Non-Monotone Games with SGN', 'desc': 'The paper introduces the Small-Gain Nash (SGN) condition, which provides a new way to certify convergence in gradient-based learning for games, especially when traditional methods fail. It constructs a weighted block metric that allows the pseudo-gradient to be strongly monotone, even in cases where it is not in Euclidean geometry. This approach translates local curvature and cross-player Lipschitz bounds into a practical certificate of contraction, ensuring that the learning process converges. The authors validate their method on quadratic games and extend it to other geometries, creating a comprehensive certification pipeline for non-monotone games.'}, 'zh': {'title': 'SGNÊù°‰ª∂ÔºöÂçöÂºàÂ≠¶‰π†Êî∂ÊïõÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'SGNÊù°‰ª∂‰∏∫Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑÂçöÂºàÂ≠¶‰π†Êèê‰æõ‰∫Ü‰∏Ä‰∏™Êî∂ÊïõÊÄßËØÅÊòéÊ°ÜÊû∂ÔºåÈÄöËøáÊûÑÂª∫Âä†ÊùÉÂùóÂ∫¶ÈáèÔºå‰ΩøÂæóÂú®Ê¨ßÂá†ÈáåÂæóÂá†‰ΩïÂ§±ÊïàÁöÑÊÉÖÂÜµ‰∏ã‰πüËÉΩÂÆûÁé∞Êî∂Êïõ„ÄÇ‰º†ÁªüÁöÑÊî∂Êïõ‰øùËØÅË¶ÅÊ±Ç‰º™Ê¢ØÂ∫¶Âú®Ê¨ßÂá†ÈáåÂæóÂá†‰Ωï‰∏≠ÊòØÔºàÂº∫ÔºâÂçïË∞ÉÁöÑÔºå‰ΩÜÂú®ËÆ∏Â§öÁÆÄÂçïÂçöÂºà‰∏≠Ëøô‰∏ÄÊù°‰ª∂Â∏∏Â∏∏‰∏çÊàêÁ´ã„ÄÇSGNÂ∞ÜÂ±ÄÈÉ®Êõ≤ÁéáÂíåË∑®Áé©ÂÆ∂ÁöÑLipschitzËÄ¶ÂêàÁïåÈôêËΩ¨Âåñ‰∏∫ÂèØÂ§ÑÁêÜÁöÑÊî∂Áº©ËØÅÊòéÔºåÂπ∂Âú®Ëøô‰∫õÁïåÈôêÊàêÁ´ãÁöÑÂå∫ÂüüÂÜÖ‰Ωø‰º™Ê¢ØÂ∫¶Âú®Âä†ÊùÉÂùóÂ∫¶Èáè‰∏ãÂèòÂæóÂº∫ÂçïË∞É„ÄÇÊàë‰ª¨ÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏Ä‰∏™ËÆ§ËØÅÁöÑ‚ÄúÊó∂Èó¥Â∞∫Â∫¶Â∏¶‚ÄùÔºå‰∏∫ÈùûÊ∏êËøëÊÄß„ÄÅÂü∫‰∫éÂ∫¶ÈáèÁöÑÊî∂ÊïõËØÅÊòéÊèê‰æõ‰∫ÜÊîØÊåÅÔºåÁ°Æ‰øùÂú®ÁâπÂÆöÁöÑÂ∫¶ÈáèÊùÉÈáç‰∏ãÔºåÂçïÊ≠•Âä®ÊÄÅÊòØÂèØËØÅÊòéÁöÑÊî∂Áº©„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05100', 'title': 'Structured Document Translation via Format Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.05100', 'abstract': 'Format Reinforcement Learning enhances structured text translation by optimizing structure-aware rewards and distinguishing between minor errors and major structural failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FormatRL), which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': 'c7232cc74496b432', 'authors': ['Haiyue Song', 'Johannes Eschbach-Dymanus', 'Hour Kaing', 'Sumire Honda', 'Hideki Tanaka', 'Bianka Buschbeck', 'Masao Utiyama'], 'affiliations': ['National Institute of Information and Communications Technology, Japan', 'SAP, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05100.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#benchmark', '#rlhf', '#machine_translation'], 'emoji': 'üìã', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Format Reinforcement Learning –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç Group Relative Policy Optimization –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥, —É—á–∏—Ç—ã–≤–∞—é—â–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä—É: TreeSim –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ XML-–¥–µ—Ä–µ–≤—å–µ–≤ –∏ Node-chrF –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —É–∑–ª–æ–≤. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ StrucAUC, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–ª–∏—á–∞–µ—Ç –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ —Å–µ—Ä—å–µ–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Å–±–æ–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ SAP –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —à–µ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫–∞–º –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–∫–ª–∞–¥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.'}, 'en': {'title': 'Optimizing Structured Text Translation with Format Reinforcement Learning', 'desc': 'This paper introduces Format Reinforcement Learning (FormatRL) to improve structured text translation, particularly for complex document formats like XML and HTML. It utilizes Group Relative Policy Optimization to enhance a supervised model by optimizing structure-aware rewards, specifically TreeSim and Node-chrF, which evaluate structural similarity and translation quality, respectively. The method also incorporates StrucAUC, a metric that differentiates between minor and major errors in structure. Experiments show that FormatRL significantly enhances translation quality and structural accuracy on the SAP software-documentation benchmark.'}, 'zh': {'title': 'Ê†ºÂºèÂº∫ÂåñÂ≠¶‰π†Ôºö‰ºòÂåñÁªìÊûÑÂåñÊñáÊú¨ÁøªËØëÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê†ºÂºèÂº∫ÂåñÂ≠¶‰π†ÔºàFormatRLÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊîπÂñÑÁªìÊûÑÂåñÊñáÊú¨ÁøªËØëÔºåÁâπÂà´ÊòØÂ§ÑÁêÜÂ§çÊùÇÁöÑÊñáÊ°£Á∫ßXMLÊàñHTMLÁªìÊûÑ„ÄÇËØ•ÊñπÊ≥ïÂú®ÁõëÁù£ÂæÆË∞ÉÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äÔºåÈááÁî®‰∫ÜÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºåÁõ¥Êé•‰ºòÂåñÊñ∞ÁöÑÁªìÊûÑÊÑüÁü•Â•ñÂä±ÔºåÂåÖÊã¨Ê†ëÁªìÊûÑÁõ∏‰ººÂ∫¶ÂíåXMLËäÇÁÇπÁøªËØëË¥®ÈáèÁöÑËØÑ‰º∞„ÄÇÈÄöËøáÂºïÂÖ•ÁªÜÁ≤íÂ∫¶ÁöÑStrucAUCÊåáÊ†áÔºåËÉΩÂ§üÂå∫ÂàÜÂ∞èÈîôËØØÂíåÈáçÂ§ßÁªìÊûÑÂ§±Ë¥•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ÂÖ≠‰∏™ÊåáÊ†á‰∏äÂùáÊúâÊòæËëóÊèêÂçáÔºåËøõ‰∏ÄÊ≠•ÂàÜÊûêÊòæÁ§∫‰∏çÂêåÂ•ñÂä±ÂáΩÊï∞ÂØπÁªìÊûÑÂíåÁøªËØëË¥®ÈáèÁöÑÊîπÂñÑÊúâÈáçË¶ÅË¥°ÁåÆ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi (1)', '#alignment (3)', '#architecture (17)', '#audio', '#benchmark (17)', '#cv (11)', '#data (2)', '#dataset (10)', '#diffusion (14)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (2)', '#inference (5)', '#interpretability (2)', '#leakage', '#long_context (4)', '#low_resource', '#machine_translation (2)', '#math (4)', '#multilingual (1)', '#multimodal (15)', '#open_source (17)', '#optimization (16)', '#plp (1)', '#rag (1)', '#reasoning (13)', '#rl (10)', '#rlhf (4)', '#robotics (4)', '#science (4)', '#security (2)', '#small_models (3)', '#story_generation (1)', '#survey (2)', '#synthetic (5)', '#training (18)', '#transfer_learning (2)', '#video (10)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-12-11 08:34',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-11 08:34')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-11 08:34')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    