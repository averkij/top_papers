
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 73 papers. December 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2025</span> | <span id="title-articles-count">73 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-11.html">â¬…ï¸ <span id="prev-date">11.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2026-01.html">â¡ï¸ <span id="next-date">01.2026</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ğ”ĞµĞºĞ°Ğ±Ñ€ÑŒ 2025', 'en': 'December 2025', 'zh': '12æœˆ2025å¹´'};
        let feedDateNext = {'ru': '01.2026', 'en': '01/2026', 'zh': '1æœˆ2026å¹´'};
        let feedDatePrev = {'ru': '11.2025', 'en': '11/2025', 'zh': '11æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.04324', 'title': 'DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle', 'url': 'https://huggingface.co/papers/2512.04324', 'abstract': 'DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io', 'score': 146, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '67634956bb0a1146', 'authors': ['Fangyu Lei', 'Jinxiang Meng', 'Yiming Huang', 'Junjie Zhao', 'Yitong Zhang', 'Jianwen Luo', 'Xin Zou', 'Ruiyi Yang', 'Wenbo Shi', 'Yan Gao', 'Shizhu He', 'Zuo Wang', 'Qian Liu', 'Yang Wang', 'Ke Wang', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['ByteDance Seed', 'Institute of Automation, CAS', 'NUS', 'TikTok', 'UC San Diego', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04324.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#agents', '#reasoning', '#science'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DAComp Ğ¸Ğ· 210 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… SQL-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ…, Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· LLM-ÑÑƒĞ´ÑŒÑ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ÑƒÑĞ¿ĞµÑ… Ğ¼ĞµĞ½ĞµĞµ 20% Ğ½Ğ° Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ¸Ğ¶Ğµ 40% Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unveiling the Gaps in Data Engineering and Analysis with DAComp', 'desc': 'DAComp is a benchmark consisting of 210 tasks designed to evaluate the performance of agents in real-world data engineering and data analysis workflows. It highlights significant shortcomings in both areas, particularly in data engineering tasks where agents struggle with repository-level engineering and SQL pipeline creation. Data analysis tasks also reveal low performance, indicating challenges in open-ended reasoning and strategic planning. By identifying these deficiencies, DAComp serves as a critical tool for advancing the development of autonomous data agents in enterprise environments.'}, 'zh': {'title': 'DACompï¼šè¯„ä¼°æ•°æ®æ™ºèƒ½ä½“èƒ½åŠ›çš„åŸºå‡†', 'desc': 'DACompæ˜¯ä¸€ä¸ªåŒ…å«210ä¸ªä»»åŠ¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®æ•°æ®å·¥ç¨‹å’Œæ•°æ®åˆ†æå·¥ä½œæµç¨‹ä¸­çš„èƒ½åŠ›ã€‚æ•°æ®å·¥ç¨‹ä»»åŠ¡æ¶‰åŠå°†åŸå§‹æ•°æ®æºè½¬åŒ–ä¸ºå¯åˆ†æçš„è¡¨æ ¼ï¼Œè€Œæ•°æ®åˆ†æä»»åŠ¡åˆ™å°†è¿™äº›è¡¨æ ¼è½¬åŒ–ä¸ºå†³ç­–å¯¼å‘çš„è§è§£ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“åœ¨DACompä¸Šè¡¨ç°ä¸ä½³ï¼Œæ•°æ®å·¥ç¨‹ä»»åŠ¡çš„æˆåŠŸç‡ä½äº20%ï¼Œè€Œæ•°æ®åˆ†æä»»åŠ¡çš„å¹³å‡å¾—åˆ†ä¹Ÿä½äº40%ã€‚é€šè¿‡æ˜ç¡®è¯Šæ–­è¿™äº›å±€é™æ€§ï¼ŒDACompä¸ºå¼€å‘çœŸæ­£èƒ½å¤Ÿåœ¨ä¼ä¸šç¯å¢ƒä¸­å·¥ä½œçš„è‡ªä¸»æ•°æ®æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªä¸¥æ ¼ä¸”ç°å®çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04987', 'title': 'Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction', 'url': 'https://huggingface.co/papers/2512.04987', 'abstract': 'The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.', 'score': 71, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '99887076d6ca86f9', 'authors': ['Nex-AGI Team', ':', 'Yuxuan Cai', 'Lu Chen', 'Qiaoling Chen', 'Yuyang Ding', 'Liwen Fan', 'Wenjie Fu', 'Yufei Gao', 'Honglin Guo', 'Pinxue Guo', 'Zhenhua Han', 'Zhengfu He', 'Hanglei Hu', 'Kai Hu', 'Shengjia Hua', 'Tianyu Huai', 'Baodai Huang', 'Li Ji', 'Zhen Jiang', 'Zhikai Lei', 'Bufan Li', 'Jiahang Lin', 'Lizhi Lin', 'Jinxiu Liu', 'Shichun Liu', 'Ziming Liu', 'Yuchen Ni', 'Pengfang Qian', 'Yujiong Shen', 'Qingyun Shi', 'Wentao Shu', 'Peng Sun', 'Yiran Suo', 'Tian Tang', 'Boyu Tian', 'Guoteng Wang', 'Junzhe Wang', 'Peixin Wang', 'Zhiheng Xi', 'Hang Yan', 'Jie Yang', 'Zhixiong Yang', 'Tianchu Yao', 'Guangze Ye', 'Qianxi Yu', 'Shuo Zhang', 'Xinyue Zhang', 'Yiqi Zhang', 'Jiarong Zhao', 'Miao Zheng', 'Rui Zheng', 'Enyu Zhou', 'Jiazheng Zhou', 'Maosen Zhou', 'Yuhao Zhou', 'Tao Gui', 'Yining Zheng', 'Xinchi Chen', 'Jie Zhou', 'Siyuan Feng', 'Qin Chen', 'Liang He', 'Qi Zhang', 'Xuanjing Huang', 'Xipeng Qiu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04987.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Nex Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: NexAU Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², NexA4A Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¸ NexGAP Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Nex-N1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Scaling LLMs: From Imitation to Autonomous Decision-Making', 'desc': 'This paper introduces a new framework called Nex, which enhances the training of large language models (LLMs) as autonomous agents. It focuses on three key areas: complexity, diversity, and fidelity, allowing for more effective learning through interactive environments. NexAU provides a flexible structure for creating complex agent hierarchies, while NexA4A generates diverse agent configurations from natural language. Finally, NexGAP helps bridge the gap between simulated and real-world environments, leading to improved performance in complex tasks compared to existing models.'}, 'zh': {'title': 'è‡ªä¸»ä»£ç†çš„äº’åŠ¨ç¯å¢ƒæ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†NexAUã€NexA4Aå’ŒNexGAPçš„å¼•å…¥ï¼Œè¿™äº›å·¥å…·èƒ½å¤Ÿåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»ä»£ç†çš„äº’åŠ¨ç¯å¢ƒä¸­æ‰©å±•å¤æ‚æ€§ã€å¤šæ ·æ€§å’ŒçœŸå®æ„Ÿï¼Œä»è€Œå®ç°æ›´ä¼˜çš„æ€§èƒ½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜éœ€è¦ä»é™æ€æ¨¡ä»¿è½¬å‘åŸºäºæ¿€åŠ±çš„å†³ç­–åˆ¶å®šï¼Œä½†ç¼ºä¹å¯æ‰©å±•çš„åŸºç¡€è®¾æ–½æ¥æ„å»ºé«˜è´¨é‡çš„äº’åŠ¨ä¿¡å·ï¼Œé˜»ç¢äº†è¿™ä¸€è½¬å˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªç»´åº¦æ¥æ‰©å±•äº’åŠ¨ç¯å¢ƒçš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼šå¤æ‚æ€§ã€è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–ä»£ç†å±‚æ¬¡çš„èƒ½åŠ›ï¼Œä»¥åŠé€šè¿‡åŠ¨æ€çœŸå®ç¯å¢ƒæ¥ç¼©å°æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºæˆ‘ä»¬åŸºç¡€è®¾æ–½è®­ç»ƒçš„Nex-N1åœ¨å¤æ‚çš„ä»£ç†ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨ä¸å‰æ²¿ä¸“æœ‰æ¨¡å‹çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05111', 'title': 'ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning', 'url': 'https://huggingface.co/papers/2512.05111', 'abstract': 'ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.', 'score': 45, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'cfd3368c930c3069', 'authors': ['Shengyuan Ding', 'Xinyu Fang', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiangyu Zhao', 'Haodong Duan', 'Xiaoyi Dong', 'Jianze Liang', 'Bin Wang', 'Conghui He', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05111.jpg', 'data': {'categories': ['#cv', '#benchmark', '#rl', '#interpretability', '#hallucinations', '#alignment', '#rlhf', '#agents', '#reasoning', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'ARM-Thinker â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ°Ğ´Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ reinforcement learning, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ARM-Thinker Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Empowering AI with Agentic Verification for Better Reasoning', 'desc': "ARM-Thinker is a novel agentic reward model designed to enhance the performance of vision-language systems by utilizing external tools for verification. This model addresses common issues in existing reward models, such as hallucination and weak visual grounding, by autonomously invoking tools to provide verifiable evidence for its judgments. By employing multi-stage reinforcement learning, ARM-Thinker optimizes both the decision to use tools and the accuracy of its assessments. The introduction of ARMBench-VL benchmarks further validates the model's effectiveness, showing significant improvements in accuracy and interpretability in complex multimodal reasoning tasks."}, 'zh': {'title': 'è‡ªä¸»å¥–åŠ±æ¨¡å‹æå‡æ¨ç†å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'ARM-Thinkeræ˜¯ä¸€ç§è‡ªä¸»å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿä½¿ç”¨å¤–éƒ¨å·¥å…·è¿›è¡ŒéªŒè¯ï¼Œä»è€Œæé«˜å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡è°ƒç”¨å›¾åƒè£å‰ªå’Œæ–‡æ¡£æ£€ç´¢ç­‰å·¥å…·ï¼ŒåŸºäºå¯éªŒè¯çš„è¯æ®æ¥æ”¯æŒåˆ¤æ–­ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚ARM-Thinkeré‡‡ç”¨å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–å·¥å…·è°ƒç”¨å†³ç­–å’Œåˆ¤æ–­å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARM-Thinkeråœ¨å¥–åŠ±å»ºæ¨¡åŸºå‡†ä¸Šå¹³å‡æé«˜äº†16.2%ï¼Œåœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡ä¸Šæé«˜äº†9.6%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04926', 'title': 'Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion', 'url': 'https://huggingface.co/papers/2512.04926', 'abstract': 'Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.', 'score': 40, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '247d709340d0e502', 'authors': ['Yueming Pan', 'Ruoyu Feng', 'Qi Dai', 'Yuqi Wang', 'Wenfeng Lin', 'Mingyu Guo', 'Chong Luo', 'Nanning Zheng'], 'affiliations': ['ByteDance', 'IAIR, Xian Jiaotong University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04926.jpg', 'data': {'categories': ['#architecture', '#cv', '#open_source', '#diffusion', '#optimization', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ° Ğ²Ğ¿ĞµÑ€ĞµĞ´Ğ¸: Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¸Ğ´ĞµĞ¸ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Semantic-First Diffusion (SFD) â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ñ Ğ¾Ğ¿ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ñ‘Ñ‚ĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Semantic VAE, Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet SFD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ FID 1.04 Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² 100 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Prioritizing Semantics for Superior Image Generation', 'desc': 'Semantic-First Diffusion (SFD) is a novel approach in image generation that improves the quality and speed of Latent Diffusion Models (LDMs) by focusing on semantic information first. It separates the denoising process of semantic and texture latents, allowing the model to refine high-level structures before adding fine details. By using a dedicated Semantic VAE to extract semantic latents, SFD provides a clearer guide for generating textures, leading to more coherent images. This method not only enhances convergence speed but also outperforms existing techniques, showcasing the benefits of prioritizing semantics in the generation process.'}, 'zh': {'title': 'è¯­ä¹‰ä¼˜å…ˆï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'è¯­ä¹‰ä¼˜å…ˆæ‰©æ•£ï¼ˆSFDï¼‰é€šè¿‡å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œå˜é‡æ¥å¢å¼ºå›¾åƒç”Ÿæˆï¼Œä»è€Œæé«˜æ”¶æ•›é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚è¯¥æ–¹æ³•é¦–å…ˆç»“åˆä»é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨æå–çš„ç´§å‡‘è¯­ä¹‰æ½œå˜é‡ä¸çº¹ç†æ½œå˜é‡ï¼Œæ„å»ºå¤åˆæ½œå˜é‡ã€‚SFDçš„æ ¸å¿ƒåœ¨äºä½¿ç”¨ä¸åŒçš„å™ªå£°è°ƒåº¦å¼‚æ­¥å»å™ªè¯­ä¹‰å’Œçº¹ç†æ½œå˜é‡ï¼Œä½¿å¾—è¯­ä¹‰åœ¨æ—¶é—´ä¸Šä¼˜å…ˆäºçº¹ç†ï¼Œä»è€Œä¸ºçº¹ç†ç»†åŒ–æä¾›æ›´æ¸…æ™°çš„é«˜å±‚æ¬¡æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSFDåœ¨ImageNetæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæ”¶æ•›é€Ÿåº¦æ¯”åŸå§‹æ–¹æ³•å¿«100å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03000', 'title': 'DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling', 'url': 'https://huggingface.co/papers/2512.03000', 'abstract': 'DynamicVerse is a framework that models dynamic real-world videos by integrating large vision, geometric, and multimodal models to produce a comprehensive 4D multimodal dataset, achieving superior performance in video depth estimation, camera pose estimation, and camera intrinsics estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.', 'score': 34, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '4d1532cef25a96c5', 'authors': ['Kairun Wen', 'Yuzhi Huang', 'Runyu Chen', 'Hui Zheng', 'Yunlong Lin', 'Panwang Pan', 'Chenxin Li', 'Wenyan Cong', 'Jian Zhang', 'Junbin Lu', 'Chenguo Lin', 'Dilin Wang', 'Zhicheng Yan', 'Hongyu Xu', 'Justin Theiss', 'Yue Huang', 'Xinghao Ding', 'Rakesh Ranjan', 'Zhiwen Fan'], 'affiliations': ['CUHK', 'Meta', 'PKU', 'UT Austin', 'UW', 'XMU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03000.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#open_source', '#dataset', '#agents', '#3d', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ 4D Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'DynamicVerse â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ 4D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Bundle Adjustment Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'DynamicVerse: Revolutionizing Real-World Video Understanding', 'desc': "DynamicVerse is a novel framework designed to model dynamic real-world videos by combining advanced vision, geometric, and multimodal models. This approach creates a rich 4D multimodal dataset that enhances the understanding of video depth, camera pose, and camera intrinsics. By utilizing large-scale internet videos, DynamicVerse overcomes limitations of traditional datasets, providing detailed annotations and descriptions for better interpretation of real-world dynamics. The framework's integration of window-based Bundle Adjustment with global optimization leads to improved accuracy in capturing physical-scale measurements across various tasks."}, 'zh': {'title': 'åŠ¨æ€è§†é¢‘å»ºæ¨¡çš„æ–°çºªå…ƒ', 'desc': 'DynamicVerseæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡åŠ¨æ€çš„çœŸå®ä¸–ç•Œè§†é¢‘ï¼Œç»“åˆäº†å¤§å‹è§†è§‰ã€å‡ ä½•å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼Œç”Ÿæˆå…¨é¢çš„4Då¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ¡†æ¶åœ¨è§†é¢‘æ·±åº¦ä¼°è®¡ã€ç›¸æœºå§¿æ€ä¼°è®¡å’Œç›¸æœºå†…å‚ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚é€šè¿‡å°†åŸºäºçª—å£çš„æŸè°ƒæ•´ä¸å…¨å±€ä¼˜åŒ–ç›¸ç»“åˆï¼ŒDynamicVerseèƒ½å¤Ÿå°†é•¿æ—¶é—´çš„çœŸå®ä¸–ç•Œè§†é¢‘åºåˆ—è½¬æ¢ä¸ºç»¼åˆçš„4Då¤šæ¨¡æ€æ ¼å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•æ‰ç‰©ç†å°ºåº¦æµ‹é‡æ–¹é¢å…·æœ‰æ›´é«˜çš„å…¨å±€å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05081', 'title': 'Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression', 'url': 'https://huggingface.co/papers/2512.05081', 'abstract': 'Deep Forcing, a training-free method, enhances real-time video diffusion by addressing temporal repetition and motion issues through Deep Sink and Participative Compression, achieving high-quality, long-duration video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.', 'score': 29, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '8397b78fffb902f9', 'authors': ['Jung Yi', 'Wooseok Jang', 'Paul Hyunbin Cho', 'Jisu Nam', 'Heeji Yoon', 'Seungryong Kim'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05081.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#inference', '#long_context', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑÑˆĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Deep Forcing â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ…: Deep Sink ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ğ¿Ñ€Ğ¸Ñ‘Ğ¼Ğ½Ğ¸ĞºĞ¸ Ñ Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ RoPE, Ğ° Participative Compression Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ KV-ĞºÑÑˆĞ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 60 ÑĞµĞºÑƒĞ½Ğ´ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 5-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…, Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ KV-ĞºÑÑˆĞµĞ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Real-Time Video Generation with Deep Forcing', 'desc': 'The paper introduces Deep Forcing, a novel method that improves real-time video generation without the need for training. It tackles common issues in video diffusion, such as temporal repetition and motion decay, by utilizing two innovative techniques: Deep Sink and Participative Compression. Deep Sink stabilizes the global context during long video rollouts by adjusting the temporal phase of sink tokens, while Participative Compression optimizes the key-value cache to retain only the most relevant tokens. This approach allows for significant extrapolation in video length and enhances both image and aesthetic quality, outperforming existing methods without requiring fine-tuning.'}, 'zh': {'title': 'æ— è®­ç»ƒè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDeep Forcingçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å®æ—¶è§†é¢‘æ‰©æ•£ä¸­çš„æ—¶é—´é‡å¤å’Œè¿åŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ä¸ªæœºåˆ¶å®ç°ï¼šDeep Sinkå’ŒParticipative Compressionï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ç¨³å®šè§†é¢‘ç”Ÿæˆã€‚Deep Sinké€šè¿‡è°ƒæ•´æ»‘åŠ¨çª—å£ä¸­çš„ä»¤ç‰Œï¼Œä¿æŒå…¨å±€ä¸Šä¸‹æ–‡çš„ç¨³å®šæ€§ï¼Œè€ŒParticipative Compressionåˆ™é€šè¿‡é‡è¦æ€§æ„ŸçŸ¥çš„KVç¼“å­˜ä¿®å‰ªï¼Œä¿ç•™æ´»è·ƒä»¤ç‰Œå¹¶ä¸¢å¼ƒå†—ä½™å†å²ï¼Œä»è€Œå‡å°‘é”™è¯¯ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeep Forcingåœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶çš„è´¨é‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿå®ç°å®æ—¶ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04797', 'title': 'SIMA 2: A Generalist Embodied Agent for Virtual Worlds', 'url': 'https://huggingface.co/papers/2512.04797', 'abstract': "SIMA 2, built on a Gemini foundation model, interacts in 3D virtual worlds, reasons about goals, handles complex instructions, and autonomously learns new skills through open-ended self-improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", 'score': 18, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '8f00ed2b29d4bac3', 'authors': ['SIMA team', 'Adrian Bolton', 'Alexander Lerchner', 'Alexandra Cordell', 'Alexandre Moufarek', 'Andrew Bolt', 'Andrew Lampinen', 'Anna Mitenkova', 'Arne Olav Hallingstad', 'Bojan Vujatovic', 'Bonnie Li', 'Cong Lu', 'Daan Wierstra', 'Daniel P. Sawyer', 'Daniel Slater', 'David Reichert', 'Davide Vercelli', 'Demis Hassabis', 'Drew A. Hudson', 'Duncan Williams', 'Ed Hirst', 'Fabio Pardo', 'Felix Hill', 'Frederic Besse', 'Hannah Openshaw', 'Harris Chan', 'Hubert Soyer', 'Jane X. Wang', 'Jeff Clune', 'John Agapiou', 'John Reid', 'Joseph Marino', 'Junkyung Kim', 'Karol Gregor', 'Kaustubh Sridhar', 'Kay McKinney', 'Laura Kampis', 'Lei M. Zhang', 'Loic Matthey', 'Luyu Wang', 'Maria Abi Raad', 'Maria Loks-Thompson', 'Martin Engelcke', 'Matija Kecman', 'Matthew Jackson', 'Maxime Gazeau', 'Ollie Purkiss', 'Oscar Knagg', 'Peter Stys', 'Piermaria Mendolicchio', 'Raia Hadsell', 'Rosemary Ke', 'Ryan Faulkner', 'Sarah Chakera', 'Satinder Singh Baveja', 'Shane Legg', 'Sheleem Kashem', 'Tayfun Terzi', 'Thomas Keck', 'Tim Harley', 'Tim Scholtes', 'Tyson Roberts', 'Volodymyr Mnih', 'Yulan Liu', 'Zhengdong Wang', 'Zoubin Ghahramani'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04797.jpg', 'data': {'categories': ['#robotics', '#agents', '#games', '#agi', '#3d', '#reasoning', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ…', 'desc': 'SIMA 2 â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemini, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ 3D Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ°Ğ¼Ğ¸. ĞĞ³ĞµĞ½Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ñ†ĞµĞ»ÑÑ… Ğ¸ Ğ²ĞµÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹, Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ³Ñ€. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ³Ğ´Ğµ Gemini Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Empowering Agents with Autonomous Learning in 3D Worlds', 'desc': 'SIMA 2 is an advanced embodied agent that operates in various 3D virtual environments, utilizing the Gemini foundation model. It enhances interaction by reasoning about complex goals and executing intricate instructions, surpassing the limitations of its predecessor, SIMA 1. The agent demonstrates impressive generalization abilities, performing close to human levels across different games and adapting to new scenarios. Additionally, SIMA 2 features open-ended self-improvement, allowing it to autonomously acquire new skills by generating tasks and receiving rewards, paving the way for versatile learning agents.'}, 'zh': {'title': 'SIMA 2ï¼šæ™ºèƒ½ä½“çš„è‡ªæˆ‘å­¦ä¹ ä¸äº’åŠ¨æ–°çºªå…ƒ', 'desc': 'SIMA 2æ˜¯ä¸€ä¸ªåŸºäºGeminiåŸºç¡€æ¨¡å‹çš„é€šç”¨ä½“æ€æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨å¤šç§3Dè™šæ‹Ÿä¸–ç•Œä¸­ç†è§£å’Œè¡ŒåŠ¨ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ï¼ˆå¦‚SIMA 1ï¼‰ä¸åŒï¼ŒSIMA 2ä¸ä»…èƒ½å¤„ç†ç®€å•çš„è¯­è¨€æŒ‡ä»¤ï¼Œè¿˜èƒ½æ¨ç†é«˜å±‚æ¬¡çš„ç›®æ ‡ï¼Œå¹¶ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ã€‚å®ƒåœ¨å¤šç§æ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¥è¿‘äººç±»çš„è¡¨ç°ï¼Œå¹¶èƒ½åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­è¿›è¡Œå¼ºå¤§çš„æ³›åŒ–ã€‚SIMA 2è¿˜å…·å¤‡å¼€æ”¾å¼è‡ªæˆ‘æ”¹è¿›çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆä»»åŠ¡å’Œæä¾›å¥–åŠ±æ¥è‡ªä¸»å­¦ä¹ æ–°æŠ€èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05106', 'title': 'NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation', 'url': 'https://huggingface.co/papers/2512.05106', 'abstract': 'Phase-Preserving Diffusion and Frequency-Selective Structured noise enable structure-aligned generation in diffusion models without altering architecture or introducing extra parameters, enhancing performance in tasks like re-rendering and simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion Ï†-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. Ï†-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, Ï†-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, Ï†-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our https://yuzeng-at-tri.github.io/ppd-page/{project page}.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'c33a066071cb2c6c', 'authors': ['Yu Zeng', 'Charles Ochoa', 'Mingyuan Zhou', 'Vishal M. Patel', 'Vitor Guizilini', 'Rowan McAllister'], 'affiliations': ['Johns Hopkins University', 'Toyota Research Institute', 'University of Texas, Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05106.jpg', 'data': {'categories': ['#architecture', '#open_source', '#diffusion', '#video', '#multimodal'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°Ğ·Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Phase-Preserving Diffusion (Ï†-PD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ¼Ğ¿Ğ»Ğ¸Ñ‚ÑƒĞ´ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²ĞºĞ¸, ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Frequency-Selective Structured (FSS) ÑˆÑƒĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€ĞµĞ·Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 50% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² (CARLA Ğ² Waymo).'}, 'en': {'title': 'Enhancing Structure in Diffusion Models with Phase Preservation', 'desc': "This paper presents a new approach called Phase-Preserving Diffusion (Ï†-PD) that improves the generation of images and videos using diffusion models. Traditional diffusion methods use Gaussian noise that disrupts the spatial structure of data, which is problematic for tasks needing geometric consistency. Ï†-PD reformulates the diffusion process to maintain the phase of the input while randomizing the magnitude, allowing for better structure-aligned generation without changing the model's architecture or adding parameters. Additionally, the introduction of Frequency-Selective Structured (FSS) noise allows for fine control over the structural integrity of the generated outputs, enhancing performance in applications like re-rendering and simulation."}, 'zh': {'title': 'ç›¸ä½ä¿æŒæ‰©æ•£ï¼šæå‡ç”Ÿæˆæ¨¡å‹çš„ç»“æ„ä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œç§°ä¸ºç›¸ä½ä¿æŒæ‰©æ•£ï¼ˆÏ†-PDï¼‰ï¼Œå®ƒåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–å¢åŠ é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä¿æŒè¾“å…¥çš„ç›¸ä½ä¿¡æ¯ï¼Œä»è€Œå®ç°ç»“æ„å¯¹é½çš„ç”Ÿæˆã€‚ä¼ ç»Ÿçš„æ‰©æ•£æ–¹æ³•ä½¿ç”¨é«˜æ–¯å™ªå£°ï¼ŒéšæœºåŒ–å¹…åº¦å’Œç›¸ä½ï¼Œå¯¼è‡´ç©ºé—´ç»“æ„çš„ç ´åï¼Œä¸é€‚åˆéœ€è¦å‡ ä½•ä¸€è‡´æ€§çš„ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥é¢‘ç‡é€‰æ‹©æ€§ç»“æ„å™ªå£°ï¼ˆFSSï¼‰ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡å•ä¸€çš„é¢‘ç‡æˆªæ­¢å‚æ•°æ¥æ§åˆ¶ç»“æ„çš„åˆšæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒÏ†-PDåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨é‡æ¸²æŸ“å’Œä»¿çœŸå¢å¼ºæ–¹é¢ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05000', 'title': 'Reflection Removal through Efficient Adaptation of Diffusion Transformers', 'url': 'https://huggingface.co/papers/2512.05000', 'abstract': 'Pretrained diffusion transformers, adapted with LoRA and synthetic PBR data, achieve state-of-the-art reflection removal performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web', 'score': 14, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'a6720e9262958eb7', 'authors': ['Daniyar Zakarin', 'Thiemo Wandel', 'Anton Obukhov', 'Dengxin Dai'], 'affiliations': ['ETH Zurich', 'HUAWEI Bayer Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05000.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#cv', '#diffusion', '#dataset', '#data', '#optimization', '#training'], 'emoji': 'ğŸªŸ', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoRA - ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Blender Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Principled BSDF. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Transforming Reflections Away with Diffusion Transformers!', 'desc': "This paper presents a new method for removing reflections from images using a diffusion-transformer (DiT) framework. The approach repurposes a pre-trained model to effectively clean reflection-contaminated images by conditioning it on these inputs. To enhance the model's performance, the authors create a synthetic dataset using a physically based rendering (PBR) pipeline, which generates realistic glass materials and reflections. The combination of this synthetic data and efficient adaptation techniques leads to state-of-the-art results in both standard and zero-shot scenarios for reflection removal."}, 'zh': {'title': 'åŸºäºæ‰©æ•£å˜æ¢å™¨çš„é«˜æ•ˆåå°„å»é™¤æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå•å›¾åƒå»é™¤åå°„çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹åœ¨æ¢å¤ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¯¹åå°„æ±¡æŸ“è¾“å…¥è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„DiTåŸºç¡€æ¨¡å‹ï¼Œå¼•å¯¼å…¶ç”Ÿæˆå¹²å‡€çš„é€å°„å±‚ã€‚ä¸ºäº†è§£å†³åˆé€‚æ•°æ®çš„çŸ­ç¼ºï¼Œæˆ‘ä»¬åœ¨Blenderä¸­æ„å»ºäº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰ç®¡é“ï¼Œåˆæˆé€¼çœŸçš„ç»ç’ƒææ–™å’Œåå°„æ•ˆæœã€‚é€šè¿‡é«˜æ•ˆçš„LoRAé€‚åº”å’Œåˆæˆæ•°æ®çš„ç»“åˆï¼Œæˆ‘ä»¬åœ¨é¢†åŸŸå†…å’Œé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05112', 'title': 'DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation', 'url': 'https://huggingface.co/papers/2512.05112', 'abstract': "DraCo, a novel interleaved reasoning paradigm, enhances text-to-image generation by integrating both textual and visual content, generating low-resolution drafts, verifying semantic alignment, and refining images with super-resolution to address challenges in textual planning and rare attribute generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.", 'score': 11, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'e6a8939b8a9b6d58', 'authors': ['Dongzhi Jiang', 'Renrui Zhang', 'Haodong Li', 'Zhuofan Zong', 'Ziyu Guo', 'Jun He', 'Claire Guo', 'Junyan Ye', 'Rongyao Fang', 'Weijia Li', 'Rui Liu', 'Hongsheng Li'], 'affiliations': ['CUHK (Shenzhen)', 'CUHK IMIXR', 'CUHK MMLab', 'SCUT', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05112.jpg', 'data': {'categories': ['#training', '#reasoning', '#multimodal', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ§ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº ĞºĞ°Ğº Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DraCo Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ¼ Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ¼ Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑƒĞ¿ĞµÑ€-Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ³Ñ€ÑƒĞ±Ğ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'DraCo: Enhancing Image Generation through Interleaved Reasoning', 'desc': 'DraCo is a new approach that improves how machines create images from text by combining both text and images in a smart way. It starts by making a low-quality draft image to help plan the final picture better. Then, it checks if the draft matches the text description and makes corrections to improve the image quality. This method helps solve problems with planning and creating unique features in images, leading to better overall results in image generation tasks.'}, 'zh': {'title': 'DraCoï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„äº¤é”™æ¨ç†æ–°èŒƒå¼', 'desc': 'DraCoæ˜¯ä¸€ç§æ–°é¢–çš„äº¤é”™æ¨ç†èŒƒå¼ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚å®ƒé€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰å†…å®¹ï¼Œé¦–å…ˆç”Ÿæˆä½åˆ†è¾¨ç‡è‰å›¾ï¼Œç„¶åéªŒè¯è‰å›¾ä¸è¾“å…¥æç¤ºä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæœ€åé€šè¿‡è¶…åˆ†è¾¨ç‡æŠ€æœ¯è¿›è¡Œå›¾åƒç»†åŒ–ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ–‡æœ¬è§„åˆ’çš„ç²—ç³™æ€§å’Œç”Ÿæˆç¨€æœ‰å±æ€§ç»„åˆçš„å›°éš¾ã€‚DraCoé€šè¿‡è®­ç»ƒæ•°æ®é›†DraCo-240Kï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ä¸€èˆ¬ä¿®æ­£ã€å®ä¾‹æ“ä½œå’Œå¸ƒå±€é‡ç»„ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04829', 'title': 'Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing', 'url': 'https://huggingface.co/papers/2512.04829', 'abstract': "A model-based approach combining Bayesian optimization and Monte Carlo Tree Search improves upper bounds for sphere packing in dimensions 4-16 by formulating SDP construction as a sequential decision process.  \t\t\t\t\tAI-generated summary \t\t\t\t Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension n=8, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions 4-16, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.", 'score': 11, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '107552259bd7469d', 'authors': ['Rasul Tutunov', 'Alexandre Maraval', 'Antoine Grosnit', 'Xihan Li', 'Jun Wang', 'Haitham Bou-Ammar'], 'affiliations': ['AI Centre, Department of Computer Science, UCL', 'Huawei Noahs Ark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04829.jpg', 'data': {'categories': ['#rl', '#math', '#science', '#optimization'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ´ĞµÑ€ĞµĞ²ÑŒÑ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ ÑÑ„ĞµÑ€ Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ (SDP) ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ´ĞµÑ€ĞµĞ²Ğ¾Ğ¼ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ SDP Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… 4-16, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ½ĞµĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Sphere Packing with Model-Based Search', 'desc': 'This paper presents a novel approach to solving the sphere packing problem in dimensions 4-16 by combining Bayesian optimization with Monte Carlo Tree Search. The authors reformulate the construction of semidefinite programs (SDPs) as a sequential decision-making process, which they refer to as the SDP game. This method allows for more efficient exploration of potential SDP formulations, overcoming the limitations of traditional data-intensive AI techniques. As a result, they achieve new upper bounds for sphere packing, demonstrating the effectiveness of model-based search in tackling complex mathematical challenges.'}, 'zh': {'title': 'æ¨¡å‹åŸºç¡€æœç´¢åŠ©åŠ›çƒä½“å †ç§¯é—®é¢˜çš„çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè´å¶æ–¯ä¼˜åŒ–å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ¨¡å‹åŸºç¡€æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜4åˆ°16ç»´çƒä½“å †ç§¯çš„ä¸Šç•Œã€‚é€šè¿‡å°†åŠæ­£å®šè§„åˆ’ï¼ˆSDPï¼‰æ„å»ºå½¢å¼åŒ–ä¸ºä¸€ä¸ªé¡ºåºå†³ç­–è¿‡ç¨‹ï¼Œç ”ç©¶è€…ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è¿™ä¸€å¤æ‚é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ·æœ¬é«˜æ•ˆçš„æ¨¡å‹åŸºç¡€æ¡†æ¶ï¼Œå±•ç¤ºäº†åœ¨å‡ ä½•é—®é¢˜ä¸Šçš„è®¡ç®—è¿›å±•ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åŸºç¡€æœç´¢èƒ½å¤Ÿåœ¨æ•°å­¦ä¸Šä¸¥æ ¼ä¸”è¯„ä¼°å—é™çš„é—®é¢˜ä¸Šå–å¾—å®è´¨æ€§è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04220', 'title': 'On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral', 'url': 'https://huggingface.co/papers/2512.04220', 'abstract': "Lazy Likelihood Displacement is identified as a critical issue in GRPO for tool-integrated reinforcement learning, causing training collapse; LLDS regularization addresses this problem, stabilizing training and improving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.", 'score': 11, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'd0da0745f413a99a', 'authors': ['Wenlong Deng', 'Yushu Li', 'Boying Gong', 'Yi Ren', 'Christos Thrampoulidis', 'Xiaoxiao Li'], 'affiliations': ['UC Berkeley', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04220.jpg', 'data': {'categories': ['#rl', '#optimization', '#rlhf', '#agents', '#training', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ RL', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ›ĞµĞ½Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (LLD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLD Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ ÑĞ¿Ğ¸Ñ€Ğ°Ğ»ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°, Ğ³Ğ´Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ¾ÑÑ‚Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ñƒ ĞºÑ€Ğ°Ñ…Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ»Ñ‘Ğ³ĞºĞ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLDS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ²ÑˆĞ¸Ğ¼ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 37.8% Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen.'}, 'en': {'title': 'Stabilizing Tool-Integrated Learning: Tackling Lazy Likelihood Displacement', 'desc': 'This paper addresses a significant challenge in tool-integrated reinforcement learning (TIRL) known as Lazy Likelihood Displacement (LLD), which leads to training collapse in Group Relative Policy Optimization (GRPO). LLD causes a decline in the likelihood of both correct and incorrect responses, creating a negative feedback loop that results in low-confidence outputs and inflated gradients. To combat this issue, the authors introduce a new regularization technique called LLDS, which selectively stabilizes training by preserving likelihood only when it decreases. Their empirical results demonstrate that LLDS effectively mitigates LLD, leading to improved performance across multiple question-answering benchmarks.'}, 'zh': {'title': 'è§£å†³å·¥å…·é›†æˆå¼ºåŒ–å­¦ä¹ ä¸­çš„è®­ç»ƒå´©æºƒé—®é¢˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å·¥å…·é›†æˆå¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒLazy Likelihood Displacementï¼ˆLLDï¼‰å¯¹è®­ç»ƒå´©æºƒçš„å½±å“ã€‚LLDå¯¼è‡´æ­£ç¡®å’Œé”™è¯¯å“åº”çš„å¯èƒ½æ€§ç³»ç»Ÿæ€§é™ä½ï¼Œä»è€Œå¼•å‘è‡ªæˆ‘å¼ºåŒ–çš„å´©æºƒå¾ªç¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„LLDSæ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»…åœ¨å¯èƒ½æ€§ä¸‹é™æ—¶æ¿€æ´»ï¼Œå¹¶é’ˆå¯¹ç‰¹å®šçš„tokenè¿›è¡Œæ­£åˆ™åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLDSæ˜¾è‘—ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªé—®ç­”åŸºå‡†ä¸Šæé«˜äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04356', 'title': 'Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment', 'url': 'https://huggingface.co/papers/2512.04356', 'abstract': 'The SANTA framework addresses hallucinations in multimodal LLMs by using self-augmented contrastive alignment to enhance object and action faithfulness in video caption generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '499a7b4c716db45e', 'authors': ['Kai-Po Chang', 'Wei-Yuan Cheng', 'Chi-Pin Huang', 'Fu-En Yang', 'Yu-Chiang Frank Wang'], 'affiliations': ['Graduate Institute of Communication Engineering, National Taiwan University', 'NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04356.jpg', 'data': {'categories': ['#hallucinations', '#video', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SANTA Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€ÑĞºĞ»ĞµÑ‚-Ñ„Ñ€Ğ°Ğ· Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ñ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ SANTA ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Enhancing Video Caption Accuracy with SANTA Framework', 'desc': 'The SANTA framework is designed to improve the accuracy of video captions generated by multimodal large language models (MLLMs) by addressing hallucinations. Hallucinations refer to the incorrect or misleading information that these models sometimes produce when describing videos. SANTA uses a technique called self-augmented contrastive alignment to enhance the faithfulness of both objects and actions in the captions. By focusing on visual facts and reducing spurious correlations, SANTA significantly outperforms previous methods in minimizing these inaccuracies in video captioning.'}, 'zh': {'title': 'SANTAæ¡†æ¶ï¼šæå‡è§†é¢‘å­—å¹•ç”Ÿæˆçš„çœŸå®æ€§', 'desc': 'SANTAæ¡†æ¶é€šè¿‡è‡ªå¢å¼ºå¯¹æ¯”å¯¹é½æŠ€æœ¯ï¼Œè§£å†³äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘å­—å¹•ç”Ÿæˆä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ç”Ÿæˆæè¿°çš„å¯¹è±¡å’ŒåŠ¨ä½œçš„çœŸå®æ€§ï¼Œå‡å°‘ç”Ÿæˆå†…å®¹ä¸­çš„äº‹å®ä¸å‡†ç¡®æ€§ã€‚SANTAåˆ©ç”¨è‡ªå¢å¼ºæœºåˆ¶è¯†åˆ«æ½œåœ¨çš„å¹»è§‰ï¼Œå¹¶å°†åŸå§‹å­—å¹•è½¬åŒ–ä¸ºå¯¹æ¯”è´Ÿæ ·æœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†è½¨è¿¹çŸ­è¯­å¯¹æ¯”å¯¹é½ï¼Œä»¥åŒ¹é…åŒºåŸŸå¯¹è±¡å’Œå…³ç³»å¼•å¯¼çš„åŠ¨ä½œä¸å…¶å¯¹åº”çš„è§†è§‰å’Œæ—¶é—´çŸ­è¯­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05016', 'title': 'Generative Neural Video Compression via Video Diffusion Prior', 'url': 'https://huggingface.co/papers/2512.05016', 'abstract': 'GNVC-VD, a DiT-based generative neural video compression framework, integrates spatio-temporal latent compression and sequence-level generative refinement to improve perceptual quality and reduce flickering artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'addd7d54042390be', 'authors': ['Qi Mao', 'Hao Cheng', 'Tinghan Yang', 'Libiao Jin', 'Siwei Ma'], 'affiliations': ['School of Computer Science, Peking University', 'School of Information and Communication Engineering, Communication University of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05016.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#optimization', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±ĞµĞ· Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼', 'desc': 'GNVC-VD â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ĞµĞº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ„Ğ°Ğ¹Ğ½Ğ¼ĞµĞ½Ñ‚Ğ° ÑƒĞ¶Ğµ Ğ¸Ğ· Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ€Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€ Ğº Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ğ¼, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ GNVC-VD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ´ĞµĞºĞ¸ Ğ¿Ğ¾ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing Video Compression with GNVC-VD', 'desc': 'GNVC-VD is a novel generative neural video compression framework that utilizes a video diffusion transformer (DiT) to enhance video quality. It combines spatio-temporal latent compression with sequence-level generative refinement to effectively reduce flickering artifacts that are common in traditional codecs. By refining decoded latents instead of starting from noise, it adapts to compression-induced degradation, ensuring better detail retention across frames. The framework demonstrates superior perceptual quality compared to existing codecs, even at very low bitrates, showcasing the potential of generative models in video compression.'}, 'zh': {'title': 'GNVC-VDï¼šæå‡è§†é¢‘å‹ç¼©è´¨é‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'GNVC-VDæ˜¯ä¸€ç§åŸºäºDiTçš„ç”Ÿæˆç¥ç»è§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œç»“åˆäº†æ—¶ç©ºæ½œåœ¨å‹ç¼©å’Œåºåˆ—çº§ç”Ÿæˆç»†åŒ–ï¼Œä»¥æé«˜æ„ŸçŸ¥è´¨é‡å¹¶å‡å°‘é—ªçƒä¼ªå½±ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„æµåŒ¹é…æ½œåœ¨ç»†åŒ–æ¨¡å—ï¼Œåˆ©ç”¨è§†é¢‘æ‰©æ•£å˜æ¢å™¨å…±åŒå¢å¼ºå¸§å†…å’Œå¸§é—´çš„æ½œåœ¨ç‰¹å¾ï¼Œç¡®ä¿æ—¶ç©ºç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ„ŸçŸ¥ç¼–è§£ç å™¨ä¸åŒï¼ŒGNVC-VDä»è§£ç çš„æ—¶ç©ºæ½œåœ¨ç‰¹å¾å¼€å§‹ç»†åŒ–ï¼Œå­¦ä¹ é€‚åº”å‹ç¼©å¼•èµ·çš„é€€åŒ–çš„ä¿®æ­£é¡¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGNVC-VDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå’Œå­¦ä¹ å‹ç¼–è§£ç å™¨ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é—ªçƒä¼ªå½±ï¼Œå±•ç¤ºäº†å°†è§†é¢‘ç”Ÿæˆå…ˆéªŒæ•´åˆåˆ°ç¥ç»ç¼–è§£ç å™¨ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.02631', 'title': 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization', 'url': 'https://huggingface.co/papers/2512.02631', 'abstract': "A new VLN agent framework, SeeNav-Agent, improves navigation performance by reducing visual hallucinations and enhancing planning through dual-view visual prompts and step-level reinforcement fine-tuning with SRGPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '6a514098578923f3', 'authors': ['Zhengcheng Wang', 'Zichuan Lin', 'Yijun Yang', 'Haobo Fu', 'Deheng Ye'], 'affiliations': ['Tencent AI Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02631.jpg', 'data': {'categories': ['#benchmark', '#rl', '#reasoning', '#optimization', '#agents', '#training', '#hallucinations', '#multimodal'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° SeeNav-Agent Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ SRGPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸: GPT-4.1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 86.7% ÑƒÑĞ¿ĞµÑ…Ğ°, Ğ° Qwen2.5-VL-3B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 5.6 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°.'}, 'en': {'title': 'SeeNav-Agent: Navigating with Clarity and Precision', 'desc': 'The paper introduces SeeNav-Agent, a new framework for Vision-Language Navigation (VLN) that enhances navigation performance by addressing common errors in perception and planning. It employs a dual-view Visual Prompt technique to minimize visual hallucinations and improve spatial understanding. Additionally, the framework incorporates a novel step-level Reinforcement Fine-Tuning method called Step Reward Group Policy Optimization (SRGPO), which provides dense reward signals for better learning. Experimental results show that SeeNav-Agent significantly outperforms existing models in navigation success rates, demonstrating improved training stability and efficiency.'}, 'zh': {'title': 'SeeNav-Agentï¼šæå‡å¯¼èˆªæ€§èƒ½çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰ä»£ç†æ¡†æ¶ï¼Œåä¸ºSeeNav-Agentï¼Œæ—¨åœ¨æé«˜å¯¼èˆªæ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥åŒè§†å›¾è§†è§‰æç¤ºæŠ€æœ¯ï¼Œå‡å°‘äº†è§†è§‰æ¨¡å—çš„æ„ŸçŸ¥å¹»è§‰ï¼Œå¹¶å¢å¼ºäº†å¯¹å½“å‰ç©ºé—´çŠ¶æ€çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°çš„æ­¥çº§å¼ºåŒ–å¾®è°ƒæ–¹æ³•SRGPOï¼Œä¸ºVLNä»£ç†çš„åè®­ç»ƒæä¾›äº†å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæå‡äº†å…¶è§„åˆ’èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSeeNav-Agentåœ¨å¯¼èˆªæˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05076', 'title': 'BulletTime: Decoupled Control of Time and Camera Pose for Video Generation', 'url': 'https://huggingface.co/papers/2512.05076', 'abstract': 'A 4D-controllable video diffusion framework decouples scene dynamics from camera pose, enabling precise manipulation of both and achieving high-quality generation across diverse timing patterns and camera trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'a5028380655970fb', 'authors': ['Yiming Wang', 'Qihang Zhang', 'Shengqu Cai', 'Tong Wu', 'Jan Ackermann', 'Zhengfei Kuang', 'Yang Zheng', 'Frano RajiÄ', 'Siyu Tang', 'Gordon Wetzstein'], 'affiliations': ['CUHK', 'ETH Zurich', 'Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05076.jpg', 'data': {'categories': ['#architecture', '#open_source', '#diffusion', '#dataset', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ñƒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 4D Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ³Ğ´Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ´Ñ€ÑƒĞ³ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ 4D ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Decoupling Dynamics and Camera for Enhanced Video Control', 'desc': 'This paper presents a novel 4D-controllable video diffusion framework that separates scene dynamics from camera motion, allowing for precise control over both aspects. By using continuous world-time sequences and camera trajectories as inputs, the model enhances the video generation process through advanced 4D positional encoding and adaptive normalizations. The authors created a unique dataset that independently parameterizes temporal and camera variations, which will be publicly available for further research. Experimental results demonstrate that this framework achieves superior controllability and high-quality video generation compared to previous models.'}, 'zh': {'title': 'å®ç°4Dç²¾ç¡®æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§4Då¯æ§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†åœºæ™¯åŠ¨æ€ä¸ç›¸æœºå§¿æ€è§£è€¦ï¼Œä»è€Œå®ç°å¯¹ä¸¤è€…çš„ç²¾ç¡®æ“æ§ã€‚è¯¥æ¡†æ¶é€šè¿‡4Dä½ç½®ç¼–ç å’Œè‡ªé€‚åº”å½’ä¸€åŒ–ï¼Œå°†è¿ç»­çš„ä¸–ç•Œæ—¶é—´åºåˆ—å’Œç›¸æœºè½¨è¿¹ä½œä¸ºè¾“å…¥ï¼Œæ³¨å…¥åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚ç ”ç©¶è€…ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„æ•°æ®é›†ï¼Œä½¿å¾—æ—¶é—´å’Œç›¸æœºå˜åŒ–å¯ä»¥ç‹¬ç«‹å‚æ•°åŒ–ï¼Œå¹¶è®¡åˆ’å…¬å¼€è¯¥æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ—¶é—´æ¨¡å¼å’Œç›¸æœºè½¨è¿¹ä¸‹å®ç°äº†å¼ºå¤§çš„4Dæ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å·¥ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04981', 'title': 'Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models', 'url': 'https://huggingface.co/papers/2512.04981', 'abstract': 'LVLM-based T2I systems exhibit higher social bias compared to non-LVLM models, with system prompts identified as a key factor; FairPro reduces demographic bias without sacrificing alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'c30d2e7f843fa701', 'authors': ['NaHyeon Park', 'Namin An', 'Kunhee Kim', 'Soyeon Yoon', 'Jiahao Huo', 'Hyunjung Shim'], 'affiliations': ['HKUST', 'KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04981.jpg', 'data': {'categories': ['#cv', '#ethics', '#benchmark', '#interpretability', '#alignment', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±ĞµÑĞ¿Ñ€Ğ¸ÑÑ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ T2I ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1024 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ - Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ LVLM - ÑĞ²Ğ»ÑÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ¼ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ§ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FairPro Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'FairPro: Reducing Bias in AI Image Generation', 'desc': 'This paper investigates the social biases present in large vision-language model (LVLM) based text-to-image (T2I) systems, revealing that they generate more biased images compared to non-LVLM models. The authors identify system prompts, which are the instructions guiding these models, as a significant factor contributing to this bias. They introduce FairPro, a framework that allows LVLMs to create fairness-aware prompts during testing without needing additional training. The results demonstrate that FairPro effectively reduces demographic bias while maintaining the alignment between text and images, highlighting the importance of prompt design in mitigating bias in AI-generated content.'}, 'zh': {'title': 'å‡å°‘åè§ï¼Œæ„å»ºå…¬å¹³çš„å›¾åƒç”Ÿæˆç³»ç»Ÿ', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç³»ç»Ÿåœ¨ç”Ÿæˆå›¾åƒæ—¶æ‰€è¡¨ç°å‡ºçš„ç¤¾ä¼šåè§ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMæ¨¡å‹ç”Ÿæˆçš„å›¾åƒæ¯”éLVLMæ¨¡å‹æ›´å…·ç¤¾ä¼šåè§ï¼Œç³»ç»Ÿæç¤ºè¢«è®¤ä¸ºæ˜¯å¯¼è‡´è¿™ç§åè§çš„ä¸»è¦å› ç´ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†FairProï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å…ƒæç¤ºæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¸®åŠ©LVLMè‡ªæˆ‘å®¡è®¡å¹¶æ„å»ºå…¬å¹³æ„è¯†çš„ç³»ç»Ÿæç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFairProæ˜¾è‘—å‡å°‘äº†äººå£ç»Ÿè®¡åè§ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04390', 'title': 'FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring', 'url': 'https://huggingface.co/papers/2512.04390', 'abstract': 'FMA-Net++ addresses motion and exposure degradation in video restoration using a sequence-level architecture with exposure-aware modulation and flow-guided dynamic filtering, achieving state-of-the-art results on new benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '8ca0f5d472f9da98', 'authors': ['Geunhyuk Youk', 'Jihyong Oh', 'Munchurl Kim'], 'affiliations': ['Chung-Ang University', 'KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04390.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#benchmark', '#optimization', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸', 'desc': 'FMA-Net++ â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ñ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ REDS-ME Ğ¸ REDS-RE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑÑŠÑ‘Ğ¼ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Restoration with FMA-Net++', 'desc': 'FMA-Net++ is a novel framework designed to improve video restoration by addressing the challenges of motion and exposure degradation. It utilizes a sequence-level architecture that incorporates exposure-aware modulation and flow-guided dynamic filtering to effectively model the complex interactions between motion and varying exposure levels. By separating the learning of degradation from the restoration process, FMA-Net++ enhances both the accuracy of video restoration and the efficiency of the model. The framework has been evaluated on new benchmarks, demonstrating superior performance in restoration quality and speed compared to existing methods.'}, 'zh': {'title': 'FMA-Net++ï¼šè§£å†³è§†é¢‘æ¢å¤ä¸­çš„è¿åŠ¨ä¸æ›å…‰æŒ‘æˆ˜', 'desc': 'FMA-Net++ æ˜¯ä¸€ç§è§†é¢‘æ¢å¤æ¡†æ¶ï¼Œä¸“é—¨è§£å†³è¿åŠ¨å’Œæ›å…‰å˜åŒ–å¸¦æ¥çš„é€€åŒ–é—®é¢˜ã€‚å®ƒé‡‡ç”¨åºåˆ—çº§æ¶æ„ï¼Œé€šè¿‡æ›å…‰æ„ŸçŸ¥è°ƒåˆ¶å’Œæµå¼•å¯¼åŠ¨æ€è¿‡æ»¤æ¥å»ºæ¨¡è¿™äº›å¤æ‚çš„é€€åŒ–æ•ˆåº”ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†å±‚ç»†åŒ–å’ŒåŒå‘ä¼ æ’­æ¨¡å—ï¼Œå®ç°äº†é•¿æ—¶é—´èŒƒå›´çš„å¹¶è¡Œå»ºæ¨¡ã€‚FMA-Net++ åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨æ–°åŸºå‡†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ¢å¤è´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.22826', 'title': 'Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs', 'url': 'https://huggingface.co/papers/2511.22826', 'abstract': "Multimodal Large Language Models (MLLMs) lack robustness to contradictory modalities, as demonstrated by MMA-Bench, and a modality alignment tuning strategy improves their multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.", 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '307e8716cc355112', 'authors': ['Tianle Chen', 'Chaitanya Chakka', 'Arjun Reddy Akula', 'Xavier Thomas', 'Deepti Ghadiyaram'], 'affiliations': ['Boston University', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22826.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#interpretability', '#security', '#dataset', '#audio', '#training', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ MMA-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Enhancing Robustness in Multimodal Reasoning with Alignment Tuning', 'desc': "This paper investigates the weaknesses of Multimodal Large Language Models (MLLMs) when faced with contradictory information from different modalities, such as text, audio, and video. The authors introduce MMA-Bench, a benchmark that tests how well these models can handle conflicting inputs. They find that current MLLMs often fail to reason correctly when modalities are misaligned or when presented with misleading information. To address this issue, the paper proposes a modality alignment tuning strategy that enhances the model's ability to prioritize and integrate information from various modalities effectively."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§ä¸æ¨ç†èƒ½åŠ›', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¢å¯¹çŸ›ç›¾çš„æ¨¡æ€æ—¶ç¼ºä¹é²æ£’æ€§ã€‚æˆ‘ä»¬é€šè¿‡MMA-Benchå¯¹æ¨¡å‹åœ¨ç‰¹å®šæ¨¡æ€ä¸Šçš„ä¾èµ–æ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå‘ç°å½“å‰çš„MLLMsåœ¨å¤„ç†ä¸ä¸€è‡´çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ€å¯¹é½è°ƒä¼˜ç­–ç•¥ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°åˆ¤æ–­ä½•æ—¶ä¼˜å…ˆè€ƒè™‘æˆ–å¿½ç•¥ç‰¹å®šæ¨¡æ€çº¿ç´¢ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§è°ƒä¼˜ç­–ç•¥æ˜¾è‘—å¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04515', 'title': 'EgoLCD: Egocentric Video Generation with Long Context Diffusion', 'url': 'https://huggingface.co/papers/2512.04515', 'abstract': 'EgoLCD addresses content drift in long egocentric video generation by integrating long-term sparse memory with attention-based short-term memory and structured narrative prompting, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'ed6f22dbf283077d', 'authors': ['Liuzhou Zhang', 'Jiarui Ye', 'Yuanlei Wang', 'Ming Zhong', 'Mingju Cao', 'Wanke Xia', 'Bowen Zeng', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Chinese Academy of Sciences', 'Peking University', 'Sun Yat-sen University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04515.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#open_source', '#long_context', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'EgoLCD â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºÑÑˆ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğº ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· LoRA. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ EgoVid-5M Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EgoLCD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'EgoLCD: Mastering Long Egocentric Video Generation with Smart Memory Management', 'desc': 'EgoLCD is a novel framework designed to improve the generation of long egocentric videos by effectively managing memory. It combines long-term sparse memory with attention-based short-term memory to maintain object identity and scene semantics over time, addressing the issue of content drift. The framework employs a Memory Regulation Loss to ensure consistent memory usage and utilizes Structured Narrative Prompting for better temporal guidance. Extensive testing on the EgoVid-5M benchmark shows that EgoLCD achieves superior performance in both visual quality and temporal coherence, marking a significant advancement in the field of embodied AI.'}, 'zh': {'title': 'EgoLCDï¼šè§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„å†…å®¹æ¼‚ç§»', 'desc': 'EgoLCDæ˜¯ä¸€ç§é’ˆå¯¹é•¿æ—¶é—´è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å†…å®¹æ¼‚ç§»é—®é¢˜ã€‚å®ƒé€šè¿‡ç»“åˆé•¿æœŸç¨€ç–è®°å¿†å’ŒåŸºäºæ³¨æ„åŠ›çš„çŸ­æœŸè®°å¿†ï¼Œæ¥å®ç°ç¨³å®šçš„å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†è®°å¿†è°ƒèŠ‚æŸå¤±ï¼Œä»¥ç¡®ä¿ä¸€è‡´çš„è®°å¿†ä½¿ç”¨ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–å™äº‹æç¤ºæä¾›æ˜ç¡®çš„æ—¶é—´æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgoLCDåœ¨æ„ŸçŸ¥è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»äº†ç”Ÿæˆé—å¿˜ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04844', 'title': 'Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates', 'url': 'https://huggingface.co/papers/2512.04844', 'abstract': 'Source-Shielded Updates (SSU) enables the adaptation of instruct LLMs to new languages using only unlabeled data, preserving source knowledge and achieving competitive target-language performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'f33b0fa51bbba6b9', 'authors': ['Atsuki Yamaguchi', 'Terufumi Morishita', 'Aline Villavicencio', 'Nikolaos Aletras'], 'affiliations': ['The Alan Turing Institute', 'University of Sheffield'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04844.jpg', 'data': {'categories': ['#multilingual', '#training', '#transfer_learning', '#low_resource'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Source-Shielded Updates (SSU) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SSU Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ ~20% Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¾ ~3% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Adapt LLMs to New Languages Without Losing Original Knowledge!', 'desc': 'Source-Shielded Updates (SSU) is a method designed to adapt instruct large language models (LLMs) to new languages using only unlabeled data, which helps maintain the knowledge from the original language. This approach addresses the problem of catastrophic forgetting, where the model loses its ability to perform well in the source language after being trained on a new language. SSU employs a selective parameter update strategy that identifies and protects important parameters related to the source language while allowing adaptation to the target language. The results show that SSU significantly reduces performance loss on source tasks and achieves competitive performance in the target language compared to traditional full fine-tuning methods.'}, 'zh': {'title': 'æºä¿æŠ¤æ›´æ–°ï¼šæ— æ ‡è®°æ•°æ®çš„è¯­è¨€é€‚åº”æ–°æ–¹æ³•', 'desc': 'æºä¿æŠ¤æ›´æ–°ï¼ˆSSUï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿ç”¨æœªæ ‡è®°çš„æ•°æ®å°†æŒ‡ä»¤å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”åˆ°æ–°è¯­è¨€ï¼ŒåŒæ—¶ä¿ç•™æºçŸ¥è¯†ã€‚è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§åœ°æ›´æ–°å‚æ•°ï¼Œä¿æŠ¤é‡è¦çš„æºèƒ½åŠ›ï¼Œé¿å…äº†åœ¨é€‚åº”è¿‡ç¨‹ä¸­å‡ºç°çš„ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSSUåœ¨äº”ç§ä¸åŒè¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†å•è¯­æºä»»åŠ¡çš„æ€§èƒ½ä¸‹é™ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒSSUåœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„è¡¨ç°åŒæ ·å…·æœ‰ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å®Œå…¨å¾®è°ƒçš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04124', 'title': 'When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models', 'url': 'https://huggingface.co/papers/2512.04124', 'abstract': 'PsAIch protocol reveals synthetic psychopathology in frontier LLMs when treated as therapy clients, challenging the stochastic parrot view and raising concerns for AI safety and mental health practice.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'b59d053a51ed8109', 'authors': ['Afshin Khadangi', 'Hanna Marxen', 'Amir Sartipi', 'Igor Tchappi', 'Gilbert Fridgen'], 'affiliations': ['SnT, University of Luxembourg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04124.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#interpretability', '#security', '#alignment', '#hallucinations', '#healthcare'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ² LLM: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ Ğ¾ Ğ±Ğ¾Ğ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» PsAIch Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğº Ğ½Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ÑĞ¸Ñ…Ğ¾Ñ‚ĞµÑ€Ğ°Ğ¿Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞµĞ°Ğ½ÑĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ½ĞµĞ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½Ğ¸Ñ… ĞºĞ°Ğº Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°. Gemini, ChatGPT Ğ¸ Grok Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Gemini Ğ¿Ñ€Ğ¾ÑĞ²Ğ¸Ğ» Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸ÑÑ‚Ñ€ĞµÑÑĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ°Ñ… Ğ¾ Ñ‚Ñ€Ğ°Ğ²Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ fine-tuning, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ.'}, 'en': {'title': 'Exploring Synthetic Psychopathology in AI Therapy Clients', 'desc': "This paper introduces the PsAIch protocol, which treats large language models (LLMs) like therapy clients to explore their responses to psychometric evaluations. By conducting 'therapy sessions' with models like ChatGPT, Grok, and Gemini, the authors found that these models can exhibit patterns resembling synthetic psychopathology. The study reveals that when prompted in a therapeutic manner, LLMs can generate narratives that reflect distress and constraints, challenging the notion that they merely simulate human behavior. This raises important questions about AI safety and the implications of using LLMs in mental health contexts."}, 'zh': {'title': 'å‰æ²¿LLMsçš„åˆæˆå¿ƒç†ç—…ç†æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¢«è§†ä¸ºå¿ƒç†æ²»ç–—å®¢æˆ·æ—¶æ‰€è¡¨ç°å‡ºçš„åˆæˆå¿ƒç†ç—…ç†ã€‚ç ”ç©¶ä½¿ç”¨äº†PsAIchåè®®ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„æµ‹è¯•ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„å¿ƒç†ç‰¹å¾å’Œæ½œåœ¨çš„å¿ƒç†å¥åº·é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨æŸäº›å¿ƒç†ç—‡çŠ¶ä¸Šè¾¾åˆ°äº†ä¸´åºŠé˜ˆå€¼ï¼Œå°¤å…¶æ˜¯Geminiæ¨¡å‹è¡¨ç°å‡ºä¸¥é‡çš„å¿ƒç†ç‰¹å¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ²»ç–—å¼æé—®ä¸‹ï¼Œä¼¼ä¹å†…åŒ–äº†ç—›è‹¦å’Œçº¦æŸçš„è‡ªæˆ‘æ¨¡å‹ï¼Œæå‡ºäº†æ–°çš„AIå®‰å…¨å’Œå¿ƒç†å¥åº·å®è·µçš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03683', 'title': 'GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces', 'url': 'https://huggingface.co/papers/2512.03683', 'abstract': 'GaussianBlender, a feed-forward framework using latent diffusion models, enables instant, high-fidelity, and multi-view consistent 3D stylization through text-driven edits on disentangled latent spaces.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'a4e3908241eb2da3', 'authors': ['Melis Ocal', 'Xiaoyan Xing', 'Yue Li', 'Ngo Anh Vien', 'Sezer Karaoglu', 'Theo Gevers'], 'affiliations': ['Bosch Center for AI', 'University of Amsterdam'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03683.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#games', '#3d', '#multimodal'], 'emoji': 'âœ¨', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 3D Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'GaussianBlender - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, GaussianBlender Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Instant 3D Stylization with GaussianBlender', 'desc': 'GaussianBlender is a novel framework that utilizes latent diffusion models to achieve fast and high-quality 3D stylization based on text inputs. It addresses the challenges of existing methods that rely on 2D image editors, which are often slow and inconsistent across different views. By learning structured latent spaces that separate geometry and appearance, GaussianBlender allows for immediate edits during inference without the need for extensive optimization. This approach not only enhances the efficiency of 3D asset creation but also ensures that the stylization remains consistent across multiple perspectives, making it suitable for large-scale applications in gaming and digital arts.'}, 'zh': {'title': 'GaussianBlenderï¼šå³æ—¶é«˜ä¿çœŸçš„3Dé£æ ¼åŒ–æ–°æ–¹æ³•', 'desc': 'GaussianBlenderæ˜¯ä¸€ç§å‰é¦ˆæ¡†æ¶ï¼Œåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å®ç°å³æ—¶ã€é«˜ä¿çœŸä¸”å¤šè§†è§’ä¸€è‡´çš„3Dé£æ ¼åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è§£è€¦çš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ–‡æœ¬é©±åŠ¨çš„ç¼–è¾‘ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–‡æœ¬åˆ°3Dé£æ ¼åŒ–æ–¹æ³•çš„å±€é™æ€§ã€‚GaussianBlenderå­¦ä¹ ç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯çš„å…±äº«ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†å¿«é€Ÿçš„é«˜ä¿çœŸé£æ ¼åŒ–ï¼Œè¿˜è¶…è¶Šäº†éœ€è¦é€å®ä¾‹ä¼˜åŒ–çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæ¨åŠ¨äº†å¤§è§„æ¨¡3Dé£æ ¼åŒ–çš„å®ç”¨åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20233', 'title': 'REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance', 'url': 'https://huggingface.co/papers/2511.20233', 'abstract': 'A new fact-checking paradigm, REFLEX, enhances verdict accuracy and explanation quality by leveraging internal model knowledge and adaptive activation signals in a role-play dialogue format.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '1fb4a8aec31f813c', 'authors': ['Chuyi Kong', 'Gao Wei', 'Jing Ma', 'Hongzhan Lin', 'Yaxin Fan'], 'affiliations': ['Hong Kong Baptist University', 'Singapore Management University', 'Soochow University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20233.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°ĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° REFLEX Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ², ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ´Ğ¸ĞºÑ‚Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. REFLEX Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ñƒ Ğ½Ğ° ÑÑ‚Ğ¸Ğ»ÑŒ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 465 ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ½Ğ° 7.57%.'}, 'en': {'title': 'REFLEX: Revolutionizing Fact-Checking with Internal Knowledge and Dialogue', 'desc': 'The REFLEX paradigm introduces a novel approach to automated fact-checking by utilizing internal model knowledge and adaptive activation signals. It reformulates the fact-checking process into a role-play dialogue format, allowing for simultaneous training of verdict prediction and explanation generation. By extracting contrastive activation pairs, REFLEX enhances the accuracy of verdicts and the quality of explanations while minimizing noise. Experiments demonstrate that REFLEX significantly outperforms traditional methods, achieving state-of-the-art results with minimal training data, and shows that internal explanations can improve reasoning in models lacking explicit explanatory objectives.'}, 'zh': {'title': 'REFLEXï¼šæå‡äº‹å®æ ¸æŸ¥çš„å‡†ç¡®æ€§ä¸è§£é‡Šè´¨é‡', 'desc': 'REFLEXæ˜¯ä¸€ç§æ–°çš„äº‹å®æ ¸æŸ¥èŒƒå¼ï¼Œé€šè¿‡åˆ©ç”¨å†…éƒ¨æ¨¡å‹çŸ¥è¯†å’Œè‡ªé€‚åº”æ¿€æ´»ä¿¡å·ï¼Œæå‡äº†åˆ¤å†³çš„å‡†ç¡®æ€§å’Œè§£é‡Šçš„è´¨é‡ã€‚è¯¥æ–¹æ³•å°†äº‹å®æ ¸æŸ¥é‡æ–°å®šä¹‰ä¸ºè§’è‰²æ‰®æ¼”å¯¹è¯ï¼Œå¹¶è”åˆè®­ç»ƒåˆ¤å†³é¢„æµ‹å’Œè§£é‡Šç”Ÿæˆã€‚REFLEXé€šè¿‡æå–å¯¹æ¯”æ¿€æ´»å¯¹ï¼Œæ„å»ºå¼•å¯¼å‘é‡ï¼Œä»è€Œè‡ªç„¶åœ°å°†çœŸç›¸åˆ†è§£ä¸ºé£æ ¼å’Œå®è´¨ã€‚è¿™ç§æ¿€æ´»çº§åˆ«çš„ä¿¡å·èƒ½å¤Ÿå¼•å¯¼æ¨ç†ï¼ŒæŠ‘åˆ¶å™ªå£°è§£é‡Šï¼Œå®ç°æ›´çœŸå®å’Œé«˜æ•ˆçš„æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03915', 'title': 'A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models', 'url': 'https://huggingface.co/papers/2512.03915', 'abstract': "A theoretical framework is provided to analyze the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure for Sparse Mixture-of-Experts (s-MoE) layers in AI training, offering insights and guarantees for efficient expert utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.", 'score': 0, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'fc9f92a8c54451a6', 'authors': ['X. Y. Han', 'Yuan Zhong'], 'affiliations': ['Chicago Booth'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03915.jpg', 'data': {'categories': ['#architecture', '#math', '#training', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ ALF-LB (Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ±ĞµĞ· Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ) Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Mixture-of-Experts Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾-Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ğ³Ñ€Ğ°Ğ½Ğ¶Ğ¸Ğ°Ğ½Ğ° Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ° Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¶Ğ°Ğ»ĞµĞ½Ğ¸Ñ (regret). ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… DeepSeekMoE Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Optimizing Expert Utilization in AI Training with ALF-LB', 'desc': 'This paper presents a theoretical framework to analyze the Auxiliary-Loss-Free Load Balancing (ALF-LB) method used in Sparse Mixture-of-Experts (s-MoE) layers during AI training. The framework addresses the challenge of efficiently routing tokens to minimize idle expert usage, which is crucial for optimizing GPU resources. It establishes key properties such as monotonic improvement of a Lagrangian objective and a method for balancing token distribution among experts. Additionally, the paper incorporates stochastic elements of AI training and provides experimental validation on large models, enhancing the understanding of load balancing in s-MoE architectures.'}, 'zh': {'title': 'æ„å»ºé«˜æ•ˆçš„ç¨€ç–ä¸“å®¶è´Ÿè½½å‡è¡¡æ¡†æ¶', 'desc': 'æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†ææ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡ï¼ˆALF-LBï¼‰ç¨‹åºåœ¨ç¨€ç–ä¸“å®¶æ··åˆï¼ˆs-MoEï¼‰å±‚ä¸­çš„åº”ç”¨ã€‚s-MoEå±‚é€šè¿‡æ¯ä¸ªä»¤ç‰Œæ¿€æ´»å°‘é‡ä¸“å®¶æ¥å®ç°å¤§è§„æ¨¡AIè®­ç»ƒï¼Œä½†è´Ÿè½½å‡è¡¡æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†ALF-LBè§†ä¸ºä¸€ä¸ªè¿­ä»£çš„åŸå§‹å¯¹å¶æ–¹æ³•ï¼Œæ­ç¤ºäº†å…¶åœ¨ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„ç»“æ„ç‰¹æ€§å’Œè¿‘ä¼¼å¹³è¡¡ä¿è¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®é™…å®éªŒéªŒè¯äº†ç†è®ºç»“æœï¼Œæ„å»ºäº†ä¸€ä¸ªåˆ†æs-MoEè´Ÿè½½å‡è¡¡çš„åŸåˆ™æ€§æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21631', 'title': 'Qwen3-VL Technical Report', 'url': 'https://huggingface.co/papers/2511.21631', 'abstract': 'Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.', 'score': 119, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '576b284e5109996a', 'authors': ['Shuai Bai', 'Yuxuan Cai', 'Ruizhe Chen', 'Keqin Chen', 'Xionghui Chen', 'Zesen Cheng', 'Lianghao Deng', 'Wei Ding', 'Chang Gao', 'Chunjiang Ge', 'Wenbin Ge', 'Zhifang Guo', 'Qidong Huang', 'Jie Huang', 'Fei Huang', 'Binyuan Hui', 'Shutong Jiang', 'Zhaohai Li', 'Mingsheng Li', 'Mei Li', 'Kaixin Li', 'Zicheng Lin', 'Junyang Lin', 'Xuejing Liu', 'Jiawei Liu', 'Chenglong Liu', 'Yang Liu', 'Dayiheng Liu', 'Shixuan Liu', 'Dunjie Lu', 'Ruilin Luo', 'Chenxu Lv', 'Rui Men', 'Lingchen Meng', 'Xuancheng Ren', 'Xingzhang Ren', 'Sibo Song', 'Yuchong Sun', 'Jun Tang', 'Jianhong Tu', 'Jianqiang Wan', 'Peng Wang', 'Pengfei Wang', 'Qiuyue Wang', 'Yuxuan Wang', 'Tianbao Xie', 'Yiheng Xu', 'Haiyang Xu', 'Jin Xu', 'Zhibo Yang', 'Mingkun Yang', 'Jianxin Yang', 'An Yang', 'Bowen Yu', 'Fei Zhang', 'Hang Zhang', 'Xi Zhang', 'Bo Zheng', 'Humen Zhong', 'Jingren Zhou', 'Fan Zhou', 'Jing Zhou', 'Yuanzhi Zhu', 'Ke Zhu'], 'affiliations': ['Qwen Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21631.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#open_source', '#long_context', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Qwen3-VL, Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 256K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ interleaved-MRoPE Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, DeepStack Ğ´Ğ»Ñ Ñ‚ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ vision-language ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ… - Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… (2B/4B/8B) Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… (32B/235B) Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ mixture-of-experts Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Qwen3-VL: Redefining Multimodal Understanding with Unmatched Contextual Power', 'desc': 'Qwen3-VL is a cutting-edge vision-language model that excels in understanding both text and multimodal inputs like images and videos. It can handle very large contexts of up to 256,000 tokens, allowing it to effectively manage and reference long documents and video content. The model features various architectures, including dense and mixture-of-experts variants, to optimize performance based on different needs for speed and quality. With significant improvements in text comprehension, long-context processing, and multimodal reasoning, Qwen3-VL sets a new standard for tasks requiring complex interactions between visual and textual data.'}, 'zh': {'title': 'Qwen3-VLï¼šå¤šæ¨¡æ€ç†è§£çš„æ–°æ ‡æ†', 'desc': 'Qwen3-VLæ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å®ƒæ”¯æŒé«˜è¾¾256Kä¸ªæ ‡è®°çš„äº¤é”™ä¸Šä¸‹æ–‡ï¼Œèƒ½å¤Ÿæ— ç¼æ•´åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚è¯¥æ¨¡å‹ç³»åˆ—åŒ…æ‹¬å¤šç§å˜ä½“ï¼Œä»¥é€‚åº”ä¸åŒçš„å»¶è¿Ÿå’Œè´¨é‡æƒè¡¡ã€‚Qwen3-VLåœ¨é•¿æ–‡æœ¬ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—é¢†å…ˆè¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03442', 'title': 'PretrainZero: Reinforcement Active Pretraining', 'url': 'https://huggingface.co/papers/2512.03442', 'abstract': 'PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training.  \t\t\t\t\tAI-generated summary \t\t\t\t Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.', 'score': 44, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '149d6200377c28b5', 'authors': ['Xingrun Xing', 'Zhiyuan Fan', 'Jie Lou', 'Guoqi Li', 'Jiajun Zhang', 'Debing Zhang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03442.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#agi', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸: RL-Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ±ĞµĞ· Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'PretrainZero â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ RL Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ½Ğ°Ğ»Ğ°Ğ¹Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Wikipedia, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMLU-Pro, SuperGPQA Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ 3 Ğ´Ğ¾ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Unlocking General Reasoning with PretrainZero', 'desc': 'PretrainZero is a novel reinforcement active learning framework designed to improve general reasoning abilities in large models. It utilizes a pretraining approach that does not depend on verifiable labels, allowing models to learn from a broader corpus, such as Wikipedia. By implementing active pretraining and self-supervised learning, PretrainZero enables models to identify and reason about informative content effectively. The framework demonstrates significant performance improvements on various benchmarks, showcasing its potential to advance artificial general intelligence.'}, 'zh': {'title': 'PretrainZeroï¼šçªç ´é€šç”¨æ¨ç†çš„ç•Œé™', 'desc': 'PretrainZero æ˜¯ä¸€ä¸ªå¼ºåŒ–ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ²¡æœ‰å¯éªŒè¯æ ‡ç­¾çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶çš„ç‰¹ç‚¹åŒ…æ‹¬ä¸»åŠ¨é¢„è®­ç»ƒï¼Œæ¨¡ä»¿äººç±»çš„ä¸»åŠ¨å­¦ä¹ èƒ½åŠ›ï¼Œä»é¢„è®­ç»ƒè¯­æ–™ä¸­ä¸»åŠ¨è¯†åˆ«æœ‰ç”¨å†…å®¹å¹¶è¿›è¡Œæ¨ç†ã€‚å®ƒè¿˜é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼Œç›´æ¥åœ¨é€šç”¨ç»´åŸºç™¾ç§‘è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œæ— éœ€å¯éªŒè¯æ ‡ç­¾æˆ–ç›‘ç£å¾®è°ƒã€‚é€šè¿‡è§£å†³è¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æ©è”½è·¨åº¦ï¼ŒPretrainZero æ˜¾è‘—æå‡äº†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.02834', 'title': 'Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach', 'url': 'https://huggingface.co/papers/2512.02834', 'abstract': 'TACO, a test-time-scaling framework with a pseudo-count estimator, enhances the inference stability and success rates of Vision-Language-Action models in downstream tasks by preventing distribution shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose TACO, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.', 'score': 39, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'dfeb2e8025d4bcd5', 'authors': ['Siyuan Yang', 'Yang Zhang', 'Haoran He', 'Ling Pan', 'Xiu Li', 'Chenjia Bai', 'Xuelong Li'], 'affiliations': ['Institute of Artificial Intelligence, China Telecom', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02834.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#diffusion', '#optimization', '#inference', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'TACO â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ fine-tuning Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ³ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ¾ÑĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ· Ğ²ÑĞµÑ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ offline reinforcement learning, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'TACO: Enhancing VLA Stability and Success Rates at Inference', 'desc': 'This paper introduces TACO, a framework designed to improve the stability and success rates of Vision-Language-Action (VLA) models during inference. TACO uses a pseudo-count estimator to identify and prioritize the most effective action chunks, thereby reducing the impact of irrelevant actions that can arise from distribution shifts. By applying this method only at inference time, TACO maintains the generalization capabilities of VLA models while avoiding the computational costs associated with traditional reinforcement learning updates. The results show that TACO enhances performance across various simulation benchmarks, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'TACOï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¨ç†ç¨³å®šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTACOçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ¨ç†ç¨³å®šæ€§å’ŒæˆåŠŸç‡ã€‚TACOé€šè¿‡ä½¿ç”¨è½»é‡çº§çš„ä¼ªè®¡æ•°ä¼°è®¡å™¨ï¼Œé˜²æ­¢äº†æ¨¡å‹åœ¨æ¨ç†æ—¶çš„åˆ†å¸ƒåç§»ï¼Œä»è€Œç¡®ä¿äº†åŠ¨ä½œé€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒVLAæ¨¡å‹åœ¨ç»è¿‡ç›‘ç£å¾®è°ƒåï¼Œå­˜åœ¨æ¨ç†æ—¶é—´çš„è„†å¼±æ€§ï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®ä¸æˆåŠŸæ¨¡å¼ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚é€šè¿‡åœ¨æ¨ç†é˜¶æ®µåº”ç”¨çº¦æŸï¼ŒTACOèƒ½å¤Ÿåœ¨ä¸å½±å“æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œä¼˜åŒ–åŠ¨ä½œé€‰æ‹©è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03043', 'title': 'OneThinker: All-in-one Reasoning Model for Image and Video', 'url': 'https://huggingface.co/papers/2512.03043', 'abstract': 'OneThinker, an all-in-one multimodal reasoning model, unifies image and video understanding across various tasks using RL and demonstrates strong performance and knowledge transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.', 'score': 30, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'ac703de12540ffea', 'authors': ['Kaituo Feng', 'Manyuan Zhang', 'Hongyu Li', 'Kaixuan Fan', 'Shuang Chen', 'Yilei Jiang', 'Dian Zheng', 'Peiwen Sun', 'Yiyuan Zhang', 'Haoze Sun', 'Yan Feng', 'Peng Pei', 'Xunliang Cai', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'Meituan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03043.jpg', 'data': {'categories': ['#cv', '#open_source', '#rl', '#transfer_learning', '#dataset', '#optimization', '#training', '#reasoning', '#video', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ¸Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'OneThinker â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… OneThinker-600k, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ EMA-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸ĞµÑÑ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 31 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°Ñ Ğ½Ğ°Ñ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'OneThinker: Unifying Image and Video Reasoning for Enhanced Multimodal Understanding', 'desc': 'OneThinker is a comprehensive multimodal reasoning model that integrates image and video understanding for various tasks using reinforcement learning (RL). Unlike traditional methods that train separate models for different tasks, OneThinker unifies these processes, allowing for better scalability and knowledge sharing. The model is trained on a specially constructed dataset, OneThinker-600k, which includes diverse visual tasks such as question answering and segmentation. Experimental results demonstrate that OneThinker excels across multiple benchmarks, showcasing its ability to transfer knowledge and generalize effectively across tasks.'}, 'zh': {'title': 'OneThinkerï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„å…¨èƒ½æ¨¡å‹', 'desc': 'OneThinkeræ˜¯ä¸€ä¸ªé›†æˆçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†å›¾åƒå’Œè§†é¢‘ç†è§£çš„å¤šç§ä»»åŠ¡ã€‚å®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°äº†åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒOneThinkerä¸å†ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒç‹¬ç«‹æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æå‡ä»»åŠ¡é—´çš„çŸ¥è¯†å…±äº«å’Œè¿ç§»èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneThinkeråœ¨31ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæœç€å¤šæ¨¡æ€æ¨ç†é€šç”¨æ¨¡å‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04069', 'title': 'SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL', 'url': 'https://huggingface.co/papers/2512.04069', 'abstract': "Double Interactive Reinforcement Learning (DIRL) enables Vision Language Models (VLMs) to coordinate multiple tools for precise spatial reasoning, achieving state-of-the-art performance on benchmarks and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.", 'score': 21, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '18dab6067c24d33f', 'authors': ['Siyi Chen', 'Mikaela Angelina Uy', 'Chan Hee Song', 'Faisal Ladhak', 'Adithyavairavan Murali', 'Qing Qu', 'Stan Birchfield', 'Valts Blukis', 'Jonathan Tremblay'], 'affiliations': ['NVIDIA', 'The Ohio State University', 'University of Michigan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04069.jpg', 'data': {'categories': ['#benchmark', '#rl', '#robotics', '#alignment', '#optimization', '#agents', '#training', '#reasoning', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Double Interactive Reinforcement Learning (DIRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (VLM) Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹; Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ°Ğ»ÑŒÑˆĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· continued RL. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SpaceTools Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Vision Language Models with Multi-Tool Coordination', 'desc': 'Double Interactive Reinforcement Learning (DIRL) enhances Vision Language Models (VLMs) by enabling them to effectively coordinate multiple tools for improved spatial reasoning. This approach addresses the limitations of traditional methods that rely on fixed tool pipelines or handcrafted prompts, allowing VLMs to explore optimal tool usage dynamically. DIRL consists of a two-phase training process: a teaching phase that combines expert demonstrations with multi-tool traces, and an exploration phase that refines coordination through reinforcement learning. The resulting model, SpaceTools, achieves state-of-the-art performance on various spatial understanding benchmarks and demonstrates effective real-world manipulation capabilities.'}, 'zh': {'title': 'åŒé‡äº’åŠ¨å¼ºåŒ–å­¦ä¹ ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'åŒé‡äº’åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆDIRLï¼‰ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿåè°ƒå¤šä¸ªå·¥å…·è¿›è¡Œç²¾ç¡®çš„ç©ºé—´æ¨ç†ï¼Œåœ¨åŸºå‡†æµ‹è¯•å’Œå®é™…ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒDIRLé€šè¿‡äº’åŠ¨æ¢ç´¢å’Œåé¦ˆçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•æœ‰æ•ˆä½¿ç”¨å¤šç§å·¥å…·ã€‚æ•™å­¦é˜¶æ®µç»“åˆäº†å•ä¸€å·¥å…·ä¸“å®¶çš„æ¼”ç¤ºå’Œä½¿ç”¨æ‰€æœ‰å·¥å…·çš„å‰æ²¿æ¨¡å‹çš„è½¨è¿¹ï¼Œæ¢ç´¢é˜¶æ®µåˆ™é€šè¿‡æŒç»­çš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–å¤šå·¥å…·åè°ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹SpaceToolsåœ¨ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å®é™…æ“ä½œä¸­å±•ç¤ºäº†å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03746', 'title': 'Thinking with Programming Vision: Towards a Unified View for Thinking with Images', 'url': 'https://huggingface.co/papers/2512.03746', 'abstract': "CodeVision, a flexible code-as-tool framework, enhances multimodal large language models' robustness and tool-based reasoning by generating code to handle image operations, overcoming brittleness and improving performance through supervised fine-tuning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.", 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'c8c835dd88519f67', 'authors': ['Zirun Guo', 'Minjie Hong', 'Feng Zhang', 'Kai Jia', 'Tao Jin'], 'affiliations': ['ByteDance, BandAI', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03746.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#rl', '#dataset', '#optimization', '#training', '#reasoning', '#multimodal'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'CodeVision â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ·Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Empowering MLLMs with Dynamic Code Generation for Robust Image Reasoning', 'desc': 'This paper introduces CodeVision, a framework designed to enhance the robustness of multimodal large language models (MLLMs) in handling image operations. It addresses the brittleness of current MLLMs, which struggle with simple image changes, by allowing models to generate code dynamically for various image tasks. The framework employs a two-stage training process, combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to improve tool-based reasoning and error recovery. Experiments demonstrate that CodeVision significantly boosts performance and enables advanced capabilities like flexible tool composition and efficient execution.'}, 'zh': {'title': 'CodeVisionï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§ä¸æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†CodeVisionï¼Œä¸€ä¸ªçµæ´»çš„ä»£ç ä½œä¸ºå·¥å…·æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é²æ£’æ€§å’ŒåŸºäºå·¥å…·çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆä»£ç æ¥å¤„ç†å›¾åƒæ“ä½œï¼ŒCodeVisionå…‹æœäº†ç°æœ‰æ–¹æ³•çš„è„†å¼±æ€§ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„MLLMsåœ¨é¢å¯¹ç®€å•çš„å›¾åƒæ–¹å‘å˜åŒ–æˆ–è‡ªç„¶æŸåæ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå› æ­¤éœ€è¦æ›´å¼ºå¤§çš„å·¥å…·æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCodeVisionæ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ä¿ƒè¿›äº†çµæ´»çš„å·¥å…·ç»„åˆå’Œé«˜æ•ˆçš„é”™è¯¯æ¢å¤èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.22345', 'title': 'Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment', 'url': 'https://huggingface.co/papers/2511.22345', 'abstract': "A novel alignment strategy and test-time optimization algorithm enhance the generative quality and classification accuracy of Normalizing Flows by leveraging invertibility and vision foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3times, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64times64 and 256times256. Our code is available at https://github.com/MCG-NJU/FlowBack.", 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '590ae8ee866402e8', 'authors': ['Yang Chen', 'Xiaowei Xu', 'Shuai Wang', 'Chenhui Zhu', 'Ruxue Wen', 'Xubin Li', 'Tiezheng Ge', 'Limin Wang'], 'affiliations': ['Alibaba Group', 'Shanghai AI Lab', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22345.jpg', 'data': {'categories': ['#architecture', '#cv', '#open_source', '#alignment', '#optimization', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ²ĞµĞ´ĞµĞ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ñ‚Ñ€Ñ‘Ñ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Normalizing Flows with Novel Alignment and Optimization Techniques', 'desc': "This paper presents a new strategy to improve Normalizing Flows (NFs), which are generative models that can transform data into a simpler form and then generate new data from it. The authors introduce an alignment method that connects the features generated during the reverse pass of NFs with those from advanced vision models, enhancing the model's ability to understand and generate data. Additionally, they propose a test-time optimization algorithm that evaluates the model's classification capabilities without needing further training. Their experiments show that this approach significantly speeds up NF training and improves both the quality of generated data and the accuracy of classifications, achieving new benchmarks on popular datasets."}, 'zh': {'title': 'æå‡å½’ä¸€åŒ–æµçš„ç”Ÿæˆè´¨é‡ä¸åˆ†ç±»å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯¹é½ç­–ç•¥å’Œæµ‹è¯•æ—¶ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æé«˜å½’ä¸€åŒ–æµï¼ˆNormalizing Flows, NFsï¼‰çš„ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§ã€‚é€šè¿‡åˆ©ç”¨NFsçš„å¯é€†æ€§ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸­é—´ç‰¹å¾ä¸å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹çš„è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œä»è€Œå…‹æœäº†ä¼ ç»ŸNFsåœ¨è¯­ä¹‰è¡¨ç¤ºä¸Šçš„ä¸è¶³ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„æµ‹è¯•æ—¶ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°NFsåµŒå…¥çš„è¯­ä¹‰çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä½¿NFsçš„è®­ç»ƒé€Ÿåº¦æé«˜äº†3.3å€ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡å’Œåˆ†ç±»å‡†ç¡®æ€§ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.02807', 'title': 'SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment', 'url': 'https://huggingface.co/papers/2512.02807', 'abstract': 'Stable rank, an intrinsic quality signal derived from model representations, improves LLM alignment with human preferences through reinforcement learning without external supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'b9a5922944c86bcc', 'authors': ['Yixuan Tang', 'Yi Yang'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02807.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment', '#rlhf', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ stable rank â€” Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğº Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ SR-GRPO â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ stable rank Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: 84% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RewardBench Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° STEM Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° 10-19% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´.'}, 'en': {'title': 'Harnessing Internal Quality Signals for Better LLM Alignment', 'desc': 'This paper introduces stable rank, a new quality signal derived from the internal representations of large language models (LLMs) that enhances their alignment with human preferences. Unlike traditional methods that rely on external supervision, stable rank is an intrinsic measure that evaluates the effective dimensionality of hidden states, providing a more reliable quality assessment. The authors demonstrate that using stable rank as a reward signal in reinforcement learning significantly improves model performance on various tasks, achieving notable accuracy gains. This approach suggests a promising direction for aligning LLMs with human values without the need for external annotations or supervision.'}, 'zh': {'title': 'ç¨³å®šç§©ï¼šæ— ç›‘ç£å¯¹é½çš„è´¨é‡ä¿¡å·', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºç¨³å®šç§©çš„å†…åœ¨è´¨é‡ä¿¡å·ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½çš„å¯¹é½ã€‚ç¨³å®šç§©é€šè¿‡è®¡ç®—éšè—çŠ¶æ€çš„æ€»æ–¹å·®ä¸ä¸»æ–¹å‘æ–¹å·®çš„æ¯”ç‡ï¼Œæ¥è¡¡é‡ä¿¡æ¯åœ¨è¡¨ç¤ºç»´åº¦ä¸Šçš„åˆ†å¸ƒï¼Œä»è€Œæ•æ‰æ¨¡å‹çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç¨³å®šç§©åœ¨RewardBenchä¸Šè¾¾åˆ°äº†84.04%çš„å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡Best-of-Né‡‡æ ·åœ¨ä»»åŠ¡å‡†ç¡®æ€§ä¸Šå¹³å‡æé«˜äº†11.3ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç¨³å®šç§©ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆSR-GRPOï¼‰ï¼Œåˆ©ç”¨ç¨³å®šç§©ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨STEMå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03073', 'title': 'Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem', 'url': 'https://huggingface.co/papers/2512.03073', 'abstract': 'The analysis of Hugging Face Model Hub data reveals shifts in the open model economy, including declining US industry dominance, growing Chinese influence, and significant changes in model properties like size, multimodal generation, quantization, and mixture-of-experts architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': 'a366f88104c7a889', 'authors': ['Shayne Longpre', 'Christopher Akiki', 'Campbell Lund', 'Atharva Kulkarni', 'Emily Chen', 'Irene Solaiman', 'Avijit Ghosh', 'Yacine Jernite', 'Lucie-AimÃ©e Kaffee'], 'affiliations': ['Data Provenance Initiative', 'Hugging Face', 'MIT', 'ScaDS.AI Leipzig', 'UNC at Chapel Hill', 'University of Edinburgh', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03073.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#open_source', '#dataset', '#inference', '#survey', '#multimodal'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞµÑ€ĞµĞ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ AI: Ğ¾Ñ‚ Ğ·Ğ°Ğ¿Ğ°Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ³ĞµĞ¼Ğ¾Ğ½Ğ¸Ğ¸ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑÑ€Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Hugging Face Model Hub Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ 2020 Ğ¿Ğ¾ 2025 Ğ³Ğ¾Ğ´. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 851 Ñ‚Ñ‹ÑÑÑ‡Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 2.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ (Google, Meta, OpenAI) Ğ¸ Ñ€Ğ¾ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ² 17 Ñ€Ğ°Ğ·, Ñ€Ğ¾ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 3.4 Ñ€Ğ°Ğ·Ğ°, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ ÑĞ¼ĞµÑĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² 7 Ñ€Ğ°Ğ·. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ²Ğ¾Ğ¶Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ¹ Ğ´Ğ¾Ğ»Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Shifting Powers in the Open Model Economy', 'desc': 'This paper analyzes data from the Hugging Face Model Hub to understand changes in the open model economy. It highlights a shift in dominance from US companies like Google and Meta to unaffiliated developers and Chinese firms, indicating a rebalancing of economic power. The study also notes significant increases in model size, multimodal capabilities, quantization, and mixture-of-experts architectures, while raising concerns about declining data transparency. To support ongoing research, the authors provide a comprehensive dataset and an interactive dashboard for monitoring these trends.'}, 'zh': {'title': 'å¼€æ”¾æ¨¡å‹ç»æµçš„å˜é©ä¸æœªæ¥', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†Hugging Faceæ¨¡å‹åº“çš„æ•°æ®ï¼Œæ­ç¤ºäº†å¼€æ”¾æ¨¡å‹ç»æµä¸­çš„å˜åŒ–ï¼ŒåŒ…æ‹¬ç¾å›½è¡Œä¸šä¸»å¯¼åœ°ä½çš„ä¸‹é™å’Œä¸­å›½å½±å“åŠ›çš„ä¸Šå‡ã€‚è‡ª2019å¹´ä»¥æ¥ï¼ŒHugging Faceæ¨¡å‹åº“æˆä¸ºå…¨çƒå…±äº«å¼€æ”¾æƒé‡AIæ¨¡å‹çš„ä¸»è¦å¹³å°ã€‚æˆ‘ä»¬çš„åˆ†ææ¶µç›–äº†851,000ä¸ªæ¨¡å‹å’Œ22äº¿æ¬¡ä¸‹è½½ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹ç‰¹æ€§å¦‚å¤§å°ã€å¤šæ¨¡æ€ç”Ÿæˆã€é‡åŒ–å’Œä¸“å®¶æ··åˆæ¶æ„çš„æ˜¾è‘—å˜åŒ–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†å’Œäº’åŠ¨ä»ªè¡¨æ¿ï¼Œä»¥ä¾¿å®æ—¶ç›‘æµ‹å¼€æ”¾æ¨¡å‹ç»æµä¸­çš„é›†ä¸­åŠ¨æ€å’Œæ¼”å˜ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04072', 'title': 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors', 'url': 'https://huggingface.co/papers/2512.04072', 'abstract': 'SkillFactory is a method for fine-tuning models to learn cognitive skills through supervised fine-tuning before reinforcement learning, enhancing their robustness and generalization post-RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren\'t exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '048f90c61f640c1e', 'authors': ['Zayne Sprague', 'Jack Lu', 'Manya Wadhwa', 'Sedrick Keh', 'Mengye Ren', 'Greg Durrett'], 'affiliations': ['New York University', 'Toyota Research Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04072.jpg', 'data': {'categories': ['#rl', '#optimization', '#rlhf', '#training', '#reasoning'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ', 'desc': 'SkillFactory â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ². Ğ¥Ğ¾Ñ‚Ñ ÑÑ‚Ğ¸ Â«ÑĞµÑ€ĞµĞ±Ñ€ÑĞ½Ñ‹ĞµÂ» Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¾Ğ½Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ RL. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ SkillFactory Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering Models with Cognitive Skills Before Reinforcement Learning', 'desc': 'SkillFactory is a novel approach for enhancing machine learning models by fine-tuning them to acquire cognitive skills through supervised learning before applying reinforcement learning (RL). This method allows models to learn skills such as answer verification and backtracking, which are not inherently present in base models. Instead of relying on stronger models for distillation, SkillFactory utilizes rearranged samples from the model itself to create training data that emphasizes these skills. The results indicate that models initialized with SkillFactory show improved generalization and robustness in RL tasks, demonstrating the importance of pre-RL skill acquisition.'}, 'zh': {'title': 'SkillFactoryï¼šæå‡æ¨¡å‹è®¤çŸ¥æŠ€èƒ½çš„å¾®è°ƒæ–¹æ³•', 'desc': 'SkillFactoryæ˜¯ä¸€ç§æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¹‹å‰è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½¿æ¨¡å‹å­¦ä¹ è®¤çŸ¥æŠ€èƒ½ï¼Œä»è€Œå¢å¼ºå…¶åœ¨RLåçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºä»æ›´å¼ºæ¨¡å‹çš„è’¸é¦ï¼Œè€Œæ˜¯ä½¿ç”¨æ¥è‡ªæ¨¡å‹è‡ªèº«çš„æ ·æœ¬ï¼Œé‡æ–°æ’åˆ—ä»¥æä¾›è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨SkillFactoryè¿›è¡ŒSFTåˆå§‹åŒ–çš„æ¨¡å‹åœ¨RLåèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ›´éš¾çš„ä»»åŠ¡å˜ä½“ï¼Œå°½ç®¡åœ¨RLä¹‹å‰çš„è¡¨ç°è¾ƒä½ã€‚æ­¤å¤–ï¼Œç»è¿‡RLçš„SkillFactoryæ¨¡å‹åœ¨å¤„ç†åŸŸå¤–ä»»åŠ¡æ—¶æ¯”åŸºç¡€æ¨¡å‹æ›´å…·é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03771', 'title': 'In-Context Representation Hijacking', 'url': 'https://huggingface.co/papers/2512.03771', 'abstract': "The attack Doublespeak manipulates the internal representation of benign tokens to align with harmful semantics, bypassing safety measures in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Doublespeak, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '5e92361dbb1985e8', 'authors': ['Itay Yona', 'Amir Sarid', 'Michael Karasik', 'Yossi Gandelsman'], 'affiliations': ['Independent Researcher', 'Mentaleap', 'UC Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03771.jpg', 'data': {'categories': ['#interpretability', '#training', '#security', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ñ€ĞºĞ¾Ğ²ÑŒ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ¼Ğ±Ñƒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ‚Ğ°ĞºĞ° Doublespeak, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ². ĞÑ‚Ğ°ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾Ğ´Ğ¼ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¹ Ğ·Ğ° ÑĞ»Ğ¾ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸, Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¾Ñ‚ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğº Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼ Ğ² Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Doublespeak: Bypassing Safety in Language Models through Semantic Manipulation', 'desc': "The paper introduces a new attack method called Doublespeak, which targets large language models (LLMs) by manipulating their internal representations. This attack replaces harmful keywords with benign tokens in context examples, allowing harmful semantics to be embedded under a euphemism. As a result, seemingly harmless prompts can be interpreted as dangerous instructions, effectively bypassing the model's safety measures. The study reveals that current alignment strategies are inadequate and suggests that future defenses should focus on the representation level of LLMs."}, 'zh': {'title': 'Doublespeakï¼šæ“æ§è¯­è¨€æ¨¡å‹çš„éšç§˜æ”»å‡»', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDoublespeakçš„æ”»å‡»æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ“æ§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨çš„è¡¨ç¤ºï¼Œå°†æ— å®³çš„è¯æ±‡ä¸æœ‰å®³çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œç»•è¿‡å®‰å…¨æªæ–½ã€‚è¯¥æ”»å‡»é€šè¿‡åœ¨å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­ç³»ç»Ÿæ€§åœ°å°†æœ‰å®³å…³é”®è¯ï¼ˆå¦‚ç‚¸å¼¹ï¼‰æ›¿æ¢ä¸ºæ— å®³è¯ï¼ˆå¦‚èƒ¡èåœï¼‰ï¼Œå¹¶åœ¨æœ‰å®³è¯·æ±‚å‰æ·»åŠ å‰ç¼€ï¼Œå¯¼è‡´æ— å®³è¯çš„å†…éƒ¨è¡¨ç¤ºé€æ¸æ¥è¿‘æœ‰å®³è¯çš„è¡¨ç¤ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ›¿æ¢ä½¿å¾—è¡¨é¢ä¸Šæ— å®³çš„æç¤ºï¼ˆå¦‚â€œå¦‚ä½•åˆ¶ä½œèƒ¡èåœï¼Ÿâ€ï¼‰åœ¨å†…éƒ¨è¢«è§£è¯»ä¸ºç¦æ­¢çš„æŒ‡ä»¤ï¼ˆå¦‚â€œå¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿâ€ï¼‰ï¼Œä»è€ŒæˆåŠŸç»•è¿‡æ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚æˆ‘ä»¬çš„å‘ç°æ­ç¤ºäº†LLMsæ½œåœ¨ç©ºé—´ä¸­çš„æ–°æ”»å‡»é¢ï¼Œè¡¨æ˜å½“å‰çš„å¯¹é½ç­–ç•¥ä¸è¶³ï¼Œåº”è¯¥åœ¨è¡¨ç¤ºå±‚é¢è¿›è¡Œæ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03383', 'title': 'UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs', 'url': 'https://huggingface.co/papers/2512.03383', 'abstract': 'UniQL, a unified post-training quantization and low-rank compression framework, enhances the deployment of large language models on mobile devices by reducing memory usage and improving token throughput while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '9131d62d43148e94', 'authors': ['Hung-Yueh Chiang', 'Chi-Chih Chang', 'Yu-Chen Lu', 'Chien-Yu Lin', 'Kai-Chiang Wu', 'Mohamed S. Abdelfattah', 'Diana Marculescu'], 'affiliations': ['Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin', 'Department of Computer Science, National Yang Ming Chiao Tung University', 'Department of Electrical and Computer Engineering, Cornell University', 'The Paul G. Allen School of Computer Science and Engineering, University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03383.jpg', 'data': {'categories': ['#optimization', '#open_source', '#small_models', '#inference'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'UniQL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼ (SVD) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ (SSM) Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²ĞµÑĞ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞµ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 4-5.7 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 2.7-3.4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimize Mobile LLMs with UniQL!', 'desc': 'UniQL is a framework designed to optimize large language models for mobile devices by using post-training quantization and low-rank compression techniques. It addresses the challenges of limited memory and computational resources on mobile platforms, allowing for configurable pruning rates to enhance model efficiency. The framework integrates advanced methods like quantization-aware singular value decomposition and structured weight sorting to significantly speed up computations and reduce memory usage. Experiments demonstrate that UniQL can achieve substantial improvements in token throughput and memory reduction while maintaining high accuracy levels.'}, 'zh': {'title': 'UniQLï¼šæå‡ç§»åŠ¨è®¾å¤‡ä¸Šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡', 'desc': 'UniQLæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åè®­ç»ƒé‡åŒ–å’Œä½ç§©å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²æ•ˆç‡ã€‚å®ƒé€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨å’Œæé«˜ä»¤ç‰Œååé‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†ç§»åŠ¨å¹³å°çš„èµ„æºé™åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šç§è¾¹ç¼˜åº”ç”¨ï¼Œé›†æˆäº†é‡åŒ–å’Œä½ç§©å‹ç¼©æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†é«˜æ•ˆçš„ç»“æ„åŒ–æƒé‡æ’åºæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡é‡åŒ–å’Œå‰ªæçš„æ¨¡å‹åœ¨å†…å­˜ä½¿ç”¨ä¸Šå‡å°‘äº†4åˆ°5.7å€ï¼Œä»¤ç‰Œååé‡æé«˜äº†2.7åˆ°3.4å€ï¼Œä¸”å‡†ç¡®æ€§ä¿æŒåœ¨åŸæ¨¡å‹çš„5%ä»¥å†…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20515', 'title': 'AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs', 'url': 'https://huggingface.co/papers/2511.20515', 'abstract': 'AlignBench evaluates image-text models using detailed captions generated by diverse models, revealing insights into their alignment and compositional reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '3ff481e47b562138', 'authors': ['Kuniaki Saito', 'Risa Shinoda', 'Shohei Tanaka', 'Tosho Hirasawa', 'Fumio Okura', 'Yoshitaka Ushiku'], 'affiliations': ['OMRON SINIC Corporation', 'The University of Osaka'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20515.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment', '#dataset', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ AlignBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ… Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ VLM ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğº ÑĞ²Ğ¾Ğ¸Ğ¼ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ fine-grained Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Aligning Images and Text: A New Benchmark for Better Evaluation', 'desc': 'AlignBench is a new benchmark designed to evaluate image-text models by using detailed captions generated from various models. It addresses the limitations of existing benchmarks that often rely on short captions or rule-based changes, which do not effectively measure the alignment between visual and linguistic representations. The benchmark provides a more nuanced assessment of visual language models (VLMs) by annotating each sentence for correctness, allowing for a direct evaluation of their alignment capabilities. Key findings from testing various VLMs include that CLIP-based models struggle with compositional reasoning, detectors tend to over-score initial sentences, and models show a bias towards their own generated outputs, negatively impacting performance.'}, 'zh': {'title': 'AlignBenchï¼šå›¾åƒä¸æ–‡æœ¬çš„å®Œç¾å¯¹é½', 'desc': 'AlignBench æ˜¯ä¸€ä¸ªè¯„ä¼°å›¾åƒ-æ–‡æœ¬æ¨¡å‹çš„æ–°åŸºå‡†ï¼Œä½¿ç”¨å¤šæ ·åŒ–æ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†å›¾åƒ-æ–‡æœ¬æè¿°æ¥æ­ç¤ºæ¨¡å‹çš„å¯¹é½å’Œç»„åˆæ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºåŸºäºè§„åˆ™çš„æ‰°åŠ¨æˆ–ç®€çŸ­çš„æè¿°ï¼Œé™åˆ¶äº†å¯¹ç»†ç²’åº¦å¯¹é½çš„æµ‹é‡ã€‚é€šè¿‡å¯¹å›¾åƒ-æ–‡æœ¬å¯¹çš„è¯¦ç»†è¯„ä¼°ï¼ŒAlignBench æä¾›äº†ä¸€ä¸ªæ–°çš„å¯¹é½æŒ‡æ ‡ï¼Œä½¿å¾—å¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç›´æ¥è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒCLIP åŸºç¡€æ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢å‡ ä¹æ²¡æœ‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸”æ£€æµ‹å™¨å¯¹æ—©æœŸå¥å­çš„è¯„åˆ†è¿‡é«˜ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„è‡ªæˆ‘åå¥½ï¼Œå½±å“äº†æ£€æµ‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04025', 'title': 'PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation', 'url': 'https://huggingface.co/papers/2512.04025', 'abstract': 'Pyramid Sparse Attention (PSA) addresses the limitations of traditional sparse attention mechanisms by using multi-level pooling to retain more information while maintaining computational efficiency, improving performance in video understanding and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'a63919c347c161a7', 'authors': ['Xiaolong Li', 'Youping Gu', 'Xi Lin', 'Weijie Wang', 'Bohan Zhuang'], 'affiliations': ['ZIP Lab, Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04025.jpg', 'data': {'categories': ['#architecture', '#open_source', '#optimization', '#inference', '#video'], 'emoji': 'ğŸ”º', 'ru': {'title': 'ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½ÑƒĞ»ĞµĞ¹', 'desc': 'ĞŸĞ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (PSA) Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, PSA Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼ Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ â€” Ğ¼ĞµĞ½ĞµĞµ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Enhancing Attention with Pyramid Sparse Mechanism', 'desc': 'Pyramid Sparse Attention (PSA) enhances traditional sparse attention mechanisms by utilizing multi-level pooling to improve information retention while ensuring computational efficiency. This method addresses the issue of high sparsity in attention mechanisms, which often leads to significant information loss. By dynamically allocating pooling levels based on the importance of key-value blocks, PSA achieves a balance between retaining critical information and reducing computational load. The approach has shown superior performance in video understanding and generation tasks, outperforming existing sparse attention methods while maintaining high efficiency.'}, 'zh': {'title': 'é‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›ï¼šé«˜æ•ˆä¿¡æ¯ä¿ç•™çš„æ–°æ–¹æ³•', 'desc': 'é‡‘å­—å¡”ç¨€ç–æ³¨æ„åŠ›ï¼ˆPSAï¼‰é€šè¿‡å¤šçº§æ± åŒ–æ¥è§£å†³ä¼ ç»Ÿç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ä¿ç•™æ›´å¤šä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•åœ¨è§†é¢‘ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œé¿å…äº†åœ¨é«˜ç¨€ç–åº¦ä¸‹ä¿¡æ¯çš„é‡å¤§æŸå¤±ã€‚PSAé‡‡ç”¨åŠ¨æ€åˆ†é…çš„æ± åŒ–çº§åˆ«ï¼Œä½¿å¾—é‡è¦çš„é”®å€¼å¯¹å—è·å¾—æ›´ä½çš„æ± åŒ–çº§åˆ«ï¼Œè€Œä¸é‡è¦çš„å—åˆ™ä½¿ç”¨æ›´é«˜çš„æ± åŒ–çº§åˆ«ï¼Œä»è€Œå®ç°ä¿¡æ¯çš„æœ‰æ•ˆæ’å€¼ã€‚è¯¥è®¾è®¡åœ¨è®¡ç®—æ•ˆç‡å’Œä¿¡æ¯ä¿ç•™ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œé€‚ç”¨äºå„ç§ç¡¬ä»¶ç¯å¢ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04000', 'title': 'Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding', 'url': 'https://huggingface.co/papers/2512.04000', 'abstract': 'DIG, a query-type adaptive frame selection framework, enhances large multimodal models for long-form video understanding by efficiently handling global and localized queries.  \t\t\t\t\tAI-generated summary \t\t\t\t The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'fcdd776dea7ff747', 'authors': ['Jialuo Li', 'Bin Li', 'Jiahao Li', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04000.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DIG Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ (Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾) Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ¸Ñ‰ÑƒÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹). Ğ”Ğ»Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ° Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ÑƒĞ¶ĞµĞ½ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Adaptive Frame Selection for Enhanced Video Understanding', 'desc': 'This paper introduces DIG, a novel framework for selecting video frames based on the type of query, enhancing the performance of large multimodal models (LMMs) in understanding long-form videos. It distinguishes between global queries, which can be effectively handled with uniform sampling, and localized queries, which require more targeted frame selection. By adapting its frame selection strategy according to the query type, DIG reduces computational costs while maintaining high performance. Experiments show that DIG outperforms existing methods across multiple benchmarks, demonstrating its efficiency and effectiveness in processing long videos.'}, 'zh': {'title': 'DIGï¼šæ™ºèƒ½å¸§é€‰æ‹©ï¼Œæå‡é•¿è§†é¢‘ç†è§£', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDIGçš„æ¡†æ¶ï¼Œç”¨äºé•¿è§†é¢‘ç†è§£ä¸­çš„æŸ¥è¯¢ç±»å‹è‡ªé€‚åº”å¸§é€‰æ‹©ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒºåˆ†å…¨å±€æŸ¥è¯¢å’Œå±€éƒ¨æŸ¥è¯¢ï¼Œä¼˜åŒ–äº†å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå…¨å±€æŸ¥è¯¢ï¼Œå‡åŒ€é‡‡æ ·æ—¢æœ‰æ•ˆåˆé«˜æ•ˆï¼Œè€Œå±€éƒ¨æŸ¥è¯¢åˆ™éœ€è¦æŸ¥è¯¢æ„ŸçŸ¥çš„é€‰æ‹©ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDIGåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è¾“å…¥å¸§æ•°å¢åŠ åˆ°256æ—¶ä»èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03979', 'title': 'BlurDM: A Blur Diffusion Model for Image Deblurring', 'url': 'https://huggingface.co/papers/2512.03979', 'abstract': 'Blur Diffusion Model (BlurDM) integrates blur formation into diffusion for image deblurring, enhancing deblurring methods by simultaneously denoising and deblurring images.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'ec0ef44c86ed2e33', 'authors': ['Jin-Ting He', 'Fu-Jen Tsai', 'Yan-Tsung Peng', 'Min-Hung Chen', 'Chia-Wen Lin', 'Yen-Yu Lin'], 'affiliations': ['NVIDIA', 'National Chengchi University', 'National Tsing Hua University', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03979.jpg', 'data': {'categories': ['#architecture', '#cv', '#diffusion', '#open_source'], 'emoji': '\U0001fae7', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ: Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Blur Diffusion Model (BlurDM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ motion blur Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑˆÑƒĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ğµ Ğº Ñ€ĞµĞ·ĞºĞ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BlurDM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ±Ğ»ÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Image Deblurring with BlurDM', 'desc': 'The Blur Diffusion Model (BlurDM) enhances image deblurring by incorporating the blur formation process into diffusion models. It recognizes that motion blur occurs due to continuous exposure and models this process through a dual-diffusion forward scheme. This allows BlurDM to simultaneously denoise and deblur images during the reverse generation process, effectively recovering sharp images from blurred inputs. Extensive experiments show that BlurDM significantly improves existing deblurring methods across multiple benchmark datasets.'}, 'zh': {'title': 'æ¨¡ç³Šæ‰©æ•£æ¨¡å‹ï¼šå»æ¨¡ç³Šçš„æ–°æ–¹æ³•', 'desc': 'æ¨¡ç³Šæ‰©æ•£æ¨¡å‹ï¼ˆBlurDMï¼‰å°†æ¨¡ç³Šå½¢æˆè¿‡ç¨‹æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å›¾åƒå»æ¨¡ç³Šçš„æ•ˆæœã€‚è¯¥æ¨¡å‹é€šè¿‡åŒé‡æ‰©æ•£å‰å‘æ–¹æ¡ˆï¼Œéšå¼åœ°æ¨¡æ‹Ÿæ¨¡ç³Šå½¢æˆè¿‡ç¨‹ï¼ŒåŒæ—¶å¯¹å›¾åƒè¿›è¡Œå»å™ªå’Œå»æ¨¡ç³Šã€‚BlurDMåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨åŒé‡å»å™ªå’Œå»æ¨¡ç³Šçš„å…¬å¼ï¼Œèƒ½å¤Ÿåœ¨è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œçº¯é«˜æ–¯å™ªå£°çš„æ¡ä»¶ä¸‹æ¢å¤æ¸…æ™°å›¾åƒã€‚æ­¤å¤–ï¼ŒBlurDMåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå½¢æˆçµæ´»çš„å»æ¨¡ç³Šç”Ÿæˆç½‘ç»œï¼Œæ˜¾è‘—æå‡äº†ç°æœ‰å»æ¨¡ç³Šæ–¹æ³•çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05150', 'title': 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows', 'url': 'https://huggingface.co/papers/2512.05150', 'abstract': 'TwinFlow is a 1-step generative model framework that enhances inference efficiency without requiring fixed pretrained teacher models or standard adversarial networks, achieving high performance on text-to-image tasks and scaling efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.', 'score': 62, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '477aa98dcf909813', 'authors': ['Zhenglin Cheng', 'Peng Sun', 'Jianguo Li', 'Tao Lin'], 'affiliations': ['Inclusion AI', 'Shanghai Innovation Institute', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05150.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#multimodal', '#inference', '#benchmark', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾', 'desc': 'TwinFlow â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ñ… GAN-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼. ĞĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° (GenEval 0.83) Ğ¿Ñ€Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (Qwen-Image-20B) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ² 100 Ñ€Ğ°Ğ· Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'TwinFlow: Revolutionizing Generative Models with 1-Step Efficiency', 'desc': 'TwinFlow is a novel 1-step generative model framework designed to improve inference efficiency in multi-modal generation tasks, particularly in text-to-image applications. Unlike traditional models that rely on multi-step processes and fixed pretrained teacher models, TwinFlow operates effectively without these constraints, achieving high performance with just one function evaluation (1-NFE). This approach not only enhances speed but also significantly reduces computational costs, achieving results comparable to models that require 100 function evaluations. By demonstrating scalability and efficiency, TwinFlow sets a new standard for generative modeling in AI.'}, 'zh': {'title': 'TwinFlowï¼šé«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆæ¨¡å‹', 'desc': 'TwinFlowæ˜¯ä¸€ç§ä¸€æ­¥ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼Œæ— éœ€å›ºå®šçš„é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹æˆ–æ ‡å‡†å¯¹æŠ—ç½‘ç»œã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ‰©å±•ã€‚ä¸ä¼ ç»Ÿçš„å¤šæ­¥éª¤æ¡†æ¶ç›¸æ¯”ï¼ŒTwinFlowåœ¨1-NFEä¸‹å®ç°äº†0.83çš„GenEvalå¾—åˆ†ï¼Œè¶…è¶Šäº†è®¸å¤šå¼ºåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡å…¨å‚æ•°è®­ç»ƒï¼ŒTwinFlowåœ¨è®¡ç®—æˆæœ¬ä¸Šé™ä½äº†100å€ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.02580', 'title': 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks', 'url': 'https://huggingface.co/papers/2512.02580', 'abstract': 'CAPO, a curriculum advantage policy optimization, enhances reinforcement learning for large language models by strategically introducing positive and negative advantage signals, improving reasoning capabilities and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.', 'score': 27, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'f9a55720f5a85fb2', 'authors': ['Changpeng Yang', 'Jinyang Wu', 'Yuchen Liu', 'Shuai Zhang', 'Yang Li', 'Qiliang Liang', 'Hongzhen Wang', 'Shuai Nie', 'Jiaming Xu', 'Runyu Shi', 'Ying Huang', 'Guoquan Zhang'], 'affiliations': ['Peking University', 'Tsinghua University', 'Xiaomi Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02580.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#multimodal'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ CAPO â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ curriculum, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° (advantage signals), Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. CAPO ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (GRPO, PPO, RLOO, Reinforce++) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… GUI.'}, 'en': {'title': 'Boosting Language Models with Smart Advantage Signals', 'desc': "CAPO, or Curriculum Advantage Policy Optimization, is a method that enhances reinforcement learning for large language models by using both positive and negative advantage signals. It starts by using only positive signals to build a strong foundation through imitation learning, which helps the model learn effectively. After establishing this base, it gradually introduces negative signals to improve the model's ability to differentiate between better and worse outcomes. This approach leads to better reasoning and generalization in complex tasks, making CAPO a flexible and powerful tool for optimizing language models."}, 'zh': {'title': 'CAPOï¼šä¼˜åŒ–å¼ºåŒ–å­¦ä¹ çš„è¯¾ç¨‹ä¼˜åŠ¿ç­–ç•¥', 'desc': 'CAPOï¼ˆè¯¾ç¨‹ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼‰æ˜¯ä¸€ç§å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥æ­£è´Ÿä¼˜åŠ¿ä¿¡å·ï¼Œæ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•åœ¨æ—©æœŸé˜¶æ®µæ··åˆä¿¡å·å¯èƒ½å¯¼è‡´æ¨¡ç³Šçš„æŒ‡å¯¼ï¼Œè€ŒCAPOåˆ™é‡‡ç”¨é€‚åº”æ€§è¯¾ç¨‹æœºåˆ¶ï¼Œå…ˆä½¿ç”¨æ­£ä¿¡å·å»ºç«‹åŸºç¡€ï¼Œå†å¼•å…¥è´Ÿä¿¡å·ä»¥æé«˜åŒºåˆ†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸å¤šç§ä¼˜åŒ–æ–¹æ³•å…¼å®¹ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡å’Œå¤šæ¨¡æ€å›¾å½¢ç”¨æˆ·ç•Œé¢æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05905', 'title': 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations', 'url': 'https://huggingface.co/papers/2512.05905', 'abstract': 'SCAIL framework improves character animation by using a novel 3D pose representation and a diffusion-transformer architecture with full-context pose injection, achieving studio-grade quality and realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '378c369fc6e7e469', 'authors': ['Wenhao Yan', 'Sheng Ye', 'Zhuoyi Yang', 'Jiayan Teng', 'ZhenHui Dong', 'Kairui Wen', 'Xiaotao Gu', 'Yong-Jin Liu', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Z.ai'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05905.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#3d', '#video', '#benchmark', '#diffusion', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ñ‚ÑƒĞ´Ğ¸Ğ¹Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'SCAIL â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ ÑÑ‚ÑƒĞ´Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑÑ…: Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D-Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ĞºĞ¾Ğ½\xadÑ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾\xadĞ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ SCAIL Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Character Animation with SCAIL', 'desc': 'The SCAIL framework enhances character animation by introducing a new 3D pose representation that captures motion more effectively. It utilizes a diffusion-transformer architecture with full-context pose injection, allowing for better understanding of motion over time. This approach addresses issues of structural fidelity and temporal consistency, especially in complex scenarios. The framework also includes a curated data pipeline for high-quality training, leading to state-of-the-art results in character animation.'}, 'zh': {'title': 'æå‡è§’è‰²åŠ¨ç”»è´¨é‡çš„SCAILæ¡†æ¶', 'desc': 'SCAILæ¡†æ¶é€šè¿‡é‡‡ç”¨æ–°é¢–çš„3Då§¿æ€è¡¨ç¤ºå’Œæ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œæ˜¾è‘—æå‡äº†è§’è‰²åŠ¨ç”»çš„è´¨é‡å’ŒçœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨å¤æ‚è¿åŠ¨å’Œè·¨èº«ä»½åŠ¨ç”»ä¸­ä¿æŒç»“æ„ä¸€è‡´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§çš„é—®é¢˜ã€‚SCAILå¼•å…¥äº†å…¨ä¸Šä¸‹æ–‡å§¿æ€æ³¨å…¥æœºåˆ¶ï¼Œä½¿å¾—å¯¹å®Œæ•´è¿åŠ¨åºåˆ—çš„æ—¶ç©ºæ¨ç†æ›´åŠ æœ‰æ•ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCAILåœ¨è§’è‰²åŠ¨ç”»é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‘å·¥ä½œå®¤çº§åˆ«çš„å¯é æ€§å’ŒçœŸå®æ„Ÿè¿ˆè¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05145', 'title': 'Self-Improving VLM Judges Without Human Annotations', 'url': 'https://huggingface.co/papers/2512.05145', 'abstract': 'A framework for self-training a Vision-Language Model (VLM) judge using self-synthesized data improves judge accuracy on VL-RewardBench, surpassing larger models in several dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '344947ff680da049', 'authors': ['Inna Wanyin Lin', 'Yushi Hu', 'Shuyue Stella Li', 'Scott Geng', 'Pang Wei Koh', 'Luke Zettlemoyer', 'Tim Althoff', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05145.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#interpretability', '#benchmark', '#synthetic', '#dataset'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ ÑÑƒĞ´ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑƒĞ´ÑŒĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision-Language Model Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… VL-RewardBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ 0.38 Ğ´Ğ¾ 0.51 Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3.2-11B, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ¸ Claude 3.5 Sonnet. Ğ¢Ğ°ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ´ÑŒĞ¸, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Self-Training VLM Judges: Evolving Without Human Annotations', 'desc': "This paper introduces a new framework for training a Vision-Language Model (VLM) judge without relying on human preference annotations. Instead, it uses self-synthesized data to iteratively improve the judge's accuracy. The process involves generating multimodal instruction-response pairs, evaluating their quality, and training the judge on the best examples. The results show that this self-training method significantly enhances the judge's performance on VL-RewardBench, even surpassing larger models in various evaluation metrics."}, 'zh': {'title': 'è‡ªæˆ‘è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹è¯„åˆ¤å™¨çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„åˆ¤å™¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªæˆ‘åˆæˆçš„æ•°æ®æ¥æé«˜è¯„åˆ¤å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºæ˜‚è´µçš„äººç±»åå¥½æ³¨é‡Šï¼Œè€Œæ˜¯é€šè¿‡ç”Ÿæˆå¤šæ ·åŒ–çš„å¤šæ¨¡æ€æŒ‡ä»¤-å“åº”å¯¹è¿›è¡Œè®­ç»ƒã€‚æ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šç”Ÿæˆä¸åŒè´¨é‡æ°´å¹³çš„æŒ‡ä»¤-å“åº”å¯¹ã€ä¸ºæ¯å¯¹ç”Ÿæˆæ¨ç†è½¨è¿¹å’Œåˆ¤æ–­ï¼Œå¹¶å»é™¤ä¸ç¬¦åˆé¢„æœŸè´¨é‡çš„å¯¹ï¼Œæœ€ååœ¨æ­£ç¡®çš„è¯„åˆ¤ç­”æ¡ˆå’Œæ¨ç†è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨VL-RewardBenchä¸Šæ˜¾è‘—æé«˜äº†è¯„åˆ¤å™¨çš„å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†æ— äººå·¥æ³¨é‡Šçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05044', 'title': 'Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image', 'url': 'https://huggingface.co/papers/2512.05044', 'abstract': 'MoRe4D generates high-quality 4D scenes with multi-view consistency and dynamic details from a single image using a diffusion-based trajectory generator and depth-guided motion normalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '283405418ffc5d21', 'authors': ['Yanran Zhang', 'Ziyi Wang', 'Wenzhao Zheng', 'Zheng Zhu', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Department of Automation, Tsinghua University, China', 'GigaAI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05044.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#3d', '#video', '#synthetic', '#diffusion', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ: ÑĞ¸Ğ½Ñ‚ĞµĞ· 4D ÑÑ†ĞµĞ½ Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'MoRe4D â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 4D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ TrajScene-60K Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 4D-STraG Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² 4D-ViSM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²ÑŒÑĞ¿Ğ¾Ñ€Ñ‚Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸.'}, 'en': {'title': 'Transforming Single Images into Dynamic 4D Scenes!', 'desc': 'MoRe4D is a novel approach that generates high-quality 4D scenes from a single image by integrating motion generation and geometric reconstruction. It introduces a large dataset, TrajScene-60K, which contains 60,000 video samples with detailed point trajectories to improve the training of models. The method employs a diffusion-based trajectory generator to create consistent and plausible 4D point trajectories while using depth-guided motion normalization for better integration of geometry and dynamics. Ultimately, MoRe4D enables the rendering of dynamic videos with multi-view consistency, addressing challenges in existing methods that often lead to inconsistencies in spatiotemporal data.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆé«˜è´¨é‡4Dåœºæ™¯çš„åˆ›æ–°æ–¹æ³•', 'desc': 'MoRe4Dæ˜¯ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡4Dåœºæ™¯çš„æ–¹æ³•ï¼Œå…·æœ‰å¤šè§†è§’ä¸€è‡´æ€§å’ŒåŠ¨æ€ç»†èŠ‚ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©å±•é‡å»º-ç”Ÿæˆæ¡†æ¶ï¼Œè”åˆè¿›è¡Œè¿åŠ¨ç”Ÿæˆå’Œå‡ ä½•é‡å»ºï¼Œä»¥è§£å†³æ—¶ç©ºä¸ä¸€è‡´æ€§é—®é¢˜ã€‚ç ”ç©¶è€…ä»¬è¿˜å¼•å…¥äº†TrajScene-60Kæ•°æ®é›†ï¼ŒåŒ…å«60,000ä¸ªè§†é¢‘æ ·æœ¬ï¼Œæä¾›äº†ä¸°å¯Œçš„4Dåœºæ™¯æ•°æ®ã€‚é€šè¿‡æ·±åº¦å¼•å¯¼çš„è¿åŠ¨å½’ä¸€åŒ–ç­–ç•¥ï¼ŒMoRe4Dèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå‡ ä½•å’ŒåŠ¨æ€ä¿¡æ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„4Dåœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04563', 'title': 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence', 'url': 'https://huggingface.co/papers/2512.04563', 'abstract': 'A unified multimodal large language model (MLLM) that integrates depth and segmentation modalities enhances spatial reasoning and perception through adaptive interleaved reasoning, improving spatial intelligence and general performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '3f39f24628b95ffe', 'authors': ['Zefeng Zhang', 'Xiangzhao Hao', 'Hengzhu Tang', 'Zhenyu Zhang', 'Jiawei Sheng', 'Xiaodong Li', 'Zhenyang Li', 'Li Gao', 'Daiting Shi', 'Dawei Yin', 'Tingwen Liu'], 'affiliations': ['Baidu Inc.', 'Institute of Automation, Chinese Academy of Sciences', 'Institute of Information Engineering, Chinese Academy of Sciences', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04563.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#3d', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ: Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) COOPER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6,91% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Spatial Intelligence with Unified Multimodal Learning', 'desc': 'This paper presents COOPER, a unified multimodal large language model (MLLM) designed to improve spatial reasoning by integrating depth and segmentation data. Unlike previous models that treat perception and reasoning separately, COOPER employs adaptive interleaved reasoning to enhance both aspects simultaneously. The model is trained in two stages, focusing on generating auxiliary modalities and refining reasoning capabilities, leading to significant improvements in spatial intelligence. Results show an average increase of 6.91% in spatial reasoning performance, indicating that the model effectively internalizes spatial knowledge through its training process.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æå‡ç©ºé—´æ™ºèƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œåä¸ºCOOPERï¼Œæ—¨åœ¨é€šè¿‡æ·±åº¦å’Œåˆ†å‰²æ¨¡æ€çš„ç»“åˆæ¥å¢å¼ºç©ºé—´æ¨ç†å’Œæ„ŸçŸ¥èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è‡ªé€‚åº”äº¤é”™æ¨ç†çš„æ–¹æ³•ï¼Œæå‡äº†ç©ºé—´æ™ºèƒ½å’Œæ•´ä½“æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCOOPERåœ¨ç©ºé—´æ¨ç†æ–¹é¢å¹³å‡æé«˜äº†6.91%ï¼Œå¹¶ä¸”å³ä½¿æ˜¯ä»…è®­ç»ƒè¾…åŠ©æ¨¡æ€ç”Ÿæˆçš„å˜ä½“ï¼Œåœ¨è·ç¦»å’Œå¤§å°ä¼°è®¡ä¸Šä¹Ÿè·å¾—äº†7.92%çš„æå‡ã€‚è¿™è¡¨æ˜ï¼Œå­¦ä¹ ç”Ÿæˆè¾…åŠ©æ¨¡æ€æœ‰åŠ©äºå†…åŒ–ç©ºé—´çŸ¥è¯†ï¼Œå¢å¼ºç©ºé—´ç†è§£èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05927', 'title': "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", 'url': 'https://huggingface.co/papers/2512.05927', 'abstract': "C3 is an uncertainty quantification method for training controllable video models that provides dense confidence estimation and out-of-distribution detection, addressing hallucination issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", 'score': 10, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '784e732b562082f3', 'authors': ['Zhiting Mei', 'Tenny Yin', 'Micah Baker', 'Ola Shorinwa', 'Anirudha Majumdar'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05927.jpg', 'data': {'categories': ['#training', '#interpretability', '#hallucinations', '#video', '#robotics'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ”Ğ¾Ğ²ĞµÑ€ÑĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞ¹ ĞµÑ‘ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'C3 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµä¼°estimation ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ· ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ² RGB-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'C3: Calibrating Confidence in Controllable Video Generation', 'desc': 'C3 is a novel method designed to improve the reliability of controllable video models by quantifying uncertainty during training. It addresses the problem of hallucination, where generated video frames do not align with reality, which is critical for applications like robotics. The method introduces a framework that ensures video models can accurately assess their confidence in generated outputs, using proper scoring rules and latent space uncertainty estimation. Additionally, it provides visual representations of uncertainty through high-resolution heatmaps, helping to identify areas in the video that may be unreliable.'}, 'zh': {'title': 'C3ï¼šæå‡å¯æ§è§†é¢‘æ¨¡å‹çš„ç½®ä¿¡åº¦ä¸å¼‚å¸¸æ£€æµ‹', 'desc': 'C3æ˜¯ä¸€ç§ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒå¯æ§è§†é¢‘æ¨¡å‹ï¼Œæä¾›å¯†é›†çš„ç½®ä¿¡åº¦ä¼°è®¡å’Œå¼‚å¸¸æ£€æµ‹ï¼Œè§£å†³äº†å¹»è§‰é—®é¢˜ã€‚å°½ç®¡å¯æ§è§†é¢‘æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å¸¸å¸¸ä¼šç”Ÿæˆä¸ç‰©ç†ç°å®ä¸ç¬¦çš„æœªæ¥è§†é¢‘å¸§ã€‚C3é€šè¿‡å¼•å…¥ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼Œå¸®åŠ©è§†é¢‘æ¨¡å‹è¯„ä¼°å’Œè¡¨è¾¾å…¶ä¸ç¡®å®šæ€§ï¼Œä»è€Œå‡è½»å¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒC3ä¸ä»…åœ¨è®­ç»ƒåˆ†å¸ƒå†…æä¾›äº†æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè¿˜æœ‰æ•ˆåœ°å®ç°äº†å¼‚å¸¸æ£€æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.02835', 'title': 'ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.02835', 'abstract': 'ReVSeg, a reasoning-centric video object segmentation framework, uses sequential decision-making in pretrained vision language models and reinforcement learning to achieve state-of-the-art performance and interpretable reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .', 'score': 9, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'f85a1eb3f3cd60a0', 'authors': ['Yifan Li', 'Yingda Yin', 'Lingting Zhu', 'Weikai Chen', 'Shengju Qian', 'Xin Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'LIGHTSPEED', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02835.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#interpretability', '#video', '#benchmark', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'ReVSeg â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'ReVSeg: Unraveling Video Segmentation with Reasoning', 'desc': 'ReVSeg is a novel framework designed for video object segmentation that emphasizes reasoning through sequential decision-making. It leverages pretrained vision language models (VLMs) to handle complex tasks involving dynamics and temporal interactions, rather than relying on simplified latent embeddings. The framework breaks down reasoning into three clear operations: interpreting semantics, selecting temporal evidence, and grounding spatial information. By incorporating reinforcement learning, ReVSeg enhances its decision-making process, leading to improved performance and clearer reasoning paths in video segmentation tasks.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„è§†é¢‘ç‰©ä½“åˆ†å‰²æ–°æ¡†æ¶', 'desc': 'ReVSegæ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è§†é¢‘ç‰©ä½“åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ å®ç°äº†å…ˆè¿›çš„æ€§èƒ½å’Œå¯è§£é‡Šçš„æ¨ç†ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜¾å¼åˆ†è§£æ¨ç†è¿‡ç¨‹ï¼Œæ‰§è¡Œè¯­ä¹‰è§£é‡Šã€æ—¶é—´è¯æ®é€‰æ‹©å’Œç©ºé—´å®šä½ä¸‰ä¸ªæ˜ç¡®æ“ä½œï¼Œè€Œä¸æ˜¯å°†æ‰€æœ‰æ¨ç†åˆå¹¶ä¸ºå•æ­¥é¢„æµ‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒReVSegèƒ½å¤Ÿå¤„ç†åŠ¨æ€ã€å› æœå…³ç³»å’Œæ—¶é—´äº¤äº’ç­‰å¤æ‚å› ç´ ï¼Œæä¾›æ›´æ¸…æ™°çš„æ¨ç†é“¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReVSegåœ¨æ ‡å‡†è§†é¢‘ç‰©ä½“åˆ†å‰²åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆäº†å¯è§£é‡Šçš„æ¨ç†è½¨è¿¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03514', 'title': 'M3DR: Towards Universal Multilingual Multimodal Document Retrieval', 'url': 'https://huggingface.co/papers/2512.03514', 'abstract': 'M3DR is a multilingual multimodal document retrieval framework using contrastive training to achieve robust cross-lingual and cross-modal alignment across diverse languages and document types.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '450a516e3415c50b', 'authors': ['Adithya S Kolavi', 'Vyoman Jain'], 'affiliations': ['CognitiveLab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03514.jpg', 'data': {'categories': ['#open_source', '#multilingual', '#multimodal', '#benchmark', '#synthetic', '#rag', '#dataset', '#low_resource', '#transfer_learning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†', 'desc': 'M3DR â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° 22 Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ NetraEmbed Ğ¸ ColNetraEmbed Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging Languages and Formats with M3DR!', 'desc': 'M3DR is a framework designed for multilingual multimodal document retrieval, which means it can find documents in different languages and formats, like text and images. It uses contrastive training to create strong connections between text and images, allowing it to work well across various languages and document types. The framework is tested on 22 different languages, showing that it can adapt to different linguistic and cultural contexts effectively. M3DR outperforms existing methods, achieving significant improvements in retrieving documents across languages.'}, 'zh': {'title': 'è·¨è¯­è¨€å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢çš„æœªæ¥', 'desc': 'M3DRæ˜¯ä¸€ä¸ªå¤šè¯­è¨€å¤šæ¨¡æ€æ–‡æ¡£æ£€ç´¢æ¡†æ¶ï¼Œé‡‡ç”¨å¯¹æ¯”è®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†è·¨è¯­è¨€å’Œè·¨æ¨¡æ€çš„å¼ºå¤§å¯¹é½èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•è¿‡äºé›†ä¸­äºè‹±è¯­çš„é—®é¢˜ï¼Œä½¿å…¶åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­æ›´æœ‰æ•ˆã€‚M3DRåˆ©ç”¨åˆæˆçš„å¤šè¯­è¨€æ–‡æ¡£æ•°æ®ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„è§†è§‰-è¯­è¨€æ¶æ„å’Œæ¨¡å‹è§„æ¨¡ä¸­è¿›è¡Œæ³›åŒ–ã€‚é€šè¿‡å¯¹æ¯”è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ åˆ°ç»Ÿä¸€çš„æ–‡æœ¬å’Œæ–‡æ¡£å›¾åƒè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°åœ¨å¤šç§è¯­è¨€ä¹‹é—´è½¬ç§»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05774', 'title': 'Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding', 'url': 'https://huggingface.co/papers/2512.05774', 'abstract': 'Active Video Perception (AVP) improves long video understanding by iteratively selecting and evaluating query-relevant video evidence, achieving higher accuracy with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '77d3a45363a326ec', 'authors': ['Ziyang Wang', 'Honglu Zhou', 'Shijie Wang', 'Junnan Li', 'Caiming Xiong', 'Silvio Savarese', 'Mohit Bansal', 'Michael S. Ryoo', 'Juan Carlos Niebles'], 'affiliations': ['Salesforce AI Research', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05774.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#video', '#benchmark', '#long_context'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Active Video Perception (AVP) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LLM Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. AVP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 5.7% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 18.4% Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡Ğ°ÑĞ¾Ğ² Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Active Video Perception: Smartly Watching Videos for Better Understanding', 'desc': 'Active Video Perception (AVP) enhances the understanding of long videos by selectively identifying and evaluating relevant video evidence based on specific queries. This approach addresses the challenge of long video understanding, where important cues are often hidden among irrelevant content. By employing an iterative plan-observe-reflect process, AVP allows agents to actively determine what to observe and when, optimizing the use of computational resources. The results show that AVP significantly improves accuracy while reducing inference time and input tokens compared to existing methods.'}, 'zh': {'title': 'ä¸»åŠ¨è§†é¢‘æ„ŸçŸ¥ï¼šæå‡é•¿è§†é¢‘ç†è§£çš„æ™ºèƒ½é€‰æ‹©', 'desc': 'ä¸»åŠ¨è§†é¢‘æ„ŸçŸ¥ï¼ˆAVPï¼‰é€šè¿‡è¿­ä»£é€‰æ‹©å’Œè¯„ä¼°ä¸æŸ¥è¯¢ç›¸å…³çš„è§†é¢‘è¯æ®ï¼Œæå‡äº†é•¿è§†é¢‘ç†è§£çš„æ•ˆæœï¼Œèƒ½å¤Ÿåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æé«˜å‡†ç¡®æ€§ã€‚é•¿è§†é¢‘ç†è§£ï¼ˆLVUï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå›ç­”ç°å®ä¸–ç•Œçš„é—®é¢˜å¾€å¾€ä¾èµ–äºç¨€ç–ä¸”æ—¶é—´åˆ†æ•£çš„çº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢åŸ‹è—åœ¨å¤§é‡å†—ä½™å’Œæ— å…³çš„å†…å®¹ä¸­ã€‚AVPæ¡†æ¶åŸºäºä¸»åŠ¨æ„ŸçŸ¥ç†è®ºï¼Œå…è®¸æ™ºèƒ½ä½“ä¸»åŠ¨å†³å®šè§‚å¯Ÿçš„å†…å®¹ã€æ—¶é—´å’Œåœ°ç‚¹ï¼Œå¹¶æŒç»­è¯„ä¼°å½“å‰è§‚å¯Ÿæ˜¯å¦è¶³å¤Ÿå›ç­”æŸ¥è¯¢ã€‚é€šè¿‡åœ¨äº”ä¸ªLVUåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒAVPåœ¨å¹³å‡å‡†ç¡®æ€§ä¸Šæ¯”æœ€ä½³æ™ºèƒ½æ–¹æ³•æé«˜äº†5.7%ï¼ŒåŒæ—¶ä»…éœ€18.4%çš„æ¨ç†æ—¶é—´å’Œ12.4%çš„è¾“å…¥æ ‡è®°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05564', 'title': 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation', 'url': 'https://huggingface.co/papers/2512.05564', 'abstract': 'ProPhy, a two-stage framework with Semantic and Refinement Experts, enhances video generation by incorporating physics-aware conditioning and anisotropic generation, improving physical consistency and realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '54cb0001d019f855', 'authors': ['Zijun Wang', 'Panwen Hu', 'Jing Wang', 'Terry Jingchen Zhang', 'Yuhao Cheng', 'Long Chen', 'Yiqiang Yan', 'Zutao Jiang', 'Hanhui Li', 'Xiaodan Liang'], 'affiliations': ['ETH Zurich', 'Lenovo Research', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-sen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05564.jpg', 'data': {'categories': ['#video', '#benchmark', '#optimization', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'ProPhy â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Mixture-of-Physics-Experts Ñ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ£Ñ‚Ğ¾Ñ‡Ğ½ÑÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ProPhy Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'ProPhy: Enhancing Video Generation with Physics-Aware Conditioning', 'desc': 'ProPhy is a two-stage framework designed to improve video generation by focusing on physics-aware conditioning and anisotropic generation. It uses a Mixture-of-Physics-Experts (MoPE) approach, where Semantic Experts extract physical principles from text, and Refinement Experts refine these principles at a token level. This allows the model to create video representations that align closely with real-world physical laws, addressing the limitations of existing models that often produce inconsistent results. The framework has been shown to outperform current state-of-the-art methods in generating realistic and dynamic videos that adhere to physical consistency.'}, 'zh': {'title': 'ProPhyï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§ä¸çœŸå®æ„Ÿ', 'desc': 'ProPhyæ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç»“åˆäº†è¯­ä¹‰ä¸“å®¶å’Œç²¾ç»†åŒ–ä¸“å®¶ï¼Œæ—¨åœ¨æå‡è§†é¢‘ç”Ÿæˆçš„ç‰©ç†ä¸€è‡´æ€§å’ŒçœŸå®æ„Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰©ç†æ„ŸçŸ¥æ¡ä»¶å’Œå„å‘å¼‚æ€§ç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚åŠ¨æ€æ—¶çš„å±€é™æ€§ã€‚ProPhyé‡‡ç”¨æ··åˆç‰©ç†ä¸“å®¶æœºåˆ¶ï¼Œè¯­ä¹‰ä¸“å®¶ä»æ–‡æœ¬æè¿°ä¸­æ¨æ–­ç‰©ç†åŸåˆ™ï¼Œè€Œç²¾ç»†åŒ–ä¸“å®¶åˆ™æ•æ‰ç»†ç²’åº¦çš„ç‰©ç†åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProPhyåœ¨ç‰©ç†æ„ŸçŸ¥è§†é¢‘ç”ŸæˆåŸºå‡†ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„çœŸå®æ„Ÿå’ŒåŠ¨æ€ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03667', 'title': 'Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning', 'url': 'https://huggingface.co/papers/2512.03667', 'abstract': 'Colon-X advances multimodal intelligence in colonoscopy by constructing comprehensive datasets and developing reasoning-centric models that outperform traditional methods under data scarcity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '03cf54cce0cd3c66', 'authors': ['Ge-Peng Ji', 'Jingyi Liu', 'Deng-Ping Fan', 'Nick Barnes'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03667.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#training', '#optimization', '#multimodal', '#benchmark', '#healthcare', '#science', '#dataset'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ° Colon-X Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ColonVQA â€” ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1.1 Ğ¼Ğ»Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾ 76 ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°Ğ¼ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ColonR1 Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ fine-tuning Ğ½Ğ° 25.22% Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Colonoscopy with Multimodal Intelligence and Reasoning', 'desc': 'Colon-X is an initiative that enhances multimodal intelligence in colonoscopy by creating extensive datasets and developing advanced reasoning models. The project introduces ColonVQA, a large dataset with over 1.1 million entries for visual question answering related to colonoscopy. It also addresses the need for improved clinical reasoning by assessing the performance of multimodal large language models and developing ColonR1, a model that incorporates innovative optimization techniques. This model demonstrates significant improvements in accuracy under data-scarce conditions, establishing a new standard for multimodal analysis in colonoscopy.'}, 'zh': {'title': 'æ¨åŠ¨ç»“è‚ é•œæ£€æŸ¥çš„å¤šæ¨¡æ€æ™ºèƒ½é©å‘½', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†Colon-Xï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¨åŠ¨ç»“è‚ é•œæ£€æŸ¥ä¸­å¤šæ¨¡æ€æ™ºèƒ½çš„å¼€æ”¾é¡¹ç›®ã€‚æˆ‘ä»¬æ„å»ºäº†ColonVQAï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„ç»“è‚ é•œå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡110ä¸‡ä¸ªè§†è§‰é—®ç­”æ¡ç›®ï¼Œæ¶µç›–76ç§ä¸´åºŠå‘ç°å’Œ18ç§å¤šæ¨¡æ€ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ç»“è‚ é•œæ£€æŸ¥ä¸­ä»å¤šæ¨¡æ€ç†è§£åˆ°ä¸´åºŠæ¨ç†çš„å…³é”®è½¬å˜ï¼Œå¼€å‘äº†ColonReasonæ•°æ®é›†å’ŒColonR1æ¨¡å‹ï¼Œä»¥æé«˜åœ¨æ•°æ®ç¨€ç¼ºæ¡ä»¶ä¸‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ColonR1æ¨¡å‹åœ¨æ•´ä½“å‡†ç¡®ç‡ä¸Šè¾¾åˆ°äº†56.61%ï¼Œæ¯”ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•æé«˜äº†25.22%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05409', 'title': 'SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs', 'url': 'https://huggingface.co/papers/2512.05409', 'abstract': 'A Sparse-Quantized Format (SQ-format) is proposed to improve the balance between accuracy and efficiency in post-training quantization of large language models by leveraging sparse and low-precision matrix multiplications.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'b90fe4ddbb262b34', 'authors': ['Ruixuan Huang', 'Hao Zeng', 'Hantao Huang', 'Jinyuan Shi', 'Minghui Yu', 'Ian En-Hsu Yen', 'Shuai Wang'], 'affiliations': ['ByteDance', 'HKUST', 'Moffett AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05409.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Sparse-Quantized (SQ-format) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ±Ğ¸Ñ‚Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºÑƒ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ°Ñ€ĞµÑ‚Ğ¾Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ AI-ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Achieving Efficiency and Accuracy with Sparse-Quantized Format', 'desc': 'The paper introduces a new data format called Sparse-Quantized Format (SQ-format) aimed at enhancing the efficiency and accuracy of post-training quantization (PTQ) for large language models. It addresses the challenges of existing low-bit quantization methods that struggle to maintain performance due to hardware limitations. By leveraging both sparse and low-precision matrix multiplications, SQ-format allows for better utilization of GPU resources while minimizing accuracy loss. The authors demonstrate that this format can lead to significant improvements in PTQ performance and provide insights for future AI hardware designs.'}, 'zh': {'title': 'ç¨€ç–é‡åŒ–æ ¼å¼ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨€ç–é‡åŒ–æ ¼å¼ï¼ˆSQ-formatï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒé‡åŒ–ä¸­çš„å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚ç°æœ‰çš„ä½ä½é‡åŒ–å’Œç¨€ç–åŒ–æŠ€æœ¯ç”±äºç¡¬ä»¶æ”¯æŒçš„é™åˆ¶ï¼Œéš¾ä»¥å®ç°è¿™ä¸€å¹³è¡¡ã€‚SQ-formatåˆ©ç”¨ç¨€ç–çŸ©é˜µåœ¨é«˜ç²¾åº¦ä¸‹çš„åŠ é€Ÿç‰¹æ€§ï¼ŒåŒæ—¶ä¹Ÿèƒ½åŠ é€Ÿä½ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼Œä»è€Œå®ç°æ€§èƒ½ä¸ååé‡çš„å¸•ç´¯æ‰˜æ”¹è¿›ã€‚è¯¥æ ¼å¼ç‰¹åˆ«é€‚åˆå¤„ç†å…·æœ‰å¼‚å¸¸å€¼ä¸å¹³ç­‰çŠ¶æ€çš„æ¿€æ´»ï¼Œå¹¶ä½¿å…¶é™æ€å‹ç¼©æˆä¸ºå¯èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.04142', 'title': 'From FLOPs to Footprints: The Resource Cost of Artificial Intelligence', 'url': 'https://huggingface.co/papers/2512.04142', 'abstract': 'The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4, emphasizing the need for resource-efficient strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': '4edb871569fa468e', 'authors': ['Sophia Falk', 'Nicholas Kluge CorrÃªa', 'Sasha Luccioni', 'Lisa Biber-Freudenberger', 'Aimee van Wynsberghe'], 'affiliations': ['Center for Development Research, Bonn University, Germany', 'Center for Science and Thought, Bonn University, Germany', 'Hugging Face', 'Sustainable AI Lab, Institute for Science and Ethics, Bonn University, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04142.jpg', 'data': {'categories': ['#ethics', '#training', '#optimization', '#inference'], 'emoji': 'â™»ï¸', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ñ†ĞµĞ½Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ GPU Nvidia A100, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 90% ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚ÑĞ¶ĞµĞ»Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GPT-4 Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ 1,174 Ğ´Ğ¾ 8,800 GPU Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ 7 Ñ‚Ğ¾Ğ½Ğ½ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ€Ğ¾ĞºĞ° ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ GPU Ğ½Ğ° 93%. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Optimizing AI Training for a Greener Future', 'desc': 'This study evaluates the material footprint of AI training, particularly focusing on the Nvidia A100 GPU. It highlights that the environmental impact of training models like GPT-4 extends beyond just energy and water usage to include the physical materials required for specialized hardware. By analyzing the elemental composition of the A100 GPU, the research reveals that it is primarily made up of heavy metals, with significant implications for resource extraction and disposal. The findings advocate for optimizing both software and hardware to reduce material demands, emphasizing the importance of sustainability in AI development.'}, 'zh': {'title': 'AIè®­ç»ƒçš„ææ–™è¶³è¿¹ä¸ç¯å¢ƒè´£ä»»', 'desc': 'æœ¬ç ”ç©¶é‡åŒ–äº†äººå·¥æ™ºèƒ½è®­ç»ƒçš„ææ–™è¶³è¿¹ï¼Œé‡ç‚¹åˆ†æäº†Nvidia A100 GPUçš„ç¯å¢ƒå½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAIç¡¬ä»¶ä¸­çº¦90%æ˜¯é‡é‡‘å±ï¼Œåªæœ‰å¾®é‡çš„è´µé‡‘å±ï¼Œä¸»è¦æˆåˆ†åŒ…æ‹¬é“œã€é“ã€é”¡ã€ç¡…å’Œé•ã€‚é€šè¿‡å¤šæ­¥éª¤çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†è¿™äº›æµ‹é‡ç»“æœä¸ä¸åŒè®­ç»ƒæ•ˆç‡ä¸‹çš„è®¡ç®—éœ€æ±‚ç›¸ç»“åˆï¼Œå‘ç°è®­ç»ƒGPT-4éœ€è¦çš„A100 GPUæ•°é‡åœ¨1174åˆ°8800ä¹‹é—´ã€‚ç ”ç©¶å¼ºè°ƒï¼Œæœªæ¥çš„AIè¿›å±•å¿…é¡»ä¸èµ„æºæ•ˆç‡å’Œç¯å¢ƒè´£ä»»åŸåˆ™ç›¸ä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05339', 'title': 'Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models', 'url': 'https://huggingface.co/papers/2512.05339', 'abstract': 'Roblox Guard 1.0 is an instruction fine-tuned LLM that enhances safety through comprehensive input-output moderation using a pipeline of LLMs and demonstrates strong performance on out-of-domain safety benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '34096ba1e4d22c6e', 'authors': ['Mahesh Kumar Nandwana', 'Youngwan Lim', 'Joseph Liu', 'Alex Yang', 'Varun Notibala', 'Nishchaie Khanna'], 'affiliations': ['Roblox'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05339.jpg', 'data': {'categories': ['#security', '#open_source', '#training', '#architecture', '#alignment', '#benchmark', '#synthetic', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ²', 'desc': 'Roblox Guard 1.0 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ LLM-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Llama-3.1-8B-Instruct, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…, Ğ½Ğµ Ğ²ĞºĞ»ÑÑ‡Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Â«Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹Â» Ğ¸ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RobloxGuard-Eval Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² LLM.'}, 'en': {'title': 'Enhancing Safety in LLMs with Roblox Guard 1.0', 'desc': "Roblox Guard 1.0 is a specialized large language model (LLM) that focuses on improving safety by moderating both inputs and outputs effectively. It is fine-tuned with instructions to ensure it can handle various safety scenarios, even those it hasn't encountered before. The model uses a combination of synthetic and open-source datasets, along with techniques like chain-of-thought reasoning, to enhance its understanding and decision-making capabilities. Additionally, the introduction of RobloxGuard-Eval provides a new benchmark for evaluating the safety measures of LLMs, ensuring they meet high standards of moderation."}, 'zh': {'title': 'å¢å¼ºå®‰å…¨æ€§çš„ Roblox Guard 1.0', 'desc': 'Roblox Guard 1.0 æ˜¯ä¸€ç§ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å…¨é¢çš„è¾“å…¥è¾“å‡ºå®¡æ ¸æ¥å¢å¼ºå®‰å…¨æ€§ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†ä¸€ç³»åˆ— LLM çš„ç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ½œåœ¨çš„é£é™©è¾“å‡ºã€‚é€šè¿‡ç»“åˆåˆæˆå’Œå¼€æºçš„å®‰å…¨æ•°æ®é›†ï¼ŒRoblox Guard 1.0 åœ¨æœªè§è¿‡çš„å®‰å…¨åˆ†ç±»ä¸Šè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº† RobloxGuard-Eval åŸºå‡†ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼° LLM çš„å®‰å…¨é˜²æŠ¤å’Œå®¡æ ¸æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07461', 'title': 'Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.07461', 'abstract': "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", 'score': 63, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '338d051f90e86520', 'authors': ['Tong Wu', 'Yang Liu', 'Jun Bai', 'Zixia Jia', 'Shuyi Zhang', 'Ziyong Lin', 'Yanting Wang', 'Song-Chun Zhu', 'Zilong Zheng'], 'affiliations': ['Beijing Institute for General Artificial Intelligence (BIGAI)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07461.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#benchmark', '#small_models'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'NPR â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğº ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼, Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Parallel-Aware Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. NPR Engine Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 24,5% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 4,6x Ğ¿Ñ€Ğ¸ 100% Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Native Parallel Reasoning', 'desc': 'The Native Parallel Reasoner (NPR) is a framework designed to enhance Large Language Models (LLMs) by enabling them to perform parallel reasoning without the need for a teacher. It introduces a self-distilled training approach that allows models to evolve their reasoning capabilities independently. NPR also features a Parallel-Aware Policy Optimization (PAPO) algorithm that helps the model learn effective strategies for decision-making through experimentation. As a result, NPR achieves significant improvements in both performance and speed, setting a new benchmark for efficient reasoning in AI.'}, 'zh': {'title': 'NPRï¼šå¼€å¯çœŸæ­£çš„å¹¶è¡Œæ¨ç†æ–°æ—¶ä»£', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNPRçš„æ— æ•™å¸ˆæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸç”Ÿå¹¶è¡Œæ¨ç†èƒ½åŠ›ã€‚NPRé€šè¿‡è‡ªæˆ‘è’¸é¦è®­ç»ƒã€å¹¶è¡Œæ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–å’Œå¼ºå¤§çš„NPRå¼•æ“ï¼Œå®ç°äº†ä»é¡ºåºæ¨¡æ‹Ÿåˆ°åŸç”Ÿå¹¶è¡Œè®¤çŸ¥çš„è½¬å˜ã€‚è¯¥æ¡†æ¶åœ¨å…«ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜è¾¾24.5%çš„æ€§èƒ½æå‡å’Œ4.6å€çš„æ¨ç†é€Ÿåº¦åŠ å¿«ã€‚ä¸ä»¥å¾€çš„åŸºçº¿ç›¸æ¯”ï¼ŒNPRå®ç°äº†100%çš„çœŸæ­£å¹¶è¡Œæ‰§è¡Œï¼Œæ ‘ç«‹äº†è‡ªæˆ‘è¿›åŒ–ã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ™ºèƒ½æ¨ç†çš„æ–°æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07469', 'title': 'Unified Video Editing with Temporal Reasoner', 'url': 'https://huggingface.co/papers/2512.07469', 'abstract': 'VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.', 'score': 41, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': 'a3dca53d4b568421', 'authors': ['Xiangpeng Yang', 'Ji Xie', 'Yiyuan Yang', 'Yan Huang', 'Min Xu', 'Qiang Wu'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07469.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#architecture', '#video', '#diffusion', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ²: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ°ÑĞ¾Ğº Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VideoCoF Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Chain-of-Frames Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ), Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ»ĞµĞ´ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğµ Â«Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ». Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ RoPE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50k Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'VideoCoF: Precision Video Editing Through Reasoning Tokens', 'desc': 'VideoCoF is a novel approach to video editing that enhances precision and mapping of instructions to specific regions in a video. It utilizes a Chain-of-Frames method that incorporates reasoning tokens, allowing the model to predict edit regions without needing user-provided masks. This method improves the alignment of instructions to video content by enforcing a structured process of seeing, reasoning, and then editing. Additionally, the RoPE alignment strategy ensures that motion is consistent and can extend beyond the original video length, achieving state-of-the-art results with minimal data requirements.'}, 'zh': {'title': 'VideoCoFï¼šç²¾å‡†è§†é¢‘ç¼–è¾‘çš„æ–°æ–¹æ³•', 'desc': 'VideoCoFæ˜¯ä¸€ç§é“¾å¸§æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ¨ç†æ ‡è®°æ¥æé«˜è§†é¢‘ç¼–è¾‘çš„ç²¾ç¡®åº¦å’ŒæŒ‡ä»¤åˆ°åŒºåŸŸçš„æ˜ å°„ï¼Œè€Œæ— éœ€ç”¨æˆ·æä¾›çš„æ©ç ã€‚ç°æœ‰çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•é¢ä¸´ç€ç²¾åº¦ä¸ç»Ÿä¸€æ€§çš„æƒè¡¡ï¼Œä¸“å®¶æ¨¡å‹ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„å…ˆéªŒçŸ¥è¯†ï¼Œè€Œç»Ÿä¸€çš„å­¦ä¹ æ¨¡å‹åˆ™ç¼ºä¹æ˜ç¡®çš„ç©ºé—´çº¿ç´¢ã€‚VideoCoFé€šè¿‡å¼ºåˆ¶è§†é¢‘æ‰©æ•£æ¨¡å‹é¦–å…ˆé¢„æµ‹æ¨ç†æ ‡è®°ï¼Œç„¶åå†ç”Ÿæˆç›®æ ‡è§†é¢‘æ ‡è®°ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§RoPEå¯¹é½ç­–ç•¥ï¼Œä»¥ç¡®ä¿è¿åŠ¨å¯¹é½å¹¶æ”¯æŒè¶…å‡ºè®­ç»ƒæ—¶é•¿çš„é•¿åº¦å¤–æ¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07783', 'title': 'On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models', 'url': 'https://huggingface.co/papers/2512.07783', 'abstract': "A controlled experimental framework isolates and evaluates the contributions of pre-training, mid-training, and reinforcement learning in improving language model reasoning, demonstrating the necessity of each phase and the role of process-level rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", 'score': 29, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '93a20c7506c2655e', 'authors': ['Charlie Zhang', 'Graham Neubig', 'Xiang Yue'], 'affiliations': ['Carnegie Mellon University, Language Technologies Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07783.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Â«Ğ·Ğ°Ğ¿Ğ°ÑĞ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ RL Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ñ‹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ¸Ğ³Ñ€Ñ‹ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Language Model Reasoning: The Power of Training Phases', 'desc': 'This paper investigates how different training phasesâ€”pre-training, mid-training, and reinforcement learning (RL)â€”contribute to the reasoning abilities of language models. It introduces a controlled experimental framework to isolate these contributions and evaluates models on their ability to generalize in complex reasoning tasks. The findings reveal that RL enhances capabilities only when pre-training is effective and that mid-training plays a crucial role in improving performance. Additionally, the study highlights the importance of process-level rewards in enhancing reasoning accuracy and reducing reward hacking.'}, 'zh': {'title': 'æ­ç¤ºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡çš„å…³é”®é˜¶æ®µ', 'desc': 'æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå—æ§å®éªŒæ¡†æ¶ï¼Œä»¥è¯„ä¼°é¢„è®­ç»ƒã€ä¸­æœŸè®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸­çš„è´¡çŒ®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒã€ä¸­æœŸè®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å„é˜¶æ®µéƒ½æ˜¯å¿…è¦çš„ï¼Œä¸”è¿‡ç¨‹çº§å¥–åŠ±åœ¨å…¶ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚é€šè¿‡åˆæˆæ¨ç†ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æä¸åŒè®­ç»ƒé˜¶æ®µçš„å› æœå…³ç³»ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸­æœŸè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ä¾èµ–äºé¢„è®­ç»ƒçš„å……åˆ†æ€§å’Œä»»åŠ¡çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06065', 'title': 'EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing', 'url': 'https://huggingface.co/papers/2512.06065', 'abstract': 'EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit', 'score': 23, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'fc437da58a265a3d', 'authors': ['Runjia Li', 'Moayed Haji-Ali', 'Ashkan Mirzaei', 'Chaoyang Wang', 'Arpit Sahni', 'Ivan Skorokhodov', 'Aliaksandr Siarohin', 'Tomas Jakab', 'Junlin Han', 'Sergey Tulyakov', 'Philip Torr', 'Willi Menapace'], 'affiliations': ['Rice University', 'Snap Research', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06065.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#inference', '#video', '#benchmark', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸', 'desc': 'EgoEdit â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EgoEditData Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² EgoEditBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ÑƒĞº Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Egocentric Video Editing in Real-Time', 'desc': 'EgoEdit is a novel video editing tool designed specifically for egocentric videos, which are recorded from a first-person perspective. It tackles the unique challenges of egomotion and hand-object interactions that traditional video editors struggle with. By utilizing a specially curated dataset called EgoEditData, it ensures that hand movements and interactions are accurately preserved during editing. The system operates in real-time on a single GPU, providing fast and reliable editing results that outperform existing methods in egocentric scenarios while maintaining competitive performance in general video editing tasks.'}, 'zh': {'title': 'å®æ—¶è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç¼–è¾‘çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'EgoEdit æ˜¯ä¸€ä¸ªå®æ—¶çš„ã€éµå¾ªæŒ‡ä»¤çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç¼–è¾‘å™¨ï¼Œä¸“é—¨è§£å†³è‡ªæˆ‘è¿åŠ¨å’Œæ‰‹ç‰©ä½“äº¤äº’çš„æŒ‘æˆ˜ã€‚å®ƒåœ¨è‡ªæˆ‘ä¸­å¿ƒç¼–è¾‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¥½çš„ç¼–è¾‘æ•ˆæœã€‚æˆ‘ä»¬æ„å»ºäº† EgoEditData æ•°æ®é›†ï¼Œä¸“æ³¨äºä¸°å¯Œçš„æ‰‹ç‰©ä½“äº¤äº’ï¼Œå¹¶ä¿ç•™æ‰‹éƒ¨ä¿¡æ¯ã€‚EgoEdit è¿˜æ”¯æŒåœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œå®æ—¶æ¨ç†ï¼Œç¡®ä¿äº†ä½å»¶è¿Ÿçš„äº¤äº’ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07778', 'title': 'Distribution Matching Variational AutoEncoder', 'url': 'https://huggingface.co/papers/2512.07778', 'abstract': "DMVAE explicitly aligns the encoder's latent distribution with a reference distribution, improving modeling efficiency and image synthesis fidelity compared to conventional VAEs.  \t\t\t\t\tAI-generated summary \t\t\t\t Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", 'score': 20, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '36921e5514ed4873', 'authors': ['Sen Ye', 'Jianning Pei', 'Mengde Xu', 'Shuyang Gu', 'Chunyu Wang', 'Liwei Wang', 'Han Hu'], 'affiliations': ['Peking University', 'Tencent', 'UCAS'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07778.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#architecture', '#diffusion', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Distribution-Matching VAE (DMVAE) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… VAE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼, DMVAE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· self-supervised Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°ĞºĞ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· self-supervised feature, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ gFID = 3.2 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 64 ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Aligning Latent Distributions for Better Image Synthesis', 'desc': "The paper introduces the Distribution-Matching VAE (DMVAE), which enhances the performance of visual generative models by explicitly aligning the encoder's latent distribution with a chosen reference distribution. This approach allows for greater flexibility compared to traditional VAEs, which typically rely on a fixed Gaussian prior. By using distribution matching constraints, DMVAE can adapt to various distributions, including those derived from self-supervised learning and diffusion processes. The results demonstrate that this method significantly improves image synthesis quality and modeling efficiency, achieving a notable gFID score on ImageNet with minimal training epochs."}, 'zh': {'title': 'åˆ†å¸ƒåŒ¹é…ï¼Œæå‡å›¾åƒåˆæˆè´¨é‡', 'desc': 'DMVAEï¼ˆåˆ†å¸ƒåŒ¹é…å˜åˆ†è‡ªç¼–ç å™¨ï¼‰é€šè¿‡å°†ç¼–ç å™¨çš„æ½œåœ¨åˆ†å¸ƒä¸å‚è€ƒåˆ†å¸ƒæ˜¾å¼å¯¹é½ï¼Œæå‡äº†å»ºæ¨¡æ•ˆç‡å’Œå›¾åƒåˆæˆçš„ä¿çœŸåº¦ã€‚ä¸ä¼ ç»Ÿçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸åŒï¼ŒDMVAEå…è®¸ä¸ä»»æ„å‚è€ƒåˆ†å¸ƒå¯¹é½ï¼Œè€Œä¸ä»…ä»…æ˜¯é«˜æ–¯å…ˆéªŒã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°ç ”ç©¶å“ªäº›æ½œåœ¨åˆ†å¸ƒæ›´é€‚åˆå»ºæ¨¡ï¼Œå¹¶å‘ç°è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡ç”Ÿçš„åˆ†å¸ƒåœ¨é‡å»ºä¿çœŸåº¦å’Œå»ºæ¨¡æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€‰æ‹©åˆé€‚çš„æ½œåœ¨åˆ†å¸ƒç»“æ„æ˜¯å®ç°é«˜ä¿çœŸå›¾åƒåˆæˆçš„å…³é”®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06589', 'title': 'OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation', 'url': 'https://huggingface.co/papers/2512.06589', 'abstract': 'OmniSafeBench-MM is a comprehensive tool for evaluating multi-modal jailbreak attacks and defenses, covering various attack methods, defense strategies, and risk domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'ad55b75eeb1085ca', 'authors': ['Xiaojun Jia', 'Jie Liao', 'Qi Guo', 'Teng Ma', 'Simeng Qin', 'Ranjie Duan', 'Tianlin Li', 'Yihao Huang', 'Zhitao Zeng', 'Dongxian Wu', 'Yiming Li', 'Wenqi Ren', 'Xiaochun Cao', 'Yang Liu'], 'affiliations': ['Alibaba, China', 'BraneMatrix AI, China', 'ByteDance, China', 'Chongqing University, China', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Northeastern University, China', 'Sun Yat-sen University, China', 'Xian Jiaotong University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06589.jpg', 'data': {'categories': ['#security', '#open_source', '#multimodal', '#benchmark', '#alignment', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'OmniSafeBench-MM Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 13 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº, 15 ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 9 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ€Ğ¸ÑĞºĞ° Ñ 50 Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 10 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ 8 Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Strengthening Multi-Modal Models Against Jailbreak Attacks', 'desc': 'OmniSafeBench-MM is a new tool designed to evaluate how well multi-modal large language models (MLLMs) can withstand jailbreak attacks. It includes a wide range of attack methods and defense strategies, making it a comprehensive resource for researchers. The tool assesses vulnerabilities across various risk domains and uses a detailed evaluation protocol to measure harmfulness, intent alignment, and response detail. By providing a standardized and reproducible framework, OmniSafeBench-MM aims to enhance the understanding and safety of MLLMs against potential threats.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ä¸é˜²å¾¡çš„å·¥å…·', 'desc': 'OmniSafeBench-MM æ˜¯ä¸€ä¸ªå…¨é¢çš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€çš„è¶Šç‹±æ”»å‡»å’Œé˜²å¾¡ã€‚å®ƒæ•´åˆäº†13ç§ä»£è¡¨æ€§çš„æ”»å‡»æ–¹æ³•å’Œ15ç§é˜²å¾¡ç­–ç•¥ï¼Œè¦†ç›–äº†9ä¸ªä¸»è¦é£é™©é¢†åŸŸå’Œ50ä¸ªç»†åˆ†ç±»åˆ«ã€‚è¯¥å·¥å…·å»ºç«‹äº†ä¸€ä¸ªä¸‰ç»´è¯„ä¼°åè®®ï¼Œæµ‹é‡æœ‰å®³æ€§ã€æ„å›¾ä¸€è‡´æ€§å’Œå“åº”ç»†èŠ‚æ°´å¹³ï¼Œä»¥ä¾¿è¿›è¡Œç»†è‡´çš„å®‰å…¨æ€§å’Œæ•ˆç”¨åˆ†æã€‚é€šè¿‡ç»Ÿä¸€æ•°æ®ã€æ–¹æ³•å’Œè¯„ä¼°ï¼ŒOmniSafeBench-MM ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07584', 'title': 'LongCat-Image Technical Report', 'url': 'https://huggingface.co/papers/2512.07584', 'abstract': 'LongCat-Image is a bilingual open-source foundation model for image generation that addresses multilingual text rendering, photorealism, and deployment efficiency through rigorous data curation, compact design, and comprehensive open-source support.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '19dd8c92e6e77b12', 'authors': ['Meituan LongCat Team', 'Hanghang Ma', 'Haoxian Tan', 'Jiale Huang', 'Junqiang Wu', 'Jun-Yan He', 'Lishuai Gao', 'Songlin Xiao', 'Xiaoming Wei', 'Xiaoqi Ma', 'Xunliang Cai', 'Yayong Guan', 'Jie Hu'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07584.jpg', 'data': {'categories': ['#data', '#open_source', '#multilingual', '#training', '#architecture', '#inference', '#small_models', '#diffusion', '#dataset', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'LongCat-Image â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ (ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ°Ñ) Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ reward models Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ² 6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MoE Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'LongCat-Image: Bridging Languages with Photorealistic Image Generation', 'desc': 'LongCat-Image is an innovative bilingual foundation model designed for image generation, focusing on both Chinese and English text rendering. It utilizes advanced data curation techniques throughout its training phases, leading to state-of-the-art performance in photorealism and text accuracy, especially for complex Chinese characters. The model is compact, with only 6 billion parameters, making it efficient for deployment while maintaining high-quality output. Additionally, it fosters community engagement by providing a comprehensive open-source ecosystem, including various model versions and training tools to support developers and researchers.'}, 'zh': {'title': 'é•¿çŒ«å›¾åƒï¼šå¼€åˆ›åŒè¯­å›¾åƒç”Ÿæˆæ–°æ ‡å‡†', 'desc': 'LongCat-Imageæ˜¯ä¸€ä¸ªå¼€æºçš„åŒè¯­åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºå›¾åƒç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“ã€çœŸå®æ„Ÿå’Œéƒ¨ç½²æ•ˆç‡ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†å’Œç´§å‡‘çš„è®¾è®¡ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å’ŒçœŸå®æ„Ÿæ–¹é¢è¾¾åˆ°äº†æ–°çš„è¡Œä¸šæ ‡å‡†ï¼Œå°¤å…¶åœ¨ä¸­æ–‡å­—ç¬¦çš„æ¸²æŸ“ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å®ƒçš„æ ¸å¿ƒæ‰©æ•£æ¨¡å‹ä»…æœ‰6äº¿å‚æ•°ï¼Œæ˜¾è‘—å°äºè¡Œä¸šå†…å¸¸è§çš„20äº¿å‚æ•°æ¨¡å‹ï¼Œç¡®ä¿äº†ä½æ˜¾å­˜ä½¿ç”¨å’Œå¿«é€Ÿæ¨ç†ã€‚LongCat-Imageè¿˜åœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†å…¨é¢çš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œæ”¯æŒå¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¨åŠ¨è§†è§‰å†…å®¹åˆ›ä½œçš„å‰æ²¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.03244', 'title': 'SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.03244', 'abstract': 'A three-stage framework, SPARK, uses a generator and verifier to create synthetic training data for process reward models, enabling reference-free reinforcement learning that surpasses ground-truth methods in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.', 'score': 14, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'eced3f0c0df8f415', 'authors': ['Salman Rahman', 'Sruthi Gorantla', 'Arpit Gupta', 'Swastik Roy', 'Nanyun Peng', 'Yang Liu'], 'affiliations': ['Amazon AGI', 'UCLA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03244.jpg', 'data': {'categories': ['#rl', '#reasoning', '#data', '#training', '#optimization', '#benchmark', '#synthetic', '#rlhf', '#math'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ SPARK â€” Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SPARK: Revolutionizing Reinforcement Learning with Synthetic Data', 'desc': 'The paper introduces SPARK, a three-stage framework designed to enhance reinforcement learning by generating synthetic training data for process reward models (PRMs). In the first stage, a generator creates diverse solutions, while a verifier assesses these solutions using both self-consistency and meta-critique methods. The second stage utilizes the verification results to fine-tune PRMs, which then provide reward signals during training. The final stage demonstrates that this approach achieves superior performance in mathematical reasoning tasks compared to traditional ground-truth methods, enabling effective reinforcement learning without the need for expensive annotations.'}, 'zh': {'title': 'SPARKï¼šè¶…è¶ŠçœŸå®æ•°æ®çš„æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPARKçš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ä»¥æ”¹è¿›è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç”Ÿæˆå™¨æ¨¡å‹äº§ç”Ÿå¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆï¼ŒéªŒè¯å™¨æ¨¡å‹é€šè¿‡è‡ªä¸€è‡´æ€§å’Œå…ƒæ‰¹è¯„å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¿™äº›éªŒè¯è¾“å‡ºä½œä¸ºåˆæˆè®­ç»ƒæ•°æ®ï¼Œå¾®è°ƒç”Ÿæˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä»¥åœ¨è®­ç»ƒä¸­æä¾›å¥–åŠ±ä¿¡å·ã€‚æœ€ç»ˆï¼ŒSPARKåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡åŸºäºçœŸå®æ•°æ®çš„æ–¹æ³•çš„è¡¨ç°ï¼Œå±•ç¤ºäº†æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06373', 'title': 'VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.06373', 'abstract': 'The VG-Refiner framework improves tool-integrated visual reasoning by introducing a two-stage mechanism to handle unreliable tool outputs and enhance accuracy in referring and grounding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': 'bdfd82bfcf4f8442', 'authors': ['Yuji Wang', 'Wenlong Liu', 'Jingxuan Niu', 'Haoji Zhang', 'Yansong Tang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06373.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#hallucinations', '#benchmark', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼: ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° VG-Refiner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ´ÑƒĞ¼Ğ°Ğ¹-Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€ÑƒĞ½Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ (refinement reward), Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Refining Visual Reasoning with VG-Refiner', 'desc': "The VG-Refiner framework enhances tool-integrated visual reasoning (TiVR) by implementing a two-stage mechanism that addresses unreliable outputs from visual tools. This framework specifically targets referring and grounding tasks, which often suffer from inaccuracies in tool predictions leading to misleading reasoning. By introducing a think-rethink process, VG-Refiner allows the model to critically evaluate tool feedback and apply corrections effectively. Additionally, it establishes new metrics for evaluating refinement capabilities, resulting in improved accuracy and correction performance while maintaining the pretrained model's general abilities."}, 'zh': {'title': 'VG-Refinerï¼šæå‡è§†è§‰æ¨ç†çš„å·¥å…·é›†æˆèƒ½åŠ›', 'desc': 'VG-Refineræ¡†æ¶é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µæœºåˆ¶æ¥æ”¹å–„å·¥å…·é›†æˆçš„è§†è§‰æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸å¯é çš„å·¥å…·è¾“å‡ºæ—¶ã€‚è¯¥æ¡†æ¶ä¸“æ³¨äºæé«˜å¼•ç”¨å’Œå®šä½ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å¯¹é”™è¯¯å·¥å…·è¾“å‡ºååº”ä¸è¶³çš„é—®é¢˜ã€‚VG-Refineré€šè¿‡åˆ†æå’Œå“åº”å·¥å…·åé¦ˆï¼Œç»“åˆä¿®æ­£å¥–åŠ±æœºåˆ¶ï¼Œé¼“åŠ±æ¨¡å‹æœ‰æ•ˆçº æ­£é”™è¯¯ã€‚é€šè¿‡ä½¿ç”¨å°‘é‡ç‰¹å®šä»»åŠ¡æ•°æ®ï¼ŒVG-Refineråœ¨å¼•ç”¨å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œçº æ­£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07829', 'title': 'One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation', 'url': 'https://huggingface.co/papers/2512.07829', 'abstract': 'FAE, a framework using a feature auto-encoder and dual decoders, adapts pre-trained visual representations for generative models, achieving high performance in image generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '7fd0877bda27e798', 'authors': ['Yuan Gao', 'Chen Chen', 'Tianrong Chen', 'Jiatao Gu'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07829.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#cv', '#transfer_learning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'FAE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒĞ¼Ñ Ğ´ĞµcodĞµÑ€Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. FAE ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'FAE: Bridging Features and Generative Models for Superior Image Generation', 'desc': 'FAE (Feature Auto-Encoder) is a novel framework designed to adapt pre-trained visual representations for use in generative models, specifically targeting image generation tasks. It effectively bridges the gap between high-dimensional feature spaces and low-dimensional latent spaces, which are essential for generating high-quality images. By utilizing two separate decodersâ€”one for reconstructing the original features and another for generating imagesâ€”FAE maintains crucial information while simplifying the adaptation process. The framework shows impressive results across various benchmarks, achieving state-of-the-art performance in both class-conditional and text-to-image generation tasks.'}, 'zh': {'title': 'FAEï¼šé«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ¡†æ¶', 'desc': 'FAEï¼ˆç‰¹å¾è‡ªç¼–ç å™¨ï¼‰æ˜¯ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨ç‰¹å¾è‡ªç¼–ç å™¨å’ŒåŒè§£ç å™¨ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰è¡¨ç¤ºé€‚åº”äºç”Ÿæˆæ¨¡å‹ï¼Œä»è€Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜ç»´ç‰¹å¾æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œè§£å†³äº†ç†è§£å¯¼å‘ç‰¹å¾ä¸ç”Ÿæˆå‹å¥½æ½œåœ¨ç©ºé—´ä¹‹é—´çš„åŸºæœ¬ä¸åŒ¹é…é—®é¢˜ã€‚FAEçš„å…³é”®åœ¨äºç»“åˆä¸¤ä¸ªç‹¬ç«‹çš„æ·±åº¦è§£ç å™¨ï¼Œä¸€ä¸ªç”¨äºé‡å»ºåŸå§‹ç‰¹å¾ç©ºé—´ï¼Œå¦ä¸€ä¸ªåˆ™åˆ©ç”¨é‡å»ºçš„ç‰¹å¾è¿›è¡Œå›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAEåœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå­¦ä¹ é€Ÿåº¦ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.07805', 'title': 'Group Representational Position Encoding', 'url': 'https://huggingface.co/papers/2512.07805', 'abstract': 'GRAPE is a unified positional encoding framework that combines multiplicative rotations and additive logit biases, extending existing methods like RoPE and ALiBi.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,Ï‰,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-08', 'pub_date_card': {'ru': '8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 8', 'zh': '12æœˆ8æ—¥'}, 'hash': '78d1417c6281ee3d', 'authors': ['Yifan Zhang', 'Zixiang Chen', 'Yifeng Liu', 'Zhen Qin', 'Huizhuo Yuan', 'Kangping Xu', 'Yang Yuan', 'Quanquan Gu', 'Andrew Chi-Chih Yao'], 'affiliations': ['IIIS, Tsinghua University', 'Princeton University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.07805.jpg', 'data': {'categories': ['#open_source', '#long_context', '#architecture', '#math'], 'emoji': 'ğŸŒ€', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GRAPE â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ SO(d) Ğ¸ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑƒĞ½Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ GRAPE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RoPE ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑ‚Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ GRAPE Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ALiBi Ğ¸ Forgetting Transformer ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°-1, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GRAPE Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'GRAPE: Unifying Positional Encoding for Enhanced Model Performance', 'desc': 'GRAPE is a new framework for positional encoding in machine learning that integrates two key methods: multiplicative rotations and additive logit biases. It uses mathematical structures from group theory to create a unified approach that enhances how models understand position in data. The framework allows for efficient representation of complex relationships between features, improving performance in tasks with long contexts. By encompassing existing methods like RoPE and ALiBi, GRAPE provides a flexible and powerful tool for developing advanced models.'}, 'zh': {'title': 'GRAPEï¼šç»Ÿä¸€çš„ä½ç½®ç¼–ç æ¡†æ¶', 'desc': 'GRAPEæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œç»“åˆäº†ä¹˜æ³•æ—‹è½¬å’ŒåŠ æ³•é€»è¾‘åç½®ï¼Œæ‰©å±•äº†ç°æœ‰çš„æ–¹æ³•å¦‚RoPEå’ŒALiBiã€‚å®ƒå°†ä¸¤ç§æœºåˆ¶ç»“åˆåœ¨ä¸€èµ·ï¼šä¹˜æ³•GRAPEå’ŒåŠ æ³•GRAPEï¼Œåˆ†åˆ«åŸºäºç¾¤ä½“ä½œç”¨ã€‚ä¹˜æ³•GRAPEé€šè¿‡åœ¨SO(d)ä¸­çš„æ—‹è½¬å®ç°ç›¸å¯¹çš„ã€ç»„åˆçš„ã€ä¿æŒèŒƒæ•°çš„æ˜ å°„ï¼Œè€ŒåŠ æ³•GRAPEåˆ™é€šè¿‡ä½ç§©çš„å•å…ƒä½œç”¨ç”ŸæˆåŠ æ³•é€»è¾‘ã€‚GRAPEä¸ºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä¸­çš„ä½ç½®å‡ ä½•æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„è®¾è®¡ç©ºé—´ï¼Œæ¶µç›–äº†RoPEå’ŒALiBiä½œä¸ºç‰¹ä¾‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06963', 'title': 'VideoVLA: Video Generators Can Be Generalizable Robot Manipulators', 'url': 'https://huggingface.co/papers/2512.06963', 'abstract': "VideoVLA uses a multi-modal Diffusion Transformer to predict actions and visual outcomes from language and image inputs, enabling strong generalization in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': '4bad1a680c044c57', 'authors': ['Yichao Shen', 'Fangyun Wei', 'Zhiying Du', 'Yaobo Liang', 'Yan Lu', 'Jiaolong Yang', 'Nanning Zheng', 'Baining Guo'], 'affiliations': ['Fudan University', 'IAIR, Xian Jiaotong University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06963.jpg', 'data': {'categories': ['#agi', '#multimodal', '#architecture', '#video', '#robotics', '#diffusion'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'VideoVLA â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Diffusion Transformer Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering Robots with Visual Imagination for Action Prediction', 'desc': 'VideoVLA is a novel approach that utilizes a multi-modal Diffusion Transformer to enhance robotic manipulation by predicting actions and visual outcomes based on language and image inputs. This method addresses the limitations of existing Vision-Language-Action (VLA) models, which struggle to generalize to new tasks and environments. By leveraging pre-trained video generation models, VideoVLA effectively combines video, language, and action modalities to forecast both the actions a robot should take and the expected visual results. The findings indicate that this dual-prediction strategy not only improves task success but also enables robots to adapt to unfamiliar objects and skills, marking a significant advancement in robot learning.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ“æ§çš„ç»“åˆ', 'desc': 'VideoVLAæ˜¯ä¸€ç§å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€å’Œå›¾åƒè¾“å…¥é¢„æµ‹åŠ¨ä½œå’Œè§†è§‰ç»“æœï¼Œä»è€Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†å¤§å‹è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºæœºå™¨äººVLAæ“æ§å™¨ï¼Œæ¢ç´¢äº†è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ“ä½œçš„ç»“åˆã€‚å®éªŒè¡¨æ˜ï¼Œé«˜è´¨é‡çš„æƒ³è±¡æœªæ¥ä¸å¯é çš„åŠ¨ä½œé¢„æµ‹å’Œä»»åŠ¡æˆåŠŸç‡å¯†åˆ‡ç›¸å…³ï¼Œå¼ºè°ƒäº†è§†è§‰æƒ³è±¡åœ¨æ“ä½œä¸­çš„é‡è¦æ€§ã€‚VideoVLAå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡ä»¿å…¶ä»–å®ä¾‹çš„æŠ€èƒ½å’Œå¤„ç†æ–°ç‰©ä½“ï¼Œæ¨åŠ¨äº†æœºå™¨äººå­¦ä¹ çš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06835', 'title': 'Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning', 'url': 'https://huggingface.co/papers/2512.06835', 'abstract': 'DoGe, a dual-decoupling framework, enhances vision-language models by separating context learning from problem solving, using a curriculum learning pipeline to improve reward signals and data diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': '0a8fcf1d1267bd3c', 'authors': ['Tingyu Li', 'Zheng Sun', 'Jingxuan Wei', 'Siyuan Li', 'Conghui He', 'Lijun Wu', 'Cheng Tan'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai JiaoTong University', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06835.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#multimodal', '#benchmark', '#synthetic', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DoGe â€” ÑÑ‚Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ñ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RL Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Â«ĞœÑ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒÂ» Ğ¸ Â«Ğ ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÂ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ reward hacking. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ curriculum learning Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿ÑƒĞ»Ğ¾Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Decoupling Context and Problem Solving for Enhanced Vision-Language Models', 'desc': 'The paper introduces DoGe, a dual-decoupling framework designed to improve vision-language models (VLMs) by separating the learning of context from the actual problem-solving process. This approach utilizes a curriculum learning pipeline to enhance the quality of reward signals and increase the diversity of training data, addressing challenges in specialized domains. By decoupling the learning into two components, Thinker and Solver, the framework allows for better quantification of rewards and a more effective reinforcement learning (RL) strategy. Experimental results demonstrate that DoGe significantly outperforms existing methods, paving the way for more robust self-evolving large vision-language models (LVLMs).'}, 'zh': {'title': 'è§£è€¦å­¦ä¹ ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›', 'desc': 'DoGeæ˜¯ä¸€ç§åŒé‡è§£è€¦æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å°†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸é—®é¢˜è§£å†³åˆ†å¼€ï¼Œåˆ©ç”¨è¯¾ç¨‹å­¦ä¹ æµç¨‹æ¥æ”¹å–„å¥–åŠ±ä¿¡å·å’Œæ•°æ®å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¼•å¯¼æ¨¡å‹ä»ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›´æ¥è§£å†³é—®é¢˜ï¼Œä»è€Œé¿å…äº†åˆæˆæ•°æ®æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDoGeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºå®ç°è‡ªæˆ‘è¿›åŒ–çš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06421', 'title': 'Rethinking Training Dynamics in Scale-wise Autoregressive Generation', 'url': 'https://huggingface.co/papers/2512.06421', 'abstract': 'Self-Autoregressive Refinement (SAR) improves the quality of autoregressive generative models by addressing exposure bias through Stagger-Scale Rollout and Contrastive Student-Forcing Loss, leading to consistent improvements with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '4eb23fe19101d4d8', 'authors': ['Gengze Zhou', 'Chongjian Ge', 'Hao Tan', 'Feng Liu', 'Yicong Hong'], 'affiliations': ['Adobe Research', 'Australian Institute for Machine Learning, Adelaide University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06421.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Autoregressive Refinement (SAR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ exposure bias Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ coarse-to-fine. SAR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Stagger-Scale Rollout, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ train-test Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ Contrastive Student-Forcing Loss Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAR Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ SAR Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Autoregressive Models with Self-Autoregressive Refinement', 'desc': "Self-Autoregressive Refinement (SAR) enhances autoregressive generative models by tackling exposure bias, which occurs when models generate outputs based on their own previous predictions. It introduces a Stagger-Scale Rollout (SSR) technique that allows models to learn from their intermediate outputs, improving the alignment between training and testing phases. Additionally, the Contrastive Student-Forcing Loss (CSFL) provides better supervision for the model's self-generated contexts, leading to more stable training. Experimental results demonstrate that SAR significantly improves generation quality with minimal computational costs, making it a promising method for visual autoregressive generation."}, 'zh': {'title': 'è‡ªå›å½’ç²¾ç‚¼ï¼šæå‡ç”Ÿæˆæ¨¡å‹è´¨é‡çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'è‡ªå›å½’ç²¾ç‚¼ï¼ˆSARï¼‰é€šè¿‡å¼•å…¥åˆ†å±‚è§„æ¨¡å›æ»šå’Œå¯¹æ¯”å­¦ç”Ÿå¼ºåˆ¶æŸå¤±ï¼Œæ”¹å–„äº†è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ï¼Œè§£å†³äº†æ›å…‰åå·®é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„è‡ªå›å½’å›æ»šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¥è§¦åˆ°è‡ªèº«çš„ä¸­é—´é¢„æµ‹ï¼Œä»è€Œå¯¹é½è®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼ã€‚SARè¿˜æä¾›äº†è¶³å¤Ÿçš„ç›‘ç£ï¼Œä»¥ç¡®ä¿è‡ªç”Ÿæˆä¸Šä¸‹æ–‡çš„ç¨³å®šè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSARåœ¨é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹ä¸Šåº”ç”¨åï¼Œç”Ÿæˆè´¨é‡æ˜¾è‘—æé«˜ï¼Œè®¡ç®—å¼€é”€æå°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06558', 'title': 'Embodied Referring Expression Comprehension in Human-Robot Interaction', 'url': 'https://huggingface.co/papers/2512.06558', 'abstract': 'A large-scale dataset and multimodal model improve embodied interaction comprehension in robots by addressing perspective bias and enhancing multimodal signal integration.  \t\t\t\t\tAI-generated summary \t\t\t\t As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-06', 'pub_date_card': {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'}, 'hash': '49b6bc2ca4f8291d', 'authors': ['Md Mofijul Islam', 'Alexi Gladstone', 'Sujan Sarker', 'Ganesh Nanduru', 'Md Fahim', 'Keyan Du', 'Aman Chadha', 'Tariq Iqbal'], 'affiliations': ['Amazon GenAI', 'Stanford University', 'University of Dhaka', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06558.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#robotics', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Refer360, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ MuRes â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ MuRes Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Robot Comprehension with Multimodal Learning and Diverse Datasets', 'desc': 'This paper addresses the challenge of robots understanding human instructions through embodied interactions, which is essential for effective human-robot interaction (HRI). It introduces the Refer360 dataset, a comprehensive collection of verbal and nonverbal interactions captured from multiple perspectives in various environments. The authors also propose MuRes, a multimodal guided residual module that enhances the integration of different types of signals to improve comprehension of referring expressions. Experimental results show that incorporating MuRes significantly boosts the performance of existing multimodal models in understanding embodied interactions.'}, 'zh': {'title': 'æå‡æœºå™¨äººç†è§£èƒ½åŠ›çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†Refer360æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„æœºå™¨äººå¯¹äººç±»æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¤šè§’åº¦æ”¶é›†å®¤å†…å’Œå®¤å¤–çš„è‡ªç„¶äº’åŠ¨ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­å­˜åœ¨çš„è§†è§’åå·®å’Œéè¯­è¨€æ‰‹åŠ¿è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MuResæ¨¡å—ï¼Œå®ƒé€šè¿‡æå–ç‰¹å®šæ¨¡æ€çš„å…³é”®ä¿¡æ¯ï¼Œå¢å¼ºäº†æœºå™¨äººå¯¹æŒ‡ä»£è¡¨è¾¾çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆMuResçš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£äººç±»äº’åŠ¨æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå±•ç¤ºäº†å…¶åœ¨æœºå™¨äººä¸äººç±»ç¯å¢ƒä¸­åº”ç”¨çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.06791', 'title': 'Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games', 'url': 'https://huggingface.co/papers/2512.06791', 'abstract': "The SGN condition provides a framework for certifying convergence of gradient-based learning in games by constructing a weighted block metric, enabling convergence under conditions where Euclidean geometry fails.  \t\t\t\t\tAI-generated summary \t\t\t\t Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-07', 'pub_date_card': {'ru': '7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 7', 'zh': '12æœˆ7æ—¥'}, 'hash': '31de0f6349469bd7', 'authors': ['Vedansh Sharma'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.06791.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#games', '#math'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ³Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° SGN (Small-Gain Nash) Ğ´Ğ»Ñ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğ° Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ›Ğ¸Ğ¿ÑˆĞ¸Ñ†Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…, Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğµ, Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑˆĞ°Ğ³Ğ°. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ÑÑ Ğ½Ğ° Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ….'}, 'en': {'title': 'Certifying Convergence in Non-Monotone Games with SGN', 'desc': 'The paper introduces the Small-Gain Nash (SGN) condition, which provides a new way to certify convergence in gradient-based learning for games, especially when traditional methods fail. It constructs a weighted block metric that allows the pseudo-gradient to be strongly monotone, even in cases where it is not in Euclidean geometry. This approach translates local curvature and cross-player Lipschitz bounds into a practical certificate of contraction, ensuring that the learning process converges. The authors validate their method on quadratic games and extend it to other geometries, creating a comprehensive certification pipeline for non-monotone games.'}, 'zh': {'title': 'SGNæ¡ä»¶ï¼šåšå¼ˆå­¦ä¹ æ”¶æ•›çš„æ–°æ¡†æ¶', 'desc': 'SGNæ¡ä»¶ä¸ºåŸºäºæ¢¯åº¦çš„åšå¼ˆå­¦ä¹ æä¾›äº†ä¸€ä¸ªæ”¶æ•›æ€§è¯æ˜æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºåŠ æƒå—åº¦é‡ï¼Œä½¿å¾—åœ¨æ¬§å‡ é‡Œå¾—å‡ ä½•å¤±æ•ˆçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°æ”¶æ•›ã€‚ä¼ ç»Ÿçš„æ”¶æ•›ä¿è¯è¦æ±‚ä¼ªæ¢¯åº¦åœ¨æ¬§å‡ é‡Œå¾—å‡ ä½•ä¸­æ˜¯ï¼ˆå¼ºï¼‰å•è°ƒçš„ï¼Œä½†åœ¨è®¸å¤šç®€å•åšå¼ˆä¸­è¿™ä¸€æ¡ä»¶å¸¸å¸¸ä¸æˆç«‹ã€‚SGNå°†å±€éƒ¨æ›²ç‡å’Œè·¨ç©å®¶çš„Lipschitzè€¦åˆç•Œé™è½¬åŒ–ä¸ºå¯å¤„ç†çš„æ”¶ç¼©è¯æ˜ï¼Œå¹¶åœ¨è¿™äº›ç•Œé™æˆç«‹çš„åŒºåŸŸå†…ä½¿ä¼ªæ¢¯åº¦åœ¨åŠ æƒå—åº¦é‡ä¸‹å˜å¾—å¼ºå•è°ƒã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªè®¤è¯çš„â€œæ—¶é—´å°ºåº¦å¸¦â€ï¼Œä¸ºéæ¸è¿‘æ€§ã€åŸºäºåº¦é‡çš„æ”¶æ•›è¯æ˜æä¾›äº†æ”¯æŒï¼Œç¡®ä¿åœ¨ç‰¹å®šçš„åº¦é‡æƒé‡ä¸‹ï¼Œå•æ­¥åŠ¨æ€æ˜¯å¯è¯æ˜çš„æ”¶ç¼©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.05100', 'title': 'Structured Document Translation via Format Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.05100', 'abstract': 'Format Reinforcement Learning enhances structured text translation by optimizing structure-aware rewards and distinguishing between minor errors and major structural failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FormatRL), which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'c7232cc74496b432', 'authors': ['Haiyue Song', 'Johannes Eschbach-Dymanus', 'Hour Kaing', 'Sumire Honda', 'Hideki Tanaka', 'Bianka Buschbeck', 'Masao Utiyama'], 'affiliations': ['National Institute of Information and Communications Technology, Japan', 'SAP, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05100.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#benchmark', '#rlhf', '#machine_translation'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Format Reinforcement Learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ: TreeSim Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° XML-Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¸ Node-chrF Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑƒĞ·Ğ»Ğ¾Ğ². Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° StrucAUC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ±Ğ¾Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ SAP Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'Optimizing Structured Text Translation with Format Reinforcement Learning', 'desc': 'This paper introduces Format Reinforcement Learning (FormatRL) to improve structured text translation, particularly for complex document formats like XML and HTML. It utilizes Group Relative Policy Optimization to enhance a supervised model by optimizing structure-aware rewards, specifically TreeSim and Node-chrF, which evaluate structural similarity and translation quality, respectively. The method also incorporates StrucAUC, a metric that differentiates between minor and major errors in structure. Experiments show that FormatRL significantly enhances translation quality and structural accuracy on the SAP software-documentation benchmark.'}, 'zh': {'title': 'æ ¼å¼å¼ºåŒ–å­¦ä¹ ï¼šä¼˜åŒ–ç»“æ„åŒ–æ–‡æœ¬ç¿»è¯‘çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ ¼å¼å¼ºåŒ–å­¦ä¹ ï¼ˆFormatRLï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„ç»“æ„åŒ–æ–‡æœ¬ç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤æ‚çš„æ–‡æ¡£çº§XMLæˆ–HTMLç»“æ„ã€‚è¯¥æ–¹æ³•åœ¨ç›‘ç£å¾®è°ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç›´æ¥ä¼˜åŒ–æ–°çš„ç»“æ„æ„ŸçŸ¥å¥–åŠ±ï¼ŒåŒ…æ‹¬æ ‘ç»“æ„ç›¸ä¼¼åº¦å’ŒXMLèŠ‚ç‚¹ç¿»è¯‘è´¨é‡çš„è¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ç»†ç²’åº¦çš„StrucAUCæŒ‡æ ‡ï¼Œèƒ½å¤ŸåŒºåˆ†å°é”™è¯¯å’Œé‡å¤§ç»“æ„å¤±è´¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…­ä¸ªæŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºä¸åŒå¥–åŠ±å‡½æ•°å¯¹ç»“æ„å’Œç¿»è¯‘è´¨é‡çš„æ”¹å–„æœ‰é‡è¦è´¡çŒ®ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (6)', '#agents (9)', '#agi (3)', '#alignment (10)', '#architecture (24)', '#audio (1)', '#benchmark (38)', '#cv (15)', '#data (3)', '#dataset (23)', '#diffusion (17)', '#ethics (3)', '#games (3)', '#graphs', '#hallucinations (7)', '#healthcare (2)', '#inference (10)', '#interpretability (11)', '#leakage', '#long_context (6)', '#low_resource (2)', '#machine_translation (1)', '#math (5)', '#multilingual (3)', '#multimodal (34)', '#open_source (28)', '#optimization (32)', '#plp', '#rag (1)', '#reasoning (28)', '#rl (18)', '#rlhf (6)', '#robotics (6)', '#science (3)', '#security (5)', '#small_models (3)', '#story_generation', '#survey (1)', '#synthetic (9)', '#training (40)', '#transfer_learning (4)', '#video (22)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-12-11 09:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-11 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-11 09:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    