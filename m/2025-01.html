
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 67 papers. January 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Январь 2025</span> | <span id="title-articles-count">67 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2024-12.html">⬅️ <span id="prev-date">12.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-02.html">➡️ <span id="next-date">02.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Январь 2025', 'en': 'January 2025', 'zh': '1月2025年'};
        let feedDateNext = {'ru': '02.2025', 'en': '02/2025', 'zh': '2月2025年'};
        let feedDatePrev = {'ru': '12.2024', 'en': '12/2024', 'zh': '12月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.02976', 'title': 'STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution', 'url': 'https://huggingface.co/papers/2501.02976', 'abstract': 'Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (e.g., CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce~\\name (Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate~\\name~outperforms state-of-the-art methods on both synthetic and real-world datasets.', 'score': 36, 'issue_id': 1527, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '13ac412646c508f5', 'authors': ['Rui Xie', 'Yinhong Liu', 'Penghao Zhou', 'Chen Zhao', 'Jun Zhou', 'Kai Zhang', 'Zhenyu Zhang', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Southwest University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02976.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#multimodal', '#video'], 'emoji': '🎥', 'ru': {'title': 'Качественное суперразрешение видео с помощью T2V моделей', 'desc': 'Представлена новая методика STAR для суперразрешения видео в реальных условиях с использованием моделей text-to-video. Предложен модуль LIEM для улучшения локальных деталей и устранения артефактов деградации. Введена функция потерь Dynamic Frequency для усиления точности восстановления на разных частотах. Эксперименты показывают превосходство STAR над современными методами на синтетических и реальных датасетах.'}, 'en': {'title': 'Enhancing Video Quality with T2V Models for Real-World Super-Resolution', 'desc': 'This paper presents a new method called Spatial-Temporal Augmentation with T2V models for Real-world video super-resolution, which aims to improve video quality by addressing issues of over-smoothing and temporal consistency. Traditional image diffusion models struggle with video because they are designed for static images, leading to challenges in capturing motion dynamics. The proposed approach incorporates a Local Information Enhancement Module to enhance local details and reduce artifacts, along with a Dynamic Frequency Loss to maintain fidelity across different frequency components. Experimental results show that this method outperforms existing techniques in both synthetic and real-world scenarios, providing better spatial and temporal quality in restored videos.'}, 'zh': {'title': '提升视频超分辨率的时空一致性', 'desc': '本文提出了一种新方法，名为~\\name~，用于提高真实世界视频超分辨率的时空质量。该方法结合了文本到视频（T2V）模型，以解决传统生成对抗网络（GAN）方法中的过平滑问题。通过引入局部信息增强模块（LIEM）和动态频率损失（DF Loss），该方法能够有效改善视频的局部细节和时间一致性。实验结果表明，~\\name~在合成和真实世界数据集上均优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2501.03226', 'title': 'BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning', 'url': 'https://huggingface.co/papers/2501.03226', 'abstract': "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.", 'score': 21, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '94a01c7d4516c725', 'authors': ['Beichen Zhang', 'Yuhong Liu', 'Xiaoyi Dong', 'Yuhang Zang', 'Pan Zhang', 'Haodong Duan', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.03226.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'BoostStep: Повышение точности рассуждений ИИ в решении математических задач', 'desc': 'Статья представляет метод BoostStep для улучшения решения сложных математических задач большими языковыми моделями. BoostStep решает проблемы несоответствия детализации и негативного шума в примерах обучения в контексте. Метод выравнивает гранулярность между извлечением и рассуждением на уровне шагов, предоставляя релевантные примеры для каждого шага рассуждения. BoostStep повышает качество рассуждений модели и может интегрироваться с методами поиска по дереву Монте-Карло для улучшения генерации кандидатов и принятия решений.'}, 'en': {'title': 'Boosting Reasoning Quality in Large Language Models with BoostStep', 'desc': "This paper introduces BoostStep, a method designed to enhance the reasoning quality of large language models (LLMs) when solving complex math problems. It addresses two main issues: granularity-mismatch and negative-effect noise in in-context learning (ICL) examples, which can lead to inaccurate reasoning. By aligning the granularity of retrieved examples with the specific reasoning steps required, BoostStep provides more relevant ICL examples, improving the model's performance. The method not only boosts standalone reasoning but also integrates effectively with Monte Carlo Tree Search (MCTS) to enhance decision-making processes."}, 'zh': {'title': '提升推理质量的BoostStep方法', 'desc': '这篇论文探讨了大型语言模型（LLMs）在解决复杂数学问题时的表现，特别是通过分而治之的策略和上下文学习（ICL）示例的辅助。研究发现，ICL示例中的粒度不匹配和负面噪声问题限制了模型的改进潜力。为了解决这些问题，论文提出了BoostStep方法，它通过对每个推理步骤的粒度进行对齐，提供更相关的ICL示例，从而提高推理质量。BoostStep不仅提升了独立推理的性能，还能与蒙特卡洛树搜索（MCTS）方法无缝集成，进一步优化候选生成和决策过程。'}}}, {'id': 'https://huggingface.co/papers/2501.03218', 'title': 'Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction', 'url': 'https://huggingface.co/papers/2501.03218', 'abstract': 'Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at https://github.com/Mark12Ding/Dispider.', 'score': 20, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '1e9974be2d206516', 'authors': ['Rui Qian', 'Shuangrui Ding', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.03218.jpg', 'data': {'categories': ['#long_context', '#video', '#optimization', '#architecture', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'Dispider: Интеллектуальное взаимодействие с видео в реальном времени', 'desc': 'Статья представляет систему Dispider для активного взаимодействия с видео в реальном времени с использованием языковых моделей. Система разделяет процессы восприятия, принятия решений и реакции, что позволяет эффективно обрабатывать потоковое видео и взаимодействовать с пользователем. Dispider использует легковесный модуль обработки видео для отслеживания потока и определения оптимальных моментов для взаимодействия. Асинхронная архитектура обеспечивает своевременные и точные ответы при длительной обработке видеопотоков.'}, 'en': {'title': 'Dispider: Real-time Interaction Redefined for Video LLMs', 'desc': 'This paper introduces Dispider, a system designed for active real-time interaction with video using large language models (LLMs). Unlike traditional offline models, Dispider can process video streams continuously while engaging with users, requiring three key capabilities: Perception, Decision, and Reaction. The system addresses conflicts between these capabilities by disentangling them, allowing for efficient monitoring and interaction without lag. Experimental results demonstrate that Dispider outperforms previous models in streaming scenarios, providing timely and contextually relevant responses during long-duration video interactions.'}, 'zh': {'title': '主动实时交互的新范式', 'desc': '本论文介绍了一种名为Dispider的系统，旨在实现视频大语言模型的主动实时交互。该系统通过分离感知、决策和反应三个能力，解决了实时交互中的固有冲突。Dispider具备轻量级的流媒体处理模块，能够实时监控视频流并识别最佳交互时机。实验结果表明，Dispider在传统视频问答任务中表现优异，并在流媒体场景响应上显著超越了之前的在线模型。'}}}, {'id': 'https://huggingface.co/papers/2501.02157', 'title': 'Personalized Graph-Based Retrieval for Large Language Models', 'url': 'https://huggingface.co/papers/2501.02157', 'abstract': 'As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.', 'score': 16, 'issue_id': 1527, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': '65e3736cfc1e3295', 'authors': ['Steven Au', 'Cameron J. Dimacali', 'Ojasmitha Pedirappagari', 'Namyong Park', 'Franck Dernoncourt', 'Yu Wang', 'Nikos Kanakaris', 'Hanieh Deilamsalehy', 'Ryan A. Rossi', 'Nesreen K. Ahmed'], 'affiliations': ['Adobe Research', 'Cisco AI Research', 'Meta AI', 'University of California Santa Cruz', 'University of Oregon', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2501.02157.jpg', 'data': {'categories': ['#rag', '#optimization', '#graphs', '#multimodal', '#benchmark', '#games'], 'emoji': '🕸️', 'ru': {'title': 'Графы знаний на службе персонализации языковых моделей', 'desc': 'Статья представляет новый подход к персонализации ответов больших языковых моделей (LLM) под названием PGraphRAG. В отличие от существующих методов, полагающихся на историю пользователя, PGraphRAG использует ориентированные на пользователя графы знаний для обогащения контекста. Этот метод улучшает понимание контекста и качество генерируемых ответов, особенно в сценариях с ограниченными данными о пользователе. Экспериментальные результаты показывают, что PGraphRAG превосходит современные методы персонализации в различных задачах.'}, 'en': {'title': 'Revolutionizing Personalization with Graph-based Retrieval', 'desc': "This paper introduces a new framework called Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) that enhances the personalization of large language models (LLMs). Unlike traditional methods that depend only on user history, PGraphRAG utilizes user-centric knowledge graphs to provide richer context for generating responses. By integrating structured user information into the retrieval process, it improves the model's understanding and the quality of its outputs, especially in situations where user data is limited. The authors also present a benchmark for evaluating personalized text generation, showing that PGraphRAG outperforms existing methods in various tasks."}, 'zh': {'title': '个性化图谱提升生成质量', 'desc': '随着大型语言模型的发展，它们在提供个性化和上下文感知的响应方面展现出巨大的潜力。现有的个性化方法通常仅依赖用户历史数据来增强提示，这在数据稀疏的冷启动场景中效果有限。为了解决这些问题，我们提出了个性化图谱检索增强生成（PGraphRAG）框架，利用以用户为中心的知识图谱来丰富个性化。实验结果表明，PGraphRAG在多种任务中显著优于现有的个性化方法，展示了基于图谱的检索在个性化中的独特优势。'}}}, {'id': 'https://huggingface.co/papers/2501.02497', 'title': 'Test-time Computing: from System-1 Thinking to System-2 Thinking', 'url': 'https://huggingface.co/papers/2501.02497', 'abstract': "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.", 'score': 15, 'issue_id': 1528, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': '7d9414c60fe7701d', 'authors': ['Yixin Ji', 'Juntao Li', 'Hai Ye', 'Kaixin Wu', 'Jia Xu', 'Linjian Mo', 'Min Zhang'], 'affiliations': ['Ant Group', 'Department of Computer Science, National University of Singapore', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02497.jpg', 'data': {'categories': ['#reasoning', '#math', '#survey', '#training'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование вычислений: путь к мышлению System-2', 'desc': 'Эта статья рассматривает масштабирование вычислений во время тестирования для улучшения производительности моделей машинного обучения. Авторы прослеживают эволюцию этой концепции от моделей System-1 до моделей System-2. В работе описываются различные методы, такие как обновление параметров, модификация входных данных и древовидный поиск. Исследование подчеркивает ключевую роль вычислений во время тестирования в переходе от моделей System-1 к сильным моделям System-2.'}, 'en': {'title': 'Unlocking Model Potential: The Power of Test-Time Computing', 'desc': 'This paper explores the concept of test-time computing scaling and its impact on machine learning models, particularly in enhancing reasoning capabilities. It distinguishes between System-1 models, which focus on improving robustness and generalization through techniques like parameter updating and output calibration, and System-2 models, which utilize methods such as repeated sampling and self-correction for complex problem-solving. The authors trace the evolution from System-1 to System-2 thinking, emphasizing how test-time computing plays a crucial role in this transition. Additionally, the paper identifies potential future research directions in this area.'}, 'zh': {'title': '测试时计算：从系统-1到强系统-2的关键转变', 'desc': '这篇论文探讨了测试时计算扩展对机器学习模型的影响，特别是在复杂推理中的应用。作者指出，测试时计算可以通过参数更新、输入修改、表示编辑和输出校准来提高模型的鲁棒性和泛化能力。对于系统-2模型，测试时计算通过重复采样、自我修正和树搜索来增强模型的推理能力。论文还强调了测试时计算在从系统-1模型向弱系统-2模型再到强系统-2模型转变中的关键作用，并提出了一些未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2501.02045', 'title': 'METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring', 'url': 'https://huggingface.co/papers/2501.02045', 'abstract': 'We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.', 'score': 12, 'issue_id': 1528, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '60a3568f555ed60f', 'authors': ['Ollie Liu', 'Sami Jaghouar', 'Johannes Hagemann', 'Shangshang Wang', 'Jason Wiemels', 'Jeff Kaufman', 'Willie Neiswanger'], 'affiliations': ['Nucleic Acid Observatory', 'Prime Intellect', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2501.02045.jpg', 'data': {'categories': ['#benchmark', '#data', '#training', '#architecture', '#science', '#dataset', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'METAGENE-1: Метагеномная модель для мониторинга здоровья населения', 'desc': 'METAGENE-1 - это автореграссивная трансформерная модель с 7 миллиардами параметров, обученная на разнообразных метагеномных последовательностях ДНК и РНК. Модель создана для анализа геномной информации из образцов сточных вод с целью мониторинга пандемий и обнаружения патогенов. Авторы описывают процесс предобучения, включая токенизацию и архитектуру модели, а также демонстрируют результаты на различных геномных задачах. METAGENE-1 показывает высокую эффективность в обнаружении патогенов человека и встраивании геномных последовательностей, что открывает перспективы для применения в общественном здравоохранении.'}, 'en': {'title': 'Unlocking Metagenomics: METAGENE-1 for Pandemic Preparedness', 'desc': 'The paper introduces METAGENE-1, a large autoregressive transformer model designed for metagenomic data analysis. It is pretrained on a vast dataset of metagenomic DNA and RNA sequences derived from human wastewater, totaling over 1.5 trillion base pairs. The model aims to enhance pandemic monitoring and pathogen detection by capturing the diverse genomic information present in wastewater samples. The authors detail their tokenization strategy and model architecture, demonstrating that METAGENE-1 achieves state-of-the-art performance in genomic benchmarks and applications related to public health.'}, 'zh': {'title': 'METAGENE-1：元基因组基础模型助力公共卫生监测', 'desc': '我们预训练了METAGENE-1，这是一个拥有70亿参数的自回归变换器模型，称为元基因组基础模型。该模型在一个包含超过1.5万亿碱基对的多样化元基因组DNA和RNA序列的新数据集上进行训练，这些数据来自大量人类废水样本。METAGENE-1的目标是捕捉废水中存在的基因组信息的完整分布，以帮助进行疫情监测和病原体检测。我们展示了该模型在元基因组数据集上的预训练结果，证明其在公共卫生应用中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.02690', 'title': 'GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking', 'url': 'https://huggingface.co/papers/2501.02690', 'abstract': '4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.', 'score': 11, 'issue_id': 1530, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'b4c147a2637166a8', 'authors': ['Weikang Bian', 'Zhaoyang Huang', 'Xiaoyu Shi', 'Yijin Li', 'Fu-Yun Wang', 'Hongsheng Li'], 'affiliations': ['Avolution AI', 'Centre for Perceptual and Interactive Intelligence', 'Multimedia Laboratory, The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.02690.jpg', 'data': {'categories': ['#video', '#games', '#diffusion', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в генерации видео: 4D-контроль с помощью гауссовых полей', 'desc': 'Эта статья представляет инновационный подход к генерации видео с 4D-контролем, используя псевдо-4D гауссовы поля и модель Diffusion Transformer (DiT). Авторы предлагают метод Dense 3D Point Tracking (D3D-PT) для эффективного построения гауссовых полей, превосходящий существующие решения по точности и скорости. Разработанная система GS-DiT позволяет генерировать видео с одинаковым динамическим содержанием, но с разными параметрами камеры, что открывает новые возможности для создания кинематографических эффектов. Метод демонстрирует сильные обобщающие способности и расширяет возможности 4D-контроля в генерации видео.'}, 'en': {'title': 'Revolutionizing Video Generation with 4D Control', 'desc': 'This paper introduces a new method for generating videos that can be controlled in four dimensions (4D), which includes both camera movement and object motion. The authors propose a framework called GS-DiT that utilizes pseudo 4D Gaussian fields to enhance video generation, allowing for advanced cinematic effects. They also present a Dense 3D Point Tracking (D3D-PT) technique that improves the accuracy and speed of tracking 3D points compared to existing methods. Overall, GS-DiT enables the creation of dynamic videos with flexible camera parameters, significantly advancing the capabilities of video generation models.'}, 'zh': {'title': '伪4D高斯场：视频生成的新突破', 'desc': '本论文提出了一种新颖的框架，利用伪4D高斯场进行视频生成，以支持复杂的镜头技术。我们通过密集的3D点跟踪构建伪4D高斯场，并为所有视频帧渲染该高斯场。为了提升GS-DiT的训练效果，我们还提出了一种高效的密集3D点跟踪方法，显著提高了准确性和推理速度。GS-DiT能够在不同的相机参数下生成具有相同动态内容的视频，扩展了视频生成的4D可控性，成为创意视频制作的强大工具。'}}}, {'id': 'https://huggingface.co/papers/2501.03059', 'title': 'Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2501.03059', 'abstract': "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.", 'score': 10, 'issue_id': 1532, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': '4f24667b663efb7d', 'authors': ['Guy Yariv', 'Yuval Kirstain', 'Amit Zohar', 'Shelly Sheynin', 'Yaniv Taigman', 'Yossi Adi', 'Sagie Benaim', 'Adam Polyak'], 'affiliations': ['FAIR, Meta', 'GenAI, Meta', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2501.03059.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Генерация реалистичных видео из статичных изображений с помощью масок траекторий движения', 'desc': 'Статья представляет новый подход к генерации видео из изображений (I2V) на основе текстового описания. Авторы предлагают двухэтапную композиционную модель, которая сначала генерирует промежуточное представление в виде маски траектории движения объектов. Затем это представление используется для генерации видео с применением объектно-ориентированных целевых функций внимания. Эксперименты показывают, что предложенный метод достигает лучших результатов по временной согласованности, реалистичности движения и соответствию текстовому описанию.'}, 'en': {'title': 'Transforming Images into Realistic Videos with Motion Precision', 'desc': 'This paper addresses the challenge of generating videos from static images using textual descriptions, known as Image-to-Video (I2V) generation. The authors propose a two-stage framework that first creates an intermediate representation to capture object semantics and motion, followed by a video generation stage that utilizes this representation. A key innovation is the use of a mask-based motion trajectory, which helps maintain accurate object motion and consistency across frames. The method is evaluated against challenging benchmarks and shows superior performance in terms of motion realism and coherence, while also introducing a new benchmark for I2V generation.'}, 'zh': {'title': '图像到视频生成的新突破', 'desc': '本文探讨了图像到视频（I2V）生成的任务，即根据文本描述将静态图像转换为逼真的视频序列。尽管近期的进展能够生成照片级真实感的输出，但在多物体场景中，视频的物体运动准确性和一致性仍然存在挑战。为了解决这些问题，我们提出了一种两阶段的组合框架，首先生成明确的中间表示，然后基于该表示生成视频。我们的创新在于引入了一种基于掩码的运动轨迹作为中间表示，能够捕捉语义物体信息和运动，从而实现运动和语义的紧凑而富有表现力的表示。'}}}, {'id': 'https://huggingface.co/papers/2501.03006', 'title': 'TransPixar: Advancing Text-to-Video Generation with Transparency', 'url': 'https://huggingface.co/papers/2501.03006', 'abstract': 'Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.', 'score': 8, 'issue_id': 1527, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'e85e5fa9a03d5d04', 'authors': ['Luozhou Wang', 'Yijun Li', 'Zhifei Chen', 'Jui-Hsien Wang', 'Zhifei Zhang', 'He Zhang', 'Zhe Lin', 'Yingcong Chen'], 'affiliations': ['Adobe Research', 'HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2501.03006.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'TransPixar: Прорыв в генерации RGBA-видео для визуальных эффектов', 'desc': 'TransPixar - это новый метод генерации RGBA-видео, расширяющий возможности предобученных видеомоделей. Он использует архитектуру диффузионного трансформера (DiT) и токены, специфичные для альфа-канала, для совместной генерации RGB и альфа-каналов с высокой согласованностью. Метод применяет тонкую настройку на основе LoRA и оптимизирует механизмы внимания для сохранения сильных сторон исходной RGB-модели. TransPixar эффективно генерирует разнообразные и согласованные RGBA-видео, открывая новые возможности для создания визуальных эффектов и интерактивного контента.'}, 'en': {'title': 'TransPixar: Bridging RGB and Alpha for Enhanced Video Generation', 'desc': 'This paper presents TransPixar, a novel method for generating RGBA videos, which include transparency information crucial for visual effects. The challenge lies in the limited datasets and the need to adapt existing models to handle alpha channels effectively. TransPixar utilizes a diffusion transformer architecture and incorporates alpha-specific tokens, allowing it to generate both RGB and alpha channels simultaneously. By optimizing attention mechanisms and employing LoRA-based fine-tuning, TransPixar achieves high consistency between RGB and alpha outputs, enhancing the quality of video generation for applications in VFX and interactive media.'}, 'zh': {'title': 'TransPixar：生成高质量RGBA视频的新方法', 'desc': '本文介绍了一种名为TransPixar的方法，旨在生成包含透明通道的RGBA视频。传统的视频生成模型在处理透明效果时面临挑战，TransPixar通过扩展预训练模型来解决这一问题。该方法利用扩散变换器架构，结合特定的透明通道标记，并通过LoRA微调实现RGB和透明通道的高一致性生成。最终，TransPixar在有限的数据集上优化了注意力机制，成功生成多样且一致的RGBA视频，推动了视觉特效和互动内容创作的可能性。'}}}, {'id': 'https://huggingface.co/papers/2501.01790', 'title': 'Ingredients: Blending Custom Photos with Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2501.01790', 'abstract': 'This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients.', 'score': 6, 'issue_id': 1528, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': 'dd1ccebdd2fcf276', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Changqian Yu', 'Mingyuan Fan'], 'affiliations': ['Kunlun Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01790.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#video', '#dataset', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Персонализированное видео из фотографий: новый уровень контроля в генеративных моделях', 'desc': 'Статья представляет новый метод под названием Ingredients для создания персонализированных видео с использованием нескольких фотографий конкретных людей. Метод состоит из трех основных модулей: экстрактора лицевых признаков, многомасштабного проектора и маршрутизатора идентификаторов. Ingredients использует тщательно подобранный набор данных текст-видео и многоэтапный протокол обучения для достижения превосходных результатов. Качественная оценка показывает преимущества предложенного метода по сравнению с существующими подходами в области генеративного контроля видео на основе архитектуры Transformer.'}, 'en': {'title': 'Transforming Photos into Personalized Videos with Ingredients', 'desc': 'This paper introduces a novel framework called Ingredients for creating personalized videos using multiple identity photos. It employs a facial extractor to accurately capture facial features, a multi-scale projector to integrate these features into video diffusion transformers, and an ID router to manage the allocation of identity embeddings across different time and space regions in the video. The framework is trained on a carefully selected text-video dataset, enhancing its ability to generate dynamic video content from custom images. The results show that Ingredients outperforms existing methods, marking a significant step forward in generative video control using Transformer architectures.'}, 'zh': {'title': '个性化视频创作的新突破', 'desc': '本文提出了一种强大的框架，通过结合多个特定身份照片，定制视频创作，称为Ingredients。该方法主要由三个模块组成：面部提取器、多个尺度投影器和身份路由器，分别用于提取面部特征、映射面部嵌入和动态分配身份嵌入。通过精心策划的文本-视频数据集和多阶段训练协议，Ingredients在将自定义照片转化为动态个性化视频内容方面表现出色。定性评估显示，该方法在基于Transformer的架构中，相较于现有方法，显著提升了生成视频控制工具的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.02576', 'title': 'DepthMaster: Taming Diffusion Models for Monocular Depth Estimation', 'url': 'https://huggingface.co/papers/2501.02576', 'abstract': "Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page.", 'score': 5, 'issue_id': 1536, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'a8429b95ef4eb7b7', 'authors': ['Ziyang Song', 'Zerong Wang', 'Bo Li', 'Hao Zhang', 'Ruijie Zhu', 'Li Liu', 'Peng-Tao Jiang', 'Tianzhu Zhang'], 'affiliations': ['School of Information Science and Technology, University of Science and Technology of China (USTC), Hefei 230026, P.R.China', 'vivo Mobile Communication Co., Ltd., Hangzhou 310030, P.R.China'], 'pdf_title_img': 'assets/pdf/title_img/2501.02576.jpg', 'data': {'categories': ['#optimization', '#training', '#diffusion', '#cv', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'DepthMaster: Однопроходная диффузионная модель для точной оценки глубины с улучшенной генерализацией', 'desc': 'DepthMaster - это однопроходная диффузионная модель для монокулярной оценки глубины. Она использует модуль выравнивания признаков для улучшения представления семантических особенностей и модуль улучшения Фурье для балансировки низкочастотной структуры и высокочастотных деталей. Модель обучается в два этапа: сначала фокусируется на глобальной структуре сцены, затем улучшает визуальное качество. DepthMaster превосходит другие диффузионные методы по обобщающей способности и сохранению деталей на различных наборах данных.'}, 'en': {'title': 'DepthMaster: Bridging Generative and Discriminative Depth Estimation', 'desc': 'This paper introduces DepthMaster, a single-step diffusion model aimed at improving monocular depth estimation. It addresses the inefficiencies of previous methods by integrating a Feature Alignment module to enhance the representation of semantic features and reduce overfitting to textures. Additionally, a Fourier Enhancement module is proposed to balance low-frequency structures with high-frequency details, ensuring finer depth estimation. The two-stage training strategy allows the model to first learn global scene structures and then refine visual quality, resulting in state-of-the-art performance across various datasets.'}, 'zh': {'title': 'DepthMaster：提升深度估计的单步扩散模型', 'desc': '本文提出了一种名为DepthMaster的单步扩散模型，用于单目深度估计。该模型通过特征对齐模块和傅里叶增强模块，优化生成特征以适应判别性深度估计任务。特征对齐模块增强了去噪网络的表示能力，而傅里叶增强模块则平衡了低频结构和高频细节。通过两阶段训练策略，DepthMaster在泛化能力和细节保留方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.01830', 'title': 'Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models', 'url': 'https://huggingface.co/papers/2501.01830', 'abstract': 'Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\\% higher success rates compared to existing methods.', 'score': 5, 'issue_id': 1529, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '5b08b81c52ec8da8', 'authors': ['Yanjiang Liu', 'Shuhen Zhou', 'Yaojie Lu', 'Huijia Zhu', 'Weiqiang Wang', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Ant Group', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01830.jpg', 'data': {'categories': ['#security', '#rl', '#rlhf'], 'emoji': '🛡️', 'ru': {'title': 'Auto-RT: Умная защита больших языковых моделей', 'desc': 'Авторы представляют Auto-RT - фреймворк на основе обучения с подкреплением для автоматизированного поиска уязвимостей в больших языковых моделях (LLM). Система использует механизмы раннего прекращения исследования и прогрессивного отслеживания наград для оптимизации стратегий атак. Auto-RT превосходит существующие методы, обнаруживая более широкий спектр уязвимостей с большей скоростью и на 16.63% более высоким уровнем успеха. Этот подход позволяет эффективно выявлять сложные уязвимости в LLM через вредоносные запросы.'}, 'en': {'title': 'Auto-RT: Revolutionizing Vulnerability Detection in LLMs', 'desc': 'This paper presents Auto-RT, a reinforcement learning framework designed to enhance automated red-teaming for large language models (LLMs). Unlike traditional methods that target isolated safety flaws, Auto-RT efficiently uncovers complex vulnerabilities by optimizing attack strategies through malicious queries. It introduces two innovative mechanisms: Early-terminated Exploration to prioritize promising attack strategies, and Progressive Reward Tracking to refine the search process dynamically. Experimental results show that Auto-RT significantly improves exploration efficiency and detection success rates, outperforming existing approaches.'}, 'zh': {'title': '自动化红队：高效发现语言模型漏洞的利器', 'desc': '自动化红队技术在发现大型语言模型（LLMs）中的漏洞方面变得至关重要。现有方法大多集中于孤立的安全缺陷，限制了其适应动态防御和高效发现复杂漏洞的能力。为了解决这个问题，我们提出了Auto-RT，一个强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞。我们的实验表明，Auto-RT显著提高了探索效率和攻击策略的自动优化，检测到更广泛的漏洞，检测速度更快，成功率提高了16.63%。'}}}, {'id': 'https://huggingface.co/papers/2501.02506', 'title': 'ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use', 'url': 'https://huggingface.co/papers/2501.02506', 'abstract': 'Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.', 'score': 5, 'issue_id': 1529, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'f785173226e5f9fc', 'authors': ['Junjie Ye', 'Zhengyin Du', 'Xuesong Yao', 'Weijian Lin', 'Yufei Xu', 'Zehui Chen', 'Zaiyuan Wang', 'Sining Zhu', 'Zhiheng Xi', 'Siyu Yuan', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang', 'Jiechao Chen'], 'affiliations': ['ByteDance', 'Institute of Modern Languages and Linguistics, Fudan University', 'School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2501.02506.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#optimization'], 'emoji': '🛠️', 'ru': {'title': 'ToolHop: новый стандарт для оценки многоэтапного использования инструментов в LLM', 'desc': 'Статья представляет новый набор данных ToolHop для оценки многоэтапного использования инструментов большими языковыми моделями (LLM). ToolHop содержит 995 пользовательских запросов и 3912 связанных инструментов, обеспечивая разнообразие запросов, взаимозависимости и возможность локального выполнения. Авторы оценили 14 LLM из пяти семейств моделей, выявив значительные трудности в обработке сценариев многоэтапного использования инструментов. Лучшая модель, GPT-4o, достигла точности 49.04%, что указывает на большой потенциал для улучшения.'}, 'en': {'title': 'ToolHop: Advancing Multi-Hop Tool Use Evaluation for LLMs', 'desc': 'This paper introduces ToolHop, a new dataset designed to evaluate how well large language models (LLMs) can use multiple tools in a single task. It includes 995 user queries and 3,912 tools, focusing on diverse and interdependent queries that can be executed locally. The authors tested 14 different LLMs, revealing that even the best-performing model, GPT-4o, only achieved 49.04% accuracy, indicating significant challenges in multi-hop tool use. The findings highlight different strategies employed by various model families, providing insights for future improvements in LLM capabilities.'}, 'zh': {'title': 'ToolHop：多跳工具使用的有效评估数据集', 'desc': '本文介绍了ToolHop数据集，该数据集包含995个用户查询和3912个相关工具，旨在有效评估大型语言模型（LLMs）在多跳工具使用中的理解、推理和功能调用能力。通过新颖的查询驱动数据构建方法，ToolHop确保了查询的多样性、工具的局部可执行性和可验证的答案。我们对14个不同模型（如LLaMA3.1、Qwen2.5等）进行了评估，发现它们在处理多跳工具使用场景时面临显著挑战。尽管GPT-4o模型的准确率为49.04%，但仍有很大的改进空间，分析还揭示了不同模型家族在工具使用策略上的差异，为未来的研究提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2501.02423', 'title': 'Scaling Laws for Floating Point Quantization Training', 'url': 'https://huggingface.co/papers/2501.02423', 'abstract': 'Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits.', 'score': 4, 'issue_id': 1537, 'pub_date': '2025-01-05', 'pub_date_card': {'ru': '5 января', 'en': 'January 5', 'zh': '1月5日'}, 'hash': 'be6872257cb9a129', 'authors': ['Xingwu Sun', 'Shuaipeng Li', 'Ruobing Xie', 'Weidong Han', 'Kan Wu', 'Zhen Yang', 'Yixing Li', 'An Wang', 'Shuai Li', 'Jinbao Xue', 'Yu Cheng', 'Yangyu Tao', 'Zhanhui Kang', 'Chengzhong Xu', 'Di Wang', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan', 'The Chinese University of Hong Kong', 'Tokyo Institute of Technology', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2501.02423.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '🧮', 'ru': {'title': 'Оптимизация точности вычислений в обучении языковых моделей', 'desc': 'Статья исследует влияние квантования с плавающей запятой на обучение больших языковых моделей (LLM). Авторы анализируют роль экспоненциальных и мантиссных битов, а также размера обучающих данных в производительности моделей. Они представляют унифицированный закон масштабирования для квантования с плавающей запятой и дают рекомендации по оптимальному соотношению битов и размеру данных. Исследование показывает, что оптимальная точность квантования находится в диапазоне 4-8 бит для широкого спектра вычислительных мощностей.'}, 'en': {'title': 'Optimizing Floating-Point Quantization for Better LLM Performance', 'desc': 'This paper investigates the impact of floating-point quantization on the training performance of large language models (LLMs). It highlights that previous research primarily focused on integer quantization, neglecting the nuances of floating-point quantization. The authors establish a unified scaling law for floating-point quantization and provide insights on the optimal ratio of exponent to mantissa bits, emphasizing that exponent bits have a greater influence on model performance. Additionally, they identify a critical data size threshold, beyond which performance may degrade, and suggest that the best precision for cost-performance lies between 4-8 bits, depending on computational power.'}, 'zh': {'title': '低精度训练：优化浮点量化的关键', 'desc': '低精度训练被认为是降低训练和推理成本的有效策略。以往的研究主要集中在整数量化上，而对浮点量化的研究相对较少，导致无法很好地适应大语言模型的损失情况。本文深入探讨了浮点量化训练中目标、指数位、尾数位和缩放因子的计算粒度对大语言模型性能的影响，并提出了统一的浮点量化缩放法则。研究结果表明，指数位对模型性能的贡献略高于尾数位，并发现了低精度训练中的关键数据大小。'}}}, {'id': 'https://huggingface.co/papers/2501.02832', 'title': 'Samba-asr state-of-the-art speech recognition leveraging structured state-space models', 'url': 'https://huggingface.co/papers/2501.02832', 'abstract': 'We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.', 'score': 4, 'issue_id': 1530, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'ed3c4a6192d0c5f9', 'authors': ['Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2501.02832.jpg', 'data': {'categories': ['#audio', '#architecture', '#benchmark', '#low_resource', '#open_source'], 'emoji': '🎙️', 'ru': {'title': 'Samba ASR: революция в распознавании речи с помощью моделей пространства состояний', 'desc': 'Представлена модель Samba ASR - первая современная система автоматического распознавания речи, использующая архитектуру Mamba в качестве энкодера и декодера на основе моделей пространства состояний (SSM). В отличие от трансформерных моделей, Samba ASR эффективно моделирует локальные и глобальные временные зависимости, достигая значительных улучшений производительности. Экспериментальные результаты показывают, что Samba ASR превосходит существующие модели с открытым исходным кодом на основе трансформеров по различным стандартным показателям. Модель демонстрирует значительное снижение показателя Word Error Rate (WER) и высокую эффективность даже при ограниченных ресурсах.'}, 'en': {'title': 'Samba ASR: Redefining Speech Recognition with State-Space Models', 'desc': 'Samba ASR is a groundbreaking Automatic Speech Recognition model that utilizes the innovative Mamba architecture, which functions as both the encoder and decoder. This model departs from traditional transformer-based approaches by employing state-space models (SSMs) to effectively capture both local and global temporal dependencies, leading to enhanced performance. By overcoming the challenges associated with transformers, such as their inefficiency with long input sequences, Samba ASR achieves superior accuracy and efficiency in recognizing speech. Extensive testing shows that Samba ASR not only outperforms existing transformer-based models but also excels in low-resource environments, making it a robust solution for various ASR applications.'}, 'zh': {'title': 'Samba ASR：超越变换器的语音识别新标杆', 'desc': '我们提出了Samba ASR，这是第一个利用新型Mamba架构作为编码器和解码器的最先进自动语音识别（ASR）模型。与基于变换器的ASR模型不同，Samba ASR通过高效的状态空间动态建模局部和全局时间依赖关系，从而实现显著的性能提升。该模型克服了变换器在处理长距离依赖和输入长度的平方扩展等方面的局限性，展现出更高的准确性和效率。实验结果表明，Samba ASR在多个标准基准测试中超越了现有的开源变换器ASR模型，确立了其在ASR领域的新标杆。'}}}, {'id': 'https://huggingface.co/papers/2501.00912', 'title': 'AutoPresent: Designing Structured Visuals from Scratch', 'url': 'https://huggingface.co/papers/2501.00912', 'abstract': "Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.", 'score': 3, 'issue_id': 1539, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': 'ea7b88fcc0a2025b', 'authors': ['Jiaxin Ge', 'Zora Zhiruo Wang', 'Xuhui Zhou', 'Yi-Hao Peng', 'Sanjay Subramanian', 'Qinyue Tan', 'Maarten Sap', 'Alane Suhr', 'Daniel Fried', 'Graham Neubig', 'Trevor Darrell'], 'affiliations': ['Carnegie Mellon University', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2501.00912.jpg', 'data': {'categories': ['#dataset', '#story_generation', '#training', '#benchmark', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'Автоматизация создания презентаций: от текста к структурированным визуальным материалам', 'desc': 'Эта статья представляет новый бенчмарк SlidesBench для автоматической генерации слайдов презентаций на основе текстовых инструкций. Авторы сравнивают методы генерации изображений и программного кода, обнаружив преимущество последнего подхода. Они создают модель AutoPresent на базе Llama для генерации кода слайдов, достигающую результатов, сопоставимых с GPT-4. Исследователи также изучают итеративное улучшение дизайна слайдов с помощью самооптимизации модели.'}, 'en': {'title': 'Automating Slide Generation with Advanced Models', 'desc': 'This paper addresses the challenge of creating automated slide presentations from natural language instructions. It introduces the SlidesBench benchmark, which includes a large dataset for training and testing slide generation models. The authors evaluate various methods, finding that programmatic approaches yield higher-quality slides. They also present AutoPresent, a model that competes with advanced models like GPT-4o, and demonstrate that iterative design refinement enhances the quality of generated slides.'}, 'zh': {'title': '自动生成高质量演示幻灯片的未来', 'desc': '本研究旨在自动生成演示幻灯片，解决内容创作和视觉规划的挑战。我们首次引入SlidesBench基准，包含7000个训练样本和585个测试样本，涵盖10个领域的310个幻灯片集。通过对比不同模型的图像生成和程序生成方法，我们发现程序生成方法在用户交互格式中生成的幻灯片质量更高。基于程序生成的成功，我们开发了AutoPresent模型，并通过自我优化过程进一步提升幻灯片的质量。'}}}, {'id': 'https://huggingface.co/papers/2501.03225', 'title': 'Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation', 'url': 'https://huggingface.co/papers/2501.03225', 'abstract': 'The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.', 'score': 1, 'issue_id': 1542, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'aa212f5e596ed0e6', 'authors': ['Yuhui Zhang', 'Yuchang Su', 'Yiming Liu', 'Xiaohan Wang', 'James Burgess', 'Elaine Sui', 'Chenyu Wang', 'Josiah Aklilu', 'Alejandro Lozano', 'Anjiang Wei', 'Ludwig Schmidt', 'Serena Yeung-Levy'], 'affiliations': ['MIT', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2501.03225.jpg', 'data': {'categories': ['#interpretability', '#agents', '#benchmark', '#cv', '#survey', '#games', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Автоматизация оценки моделей машинного зрения и языка', 'desc': 'Исследователи представили AutoConverter - агентную систему для автоматического преобразования открытых вопросов в вопросы с множественным выбором для оценки моделей машинного зрения и языка (VLM). Эта система позволяет объективно оценивать VLM, избегая сложностей, связанных с вариативностью естественно-языковых ответов. На основе AutoConverter был создан бенчмарк VMCBench, включающий 9018 вопросов из 20 существующих наборов данных для визуальных вопросов и ответов (VQA). VMCBench был использован для всесторонней оценки 33 современных VLM, устанавливая новый стандарт масштабируемой и воспроизводимой оценки таких моделей.'}, 'en': {'title': 'Transforming VQA for Objective Evaluation with AutoConverter', 'desc': 'This paper presents AutoConverter, a framework designed to improve the evaluation of vision language models (VLMs) by converting open-ended visual question answering (VQA) questions into a multiple-choice format. This transformation allows for more objective assessments of VLM performance, addressing the challenges posed by the variability of natural language responses. The authors demonstrate that VLMs perform similarly or worse on these newly generated questions compared to those created by humans, indicating the rigor of the new benchmark. Additionally, they introduce VMCBench, a comprehensive dataset that standardizes 20 existing VQA datasets into a unified multiple-choice format, facilitating scalable and reproducible evaluations of VLMs.'}, 'zh': {'title': '自动化评估视觉语言模型的新标准', 'desc': '随着视觉语言模型（VLMs）的快速发展，评估这些模型的准确性变得尤为重要。现有的视觉问答（VQA）基准往往依赖开放式问题，这使得评估变得困难，因为自然语言回答的多样性很大。为了解决这个问题，我们提出了AutoConverter，这是一种自动将开放式问题转换为多项选择格式的框架，从而实现客观评估并减少问题创建的成本。通过使用AutoConverter，我们构建了VMCBench，这是一个将20个现有VQA数据集转化为统一多项选择格式的基准，包含9,018个问题，全面评估了33个最先进的VLMs，设定了可扩展、一致和可重复的VLM评估新标准。'}}}, {'id': 'https://huggingface.co/papers/2412.18525', 'title': 'Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization', 'url': 'https://huggingface.co/papers/2412.18525', 'abstract': "Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we explore the idea that CV adopts discrete and terminological task definitions (\\eg, ``image segmentation''), which may be a key barrier to zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks--due to these terminological definitions--deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ``image input to explanatory instruction to output'' triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository.", 'score': 48, 'issue_id': 1406, 'pub_date': '2024-12-24', 'pub_date_card': {'ru': '24 декабря', 'en': 'December 24', 'zh': '12月24日'}, 'hash': '23f11aceae00534d', 'authors': ['Yang Shen', 'Xiu-Shen Wei', 'Yifan Sun', 'Yuxin Song', 'Tao Yuan', 'Jian Jin', 'Heyang Xu', 'Yazhou Yao', 'Errui Ding'], 'affiliations': ['Baidu', 'Nanjing University of Science and Technology', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2412.18525.jpg', 'data': {'categories': ['#dataset', '#open_source', '#cv', '#multimodal', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Лингвистические инструкции - ключ к обобщению в компьютерном зрении', 'desc': "В статье исследуется проблема недостаточной способности моделей компьютерного зрения к обобщению на новые задачи без предварительного обучения. Авторы предлагают использовать подробные лингвистические инструкции для определения задач вместо дискретных терминологических определений. Они создали большой датасет из 12 миллионов примеров 'изображение-инструкция-результат' и обучили авторегрессионную мультимодальную модель следовать этим инструкциям. Эксперименты показали, что такой подход позволяет модели лучше обобщаться на новые задачи компьютерного зрения без дополнительного обучения."}, 'en': {'title': 'Unlocking Zero-Shot Generalization in Computer Vision with Explanatory Instructions', 'desc': "This paper addresses the challenge of zero-shot task generalization in Computer Vision (CV), which has not reached the levels seen in Natural Language Processing (NLP). The authors argue that the use of specific terminological definitions for tasks in CV, like 'image segmentation', limits the models' ability to generalize to new tasks. To overcome this, they propose 'Explanatory Instructions' that transform image inputs into detailed linguistic outputs, helping models understand tasks better. They introduce a large dataset of 12 million triplets and train an auto-regressive vision-language model that successfully demonstrates zero-shot capabilities for both seen and unseen tasks."}, 'zh': {'title': '突破计算机视觉的零样本任务泛化', 'desc': '本文探讨了计算机视觉（CV）在零样本任务泛化方面的挑战，尤其是与自然语言处理（NLP）的对比。我们认为，CV使用的术语性任务定义（如“图像分割”）可能是阻碍零样本任务泛化的关键因素。为了解决这个问题，我们引入了“解释性指令”，通过详细的语言转换来直观地定义CV任务目标。我们创建了一个包含1200万对“图像输入、解释性指令和输出”的大规模数据集，并训练了一个基于自回归的视觉语言模型，实现了对已见任务的指令级零样本能力，并在未见的CV任务上展示了强大的零样本泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2412.20070', 'title': 'On the Compositional Generalization of Multimodal LLMs for Medical Imaging', 'url': 'https://huggingface.co/papers/2412.20070', 'abstract': 'Multimodal large language models (MLLMs) hold significant potential in the medical field, but their capabilities are often limited by insufficient data in certain medical domains, highlighting the need for understanding what kinds of images can be used by MLLMs for generalization. Current research suggests that multi-task training outperforms single-task as different tasks can benefit each other, but they often overlook the internal relationships within these tasks, providing limited guidance on selecting datasets to enhance specific tasks. To analyze this phenomenon, we attempted to employ compositional generalization (CG)-the ability of models to understand novel combinations by recombining learned elements-as a guiding framework. Since medical images can be precisely defined by Modality, Anatomical area, and Task, naturally providing an environment for exploring CG. Therefore, we assembled 106 medical datasets to create Med-MAT for comprehensive experiments. The experiments confirmed that MLLMs can use CG to understand unseen medical images and identified CG as one of the main drivers of the generalization observed in multi-task training. Additionally, further studies demonstrated that CG effectively supports datasets with limited data and delivers consistent performance across different backbones, highlighting its versatility and broad applicability. Med-MAT is publicly available at https://github.com/FreedomIntelligence/Med-MAT.', 'score': 36, 'issue_id': 1405, 'pub_date': '2024-12-28', 'pub_date_card': {'ru': '28 декабря', 'en': 'December 28', 'zh': '12月28日'}, 'hash': '34f9c6ec4611d6ec', 'authors': ['Zhenyang Cai', 'Junying Chen', 'Rongsheng Wang', 'Weihong Wang', 'Yonglin Deng', 'Dingjie Song', 'Yize Chen', 'Zixu Zhang', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2412.20070.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#open_source', '#multimodal', '#transfer_learning'], 'emoji': '🩺', 'ru': {'title': 'Композиционная генерализация - ключ к пониманию медицинских изображений для MLLM', 'desc': 'Статья исследует возможности мультимодальных больших языковых моделей (MLLM) в медицинской сфере, фокусируясь на композиционной генерализации (CG). Авторы создали набор данных Med-MAT из 106 медицинских датасетов для изучения способности моделей понимать новые комбинации изображений. Эксперименты показали, что MLLM могут использовать CG для интерпретации ранее невиданных медицинских изображений. Исследование также выявило эффективность CG для датасетов с ограниченными данными и стабильность результатов на разных архитектурах моделей.'}, 'en': {'title': 'Unlocking Medical Insights with Compositional Generalization', 'desc': "This paper explores the use of multimodal large language models (MLLMs) in the medical field, focusing on how they can generalize from limited data. It highlights the advantages of multi-task training over single-task training, emphasizing the importance of understanding the relationships between different tasks. The authors introduce compositional generalization (CG) as a framework to enhance the model's ability to interpret new combinations of medical images. They created a dataset called Med-MAT, which consists of 106 medical datasets, and found that CG significantly improves the performance of MLLMs, especially in scenarios with scarce data."}, 'zh': {'title': '组合泛化助力医学图像理解', 'desc': '多模态大型语言模型（MLLMs）在医学领域具有重要潜力，但在某些医学领域的数据不足限制了其能力。当前研究表明，多任务训练优于单任务训练，因为不同任务可以相互促进，但往往忽视了这些任务之间的内部关系。我们采用组合泛化（CG）作为指导框架，分析模型如何理解新组合的能力，并组建了106个医学数据集以创建Med-MAT进行全面实验。实验结果确认，MLLMs能够利用CG理解未见过的医学图像，并且CG是多任务训练中观察到的泛化的主要驱动因素之一。'}}}, {'id': 'https://huggingface.co/papers/2412.20422', 'title': 'Bringing Objects to Life: 4D generation from 3D objects', 'url': 'https://huggingface.co/papers/2412.20422', 'abstract': 'Recent advancements in generative modeling now enable the creation of 4D content (moving 3D objects) controlled with text prompts. 4D generation has large potential in applications like virtual worlds, media, and gaming, but existing methods provide limited control over the appearance and geometry of generated content. In this work, we introduce a method for animating user-provided 3D objects by conditioning on textual prompts to guide 4D generation, enabling custom animations while maintaining the identity of the original object. We first convert a 3D mesh into a ``static" 4D Neural Radiance Field (NeRF) that preserves the visual attributes of the input object. Then, we animate the object using an Image-to-Video diffusion model driven by text. To improve motion realism, we introduce an incremental viewpoint selection protocol for sampling perspectives to promote lifelike movement and a masked Score Distillation Sampling (SDS) loss, which leverages attention maps to focus optimization on relevant regions. We evaluate our model in terms of temporal coherence, prompt adherence, and visual fidelity and find that our method outperforms baselines that are based on other approaches, achieving up to threefold improvements in identity preservation measured using LPIPS scores, and effectively balancing visual quality with dynamic content.', 'score': 29, 'issue_id': 1408, 'pub_date': '2024-12-29', 'pub_date_card': {'ru': '29 декабря', 'en': 'December 29', 'zh': '12月29日'}, 'hash': 'de742e56a5ec379f', 'authors': ['Ohad Rahamim', 'Ori Malca', 'Dvir Samuel', 'Gal Chechik'], 'affiliations': ['Bar-Ilan University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2412.20422.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#games', '#diffusion', '#video', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Оживление 3D-объектов с помощью текста: новый рубеж в генеративном моделировании', 'desc': 'Статья представляет новый метод анимации 3D-объектов с помощью текстовых подсказок. Авторы используют генеративную модель для создания 4D-контента (движущихся 3D-объектов), сохраняя при этом исходный вид объекта. Метод включает преобразование 3D-меша в статическое 4D нейронное радиальное поле (NeRF) и последующую анимацию с помощью диффузионной модели Image-to-Video. Для улучшения реалистичности движения введены протокол выбора ракурсов и маскированная функция потерь Score Distillation Sampling.'}, 'en': {'title': 'Animating 3D Objects with Text Prompts for Realistic 4D Generation', 'desc': "This paper presents a novel approach to generating 4D content by animating 3D objects based on text prompts. The method involves converting a 3D mesh into a static 4D Neural Radiance Field (NeRF) to retain the object's visual characteristics. It then utilizes an Image-to-Video diffusion model to create animations while ensuring the original object's identity is preserved. The authors enhance motion realism through a viewpoint selection protocol and a masked Score Distillation Sampling loss, leading to significant improvements in visual quality and dynamic content generation."}, 'zh': {'title': '文本驱动的4D动画生成新方法', 'desc': '本研究提出了一种新方法，可以通过文本提示来控制4D内容的生成，特别是动画用户提供的3D对象。我们首先将3D网格转换为静态的4D神经辐射场（NeRF），以保留输入对象的视觉特征。然后，利用图像到视频的扩散模型进行动画制作，确保生成的动画与文本提示相符。通过引入增量视角选择协议和掩码评分蒸馏损失，我们提高了运动的真实感，并在多个评估指标上超越了现有方法。'}}}, {'id': 'https://huggingface.co/papers/2412.20993', 'title': 'Efficiently Serving LLM Reasoning Programs with Certaindex', 'url': 'https://huggingface.co/papers/2412.20993', 'abstract': 'The rapid evolution of large language models (LLMs) has unlocked their capabilities in advanced reasoning tasks like mathematical problem-solving, code generation, and legal analysis. Central to this progress are inference-time reasoning algorithms, which refine outputs by exploring multiple solution paths, at the cost of increasing compute demands and response latencies. Existing serving systems fail to adapt to the scaling behaviors of these algorithms or the varying difficulty of queries, leading to inefficient resource use and unmet latency targets.   We present Dynasor, a system that optimizes inference-time compute for LLM reasoning queries. Unlike traditional engines, Dynasor tracks and schedules requests within reasoning queries and uses Certaindex, a proxy that measures statistical reasoning progress based on model certainty, to guide compute allocation dynamically. Dynasor co-adapts scheduling with reasoning progress: it allocates more compute to hard queries, reduces compute for simpler ones, and terminates unpromising queries early, balancing accuracy, latency, and cost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50% in batch processing and sustaining 3.3x higher query rates or 4.7x tighter latency SLOs in online serving.', 'score': 24, 'issue_id': 1406, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '7fe76ed90463d977', 'authors': ['Yichao Fu', 'Junda Chen', 'Siqi Zhu', 'Zheyu Fu', 'Zhongdongming Dai', 'Aurick Qiao', 'Hao Zhang'], 'affiliations': ['Snowflake', 'Tsinghua University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2412.20993.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Dynasor: умное распределение ресурсов для эффективных LLM-рассуждений', 'desc': 'Статья представляет систему Dynasor, оптимизирующую вычисления для задач рассуждения с использованием больших языковых моделей (LLM). Dynasor отслеживает и планирует запросы, используя прокси Certaindex для измерения прогресса рассуждений на основе уверенности модели. Система динамически распределяет вычислительные ресурсы, уделяя больше внимания сложным запросам и меньше простым, а также прекращая бесперспективные запросы. Dynasor показывает значительное снижение вычислительных затрат и улучшение производительности на различных наборах данных и алгоритмах.'}, 'en': {'title': 'Dynasor: Smart Compute Allocation for Efficient LLM Reasoning', 'desc': "This paper introduces Dynasor, a system designed to optimize the compute resources used during inference for large language models (LLMs) when handling reasoning queries. It addresses the inefficiencies of existing serving systems that do not adapt to the complexity of different queries or the scaling needs of inference-time reasoning algorithms. Dynasor employs a dynamic scheduling approach that allocates compute resources based on the difficulty of the query, using a proxy called Certaindex to measure the model's certainty in its reasoning. As a result, Dynasor can significantly reduce compute usage while improving query processing rates and meeting latency targets more effectively."}, 'zh': {'title': 'Dynasor：优化推理查询的计算效率', 'desc': '这篇论文介绍了Dynasor系统，它优化了大型语言模型（LLM）在推理查询时的计算效率。Dynasor通过跟踪和调度推理查询中的请求，动态分配计算资源，以应对不同难度的查询。该系统使用Certaindex代理，根据模型的确定性来衡量推理进展，从而指导计算分配。通过在多种数据集和算法上测试，Dynasor在批处理时减少了多达50%的计算需求，同时在在线服务中实现了3.3倍更高的查询速率或4.7倍更严格的延迟服务水平目标。'}}}, {'id': 'https://huggingface.co/papers/2412.21037', 'title': 'TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization', 'url': 'https://huggingface.co/papers/2412.21037', 'abstract': 'We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes preference data to enhance TTA alignment. We demonstrate that the audio preference dataset generated using CRPO outperforms existing alternatives. With this framework, TangoFlux achieves state-of-the-art performance across both objective and subjective benchmarks. We open source all code and models to support further research in TTA generation.', 'score': 19, 'issue_id': 1405, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': 'bb669623871df661', 'authors': ['Chia-Yu Hung', 'Navonil Majumder', 'Zhifeng Kong', 'Ambuj Mehrish', 'Rafael Valle', 'Bryan Catanzaro', 'Soujanya Poria'], 'affiliations': ['NVIDIA', 'Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2412.21037.jpg', 'data': {'categories': ['#dataset', '#audio', '#open_source', '#benchmark', '#alignment', '#rlhf', '#small_models'], 'emoji': '🎵', 'ru': {'title': 'TangoFlux: Революция в генерации аудио из текста', 'desc': 'TangoFlux - это эффективная генеративная модель для преобразования текста в аудио (Text-to-Audio, TTA) с 515 миллионами параметров. Модель способна генерировать до 30 секунд аудио с частотой 44,1 кГц всего за 3,7 секунды на одном GPU A40. Авторы представляют новую методику CLAP-Ranked Preference Optimization (CRPO) для улучшения согласованности TTA моделей путем итеративной генерации и оптимизации данных о предпочтениях. TangoFlux достигает передовых результатов в объективных и субъективных тестах, а код и модели открыты для дальнейших исследований.'}, 'en': {'title': 'TangoFlux: Revolutionizing Text-to-Audio Generation with CRPO', 'desc': "TangoFlux is a powerful Text-to-Audio generative model that can create high-quality audio quickly and efficiently. It addresses the challenge of aligning TTA models by introducing a new method called CLAP-Ranked Preference Optimization (CRPO), which helps generate and optimize preference data. This approach improves the model's ability to understand and produce audio that aligns with user preferences. The results show that TangoFlux not only meets but exceeds current standards in both objective and subjective evaluations, and the team has made their code and models available for further research."}, 'zh': {'title': 'TangoFlux：高效的文本到音频生成模型', 'desc': '我们介绍了TangoFlux，这是一种高效的文本到音频生成模型，拥有5.15亿个参数，能够在单个A40 GPU上以3.7秒的速度生成最长30秒的44.1kHz音频。TTA模型对齐的一个主要挑战是创建偏好对的困难，因为TTA缺乏像大型语言模型（LLMs）那样的可验证奖励或标准答案的结构化机制。为了解决这个问题，我们提出了CLAP-Ranked Preference Optimization（CRPO），这是一个新颖的框架，通过迭代生成和优化偏好数据来增强TTA的对齐。我们证明了使用CRPO生成的音频偏好数据集在现有替代方案中表现更优，TangoFlux在客观和主观基准测试中都达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.21079', 'title': 'Edicho: Consistent Image Editing in the Wild', 'url': 'https://huggingface.co/papers/2412.21079', 'abstract': 'As a verified need, consistent editing across in-the-wild images remains a technical challenge arising from various unmanageable factors, like object poses, lighting conditions, and photography environments. Edicho steps in with a training-free solution based on diffusion models, featuring a fundamental design principle of using explicit image correspondence to direct editing. Specifically, the key components include an attention manipulation module and a carefully refined classifier-free guidance (CFG) denoising strategy, both of which take into account the pre-estimated correspondence. Such an inference-time algorithm enjoys a plug-and-play nature and is compatible to most diffusion-based editing methods, such as ControlNet and BrushNet. Extensive results demonstrate the efficacy of Edicho in consistent cross-image editing under diverse settings. We will release the code to facilitate future studies.', 'score': 17, 'issue_id': 1405, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '8068418a331b2086', 'authors': ['Qingyan Bai', 'Hao Ouyang', 'Yinghao Xu', 'Qiuyu Wang', 'Ceyuan Yang', 'Ka Leong Cheng', 'Yujun Shen', 'Qifeng Chen'], 'affiliations': ['Ant Group', 'CUHK', 'HKUST', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2412.21079.jpg', 'data': {'categories': ['#cv', '#diffusion', '#open_source', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'Edicho: согласованное редактирование изображений без обучения', 'desc': 'Статья представляет Edicho - решение для согласованного редактирования изображений без обучения, основанное на диффузионных моделях. Ключевые компоненты включают модуль манипуляции вниманием и стратегию шумоподавления без классификатора, использующие предварительно оцененное соответствие между изображениями. Этот алгоритм совместим с большинством методов редактирования на основе диффузии, таких как ControlNet и BrushNet. Результаты демонстрируют эффективность Edicho в согласованном редактировании изображений в различных условиях.'}, 'en': {'title': 'Edicho: Consistent Image Editing Made Easy with Diffusion Models', 'desc': 'This paper introduces Edicho, a novel approach for consistent editing of images that addresses challenges like varying object poses and lighting. It utilizes diffusion models without the need for prior training, focusing on explicit image correspondence to guide the editing process. Key innovations include an attention manipulation module and a refined classifier-free guidance denoising strategy, which enhance the editing quality by considering pre-estimated correspondences. The method is designed to be easily integrated with existing diffusion-based editing techniques, showing strong performance across different scenarios.'}, 'zh': {'title': 'Edicho：无训练一致性图像编辑的新方法', 'desc': 'Edicho 是一种基于扩散模型的无训练解决方案，旨在解决在不同环境下进行一致性图像编辑的挑战。它的设计原则是利用显式图像对应关系来指导编辑，确保在不同的拍摄条件下保持一致性。该方法包括一个注意力操作模块和经过精细调整的无分类器引导去噪策略，能够有效处理预估的对应关系。Edicho 具有即插即用的特性，兼容大多数基于扩散的编辑方法，实验结果显示其在多种设置下的有效性。'}}}, {'id': 'https://huggingface.co/papers/2412.21187', 'title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'url': 'https://huggingface.co/papers/2412.21187', 'abstract': 'The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.', 'score': 11, 'issue_id': 1415, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '78da22eae14fe26c', 'authors': ['Xingyu Chen', 'Jiahao Xu', 'Tian Liang', 'Zhiwei He', 'Jianhui Pang', 'Dian Yu', 'Linfeng Song', 'Qiuzhi Liu', 'Mengfei Zhou', 'Zhuosheng Zhang', 'Rui Wang', 'Zhaopeng Tu', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2412.21187.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#math', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное мышление ИИ: борьба с избыточными вычислениями', 'desc': 'Статья исследует проблему избыточных вычислений (overthinking) в крупных языковых моделях типа OpenAI o1 при решении задач. Авторы вводят новые метрики эффективности для оценки рационального использования вычислительных ресурсов такими моделями. Предлагается стратегия на основе самообучения для оптимизации рассуждений модели без потери точности. Экспериментальные результаты показывают успешное снижение вычислительных затрат при сохранении производительности на различных наборах тестов.'}, 'en': {'title': 'Streamlining Reasoning: Tackling Overthinking in AI Models', 'desc': "This paper investigates the phenomenon of overthinking in advanced machine learning models, particularly those like OpenAI's o1, which excel at long-term reasoning. It highlights the inefficiencies that arise when these models allocate excessive computational resources to solve simple problems, leading to minimal gains in performance. The authors propose new efficiency metrics to assess how well these models utilize their computational power during inference. By implementing a self-training approach, they present strategies to reduce overthinking, achieving a balance between computational efficiency and model accuracy across various challenging test sets."}, 'zh': {'title': '优化计算资源，提升模型效率', 'desc': '本文探讨了像OpenAI o1这样的模型在推理过程中模拟人类长期思考的能力。研究指出，这些模型在解决问题时常常会过度思考，导致在简单问题上分配过多的计算资源。我们提出了新的效率指标，从结果和过程两个角度评估计算资源的合理使用，并提出了自我训练的策略来减少过度思考。实验结果表明，我们的方法在不同难度的测试集上成功降低了计算开销，同时保持了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2412.20005', 'title': 'OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System', 'url': 'https://huggingface.co/papers/2412.20005', 'abstract': "We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code at https://github.com/zjunlp/OneKE and released a Video at http://oneke.openkg.cn/demo.mp4.", 'score': 10, 'issue_id': 1405, 'pub_date': '2024-12-28', 'pub_date_card': {'ru': '28 декабря', 'en': 'December 28', 'zh': '12月28日'}, 'hash': 'da8469c61421cefb', 'authors': ['Yujie Luo', 'Xiangyuan Ru', 'Kangwei Liu', 'Lin Yuan', 'Mengshu Sun', 'Ningyu Zhang', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Lanning Wei', 'Da Zheng', 'Haofen Wang', 'Huajun Chen'], 'affiliations': ['Ant Group', 'Tongji University', 'ZJU-Ant Group Joint Research Center for Knowledge Graphs', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.20005.jpg', 'data': {'categories': ['#dataset', '#agents', '#open_source', '#benchmark', '#multimodal', '#science'], 'emoji': '🧠', 'ru': {'title': 'OneKE: Универсальный инструмент для извлечения знаний из разнородных источников', 'desc': 'OneKE - это докеризованная система извлечения знаний, управляемая схемой. Она способна извлекать информацию из веб-ресурсов и PDF-книг, поддерживая различные домены, такие как наука и новости. Система использует множество агентов и настраиваемую базу знаний для выполнения различных сценариев извлечения. OneKE демонстрирует высокую эффективность на эталонных наборах данных и адаптируемость к разнообразным задачам в различных областях.'}, 'en': {'title': 'OneKE: Versatile Knowledge Extraction for Diverse Domains', 'desc': "OneKE is a knowledge extraction system designed to gather information from the Web and raw PDF books across various domains like science and news. It utilizes multiple agents, each responsible for specific tasks, which enhances its ability to handle different extraction scenarios effectively. The system includes a configurable knowledge base that aids in schema setup, debugging, and error correction, leading to improved performance. Empirical tests on benchmark datasets confirm OneKE's effectiveness, and case studies showcase its versatility in tackling diverse tasks."}, 'zh': {'title': 'OneKE：多领域知识提取的智能系统', 'desc': 'OneKE是一个基于Docker的知识提取系统，能够从网络和原始PDF书籍中提取知识，支持多个领域（如科学、新闻等）。该系统设计了多个智能代理，各自承担不同的角色，以适应各种提取场景。配置知识库的设计使得模式配置、错误调试和修正变得更加高效，从而提升了系统的性能。通过在基准数据集上的实证评估，OneKE展示了其有效性，并通过案例研究进一步说明了其在多个领域的适应性和广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.20631', 'title': "Slow Perception: Let's Perceive Geometric Figures Step-by-step", 'url': 'https://huggingface.co/papers/2412.20631', 'abstract': 'Recently, "visual o1" began to enter people\'s vision, with expectations that this slow-thinking design can solve visual reasoning tasks, especially geometric math problems. However, the reality is that current LVLMs (Large Vision Language Models) can hardly even accurately copy a geometric figure, let alone truly understand the complex inherent logic and spatial relationships within geometric shapes. We believe accurate copying (strong perception) is the first step to visual o1. Accordingly, we introduce the concept of "slow perception" (SP), which guides the model to gradually perceive basic point-line combinations, as our humans, reconstruct complex geometric structures progressively. There are two-fold stages in SP: a) perception decomposition. Perception is not instantaneous. In this stage, complex geometric figures are broken down into basic simple units to unify geometry representation. b) perception flow, which acknowledges that accurately tracing a line is not an easy task. This stage aims to avoid "long visual jumps" in regressing line segments by using a proposed "perceptual ruler" to trace each line stroke-by-stroke. Surprisingly, such a human-like perception manner enjoys an inference time scaling law -- the slower, the better. Researchers strive to speed up the model\'s perception in the past, but we slow it down again, allowing the model to read the image step-by-step and carefully.', 'score': 9, 'issue_id': 1415, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': 'f99c59b7ef92c667', 'authors': ['Haoran Wei', 'Youyang Yin', 'Yumeng Li', 'Jia Wang', 'Liang Zhao', 'Jianjian Sun', 'Zheng Ge', 'Xiangyu Zhang'], 'affiliations': ['Beihang University', 'Stepfun'], 'pdf_title_img': 'assets/pdf/title_img/2412.20631.jpg', 'data': {'categories': ['#cv', '#math', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Медленнее значит лучше: новый подход к компьютерному зрению', 'desc': "Статья представляет концепцию 'медленного восприятия' (slow perception) для улучшения способности моделей компьютерного зрения копировать геометрические фигуры. Авторы предлагают двухэтапный подход: декомпозиция восприятия, разбивающая сложные фигуры на простые элементы, и поток восприятия, использующий 'перцептивную линейку' для точного отслеживания линий. Исследователи обнаружили, что более медленное восприятие приводит к лучшим результатам, что противоречит традиционному стремлению ускорить обработку изображений. Эта методика может стать первым шагом к решению задач визуального рассуждения и геометрических задач большими визуально-языковыми моделями."}, 'en': {'title': 'Slow Down to See Better: Enhancing Visual Reasoning with Slow Perception', 'desc': "This paper introduces the concept of 'slow perception' (SP) to enhance the capabilities of Large Vision Language Models (LVLMs) in visual reasoning tasks, particularly in understanding geometric shapes. SP consists of two stages: perception decomposition, where complex figures are simplified into basic components, and perception flow, which emphasizes careful tracing of lines to avoid errors. The authors argue that this method mimics human cognitive processes, allowing for a more accurate understanding of spatial relationships. Interestingly, they find that a slower, more deliberate approach to perception improves the model's performance, challenging the traditional focus on speed in machine learning."}, 'zh': {'title': '慢感知：逐步理解几何结构的关键', 'desc': '最近，"视觉o1"开始引起人们的关注，期望这种慢思维设计能够解决视觉推理任务，尤其是几何数学问题。然而，当前的大型视觉语言模型（LVLMs）在准确复制几何图形方面几乎无能为力，更不用说真正理解几何形状内在的复杂逻辑和空间关系。我们提出了"慢感知"（SP）的概念，指导模型逐步感知基本的点线组合，像人类一样逐步重建复杂的几何结构。SP包括两个阶段：感知分解和感知流，前者将复杂的几何图形分解为基本单元，后者通过使用"感知尺"逐步追踪每条线段，避免"长视觉跳跃"。'}}}, {'id': 'https://huggingface.co/papers/2412.21140', 'title': 'Facilitating large language model Russian adaptation with Learned Embedding Propagation', 'url': 'https://huggingface.co/papers/2412.21140', 'abstract': 'Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a language specific LLMs as improved inference computation efficiency becomes the only guaranteed advantage of such costly procedure. More cost-efficient options such as vocabulary extension and subsequent continued pre-training are also inhibited by the lack of access to high-quality instruction-tuning data since it is the major factor behind the resulting LLM task-solving capabilities. To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP). Unlike existing approaches our method has lower training data size requirements due to minimal impact on existing LLM knowledge which we reinforce using novel ad-hoc embedding propagation procedure that allows to skip the instruction-tuning step and instead implant the new language knowledge directly into any existing instruct-tuned variant. We evaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B, showing that LEP is competitive with traditional instruction-tuning methods, achieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with further improvements via self-calibration and continued tuning enhancing task-solving capabilities.', 'score': 9, 'issue_id': 1412, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '093f3929e323d180', 'authors': ['Mikhail Tikhomirov', 'Daniil Chernyshev'], 'affiliations': ['Lomonosov Moscow State University, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2412.21140.jpg', 'data': {'categories': ['#data', '#training', '#low_resource', '#transfer_learning', '#dataset', '#open_source', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Эффективная адаптация языковых моделей без масштабного переобучения', 'desc': 'Статья представляет новый метод адаптации больших языковых моделей (LLM) к другим языкам, называемый Learned Embedding Propagation (LEP). Этот подход позволяет эффективно внедрять знания нового языка в существующие инструктированные LLM без необходимости повторного обучения на больших объемах данных. Авторы провели эксперименты с адаптацией моделей LLaMa-3-8B и Mistral-7B к русскому языку, показав, что LEP конкурентоспособен с традиционными методами инструктирования. Результаты демонстрируют, что LEP достигает производительности, сравнимой с OpenChat 3.5 и LLaMa-3-8B-Instruct, с возможностью дальнейшего улучшения через самокалибровку и дополнительную настройку.'}, 'en': {'title': 'Efficient Language Adaptation with Learned Embedding Propagation', 'desc': 'This paper introduces Learned Embedding Propagation (LEP), a novel method for adapting large language models (LLMs) to new languages without the need for extensive instruction-tuning data. LEP minimizes the training data requirements by directly embedding new language knowledge into existing instruct-tuned models, thus bypassing traditional instruction-tuning steps. The authors demonstrate that LEP can effectively adapt LLaMa-3-8B and Mistral-7B for Russian vocabulary, achieving performance on par with state-of-the-art models like OpenChat 3.5. This approach not only reduces costs but also enhances the efficiency of language adaptation in multilingual contexts.'}, 'zh': {'title': '学习嵌入传播：降低语言适应成本的新方法', 'desc': '这篇论文介绍了一种名为学习嵌入传播（LEP）的方法，旨在降低语言适应过程的成本。LEP方法通过最小化对现有大语言模型（LLM）知识的影响，减少了对训练数据的需求。与传统的指令调优方法相比，LEP能够直接将新的语言知识植入到现有的指令调优模型中，从而跳过指令调优步骤。实验结果表明，LEP在俄语词汇适应方面的表现与传统方法相当，且通过自我校准和持续调优进一步提升了任务解决能力。'}}}, {'id': 'https://huggingface.co/papers/2412.21139', 'title': 'Training Software Engineering Agents and Verifiers with SWE-Gym', 'url': 'https://huggingface.co/papers/2412.21139', 'abstract': 'We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to train language model based SWE agents , achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve 32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new state-of-the-art for open-weight SWE agents. To facilitate further research, we publicly release SWE-Gym, models, and agent trajectories.', 'score': 9, 'issue_id': 1406, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '800bb3f4c48e2cf9', 'authors': ['Jiayi Pan', 'Xingyao Wang', 'Graham Neubig', 'Navdeep Jaitly', 'Heng Ji', 'Alane Suhr', 'Yizhe Zhang'], 'affiliations': ['Apple', 'CMU', 'UC Berkeley', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2412.21139.jpg', 'data': {'categories': ['#dataset', '#open_source', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'SWE-Gym: революция в обучении ИИ-агентов для разработки ПО', 'desc': 'SWE-Gym - это новая среда для обучения агентов программной инженерии на реальных задачах. Она содержит 2438 экземпляров задач на Python с исполняемой средой, юнит-тестами и описанием на естественном языке. Авторы использовали SWE-Gym для обучения агентов на основе языковых моделей, достигнув улучшения на 19% в решении задач из наборов SWE-Bench. Комбинация обученных агентов и верификаторов позволила достичь нового рекорда производительности для открытых моделей в программной инженерии.'}, 'en': {'title': 'Revolutionizing Software Engineering with SWE-Gym', 'desc': 'SWE-Gym is a novel environment designed for training software engineering agents using real-world Python tasks. It includes 2,438 task instances, each with a codebase, executable environment, unit tests, and natural language task descriptions. The paper demonstrates that language model-based agents trained in SWE-Gym can significantly improve their performance, achieving up to 19% higher resolve rates on benchmark tests. Additionally, the authors explore scaling inference through verifiers, leading to state-of-the-art results for open-weight software engineering agents, and they provide resources for further research.'}, 'zh': {'title': 'SWE-Gym：软件工程代理的新起点', 'desc': '我们提出了SWE-Gym，这是第一个用于训练真实世界软件工程（SWE）代理的环境。SWE-Gym包含2438个真实的Python任务实例，每个实例都有可执行的运行环境、单元测试和用自然语言指定的任务。通过使用SWE-Gym，我们训练的基于语言模型的SWE代理在流行的SWE-Bench验证和Lite测试集上实现了高达19%的绝对解决率提升。我们还通过在SWE-Gym中采样的代理轨迹训练验证器，进行推理时的扩展，结合我们微调的SWE代理，在SWE-Bench验证和Lite上分别达到了32.0%和26.0%的新状态，成为开放权重SWE代理的新标杆。'}}}, {'id': 'https://huggingface.co/papers/2412.21206', 'title': 'PERSE: Personalized 3D Generative Avatars from A Single Portrait', 'url': 'https://huggingface.co/papers/2412.21206', 'abstract': "We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person.", 'score': 8, 'issue_id': 1415, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '334a60a17f9a9477', 'authors': ['Hyunsoo Cha', 'Inhee Lee', 'Hanbyul Joo'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2412.21206.jpg', 'data': {'categories': ['#3d', '#cv', '#dataset', '#synthetic'], 'emoji': '🎭', 'ru': {'title': 'Персонализированные аватары с гибким редактированием черт лица', 'desc': 'PERSE - это метод создания анимируемого персонализированного генеративного аватара на основе портрета. Он позволяет редактировать лицевые атрибуты в непрерывном и разделенном латентном пространстве, сохраняя при этом индивидуальность человека. Метод использует синтетические наборы 2D-видео для обучения модели на основе 3D Gaussian Splatting. PERSE демонстрирует высокое качество генерации аватаров с интерполированными атрибутами, сохраняя идентичность исходного человека.'}, 'en': {'title': 'Create Your Unique Avatar with PERSE!', 'desc': "PERSE is a novel method for creating personalized generative avatars from a single reference portrait. It allows users to edit facial attributes in a smooth and controlled manner within a continuous latent space, ensuring that the individual's identity remains intact. The approach involves generating large-scale synthetic 2D video datasets that showcase variations in facial expressions and attributes, which are then used to train the avatar model. By employing 3D Gaussian Splatting and a latent space regularization technique, PERSE achieves high-quality, photorealistic avatars with seamless attribute transitions."}, 'zh': {'title': '个性化生成头像的新方法', 'desc': '本文介绍了一种名为PERSE的方法，用于从参考肖像构建可动画的个性化生成头像。该头像模型能够在连续且解耦的潜在空间中编辑面部属性，同时保持个体的身份。我们的方法首先合成大规模的合成2D视频数据集，每个视频包含面部表情和视角的一致变化，并结合原始输入中特定面部属性的变化。通过引入潜在空间正则化技术，我们实现了高质量、逼真的2D视频生成，并在此基础上提出了一种个性化头像创建方法。'}}}, {'id': 'https://huggingface.co/papers/2412.21199', 'title': 'HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation', 'url': 'https://huggingface.co/papers/2412.21199', 'abstract': "We introduce self-invoking code generation, a new task designed to evaluate the progressive reasoning and problem-solving capabilities of LLMs. In this task, models are presented with a base problem and a related, more complex problem. They must solve the base problem and then utilize its solution to address the more complex one. This work features three key contributions. First, we propose a general recipe for generating more challenging versions of existing benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on self-invoking code generation. Second, from the analysis of experimental results over twenty LLMs on our benchmarks, we have two important observations: (i) Most LLMs excel in traditional code generation benchmarks like HumanEval and MBPP, but their performance declines on self-invoking tasks. For example, o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro. (ii) On self-invoking code generation task, the instruction-tuned models demonstrate only marginal improvements compared to the base models. Third, we disclose the types of failure modes that exist in our evaluation results. All these results underscore the need for further advancements in self-invoking code generation tasks and provide a new direction for future research on enhancing LLMs' code reasoning capabilities.", 'score': 6, 'issue_id': 1408, 'pub_date': '2024-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '9d2cebc8f30f722c', 'authors': ['Zhaojian Yu', 'Yilun Zhao', 'Arman Cohan', 'Xiao-Ping Zhang'], 'affiliations': ['Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2412.21199.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Самовызывающийся код: новый рубеж для языковых моделей', 'desc': 'Статья представляет новую задачу для оценки возможностей языковых моделей (LLM) - генерацию самовызывающегося кода. В рамках этой задачи модели должны решить базовую проблему, а затем использовать ее решение для более сложной задачи. Авторы создали три новых бенчмарка: HumanEval Pro, MBPP Pro и BigCodeBench-Lite Pro. Эксперименты показали, что большинство LLM хорошо справляются с традиционными задачами генерации кода, но их производительность снижается на самовызывающихся задачах. Результаты подчеркивают необходимость дальнейших исследований в области улучшения способностей LLM к рассуждению при работе с кодом.'}, 'en': {'title': 'Enhancing LLMs: The Challenge of Self-Invoking Code Generation', 'desc': 'This paper introduces a new task called self-invoking code generation, which tests the reasoning and problem-solving skills of large language models (LLMs). In this task, models first solve a simple problem and then use that solution to tackle a more complex one. The authors create three new benchmarks to evaluate LLMs on this task, revealing that while many models perform well on standard code generation tasks, their performance drops significantly on self-invoking tasks. The findings highlight the limitations of current models and suggest that more research is needed to improve their code reasoning abilities.'}, 'zh': {'title': '自调用代码生成：提升LLMs推理能力的新方向', 'desc': '本文介绍了一种新的任务——自调用代码生成，旨在评估大型语言模型（LLMs）的推理和问题解决能力。在这个任务中，模型需要先解决一个基础问题，然后利用其解决方案来处理一个更复杂的问题。研究提出了三项重要贡献，包括生成更具挑战性的基准测试的通用方法，并创建了三个新基准：HumanEval Pro、MBPP Pro和BigCodeBench-Lite Pro。实验结果显示，大多数LLMs在传统代码生成基准上表现良好，但在自调用任务上的表现却有所下降，表明在自调用代码生成任务上仍需进一步的研究和改进。'}}}, {'id': 'https://huggingface.co/papers/2501.02955', 'title': 'MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models', 'url': 'https://huggingface.co/papers/2501.02955', 'abstract': "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io .", 'score': 23, 'issue_id': 1551, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'a7051c2d239484b4', 'authors': ['Wenyi Hong', 'Yean Cheng', 'Zhuoyi Yang', 'Weihan Wang', 'Lefan Wang', 'Xiaotao Gu', 'Shiyu Huang', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2501.02955.jpg', 'data': {'categories': ['#architecture', '#optimization', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'MotionBench: новый рубеж в понимании движения для моделей компьютерного зрения', 'desc': 'Статья представляет новый бенчмарк MotionBench для оценки способности моделей компьютерного зрения понимать детальные движения в видео. Авторы обнаружили, что существующие модели плохо справляются с этой задачей. Для улучшения результатов предложен новый метод Through-Encoder Fusion, а также использование видео с более высокой частотой кадров. Бенчмарк призван стимулировать развитие более совершенных моделей понимания видео.'}, 'en': {'title': 'Enhancing Video Understanding with Fine-Grained Motion Comprehension', 'desc': "This paper introduces MotionBench, a new benchmark for evaluating how well vision language models (VLMs) understand fine-grained motion in videos. It identifies a gap in current models' abilities to comprehend detailed motion, which is crucial for accurate video analysis. The benchmark includes various motion-oriented question types and diverse video data to ensure comprehensive testing. The authors also propose a Through-Encoder Fusion method to improve VLM performance, highlighting the need for further advancements in fine-grained motion comprehension."}, 'zh': {'title': '提升视频理解的细粒度运动能力', 'desc': '近年来，视觉语言模型（VLMs）在视频理解方面取得了显著进展。然而，细粒度运动理解这一关键能力在当前基准测试中仍未得到充分探索。为了解决这一问题，我们提出了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。实验结果表明，现有的VLM在理解细粒度运动方面表现不佳，因此我们提出了一种新颖的Through-Encoder（TE）融合方法，以提高模型的运动理解能力。'}}}, {'id': 'https://huggingface.co/papers/2501.03262', 'title': 'REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models', 'url': 'https://huggingface.co/papers/2501.03262', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical approach for aligning large language models with human preferences, witnessing rapid algorithmic evolution through methods such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We present REINFORCE++, an enhanced variant of the classical REINFORCE algorithm that incorporates key optimization techniques from PPO while eliminating the need for a critic network. REINFORCE++ achieves three primary objectives: (1) simplicity (2) enhanced training stability, and (3) reduced computational overhead. Through extensive empirical evaluation, we demonstrate that REINFORCE++ exhibits superior stability compared to GRPO and achieves greater computational efficiency than PPO while maintaining comparable performance. The implementation is available at https://github.com/OpenRLHF/OpenRLHF.', 'score': 21, 'issue_id': 1553, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': 'a05acf5aab0c07dd', 'authors': ['Jian Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.03262.jpg', 'data': {'categories': ['#training', '#rlhf', '#optimization', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'REINFORCE++: Простой и эффективный алгоритм для RLHF', 'desc': 'В статье представлен REINFORCE++, улучшенная версия алгоритма REINFORCE для обучения с подкреплением на основе обратной связи от человека (RLHF). REINFORCE++ сочетает ключевые техники оптимизации из PPO, но не требует использования критической нейронной сети. Алгоритм отличается простотой, повышенной стабильностью обучения и сниженными вычислительными затратами. Эмпирические исследования показывают, что REINFORCE++ демонстрирует лучшую стабильность по сравнению с GRPO и большую вычислительную эффективность, чем PPO, при сохранении сопоставимой производительности.'}, 'en': {'title': 'REINFORCE++: Simplifying Reinforcement Learning with Human Feedback', 'desc': 'This paper introduces REINFORCE++, a new version of the REINFORCE algorithm designed to improve the training of reinforcement learning models using human feedback. It combines the strengths of Proximal Policy Optimization (PPO) while removing the need for a critic network, making it simpler and more efficient. The authors highlight that REINFORCE++ offers better training stability and lower computational costs compared to existing methods like GRPO and PPO. Their experiments show that REINFORCE++ performs well while being easier to use and faster to train.'}, 'zh': {'title': 'REINFORCE++：简化与高效的强化学习新选择', 'desc': '强化学习中的人类反馈（RLHF）是一种重要的方法，用于使大型语言模型更符合人类的偏好。本文提出了REINFORCE++，这是经典REINFORCE算法的增强版本，结合了PPO的优化技术，并且不再需要评论网络。REINFORCE++的主要目标是实现简单性、提高训练稳定性和减少计算开销。通过大量实证评估，我们证明了REINFORCE++在稳定性上优于GRPO，并且在计算效率上超过PPO，同时保持了相似的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.03895', 'title': 'LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token', 'url': 'https://huggingface.co/papers/2501.03895', 'abstract': 'The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.', 'score': 8, 'issue_id': 1550, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '925d2f81d6fcbb0b', 'authors': ['Shaolei Zhang', 'Qingkai Fang', 'Zhe Yang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.03895.jpg', 'data': {'categories': ['#agi', '#video', '#multimodal', '#architecture', '#optimization', '#cv', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективность через минимизацию: революция в мультимодальных моделях', 'desc': 'Статья представляет LLaVA-Mini - эффективную мультимодальную модель с минимальным количеством визуальных токенов. Авторы обнаружили, что большинство визуальных токенов играют ключевую роль только в ранних слоях языковой модели. LLaVA-Mini вводит предварительное слияние модальностей, чтобы объединить визуальную информацию с текстовыми токенами заранее. Эксперименты показывают, что LLaVA-Mini превосходит LLaVA-v1.5, используя всего 1 визуальный токен вместо 576, что значительно повышает эффективность обработки.'}, 'en': {'title': 'Maximizing Efficiency with Minimal Vision Tokens in LMMs', 'desc': 'This paper presents LLaVA-Mini, an efficient large multimodal model (LMM) designed to reduce the number of vision tokens while maintaining visual information integrity. The authors identify that most vision tokens are primarily important in the early layers of the language model, where they integrate visual data with text. By implementing a technique called modality pre-fusion, LLaVA-Mini compresses the input from 576 vision tokens to just one, significantly enhancing efficiency. Experimental results show that LLaVA-Mini not only outperforms its predecessor but also achieves a 77% reduction in computational load and rapid processing times for high-resolution images and videos.'}, 'zh': {'title': '高效多模态模型LLaVA-Mini的创新之路', 'desc': '本文介绍了一种高效的多模态模型LLaVA-Mini，该模型通过减少视觉标记的数量来提高效率。研究发现，大多数视觉标记在大型语言模型的早期层中起着关键作用，因此可以在此之前将视觉信息与文本标记融合。LLaVA-Mini采用了模态预融合的方法，将视觉信息提前融合，从而将输入到语言模型的视觉标记压缩为一个标记。实验结果表明，LLaVA-Mini在多个基准测试中表现优于之前的模型，且显著降低了计算复杂度和延迟。'}}}, {'id': 'https://huggingface.co/papers/2501.03575', 'title': 'Cosmos World Foundation Model Platform for Physical AI', 'url': 'https://huggingface.co/papers/2501.03575', 'abstract': 'Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via https://github.com/NVIDIA/Cosmos.', 'score': 5, 'issue_id': 1552, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': 'f4b2044cbc1076a8', 'authors': ['NVIDIA', ':', 'Niket Agarwal', 'Arslan Ali', 'Maciej Bala', 'Yogesh Balaji', 'Erik Barker', 'Tiffany Cai', 'Prithvijit Chattopadhyay', 'Yongxin Chen', 'Yin Cui', 'Yifan Ding', 'Daniel Dworakowski', 'Jiaojiao Fan', 'Michele Fenzi', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Songwei Ge', 'Yunhao Ge', 'Jinwei Gu', 'Siddharth Gururani', 'Ethan He', 'Jiahui Huang', 'Jacob Huffman', 'Pooya Jannaty', 'Jingyi Jin', 'Seung Wook Kim', 'Gergely Klár', 'Grace Lam', 'Shiyi Lan', 'Laura Leal-Taixe', 'Anqi Li', 'Zhaoshuo Li', 'Chen-Hsuan Lin', 'Tsung-Yi Lin', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Kaichun Mo', 'Arsalan Mousavian', 'Seungjun Nah', 'Sriharsha Niverty', 'David Page', 'Despoina Paschalidou', 'Zeeshan Patel', 'Lindsey Pavao', 'Morteza Ramezanali', 'Fitsum Reda', 'Xiaowei Ren', 'Vasanth Rao Naik Sabavat', 'Ed Schmerling', 'Stella Shi', 'Bartosz Stefaniak', 'Shitao Tang', 'Lyne Tchapmi', 'Przemek Tredak', 'Wei-Cheng Tseng', 'Jibin Varghese', 'Hao Wang', 'Haoxiang Wang', 'Heng Wang', 'Ting-Chun Wang', 'Fangyin Wei', 'Xinyue Wei', 'Jay Zhangjie Wu', 'Jiashu Xu', 'Wei Yang', 'Lin Yen-Chen', 'Xiaohui Zeng', 'Yu Zeng', 'Jing Zhang', 'Qinsheng Zhang', 'Yuxuan Zhang', 'Qingqing Zhao', 'Artur Zolkowski'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2501.03575.jpg', 'data': {'categories': ['#open_source', '#data', '#benchmark', '#architecture', '#video', '#multimodal', '#dataset', '#training'], 'emoji': '🌍', 'ru': {'title': 'Цифровой двойник мира для обучения физического ИИ', 'desc': 'Статья представляет платформу Cosmos World Foundation Model для разработки моделей мира в физическом ИИ. Авторы предлагают концепцию базовой модели мира, которую можно дообучать для конкретных приложений. Платформа включает конвейер курации видео, предобученные базовые модели мира, примеры дообучения и токенизаторы видео. Проект открытый и доступен на GitHub для помощи разработчикам физического ИИ в решении важных проблем общества.'}, 'en': {'title': 'Empowering Physical AI with Customizable World Models', 'desc': 'This paper introduces the Cosmos World Foundation Model Platform, designed to assist developers in creating tailored world models for Physical AI systems. It emphasizes the necessity of having a digital twin of both the AI and its environment to enable effective training. The platform includes a comprehensive video curation pipeline, pre-trained models, and tools for fine-tuning these models for specific applications. By making the platform and models open-source, the authors aim to empower developers to address significant societal challenges using Physical AI.'}, 'zh': {'title': '构建物理AI的数字双胞胎与世界模型', 'desc': '这篇论文介绍了物理人工智能（Physical AI）在数字训练中的重要性。为了实现这一目标，需要构建一个数字双胞胎（digital twin）和一个世界模型（world model）。我们提出了Cosmos世界基础模型平台，帮助开发者为物理人工智能定制世界模型。该平台提供了视频策划管道、预训练的世界基础模型以及后训练示例，旨在解决社会中的关键问题，并且是开源的。'}}}, {'id': 'https://huggingface.co/papers/2501.03847', 'title': 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control', 'url': 'https://huggingface.co/papers/2501.03847', 'abstract': 'Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.', 'score': 4, 'issue_id': 1552, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '975d5fa9d59bde28', 'authors': ['Zekai Gu', 'Rui Yan', 'Jiahao Lu', 'Peng Li', 'Zhiyang Dou', 'Chenyang Si', 'Zhen Dong', 'Qifeng Liu', 'Cheng Lin', 'Ziwei Liu', 'Wenping Wang', 'Yuan Liu'], 'affiliations': ['Hong Kong University of Science and Technology, China', 'Nanyang Technological University, Singapore', 'Texas A&M University, U.S.A', 'The University of Hong Kong, China', 'Wuhan University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.03847.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': '🎬', 'ru': {'title': 'DaS: Универсальный контроль над генерацией видео через 3D-сигналы', 'desc': 'Авторы представляют новый подход под названием Diffusion as Shader (DaS) для контролируемой генерации видео с помощью диффузионных моделей. В отличие от существующих методов, ограниченных одним типом контроля, DaS поддерживает множество задач управления видео в единой архитектуре. Ключевая идея заключается в использовании 3D-сигналов управления, что делает процесс диффузии видео изначально 3D-ориентированным. DaS демонстрирует сильные возможности управления в различных задачах, включая генерацию видео из 3D-моделей, контроль камеры, перенос движения и манипуляции с объектами.'}, 'en': {'title': 'Empowering Video Generation with 3D Control Signals', 'desc': 'This paper presents Diffusion as Shader (DaS), a new method for generating videos that allows for precise control over various aspects of video creation. Unlike previous models that only used 2D control signals, DaS utilizes 3D tracking videos, which helps in managing the dynamic nature of video content. This approach enables users to manipulate video elements like camera angles and object movements more effectively. The results show that DaS can maintain high-quality video generation while ensuring temporal consistency across frames, even with limited training data.'}, 'zh': {'title': '多样化视频控制的新方法：扩散作为着色器', 'desc': '扩散模型在从文本提示或图像生成高质量视频方面表现出色。然而，精确控制视频生成过程，如相机操作或内容编辑，仍然是一个重大挑战。现有的受控视频生成方法通常仅限于单一控制类型，缺乏处理多样化控制需求的灵活性。本文提出了一种新方法——扩散作为着色器（DaS），它在统一架构中支持多种视频控制任务，利用3D控制信号来实现更灵活的视频控制。'}}}, {'id': 'https://huggingface.co/papers/2501.02260', 'title': 'MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control', 'url': 'https://huggingface.co/papers/2501.02260', 'abstract': "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.", 'score': 2, 'issue_id': 1550, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': '9eeeb5b132839793', 'authors': ['Mengting Wei', 'Tuomas Varanka', 'Xingxun Jiang', 'Huai-Qian Khor', 'Guoying Zhao'], 'affiliations': ['Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland', 'Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.02260.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#open_source', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Точное редактирование мимики с сохранением личности', 'desc': 'Статья представляет новый подход к редактированию мимики лица с использованием диффузионной модели, названной MagicFace. Модель позволяет точно и интерпретируемо изменять выражение лица конкретного человека, сохраняя его идентичность, позу и фоновые детали. Ключевым элементом является условная генерация на основе вариаций лицевых единиц действия (AU) и использование ID-энкодера для сохранения деталей лица. MagicFace демонстрирует превосходные результаты в высококачественном редактировании выражений лица по сравнению с другими методами.'}, 'en': {'title': 'MagicFace: Fine-Grained Facial Expression Editing with Consistent Identity', 'desc': 'This paper presents a method for editing facial expressions while maintaining the identity and other attributes of the person. The proposed model, named MagicFace, utilizes a diffusion model that is conditioned on facial action unit (AU) variations, allowing for fine-grained control over expressions. It incorporates a pretrained Stable-Diffusion model and an ID encoder to ensure high consistency in facial details. Additionally, an Attribute Controller is introduced to maintain background and pose consistency during the editing process, resulting in high-fidelity expression animations.'}, 'zh': {'title': '魔法面孔：高保真面部表情编辑的创新之路', 'desc': '我们提出了一种面部表情编辑的方法，通过控制同一人的面部动作单元（AU）的相对变化来实现。这种方法可以细致、连续且可解释地编辑特定人的表情，同时保持他们的身份、姿势、背景和面部细节。我们的模型称为MagicFace，核心是一个基于AU变化的扩散模型和一个ID编码器，以保持面部细节的一致性。通过将AU变化注入去噪UNet，我们的模型能够以高保真度编辑面部表情，效果优于其他相关工作。'}}}, {'id': 'https://huggingface.co/papers/2501.03931', 'title': 'Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2501.03931', 'abstract': 'We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/', 'score': 2, 'issue_id': 1550, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '1c9696a99b57f781', 'authors': ['Yuechen Zhang', 'Yaoyang Liu', 'Bin Xia', 'Bohao Peng', 'Zexin Yan', 'Eric Lo', 'Jiaya Jia'], 'affiliations': ['CMU', 'CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2501.03931.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#open_source', '#synthetic', '#architecture', '#diffusion'], 'emoji': '🪞', 'ru': {'title': 'Магическое зеркало: видео с сохранением личности и естественным движением', 'desc': 'Magic Mirror - это новая система для создания видео с сохранением идентичности и кинематографическим качеством. Она использует модель видеодиффузии и вводит три ключевых компонента: двойной экстрактор лицевых признаков, легкий кросс-модальный адаптер и двухэтапную стратегию обучения. Система эффективно сочетает сохранение идентичности с естественным движением, превосходя существующие методы по нескольким метрикам. Magic Mirror требует минимального добавления параметров и будет доступна в открытом доступе.'}, 'en': {'title': 'Magic Mirror: Identity-Preserved Video Generation with Cinematic Quality', 'desc': 'Magic Mirror is a new framework designed to create high-quality videos that maintain the identity of individuals while showcasing dynamic motion. It addresses the challenges faced by previous video generation methods, which often struggled to keep a consistent identity or required extensive fine-tuning for specific individuals. The framework utilizes Video Diffusion Transformers and introduces innovative components like a dual-branch facial feature extractor and a cross-modal adapter to enhance identity integration. Through a two-stage training approach, Magic Mirror achieves a remarkable balance between identity preservation and natural motion, outperforming existing techniques with fewer additional parameters.'}, 'zh': {'title': 'Magic Mirror：保持身份一致的动态视频生成', 'desc': '本文介绍了Magic Mirror，一个用于生成保持身份一致的视频框架，具有电影级质量和动态运动。尽管最近的视频扩散模型在文本到视频生成方面取得了显著进展，但在生成自然运动的同时保持一致的身份仍然具有挑战性。我们的方法基于视频扩散变换器，提出了三个关键组件，以有效整合身份信息并保持运动多样性。实验结果表明，Magic Mirror在多个指标上超越了现有方法，同时增加的参数极少。'}}}, {'id': 'https://huggingface.co/papers/2501.01895', 'title': 'EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation', 'url': 'https://huggingface.co/papers/2501.01895', 'abstract': "We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.", 'score': 41, 'issue_id': 1506, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': 'bae2a6e63f87958d', 'authors': ['Siyuan Huang', 'Liliang Chen', 'Pengfei Zhou', 'Shengcong Chen', 'Zhengkai Jiang', 'Yue Hu', 'Peng Gao', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['AgiBot', 'CUHK', 'FDU', 'HIT', 'HKUST', 'SJTU', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2501.01895.jpg', 'data': {'categories': ['#3d', '#data', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'EnerVerse: Революция в пространственном моделировании для роботов-манипуляторов', 'desc': 'EnerVerse - это комплексная система для генерации пространства будущего в задачах роботизированной манипуляции. Она использует сверточные механизмы и двунаправленное внимание для моделирования внутренних фрагментов пространства, обеспечивая согласованность на низком уровне. Система вводит пространство Free Anchor View для гибких перспектив наблюдения и анализа, улучшая обобщение и адаптивность робота. EnerVerse также включает конвейер данных, интегрирующий генеративную модель с 4D Gaussian Splatting для сужения разрыва между симуляцией и реальностью.'}, 'en': {'title': 'Empowering Robots with EnerVerse: A New Era in Space Generation and Manipulation', 'desc': 'EnerVerse is a new framework designed to help robots better understand and manipulate their environments. It uses advanced techniques like convolutional and bidirectional attention mechanisms to create a consistent model of space. By recognizing that video data often has unnecessary information, EnerVerse employs a sparse memory context to generate long sequences efficiently. Additionally, the Free Anchor View (FAV) space allows robots to observe from different angles, improving their ability to adapt and perform tasks in various settings.'}, 'zh': {'title': 'EnerVerse：提升机器人操作的未来空间生成框架', 'desc': '本文介绍了EnerVerse，这是一个专为机器人操作任务设计的未来空间生成框架。EnerVerse结合了卷积和双向注意机制，以确保内部空间建模的一致性和连续性。我们提出了一种稀疏记忆上下文和单向生成范式的结合，能够生成无限长的序列，从而提高机器人的能力。通过引入自由锚视图空间（FAV），我们增强了观察和分析的灵活性，显著改善了机器人在各种任务和环境中的泛化能力和适应性。'}}}, {'id': 'https://huggingface.co/papers/2501.01957', 'title': 'VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction', 'url': 'https://huggingface.co/papers/2501.01957', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.', 'score': 19, 'issue_id': 1506, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': 'b6690c7efedf5a39', 'authors': ['Chaoyou Fu', 'Haojia Lin', 'Xiong Wang', 'Yi-Fan Zhang', 'Yunhang Shen', 'Xiaoyu Liu', 'Yangze Li', 'Zuwei Long', 'Heting Gao', 'Ke Li', 'Xiawu Zheng', 'Rongrong Ji', 'Xing Sun', 'Caifeng Shan', 'Ran He'], 'affiliations': ['CASIA', 'NJU', 'Tencent Youtu Lab', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2501.01957.jpg', 'data': {'categories': ['#training', '#cv', '#multimodal', '#benchmark', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в мультимодальном взаимодействии: речь и зрение в одной модели', 'desc': 'В статье представлена новая методология обучения мультимодальных языковых моделей, объединяющая визуальную и речевую модальности. Авторы предлагают поэтапный подход к обучению, который позволяет модели эффективно понимать как визуальную, так и речевую информацию. Модель демонстрирует высокую производительность в задачах обработки изображений, видео и речи, превосходя современные аналоги. Этот подход обеспечивает возможность ведения диалога с использованием речи и изображений в режиме, близком к реальному времени.'}, 'en': {'title': 'Enhancing Multimodal Interaction with Speech and Vision Integration', 'desc': 'This paper introduces a novel training methodology for Multimodal Large Language Models (MLLMs) that enhances their ability to process both visual and speech data. The proposed multi-stage training approach allows the model to progressively learn and integrate information from images, videos, and spoken language, facilitating seamless interaction. By eliminating the need for separate Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) modules, the model achieves faster response times in multimodal dialogues. Experimental results show that this method not only maintains strong vision-language performance but also excels in speech tasks, enabling near real-time interactions.'}, 'zh': {'title': '实现流畅的视觉与语音交互', 'desc': '最近的多模态大型语言模型（MLLMs）主要集中在视觉和文本的整合上，而对语音在增强交互中的作用关注较少。然而，语音在多模态对话系统中起着至关重要的作用，如何在视觉和语音任务中实现高性能仍然是一个重大挑战。本文提出了一种精心设计的多阶段训练方法，逐步训练大型语言模型理解视觉和语音信息，从而实现流畅的视觉和语音交互。我们的方法不仅保持了强大的视觉-语言能力，还实现了高效的语音对话能力，显著加快了多模态端到端的响应速度。'}}}, {'id': 'https://huggingface.co/papers/2501.01904', 'title': 'Virgo: A Preliminary Exploration on Reproducing o1-like MLLM', 'url': 'https://huggingface.co/papers/2501.01904', 'abstract': 'Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.', 'score': 12, 'issue_id': 1505, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '576423a20b419d0f', 'authors': ['Yifan Du', 'Zikang Liu', 'Yifan Li', 'Wayne Xin Zhao', 'Yuqi Huo', 'Bingning Wang', 'Weipeng Chen', 'Zheng Liu', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'Baichuan AI', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01904.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#transfer_learning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Обучение мультимодальных ИИ длительным рассуждениям через текст', 'desc': 'Статья описывает исследование в области мультимодальных больших языковых моделей (MLLM) и их способности к медленному мышлению. Авторы предлагают метод Virgo, который позволяет обучить MLLM длительным рассуждениям с помощью небольшого количества текстовых данных. Результаты показывают, что текстовые данные для обучения рассуждениям могут быть даже эффективнее визуальных. Это исследование демонстрирует, что способности к медленному мышлению в основном связаны с языковым компонентом модели и могут переноситься между модальностями.'}, 'en': {'title': 'Unlocking Slow-Thinking in Multimodal Models with Textual Reasoning', 'desc': 'This paper discusses the development of a multimodal slow-thinking reasoning system called Virgo, which is based on fine-tuning a multimodal large language model (MLLM) using long-form textual reasoning data. The authors found that incorporating long-form reasoning in natural language significantly enhances the slow-thinking capabilities of MLLMs, even more so than using visual reasoning data. This suggests that the slow-thinking abilities are closely linked to the language model aspect, allowing for effective transfer across different data modalities. The research indicates a promising direction for creating advanced reasoning systems that can handle complex data semantics.'}, 'zh': {'title': '多模态慢思维推理的探索', 'desc': '最近，基于大型语言模型（LLMs）的慢思维推理系统引起了广泛关注，尤其是在推理过程中延长思考时间的能力。本文探讨了如何将这种能力应用于多模态大型语言模型（MLLMs），尽管处理不同模态的复杂数据语义更具挑战性。我们通过微调一个强大的MLLM，使用少量的长文本思维数据，成功构建了一个多模态慢思维系统，命名为Virgo（视觉推理与长思维）。研究表明，长文本推理过程可以有效转移到MLLMs，并且这种文本推理数据在激发MLLMs的慢思维能力方面，似乎比视觉推理数据更有效。'}}}, {'id': 'https://huggingface.co/papers/2412.21059', 'title': 'VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation', 'url': 'https://huggingface.co/papers/2412.21059', 'abstract': 'We present a general strategy to aligning visual generation models -- both image and video generation -- with human preference. To start with, we build VisionReward -- a fine-grained and multi-dimensional reward model. We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score. To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2% and achieve top performance for video preference prediction. Based on VisionReward, we develop a multi-objective preference learning algorithm that effectively addresses the issue of confounding factors within preference data. Our approach significantly outperforms existing image and video scoring methods on both machine metrics and human evaluation. All code and datasets are provided at https://github.com/THUDM/VisionReward.', 'score': 11, 'issue_id': 1510, 'pub_date': '2025-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '1f3bb267ffa751d9', 'authors': ['Jiazheng Xu', 'Yu Huang', 'Jiale Cheng', 'Yuanming Yang', 'Jiajun Xu', 'Yuan Wang', 'Wenbo Duan', 'Shen Yang', 'Qunlin Jin', 'Shurun Li', 'Jiayan Teng', 'Zhuoyi Yang', 'Wendi Zheng', 'Xiao Liu', 'Ming Ding', 'Xiaohan Zhang', 'Xiaotao Gu', 'Shiyu Huang', 'Minlie Huang', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.21059.jpg', 'data': {'categories': ['#rag', '#training', '#open_source', '#cv', '#video', '#optimization', '#alignment'], 'emoji': '🎥', 'ru': {'title': 'VisionReward: многомерная оценка визуального контента с учетом человеческих предпочтений', 'desc': 'Исследователи представили стратегию для согласования моделей генерации визуального контента с человеческими предпочтениями. Они разработали VisionReward - многомерную модель вознаграждения, которая декомпозирует предпочтения в изображениях и видео на несколько измерений. Для оценки качества видео были проанализированы различные динамические характеристики, что позволило VisionReward превзойти существующие методы на 17.2%. На основе VisionReward был разработан алгоритм многоцелевого обучения предпочтениям, эффективно решающий проблему конфаундинг-факторов в данных о предпочтениях.'}, 'en': {'title': 'Aligning Visual Generation with Human Preferences', 'desc': 'This paper introduces a method for aligning visual generation models, such as those for images and videos, with human preferences. The authors create a reward model called VisionReward, which breaks down human preferences into multiple dimensions assessed through specific judgment questions. They enhance video quality assessment by analyzing dynamic features, leading to a 17.2% improvement over previous methods. Additionally, a multi-objective preference learning algorithm is developed to manage confounding factors in preference data, resulting in superior performance compared to existing scoring methods.'}, 'zh': {'title': '视觉生成模型与人类偏好的完美对齐', 'desc': '本文提出了一种通用策略，用于将视觉生成模型（包括图像和视频生成）与人类偏好对齐。我们构建了VisionReward，这是一个细粒度和多维度的奖励模型，能够将人类对图像和视频的偏好分解为多个维度。通过分析视频的动态特征，VisionReward在视频偏好预测中超越了现有方法，提升了17.2%的性能。基于VisionReward，我们开发了一种多目标偏好学习算法，有效解决了偏好数据中的混淆因素问题。'}}}, {'id': 'https://huggingface.co/papers/2501.01821', 'title': 'SDPO: Segment-Level Direct Preference Optimization for Social Agents', 'url': 'https://huggingface.co/papers/2501.01821', 'abstract': "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.", 'score': 10, 'issue_id': 1514, 'pub_date': '2025-01-03', 'pub_date_card': {'ru': '3 января', 'en': 'January 3', 'zh': '1月3日'}, 'hash': '499b008b0bce4f74', 'authors': ['Aobo Kong', 'Wentao Ma', 'Shiwan Zhao', 'Yongbin Li', 'Yuchuan Wu', 'Ke Wang', 'Xiaoqian Liu', 'Qicheng Li', 'Yong Qin', 'Fei Huang'], 'affiliations': ['TMCC, CS, Nankai University', 'Tongyi Lab', 'alibaba-inc.com'], 'pdf_title_img': 'assets/pdf/title_img/2501.01821.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rlhf', '#agents', '#alignment', '#training'], 'emoji': '🤖', 'ru': {'title': 'SDPO: Новый шаг к созданию социально интеллектуальных ИИ-агентов', 'desc': 'В статье представлен новый метод оптимизации поведения языковых моделей (LLM) в сложных многоходовых социальных диалогах - Segment-Level Direct Preference Optimization (SDPO). SDPO фокусируется на ключевых сегментах взаимодействия, что позволяет эффективнее оптимизировать поведение агентов по сравнению с существующими методами. Эксперименты на бенчмарке SOTOPIA показали, что агенты, настроенные с помощью SDPO, превосходят как другие методы на основе DPO, так и проприетарные модели вроде GPT-4. Это демонстрирует потенциал SDPO для повышения социального интеллекта агентов на основе LLM.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with SDPO', 'desc': "This paper introduces Segment-Level Direct Preference Optimization (SDPO), a new method for improving the performance of social agents powered by large language models (LLMs) in complex dialogues. Unlike existing methods that either focus too narrowly on individual turns or too broadly on entire sessions, SDPO targets specific key segments of conversations to better align agent behavior with human preferences. The approach reduces training noise and enhances the agent's ability to engage in multi-turn interactions effectively. Evaluations show that agents trained with SDPO outperform both traditional DPO methods and advanced LLMs like GPT-4o, highlighting its effectiveness in enhancing social intelligence."}, 'zh': {'title': '提升社交智能的新方法：分段级直接偏好优化', 'desc': '本论文提出了一种新的方法，称为分段级直接偏好优化（SDPO），旨在提高大型语言模型（LLM）在多轮社交对话中的表现。现有的直接偏好优化（DPO）方法在处理多轮交互时存在细粒度和粗粒度的局限性，导致训练噪声。SDPO通过关注交互中的关键段落，优化代理的多轮行为，从而减少训练噪声。实验结果表明，SDPO调优的代理在SOTOPIA基准测试中表现优于现有的DPO方法和其他大型语言模型，显示出其在提升社交智能方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.01073', 'title': 'Graph Generative Pre-trained Transformer', 'url': 'https://huggingface.co/papers/2501.01073', 'abstract': "Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction.", 'score': 9, 'issue_id': 1508, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '596abc88d57e0650', 'authors': ['Xiaohui Chen', 'Yinkai Wang', 'Jiaxing He', 'Yuanqi Du', 'Soha Hassoun', 'Xiaolin Xu', 'Li-Ping Liu'], 'affiliations': ['Cornell University', 'Northeastern University', 'Tufts University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01073.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#architecture', '#data', '#graphs'], 'emoji': '🕸️', 'ru': {'title': 'G2PT: Универсальный трансформер для эффективной генерации графов', 'desc': 'В статье представлена новая модель генерации графов - Graph Generative Pre-trained Transformer (G2PT). G2PT использует альтернативный подход к представлению графов в виде последовательностей множеств узлов и рёбер вместо матриц смежности. Модель обучается предсказывать следующий токен автореgressивным способом. G2PT показывает превосходные результаты в генерации как общих графов, так и молекул, а также демонстрирует хорошую адаптивность к различным задачам.'}, 'en': {'title': 'Revolutionizing Graph Generation with G2PT', 'desc': 'This paper focuses on improving graph generation, which is important for tasks like designing molecules and analyzing social networks. Instead of using the common adjacency matrix, it proposes a new way to represent graphs as sequences of node and edge sets, making the encoding more efficient. The authors introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns to generate graph structures by predicting the next token in a sequence. Through various experiments, they demonstrate that G2PT outperforms existing models in generating graphs and is effective in applications like molecular design and predicting graph properties.'}, 'zh': {'title': '图生成的创新：G2PT模型', 'desc': '图生成在许多领域中非常重要，比如分子设计和社交网络分析，因为它能够建模复杂的关系和结构化数据。本文提出了一种新的图表示方法，将图表示为节点集和边集的序列，而不是传统的邻接矩阵。基于这种表示，我们引入了图生成预训练变换器（G2PT），这是一种通过下一个标记预测学习图结构的自回归模型。实验结果表明，G2PT在通用图和分子数据集上表现出色，并且在分子设计和属性预测等下游任务中具有很强的适应性和多功能性。'}}}, {'id': 'https://huggingface.co/papers/2501.00874', 'title': 'LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models', 'url': 'https://huggingface.co/papers/2501.00874', 'abstract': "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.", 'score': 7, 'issue_id': 1507, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': '5bdfec436923a2a6', 'authors': ['Hieu Man', 'Nghia Trung Ngo', 'Viet Dac Lai', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research, USA', 'Dept. of Computer Science, University of Oregon, OR, USA'], 'pdf_title_img': 'assets/pdf/title_img/2501.00874.jpg', 'data': {'categories': ['#transfer_learning', '#architecture', '#benchmark', '#multilingual', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Универсальные многоязычные эмбеддинги без многоязычного обучения', 'desc': 'LUSIFER - это новый подход к созданию многоязычных эмбеддингов без использования многоязычных обучающих данных. Он объединяет многоязычный энкодер и LLM-модель для эмбеддингов через набор обучаемых параметров. Авторы также представили новый бенчмарк для оценки качества многоязычных эмбеддингов, охватывающий 5 основных задач, 123 датасета и 14 языков. Эксперименты показали, что LUSIFER значительно улучшает многоязычную производительность, особенно для языков с ограниченными ресурсами.'}, 'en': {'title': 'LUSIFER: Bridging Multilingual Gaps in Text Embedding', 'desc': "This paper introduces LUSIFER, a new method that enhances large language models (LLMs) for multilingual text embedding tasks. Unlike existing models that mainly focus on English, LUSIFER uses a zero-shot approach to adapt LLMs for multiple languages without needing multilingual training data. It combines a multilingual encoder with an LLM-based embedding model, allowing for effective language understanding and embedding performance. The authors also present a comprehensive benchmark to evaluate LUSIFER's performance across various languages and tasks, showing significant improvements, especially for less-resourced languages."}, 'zh': {'title': 'LUSIFER：无监督多语言嵌入的新突破', 'desc': '最近，大型语言模型（LLMs）在文本嵌入任务中取得了新的突破，尤其是在基于密集向量的检索方面。然而，这些模型主要集中在英语上，导致多语言嵌入能力尚未得到充分探索。为了解决这个问题，我们提出了LUSIFER，这是一种新颖的零样本方法，可以在不需要多语言监督的情况下，将LLM嵌入模型适应于多语言任务。LUSIFER的架构结合了一个多语言编码器和一个针对嵌入特定任务优化的LLM嵌入模型，通过一组最小的可训练参数实现无缝连接，有效地将多语言编码器的语言理解能力转移到专门的嵌入模型上。'}}}, {'id': 'https://huggingface.co/papers/2501.01540', 'title': 'BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery', 'url': 'https://huggingface.co/papers/2501.01540', 'abstract': "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.", 'score': 4, 'issue_id': 1510, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '0f853b1681ef29b5', 'authors': ['Kanishk Gandhi', 'Michael Y. Li', 'Lyle Goodyear', 'Louise Li', 'Aditi Bhaskar', 'Mohammed Zaman', 'Noah D. Goodman'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01540.jpg', 'data': {'categories': ['#benchmark', '#data', '#science', '#agents'], 'emoji': '🧪', 'ru': {'title': 'BoxingGym: новый вызов для ИИ в научном моделировании', 'desc': 'Статья представляет новый бенчмарк BoxingGym для оценки способности языковых моделей (LLM) к научному открытию. Бенчмарк включает 10 сред, моделирующих различные научные области, и позволяет тестировать планирование экспериментов и построение теорий. Для оценки качества экспериментов используется ожидаемый прирост информации (EIG), а для оценки теорий - их способность объяснять и предсказывать. Результаты показывают, что современные LLM, включая GPT-4, пока слабо справляются с этими задачами.'}, 'en': {'title': 'BoxingGym: Evaluating LLMs in Scientific Discovery', 'desc': 'This paper introduces BoxingGym, a benchmark designed to evaluate the capabilities of large language models (LLMs) in scientific discovery tasks. It focuses on two main aspects: experimental design, which involves collecting data to test scientific theories, and model discovery, which includes proposing and revising these theories. The benchmark consists of 10 environments modeled as generative probabilistic models from various scientific fields, allowing for interactive experimentation. The study finds that current LLMs, like GPT-4o, face challenges in both areas, and adding a statistical model does not consistently enhance their performance.'}, 'zh': {'title': '评估人工智能在科学研究中的能力', 'desc': '这篇论文探讨了人工智能在科学研究中的应用，特别是大型语言模型（LLM）在提出科学理论和设计实验方面的能力。作者提出了一个名为BoxingGym的基准测试，包含10个环境，用于系统评估实验设计和模型发现的能力。通过计算期望信息增益（EIG），论文量化了科学代理收集实验数据的有效性，并评估其提出的模型是否能进行可靠预测。研究发现，当前的LLM在实验设计和模型发现方面表现不佳，且简单地增加统计模型并未显著改善结果。'}}}, {'id': 'https://huggingface.co/papers/2501.00958', 'title': '2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining', 'url': 'https://huggingface.co/papers/2501.00958', 'abstract': 'Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}.', 'score': 68, 'issue_id': 1475, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': 'b10f0cd62f6334fc', 'authors': ['Wenqi Zhang', 'Hang Zhang', 'Xin Li', 'Jiashuo Sun', 'Yongliang Shen', 'Weiming Lu', 'Deli Zhao', 'Yueting Zhuang', 'Lidong Bing'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University', 'DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2501.00958.jpg', 'data': {'categories': ['#science', '#dataset', '#reasoning', '#multimodal', '#cv', '#video'], 'emoji': '📚', 'ru': {'title': 'Мультимодальный учебник: новый стандарт для обучения VLM', 'desc': 'Эта статья представляет новый подход к обучению моделей компьютерного зрения и обработки естественного языка (VLM) с использованием мультимодального учебного корпуса. Авторы создали базу данных из 22 000 часов обучающих видео, систематически собранных с помощью таксономии, предложенной языковой моделью (LLM). Этот корпус отличается более высокой плотностью знаний, лучшей связью между изображениями и текстом, а также логической согласованностью по сравнению с существующими наборами данных. Эксперименты показывают превосходную производительность предобучения на этом корпусе, особенно в задачах, требующих глубоких знаний и рассуждений.'}, 'en': {'title': 'Harnessing Instructional Videos for Superior Vision-Language Model Training', 'desc': 'This paper presents a new approach to training Vision-Language Models (VLMs) using a multimodal textbook corpus derived from instructional videos. Unlike traditional datasets that often suffer from low knowledge density and weak image-text relationships, this corpus offers a richer and more coherent context for VLM pretraining. The authors systematically extract visual, audio, and textual information from over 22,000 hours of instructional content, enhancing the alignment between images and text. Experiments show that VLMs trained on this video-centric dataset perform significantly better on knowledge-intensive tasks, demonstrating improved reasoning and context awareness.'}, 'zh': {'title': '视频教材：提升视觉语言模型的知识与推理能力', 'desc': '本文提出了一种高质量的多模态教材语料库，旨在为视觉语言模型（VLM）提供更丰富的基础知识。该语料库收集了超过2.5年的教学视频，总计22,000小时，系统性地提取了视频中的视觉、音频和文本知识。与现有的数据集相比，这种视频中心的教材提供了更连贯的上下文、更丰富的知识和更好的图像-文本对齐。实验结果表明，基于该教材预训练的VLM在知识和推理密集型任务中表现优异，尤其在ScienceQA和MathVista等任务中。'}}}, {'id': 'https://huggingface.co/papers/2501.01427', 'title': 'VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control', 'url': 'https://huggingface.co/papers/2501.01427', 'abstract': 'Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.', 'score': 39, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '4c67f688775a3eca', 'authors': ['Yuanpeng Tu', 'Hao Luo', 'Xi Chen', 'Sihui Ji', 'Xiang Bai', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group', 'HUST', 'Hupan Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.01427.jpg', 'data': {'categories': ['#diffusion', '#games', '#video'], 'emoji': '🎬', 'ru': {'title': 'Точная вставка объектов в видео с сохранением деталей', 'desc': 'В этой статье представлен VideoAnydoor - фреймворк для вставки объектов в видео без предварительного обучения. Он использует экстрактор идентификаторов и последовательность ограничивающих рамок для контроля движения объекта. Ключевым компонентом является пиксельный варпер, который сохраняет детали внешнего вида и позволяет точно управлять движением. Предложенная стратегия обучения с использованием видео и статических изображений улучшает качество вставки объектов.'}, 'en': {'title': 'Seamless Object Insertion in Videos with VideoAnydoor', 'desc': 'This paper introduces VideoAnydoor, a novel framework for zero-shot video object insertion that excels in maintaining high-fidelity details and precise motion control. The approach begins with a text-to-video model and incorporates an ID extractor to ensure consistent object identity while using a box sequence for motion management. A key innovation is the pixel warper, which adjusts pixel details based on key-point trajectories, enhancing both detail preservation and user control over motion. The proposed training strategy, which combines videos and static images with a reweighted reconstruction loss, significantly improves the quality of object insertion, making VideoAnydoor versatile for various applications without needing specific fine-tuning.'}, 'zh': {'title': '高保真视频对象插入的新突破', 'desc': '尽管视频生成技术取得了显著进展，但将特定对象插入视频仍然是一项具有挑战性的任务。本文提出了VideoAnydoor，这是一个零-shot视频对象插入框架，能够高保真地保留细节并精确控制运动。我们设计了一种像素变形器，能够根据关键点轨迹扭曲像素细节，并与扩散U-Net融合，从而提高细节保留能力。VideoAnydoor在现有方法中表现出显著优势，并支持多种下游应用，无需特定任务的微调。'}}}, {'id': 'https://huggingface.co/papers/2501.01257', 'title': 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings', 'url': 'https://huggingface.co/papers/2501.01257', 'abstract': 'With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.', 'score': 36, 'issue_id': 1475, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'e31430bb6ba5dfc8', 'authors': ['Shanghaoran Quan', 'Jiaxi Yang', 'Bowen Yu', 'Bo Zheng', 'Dayiheng Liu', 'An Yang', 'Xuancheng Ren', 'Bofei Gao', 'Yibo Miao', 'Yunlong Feng', 'Zekun Wang', 'Jian Yang', 'Zeyu Cui', 'Yang Fan', 'Yichang Zhang', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2501.01257.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#reasoning', '#optimization', '#open_source'], 'emoji': '🏆', 'ru': {'title': 'CodeElo: новый стандарт оценки LLM в соревновательном программировании', 'desc': 'Статья представляет новый бенчмарк CodeElo для оценки способностей больших языковых моделей (LLM) в решении задач по программированию соревновательного уровня. CodeElo основан на платформе CodeForces и включает проблемы с детальной информацией о сложности и алгоритмических тегах. Авторы разработали систему расчета рейтинга Эло, сопоставимую с рейтингами человеческих участников. Результаты тестирования 33 LLM показали, что модели o1-mini и QwQ-32B-Preview значительно превосходят остальные, достигая рейтингов 1578 и 1261 соответственно.'}, 'en': {'title': 'CodeElo: Elevating Code Generation Benchmarks for LLMs', 'desc': 'This paper presents CodeElo, a new benchmark designed to evaluate the coding abilities of large language models (LLMs) in a competitive setting. Unlike existing benchmarks, CodeElo addresses limitations such as the lack of private test cases and misaligned execution environments by utilizing the CodeForces platform. The benchmark includes a unique judging method and an Elo rating system that allows for fair comparisons between LLMs and human participants. Results indicate that certain models, like o1-mini, perform significantly better than others, highlighting the varying capabilities of LLMs in code generation tasks.'}, 'zh': {'title': 'CodeElo：提升代码生成能力的标准化基准测试', 'desc': '随着大型语言模型（LLMs）在代码推理能力上的提升，开发更具挑战性和全面性的基准测试变得愈发重要。现有的基准测试如LiveCodeBench和USACO存在一些不足，例如缺乏私有测试用例和特殊评判支持。为了解决这些问题，我们提出了CodeElo，这是一个标准化的竞赛级代码生成基准，首次有效应对这些挑战。通过在CodeForces平台上编译最近六个月的竞赛问题，我们为30个流行的开源和3个专有LLMs提供了Elo评分，结果显示o1-mini和QwQ-32B-Preview表现突出。'}}}, {'id': 'https://huggingface.co/papers/2501.00599', 'title': 'VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM', 'url': 'https://huggingface.co/papers/2501.00599', 'abstract': 'Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.', 'score': 31, 'issue_id': 1474, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': 'daee687ce36ef3db', 'authors': ['Yuqian Yuan', 'Hang Zhang', 'Wentong Li', 'Zesen Cheng', 'Boqiang Zhang', 'Long Li', 'Xin Li', 'Deli Zhao', 'Wenqiao Zhang', 'Yueting Zhuang', 'Jianke Zhu', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00599.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#optimization', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точное пространственно-временное понимание видео с помощью VideoRefer Suite', 'desc': 'Статья представляет VideoRefer Suite - комплексный подход к улучшению пространственно-временного понимания видео большими языковыми моделями. Авторы разработали масштабный набор данных VideoRefer-700K с инструкциями на уровне объектов, созданный с помощью мультиагентного движка. Они также представили модель VideoRefer с универсальным пространственно-временным кодировщиком объектов. Для оценки возможностей видео-LLM был создан бенчмарк VideoRefer-Bench, охватывающий различные аспекты понимания видео.'}, 'en': {'title': 'Empowering Video LLMs for Fine-Grained Understanding', 'desc': 'This paper introduces the VideoRefer Suite, which enhances Video Large Language Models (Video LLMs) for better understanding of videos by focusing on fine-grained spatial and temporal details. It addresses the limitations of existing models that primarily focus on overall comprehension and lack high-quality object-level instruction data. The suite includes a new dataset called VideoRefer-700K, a specialized VideoRefer model with a spatial-temporal object encoder, and a benchmark for evaluating video understanding capabilities. Experimental results show that the VideoRefer model significantly improves performance on video referring tasks while also enhancing general video comprehension.'}, 'zh': {'title': '提升视频理解，细致捕捉空间与时间', 'desc': '视频大型语言模型（Video LLMs）在视频理解方面展现了出色的能力，但在捕捉细粒度的空间和时间细节上存在困难。为了应对这些挑战，我们提出了VideoRefer Suite，以增强视频LLM在空间-时间视频理解方面的能力。我们开发了一个多代理数据引擎，创建了一个高质量的对象级视频指令数据集VideoRefer-700K，并提出了VideoRefer模型，配备了多功能的空间-时间对象编码器。最后，我们创建了VideoRefer-Bench，以全面评估视频LLM的空间-时间理解能力，实验结果表明我们的模型在视频引用基准上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2501.01423', 'title': 'Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models', 'url': 'https://huggingface.co/papers/2501.01423', 'abstract': 'Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.', 'score': 30, 'issue_id': 1473, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '173fa21b6e47d04c', 'authors': ['Jingfeng Yao', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2501.01423.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#architecture', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Революция в латентных диффузионных моделях: быстрее, лучше, эффективнее', 'desc': 'Статья представляет новый подход к улучшению латентных диффузионных моделей с архитектурой Трансформер для генерации изображений высокого качества. Авторы предлагают метод VA-VAE, который выравнивает латентное пространство с предобученными моделями компьютерного зрения. Это позволяет значительно расширить границы реконструкции-генерации и ускорить сходимость Диффузионных Трансформеров в высокоразмерных латентных пространствах. На основе VA-VAE авторы создали улучшенную модель LightningDiT, достигающую современного уровня производительности на задаче генерации изображений ImageNet 256x256.'}, 'en': {'title': 'Accelerating Image Generation with Aligned Latent Spaces', 'desc': 'This paper discusses the challenges faced by latent diffusion models, particularly when using Transformer architectures for image generation. It highlights an optimization issue where increasing the feature dimensions in visual tokenizers can lead to larger models and longer training times, often resulting in sub-optimal image quality. The authors propose a solution by aligning the latent space with pre-trained vision models, introducing a new framework called VA-VAE to enhance the training process. Their improved model, LightningDiT, achieves state-of-the-art performance in image generation while significantly speeding up the training process.'}, 'zh': {'title': '优化潜在扩散模型，提升图像生成效率', 'desc': '本论文探讨了潜在扩散模型与变换器架构在生成高质量图像时的优化困境。研究表明，虽然增加视觉标记器中的每个标记特征维度可以提高重建质量，但这也导致需要更大的扩散模型和更多的训练迭代。为了解决这一问题，作者提出将潜在空间与预训练的视觉基础模型对齐，从而提高训练效率。最终，提出的VA-VAE模型显著提升了潜在扩散模型的重建生成能力，并在ImageNet数据集上实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.00103', 'title': 'LTX-Video: Realtime Video Latent Diffusion', 'url': 'https://huggingface.co/papers/2501.00103', 'abstract': "We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.", 'score': 29, 'issue_id': 1484, 'pub_date': '2025-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': 'a2358f7cf156ff08', 'authors': ['Yoav HaCohen', 'Nisan Chiprut', 'Benny Brazowski', 'Daniel Shalem', 'Dudu Moshe', 'Eitan Richardson', 'Eran Levin', 'Guy Shiran', 'Nir Zabari', 'Ori Gordon', 'Poriya Panet', 'Sapir Weissbuch', 'Victor Kulikov', 'Yaki Bitterman', 'Zeev Melumian', 'Ofir Bibi'], 'affiliations': ['Lightricks'], 'pdf_title_img': 'assets/pdf/title_img/2501.00103.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: быстрее реального времени', 'desc': 'LTX-Video - это трансформерная модель латентной диффузии для генерации видео. Она объединяет функции Video-VAE и шумоподавляющего трансформера, оптимизируя их взаимодействие. Модель использует сильно сжатое латентное пространство, позволяя трансформеру эффективно выполнять полное пространственно-временное самовнимание. LTX-Video поддерживает генерацию видео из текста и изображений, превосходя существующие модели по скорости и качеству.'}, 'en': {'title': 'Revolutionizing Video Generation with LTX-Video', 'desc': "LTX-Video is a novel transformer-based latent diffusion model designed for efficient video generation by integrating the roles of Video-VAE and denoising transformers. It achieves a high compression ratio of 1:192, allowing the model to operate in a compressed latent space while maintaining spatiotemporal self-attention for generating high-resolution videos. The model's VAE decoder performs both latent-to-pixel conversion and denoising, enabling the generation of fine details without the need for a separate upsampling module. With capabilities for text-to-video and image-to-video generation, LTX-Video produces videos faster than real-time, setting a new standard in the field."}, 'zh': {'title': 'LTX-Video：高效视频生成的新标准', 'desc': 'LTX-Video是一种基于变换器的潜在扩散模型，旨在通过整合视频生成中的Video-VAE和去噪变换器的功能来提高效率和质量。该模型的核心是一个高压缩比的Video-VAE，能够在压缩的潜在空间中高效执行时空自注意力，从而生成高分辨率且具有时间一致性的视频。为了克服高压缩带来的细节损失，VAE解码器同时负责潜在到像素的转换和最终的去噪步骤，直接在像素空间中生成清晰的结果。LTX-Video支持多种应用场景，包括文本到视频和图像到视频的生成，并且在Nvidia H100 GPU上以超实时速度生成视频，设立了视频生成的新基准。'}}}, {'id': 'https://huggingface.co/papers/2501.01264', 'title': 'ProgCo: Program Helps Self-Correction of Large Language Models', 'url': 'https://huggingface.co/papers/2501.01264', 'abstract': 'Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.', 'score': 22, 'issue_id': 1473, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'bda3f96e83319526', 'authors': ['Xiaoshuai Song', 'Yanan Wu', 'Weixun Wang', 'Jiaheng Liu', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Taobao & Tmall Group of Alibaba'], 'pdf_title_img': 'assets/pdf/title_img/2501.01264.jpg', 'data': {'categories': ['#training', '#math', '#reasoning', '#interpretability', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'ProgCo: Самокоррекция языковых моделей через программно-управляемую верификацию и уточнение', 'desc': 'Эта статья представляет новый подход к самокоррекции больших языковых моделей (LLM) под названием Program-driven Self-Correction (ProgCo). Метод включает в себя программно-управляемую верификацию (ProgVe), которая использует самогенерируемые и самовыполняющиеся псевдопрограммы для сложной логики проверки. Затем программно-управляемое уточнение (ProgRe) проводит двойную рефлексию и улучшение как ответов, так и программ верификации. Эксперименты показали, что ProgCo эффективен в самокоррекции и может дополнительно улучшить производительность при комбинировании с реальными программными инструментами.'}, 'en': {'title': 'Empowering LLMs with Program-Driven Self-Correction', 'desc': 'This paper introduces Program-driven Self-Correction (ProgCo) to improve the self-verification and self-refinement capabilities of large language models (LLMs). It addresses the common issue where LLMs struggle to provide accurate feedback, which can lead to incorrect refinements, particularly in complex reasoning tasks. ProgCo utilizes program-driven verification (ProgVe) to create self-executing verification pseudo-programs that enhance the verification process. Additionally, program-driven refinement (ProgRe) allows the model to reflect on and refine both its responses and the verification programs, leading to more reliable self-correction outcomes.'}, 'zh': {'title': '基于程序的自我纠正：提升语言模型的自我验证能力', 'desc': '自我纠正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs往往无法有效自我验证并生成正确的反馈，这会进一步误导其完善过程，尤其是在复杂推理任务中。本文提出了基于程序的自我纠正（ProgCo），通过自生成、自执行的验证伪程序实现复杂的验证逻辑和广泛的验证。实验结果表明，ProgCo在三个指令遵循和数学基准测试中实现了有效的自我纠正，并且与真实程序工具结合时可以进一步提升性能。'}}}, {'id': 'https://huggingface.co/papers/2501.00316', 'title': 'MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models', 'url': 'https://huggingface.co/papers/2501.00316', 'abstract': "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.", 'score': 20, 'issue_id': 1477, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': 'a4e45c6bd9d30ff4', 'authors': ['Mahir Labib Dihan', 'Md Tanvir Hassan', 'Md Tanvir Parvez', 'Md Hasebul Hasan', 'Md Almash Alam', 'Muhammad Aamir Cheema', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], 'affiliations': ['Bangladesh Computer Council (BCC)', 'Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)', 'Monash University', 'Qatar Computing Research Institute (QCRI)', 'Statistics, Islamic University Bangladesh'], 'pdf_title_img': 'assets/pdf/title_img/2501.00316.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#survey'], 'emoji': '🗺️', 'ru': {'title': 'MapEval: Новый рубеж в геопространственном ИИ', 'desc': 'Статья представляет MapEval - новый бенчмарк для оценки способностей моделей искусственного интеллекта в области пространственных рассуждений и работы с картами. MapEval включает 700 вопросов с множественным выбором, охватывающих 180 городов и 54 страны, и оценивает навыки моделей в понимании пространственных отношений, инфографики карт, планирования путешествий и навигации. Авторы провели оценку 28 ведущих фундаментальных моделей, выявив значительные различия в производительности, при этом все модели все еще отстают от человеческого уровня более чем на 20%. Результаты исследования подчеркивают важность MapEval для развития моделей с более сильным геопространственным пониманием.'}, 'en': {'title': "Enhancing AI's Geo-Spatial Reasoning with MapEval", 'desc': 'This paper introduces MapEval, a benchmark designed to evaluate the performance of foundation models in map-based reasoning tasks. It focuses on assessing how well these models can handle complex geo-spatial queries, which are essential for navigation and resource discovery. The benchmark includes various task types that require models to process diverse information, such as travel distances and user reviews, and perform compositional reasoning. The evaluation reveals that while some models perform competitively, they still lag behind human capabilities, indicating a need for further advancements in geo-spatial understanding within AI systems.'}, 'zh': {'title': '提升地图推理能力的基准评估', 'desc': '最近基础模型的进展提升了人工智能系统在自主工具使用和推理方面的能力。然而，它们在基于位置或地图的推理能力上尚未得到系统研究，这对于优化导航、资源发现和物流管理至关重要。为了解决这个问题，我们引入了MapEval，一个旨在评估复杂地图用户查询的基准，涉及地理空间推理。MapEval包含700个关于180个城市和54个国家的独特多项选择题，评估基础模型在处理空间关系、地图信息、旅行规划和导航挑战方面的能力。'}}}, {'id': 'https://huggingface.co/papers/2501.01149', 'title': 'A3: Android Agent Arena for Mobile GUI Agents', 'url': 'https://huggingface.co/papers/2501.01149', 'abstract': 'AI agents have become increasingly prevalent in recent years, driven by significant advancements in the field of large language models (LLMs). Mobile GUI agents, a subset of AI agents, are designed to autonomously perform tasks on mobile devices. While numerous studies have introduced agents, datasets, and benchmarks to advance mobile GUI agent research, many existing datasets focus on static frame evaluations and fail to provide a comprehensive platform for assessing performance on real-world, in-the-wild tasks. To address this gap, we present Android Agent Arena (A3), a novel evaluation platform. Unlike existing in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as real-time online information retrieval and operational instructions; (2) a larger, more flexible action space, enabling compatibility with agents trained on any dataset; and (3) automated business-level LLM-based evaluation process. A3 includes 21 widely used general third-party apps and 201 tasks representative of common user scenarios, providing a robust foundation for evaluating mobile GUI agents in real-world situations and a new autonomous evaluation process for less human labor and coding expertise. The project is available at https://yuxiangchai.github.io/Android-Agent-Arena/.', 'score': 20, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '050f155aa526c100', 'authors': ['Yuxiang Chai', 'Hanhao Li', 'Jiayu Zhang', 'Liang Liu', 'Guozhi Wang', 'Shuai Ren', 'Siyuan Huang', 'Hongsheng Li'], 'affiliations': ['EE department @ CUHK', 'MMLab @ CUHK'], 'pdf_title_img': 'assets/pdf/title_img/2501.01149.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents'], 'emoji': '🤖', 'ru': {'title': 'A3: Арена для тестирования мобильных AI-агентов в реальном мире', 'desc': 'Статья представляет новую платформу для оценки мобильных GUI-агентов под названием Android Agent Arena (A3). A3 предлагает реалистичные задачи, широкое пространство действий и автоматизированную оценку на основе больших языковых моделей. Платформа включает 21 популярное стороннее приложение и 201 задачу, отражающую типичные пользовательские сценарии. A3 позволяет оценивать производительность агентов в реальных условиях, что отличает её от существующих статических наборов данных.'}, 'en': {'title': 'Revolutionizing Mobile GUI Agent Evaluation with A3', 'desc': 'This paper introduces the Android Agent Arena (A3), a new evaluation platform for mobile GUI agents that addresses limitations in existing datasets. A3 focuses on real-world tasks, providing a larger action space that accommodates agents trained on various datasets. It features 21 popular third-party apps and 201 tasks that reflect common user scenarios, enhancing the assessment of agent performance. Additionally, A3 incorporates an automated evaluation process using large language models, reducing the need for extensive human involvement and coding skills.'}, 'zh': {'title': 'Android Agent Arena：移动GUI代理的新评估平台', 'desc': '近年来，人工智能代理的应用越来越广泛，尤其是在大型语言模型（LLMs）领域的进步推动下。移动图形用户界面（GUI）代理是人工智能代理的一种，旨在自主执行移动设备上的任务。现有的研究虽然提出了许多代理、数据集和基准，但大多数数据集仅关注静态框架评估，无法全面评估真实世界中的任务表现。为了解决这一问题，我们提出了Android Agent Arena（A3），这是一个新颖的评估平台，提供了实际的任务和更灵活的操作空间，支持基于LLM的自动化评估过程。'}}}, {'id': 'https://huggingface.co/papers/2501.00192', 'title': 'MLLM-as-a-Judge for Image Safety without Human Labeling', 'url': 'https://huggingface.co/papers/2501.00192', 'abstract': 'Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.', 'score': 20, 'issue_id': 1474, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': '2a62bcbb87c1b7a5', 'authors': ['Zhenting Wang', 'Shuming Hu', 'Shiyu Zhao', 'Xiaowen Lin', 'Felix Juefei-Xu', 'Zhuowei Li', 'Ligong Han', 'Harihar Subramanyam', 'Li Chen', 'Jianfa Chen', 'Nan Jiang', 'Lingjuan Lyu', 'Shiqing Ma', 'Dimitris N. Metaxas', 'Ankit Jain'], 'affiliations': ['GenAI @ Meta', 'Rutgers University', 'UMass Amherst', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00192.jpg', 'data': {'categories': ['#reasoning', '#training', '#ethics', '#cv', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Интеллектуальная защита: Zero-shot оценка безопасности изображений с помощью MLLM', 'desc': 'Статья представляет метод определения безопасности изображений с использованием мультимодальных больших языковых моделей (MLLM) в режиме zero-shot. Авторы предлагают подход, включающий объективизацию правил безопасности, оценку релевантности между правилами и изображениями, и быстрое принятие решений на основе дебиасированных вероятностей токенов. Метод также включает каскадные цепочки рассуждений для более глубокого анализа при необходимости. Эксперименты показывают высокую эффективность предложенного метода для задач оценки безопасности изображений без предварительного обучения.'}, 'en': {'title': 'Zero-Shot Image Safety Detection with MLLMs', 'desc': 'This paper addresses the challenge of identifying unsafe images in the context of AI-generated content using Multimodal Large Language Models (MLLMs). The authors propose a novel approach that allows for zero-shot detection of harmful images by utilizing predefined safety rules without the need for extensive human labeling. They highlight the limitations of traditional methods, such as the subjectivity of safety rules and the biases present in models. The proposed method enhances safety judgment by objectifying rules, assessing their relevance to images, and employing a reasoning process that simplifies complex safety guidelines.'}, 'zh': {'title': '利用MLLMs实现零样本图像安全判断', 'desc': '随着在线平台视觉媒体的兴起，图像内容安全成为一个重要挑战。许多图像生成模型能够产生有害内容，因此识别不安全图像变得至关重要。我们提出了一种基于预训练多模态大语言模型（MLLMs）的方法，通过查询这些模型来检测不安全图像，而无需依赖人工标注。实验结果表明，我们的方法在零样本图像安全判断任务中非常有效。'}}}, {'id': 'https://huggingface.co/papers/2501.01426', 'title': 'Unifying Specialized Visual Encoders for Video Language Models', 'url': 'https://huggingface.co/papers/2501.01426', 'abstract': 'The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.', 'score': 19, 'issue_id': 1488, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'c868a7ebcbafa704', 'authors': ['Jihoon Chung', 'Tyler Zhu', 'Max Gonzalez Saez-Diez', 'Juan Carlos Niebles', 'Honglu Zhou', 'Olga Russakovsky'], 'affiliations': ['Princeton University', 'Salesforce Research'], 'pdf_title_img': 'assets/pdf/title_img/2501.01426.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#video', '#benchmark', '#multimodal', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'MERV: Многоэнкодерное представление видео для улучшенного машинного понимания', 'desc': 'Статья представляет MERV - новый метод для улучшения понимания видео с помощью больших языковых моделей. MERV использует несколько замороженных визуальных энкодеров для создания единого представления видео, что позволяет охватить больший объем визуальной информации. Этот подход превосходит предыдущие методы в точности на стандартных тестах понимания видео. MERV вводит минимальное количество дополнительных параметров и обучается быстрее, чем эквивалентные методы с одним энкодером.'}, 'en': {'title': 'Unlocking Video Understanding with Multi-Encoder Magic!', 'desc': 'This paper introduces MERV, a method that enhances Video Large Language Models (VideoLLMs) by using multiple visual encoders instead of just one. By combining the outputs of these encoders, MERV creates a richer representation of videos, which helps the model understand complex video content better. The approach allows for improved performance on various video understanding tasks, achieving higher accuracy than previous models. Additionally, MERV is efficient, requiring fewer parameters and training time while effectively leveraging the strengths of each encoder.'}, 'zh': {'title': '多编码器提升视频理解能力', 'desc': '本文介绍了一种名为MERV（多编码器视频表示）的方法，旨在提升视频理解的能力。MERV通过使用多个冻结的视觉编码器，创建视频的统一表示，从而为视频大型语言模型（VideoLLM）提供更全面的视觉知识。通过时空对齐每个编码器的特征，MERV能够更好地处理开放式和多选的视频理解问题，且在准确性上超越了之前的最佳模型。该方法不仅提高了性能，还在参数和训练速度上优于单编码器方法，展示了多视觉编码器在视频理解中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.01054', 'title': 'Dynamic Scaling of Unit Tests for Code Reward Modeling', 'url': 'https://huggingface.co/papers/2501.01054', 'abstract': 'Current large language models (LLMs) often struggle to produce accurate responses on the first attempt for complex reasoning tasks like code generation. Prior research tackles this challenge by generating multiple candidate solutions and validating them with LLM-generated unit tests. The execution results of unit tests serve as reward signals to identify correct solutions. As LLMs always confidently make mistakes, these unit tests are not reliable, thereby diminishing the quality of reward signals. Motivated by the observation that scaling the number of solutions improves LLM performance, we explore the impact of scaling unit tests to enhance reward signal quality. Our pioneer experiment reveals a positive correlation between the number of unit tests and reward signal quality, with greater benefits observed in more challenging problems. Based on these insights, we propose CodeRM-8B, a lightweight yet effective unit test generator that enables efficient and high-quality unit test scaling. Additionally, we implement a dynamic scaling mechanism that adapts the number of unit tests based on problem difficulty, further improving efficiency. Experimental results show that our approach significantly improves performance across various models on three benchmarks (e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on HumanEval Plus).', 'score': 15, 'issue_id': 1474, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '33b9590f2acb0e48', 'authors': ['Zeyao Ma', 'Xiaokang Zhang', 'Jing Zhang', 'Jifan Yu', 'Sijia Luo', 'Jie Tang'], 'affiliations': ['Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, China', 'School of Information, Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01054.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#small_models', '#rlhf', '#optimization'], 'emoji': '🧪', 'ru': {'title': 'Масштабирование юнит-тестов для повышения точности LLM в сложных задачах', 'desc': 'Эта статья посвящена улучшению точности больших языковых моделей (LLM) в задачах сложного мышления, таких как генерация кода. Авторы предлагают метод масштабирования юнит-тестов для повышения качества сигналов вознаграждения при оценке решений. Они разработали легковесный генератор юнит-тестов CodeRM-8B и механизм динамического масштабирования, адаптирующийся к сложности задачи. Эксперименты показали значительное улучшение производительности различных моделей на нескольких тестовых наборах.'}, 'en': {'title': 'Enhancing LLM Performance through Scaled Unit Testing', 'desc': 'This paper addresses the limitations of large language models (LLMs) in generating accurate responses for complex tasks like code generation. It highlights the issue of unreliable reward signals from LLM-generated unit tests, which can lead to incorrect solutions. The authors propose a novel approach, CodeRM-8B, which generates a larger number of unit tests to improve the quality of these reward signals. Their experiments demonstrate that scaling unit tests enhances LLM performance, particularly for more challenging problems, leading to significant improvements across various models.'}, 'zh': {'title': '提升单元测试质量，增强模型性能', 'desc': '当前的大型语言模型（LLMs）在复杂推理任务（如代码生成）中，往往难以在第一次尝试时产生准确的响应。以往的研究通过生成多个候选解决方案并使用LLM生成的单元测试进行验证来应对这一挑战。单元测试的执行结果作为奖励信号，用于识别正确的解决方案。然而，由于LLMs常常自信地犯错，这些单元测试的可靠性不足，从而降低了奖励信号的质量。我们提出了CodeRM-8B，一个轻量级且有效的单元测试生成器，能够高效地扩展单元测试，并根据问题的难度动态调整单元测试的数量，从而进一步提高效率。'}}}, {'id': 'https://huggingface.co/papers/2501.01320', 'title': 'SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration', 'url': 'https://huggingface.co/papers/2501.01320', 'abstract': "Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.", 'score': 8, 'issue_id': 1479, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': 'fa277e5baed864a4', 'authors': ['Jianyi Wang', 'Zhijie Lin', 'Meng Wei', 'Yang Zhao', 'Ceyuan Yang', 'Chen Change Loy', 'Lu Jiang'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01320.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#long_context', '#video', '#training', '#diffusion', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'SeedVR: Восстановление видео нового поколения с помощью диффузионных трансформеров', 'desc': 'SeedVR - это диффузионный трансформер для восстановления видео в реальных условиях. Он использует сдвинутое оконное внимание для эффективной обработки длинных видеопоследовательностей. SeedVR поддерживает окна переменного размера на границах пространственных и временных измерений, преодолевая ограничения традиционного оконного внимания. Благодаря современным практикам, таким как каузальный видеоавтоэнкодер и прогрессивное обучение, SeedVR достигает высоких результатов на синтетических и реальных тестовых наборах.'}, 'en': {'title': 'SeedVR: Revolutionizing Video Restoration with Diffusion Transformers', 'desc': 'This paper introduces SeedVR, a novel diffusion transformer aimed at improving video restoration by effectively managing long sequences and varying resolutions. It utilizes shifted window attention to enhance the restoration process, allowing for better handling of temporal consistency and fidelity in videos. SeedVR incorporates advanced techniques such as causal video autoencoders and mixed training strategies to boost its performance on both synthetic and real-world datasets. The results show that SeedVR outperforms existing video restoration methods, making it a significant advancement in the field.'}, 'zh': {'title': 'SeedVR：高效的视频修复新方法', 'desc': '视频修复面临着在恢复未知退化的同时保持细节一致性的挑战。尽管基于扩散的修复方法有所进展，但它们在生成能力和采样效率上仍存在局限性。本文提出了SeedVR，这是一种专为处理任意长度和分辨率的真实视频修复而设计的扩散变换器。SeedVR通过移动窗口注意力机制，有效地处理长视频序列，并在空间和时间维度的边界附近支持可变大小的窗口，克服了传统窗口注意力的分辨率限制。'}}}, {'id': 'https://huggingface.co/papers/2412.21015', 'title': 'MapQaTor: A System for Efficient Annotation of Map Query Datasets', 'url': 'https://huggingface.co/papers/2412.21015', 'abstract': 'Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.', 'score': 8, 'issue_id': 1477, 'pub_date': '2025-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '0d1081756b5bc4f7', 'authors': ['Mahir Labib Dihan', 'Mohammed Eunus Ali', 'Md Rizwan Parvez'], 'affiliations': ['Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)', 'Qatar Computing Research Institute (QCRI)'], 'pdf_title_img': 'assets/pdf/title_img/2412.21015.jpg', 'data': {'categories': ['#dataset', '#science', '#reasoning', '#data', '#benchmark'], 'emoji': '🗺️', 'ru': {'title': 'MapQaTor: Революция в создании геопространственных данных для ИИ', 'desc': 'MapQaTor - это веб-приложение, которое упрощает создание воспроизводимых наборов данных для вопросно-ответных систем на основе карт. Оно интегрируется с любым картографическим API и позволяет собирать и визуализировать данные из различных источников. MapQaTor кэширует ответы API, обеспечивая согласованность данных, и централизует процессы сбора, аннотации и визуализации. Приложение ускоряет процесс аннотации в 30 раз по сравнению с ручными методами, что делает его полезным инструментом для развития геопространственных ресурсов и оценки возможностей больших языковых моделей в области геопространственных рассуждений.'}, 'en': {'title': 'Streamlining Geospatial QA with MapQaTor', 'desc': 'This paper presents MapQaTor, a web application designed to facilitate the creation of geospatial question answering (QA) datasets using map services. It leverages recent advancements in Large Language Models (LLMs) to improve the handling of natural language queries related to locations. The platform features a plug-and-play architecture that integrates with various maps APIs, allowing users to efficiently gather, annotate, and visualize geospatial data. By caching API responses, MapQaTor ensures consistent and reliable data, significantly speeding up the annotation process and enhancing the evaluation of LLM-based geospatial reasoning capabilities.'}, 'zh': {'title': 'MapQaTor：提升地图问答数据集创建效率的利器', 'desc': '本文介绍了MapQaTor，一个用于创建地图问答数据集的网络应用程序。它利用大型语言模型的优势，简化了从地图服务生成可重复和可追溯的数据集的过程。MapQaTor支持与任何地图API的无缝集成，并通过缓存API响应来确保数据的一致性。该平台显著提高了数据标注的效率，展示了在地理空间推理方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.01407', 'title': 'Nested Attention: Semantic-aware Attention Values for Concept Personalization', 'url': 'https://huggingface.co/papers/2501.01407', 'abstract': "Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.", 'score': 7, 'issue_id': 1487, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '537e7bcc16fb17f5', 'authors': ['Or Patashnik', 'Rinon Gal', 'Daniil Ostashev', 'Sergey Tulyakov', 'Kfir Aberman', 'Daniel Cohen-Or'], 'affiliations': ['Snap Research', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2501.01407.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Nested Attention: новый подход к персонализации генерации изображений', 'desc': "Статья представляет новый метод под названием 'Nested Attention' для персонализации моделей text-to-image. Этот механизм внедряет богатое и выразительное представление изображения в существующие слои кросс-внимания модели. Ключевая идея заключается в генерации зависимых от запроса значений субъекта, полученных из вложенных слоев внимания. Метод позволяет достичь высокого сохранения идентичности при соблюдении входных текстовых подсказок."}, 'en': {'title': 'Nested Attention: Balancing Identity and Text Alignment in Image Generation', 'desc': 'This paper presents a new method called Nested Attention for personalizing text-to-image models. The method addresses the challenge of balancing identity preservation of subjects with the alignment to text prompts. By using query-dependent subject values from nested attention layers, the model can effectively select relevant features for each part of the generated image. This approach not only maintains high identity fidelity but also allows for the integration of multiple personalized subjects from different domains into a single image.'}, 'zh': {'title': '嵌套注意力：个性化图像生成的新方法', 'desc': '本文介绍了一种新的机制，称为嵌套注意力，用于个性化文本到图像模型。该方法通过在模型的交叉注意力层中注入丰富的图像表示，解决了身份保留与文本提示对齐之间的平衡问题。嵌套注意力层能够为生成图像的每个区域选择相关的主题特征，从而实现高效的个性化。我们的研究表明，这种方法可以在多个领域进行训练，并允许在单个图像中结合来自不同领域的多个个性化主题。'}}}, {'id': 'https://huggingface.co/papers/2501.00658', 'title': 'Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing', 'url': 'https://huggingface.co/papers/2501.00658', 'abstract': "Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.", 'score': 6, 'issue_id': 1476, 'pub_date': '2025-12-31', 'pub_date_card': {'ru': '31 декабря', 'en': 'December 31', 'zh': '12月31日'}, 'hash': '253304ea64defbe0', 'authors': ['Peihao Wang', 'Ruisi Cai', 'Yuehao Wang', 'Jiajun Zhu', 'Pragya Srivastava', 'Zhangyang Wang', 'Pan Li'], 'affiliations': ['Georgia Tech', 'Google DeepMind', 'University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00658.jpg', 'data': {'categories': ['#training', '#open_source', '#long_context', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений SSM: баланс между недавностью и сглаживанием', 'desc': 'Структурированные модели пространства состояний (SSM) рассматриваются как альтернатива трансформерам в обработке длинных последовательностей. Исследование показало, что SSM имеют существенное ограничение в виде сильного смещения к недавним данным, что затрудняет запоминание отдаленной информации. Увеличение глубины SSM улучшает обработку длинных контекстов, но приводит к проблеме чрезмерного сглаживания. Авторы предлагают метод поляризации каналов матриц перехода состояний для решения этих проблем, что улучшает точность ассоциативного извлечения дальних токенов.'}, 'en': {'title': 'Balancing Recency and Over-Smoothing in SSMs', 'desc': "This paper discusses Structured State Space Models (SSMs) as alternatives to transformers, highlighting their limitations due to strong recency bias. This bias affects the models' ability to remember distant information and creates robustness issues. The authors propose a solution by polarizing the state transition matrices, which helps mitigate both recency bias and over-smoothing that occurs with deeper architectures. Their experiments show that this new approach improves the accuracy of recalling long-range tokens, allowing SSMs to effectively utilize deeper structures."}, 'zh': {'title': '解决近期偏见与过平滑的双重挑战', 'desc': '结构状态空间模型（SSMs）作为变换器的替代方案，虽然在捕捉长序列依赖性方面表现出色，但存在强烈的近期偏见限制。我们的实证研究表明，这种偏见影响了模型对远程信息的回忆能力，并引入了鲁棒性问题。通过扩展实验，我们发现SSMs的深层结构可以促进长上下文的学习，但理论分析显示，随着深度增加，模型会出现过平滑的趋势，使得标记表示变得难以区分。我们提出的极化技术通过将状态转移矩阵的两个通道设置为零和一，解决了近期偏见和过平滑的问题，显著提高了长距离标记的关联回忆准确性。'}}}, {'id': 'https://huggingface.co/papers/2501.01245', 'title': 'SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization', 'url': 'https://huggingface.co/papers/2501.01245', 'abstract': 'Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model\'s predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.', 'score': 5, 'issue_id': 1475, 'pub_date': '2025-01-02', 'pub_date_card': {'ru': '2 января', 'en': 'January 2', 'zh': '1月2日'}, 'hash': '30d94590a5c78569', 'authors': ['Yongle Huang', 'Haodong Chen', 'Zhenbang Xu', 'Zihan Jia', 'Haozhou Sun', 'Dian Shao'], 'affiliations': ['School of Automation, Northwestern Polytechnical University, Xian, China', 'School of Computer Science, Northwestern Polytechnical University, Xian, China', 'School of Software, Northwestern Polytechnical University, Xian, China', 'Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.01245.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#multimodal', '#optimization', '#training'], 'emoji': '🤸', 'ru': {'title': 'SeFAR: Прорыв в распознавании детализированных действий с помощью полу-контролируемого обучения', 'desc': 'Статья представляет новый подход к задаче распознавания детализированных действий (Fine-grained Action Recognition, FAR) с использованием полу-контролируемого обучения. Авторы предлагают фреймворк SeFAR, который включает в себя двухуровневые временные элементы для более эффективного представления действий и новую стратегию аугментации данных. SeFAR также использует адаптивную регуляцию для стабилизации процесса обучения при работе с неопределенностью в предсказаниях модели-учителя. Эксперименты показывают, что SeFAR достигает лучших результатов на нескольких наборах данных FAR и классических наборах данных для распознавания действий.'}, 'en': {'title': 'SeFAR: Elevating Fine-grained Action Recognition with Semi-supervised Learning', 'desc': "This paper focuses on improving Fine-grained Action Recognition (FAR), which identifies specific actions in short time frames. The authors introduce a semi-supervised learning framework called SeFAR, which uses innovative techniques to enhance the learning process despite the challenges of limited labeled data. They develop Dual-level temporal elements for better visual representation and implement a strong augmentation strategy within a Teacher-Student learning setup. The results demonstrate that SeFAR achieves top performance on FAR datasets and enhances multimodal models' understanding of detailed actions."}, 'zh': {'title': '细粒度动作识别的新突破', 'desc': '人类动作理解对多模态系统的发展至关重要。本文提出了一种新的框架SeFAR，专注于细粒度动作识别（FAR），旨在处理短时间内的详细语义标签。我们采用半监督学习（SSL）来减少对大量标注数据的需求，并通过构建双层时间元素和新的强增强策略来提高模型的表现。实验结果表明，SeFAR在多个数据集上达到了最先进的性能，证明了我们设计的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.00910', 'title': 'Population Aware Diffusion for Time Series Generation', 'url': 'https://huggingface.co/papers/2501.00910', 'abstract': 'Diffusion models have shown promising ability in generating high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to preserving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional dependencies (e.g., cross-correlation, CC) between different dimensions. For instance, when generating house energy consumption TS data, the value distributions of the outside temperature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is often overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the original data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better preserves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incorporating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better captures the TS data structure. Empirical results in major benchmark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.', 'score': 4, 'issue_id': 1486, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': 'cd3f9282d55e15f2', 'authors': ['Yang Li', 'Han Meng', 'Zhenyu Bi', 'Ingolv T. Urnes', 'Haipeng Chen'], 'affiliations': ['Generated Health', 'Virginia Tech', 'William & Mary'], 'pdf_title_img': 'assets/pdf/title_img/2501.00910.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#data', '#training', '#architecture', '#diffusion'], 'emoji': '📊', 'ru': {'title': 'Генерация временных рядов с сохранением свойств популяции', 'desc': 'Статья представляет новую модель генерации временных рядов под названием PaD-TS (Population-aware Diffusion for Time Series). Модель нацелена на сохранение свойств на уровне популяции, таких как распределения значений и функциональные зависимости между измерениями. PaD-TS использует новый метод обучения, явно включающий сохранение свойств временных рядов на уровне популяции, и новую архитектуру модели с двухканальным энкодером. Эмпирические результаты показывают значительное улучшение в сохранении распределения кросс-корреляций при сравнимой аутентичности на индивидуальном уровне.'}, 'en': {'title': 'Preserving Population Insights in Time Series Generation', 'desc': 'This paper introduces a new model called Population-aware Diffusion for Time Series (PaD-TS) that focuses on generating time series data while preserving important population-level properties. Unlike previous models that mainly ensure individual data authenticity, PaD-TS emphasizes maintaining the overall statistical characteristics of the dataset, such as value distributions and cross-correlations between different dimensions. The model employs a novel training method and a dual-channel encoder architecture to effectively capture the structure of time series data. Experimental results demonstrate that PaD-TS significantly reduces distribution shifts in generated data while achieving comparable performance in individual-level authenticity to existing state-of-the-art models.'}, 'zh': {'title': '保留人口级特性，提升时间序列生成质量', 'desc': '扩散模型在生成高质量时间序列数据方面表现出色。然而，现有研究主要关注个体数据的真实性，而忽视了整个数据集的人口级特性。我们提出了一种新的时间序列生成模型PaD-TS，旨在更好地保留这些人口级特性，包括值分布和不同维度之间的交叉相关性。实验结果表明，PaD-TS在保持个体级真实性的同时，显著改善了真实数据与合成数据之间的分布差异。'}}}, {'id': 'https://huggingface.co/papers/2501.00712', 'title': 'Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding', 'url': 'https://huggingface.co/papers/2501.00712', 'abstract': 'Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose conTextualized equivariAnt Position Embedding (TAPE), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques.', 'score': 4, 'issue_id': 1485, 'pub_date': '2025-01-01', 'pub_date_card': {'ru': '1 января', 'en': 'January 1', 'zh': '1月1日'}, 'hash': 'e5119d0e83ce2af2', 'authors': ['Jiajun Zhu', 'Peihao Wang', 'Ruisi Cai', 'Jason D. Lee', 'Pan Li', 'Zhangyang Wang'], 'affiliations': ['Georgia Tech', 'Princeton University', 'University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2501.00712.jpg', 'data': {'categories': ['#long_context', '#optimization', '#training', '#architecture', '#reasoning'], 'emoji': '🔀', 'ru': {'title': 'Динамические позиционные эмбеддинги для улучшения работы трансформеров', 'desc': 'Авторы предлагают новый метод позиционного кодирования для трансформеров под названием TAPE. Этот подход учитывает контекст последовательности и создает динамические позиционные эмбеддинги, адаптированные к конкретным задачам. TAPE обеспечивает стабильность кодирования благодаря свойствам перестановочной и ортогональной эквивариантности. Метод легко интегрируется в предобученные модели и показывает превосходные результаты в задачах языкового моделирования, арифметических рассуждений и поиска в длинных контекстах.'}, 'en': {'title': 'Enhancing Transformers with Context-Aware Positional Embeddings', 'desc': "This paper introduces a new method called conTextualized equivariAnt Position Embedding (TAPE) to improve how transformers use positional information. Traditional positional encodings often restrict the model's ability to understand long-range relationships in data. TAPE enhances these encodings by making them dynamic and context-aware, allowing them to adapt to different sequences and tasks. The method shows better performance in various applications, such as language modeling and reasoning, while being easy to integrate into existing transformer models."}, 'zh': {'title': '提升变换器模型的位置信息处理能力', 'desc': '本文提出了一种新的位置编码方法，称为TAPE（conTextualized equivariAnt Position Embedding），旨在提高变换器模型的预测能力。传统的位置编码方法往往限制了模型对长距离依赖关系的建模能力，而TAPE通过引入动态的、上下文感知的位置编码来克服这一问题。该方法确保了位置编码在更新过程中的稳定性，从而提高了模型的鲁棒性和适应性。实验结果表明，TAPE在语言建模、算术推理和长上下文检索任务中表现优于现有的位置编码技术。'}}}, {'id': 'https://huggingface.co/papers/2412.19723', 'title': 'OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis', 'url': 'https://huggingface.co/papers/2412.19723', 'abstract': "Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/{OS-Genesis Homepage}.", 'score': 50, 'issue_id': 1455, 'pub_date': '2025-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': 'b331198d09aa8650', 'authors': ['Qiushi Sun', 'Kanzhi Cheng', 'Zichen Ding', 'Chuanyang Jin', 'Yian Wang', 'Fangzhi Xu', 'Zhenyu Wu', 'Chengyou Jia', 'Liheng Chen', 'Zhoumianze Liu', 'Ben Kao', 'Guohao Li', 'Junxian He', 'Yu Qiao', 'Zhiyong Wu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Johns Hopkins University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2412.19723.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#dataset', '#optimization', '#training', '#data', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'Революция в обучении ИИ-агентов: от заданий к исследованию', 'desc': 'Статья представляет OS-Genesis - новый метод синтеза данных для обучения ИИ-агентов взаимодействию с графическим интерфейсом. Вместо предопределенных задач, агенты сначала исследуют среду и выполняют пошаговые действия, а затем ретроспективно формируют качественные траектории. Используется модель вознаграждения для обеспечения качества сгенерированных траекторий. Результаты показывают значительное улучшение производительности агентов на сложных онлайн-бенчмарках по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing GUI Agent Training with OS-Genesis', 'desc': 'This paper introduces OS-Genesis, a new method for generating high-quality trajectory data for training GUI agents using Vision-Language Models (VLMs). Unlike traditional methods that rely on human supervision or predefined tasks, OS-Genesis allows agents to first interact with their environment and then derive tasks retrospectively. This approach enhances data diversity and quality by enabling agents to explore and learn from real-world interactions. The results show that GUI agents trained with OS-Genesis perform significantly better on challenging benchmarks, demonstrating the effectiveness of this novel data synthesis pipeline.'}, 'zh': {'title': 'OS-Genesis：提升GUI代理性能的新方法', 'desc': '本论文提出了一种名为OS-Genesis的新型图形用户界面（GUI）数据合成管道，旨在解决高质量轨迹数据收集的瓶颈。传统方法依赖于人类监督或合成数据生成，往往资源消耗大且数据质量难以保证。OS-Genesis通过让代理先感知环境并进行逐步交互，随后回溯生成高质量任务，从而实现轨迹级探索。实验结果表明，使用OS-Genesis训练的GUI代理在复杂的在线基准测试中表现显著提升，且其数据质量和多样性优于现有合成方法。'}}}, {'id': 'https://huggingface.co/papers/2412.19638', 'title': 'Xmodel-2 Technical Report', 'url': 'https://huggingface.co/papers/2412.19638', 'abstract': 'Xmodel-2 is a 1.2-billion-parameter large language model designed specifically for reasoning tasks. Its architecture enables different model scales to share a unified set of hyperparameters, allowing for extensive experimentation on smaller models and seamless transfer of optimal configurations to larger models. To maximize training efficiency and stability, Xmodel-2 employs the WSD learning rate scheduler from MiniCPM. Pretrained on 1.5 trillion tokens from diverse sources, Xmodel-2 achieves state-of-the-art performance in complex reasoning and agent-based tasks, while maintaining low training costs. These results highlight the potential of efficient model design and training strategies in advancing reasoning capabilities. Model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/Xmodel-2', 'score': 11, 'issue_id': 1453, 'pub_date': '2025-12-27', 'pub_date_card': {'ru': '27 декабря', 'en': 'December 27', 'zh': '12月27日'}, 'hash': '4707dc8ac5a87e66', 'authors': ['Wang Qun', 'Liu Yang', 'Lin Qingquan', 'Qu Zhijiu', 'Jiang Ling'], 'affiliations': ['AI Lab, Xiaodu Technology'], 'pdf_title_img': 'assets/pdf/title_img/2412.19638.jpg', 'data': {'categories': ['#optimization', '#training', '#small_models', '#reasoning', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с Xmodel-2: мощь в компактности', 'desc': 'Xmodel-2 - это языковая модель с 1,2 миллиардами параметров, специализирующаяся на задачах рассуждения. Её архитектура позволяет разным масштабам модели использовать единый набор гиперпараметров, что облегчает эксперименты и перенос оптимальных конфигураций. Модель использует планировщик скорости обучения WSD из MiniCPM для повышения эффективности и стабильности. Предобученная на 1,5 триллионах токенов, Xmodel-2 достигает передовых результатов в сложных задачах рассуждения, сохраняя низкие затраты на обучение.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Model Design', 'desc': 'Xmodel-2 is a large language model with 1.2 billion parameters, specifically built for reasoning tasks. It features a flexible architecture that allows different model sizes to use the same hyperparameters, facilitating experimentation and optimization across scales. The model utilizes the WSD learning rate scheduler to enhance training efficiency and stability. With pretraining on 1.5 trillion tokens, Xmodel-2 demonstrates superior performance in complex reasoning tasks while keeping training costs low, showcasing the benefits of efficient model design.'}, 'zh': {'title': '高效推理能力的模型设计与训练策略', 'desc': 'Xmodel-2 是一个拥有 12 亿参数的大型语言模型，专门设计用于推理任务。它的架构允许不同规模的模型共享统一的超参数，从而可以在较小的模型上进行广泛实验，并将最佳配置无缝转移到更大的模型上。为了最大化训练效率和稳定性，Xmodel-2 采用了 MiniCPM 的 WSD 学习率调度器。经过在 1.5 万亿个来自多样化来源的标记上进行预训练，Xmodel-2 在复杂推理和基于代理的任务中达到了最先进的性能，同时保持了较低的训练成本。'}}}, {'id': 'https://huggingface.co/papers/2412.20735', 'title': 'HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2412.20735', 'abstract': 'We introduce HunyuanProver, an language model finetuned from the Hunyuan 7B for interactive automatic theorem proving with LEAN4. To alleviate the data sparsity issue, we design a scalable framework to iterative synthesize data with low cost. Besides, guided tree search algorithms are designed to enable effective ``system 2 thinking`` of the prover. HunyuanProver achieves state-of-the-art (SOTA) performances on major benchmarks. Specifically, it achieves a pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA results. It proves 4 IMO statements (imo_1960_p2, imo_1962_p2}, imo_1964_p2 and imo_1983_p6) in miniF2F-test. To benefit the community, we will open-source a dataset of 30k synthesized instances, where each instance contains the original question in natural language, the converted statement by autoformalization, and the proof by HunyuanProver.', 'score': 3, 'issue_id': 1464, 'pub_date': '2025-12-30', 'pub_date_card': {'ru': '30 декабря', 'en': 'December 30', 'zh': '12月30日'}, 'hash': '18d70581e862bf86', 'authors': ['Yang Li', 'Dong Du', 'Linfeng Song', 'Chen Li', 'Weikang Wang', 'Tao Yang', 'Haitao Mi'], 'affiliations': ['Tencent', 'Tencent Hunyuan Teams'], 'pdf_title_img': 'assets/pdf/title_img/2412.20735.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data', '#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматическом доказательстве теорем с помощью ИИ', 'desc': "HunyuanProver - это языковая модель, настроенная для автоматического доказательства теорем с использованием LEAN4. Модель использует масштабируемую структуру для итеративного синтеза данных и алгоритмы направленного поиска по дереву для эффективного 'системного мышления'. HunyuanProver достигает лучших результатов на основных бенчмарках, включая 68.4% прохождения на miniF2F-test. Авторы планируют открыть доступ к набору данных из 30 тысяч синтезированных примеров для пользы сообщества."}, 'en': {'title': 'HunyuanProver: Advancing Theorem Proving with AI', 'desc': 'HunyuanProver is a language model specifically fine-tuned for interactive automatic theorem proving using LEAN4. To address the challenge of data sparsity, the authors developed a scalable framework that allows for the iterative synthesis of data at a low cost. They also implemented guided tree search algorithms to enhance the reasoning capabilities of the prover, enabling it to perform complex logical deductions. HunyuanProver has achieved state-of-the-art performance on key benchmarks, including a notable pass rate of 68.4% on the miniF2F-test, surpassing previous results and proving several significant mathematical statements.'}, 'zh': {'title': 'HunyuanProver：自动定理证明的新突破', 'desc': '本文介绍了HunyuanProver，这是一个基于Hunyuan 7B微调的语言模型，旨在与LEAN4进行交互式自动定理证明。为了缓解数据稀疏问题，我们设计了一个可扩展的框架，以低成本迭代合成数据。此外，我们还设计了引导树搜索算法，以实现证明者的有效“系统2思维”。HunyuanProver在主要基准测试中达到了最先进的性能，特别是在miniF2F-test中取得了68.4%的通过率，超越了当前的65.9%最先进结果。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (7)', '#agi (1)', '#alignment (4)', '#architecture (19)', '#audio (3)', '#benchmark (29)', '#cv (15)', '#data (10)', '#dataset (24)', '#diffusion (15)', '#ethics (1)', '#games (5)', '#graphs (2)', '#hallucinations', '#healthcare (2)', '#inference (4)', '#interpretability (3)', '#leakage', '#long_context (4)', '#low_resource (3)', '#machine_translation', '#math (6)', '#multilingual (2)', '#multimodal (21)', '#open_source (19)', '#optimization (27)', '#plp', '#rag (2)', '#reasoning (20)', '#rl (1)', '#rlhf (6)', '#robotics (1)', '#science (5)', '#security (1)', '#small_models (3)', '#story_generation (1)', '#survey (3)', '#synthetic (6)', '#training (33)', '#transfer_learning (6)', '#video (19)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-08 09:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-08 09:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-08 09:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    