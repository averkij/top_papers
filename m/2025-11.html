
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 412 papers. November 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ¾ÑĞ±Ñ€ÑŒ 2025</span> | <span id="title-articles-count">412 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-10.html">â¬…ï¸ <span id="prev-date">10.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-12.html">â¡ï¸ <span id="next-date">12.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ¾ÑĞ±Ñ€ÑŒ 2025', 'en': 'November 2025', 'zh': '11æœˆ2025å¹´'};
        let feedDateNext = {'ru': '12.2025', 'en': '12/2025', 'zh': '12æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '10.2025', 'en': '10/2025', 'zh': '10æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.27492', 'title': 'ThinkMorph: Emergent Properties in Multimodal Interleaved\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2510.27492', 'abstract': 'ThinkMorph, a unified model fine-tuned on interleaved reasoning traces, enhances multimodal reasoning by generating coherent text-image steps, achieving significant performance gains and demonstrating emergent capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary, rather than isomorphic, modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7% over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts.These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning.', 'score': 81, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '0ba7de079f5629e7', 'authors': ['Jiawei Gu', 'Yunzhuo Hao', 'Huichen Will Wang', 'Linjie Li', 'Michael Qizhe Shieh', 'Yejin Choi', 'Ranjay Krishna', 'Yu Cheng'], 'affiliations': ['National University of Singapore', 'Stanford University', 'The Chinese University of Hong Kong', 'University of Washington', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27492.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'ThinkMorph â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 24K Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ½Ğµ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ¼Ğ¾Ñ€Ñ„Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¿Ğ¸Ğ¸. ThinkMorph Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 34.7% Ğ²Ñ‹ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°.'}, 'en': {'title': 'ThinkMorph: Unifying Text and Image for Enhanced Multimodal Reasoning', 'desc': 'ThinkMorph is a unified model designed to improve multimodal reasoning by effectively combining text and image processing. It is fine-tuned on a large dataset of interleaved reasoning traces, allowing it to generate coherent reasoning steps that integrate both visual and verbal elements. The model shows significant performance improvements on vision-centric tasks and demonstrates the ability to generalize to new challenges, outperforming larger models. Additionally, ThinkMorph reveals emergent capabilities such as advanced visual manipulation and adaptive reasoning strategies, highlighting its potential in the field of multimodal AI.'}, 'zh': {'title': 'ThinkMorphï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'ThinkMorph æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´ï¼Œä¸“æ³¨äºäº¤é”™æ¨ç†è½¨è¿¹ï¼Œæå‡äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬-å›¾åƒæ¨ç†æ­¥éª¤ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†è§†è§‰å†…å®¹ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€é€»è¾‘çš„ä¸€è‡´æ€§ã€‚é€šè¿‡åœ¨ 24,000 ä¸ªé«˜è´¨é‡çš„äº¤é”™æ¨ç†è½¨è¿¹ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒThinkMorph åœ¨è§†è§‰ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®ƒä¸ä»…åœ¨å·²çŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½åœ¨æœªçŸ¥é¢†åŸŸçš„ä»»åŠ¡ä¸­è¶…è¶Šæ›´å¤§è§„æ¨¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå±•ç°å‡ºæ–°å…´çš„å¤šæ¨¡æ€æ™ºèƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25602', 'title': 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats', 'url': 'https://huggingface.co/papers/2510.25602', 'abstract': "A comprehensive comparison of low-precision floating-point and integer quantization in large language models reveals that fine-grained integer formats, especially MXINT8, offer superior accuracy and efficiency over floating-point formats in many cases.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.", 'score': 77, 'issue_id': 6765, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '154a163d9d96d4af', 'authors': ['Mengzhao Chen', 'Meng Wu', 'Hui Jin', 'Zhihang Yuan', 'Jing Liu', 'Chaoyi Zhang', 'Yunshui Li', 'Jie Huang', 'Jin Ma', 'Zeyue Xue', 'Zhiheng Liu', 'Xingyan Bin', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'PicoHeart', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.25602.jpg', 'data': {'categories': ['#inference', '#optimization'], 'emoji': 'âš™ï¸', 'ru': {'title': 'MXINT8 Ğ»ÑƒÑ‡ÑˆĞµ: Ñ†ĞµĞ»Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ĞµĞµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¸ Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğµ Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ MXINT8, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Unlocking Efficiency: Fine-Grained INT Formats Outperform FP in LLMs', 'desc': 'This paper compares low-precision floating-point (FP) and integer (INT) quantization methods in large language models (LLMs) to determine which offers better performance. It finds that fine-grained integer formats, particularly MXINT8, generally provide higher accuracy and efficiency compared to floating-point formats. The study highlights a performance crossover where FP is better for coarse-grained quantization, but INT formats excel at finer levels. Additionally, the paper introduces techniques to improve INT training, suggesting that a tailored approach using fine-grained INT formats is more effective than a uniform FP strategy.'}, 'zh': {'title': 'ç»†ç²’åº¦æ•´æ•°æ ¼å¼ä¼˜äºæµ®ç‚¹æ ¼å¼çš„é‡åŒ–ç ”ç©¶', 'desc': 'è¿™ç¯‡è®ºæ–‡å¯¹å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ä½ç²¾åº¦æµ®ç‚¹å’Œæ•´æ•°é‡åŒ–è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œç»†ç²’åº¦çš„æ•´æ•°æ ¼å¼ï¼Œç‰¹åˆ«æ˜¯MXINT8ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹æ¯”æµ®ç‚¹æ ¼å¼æä¾›æ›´å¥½çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å°½ç®¡ç°ä»£AIç¡¬ä»¶è¶Šæ¥è¶Šå€¾å‘äºä½¿ç”¨ä½ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼Œä½†è¿™ç¯‡è®ºæ–‡å¡«è¡¥äº†FPå’ŒINTé‡åŒ–ä¹‹é—´ç¼ºä¹ç»Ÿä¸€æ¯”è¾ƒçš„ç©ºç™½ã€‚ç»“æœè¡¨æ˜ï¼Œç»†ç²’åº¦çš„INTæ ¼å¼åœ¨æœªæ¥çš„AIåŠ é€Ÿå™¨ä¸­æ›´å…·ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24411', 'title': 'OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid\n  Validation in Realistic Workflows', 'url': 'https://huggingface.co/papers/2510.24411', 'abstract': 'A hybrid safety detection framework combining formal verification and VLM-based contextual assessment improves the detection of unsafe operations in mobile agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents.', 'score': 71, 'issue_id': 6765, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '608a19e1a40bdb59', 'authors': ['Qiushi Sun', 'Mukai Li', 'Zhoumianze Liu', 'Zhihui Xie', 'Fangzhi Xu', 'Zhangyue Yin', 'Kanzhi Cheng', 'Zehao Li', 'Zichen Ding', 'Qi Liu', 'Zhiyong Wu', 'Zhuosheng Zhang', 'Ben Kao', 'Lingpeng Kong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University', 'Shanghai Jiao Tong University', 'University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24411.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#agents', '#multimodal', '#dataset', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision-Language Models (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ OS-Sentinel, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ VLM-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ğ¸Ñ†Ğ° MobileRisk-Live Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10-30% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Mobile Agent Safety with Hybrid Detection Frameworks', 'desc': 'This paper presents a new framework called OS-Sentinel that enhances the detection of unsafe operations in mobile agents using a combination of formal verification and Vision-Language Models (VLMs). The framework addresses the challenges of identifying safety concerns in complex mobile environments by integrating system-level checks with contextual assessments of agent actions. The authors introduce MobileRisk-Live, a dynamic testing environment that provides realistic scenarios for evaluating safety detection methods. Experimental results indicate that OS-Sentinel significantly outperforms existing safety detection techniques, improving detection rates by 10% to 30%.'}, 'zh': {'title': 'æå‡ç§»åŠ¨ä»£ç†å®‰å…¨æ€§çš„æ··åˆæ£€æµ‹æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆå®‰å…¨æ£€æµ‹æ¡†æ¶ï¼Œç»“åˆäº†å½¢å¼éªŒè¯å’ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ä¸Šä¸‹æ–‡è¯„ä¼°ï¼Œä»¥æé«˜ç§»åŠ¨ä»£ç†ä¸­ä¸å®‰å…¨æ“ä½œçš„æ£€æµ‹èƒ½åŠ›ã€‚éšç€è®¡ç®—æœºä»£ç†åœ¨æ•°å­—ç¯å¢ƒä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå®‰å…¨éšæ‚£å¦‚ç³»ç»Ÿæ¼æ´å’Œéšç§æ³„éœ²å¼•å‘äº†äººä»¬çš„å…³æ³¨ã€‚æˆ‘ä»¬å¼•å…¥äº†MobileRisk-Liveï¼Œä¸€ä¸ªåŠ¨æ€æ²™ç®±ç¯å¢ƒï¼Œå¹¶æä¾›äº†åŒ…å«çœŸå®è½¨è¿¹å’Œç»†ç²’åº¦æ³¨é‡Šçš„å®‰å…¨æ£€æµ‹åŸºå‡†ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†OS-Sentinelæ¡†æ¶ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šæ¯”ç°æœ‰æ–¹æ³•æé«˜äº†10%-30%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27688', 'title': 'Continuous Autoregressive Language Models', 'url': 'https://huggingface.co/papers/2510.27688', 'abstract': 'Continuous Autoregressive Language Models (CALM) improve language model efficiency by predicting continuous vectors instead of discrete tokens, reducing computational cost while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM.', 'score': 70, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '7e6112858ea073c4', 'authors': ['Chenze Shao', 'Darren Li', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Qiuzhen College, Tsinghua University', 'WeChat AI, Tencent Inc'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27688.jpg', 'data': {'categories': ['#open_source', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ 99,9%. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² K Ñ€Ğ°Ğ·, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'Revolutionizing Language Models with Continuous Predictions', 'desc': 'Continuous Autoregressive Language Models (CALM) represent a new approach in language modeling by predicting continuous vectors instead of discrete tokens. This method enhances efficiency by allowing the model to generate language in fewer steps, reducing computational costs significantly. By utilizing a high-fidelity autoencoder, CALM can compress multiple tokens into a single vector while maintaining high accuracy in reconstruction. The framework developed for CALM supports robust training and evaluation, making it a promising advancement for scalable and efficient language models.'}, 'zh': {'title': 'è¿ç»­å‘é‡é¢„æµ‹ï¼šè¯­è¨€æ¨¡å‹çš„æ–°é©å‘½', 'desc': 'è¿ç»­è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆCALMï¼‰é€šè¿‡é¢„æµ‹è¿ç»­å‘é‡è€Œéç¦»æ•£æ ‡è®°ï¼Œæé«˜äº†è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä½¿ç”¨é«˜ä¿çœŸè‡ªç¼–ç å™¨å°†Kä¸ªæ ‡è®°å‹ç¼©ä¸ºä¸€ä¸ªè¿ç»­å‘é‡ï¼Œä»è€Œä»¥è¶…è¿‡99.9%çš„å‡†ç¡®ç‡é‡å»ºåŸå§‹æ ‡è®°ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—è¯­è¨€å»ºæ¨¡å¯ä»¥è§†ä¸ºè¿ç»­å‘é‡çš„åºåˆ—ï¼Œå‡å°‘äº†ç”Ÿæˆæ­¥éª¤çš„æ•°é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒCALMåœ¨æ€§èƒ½ä¸è®¡ç®—æˆæœ¬çš„æƒè¡¡ä¸Šæ˜¾è‘—æ”¹å–„ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹å‘å±•è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25889', 'title': 'Ï€_RL: Online RL Fine-tuning for Flow-based\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2510.25889', 'abstract': 'The framework Ï€<sub>RL</sub> uses reinforcement learning to train flow-based Vision-Language-Action models, addressing challenges with intractable action log-likelihoods and achieving significant performance improvements over supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., pi_0, pi_{0.5}) remains challenging due to intractable action log-likelihoods from iterative denoising.   We address this challenge with pi_{RL}, an open-source framework for training flow-based VLAs in parallel simulation. pi_{RL} implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration.   We evaluate pi_{RL} on LIBERO and ManiSkill benchmarks. On LIBERO, pi_{RL} boosts few-shot SFT models pi_0 and pi_{0.5} from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train pi_{RL} in 320 parallel environments, improving pi_0 from 41.6% to 85.7% and pi_{0.5} from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation.   Overall, pi_{RL} achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.', 'score': 64, 'issue_id': 6765, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '0bd1022f3cfefd3f', 'authors': ['Kang Chen', 'Zhihao Liu', 'Tonghe Zhang', 'Zhen Guo', 'Si Xu', 'Hao Lin', 'Hongzhi Zang', 'Xiang Li', 'Quanlu Zhang', 'Zhaofei Yu', 'Guoliang Fan', 'Tiejun Huang', 'Yu Wang', 'Chao Yu'], 'affiliations': ['Carnegie Mellon University', 'Infinigence AI', 'Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'Tsinghua University', 'Zhongguancun Academy'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.25889.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#robotics', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ï€_RL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ»Ğ¾ Ğ½ĞµÑ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ğ¼Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°: Flow-Noise, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ MDP Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¹ ÑĞµÑ‚ÑŒÑ ÑˆÑƒĞ¼Ğ°, Ğ¸ Flow-SDE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ODE-Ğ²-SDE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ RL-ÑĞºÑĞ¿Ğ»Ğ¾Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO Ğ¸ ManiSkill.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Vision-Language-Action Models', 'desc': "The paper introduces Ï€<sub>RL</sub>, a framework that leverages reinforcement learning (RL) to enhance flow-based Vision-Language-Action (VLA) models. It tackles the issue of intractable action log-likelihoods that arise during the training of these models, which has hindered their scalability. By implementing two innovative RL algorithms, Flow-Noise and Flow-SDE, Ï€<sub>RL</sub> enables efficient exploration and training in parallel simulations. The results show substantial performance improvements on benchmark tasks, demonstrating the framework's ability to generalize better than traditional supervised fine-tuning methods."}, 'zh': {'title': 'åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºÏ€<sub>RL</sub>çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒåŸºäºæµçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä»¥è§£å†³ä¸å¯å¤„ç†çš„åŠ¨ä½œå¯¹æ•°ä¼¼ç„¶æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¹¶è¡Œä»¿çœŸæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚Ï€<sub>RL</sub>å®ç°äº†ä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ†åˆ«ä¸ºFlow-Noiseå’ŒFlow-SDEï¼Œå‰è€…é€šè¿‡å¯å­¦ä¹ çš„å™ªå£°ç½‘ç»œç²¾ç¡®è®¡ç®—å¯¹æ•°ä¼¼ç„¶ï¼Œåè€…åˆ™ç»“åˆäº†å»å™ªä¸æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÏ€<sub>RL</sub>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹çš„è¡¨ç°ï¼ŒéªŒè¯äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ åœ¨æµå¼VLAä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26788', 'title': 'Defeating the Training-Inference Mismatch via FP16', 'url': 'https://huggingface.co/papers/2510.26788', 'abstract': 'Using FP16 precision in reinforcement learning fine-tuning of large language models improves stability, convergence, and performance by addressing numerical mismatches.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.', 'score': 29, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'b91a7d9e620c1232', 'authors': ['Penghui Qi', 'Zichen Liu', 'Xiangxin Zhou', 'Tianyu Pang', 'Chao Du', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26788.jpg', 'data': {'categories': ['#inference', '#rl', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'FP16 ĞºĞ°Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² reinforcement learning fine-tuning', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ½ĞµĞ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ¾ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ FP16 Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ BF16 ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼. ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Boosting RL Stability with FP16 Precision', 'desc': 'This paper discusses the use of FP16 precision in reinforcement learning (RL) fine-tuning of large language models (LLMs) to enhance stability and performance. It identifies that numerical mismatches between training and inference policies are a major source of instability, primarily caused by the rounding errors associated with BF16 precision. The authors demonstrate that switching to FP16 precision resolves these issues without requiring changes to the model architecture or learning algorithms. Their findings indicate that FP16 leads to more stable optimization, faster convergence, and improved performance across various tasks and frameworks.'}, 'zh': {'title': 'ä½¿ç”¨FP16æå‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä½¿ç”¨FP16ç²¾åº¦å¯ä»¥æé«˜ç¨³å®šæ€§ã€æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ã€‚è¿™æ˜¯å› ä¸ºè®­ç»ƒå’Œæ¨ç†ç­–ç•¥ä¹‹é—´çš„æ•°å€¼ä¸åŒ¹é…æ˜¯å¯¼è‡´ä¸ç¨³å®šæ€§çš„æ ¹æœ¬åŸå› ã€‚ä»¥å¾€çš„ç ”ç©¶å°è¯•é€šè¿‡ç®—æ³•ä¿®æ­£æˆ–å·¥ç¨‹å¯¹é½æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æˆ‘ä»¬å‘ç°ï¼ŒBF16è™½ç„¶åŠ¨æ€èŒƒå›´å¤§ï¼Œä½†ä¼šå¼•å…¥è¾ƒå¤§çš„èˆå…¥è¯¯å·®ï¼Œç ´åè®­ç»ƒä¸æ¨ç†çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°ä½¿ç”¨FP16å¯ä»¥æœ‰æ•ˆæ¶ˆé™¤è¿™ç§ä¸åŒ¹é…ï¼Œå¸¦æ¥æ›´ç¨³å®šçš„ä¼˜åŒ–å’Œæ›´å¿«çš„æ”¶æ•›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27606', 'title': 'Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.27606', 'abstract': 'Spatial-SSRL, a self-supervised reinforcement learning paradigm, enhances spatial understanding in Large Vision-Language Models using verifiable signals from RGB or RGB-D images without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.', 'score': 27, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'ee8d3a3f8441506a', 'authors': ['Yuhong Liu', 'Beichen Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Long Xing', 'Xiaoyi Dong', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27606.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#training', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'ĞŸaĞ¿ĞµÑ€ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Spatial-SSRL â€” Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ², ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ´Ğ°ÑÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· RGB Ğ¸Ğ»Ğ¸ RGB-D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.89-4.63% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Qwen2.5-VL.'}, 'en': {'title': 'Enhancing Spatial Intelligence in LVLMs with Self-Supervised Learning', 'desc': 'Spatial-SSRL is a self-supervised reinforcement learning approach that improves spatial understanding in Large Vision-Language Models (LVLMs) using signals from RGB or RGB-D images without needing human labels. It addresses the limitations of traditional supervised fine-tuning and reinforcement learning methods that require expensive supervision. The method introduces five pretext tasks that help the model learn about 2D and 3D spatial structures, such as patch reordering and depth ordering. By training on these tasks, Spatial-SSRL enhances spatial reasoning capabilities while maintaining overall visual performance, achieving notable accuracy improvements on various benchmarks.'}, 'zh': {'title': 'è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æå‡ç©ºé—´ç†è§£èƒ½åŠ›', 'desc': 'Spatial-SSRLæ˜¯ä¸€ç§è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡RGBæˆ–RGB-Då›¾åƒä¸­çš„å¯éªŒè¯ä¿¡å·å¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œè‡ªåŠ¨ç”Ÿæˆäº”ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œæ•æ‰2Då’Œ3Dç©ºé—´ç»“æ„ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬æ‰“ä¹±çš„è¡¥ä¸é‡æ’åºã€ç¿»è½¬è¡¥ä¸è¯†åˆ«ã€è£å‰ªè¡¥ä¸ä¿®å¤ã€åŒºåŸŸæ·±åº¦æ’åºå’Œç›¸å¯¹3Dä½ç½®é¢„æµ‹ï¼Œæä¾›æ˜“äºéªŒè¯çš„çœŸå®ç­”æ¡ˆã€‚é€šè¿‡åœ¨è¿™äº›ä»»åŠ¡ä¸Šè®­ç»ƒï¼ŒSpatial-SSRLæ˜¾è‘—æé«˜äº†ç©ºé—´æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨çš„è§†è§‰èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27684', 'title': 'Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals', 'url': 'https://huggingface.co/papers/2510.27684', 'abstract': 'Phased DMD enhances multi-step distillation of score-based generative models by dividing SNR ranges and using progressive distribution matching, improving diversity and generative capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models.', 'score': 22, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '5cc1bfdf8fe04b6a', 'authors': ['Xiangyu Fan', 'Zesong Qiu', 'Zhuguanyu Wu', 'Fanzhou Wang', 'Zhiqian Lin', 'Tianxiang Ren', 'Dahua Lin', 'Ruihao Gong', 'Lei Yang'], 'affiliations': ['Beihang University', 'SenseTime Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27684.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#architecture', '#diffusion', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ¤Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Phased DMD â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑˆÑƒĞ¼Ğ° (score-based generative models). ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»-ÑˆÑƒĞ¼ (SNR) Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Phased DMD ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ DMD, Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Generative Models with Phased DMD', 'desc': "Phased DMD is a new framework that improves the process of multi-step distillation for score-based generative models. It works by dividing the Signal-to-Noise Ratio (SNR) into smaller ranges and progressively matching distributions, which helps the model learn complex patterns more effectively. This method enhances the model's capacity and maintains diversity in generated outputs, addressing issues seen in previous approaches. By validating this technique on advanced image and video generation models, Phased DMD shows better performance compared to traditional methods."}, 'zh': {'title': 'æå‡ç”Ÿæˆæ¨¡å‹å¤šæ ·æ€§çš„Phased DMD', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPhased DMDçš„å¤šæ­¥è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå¤šæ ·æ€§ã€‚é€šè¿‡å°†ä¿¡å™ªæ¯”(SNR)èŒƒå›´åˆ’åˆ†ä¸ºå­åŒºé—´ï¼Œå¹¶åœ¨æ¯ä¸ªå­åŒºé—´å†…è¿›è¡Œæ¸è¿›çš„åˆ†å¸ƒåŒ¹é…ï¼ŒPhased DMDèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„åˆ†å¸ƒç‰¹å¾ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é˜¶æ®µæ€§è’¸é¦å’Œä¸“å®¶æ··åˆæ¨¡å‹çš„æ€æƒ³ï¼Œé™ä½äº†å­¦ä¹ éš¾åº¦ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡å‹çš„å®¹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhased DMDåœ¨ä¿æŒç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™è¾“å‡ºçš„å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27266', 'title': 'HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration', 'url': 'https://huggingface.co/papers/2510.27266', 'abstract': 'HyperClick enhances GUI automation by calibrating confidence and reducing overconfidence through a dual reward mechanism and spatial confidence modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing a misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, a novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces a dual reward mechanism, combining a binary reward for correct actions with a truncated Gaussian-based spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation.', 'score': 20, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '32330517b5449dd1', 'authors': ['Shaojie Zhang', 'Pei Fu', 'Ruoceng Zhang', 'Jiahui Yang', 'Anan Du', 'Xiuwen Xi', 'Shaokang Wang', 'Ying Huang', 'Bin Qin', 'Zhenbo Luo', 'Jian Luan'], 'affiliations': ['Xiaomi Inc'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27266.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ GUI Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºÑƒ', 'desc': 'HyperClick â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ GUI Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ°, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Ğ‘Ñ€Ğ¸ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ HyperClick Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ñ‚ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'HyperClick: Calibrating Confidence for Reliable GUI Automation', 'desc': 'HyperClick is a framework designed to improve the reliability of GUI automation by addressing the issue of overconfidence in model predictions. It introduces a dual reward mechanism that combines traditional rewards for correct actions with a spatial confidence model based on Gaussian distributions. This approach allows the model to better calibrate its confidence levels, ensuring that predictions align more closely with actual performance. Through extensive testing, HyperClick demonstrates superior accuracy and well-calibrated confidence, making it a significant advancement in the field of automated GUI agents.'}, 'zh': {'title': 'HyperClickï¼šæå‡GUIè‡ªåŠ¨åŒ–çš„å¯é æ€§ä¸è‡ªä¿¡åº¦', 'desc': 'HyperClick æ˜¯ä¸€ç§å¢å¼ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡åŒé‡å¥–åŠ±æœºåˆ¶å’Œç©ºé—´ç½®ä¿¡åº¦å»ºæ¨¡æ¥æ ¡å‡†ç½®ä¿¡åº¦å¹¶å‡å°‘è¿‡åº¦è‡ªä¿¡ã€‚ç°æœ‰æ¨¡å‹åœ¨æ‰§è¡Œç”¨æˆ·å‘½ä»¤æ—¶ç¼ºä¹å¯¹è‡ªèº«èƒ½åŠ›è¾¹ç•Œçš„è‡ªæˆ‘æ„è¯†ï¼Œå¯¼è‡´é¢„æµ‹ä¸å¯é ã€‚HyperClick é€šè¿‡å¼•å…¥åŸºäº Brier åˆ†æ•°çš„æˆªæ–­é«˜æ–¯ç©ºé—´ç½®ä¿¡åº¦å»ºæ¨¡ï¼Œç»“åˆæ­£ç¡®åŠ¨ä½œçš„äºŒå…ƒå¥–åŠ±ï¼Œä¼˜åŒ–äº†åŸºç¡€å‡†ç¡®æ€§å’Œç½®ä¿¡åº¦çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHyperClick åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›äº†è‰¯å¥½æ ¡å‡†çš„ç½®ä¿¡åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23095', 'title': 'Revisiting Multimodal Positional Encoding in Vision-Language Models', 'url': 'https://huggingface.co/papers/2510.23095', 'abstract': 'A comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) leads to the proposal of Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), which improve multimodal understanding in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.', 'score': 20, 'issue_id': 6765, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '692489f3283da5a9', 'authors': ['Jie Huang', 'Xuejing Liu', 'Sibo Song', 'Ruibing Hou', 'Hong Chang', 'Junyang Lin', 'Shuai Bai'], 'affiliations': ['Alibaba Group', 'Institute of Computing Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.23095.jpg', 'data': {'categories': ['#architecture', '#multimodal'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° RoPE Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ (RoPE) Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ LLM. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° - Multi-Head RoPE Ğ¸ MRoPE-Interleave, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Multimodal Understanding with MHRoPE and MRoPE-I', 'desc': "This paper explores the importance of multimodal position encoding in vision-language models, focusing on Rotary Positional Embedding (RoPE). The authors analyze RoPE's components, such as position design and frequency allocation, to derive three essential guidelines for effective encoding. They introduce two new methods, Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), which enhance multimodal understanding without requiring changes to existing architectures. Experimental results show that these methods significantly improve performance on various benchmarks, demonstrating their effectiveness in both general and detailed multimodal tasks."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€ç†è§£çš„åˆ›æ–°ç¼–ç æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡å¯¹å¤šæ¨¡æ€æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰è¿›è¡Œäº†å…¨é¢åˆ†æï¼Œæå‡ºäº†å¤šå¤´RoPEï¼ˆMHRoPEï¼‰å’ŒMRoPE-äº¤é”™ï¼ˆMRoPE-Iï¼‰ä¸¤ç§æ–°æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä½ç½®è®¾è®¡å’Œé¢‘ç‡åˆ†é…çš„æ·±å…¥ç ”ç©¶ï¼Œä½œè€…æ€»ç»“äº†ä¸‰ä¸ªå…³é”®æŒ‡å¯¼åŸåˆ™ï¼šä½ç½®ä¸€è‡´æ€§ã€é¢‘ç‡å……åˆ†åˆ©ç”¨å’Œæ–‡æœ¬å…ˆéªŒçš„ä¿ç•™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€ç†è§£çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24940', 'title': 'SemCoT: Accelerating Chain-of-Thought Reasoning through\n  Semantically-Aligned Implicit Tokens', 'url': 'https://huggingface.co/papers/2510.24940', 'abstract': "SemCoT enhances CoT efficiency by optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning using contrastive training and knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.", 'score': 16, 'issue_id': 6765, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '4fddfc6e0ebc526b', 'authors': ['Yinhan He', 'Wendy Zheng', 'Yaochen Zhu', 'Zaiyi Zheng', 'Lin Su', 'Sriram Vasudevan', 'Qi Guo', 'Liangjie Hong', 'Jundong Li'], 'affiliations': ['LinkedIn Inc.', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24940.jpg', 'data': {'categories': ['#optimization', '#reasoning'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½ĞµÑĞ²Ğ½Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'SemCoT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ§Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Boosting CoT Efficiency with SemCoT: Fast and Accurate Reasoning!', 'desc': 'SemCoT is a novel framework that improves the efficiency of Chain-of-Thought (CoT) reasoning in machine learning applications. It addresses two main challenges: maintaining semantic alignment between implicit and explicit reasoning, and optimizing the speed of token generation. By using contrastive training and knowledge distillation, SemCoT ensures that the generated reasoning closely matches the ground-truth while also speeding up the process. Experimental results show that SemCoT outperforms existing methods in both efficiency and effectiveness, making it a significant advancement in the field.'}, 'zh': {'title': 'æå‡é“¾å¼æ€ç»´æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'SemCoTæ˜¯ä¸€ç§æ–°é¢–çš„éšå¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆé€Ÿåº¦å¹¶ä¿æŒä¸çœŸå®æ¨ç†çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡å¯¹æ¯”è®­ç»ƒçš„å¥å­å˜æ¢å™¨æ¥è¯„ä¼°éšå¼æ¨ç†ä¸æ˜¾å¼æ¨ç†ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œä¼˜åŒ–éšå¼æ¨ç†çš„è¿‡ç¨‹ã€‚ä¸ºäº†æé«˜ç”Ÿæˆæ•ˆç‡ï¼ŒSemCoTè¿˜å¼•å…¥äº†ä¸€ç§è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦è¿›è¡Œå¾®è°ƒï¼Œç”Ÿæˆè¯­ä¹‰ä¸€è‡´çš„éšå¼æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSemCoTåœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27258', 'title': 'Higher-order Linear Attention', 'url': 'https://huggingface.co/papers/2510.27258', 'abstract': 'Higher-order Linear Attention (HLA) is a scalable, causal, and efficient mechanism for long-context autoregressive language models, combining attention-like mixing with recurrent architecture efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic cost of scaled dot-product attention is a central obstacle to scaling autoregressive language models to long contexts. Linear-time attention and State Space Models (SSMs) provide scalable alternatives but are typically restricted to first-order or kernel-based approximations, which can limit expressivity. We introduce Higher-order Linear Attention (HLA), a causal, streaming mechanism that realizes higher interactions via compact prefix sufficient statistics. In the second-order case, HLA maintains a constant-size state and computes per-token outputs in linear time without materializing any n times n matrices. We give closed-form streaming identities, a strictly causal masked variant using two additional summaries, and a chunk-parallel training scheme based on associative scans that reproduces the activations of a serial recurrence exactly. We further outline extensions to third and higher orders. Collectively, these results position HLA as a principled, scalable building block that combines attention-like, data-dependent mixing with the efficiency of modern recurrent architectures. Project Page: https://github.com/yifanzhang-pro/HLA.', 'score': 14, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '2941fb4cce36d8a7', 'authors': ['Yifan Zhang', 'Zhen Qin', 'Quanquan Gu'], 'affiliations': ['Princeton University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27258.jpg', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#long_context', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Higher-order Linear Attention (HLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. HLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞºĞ°Ğ½Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€.'}, 'en': {'title': 'Efficient Long-Context Language Modeling with Higher-order Linear Attention', 'desc': 'Higher-order Linear Attention (HLA) is a new method designed to improve the efficiency of long-context autoregressive language models. It addresses the high computational cost of traditional attention mechanisms by using a linear-time approach that allows for higher-order interactions without the need for large matrices. HLA maintains a constant-size state and computes outputs efficiently, making it suitable for streaming applications. This method combines the benefits of attention mechanisms with the efficiency of recurrent architectures, paving the way for more scalable language models.'}, 'zh': {'title': 'é«˜é˜¶çº¿æ€§æ³¨æ„åŠ›ï¼šé«˜æ•ˆå¤„ç†é•¿æ–‡æœ¬çš„åˆ›æ–°æœºåˆ¶', 'desc': 'é«˜é˜¶çº¿æ€§æ³¨æ„åŠ›ï¼ˆHLAï¼‰æ˜¯ä¸€ç§å¯æ‰©å±•ã€å› æœä¸”é«˜æ•ˆçš„æœºåˆ¶ï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚å®ƒç»“åˆäº†ç±»ä¼¼æ³¨æ„åŠ›çš„æ··åˆæ–¹å¼ä¸é€’å½’æ¶æ„çš„é«˜æ•ˆæ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿç‚¹ç§¯æ³¨æ„åŠ›çš„äºŒæ¬¡æˆæœ¬é—®é¢˜ã€‚HLAé€šè¿‡ç´§å‡‘çš„å‰ç¼€å……åˆ†ç»Ÿè®¡å®ç°æ›´é«˜é˜¶çš„äº¤äº’ï¼Œèƒ½å¤Ÿåœ¨ä¸ç”Ÿæˆå¤§è§„æ¨¡çŸ©é˜µçš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§æ—¶é—´è®¡ç®—æ¯ä¸ªæ ‡è®°çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•ä¸ºè‡ªå›å½’æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆæ„å»ºå—ï¼Œé€‚åˆå¤„ç†é•¿æ–‡æœ¬çš„ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27623', 'title': 'Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning', 'url': 'https://huggingface.co/papers/2510.27623', 'abstract': 'BEAT is a framework that injects visual backdoors into MLLM-based embodied agents using object triggers, achieving high attack success rates while maintaining task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.', 'score': 12, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '5e34a1d5f6495802', 'authors': ['Qiusi Zhan', 'Hyeonjeong Ha', 'Rui Yang', 'Sirui Xu', 'Hanyang Chen', 'Liang-Yan Gui', 'Yu-Xiong Wang', 'Huan Zhang', 'Heng Ji', 'Daniel Kang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27623.jpg', 'data': {'categories': ['#agents', '#multimodal', '#training', '#security'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ² Ğ³Ğ»Ğ°Ğ·Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° MLLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±ÑĞºĞ´Ğ¾Ñ€Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° BEAT Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ÑĞºĞ´Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ°Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Contrastive Trigger Learning (CTL). CTL Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ² ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ñ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ¾Ğ¼ Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾, ÑĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ¾ÑÑ‚Ñ€ÑÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ±ÑĞºĞ´Ğ¾Ñ€Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ 80% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ñ‹ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'BEAT: Unleashing Visual Backdoors in MLLM Agents', 'desc': 'The paper presents BEAT, a framework designed to inject visual backdoors into multimodal large language model (MLLM)-based embodied agents using object triggers. These visual backdoors allow the agent to perform normally until a specific visual trigger is detected, at which point it executes a predetermined policy set by an attacker. BEAT overcomes the challenge of variability in object triggers by creating a diverse training set and employing a two-stage training process that includes supervised fine-tuning and a novel Contrastive Trigger Learning method. The results show that BEAT can achieve high attack success rates while still maintaining strong performance on benign tasks, highlighting a significant security vulnerability in MLLM-based systems.'}, 'zh': {'title': 'BEATï¼šæ­ç¤ºå…·èº«æ™ºèƒ½ä½“çš„è§†è§‰åé—¨æ”»å‡»é£é™©', 'desc': 'BEATæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç‰©ä½“è§¦å‘å™¨å‘åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å…·èº«æ™ºèƒ½ä½“æ³¨å…¥è§†è§‰åé—¨ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾80%çš„æ”»å‡»æˆåŠŸç‡ã€‚BEATé€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„è®­ç»ƒé›†å’Œå¼•å…¥ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œè§£å†³äº†ç‰©ä½“è§¦å‘å™¨åœ¨ä¸åŒè§†è§’å’Œå…‰ç…§ä¸‹çš„å˜åŒ–æ€§é—®é¢˜ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†åŸºäºMLLMçš„å…·èº«æ™ºèƒ½ä½“ä¸­å­˜åœ¨çš„å®‰å…¨é£é™©ï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨å‰éœ€è¦åŠ å¼ºé˜²å¾¡æªæ–½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26707', 'title': 'Value Drifts: Tracing Value Alignment During LLM Post-Training', 'url': 'https://huggingface.co/papers/2510.26707', 'abstract': "Research investigates how and when value alignment occurs during the post-training phase of LLMs, finding that supervised fine-tuning establishes values, while preference optimization has limited impact.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.", 'score': 12, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '266c6e0c5ff945d5', 'authors': ['Mehar Bhatia', 'Shravan Nayak', 'Gaurav Kamath', 'Marius Mosbach', 'Karolina StaÅ„czak', 'Vered Shwartz', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'ETH Zurich', 'McGill University', 'Mila - Quebec AI Institute', 'Universite de Montreal', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26707.jpg', 'data': {'categories': ['#synthetic', '#rlhf', '#alignment', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¦ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Llama-3 Ğ¸ Qwen-3, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñƒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸.'}, 'en': {'title': 'Understanding Value Alignment in Post-Training of LLMs', 'desc': 'This research explores how large language models (LLMs) align with human values after their initial training. It reveals that supervised fine-tuning (SFT) is crucial for establishing these values, while preference optimization has a limited effect on re-aligning them. The study analyzes the timing and magnitude of value changes during post-training, using various models and datasets. The findings suggest that understanding these dynamics can enhance data curation and the choice of algorithms for better alignment with human values.'}, 'zh': {'title': 'åè®­ç»ƒé˜¶æ®µçš„ä»·å€¼å¯¹é½ç ”ç©¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åè®­ç»ƒé˜¶æ®µå¦‚ä½•ä»¥åŠä½•æ—¶å®ç°ä»·å€¼å¯¹é½ã€‚ç ”ç©¶å‘ç°ï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µç¡®ç«‹äº†æ¨¡å‹çš„ä»·å€¼è§‚ï¼Œè€Œåå¥½ä¼˜åŒ–å¯¹ä»·å€¼çš„å½±å“æœ‰é™ã€‚é€šè¿‡å¯¹ä¸åŒå¤§å°çš„Llama-3å’ŒQwen-3æ¨¡å‹è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¸åŒçš„åå¥½ä¼˜åŒ–ç®—æ³•ä¼šå¯¼è‡´ä¸åŒçš„ä»·å€¼å¯¹é½ç»“æœã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºç†è§£æ¨¡å‹åœ¨åè®­ç»ƒé˜¶æ®µå¦‚ä½•å­¦ä¹ ä»·å€¼è§‚æä¾›äº†å¯è¡Œçš„è§è§£ï¼Œå¹¶ä¸ºæ•°æ®ç­–åˆ’å’Œæ¨¡å‹é€‰æ‹©æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27607', 'title': 'Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model', 'url': 'https://huggingface.co/papers/2510.27607', 'abstract': "A multimodal diffusion transformer framework, DUal-STream diffusion (DUST), enhances Vision-Language-Action models by maintaining separate modality streams and enabling cross-modal knowledge sharing, achieving improved performance in both simulated and real-world robotic tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing. In addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods, while our test-time scaling approach provides an additional 2-5% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST's potential for large-scale VLA pretraining.", 'score': 8, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '83bedbbaf65d759f', 'authors': ['John Won', 'Kyungmin Lee', 'Huiwon Jang', 'Dongyoung Kim', 'Jinwoo Shin'], 'affiliations': ['KAIST', 'RLWRLD'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27607.jpg', 'data': {'categories': ['#video', '#transfer_learning', '#architecture', '#diffusion', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° DUST (DUal-STream diffusion) â€” Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ) Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ±Ğ¼ĞµĞ½ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½ÑƒÑ loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ flow-matching, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 6% Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ… Ğ¸ 13% Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Franka, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Robotic Learning with DUST: A Dual-Stream Approach', 'desc': 'The paper introduces DUal-STream diffusion (DUST), a novel framework that enhances Vision-Language-Action (VLA) models by maintaining separate streams for different modalities while allowing for cross-modal knowledge sharing. This approach addresses the challenges of predicting next-state observations and action sequences by decoupling the modalities during training. DUST employs a multimodal diffusion transformer architecture with independent noise perturbations and a decoupled flow-matching loss, enabling effective learning of joint distributions without a unified latent space. Experimental results demonstrate that DUST significantly improves performance in both simulated and real-world robotic tasks, achieving notable gains over baseline methods.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œæå‡æœºå™¨äººæ™ºèƒ½ï¼', 'desc': 'DUal-STreamæ‰©æ•£ï¼ˆDUSTï¼‰æ˜¯ä¸€ç§å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¿æŒç‹¬ç«‹çš„æ¨¡æ€æµå¹¶å®ç°è·¨æ¨¡æ€çŸ¥è¯†å…±äº«ï¼Œè§£å†³äº†æ¨¡æ€ä¹‹é—´çš„å†²çªé—®é¢˜ã€‚DUSTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨RoboCasaå’ŒGR-1åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æå‡å¯è¾¾6%ã€‚æ­¤å¤–ï¼ŒDUSTåœ¨çœŸå®ä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†13%ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26887', 'title': 'The Denario project: Deep knowledge AI agents for scientific discovery', 'url': 'https://huggingface.co/papers/2510.26887', 'abstract': 'Denario, an AI multi-agent system, performs various scientific research tasks and generates papers across multiple disciplines, demonstrating its capabilities and limitations through expert evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.', 'score': 6, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '936fe8d7b42cc137', 'authors': ['Francisco Villaescusa-Navarro', 'Boris Bolliet', 'Pablo Villanueva-Domingo', 'Adrian E. Bayer', 'Aidan Acquah', 'Chetana Amancharla', 'Almog Barzilay-Siegal', 'Pablo Bermejo', 'Camille Bilodeau', 'Pablo CÃ¡rdenas RamÃ­rez', 'Miles Cranmer', 'Urbano L. FranÃ§a', 'ChangHoon Hahn', 'Yan-Fei Jiang', 'Raul Jimenez', 'Jun-Young Lee', 'Antonio Lerario', 'Osman Mamun', 'Thomas Meier', 'Anupam A. Ojha', 'Pavlos Protopapas', 'Shimanto Roy', 'David N. Spergel', 'Pedro TarancÃ³n-Ãlvarez', 'Ujjwal Tiwari', 'Matteo Viel', 'Digvijay Wadekar', 'Chi Wang', 'Bonny Y. Wang', 'Licong Xu', 'Yossi Yovel', 'Shuwen Yue', 'Wen-Han Zhou', 'Qiyao Zhu', 'Jiajun Zou', 'ÃÃ±igo Zubeldia'], 'affiliations': ['Big Data Institute, University of Oxford', 'Cavendish Astrophysics, University of Cambridge', 'Center for Computational Astrophysics, Flatiron Institute', 'Center for Computational Quantum Physics, Flatiron Institute', 'Chemical Engineering Department, University of Virginia', 'Computer Vision Center, Universitat AutÃ²noma de Barcelona', 'Department of Applied Physics, University of the Basque Country', 'Department of Astrophysical Sciences, Princeton University', 'Donostia International Physics Center', 'Infosys Ltd', 'Kavli Institute for Cosmology, University of Cambridge', 'Ragon Institute of Mass General, MIT', 'Robert F. Smith School of Chemical and Biomolecular Engineering, Cornell University', 'School of Zoology, Faculty of Life Sciences, Tel-Aviv University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26887.jpg', 'data': {'categories': ['#open_source', '#science'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚: Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ AI', 'desc': 'Denario â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ¹ Ğ¸ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼Ğ¸ â€” Ğ¾Ñ‚ Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ´ĞµĞ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ°Ğº ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ AI-driven Ğ½Ğ°ÑƒĞºĞ¸ Ğ¸ ĞµÑ‘ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Denario: Revolutionizing Scientific Research with AI Multi-Agent Systems', 'desc': 'Denario is an AI multi-agent system that assists in scientific research by performing a variety of tasks, including literature review, research planning, and paper writing. Its modular architecture allows it to specialize in specific functions or conduct comprehensive analyses using a deep-research backend called Cmbagent. The system has generated multiple research papers across diverse fields, showcasing its ability to integrate concepts from different disciplines, such as combining quantum physics with machine learning for astrophysical studies. Expert evaluations of the generated papers reveal both the strengths and limitations of Denario, prompting discussions on the ethical implications of AI in scientific research.'}, 'zh': {'title': 'Denarioï¼šè·¨å­¦ç§‘çš„AIç§‘å­¦ç ”ç©¶åŠ©æ‰‹', 'desc': 'Denarioæ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨ä½œä¸ºç§‘å­¦ç ”ç©¶åŠ©æ‰‹ã€‚å®ƒèƒ½å¤Ÿæ‰§è¡Œå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç”Ÿæˆåˆ›æ„ã€æ£€æŸ¥æ–‡çŒ®ã€åˆ¶å®šç ”ç©¶è®¡åˆ’ã€ç¼–å†™å’Œæ‰§è¡Œä»£ç ã€åˆ¶ä½œå›¾è¡¨ä»¥åŠæ’°å†™å’Œå®¡é˜…ç§‘å­¦è®ºæ–‡ã€‚è¯¥ç³»ç»Ÿå…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œå¯ä»¥å¤„ç†ç‰¹å®šä»»åŠ¡ï¼Œå¹¶èƒ½å¤Ÿç»“åˆä¸åŒå­¦ç§‘çš„æ€æƒ³ï¼Œå±•ç¤ºå…¶è·¨å­¦ç§‘çš„ç ”ç©¶èƒ½åŠ›ã€‚é€šè¿‡ä¸“å®¶è¯„ä¼°ï¼ŒDenarioçš„ä¼˜ç¼ºç‚¹å’Œå±€é™æ€§å¾—åˆ°äº†è¯¦ç»†çš„åé¦ˆï¼Œè®¨è®ºäº†AIé©±åŠ¨ç ”ç©¶çš„ä¼¦ç†å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27044', 'title': 'Limits of Generalization in RLVR: Two Case Studies in Mathematical\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.27044', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) enhances evaluation metrics on combinatorial problems but often reinforces superficial heuristics rather than genuine reasoning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: Activity Scheduling and the Longest Increasing Subsequence, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.', 'score': 5, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '13cb921258ffb932', 'authors': ['Md Tanvirul Alam', 'Nidhi Rastogi'], 'affiliations': ['Rochester Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27044.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#rl', '#math'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‹Ğ²Ğ°ĞµÑ‚: Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ RLVR Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ¿Ğ¾Ğ´Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ÑĞºĞ¸, Ğ° Ğ½Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Enhancing AI Reasoning: The Limits of RLVR', 'desc': 'Reinforcement Learning with Verifiable Rewards (RLVR) aims to improve the evaluation of combinatorial problems by providing rewards that can be verified. However, the study reveals that while RLVR enhances performance metrics, it often leads to the reinforcement of superficial heuristics instead of developing deeper reasoning strategies. The research focuses on two specific combinatorial problems, demonstrating that RLVR can improve results but may not foster true mathematical reasoning. This highlights the need for better benchmarks that can differentiate between genuine reasoning and shortcut methods in AI models.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ï¼šè¶…è¶Šè¡¨é¢å¯å‘å¼çš„æ¨ç†æŒ‘æˆ˜', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨ç»„åˆé—®é¢˜çš„è¯„ä¼°æŒ‡æ ‡ä¸Šæœ‰æ‰€æå‡ï¼Œä½†å¾€å¾€å¼ºåŒ–äº†è¡¨é¢çš„å¯å‘å¼æ–¹æ³•ï¼Œè€ŒéçœŸæ­£çš„æ¨ç†ç­–ç•¥ã€‚æ•°å­¦æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ä»…éœ€è¦æ­£ç¡®çš„ç­”æ¡ˆï¼Œè¿˜éœ€è¦å¯ä¿¡çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰å®Œå…¨å¯éªŒè¯è§£çš„ç»„åˆé—®é¢˜ä¸Šç ”ç©¶äº†RLVRï¼Œå‘ç°å°½ç®¡RLVRæé«˜äº†è¯„ä¼°æŒ‡æ ‡ï¼Œä½†é€šå¸¸æ˜¯é€šè¿‡å¼ºåŒ–è¡¨é¢å¯å‘å¼ï¼Œè€Œä¸æ˜¯è·å¾—æ–°çš„æ¨ç†ç­–ç•¥ã€‚è¿™äº›å‘ç°çªæ˜¾äº†RLVRæ³›åŒ–çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†åŒºåˆ†çœŸæ­£æ•°å­¦æ¨ç†ä¸æ·å¾„åˆ©ç”¨çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å¯ä¿¡çš„è¿›å±•è¡¡é‡æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24795', 'title': 'A Survey on Efficient Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2510.24795', 'abstract': 'This survey reviews Efficient Vision-Language-Action models, addressing computational and data challenges through model design, training, and data collection techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/', 'score': 5, 'issue_id': 6765, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'ac5d013a93e48a28', 'authors': ['Zhaoshu Yu', 'Bo Wang', 'Pengpeng Zeng', 'Haonan Zhang', 'Ji Zhang', 'Lianli Gao', 'Jingkuan Song', 'Nicu Sebe', 'Heng Tao Shen'], 'affiliations': ['Department of Information Engineering and Computer Science, University of Trento, Italy', 'School of Computer Science and Engineering, University of Electronic Science and Technology of China, China', 'School of Computer Science and Technology, Tongji University, China', 'School of Computing and Artificial Intelligence, Southwest Jiaotong University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24795.jpg', 'data': {'categories': ['#robotics', '#optimization', '#survey', '#data', '#multimodal', '#training', '#inference'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ·Ğ´ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼', 'desc': 'ĞĞ±Ğ·Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ñ… Vision-Language-Action ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Bridging Digital Knowledge with Efficient Action in the Real World', 'desc': 'This paper surveys Efficient Vision-Language-Action (VLA) models, which aim to connect digital information with real-world actions. It highlights the challenges of high computational and data demands that hinder the use of large-scale foundation models in these applications. The authors propose a unified taxonomy to categorize techniques into three main areas: Efficient Model Design, Efficient Training, and Efficient Data Collection. By reviewing current methods and applications, the paper provides a foundational reference for future research in this evolving field.'}, 'zh': {'title': 'é«˜æ•ˆè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æœªæ¥ä¹‹è·¯', 'desc': 'æœ¬è°ƒæŸ¥å›é¡¾äº†é«˜æ•ˆçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼Œè§£å†³äº†æ¨¡å‹è®¾è®¡ã€è®­ç»ƒå’Œæ•°æ®æ”¶é›†æŠ€æœ¯ä¸­çš„è®¡ç®—å’Œæ•°æ®æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹æ—¨åœ¨å°†æ•°å­—çŸ¥è¯†ä¸ç‰©ç†ä¸–ç•Œçš„äº’åŠ¨ç»“åˆèµ·æ¥ï¼Œå°½ç®¡å®ƒä»¬å±•ç°äº†å‡ºè‰²çš„é€šç”¨èƒ½åŠ›ï¼Œä½†ç”±äºåŸºç¡€å¤§è§„æ¨¡æ¨¡å‹çš„è®¡ç®—å’Œæ•°æ®éœ€æ±‚ï¼Œéƒ¨ç½²å—åˆ°ä¸¥é‡é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•ï¼Œå°†å½“å‰æŠ€æœ¯åˆ†ä¸ºä¸‰ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šé«˜æ•ˆæ¨¡å‹è®¾è®¡ã€é«˜æ•ˆè®­ç»ƒå’Œé«˜æ•ˆæ•°æ®æ”¶é›†ã€‚é€šè¿‡å¯¹ç°æœ‰æ–¹æ³•çš„æ‰¹åˆ¤æ€§å›é¡¾ï¼Œæœ¬è°ƒæŸ¥ä¸ºç¤¾åŒºå»ºç«‹äº†åŸºç¡€å‚è€ƒï¼Œå¹¶æ€»ç»“äº†ä»£è¡¨æ€§åº”ç”¨ã€å…³é”®æŒ‘æˆ˜ä»¥åŠæœªæ¥ç ”ç©¶çš„è·¯çº¿å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.20150', 'title': 'Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.20150', 'abstract': 'ConvRec-R1, a two-stage framework, enhances LLM-based conversational recommender systems by using behavioral cloning and Rank-GRPO to improve recommendation quality and convergence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.', 'score': 4, 'issue_id': 6765, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 23', 'zh': '10æœˆ23æ—¥'}, 'hash': 'd86bde6990d935bb', 'authors': ['Yaochen Zhu', 'Harald Steck', 'Dawen Liang', 'Yinhan He', 'Vito Ostuni', 'Jundong Li', 'Nathan Kallus'], 'affiliations': ['Cornell University', 'Netflix Research', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.20150.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#rl', '#training'], 'emoji': 'ğŸ’¬', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¿Ğ¸ÑĞºĞµ', 'desc': 'ConvRec-R1 â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ¾Ğ¼ Remap-Reflect-Adjust Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Rank-GRPO â€” Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ² ÑĞ¿Ğ¸ÑĞºĞµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ²ÑĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ConvRec-R1 Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Recall Ğ¸ NDCG Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Conversational Recommendations with ConvRec-R1', 'desc': 'The paper introduces ConvRec-R1, a two-stage framework designed to enhance conversational recommender systems that utilize large language models (LLMs). In the first stage, it creates a behavioral-cloning dataset to generate high-quality recommendations grounded in a catalog, which helps in initializing the reinforcement learning (RL) training. The second stage employs Rank-GRPO, a novel approach that optimizes the ranking of recommendations by treating each rank as a unit, thus improving the stability of policy updates. Experimental results demonstrate that ConvRec-R1 outperforms existing methods in terms of faster convergence and better recommendation quality, as measured by Recall and NDCG metrics.'}, 'zh': {'title': 'æå‡å¯¹è¯æ¨èç³»ç»Ÿçš„è´¨é‡ä¸æ”¶æ•›æ€§', 'desc': 'ConvRec-R1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹è¯æ¨èç³»ç»Ÿçš„æ¨èè´¨é‡å’Œæ”¶æ•›æ€§ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡Remap-Reflect-Adjustæµç¨‹æ„å»ºäº†ä¸€ä¸ªè¡Œä¸ºå…‹éš†æ•°æ®é›†ï¼Œä»¥ä¾¿ä»å¼ºå¤§çš„é»‘ç®±LLMä¸­ç”Ÿæˆé«˜è´¨é‡çš„ã€åŸºäºç›®å½•çš„æ¼”ç¤ºï¼Œæ¥ä¸ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæä¾›çƒ­å¯åŠ¨ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†Rank-GRPOï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ’åè¾“å‡ºä»»åŠ¡çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ‰©å±•ï¼Œé‡æ–°å®šä¹‰äº†å¥–åŠ±æœºåˆ¶ä»¥æ¶ˆé™¤éå› æœä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥åŸºäºæ’åçš„tokenæ¦‚ç‡å‡ ä½•å¹³å‡çš„æ’åé‡è¦æ€§æ¯”ç‡ï¼Œä»¥ç¨³å®šç­–ç•¥æ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConvRec-R1åœ¨å…¬å…±çš„Reddit-v2æ•°æ®é›†ä¸Šæ”¶æ•›æ›´å¿«ï¼Œä¸”åœ¨å¬å›ç‡å’ŒNDCGæŒ‡æ ‡ä¸Šä¼˜äºGRPOé£æ ¼çš„åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27224', 'title': 'Mask-to-Height: A YOLOv11-Based Architecture for Joint Building Instance\n  Segmentation and Height Classification from Satellite Imagery', 'url': 'https://huggingface.co/papers/2510.27224', 'abstract': "YOLOv11, an advanced deep learning model, achieves high accuracy in building instance segmentation and height classification from satellite imagery, outperforming earlier models in both detection and inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate building instance segmentation and height classification are critical for urban planning, 3D city modeling, and infrastructure monitoring. This paper presents a detailed analysis of YOLOv11, the recent advancement in the YOLO series of deep learning models, focusing on its application to joint building extraction and discrete height classification from satellite imagery. YOLOv11 builds on the strengths of earlier YOLO models by introducing a more efficient architecture that better combines features at different scales, improves object localization accuracy, and enhances performance in complex urban scenes. Using the DFC2023 Track 2 dataset -- which includes over 125,000 annotated buildings across 12 cities -- we evaluate YOLOv11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLOv11 achieves strong instance segmentation performance with 60.4\\% mAP@50 and 38.3\\% mAP@50--95 while maintaining robust classification accuracy across five predefined height tiers. The model excels in handling occlusions, complex building shapes, and class imbalance, particularly for rare high-rise structures. Comparative analysis confirms that YOLOv11 outperforms earlier multitask frameworks in both detection accuracy and inference speed, making it well-suited for real-time, large-scale urban mapping. This research highlights YOLOv11's potential to advance semantic urban reconstruction through streamlined categorical height modeling, offering actionable insights for future developments in remote sensing and geospatial intelligence.", 'score': 2, 'issue_id': 6765, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '5aa11cd56f9537f4', 'authors': ['Mahmoud El Hussieni', 'BahadÄ±r K. GÃ¼ntÃ¼rk', 'Hasan F. AteÅŸ', 'OÄŸuz HanoÄŸlu'], 'affiliations': ['Huawei Turkiye R&D Center', 'Istanbul Medipol University', 'Ozyegin University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27224.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#architecture', '#cv', '#inference'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'YOLOv11: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ YOLOv11, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ ÑĞµÑ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ YOLO, Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (60.4% mAP@50) Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹. YOLOv11 Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´ĞºĞ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ´Ğ¾Ğ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'YOLOv11: Revolutionizing Urban Mapping with Speed and Precision', 'desc': "The paper introduces YOLOv11, a state-of-the-art deep learning model designed for building instance segmentation and height classification using satellite imagery. It enhances the capabilities of previous YOLO models by implementing a more efficient architecture that improves object localization and performance in complex urban environments. Evaluated on the DFC2023 Track 2 dataset, YOLOv11 demonstrates impressive metrics, achieving 60.4% mAP@50 and 38.3% mAP@50-95, while effectively managing challenges like occlusions and class imbalance. This research underscores YOLOv11's potential to significantly improve urban mapping and geospatial intelligence applications."}, 'zh': {'title': 'YOLOv11ï¼šåŸå¸‚æ˜ å°„çš„æ–°æ ‡æ†', 'desc': 'YOLOv11æ˜¯ä¸€ç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿä»å«æ˜Ÿå›¾åƒä¸­å®ç°é«˜ç²¾åº¦çš„å»ºç­‘å®ä¾‹åˆ†å‰²å’Œé«˜åº¦åˆ†ç±»ã€‚è¯¥æ¨¡å‹åœ¨æ£€æµ‹å’Œæ¨ç†é€Ÿåº¦ä¸Šè¶…è¶Šäº†æ—©æœŸçš„YOLOæ¨¡å‹ï¼Œç‰¹åˆ«é€‚åˆåŸå¸‚è§„åˆ’å’ŒåŸºç¡€è®¾æ–½ç›‘æµ‹ã€‚é€šè¿‡å¯¹DFC2023 Track 2æ•°æ®é›†çš„è¯„ä¼°ï¼ŒYOLOv11åœ¨å®ä¾‹åˆ†å‰²å’Œåˆ†ç±»å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚åŸå¸‚åœºæ™¯æ—¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒYOLOv11åœ¨å®æ—¶å¤§è§„æ¨¡åŸå¸‚æ˜ å°„ä¸­å…·æœ‰å¾ˆå¤§çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26345', 'title': 'MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data', 'url': 'https://huggingface.co/papers/2510.26345', 'abstract': 'Introducing synthetic fallacy data through MisSynth enhances the zero-shot classification performance of large language models in detecting scientific misinformation.  \t\t\t\t\tAI-generated summary \t\t\t\t Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on https://github.com/mxpoliakov/MisSynth.', 'score': 2, 'issue_id': 6765, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '7b2563dc21819dc5', 'authors': ['Mykhailo Poliakov', 'Nadiya Shvai'], 'affiliations': ['National University of Kyiv-Mohyla Academy, Kyiv, Ukraine'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26345.jpg', 'data': {'categories': ['#rag', '#healthcare', '#open_source', '#data', '#training', '#dataset', '#science', '#synthetic'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ MisSynth â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ retrieval-augmented generation Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ»Ñ‘Ğ³ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MISSCI. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaMA 3.1 8B ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° F1-score Ğ½Ğ° 35% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Boosting Misinformation Detection with Synthetic Fallacy Data', 'desc': 'This paper presents MisSynth, a method for generating synthetic fallacy data to improve the performance of large language models (LLMs) in identifying scientific misinformation. By utilizing retrieval-augmented generation (RAG), MisSynth creates fallacy samples that are used to fine-tune LLMs, enhancing their ability to detect misleading claims. The study shows that fine-tuned models, such as LLaMA 3.1 8B, achieve significant improvements in accuracy, with over a 35% increase in F1-score compared to their original versions. This approach demonstrates that synthetic data can effectively augment limited annotated datasets, boosting zero-shot classification capabilities in real-world scenarios.'}, 'zh': {'title': 'åˆæˆæ•°æ®æå‡ç§‘å­¦é”™è¯¯ä¿¡æ¯è¯†åˆ«èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMisSynthçš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ç§‘å­¦é”™è¯¯ä¿¡æ¯æ–¹é¢çš„é›¶-shot åˆ†ç±»æ€§èƒ½ã€‚é€šè¿‡ä½¿ç”¨MISSCIæ•°æ®é›†ï¼Œç ”ç©¶è¡¨æ˜åˆæˆçš„è°¬è¯¯æ ·æœ¬å¯ä»¥æœ‰æ•ˆåœ°ç”¨äºå¯¹æ¨¡å‹è¿›è¡Œè½»é‡çº§å¾®è°ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„LLaMA 3.1 8Bæ¨¡å‹åœ¨MISSCIæµ‹è¯•é›†ä¸Šå–å¾—äº†è¶…è¿‡35%çš„F1åˆ†æ•°æå‡ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨åˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç§‘å­¦é”™è¯¯ä¿¡æ¯ä»»åŠ¡ä¸­çš„åˆ†ç±»èƒ½åŠ›ï¼Œå³ä½¿åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24078', 'title': 'Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained\n  Classification', 'url': 'https://huggingface.co/papers/2510.24078', 'abstract': "A fine-tuning strategy called BOB improves synthetic data quality for low-shot fine-grained classification by conditioning on class-agnostic attributes and marginalizing them during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models are increasingly used for synthetic dataset generation, but generating effective synthetic training data for classification remains challenging. Fine-tuning a T2I model with a few real examples can help improve the quality of synthetic training data; however, it may also cause overfitting and reduce diversity in the generated samples. We propose a fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for fine-grained classification. Given a small set of real examples, we first extract class-agnostic attributes such as scene background and object pose. We then explicitly condition on these attributes during fine-tuning of the T2I model and marginalize them out during generation. This design mitigates overfitting, preserves the T2I model's generative prior, reduces estimation errors, and further minimizes unintended inter-class associations. Extensive experiments across multiple T2I models, backbones, and datasets show that our method achieves state-of-the-art performance in low-shot fine-grained classification when augmented with synthetic data. Concretely, BOB outperforms DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning a CLIP classifier with five real images augmented with 100 synthetic images). In three of the four benchmarks, fine-tuning downstream models with 5 real images augmented with BOB achieves better performance than fine-tuning with 10 real images. Collectively, BOB outperforms prior art in 18 of 24 experimental settings, with 2+% accuracy improvements in 14 of these settings.", 'score': 2, 'issue_id': 6765, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'f7f6b4dfb67921c2', 'authors': ['William Yang', 'Xindi Wu', 'Zhiwei Deng', 'Esin Tureci', 'Olga Russakovsky'], 'affiliations': ['Google DeepMind', 'Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24078.jpg', 'data': {'categories': ['#optimization', '#cv', '#data', '#training', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²: Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ BOB Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ»Ğ°ÑÑĞ¾-Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² (Ñ„Ğ¾Ğ½, Ğ¿Ğ¾Ğ·Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°) Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BOB Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² 18 Ğ¸Ğ· 24 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞº.'}, 'en': {'title': 'Enhancing Synthetic Data Quality with BOB for Fine-Grained Classification', 'desc': 'The paper introduces a fine-tuning strategy called BOB (BeyondOBjects) that enhances the quality of synthetic data for low-shot fine-grained classification tasks. By conditioning on class-agnostic attributes and marginalizing them during the generation process, BOB effectively reduces overfitting and maintains diversity in the generated samples. This approach allows for better utilization of a small number of real examples, leading to improved performance in classification tasks. Experimental results demonstrate that BOB significantly outperforms existing methods, achieving state-of-the-art results across various datasets and models.'}, 'zh': {'title': 'BOBï¼šæå‡åˆæˆæ•°æ®è´¨é‡çš„å¾®è°ƒç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBOBï¼ˆBeyondOBjectsï¼‰çš„å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ä½æ ·æœ¬ç»†ç²’åº¦åˆ†ç±»çš„åˆæˆæ•°æ®è´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ä¸ç±»åˆ«æ— å…³çš„å±æ€§ï¼ˆå¦‚åœºæ™¯èƒŒæ™¯å’Œç‰©ä½“å§¿æ€ï¼‰ï¼Œå¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¯¹è¿™äº›å±æ€§è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆå’Œç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§é—®é¢˜ã€‚BOBåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹è¿™äº›å±æ€§è¿›è¡Œè¾¹é™…åŒ–å¤„ç†ï¼Œä¿æŒäº†ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒåˆ†å¸ƒï¼Œé™ä½äº†ä¼°è®¡è¯¯å·®ï¼Œå¹¶æœ€å°åŒ–äº†ä¸å¿…è¦çš„ç±»é—´å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBOBåœ¨å¤šä¸ªT2Iæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†ä½æ ·æœ¬ç»†ç²’åº¦åˆ†ç±»çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25080', 'title': 'Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response\n  Games', 'url': 'https://huggingface.co/papers/2510.25080', 'abstract': "A modified version of Monopoly Deal, featuring Bounded One-Sided Response Games, is used to study sequential decision-making with Counterfactual Regret Minimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. These games typically fall into three categories based on the flow of control: strictly sequential (players alternate single actions), deterministic response (some actions trigger a fixed outcome), and unbounded reciprocal response (alternating counterplays are permitted). A less-explored but strategically rich structure is the bounded one-sided response, where a player's action briefly transfers control to the opponent, who must satisfy a fixed condition through one or more moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs). We introduce a modified version of Monopoly Deal as a benchmark environment that isolates this dynamic, where a Rent action forces the opponent to choose payment assets. The gold-standard algorithm, Counterfactual Regret Minimization (CFR), converges on effective strategies without novel algorithmic extensions. A lightweight full-stack research platform unifies the environment, a parallelized CFR runtime, and a human-playable web interface. The trained CFR agent and source code are available at https://monopolydeal.ai.", 'score': 1, 'issue_id': 6765, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '3bee6acf461fcfec', 'authors': ['Will Wolf'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.25080.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ°', 'ru': {'title': 'ĞÑĞ²Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€: CFR Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹ Monopoly Deal Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Counterfactual Regret Minimization (CFR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ¸Ğ³Ñ€ - Bounded One-Sided Response Games (BORGs), Ğ³Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ° Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ñ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ñ…Ğ¾Ğ´Ğ°. ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ CFR ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ÑÑ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ, Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CFR-runtime Ğ¸ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Exploring Strategic Depth in Bounded One-Sided Response Games', 'desc': "This paper explores a new type of game called Bounded One-Sided Response Games (BORGs), which allows for unique strategic interactions in sequential decision-making. The authors modify Monopoly Deal to create a benchmark environment that highlights this game structure, where one player's action can temporarily shift control to the opponent. They apply Counterfactual Regret Minimization (CFR), a well-known algorithm in game theory, to develop effective strategies within this framework. The research also provides a platform for further exploration, including a web interface for human players and access to the trained CFR agent."}, 'zh': {'title': 'æ¢ç´¢æœ‰ç•Œå•å‘ååº”åšå¼ˆçš„å†³ç­–ç­–ç•¥', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§ä¿®æ”¹ç‰ˆçš„å„æ–­äº¤æ˜“æ¸¸æˆï¼Œåˆ©ç”¨æœ‰ç•Œå•å‘ååº”åšå¼ˆæ¥åˆ†æé¡ºåºå†³ç­–ã€‚è¯¥æ¸¸æˆé€šè¿‡ç§Ÿé‡‘è¡ŒåŠ¨è¿«ä½¿å¯¹æ‰‹é€‰æ‹©æ”¯ä»˜èµ„äº§ï¼Œå±•ç¤ºäº†æœ‰ç•Œå•å‘ååº”çš„åŠ¨æ€ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨åäº‹å®é—æ†¾æœ€å°åŒ–ï¼ˆCFRï¼‰ç®—æ³•æ¥å¯»æ‰¾æœ‰æ•ˆç­–ç•¥ï¼Œä¸”æ— éœ€æ–°çš„ç®—æ³•æ‰©å±•ã€‚ç ”ç©¶å¹³å°æ•´åˆäº†æ¸¸æˆç¯å¢ƒã€å¹¶è¡ŒCFRè¿è¡Œæ—¶å’Œå¯ä¾›äººç±»ç©å®¶ä½¿ç”¨çš„ç½‘é¡µç•Œé¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22115', 'title': 'Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open\n  Language Foundation', 'url': 'https://huggingface.co/papers/2510.22115', 'abstract': 'Ling 2.0, a reasoning-oriented language model series, achieves high efficiency and accuracy through a Mixture-of-Experts paradigm, sparse activation, and innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.', 'score': 83, 'issue_id': 6780, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': 'd8f4a5a5ca5b930e', 'authors': ['Ling Team', 'Ang Li', 'Ben Liu', 'Binbin Hu', 'Bing Li', 'Bingwei Zeng', 'Borui Ye', 'Caizhi Tang', 'Changxin Tian', 'Chao Huang', 'Chao Zhang', 'Chen Qian', 'Chenchen Ju', 'Chenchen Li', 'Chengfu Tang', 'Chilin Fu', 'Chunshao Ren', 'Chunwei Wu', 'Cong Zhang', 'Cunyin Peng', 'Dafeng Xu', 'Daixin Wang', 'Dalong Zhang', 'Dingnan Jin', 'Dingyuan Zhu', 'Dongke Hu', 'Fangzheng Zhao', 'Feifan Wu', 'Feng Zhu', 'Gangshan Wang', 'Haitao Zhang', 'Hailin Zhao', 'Hanxiao Zhang', 'Hanzi Wang', 'Hao Qian', 'Haoyi Yu', 'Heng Zhang', 'Hongliang Zhang', 'Hongzhi Luan', 'Huirong Dong', 'Huizhong Li', 'Jia Li', 'Jia Liu', 'Jialong Zhu', 'Jian Sha', 'Jianping Wei', 'Jiaolong Yang', 'Jieyue Ma', 'Jiewei Wu', 'Jinjing Huang', 'Jingyun Tian', 'Jingyuan Zhang', 'Jinquan Sun', 'Juanhui Tu', 'Jun Liu', 'Jun Xu', 'Jun Zhou', 'Junjie Ou', 'Junpeng Fang', 'Kaihong Zhang', 'Kaiqin Hu', 'Ke Shi', 'Kun Tang', 'Kunlong Chen', 'Lanyin Mei', 'Lei Liang', 'Lei Xu', 'Libo Zhang', 'Lin Ju', 'Lin Yuan', 'Ling Zhong', 'Lintao Ma', 'Lu Liu', 'Lu Yu', 'Lun Cai', 'Meiqi Zhu', 'Mengying Li', 'Min Chen', 'Minghao Xue', 'Minghong Cai', 'Mingming Yin', 'Peijie Jiang', 'Peilong Zhao', 'Pingping Liu', 'Qian Zhao', 'Qing Cui', 'Qingxiang Huang', 'Qingyuan Yang', 'Quankun Yu', 'Shaowei Wei', 'Shijie Lian', 'Shoujian Zheng', 'Shun Song', 'Shungen Zhang', 'Shuo Zhang', 'Siyuan Li', 'Song Liu', 'Ting Guo', 'Tong Zhao', 'Wanli Gu', 'Weichang Wu', 'Weiguang Han', 'Wenjing Fang', 'Wubin Wang', 'Xiang Shu', 'Xiao Shi', 'Xiaoshun Lan', 'Xiaolu Zhang', 'Xiaqing Sun', 'Xin Zhao', 'Xingyu Lu', 'Xiong Xu', 'Xudong Wang', 'Xudong Wang', 'Xuemin Yang', 'Yajie Yang', 'Yang Xiang', 'Yanzhe Li', 'Yi Zhang', 'Yilong Wang', 'Yingxue Li', 'Yongzhen Guo', 'Yuzhuo Fu', 'Yuanyuan Wang', 'Yue Yang', 'Yue Yu', 'Yufeng Deng', 'Yun Zhang', 'Yunfei Yu', 'Yuqi Zhang', 'Yuxiao He', 'Zengke Gui', 'Zhaoxin Huan', 'Zhaoyang Wang', 'Zhibo Zhu', 'Zhihao Wang', 'Zhiqiang Zhang', 'Zhoufei Wang', 'Zihang Zeng', 'Ziqi Liu', 'Zitao Xuan', 'Zuoli Tang'], 'affiliations': ['Inclusion AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.22115.jpg', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#training', '#rlhf', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞµÑ€Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ling 2.0, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Mixture-of-Experts Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚ 16 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞµĞ¼Ğ¸ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ’ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ reasoning-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸ (CoT) Ğ¸ reinforcement-based fine-tuning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ling 2.0 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ.'}, 'en': {'title': 'Ling 2.0: Revolutionizing Reasoning with Sparse Activation and MoE', 'desc': 'Ling 2.0 is a series of reasoning-oriented language models that utilize a Mixture-of-Experts (MoE) approach to enhance efficiency and accuracy. By employing sparse activation and innovative training methods, these models can scale from 16 billion to 1 trillion parameters while maintaining high computational efficiency. The series includes three models, each designed to optimize reasoning capabilities through coordinated innovations in architecture and training techniques. Ultimately, Ling 2.0 sets a new standard for balancing reasoning accuracy with computational efficiency, paving the way for future advancements in AI models.'}, 'zh': {'title': 'Ling 2.0ï¼šé«˜æ•ˆæ¨ç†çš„æ–°çºªå…ƒ', 'desc': 'Ling 2.0 æ˜¯ä¸€ç³»åˆ—ä»¥æ¨ç†ä¸ºå¯¼å‘çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMixture-of-Experts, MoEï¼‰èŒƒå¼ï¼Œå…·æœ‰é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ç¨€ç–æ¿€æ´»å’Œåˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä»æ•°åäº¿åˆ°ä¸€ä¸‡äº¿å‚æ•°çš„èŒƒå›´å†…æ‰©å±•ï¼Œå¼ºè°ƒé«˜ç¨€ç–æ€§å’Œè·¨å°ºåº¦ä¸€è‡´æ€§ã€‚Ling 2.0 åŒ…å«ä¸‰ç§éæ€è€ƒæ¨¡å‹ï¼Œå…·æœ‰é«˜è¾¾ 7 å€çš„è®¡ç®—æ•ˆç‡ï¼Œç›¸æ¯”äºå¯†é›†æ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚æ•´ä½“è€Œè¨€ï¼ŒLing 2.0 ä¸ºæœªæ¥æ¨ç†å’Œæ€ç»´æ¨¡å‹çš„è¿›æ­¥æä¾›äº†ä¸€ä¸ªä¸€è‡´ã€å¼€æ”¾å’Œé«˜æ•ˆçš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27545', 'title': 'EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities', 'url': 'https://huggingface.co/papers/2510.27545', 'abstract': "EBT-Policy, an energy-based architecture, outperforms diffusion-based policies in robotic tasks by offering improved robustness, reduced computational cost, and emergent zero-shot recovery capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Implicit policies parameterized by generative models, such as Diffusion Policy, have become the standard for policy learning and Vision-Language-Action (VLA) models in robotics. However, these approaches often suffer from high computational cost, exposure bias, and unstable inference dynamics, which lead to divergence under distribution shifts. Energy-Based Models (EBMs) address these issues by learning energy landscapes end-to-end and modeling equilibrium dynamics, offering improved robustness and reduced exposure bias. Yet, policies parameterized by EBMs have historically struggled to scale effectively. Recent work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs to high-dimensional spaces, but their potential for solving core challenges in physically embodied models remains underexplored. We introduce a new energy-based architecture, EBT-Policy, that solves core issues in robotic and real-world settings. Across simulated and real-world tasks, EBT-Policy consistently outperforms diffusion-based policies, while requiring less training and inference computation. Remarkably, on some tasks it converges within just two inference steps, a 50x reduction compared to Diffusion Policy's 100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior models, such as zero-shot recovery from failed action sequences using only behavior cloning and without explicit retry training. By leveraging its scalar energy for uncertainty-aware inference and dynamic compute allocation, EBT-Policy offers a promising path toward robust, generalizable robot behavior under distribution shifts.", 'score': 48, 'issue_id': 6780, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'f8d49ad92ce517b7', 'authors': ['Travis Davies', 'Yiqi Huang', 'Alexi Gladstone', 'Yunxin Liu', 'Xiang Chen', 'Heng Ji', 'Huxian Liu', 'Luhui Hu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27545.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization', '#robotics'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ­Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ĞµĞµ, ÑƒĞ¼Ğ½ĞµĞµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ EBT-Policy â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±ÑƒÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ñ‹ Ğ¸ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'EBT-Policy: Revolutionizing Robotics with Energy-Based Efficiency', 'desc': 'The paper introduces EBT-Policy, an energy-based architecture that enhances robotic task performance compared to traditional diffusion-based policies. It addresses common issues like high computational costs and instability during inference, which often lead to poor performance in changing environments. EBT-Policy demonstrates significant improvements in robustness and efficiency, achieving convergence in just two inference steps, which is a substantial reduction in computation time. Additionally, it showcases unique capabilities such as zero-shot recovery from failed actions, making it a promising solution for real-world robotic applications.'}, 'zh': {'title': 'EBT-Policyï¼šæœºå™¨äººä»»åŠ¡çš„æ–°çªç ´', 'desc': 'EBT-Policyæ˜¯ä¸€ç§åŸºäºèƒ½é‡çš„æ¶æ„ï¼Œåœ¨æœºå™¨äººä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸºäºæ‰©æ•£çš„ç­–ç•¥ã€‚å®ƒé€šè¿‡å­¦ä¹ èƒ½é‡æ™¯è§‚å’Œå»ºæ¨¡å¹³è¡¡åŠ¨æ€ï¼Œæä¾›äº†æ›´å¥½çš„é²æ£’æ€§å’Œé™ä½çš„è®¡ç®—æˆæœ¬ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£ç­–ç•¥ç›¸æ¯”ï¼ŒEBT-Policyåœ¨è®­ç»ƒå’Œæ¨ç†è®¡ç®—ä¸Šéœ€æ±‚æ›´å°‘ï¼Œå¹¶ä¸”åœ¨æŸäº›ä»»åŠ¡ä¸­ä»…éœ€ä¸¤æ­¥æ¨ç†å³å¯æ”¶æ•›ã€‚æ­¤å¤–ï¼ŒEBT-Policyå±•ç°äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œå¦‚åœ¨æ²¡æœ‰æ˜ç¡®é‡è¯•è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»å¤±è´¥çš„åŠ¨ä½œåºåˆ—ä¸­å®ç°é›¶-shotæ¢å¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00086', 'title': 'Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph', 'url': 'https://huggingface.co/papers/2511.00086', 'abstract': 'Agent-REINFORCE optimizes multi-LLM collaboration graphs for test-time scaling, improving sample efficiency and search performance under accuracy and latency constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.', 'score': 41, 'issue_id': 6780, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '5da45ea159eaeff7', 'authors': ['Fali Wang', 'Jihai Chen', 'Shuhua Yang', 'Runxue Bao', 'Tianxiang Zhao', 'Zhiwei Zhang', 'Xianfeng Tang', 'Hui Liu', 'Qi He', 'Suhang Wang'], 'affiliations': ['Amazon', 'Microsoft', 'The Pennsylvania State University', 'University of Pittsburgh'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00086.jpg', 'data': {'categories': ['#architecture', '#agents', '#training', '#inference', '#rl'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ LLM Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ³Ğ´Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€Ğ¾Ğ»Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ€Ñ‘Ğ±Ñ€Ğ° â€” Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-REINFORCE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒÑ‡Ñ‘Ñ‚Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Optimizing Multi-LLM Collaboration for Efficient Inference', 'desc': 'This paper introduces Agent-REINFORCE, a framework designed to optimize the collaboration of multiple large language models (LLMs) during inference, particularly focusing on test-time scaling (TTS). It addresses the challenge of finding the best combinations of models and architectures for specific tasks while adhering to computational constraints. The authors reformulate the optimization problem as a probabilistic graph, where models are represented as nodes and their interactions as edges, allowing for efficient exploration of the solution space. Experimental results demonstrate that Agent-REINFORCE significantly enhances sample efficiency and search performance compared to existing methods, achieving better accuracy and lower latency.'}, 'zh': {'title': 'ä¼˜åŒ–å¤šLLMåä½œå›¾ï¼Œæé«˜è®¡ç®—æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent-REINFORCEçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åä½œå›¾ï¼Œä»¥æé«˜æµ‹è¯•æ—¶çš„è®¡ç®—æ•ˆç‡å’Œæœç´¢æ€§èƒ½ã€‚æˆ‘ä»¬å°†é—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¤šLLMåä½œå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºæ¨¡å‹è§’è‰²å’Œåˆ†é…ï¼Œè¾¹è¡¨ç¤ºä¿¡æ¯æµã€‚é€šè¿‡å°†é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ¦‚ç‡å›¾ä¼˜åŒ–ï¼ŒAgent-REINFORCEèƒ½å¤Ÿæœ‰æ•ˆåœ°æœç´¢æœ€ä½³çš„æ¨¡å‹ç»„åˆå’Œæ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡å’Œæœç´¢æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•å’ŒåŸºäºLLMçš„åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00062', 'title': 'World Simulation with Video Foundation Models for Physical AI', 'url': 'https://huggingface.co/papers/2511.00062', 'abstract': 'Cosmos-Predict2.5 and Cosmos-Transfer2.5 are advanced Physical AI models that unify text, image, and video generation, improve video quality and instruction alignment, and enable Sim2Real and Real2Real world translation with higher fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5times smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.', 'score': 40, 'issue_id': 6780, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '7762bcd0ac3fdf9d', 'authors': ['NVIDIA', ':', 'Arslan Ali', 'Junjie Bai', 'Maciej Bala', 'Yogesh Balaji', 'Aaron Blakeman', 'Tiffany Cai', 'Jiaxin Cao', 'Tianshi Cao', 'Elizabeth Cha', 'Yu-Wei Chao', 'Prithvijit Chattopadhyay', 'Mike Chen', 'Yongxin Chen', 'Yu Chen', 'Shuai Cheng', 'Yin Cui', 'Jenna Diamond', 'Yifan Ding', 'Jiaojiao Fan', 'Linxi Fan', 'Liang Feng', 'Francesco Ferroni', 'Sanja Fidler', 'Xiao Fu', 'Ruiyuan Gao', 'Yunhao Ge', 'Jinwei Gu', 'Aryaman Gupta', 'Siddharth Gururani', 'Imad El Hanafi', 'Ali Hassani', 'Zekun Hao', 'Jacob Huffman', 'Joel Jang', 'Pooya Jannaty', 'Jan Kautz', 'Grace Lam', 'Xuan Li', 'Zhaoshuo Li', 'Maosheng Liao', 'Chen-Hsuan Lin', 'Tsung-Yi Lin', 'Yen-Chen Lin', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Kaichun Mo', 'Seungjun Nah', 'Yashraj Narang', 'Abhijeet Panaskar', 'Lindsey Pavao', 'Trung Pham', 'Morteza Ramezanali', 'Fitsum Reda', 'Scott Reed', 'Xuanchi Ren', 'Haonan Shao', 'Yue Shen', 'Stella Shi', 'Shuran Song', 'Bartosz Stefaniak', 'Shangkun Sun', 'Shitao Tang', 'Sameena Tasmeen', 'Lyne Tchapmi', 'Wei-Cheng Tseng', 'Jibin Varghese', 'Andrew Z. Wang', 'Hao Wang', 'Haoxiang Wang', 'Heng Wang', 'Ting-Chun Wang', 'Fangyin Wei', 'Jiashu Xu', 'Dinghao Yang', 'Xiaodong Yang', 'Haotian Ye', 'Seonghyeon Ye', 'Xiaohui Zeng', 'Jing Zhang', 'Qinsheng Zhang', 'Kaiwen Zheng', 'Andrew Zhu', 'Yuke Zhu'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00062.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#architecture', '#multimodal', '#training', '#robotics', '#dataset', '#synthetic'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Cosmos-Predict2.5 Ğ¸ Cosmos-Transfer2.5 â€” Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ AI Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ flow-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° 200 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸. Cosmos-Transfer2.5 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ğ¸ Sim2Real Ğ¸ Real2Real, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ² 3.5 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸Ğº. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Unifying Media Generation for Enhanced Physical AI', 'desc': 'Cosmos-Predict2.5 and Cosmos-Transfer2.5 are innovative Physical AI models designed to enhance the generation of text, images, and videos. They utilize a flow-based architecture to integrate various forms of media generation into a single framework, improving video quality and aligning instructions more effectively. The models are trained on a vast dataset of 200 million video clips and utilize reinforcement learning for post-training, resulting in significant advancements over previous versions. These tools facilitate reliable synthetic data generation and support robotics and autonomous systems through improved Sim2Real and Real2Real translations.'}, 'zh': {'title': 'ç»Ÿä¸€ç”Ÿæˆï¼Œæå‡ç‰©ç†AIçš„æ™ºèƒ½', 'desc': 'Cosmos-Predict2.5å’ŒCosmos-Transfer2.5æ˜¯å…ˆè¿›çš„ç‰©ç†äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œèƒ½å¤Ÿç»Ÿä¸€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚å®ƒä»¬é€šè¿‡æµå¼æ¶æ„å®ç°äº†æ›´é«˜çš„è§†é¢‘è´¨é‡å’ŒæŒ‡ä»¤å¯¹é½ï¼Œå¹¶æ”¯æŒSim2Realå’ŒReal2Realçš„ä¸–ç•Œè½¬æ¢ã€‚Cosmos-Predict2.5åœ¨200Mç²¾å¿ƒæŒ‘é€‰çš„è§†é¢‘ç‰‡æ®µä¸Šè®­ç»ƒï¼Œå¹¶é€šè¿‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚è¿™äº›æ¨¡å‹ä¸ºæœºå™¨äººå’Œè‡ªä¸»ç³»ç»Ÿçš„åˆæˆæ•°æ®ç”Ÿæˆã€ç­–ç•¥è¯„ä¼°å’Œé—­ç¯ä»¿çœŸæä¾›äº†æ›´å¯é çš„å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01295', 'title': 'UniREditBench: A Unified Reasoning-based Image Editing Benchmark', 'url': 'https://huggingface.co/papers/2511.01295', 'abstract': 'UniREditBench is a unified benchmark for reasoning-based image editing that addresses limitations in existing benchmarks by including multi-object interactions, game-world scenarios, and multimodal dual-reference evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.', 'score': 37, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '4d5c971763d4c43e', 'authors': ['Feng Han', 'Yibin Wang', 'Chenglin Li', 'Zheming Liang', 'Dianyi Wang', 'Yang Jiao', 'Zhipeng Wei', 'Chao Gong', 'Cheng Jin', 'Jingjing Chen', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'UC Berkeley', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01295.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ UniREditBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2700 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Image Editing Evaluation with UniREditBench', 'desc': 'UniREditBench is a new benchmark designed to evaluate image editing models based on their reasoning capabilities. It addresses the shortcomings of existing benchmarks by incorporating multi-object interactions and scenarios from game worlds, which are often overlooked. The benchmark includes 2,700 samples and uses a dual-reference evaluation method, combining textual and image references for more accurate assessments. Additionally, it features a large synthetic dataset with reasoning annotations, allowing for better training and evaluation of image editing models.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„å›¾åƒç¼–è¾‘æ–°åŸºå‡†', 'desc': 'UniREditBenchæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œç”¨äºåŸºäºæ¨ç†çš„å›¾åƒç¼–è¾‘è¯„ä¼°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†çš„å±€é™æ€§ã€‚å®ƒåŒ…å«å¤šå¯¹è±¡äº¤äº’ã€æ¸¸æˆä¸–ç•Œåœºæ™¯å’Œå¤šæ¨¡æ€åŒé‡å‚è€ƒè¯„ä¼°ï¼Œæ¶µç›–2700ä¸ªç²¾å¿ƒç­–åˆ’çš„æ ·æœ¬ã€‚è¯¥åŸºå‡†é€šè¿‡å¼•å…¥æ–‡æœ¬å’ŒçœŸå®å›¾åƒå‚è€ƒï¼Œæé«˜äº†è¯„ä¼°çš„å¯é æ€§ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤§å‹åˆæˆæ•°æ®é›†UniREdit-Data-100Kï¼Œæä¾›é«˜è´¨é‡çš„æ¨ç†æ³¨é‡Šã€‚é€šè¿‡å¯¹å›¾åƒç¼–è¾‘æ¨¡å‹çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨ä¸åŒæ–¹é¢çš„ä¼˜ç¼ºç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24788', 'title': 'The Underappreciated Power of Vision Models for Graph Structural\n  Understanding', 'url': 'https://huggingface.co/papers/2510.24788', 'abstract': "Vision models outperform Graph Neural Networks on tasks requiring global structural understanding and scale-invariant reasoning, as demonstrated by the new GraphAbstract benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.", 'score': 35, 'issue_id': 6780, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '5e11d715260f761b', 'authors': ['Xinjian Zhao', 'Wei Pang', 'Zhongkai Xue', 'Xiangru Jian', 'Lei Zhang', 'Yaoyao Xu', 'Xiaozhuang Song', 'Shu Wu', 'Tianshu Yu'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Institute of Automation, Chinese Academy of Sciences', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24788.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#interpretability', '#cv', '#dataset'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ GNN Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ° Ğ½Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ·Ğ»Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GraphAbstract, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°: Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ°Ñ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ñ‹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ GNN Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ³Ñ€Ğ°Ñ„Ğ°.'}, 'en': {'title': 'Vision Models: The New Frontier in Graph Understanding', 'desc': 'This paper explores the effectiveness of vision models in understanding graph structures, showing that they can outperform Graph Neural Networks (GNNs) in tasks that require a global understanding of graphs. Unlike GNNs, which use a bottom-up approach, vision models capture overall structures first, similar to human perception. The authors introduce a new benchmark called GraphAbstract to assess models on their ability to recognize global properties of graphs, such as symmetry and connectivity. The findings suggest that vision models are better suited for tasks needing holistic structural awareness, especially as graph sizes increase, highlighting their potential for future graph-based applications.'}, 'zh': {'title': 'è§†è§‰æ¨¡å‹è¶…è¶Šå›¾ç¥ç»ç½‘ç»œçš„å…¨å±€ç†è§£èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†è§‰æ¨¡å‹åœ¨å›¾ç»“æ„ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å…¨å±€ç»“æ„ç†è§£å’Œå°ºåº¦ä¸å˜æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰æ¨¡å‹åœ¨æ–°çš„GraphAbstractåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œå°½ç®¡ä¸¤è€…çš„å­¦ä¹ æ¨¡å¼æˆªç„¶ä¸åŒã€‚è§†è§‰æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯†åˆ«å›¾çš„å…¨å±€å±æ€§ï¼Œå¦‚ç»„ç»‡æ¨¡å¼ã€å¯¹ç§°æ€§å’Œè¿æ¥å¼ºåº¦ï¼Œè€ŒGNNsåœ¨å¤„ç†å¤§è§„æ¨¡å›¾æ—¶è¡¨ç°ä¸ä½³ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è§†è§‰æ¨¡å‹åœ¨å›¾ç»“æ„ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å…¨å±€æ‹“æ‰‘æ„è¯†çš„ä»»åŠ¡ä¸­ï¼Œæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01678', 'title': 'UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback', 'url': 'https://huggingface.co/papers/2511.01678', 'abstract': 'UniLumos, a unified relighting framework, enhances physical plausibility by integrating RGB-space geometry feedback into a flow matching backbone, achieving state-of-the-art results with improved consistency and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom.', 'score': 34, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '661f442fcb296f08', 'authors': ['Ropeway Liu', 'Hangjie Yuan', 'Bo Dong', 'Jiazheng Xing', 'Jinwang Wang', 'Rui Zhao', 'Yan Xing', 'Weihua Chen', 'Fan Wang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01678.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#cv', '#diffusion', '#multimodal', '#dataset'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'UniLumos â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ flow matching Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸Ğ· RGB-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² (Ğ¿ĞµÑ€ĞµÑĞ²ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸, Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ¸) Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ path consistency learning, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ LumosBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UniLumos: Realistic Relighting Made Fast and Accurate', 'desc': 'UniLumos is a unified framework designed for relighting images and videos, enhancing the realism of lighting effects by integrating RGB-space geometry feedback into a flow matching backbone. This approach addresses common issues in existing models, such as unrealistic highlights and misaligned shadows, by aligning lighting effects with the actual scene structure using depth and normal maps. To improve efficiency, UniLumos employs path consistency learning, which allows for effective supervision even with fewer training steps, reducing computational costs. Additionally, the framework introduces a structured annotation protocol and a benchmark called LumosBench to evaluate and improve the controllability of lighting attributes in relighting tasks.'}, 'zh': {'title': 'UniLumosï¼šé‡å…‰é¢†åŸŸçš„é€Ÿåº¦ä¸è´¨é‡é©å‘½', 'desc': 'UniLumosæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é‡å…‰æ¡†æ¶ï¼Œé€šè¿‡å°†RGBç©ºé—´å‡ ä½•åé¦ˆæ•´åˆåˆ°æµåŒ¹é…éª¨å¹²ç½‘ç»œä¸­ï¼Œå¢å¼ºäº†ç‰©ç†åˆç†æ€§ã€‚è¯¥æ¡†æ¶åœ¨å›¾åƒå’Œè§†é¢‘çš„é‡å…‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹é½å…‰ç…§æ•ˆæœä¸åœºæ™¯ç»“æ„ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒUniLumosé‡‡ç”¨äº†è·¯å¾„ä¸€è‡´æ€§å­¦ä¹ ï¼Œä½¿å¾—åœ¨å°‘æ­¥è®­ç»ƒä¸‹ä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆçš„ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniLumosåœ¨é‡å…‰è´¨é‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å›¾åƒå’Œè§†é¢‘é‡å…‰é€Ÿåº¦ä¸Šæå‡äº†20å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01163', 'title': 'ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal\n  Generation', 'url': 'https://huggingface.co/papers/2511.01163', 'abstract': 'ROVER is a benchmark that evaluates reciprocal cross-modal reasoning in unified multimodal models, showing that cross-modal interactions significantly impact visual generation quality and that models struggle with symbolic reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.', 'score': 31, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'b7660e491d21d85e', 'authors': ['Yongyuan Liang', 'Wei Chow', 'Feng Li', 'Ziqiao Ma', 'Xiyao Wang', 'Jiageng Mao', 'Jiuhai Chen', 'Jiatao Gu', 'Yue Wang', 'Furong Huang'], 'affiliations': ['The Hong Kong University of Science and Technology', 'University of Maryland, College Park', 'University of Michigan', 'University of Pennsylvania', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01163.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#dataset', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ', 'desc': 'ROVER - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1312 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ 1876 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ - Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 17 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¿Ğ»ĞµÑ‚Ñ‘Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸: Ğ¾Ğ½Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ±ÑƒĞºĞ²Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Unlocking the Power of Cross-Modal Reasoning in AI', 'desc': 'ROVER is a benchmark designed to evaluate how well unified multimodal models can perform reciprocal cross-modal reasoning, which is the ability to use one type of data (like text) to improve understanding or generation of another type (like images). The study reveals that cross-modal interactions are crucial for enhancing the quality of visual outputs, indicating that models that integrate reasoning across modalities perform better than those that do not. Additionally, the research shows that while models can handle literal interpretations of visual data, they struggle with more abstract symbolic reasoning tasks, leading to poorer performance in those areas. Overall, ROVER emphasizes the importance of testing and improving reciprocal reasoning capabilities in multimodal AI systems.'}, 'zh': {'title': 'è·¨æ¨¡æ€æ¨ç†ï¼šå…¨æ¨¡æ€ç”Ÿæˆçš„å…³é”®', 'desc': 'ROVERæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„äº’æƒ è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè·¨æ¨¡æ€äº¤äº’å¯¹è§†è§‰ç”Ÿæˆè´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œè€Œæ¨¡å‹åœ¨ç¬¦å·æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚ROVERåŒ…å«1312ä¸ªä»»åŠ¡ï¼ŒåŸºäº1876å¼ å›¾åƒï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹å¦‚ä½•åˆ©ç”¨ä¸€ç§æ¨¡æ€æ¥æŒ‡å¯¼æˆ–éªŒè¯å¦ä¸€ç§æ¨¡æ€çš„è¾“å‡ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè·¨æ¨¡æ€æ¨ç†æ˜¯å®ç°çœŸæ­£çš„å…¨æ¨¡æ€ç”Ÿæˆçš„å…³é”®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24794', 'title': 'MR-Align: Meta-Reasoning Informed Factuality Alignment for Large\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2510.24794', 'abstract': "MR-ALIGN, a Meta-Reasoning informed alignment framework, enhances the factuality of large reasoning models by aligning their reasoning process, improving accuracy and reducing misleading reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs.", 'score': 31, 'issue_id': 6780, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'b784ce4c6ba8a415', 'authors': ['Xinming Wang', 'Jian Xu', 'Bin Yu', 'Sheng Lian', 'Hongzhu Yi', 'Yi Chen', 'Yingjian Zhu', 'Boran Wang', 'Hongming Yang', 'Han Hu', 'Xu-Yao Zhang', 'Cheng-Lin Liu'], 'affiliations': ['Harbin Institute of Technology', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, UCAS', 'School of Computer Science and Technology, UCAS', 'Tencent', 'Zhongguancun Academy'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24794.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²', 'desc': 'MR-ALIGN â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ°Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºtÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¸Ñ… Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞºÑ€ĞµĞ¿Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºtÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Aligning Reasoning for Enhanced Factuality in AI Models', 'desc': 'The paper introduces MR-ALIGN, a framework designed to improve the factual accuracy of large reasoning models (LRMs) by aligning their reasoning processes. It identifies a common issue where models recognize correct facts during reasoning but fail to integrate them into their final answers, leading to inaccuracies. MR-ALIGN addresses this by using meta-reasoning to create a reward system that encourages correct reasoning patterns and discourages faulty ones. Through empirical testing, the framework demonstrates significant improvements in both accuracy and truthfulness across various factual question-answering datasets.'}, 'zh': {'title': 'æå‡æ¨ç†æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§', 'desc': 'MR-ALIGNæ˜¯ä¸€ç§åŸºäºå…ƒæ¨ç†çš„å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¨¡å‹æ¨ç†è¿‡ç¨‹è¿›è¡Œå¯¹é½ï¼Œæ”¹å–„äº†æ¨¡å‹åœ¨å¤„ç†ä¾èµ–è¯æ®çš„äº‹å®é—®é¢˜æ—¶çš„è¡¨ç°ã€‚MR-ALIGNé‡åŒ–äº†æ¨¡å‹æ€ç»´è¿‡ç¨‹ä¸­çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªéšå¼å¥–åŠ±æœºåˆ¶ï¼Œä»¥å¼ºåŒ–æœ‰ç›Šçš„æ¨ç†æ¨¡å¼ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒMR-ALIGNåœ¨å¤šä¸ªäº‹å®é—®ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’ŒçœŸå®æ€§ï¼Œå‡å°‘äº†è¯¯å¯¼æ€§æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26236', 'title': 'PHUMA: Physically-Grounded Humanoid Locomotion Dataset', 'url': 'https://huggingface.co/papers/2510.26236', 'abstract': 'PHUMA, a large-scale and physically reliable humanoid locomotion dataset, improves motion imitation by addressing physical artifacts in human video data.  \t\t\t\t\tAI-generated summary \t\t\t\t Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.', 'score': 28, 'issue_id': 6780, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'c99012c8c1902039', 'authors': ['Kyungmin Lee', 'Sibeen Kim', 'Minho Park', 'Hyunseung Kim', 'Dongyoon Hwang', 'Hojoon Lee', 'Jaegul Choo'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26236.jpg', 'data': {'categories': ['#dataset', '#data', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° PHUMA â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² (Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ», ÑĞºĞ¾Ğ»ÑŒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ³), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼Ğ¸: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¾Ğ² ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¼ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ğ»ÑŒĞ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° PHUMA, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼.'}, 'en': {'title': 'PHUMA: Reliable Motion Imitation for Humanoids', 'desc': 'PHUMA is a new dataset designed to improve humanoid locomotion by providing high-quality motion data that is physically reliable. It addresses the common issues found in existing datasets, such as floating and foot skating, which can disrupt motion imitation. By using a large number of human videos and applying physics-based corrections, PHUMA ensures that the motions are realistic and adhere to physical constraints. The dataset has been shown to enhance the performance of imitation learning algorithms, outperforming previous datasets in generating diverse and stable humanoid movements.'}, 'zh': {'title': 'PHUMAï¼šæå‡äººå½¢è¿åŠ¨æ¨¡ä»¿çš„ç‰©ç†å¯é æ€§', 'desc': 'PHUMAæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ä¸”ç‰©ç†å¯é çš„äººå½¢è¿åŠ¨æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡è§£å†³äººç±»è§†é¢‘æ•°æ®ä¸­çš„ç‰©ç†ä¼ªå½±æ¥æ”¹å–„è¿åŠ¨æ¨¡ä»¿ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºé«˜è´¨é‡çš„è¿åŠ¨æ•æ‰æ•°æ®é›†ï¼Œä½†è¿™äº›æ•°æ®é›†ç¨€ç¼ºä¸”æ˜‚è´µï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œå¤šæ ·æ€§ã€‚PHUMAé€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ•´ç†å’Œç‰©ç†çº¦æŸçš„é‡å®šå‘ï¼Œåˆ©ç”¨å¤§è§„æ¨¡çš„äººç±»è§†é¢‘ï¼ŒåŒæ—¶æ¶ˆé™¤äº†æµ®åŠ¨ã€ç©¿é€å’Œè„šæ»‘ç­‰ç‰©ç†ä¼ªå½±ã€‚ç»è¿‡è¯„ä¼°ï¼ŒPHUMAåœ¨æ¨¡ä»¿æœªè§è¿åŠ¨å’Œè·¯å¾„è·Ÿéšä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ ·åŒ–è¿åŠ¨çš„æ¨¡ä»¿èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01266', 'title': 'MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls', 'url': 'https://huggingface.co/papers/2511.01266', 'abstract': 'MotionStream enables real-time video generation with sub-second latency and up to 29 FPS by distilling a text-to-video model with motion control into a causal student using Self Forcing with Distribution Matching Distillation and sliding-window causal attention with attention sinks.  \t\t\t\t\tAI-generated summary \t\t\t\t Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.', 'score': 27, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '41fc72b88025813c', 'authors': ['Joonghyuk Shin', 'Zhengqi Li', 'Richard Zhang', 'Jun-Yan Zhu', 'Jaesik Park', 'Eli Shechtman', 'Xun Huang'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'Seoul National University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01266.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MotionStream â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼ĞµĞ½ĞµĞµ ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ¸ Ğ´Ğ¾ 29 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ (causal) ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Self Forcing Ñ Distribution Matching Distillation. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ attention sinks, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ€Ğ¸ÑÑƒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Real-Time Video Generation with Motion Control', 'desc': 'MotionStream is a novel approach for real-time video generation that achieves sub-second latency and up to 29 frames per second (FPS) by transforming a text-to-video model into a causal student model. It utilizes Self Forcing with Distribution Matching Distillation to enable fast inference while maintaining high video quality and motion control. The method addresses challenges such as bridging the domain gap for infinite video lengths and preventing error accumulation during generation. By implementing sliding-window causal attention and attention sinks, MotionStream allows for interactive video creation, enabling users to manipulate trajectories and camera angles in real-time.'}, 'zh': {'title': 'MotionStreamï¼šå®æ—¶è§†é¢‘ç”Ÿæˆçš„æœªæ¥', 'desc': 'MotionStream æ˜¯ä¸€ç§å®æ—¶è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„å»¶è¿Ÿå†…ç”Ÿæˆé«˜è¾¾ 29 å¸§æ¯ç§’çš„è§†é¢‘ã€‚å®ƒé€šè¿‡å°†æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸è¿åŠ¨æ§åˆ¶ç›¸ç»“åˆï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘å¼ºåˆ¶å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦çš„æ–¹æ³•ï¼Œå°†åŒå‘æ•™å¸ˆæ¨¡å‹è’¸é¦ä¸ºå› æœå­¦ç”Ÿæ¨¡å‹ã€‚è¯¥æ–¹æ³•è§£å†³äº†ç”Ÿæˆé•¿æ—¶é—´è§†é¢‘æ—¶çš„å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»æœ‰é™é•¿åº¦è®­ç»ƒåˆ°æ— é™æ—¶é—´èŒƒå›´çš„é¢†åŸŸå·®è·ã€ä¿æŒé«˜è´¨é‡ä»¥é˜²æ­¢é”™è¯¯ç´¯ç§¯ï¼Œä»¥åŠåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ç»´æŒå¿«é€Ÿæ¨ç†ã€‚é€šè¿‡å¼•å…¥æ»‘åŠ¨çª—å£å› æœæ³¨æ„åŠ›å’Œæ³¨æ„åŠ›æ±‡ï¼ŒMotionStream å®ç°äº†é«˜æ•ˆçš„å®æ—¶æµåª’ä½“ç”Ÿæˆï¼Œç”¨æˆ·å¯ä»¥å®æ—¶æ§åˆ¶è¿åŠ¨è½¨è¿¹å’Œç›¸æœºï¼Œäº«å—äº’åŠ¨ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00279', 'title': 'LongCat-Flash-Omni Technical Report', 'url': 'https://huggingface.co/papers/2511.00279', 'abstract': 'LongCat-Flash-Omni, a 560 billion parameter omni-modal model, achieves real-time audio-visual interaction through curriculum-inspired training and modality-decoupled parallelism.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.', 'score': 22, 'issue_id': 6780, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'e14acc53d087c71b', 'authors': ['Meituan LongCat Team', 'Bairui Wang', 'Bayan', 'Bin Xiao', 'Bo Zhang', 'Bolin Rong', 'Borun Chen', 'Chang Wan', 'Chao Zhang', 'Chen Huang', 'Chen Chen', 'Chen Chen', 'Chengxu Yang', 'Chengzuo Yang', 'Cong Han', 'Dandan Peng', 'Delian Ruan', 'Detai Xin', 'Disong Wang', 'Dongchao Yang', 'Fanfan Liu', 'Fengjiao Chen', 'Fengyu Yang', 'Gan Dong', 'Gang Huang', 'Gang Xu', 'Guanglu Wan', 'Guoqiang Tan', 'Guoqiao Yu', 'Haibo Qiu', 'Hao Lu', 'Hongbo Liu', 'Hongyu Xiang', 'Jiaheng Wu', 'Jian Yang', 'Jiaxing Liu', 'Jing Huang', 'Jingang Wang', 'Jinrui Ding', 'Juchao Jiang', 'Jun Kuang', 'Jun Wang', 'Junhui Mei', 'Ke Ding', 'Kefeng Zhang', 'Lei Chen', 'Liang Shi', 'Limeng Qiao', 'Liming Zheng', 'Lin Ma', 'Liuyang Guo', 'Liya Ma', 'Luying Sun', 'Man Gao', 'Mengshen Zhu', 'Miao Cao', 'Minliang Lin', 'Nuo Xu', 'Peng Shi', 'Qi Zhang', 'Qian Fang', 'Qian Wang', 'Qian Yang', 'Quanxiu Wang', 'Rongxiang Weng', 'Rongxin Guo', 'Ruoxuan Liang', 'Senbin Yang', 'Shanbo Xu', 'Shanglin Lei', 'Shengze Ye', 'Shimin Chen', 'Shuaiqi Chen', 'Shujie Hu', 'Shuo Li', 'Siqi Yang', 'Siyu Xu', 'Siyu Ren', 'Song Li', 'Songxiang Liu', 'Tianhao Bai', 'Tianye Dai', 'Wei Hong', 'Wei Wang', 'Weixiao Zhao', 'Wengang Cao', 'Wenlong Zhu', 'Wenlong He', 'Xi Su', 'Xi Nan', 'Xiaohan Zhao', 'Xiaohao Wang', 'Xiaoyu Zhao', 'Xiaoyu Wang', 'Xiaoyu Li', 'Xin Pan', 'Xin Chen', 'Xiusong Sun', 'Xu Xiang', 'Xudong Xing', 'Xuezhi Cao', 'Xunliang Cai', 'Yang Yang', 'Yanli Tan', 'Yao Yao', 'Yerui Sun', 'Yi Chen', 'Yifan Lu', 'Yin Gong', 'Yining Zhang', 'Yitian Chen', 'Yiyang Gan', 'Yuchen Tang', 'Yuchen Xie', 'Yueqian Wang', 'Yuewen Zheng', 'Yufei Zhang', 'Yufeng Zhong', 'Yulei Qian', 'Yuqi Peng', 'Yuqian Li', 'Yuwei Jiang', 'Zeyang Hu', 'Zheng Zhang', 'Zhengkun Tian', 'Zhiqing Hong', 'Zhixiong Zeng', 'Zhuqi Mi', 'Ziran Li', 'Ziwen Wang', 'Ziyi Zhao', 'Ziyuan Zhuang', 'Zizhe Zhao'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00279.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#architecture', '#audio', '#long_context', '#multimodal', '#training', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ÑĞ±Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LongCat-Flash-Omni, Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 560 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 27 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ curriculum learning - Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğº Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğº Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ 90% ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Real-Time Audio-Visual Interaction with 560B Parameters!', 'desc': 'LongCat-Flash-Omni is a cutting-edge omni-modal model with 560 billion parameters that excels in real-time audio-visual interactions. It employs a curriculum-inspired training approach, gradually increasing task complexity to enhance its multimodal capabilities while ensuring strong performance in individual modalities. The model utilizes a Shortcut-connected Mixture-of-Experts architecture, allowing efficient processing with minimal latency. With its innovative modality-decoupled parallelism, LongCat-Flash-Omni achieves high throughput and sets new benchmarks in both omni-modal and modality-specific tasks.'}, 'zh': {'title': 'å®æ—¶éŸ³é¢‘-è§†è§‰äº¤äº’çš„å…¨æ–°çªç ´', 'desc': 'LongCat-Flash-Omni æ˜¯ä¸€ä¸ªæ‹¥æœ‰5600äº¿å‚æ•°çš„å…¨æ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶çš„éŸ³é¢‘-è§†è§‰äº¤äº’ã€‚å®ƒé‡‡ç”¨äº†åŸºäºè¯¾ç¨‹çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œä»ç®€å•åˆ°å¤æ‚çš„æ¨¡æ€åºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼Œæå‡äº†å¤šæ¨¡æ€èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç»“åˆäº†é«˜æ€§èƒ½çš„æ··åˆä¸“å®¶æ¶æ„å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€æ„ŸçŸ¥æ¨¡å—ï¼Œå°½ç®¡å‚æ•°é‡å·¨å¤§ï¼Œä½†ä»èƒ½å®ç°ä½å»¶è¿Ÿçš„å®æ—¶äº¤äº’ã€‚é€šè¿‡åˆ›æ–°çš„æ¨¡æ€è§£è€¦å¹¶è¡Œæ–¹æ¡ˆï¼ŒLongCat-Flash-Omni åœ¨å¤§è§„æ¨¡å¤šæ¨¡æ€è®­ç»ƒä¸­å±•ç°å‡ºå“è¶Šçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27363', 'title': 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use', 'url': 'https://huggingface.co/papers/2510.27363', 'abstract': 'ToolScope, an agentic framework for multimodal large language models, enhances visual question answering by integrating external tools and achieving significant performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a "telescope", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.', 'score': 22, 'issue_id': 6780, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'a99cbdb0fc599728', 'authors': ['Mengjie Deng', 'Guanting Dong', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27363.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#architecture', '#agents', '#multimodal', '#reasoning'], 'emoji': 'ğŸ”­', 'ru': {'title': 'Ğ¢ĞµĞ»ĞµÑĞºĞ¾Ğ¿ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ: Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… MLLMs', 'desc': 'ToolScope â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ), Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ (Perceive tool) Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (VQA 2.0, ScienceQA, MAT-Search Ğ¸ MathVista) Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 6.69% Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Enhancing VQA with ToolScope: A New Era for Multimodal Reasoning', 'desc': 'ToolScope is a framework designed to enhance multimodal large language models (MLLMs) for visual question answering (VQA) by integrating external tools. It addresses the challenge of effectively utilizing multimodal information during reasoning by combining global planning with local perception. The framework includes three main components: the Global Navigator for strategic guidance, the Agentic Executor for iterative tool integration, and the Response Synthesizer for coherent output. ToolScope has shown significant performance improvements, achieving an average increase of 6.69% across various VQA benchmarks.'}, 'zh': {'title': 'ToolScopeï¼šæå‡è§†è§‰é—®ç­”çš„æ–°æ¡†æ¶', 'desc': 'ToolScopeæ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·æ¥å¢å¼ºè§†è§‰é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†å¤šæ¨¡æ€ä¿¡æ¯å¤æ‚æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä½¿å¾—æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿçµæ´»é«˜æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·ã€‚ToolScopeåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šå…¨çƒå¯¼èˆªå™¨ã€æ™ºèƒ½æ‰§è¡Œå™¨å’Œå“åº”åˆæˆå™¨ï¼Œåˆ†åˆ«è´Ÿè´£æä¾›æˆ˜ç•¥æŒ‡å¯¼ã€å¢å¼ºå±€éƒ¨æ„ŸçŸ¥å’Œç»„ç»‡æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡åœ¨å¤šä¸ªè§†è§‰é—®ç­”åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒToolScopeå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹³å‡æ€§èƒ½æå‡è¾¾6.69%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00602', 'title': 'OpenSIR: Open-Ended Self-Improving Reasoner', 'url': 'https://huggingface.co/papers/2511.00602', 'abstract': "OpenSIR is a self-play framework that enables large language models to improve their reasoning abilities through open-ended problem generation and solving without external supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) reasoning through reinforcement learning rely on annotated datasets for verifiable rewards, which may limit models' ability to surpass human-level performance. While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner (OpenSIR), a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. To generate novel problems, OpenSIR optimises for both difficulty and diversity, rewarding problems that challenge appropriately while exploring distinct concepts, enabling open-ended mathematical discovery. Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.", 'score': 20, 'issue_id': 6780, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '5e823c8fb3df35f9', 'authors': ['Wai-Chung Kwan', 'Joshua Ong Jun Leang', 'Pavlos Vougiouklis', 'Jeff Z. Pan', 'Marco Valentino', 'Pasquale Minervini'], 'affiliations': ['Huawei Technologies Research & Development (UK) Limited', 'Imperial College London', 'Miniml.AI', 'University of Edinburgh', 'University of Sheffield'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00602.jpg', 'data': {'categories': ['#small_models', '#open_source', '#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'OpenSIR â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡ĞµÑ€ĞµĞ´ÑƒÑ Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ·Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ¿ÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒÑÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Llama-3.2-3B-Instruct Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ 73.9 Ğ´Ğ¾ 78.3 Ğ½Ğ° GSM8K, Ğ° Gemma-2-2B-Instruct Ñ 38.5 Ğ´Ğ¾ 58.7 Ğ½Ğ° Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering LLMs through Self-Play for Open-Ended Learning', 'desc': 'OpenSIR is a self-play framework designed to enhance the reasoning capabilities of large language models (LLMs) by allowing them to generate and solve problems independently. Unlike traditional methods that rely on annotated datasets for reinforcement learning, OpenSIR enables LLMs to alternate between teacher and student roles, fostering an environment of open-ended learning without external supervision. The framework focuses on generating problems that are both challenging and diverse, promoting mathematical discovery and improving model performance on various benchmarks. As a result, models like Llama-3.2-3B-Instruct and Gemma-2-2B-Instruct show significant advancements in their problem-solving abilities, demonstrating the effectiveness of self-play in LLM training.'}, 'zh': {'title': 'å¼€æ”¾å¼è‡ªæˆ‘æå‡æ¨ç†å™¨ï¼šè‡ªä¸»å­¦ä¹ çš„æ–°æ–¹æ³•', 'desc': 'OpenSIRæ˜¯ä¸€ç§è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆå’Œè§£å†³å¼€æ”¾æ€§é—®é¢˜æ¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚è¯¥æ–¹æ³•é€šè¿‡äº¤æ›¿æ‰®æ¼”æ•™å¸ˆå’Œå­¦ç”Ÿè§’è‰²ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå’Œè§£å†³æ–°é—®é¢˜ï¼Œé¿å…äº†å¯¹å¤–éƒ¨éªŒè¯è€…çš„ä¾èµ–ã€‚OpenSIRåœ¨é—®é¢˜ç”Ÿæˆæ—¶ä¼˜åŒ–éš¾åº¦å’Œå¤šæ ·æ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢ä¸åŒçš„æ¦‚å¿µï¼Œä»è€Œå®ç°å¼€æ”¾å¼çš„æ•°å­¦å‘ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenSIRæ˜¾è‘—æå‡äº†æŒ‡ä»¤æ¨¡å‹çš„è¡¨ç°ï¼Œå±•ç¤ºäº†å…¶åœ¨ä»åŸºç¡€åˆ°é«˜çº§æ•°å­¦çš„è‡ªä¸»å­¦ä¹ èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27571', 'title': 'Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum', 'url': 'https://huggingface.co/papers/2510.27571', 'abstract': "A framework combining a diagnostic benchmark, data synthesis, and a modality pyramid curriculum achieves state-of-the-art zero-shot generalization in video retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.", 'score': 17, 'issue_id': 6780, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'cde2718fd05fefd7', 'authors': ['Zhuoning Guo', 'Mingxin Li', 'Yanzhao Zhang', 'Dingkun Long', 'Pengjun Xie', 'Xiaowen Chu'], 'affiliations': ['AI Thrust, HKUST(GZ)', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27571.jpg', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ ÑƒĞ·ĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº (UVRB) Ğ¸Ğ· 16 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ» 1,55 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²ĞµĞ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Modality Pyramid, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ General Video Embedder (GVE) Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ĞµĞ¹ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ².'}, 'en': {'title': 'Unlocking Universal Video Retrieval with Innovative Frameworks', 'desc': 'This paper presents a new framework for video retrieval that enhances zero-shot generalization by integrating evaluation, data synthesis, and a training curriculum. It introduces the Universal Video Retrieval Benchmark (UVRB), which consists of 16 datasets aimed at identifying and addressing gaps in video retrieval capabilities. The framework also includes a data synthesis process that generates a large number of high-quality data pairs to improve model training. Finally, the Modality Pyramid curriculum is designed to train the General Video Embedder (GVE) by utilizing the relationships within diverse data, leading to superior performance in video retrieval tasks.'}, 'zh': {'title': 'çªç ´è§†é¢‘æ£€ç´¢çš„å±€é™ï¼Œå®ç°é€šç”¨èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè¯Šæ–­åŸºå‡†ã€æ•°æ®åˆæˆå’Œæ¨¡æ€é‡‘å­—å¡”è¯¾ç¨‹çš„æ¡†æ¶ï¼Œä»¥å®ç°è§†é¢‘æ£€ç´¢ä¸­çš„æœ€å…ˆè¿›é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å»ºç«‹äº†é€šç”¨è§†é¢‘æ£€ç´¢åŸºå‡†ï¼ˆUVRBï¼‰ï¼ŒåŒ…å«16ä¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨æµ‹é‡æ€§èƒ½å¹¶è¯Šæ–­ä»»åŠ¡å’Œé¢†åŸŸä¸­çš„å…³é”®èƒ½åŠ›å·®è·ã€‚é€šè¿‡UVRBçš„è¯Šæ–­æŒ‡å¯¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯æ‰©å±•çš„æ•°æ®åˆæˆå·¥ä½œæµç¨‹ï¼Œç”Ÿæˆ155ä¸‡ä¸ªé«˜è´¨é‡çš„é…å¯¹ï¼Œä»¥å¡«å……æ‰€éœ€çš„è¯­ä¹‰ç©ºé—´ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†æ¨¡æ€é‡‘å­—å¡”è¯¾ç¨‹ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ ·æ•°æ®ä¸­çš„æ½œåœ¨å…³è”ï¼Œè®­ç»ƒæˆ‘ä»¬çš„é€šç”¨è§†é¢‘åµŒå…¥å™¨ï¼ˆGVEï¼‰ï¼Œå®éªŒç»“æœè¡¨æ˜GVEåœ¨UVRBä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶-shotæ³›åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01833', 'title': 'TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images\n  Reasoning', 'url': 'https://huggingface.co/papers/2511.01833', 'abstract': 'TIR-Bench evaluates advanced visual reasoning capabilities in multimodal models through diverse tasks requiring tool use and chain-of-thought, demonstrating the need for genuine thinking-with-images.  \t\t\t\t\tAI-generated summary \t\t\t\t The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-with-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-with-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce TIR-Bench, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.', 'score': 15, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'db7aa80001628cea', 'authors': ['Ming Li', 'Jike Zhong', 'Shitian Zhao', 'Haoquan Zhang', 'Shaoheng Lin', 'Yuxiang Lai', 'Chen Wei', 'Konstantinos Psounis', 'Kaipeng Zhang'], 'affiliations': ['Chinese University of Hong Kong', 'Emory University', 'Rice University', 'Shanghai AI Laboratory', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01833.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset', '#multimodal'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° TIR-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 13 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 22 MLLM Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº fine-tuning Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unlocking Advanced Visual Reasoning with TIR-Bench', 'desc': 'TIR-Bench is a new benchmark designed to assess advanced visual reasoning in multimodal models, focusing on their ability to use tools and engage in chain-of-thought reasoning with images. Current benchmarks, like Visual Search, only evaluate basic image operations, which do not reflect the complexity of real-world problem-solving. TIR-Bench includes 13 diverse tasks that require innovative tool use for image processing, pushing the limits of what models can achieve in visual reasoning. Our evaluation of 22 multimodal large language models shows that effective performance on TIR-Bench necessitates true thinking-with-images capabilities.'}, 'zh': {'title': 'TIR-Benchï¼šè¯„ä¼°å›¾åƒæ€ç»´èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'TIR-Bench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ–°åŸºå‡†ï¼Œç‰¹åˆ«å…³æ³¨å·¥å…·ä½¿ç”¨å’Œæ€ç»´é“¾çš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºå‡†æ— æ³•å……åˆ†æ•æ‰åˆ°è¿™äº›é«˜çº§èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå¤„ç†å’Œæ“ä½œæ–¹é¢ã€‚æˆ‘ä»¬é€šè¿‡ 13 ä¸ªå¤šæ ·åŒ–çš„ä»»åŠ¡æ¥æµ‹è¯•æ¨¡å‹çš„æ€ç»´èƒ½åŠ›ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦æ–°é¢–çš„å·¥å…·ä½¿ç”¨ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„è¡¨ç°éœ€è¦çœŸæ­£çš„å›¾åƒæ€ç»´èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26909', 'title': 'NaviTrace: Evaluating Embodied Navigation of Vision-Language Models', 'url': 'https://huggingface.co/papers/2510.26909', 'abstract': "NaviTrace is a Visual Question Answering benchmark for evaluating robotic navigation capabilities using a semantic-aware trace score across various scenarios and embodiment types.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.", 'score': 13, 'issue_id': 6780, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '086ba2cba84d2f5d', 'authors': ['Tim Windecker', 'Manthan Patel', 'Moritz Reuss', 'Richard Schwarzkopf', 'Cesar Cadena', 'Rudolf Lioutikov', 'Marco Hutter', 'Jonas Frey'], 'affiliations': ['ETH Zurich', 'Robotic Systems Lab', 'University of Stuttgart'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26909.jpg', 'data': {'categories': ['#benchmark', '#games', '#cv', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'NaviTrace â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Visual Question Answering Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğº Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ñ‚Ğ¸Ğ¿ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ (Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ¾Ğ½Ğ¾Ğ³Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚, ĞºĞ¾Ğ»Ñ‘ÑĞ½Ñ‹Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚, Ğ²ĞµĞ»Ğ¾ÑĞ¸Ğ¿ĞµĞ´) Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Dynamic Time Warping, Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¸ ÑˆÑ‚Ñ€Ğ°Ñ„Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 1000 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ»Ğ°Ğ±Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'NaviTrace: Advancing Robotic Navigation Evaluation with Visual Question Answering', 'desc': 'NaviTrace is a new benchmark designed to assess how well robots can navigate using visual question answering. It evaluates various types of robots, including humans, legged robots, and wheeled robots, by having them follow instructions to create a 2D navigation path. The benchmark uses a unique semantic-aware trace score that measures the accuracy of navigation by considering factors like distance and goal location. This tool helps identify gaps in robot performance compared to humans, particularly in understanding space and reaching targets accurately.'}, 'zh': {'title': 'NaviTraceï¼šæœºå™¨äººå¯¼èˆªèƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'NaviTraceæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æœºå™¨äººå¯¼èˆªèƒ½åŠ›çš„è§†è§‰é—®ç­”åŸºå‡†ï¼Œé‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„è½¨è¿¹è¯„åˆ†æ–¹æ³•ã€‚è¯¥åŸºå‡†å…è®¸æ¨¡å‹æ ¹æ®æŒ‡ä»¤å’Œä¸åŒçš„å®ä½“ç±»å‹ï¼ˆå¦‚äººç±»ã€å››è¶³æœºå™¨äººã€è½®å¼æœºå™¨äººå’Œè‡ªè¡Œè½¦ï¼‰ç”Ÿæˆ2Då¯¼èˆªè½¨è¿¹ã€‚é€šè¿‡å¯¹1000ä¸ªåœºæ™¯å’Œ3000å¤šä¸ªä¸“å®¶è½¨è¿¹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§æ–°çš„è¯­ä¹‰æ„ŸçŸ¥è½¨è¿¹è¯„åˆ†æŒ‡æ ‡ï¼Œç»“åˆäº†åŠ¨æ€æ—¶é—´è§„æ•´è·ç¦»ã€ç›®æ ‡ç«¯ç‚¹è¯¯å·®å’ŒåŸºäºè¯­ä¹‰çš„æƒ©ç½šã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ç©ºé—´å®šä½å’Œç›®æ ‡å®šä½æ–¹é¢ä¸äººç±»è¡¨ç°å­˜åœ¨ä¸€è‡´çš„å·®è·ï¼ŒNaviTraceä¸ºç°å®ä¸–ç•Œçš„æœºå™¨äººå¯¼èˆªå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯é‡å¤çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01340', 'title': 'left|,circlearrowright,text{BUS},right|: A Large and\n  Diverse Multimodal Benchmark for evaluating the ability of Vision-Language\n  Models to understand Rebus Puzzles', 'url': 'https://huggingface.co/papers/2511.01340', 'abstract': "A benchmark and framework improve Vision-Language Models' performance on Rebus Puzzles through structured reasoning and example selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present left|,circlearrowright,text{BUS},right|, a large and diverse benchmark of 1,333 English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose RebusDescProgICE, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on left|,circlearrowright,text{BUS},right| by 2.1-4.1% and 20-30% using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.", 'score': 12, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'f2159a05b3bb9ef3', 'authors': ['Trishanu Das', 'Abhilash Nandy', 'Khush Bajaj', 'Deepiha S'], 'affiliations': ['Tredence Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01340.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ±ÑƒÑĞ¾Ğ² Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº REBUS, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,333 Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ñ… Ñ€ĞµĞ±ÑƒÑĞ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Vision-Language Models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº RebusDescProgICE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ±ÑƒÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ in-context learning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 2.1-4.1% Ğ´Ğ»Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 20-30% Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Vision-Language Models for Rebus Puzzles with Structured Reasoning', 'desc': 'This paper addresses the challenges faced by Vision-Language Models in solving Rebus Puzzles, which require complex reasoning and image interpretation. It introduces a new benchmark called BUS, consisting of 1,333 diverse Rebus Puzzles across various categories and difficulty levels. The authors propose a framework named RebusDescProgICE that enhances model performance through structured reasoning and improved example selection. The results show significant performance gains of 2.1-4.1% for closed-source models and 20-30% for open-source models compared to traditional Chain-of-Thought Reasoning.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹è§£è°œèƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ¡†æ¶ï¼Œä»¥æé«˜è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨è°œè¯­æ‹¼å›¾ï¼ˆRebus Puzzlesï¼‰ä¸Šçš„è¡¨ç°ã€‚è°œè¯­æ‹¼å›¾éœ€è¦å›¾åƒè¯†åˆ«ã€å¸¸è¯†æ¨ç†å’Œå¤šæ­¥éª¤æ¨ç†ç­‰å¤šç§æŠ€èƒ½ï¼Œç»™ç°æœ‰æ¨¡å‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«1333ä¸ªè‹±è¯­è°œè¯­æ‹¼å›¾çš„å¤§å‹å¤šæ ·åŒ–åŸºå‡†ï¼Œæ¶µç›–18ä¸ªä¸åŒç±»åˆ«ã€‚é€šè¿‡å¼•å…¥RebusDescProgICEæ¡†æ¶ï¼Œæˆ‘ä»¬ç»“åˆäº†éç»“æ„åŒ–æè¿°å’ŒåŸºäºä»£ç çš„ç»“æ„åŒ–æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26865', 'title': 'Do Vision-Language Models Measure Up? Benchmarking Visual Measurement\n  Reading with MeasureBench', 'url': 'https://huggingface.co/papers/2510.26865', 'abstract': 'MeasureBench evaluates vision-language models on reading measurements from images, revealing challenges in indicator localization and fine-grained spatial grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Reading measurement instruments is effortless for humans and requires relatively little domain expertise, yet it remains surprisingly challenging for current vision-language models (VLMs) as we find in preliminary evaluation. In this work, we introduce MeasureBench, a benchmark on visual measurement reading covering both real-world and synthesized images of various types of measurements, along with an extensible pipeline for data synthesis. Our pipeline procedurally generates a specified type of gauge with controllable visual appearance, enabling scalable variation in key details such as pointers, scales, fonts, lighting, and clutter. Evaluation on popular proprietary and open-weight VLMs shows that even the strongest frontier VLMs struggle measurement reading in general. A consistent failure mode is indicator localization: models can read digits or labels but misidentify the key positions of pointers or alignments, leading to big numeric errors despite plausible textual reasoning. We have also conducted preliminary experiments with reinforcement learning over synthetic data, and find encouraging results on in-domain synthetic subset but less promising for real-world images. Our analysis highlights a fundamental limitation of current VLMs in fine-grained spatial grounding. We hope this resource can help future advances on visually grounded numeracy and precise spatial perception of VLMs, bridging the gap between recognizing numbers and measuring the world.', 'score': 11, 'issue_id': 6780, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '2ca5c4530dffcdc1', 'authors': ['Fenfen Lin', 'Yesheng Liu', 'Haiyu Xu', 'Chen Yue', 'Zheqi He', 'Mingxuan Zhao', 'Miguel Hu Chen', 'Jiakang Liu', 'JG Yao', 'Xi Yang'], 'affiliations': ['BAAI FlagEval Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26865.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#dataset', '#rl'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ğ¼Ğ¸Ñ€: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ¿Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MeasureBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ğ½Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ pipeline Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ĞµĞ»Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… VLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ¼Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Accurate Measurement Reading', 'desc': 'MeasureBench is a new benchmark designed to evaluate vision-language models (VLMs) on their ability to read measurements from images. The study reveals that while humans can easily interpret measurement instruments, VLMs face significant challenges, particularly in accurately localizing indicators and achieving fine-grained spatial grounding. The authors introduce a data synthesis pipeline that generates various types of gauges with customizable features, allowing for extensive testing. Results show that even advanced VLMs struggle with measurement reading, often misidentifying key positions, which leads to substantial errors despite their ability to recognize text.'}, 'zh': {'title': 'æµ‹é‡å·¥å…·è¯»å–çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†MeasureBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¯»å–å›¾åƒä¸­æµ‹é‡å·¥å…·çš„åŸºå‡†ã€‚å°½ç®¡äººç±»åœ¨è¯»å–æµ‹é‡ä»ªå™¨æ—¶ç›¸å¯¹å®¹æ˜“ï¼Œä½†å½“å‰çš„VLMsåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ç¤ºå™¨å®šä½å’Œç²¾ç»†ç©ºé—´å®šä½æ–¹é¢ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œå¯ä»¥ç”Ÿæˆå„ç§ç±»å‹çš„æµ‹é‡ä»ªå™¨å›¾åƒï¼Œå¹¶è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å¼ºçš„VLMsåœ¨è¯»å–æµ‹é‡æ—¶ä¹Ÿå­˜åœ¨æ˜¾è‘—é”™è¯¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å½“å‰VLMsåœ¨ç²¾ç»†ç©ºé—´å®šä½æ–¹é¢çš„åŸºæœ¬å±€é™æ€§ï¼ŒæœŸæœ›èƒ½ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¸®åŠ©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01857', 'title': 'Trove: A Flexible Toolkit for Dense Retrieval', 'url': 'https://huggingface.co/papers/2511.01857', 'abstract': "Trove is an open-source retrieval toolkit that streamlines data management and experimentation with efficient on-the-fly processing and customizable components.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.", 'score': 10, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '447d2c76748bcbc1', 'authors': ['Reza Esfandiarpoor', 'Max Zuo', 'Stephen H. Bach'], 'affiliations': ['Department of Computer Science, Brown University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01857.jpg', 'data': {'categories': ['#rag', '#benchmark', '#open_source', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸', 'desc': 'Trove â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ğ¹. ĞĞ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒĞ·Ğ»Ğ°Ñ…, Trove Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Streamline Your Retrieval Experiments with Trove!', 'desc': 'Trove is an open-source toolkit designed to enhance the efficiency of data retrieval and management in machine learning experiments. It allows users to process datasets dynamically, enabling filtering, selection, transformation, and combination of data with minimal coding effort. The toolkit is highly customizable, allowing researchers to modify or replace components to suit their specific needs. Additionally, Trove optimizes memory usage and improves inference times, making it a valuable resource for conducting exploratory research in retrieval tasks.'}, 'zh': {'title': 'Troveï¼šç®€åŒ–æ£€ç´¢å®éªŒçš„å¼€æºå·¥å…·', 'desc': 'Troveæ˜¯ä¸€ä¸ªå¼€æºçš„æ£€ç´¢å·¥å…·åŒ…ï¼Œæ—¨åœ¨ç®€åŒ–æ•°æ®ç®¡ç†å’Œå®éªŒè¿‡ç¨‹ã€‚å®ƒæ”¯æŒå®æ—¶å¤„ç†å’Œçµæ´»çš„ç»„ä»¶å®šåˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾åœ°å®éªŒä¸åŒçš„æ•°æ®é›†é…ç½®ã€‚Troveçš„é«˜æ•ˆæ•°æ®ç®¡ç†åŠŸèƒ½å¯ä»¥åœ¨è¿è¡Œæ—¶åŠ è½½å’Œå¤„ç†æ£€ç´¢æ•°æ®é›†ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå¹¶ä¸”æ”¯æŒå¤šèŠ‚ç‚¹æ‰§è¡Œã€‚é€šè¿‡ä½ä»£ç çš„ç»Ÿä¸€ç®¡é“ï¼ŒTroveç®€åŒ–äº†æ£€ç´¢å®éªŒï¼Œä¿ƒè¿›äº†æ¢ç´¢æ€§ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.26491', 'title': 'Data-Efficient RLVR via Off-Policy Influence Guidance', 'url': 'https://huggingface.co/papers/2510.26491', 'abstract': 'Influence functions and off-policy estimation improve data selection in RLVR, accelerating training and reducing data usage for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop Curriculum RL with Off-Policy Influence guidance (CROPI), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR.', 'score': 10, 'issue_id': 6780, 'pub_date': '2025-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '90ede1ba680b5efe', 'authors': ['Erle Zhu', 'Dazhi Jiang', 'Yuan Wang', 'Xujun Li', 'Jiale Cheng', 'Yuxian Gu', 'Yilin Niu', 'Aohan Zeng', 'Jie Tang', 'Minlie Huang', 'Hongning Wang'], 'affiliations': ['CoAI Group, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.26491.jpg', 'data': {'categories': ['#optimization', '#data', '#training', '#rl', '#reasoning'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ off-policy Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… rollout-Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ LLM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº CROPI Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 2.66 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 10% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ.'}, 'en': {'title': 'Efficient Data Selection for Faster RL Training', 'desc': 'This paper presents a new method for selecting data in Reinforcement Learning with Verifiable Rewards (RLVR) to improve the training of large language models (LLMs). It introduces influence functions to assess how much each data point contributes to the learning process, moving away from traditional heuristic methods. To make this process more efficient, the authors propose an off-policy influence estimation technique that uses previously collected data instead of requiring new policy rollouts. The resulting framework, called Curriculum RL with Off-Policy Influence guidance (CROPI), significantly speeds up training while reducing data usage, demonstrating its effectiveness on large models.'}, 'zh': {'title': 'åŸºäºå½±å“å‡½æ•°çš„é«˜æ•ˆæ•°æ®é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå½±å“å‡½æ•°çš„ç†è®ºæ–¹æ³•ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ•°æ®é€‰æ‹©ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä¼°è®¡æ¯ä¸ªæ•°æ®ç‚¹å¯¹å­¦ä¹ ç›®æ ‡çš„è´¡çŒ®ï¼Œæ”¹è¿›äº†å½“å‰ä¸»è¦ä¾èµ–å¯å‘å¼çš„æ–¹æ³•ã€‚ä¸ºäº†è§£å†³åœ¨çº¿å½±å“ä¼°è®¡çš„è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç¦»çº¿å½±å“ä¼°è®¡æ–¹æ³•ï¼Œåˆ©ç”¨é¢„å…ˆæ”¶é›†çš„è½¨è¿¹é«˜æ•ˆè¿‘ä¼¼æ•°æ®å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„CROPIæ¡†æ¶åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæ˜¾è‘—åŠ å¿«ï¼ŒåŒæ—¶å‡å°‘äº†æ•°æ®ä½¿ç”¨é‡ï¼Œå±•ç¤ºäº†åŸºäºå½±å“çš„æ•°æ®é€‰æ‹©åœ¨RLVRä¸­çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01618', 'title': 'Actial: Activate Spatial Reasoning Ability of Multimodal Large Language\n  Models', 'url': 'https://huggingface.co/papers/2511.01618', 'abstract': 'Viewpoint Learning enhances the spatial reasoning capabilities of Multimodal Large Language Models using a two-stage fine-tuning strategy and a hybrid cold-start initialization method, improving performance on both in-domain and out-of-domain 3D reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.', 'score': 9, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'd5c38f7fa1d22f7c', 'authors': ['Xiaoyu Zhan', 'Wenxuan Huang', 'Hao Sun', 'Xinyu Fu', 'Changfeng Ma', 'Shaosheng Cao', 'Bohan Jia', 'Shaohui Lin', 'Zhenfei Yin', 'Lei Bai', 'Wanli Ouyang', 'Yuanqi Li', 'Jie Guo', 'Yanwen Guo'], 'affiliations': ['East China Normal University', 'Nanjing University', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'University of Oxford', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01618.jpg', 'data': {'categories': ['#open_source', '#3d', '#optimization', '#transfer_learning', '#multimodal', '#training', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğµ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Viewpoint Learning â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Viewpoint-100K Ñ 100K Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Supervised Fine-Tuning, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Reinforcement Learning Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Group Relative Policy Optimization. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Spatial Reasoning in MLLMs with Viewpoint Learning', 'desc': 'This paper introduces Viewpoint Learning, a method to enhance the spatial reasoning abilities of Multimodal Large Language Models (MLLMs) through a two-stage fine-tuning process. The authors present the Viewpoint-100K dataset, which includes 100,000 object-centric image pairs and related questions to train the models on diverse viewpoints. The first stage involves Supervised Fine-Tuning (SFT) to inject foundational knowledge, while the second stage uses Reinforcement Learning with Group Relative Policy Optimization (GRPO) to improve generalization. The results demonstrate that this approach significantly boosts the spatial reasoning capabilities of MLLMs, benefiting both in-domain and out-of-domain 3D reasoning tasks.'}, 'zh': {'title': 'è§†è§’å­¦ä¹ ï¼šæå‡ç©ºé—´æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§’å­¦ä¹ ï¼ˆViewpoint Learningï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸¤é˜¶æ®µçš„å¾®è°ƒç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨Viewpoint-100Kæ•°æ®é›†ä¸Šæ³¨å…¥åŸºç¡€çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«10ä¸‡ä¸ªç‰©ä½“ä¸­å¿ƒçš„å›¾åƒå¯¹ï¼Œå…·æœ‰å¤šæ ·çš„è§†è§’å’Œç›¸åº”çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†MLLMåœ¨3Dæ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¼ºè°ƒäº†åœ¨æœªæ¥æœºå™¨äººå’Œè‡ªä¸»ç³»ç»Ÿå‘å±•ä¸­åŸ¹å…»åŸºç¡€ç©ºé—´æŠ€èƒ½çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01846', 'title': 'Towards Robust Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2511.01846', 'abstract': 'IMO-Bench, a suite of advanced reasoning benchmarks, evaluates mathematical reasoning capabilities of foundation models using IMO-level problems and detailed grading guidelines, achieving gold-level performance with Gemini Deep Think.  \t\t\t\t\tAI-generated summary \t\t\t\t Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.', 'score': 7, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '027047e8d79aea9a', 'authors': ['Thang Luong', 'Dawsen Hwang', 'Hoang H. Nguyen', 'Golnaz Ghiasi', 'Yuri Chervonyi', 'Insuk Seo', 'Junsu Kim', 'Garrett Bingham', 'Jonathan Lee', 'Swaroop Mishra', 'Alex Zhai', 'Clara Huiyi Hu', 'Henryk Michalewski', 'Jimin Kim', 'Jeonghyun Ahn', 'Junhwi Bae', 'Xingyou Song', 'Trieu H. Trinh', 'Quoc V. Le', 'Junehyuk Jung'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01846.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#math', '#reasoning'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞĞ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° IMO-Bench â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ foundation models Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ IMO-AnswerBench Ñ 400 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ IMO-Proof Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini Deep Think Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 80% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° IMO-AnswerBench Ğ¸ 65.7% Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¼ IMO-Proof Bench, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ñ‹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ IMO-GradingBench Ñ 1000 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': "Advancing AI's Mathematical Reasoning with IMO-Bench", 'desc': 'The paper introduces IMO-Bench, a set of benchmarks designed to assess the mathematical reasoning abilities of foundation models using problems from the International Mathematical Olympiad (IMO). It highlights the importance of finding appropriate metrics for evaluating these models, as existing tests often do not challenge them sufficiently. The benchmarks include IMO-AnswerBench for short answer questions and IMO-Proof Bench for evaluating proof-writing skills, both of which have been rigorously vetted by experts. The results demonstrate that the Gemini Deep Think model achieved impressive scores, indicating significant advancements in the field of mathematical reasoning for AI.'}, 'zh': {'title': 'æ¨åŠ¨æ•°å­¦æ¨ç†çš„åŸºå‡†æµ‹è¯•', 'desc': 'IMO-Bench æ˜¯ä¸€å¥—å…ˆè¿›çš„æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°åŸºç¡€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ï¼ˆIMOï¼‰æ°´å¹³çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŒ…æ‹¬ IMO-AnswerBench å’Œ IMO-Proof Benchï¼Œå‰è€…æµ‹è¯•æ¨¡å‹åœ¨ 400 é“å¤šæ ·åŒ–çš„å¥¥æ—åŒ¹å…‹é—®é¢˜ä¸Šçš„çŸ­ç­”æ¡ˆèƒ½åŠ›ï¼Œåè€…åˆ™è¯„ä¼°æ¨¡å‹çš„è¯æ˜å†™ä½œèƒ½åŠ›ã€‚é€šè¿‡è¿™äº›åŸºå‡†ï¼ŒGemini Deep Think æ¨¡å‹åœ¨ IMO 2025 ä¸­å–å¾—äº†é‡‘ç‰Œè¡¨ç°ï¼Œåˆ†åˆ«åœ¨ IMO-AnswerBench å’Œ IMO-Proof Bench ä¸Šè¾¾åˆ°äº† 80.0% å’Œ 65.7% çš„é«˜åˆ†ã€‚æˆ‘ä»¬å¸Œæœ› IMO-Bench èƒ½å¤Ÿæ¨åŠ¨æ•°å­¦æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¹¶ä¿ƒè¿›è‡ªåŠ¨è¯„ä¼°çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01775', 'title': 'How Far Are Surgeons from Surgical World Models? A Pilot Study on\n  Zero-shot Surgical Video Generation with Expert Assessment', 'url': 'https://huggingface.co/papers/2511.01775', 'abstract': 'SurgVeo, a benchmark for video generation in surgery, and the Surgical Plausibility Pyramid reveal a gap between visual plausibility and causal understanding in surgical AI models.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.', 'score': 6, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'fe53da4979233ebe', 'authors': ['Zhen Chen', 'Qing Xu', 'Jinlin Wu', 'Biao Yang', 'Yuhao Zhai', 'Geng Guo', 'Jing Zhang', 'Yinlu Ding', 'Nassir Navab', 'Jiebo Luo'], 'affiliations': ['Department of Gastrointestinal Surgery, The Second Qilu Hospital, Shandong University', 'Department of Neurosurgery, The First Hospital, Shanxi Medical University', 'Institute of Automation, Chinese Academy of Sciences', 'Technical University of Munich', 'University of Nottingham', 'University of Rochester', 'Yale University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01775.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#video', '#dataset', '#science'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ’Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ğµ â€” Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ¼ Ğ² Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ SurgVeo â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ğ¸, Ğ¸ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ° Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ (SPP) â€” Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Veo-3 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ»Ğ°Ğ¿Ğ°Ñ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğµ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Â«Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸Â»: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ´Ğ»Ñ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Gap: From Visuals to Surgical Understanding in AI', 'desc': 'The paper introduces SurgVeo, a benchmark designed to evaluate video generation models specifically in the context of surgery. It highlights the Surgical Plausibility Pyramid (SPP), a framework that assesses AI-generated surgical videos on various levels of plausibility, from basic visuals to complex surgical strategies. The study reveals that while the Veo-3 model produces visually convincing videos, it struggles with deeper causal understanding necessary for surgical tasks. This research underscores the need for advanced AI models that can bridge the gap between visual realism and the intricate knowledge required in high-stakes medical environments.'}, 'zh': {'title': 'å¤–ç§‘è§†é¢‘ç”Ÿæˆçš„å¯ä¿¡åº¦ä¸å› æœç†è§£ä¹‹å·®', 'desc': 'SurgVeoæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤–ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å¤–ç§‘äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è§†è§‰å¯ä¿¡åº¦å’Œå› æœç†è§£ä¹‹é—´çš„å·®è·ã€‚å°½ç®¡åŸºç¡€æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦æ·±åšä¸“ä¸šçŸ¥è¯†çš„å¤–ç§‘é¢†åŸŸï¼Œå®ƒä»¬çš„åº”ç”¨ä»ç„¶å­˜åœ¨é‡è¦çš„æœªæ¢ç´¢é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†å¤–ç§‘å¯ä¿¡åº¦é‡‘å­—å¡”ï¼ˆSPPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå››å±‚æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹è¾“å‡ºçš„å¤æ‚æ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶Veo-3åœ¨è§†è§‰æ„ŸçŸ¥å¯ä¿¡åº¦ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ›´é«˜å±‚æ¬¡çš„å› æœç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01718', 'title': 'Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete\n  Denoising Diffusion Process', 'url': 'https://huggingface.co/papers/2511.01718', 'abstract': 'A Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P) model integrates multiple modalities through a synchronous denoising process, achieving state-of-the-art performance in vision-language-action tasks with faster inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act -- reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4times faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at https://irpn-eai.github.io/UD-VLA.github.io/.', 'score': 6, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '0eaaf37e2b1fa4bf', 'authors': ['Jiayi Chen', 'Wenxuan Song', 'Pengxiang Ding', 'Ziyang Zhou', 'Han Zhao', 'Feilong Tang', 'Donglin Wang', 'Haoang Li'], 'affiliations': ['HKUST(GZ)', 'Monash University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01718.jpg', 'data': {'categories': ['#open_source', '#architecture', '#agents', '#diffusion', '#multimodal', '#robotics', '#inference'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… CALVIN, LIBERO Ğ¸ SimplerEnv, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Synergizing Vision, Language, and Action with JD3P', 'desc': 'The paper presents a new model called Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P) that enhances vision-language-action (VLA) tasks by integrating multiple modalities through a synchronous denoising process. This model allows for the simultaneous understanding, generation, and execution of actions based on natural language and visual inputs, improving the synergy between these tasks. By optimizing the generation and action prediction together, the model achieves faster inference times and better performance on various benchmarks compared to traditional methods. The approach utilizes a unified tokenized space and a hybrid attention mechanism, demonstrating significant advancements in efficiency and effectiveness in real-world applications.'}, 'zh': {'title': 'ç»Ÿä¸€æ‰©æ•£æ¨¡å‹ï¼Œæå‡è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ä»»åŠ¡çš„æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ‰©æ•£VLAå’Œè”åˆç¦»æ•£å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼ˆJD3Pï¼‰æ¨¡å‹ï¼Œé€šè¿‡åŒæ­¥å»å™ªè¿‡ç¨‹æ•´åˆå¤šç§æ¨¡æ€ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¨¡å‹æ—¨åœ¨ç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿï¼Œå¹¶ä½œä¸ºå…·èº«ä»£ç†æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚ä¸ä»¥å¾€æ¨¡å‹ä¸åŒï¼ŒJD3Pé€šè¿‡è”åˆä¼˜åŒ–ç”Ÿæˆå’Œè¡ŒåŠ¨ï¼Œåˆ©ç”¨è¿­ä»£ç²¾ç‚¼ä½¿å¾—åŠ¨ä½œåœ¨å……åˆ†çš„è§†è§‰æŒ‡å¯¼ä¸‹ä¸æ–­æ¼”å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨ç†é€Ÿåº¦æ¯”è‡ªå›å½’æ–¹æ³•å¿«å››å€ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00405', 'title': 'UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings', 'url': 'https://huggingface.co/papers/2511.00405', 'abstract': 'UME-R1, a generative multimodal embedding framework, enhances performance through reasoning-driven generation and reinforcement learning, outperforming conventional discriminative models.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.', 'score': 5, 'issue_id': 6780, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '7866a0534b62a36f', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00405.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#multimodal', '#training', '#rlhf', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ UME-R1 â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ° Ğ½Ğ° 78 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ³Ğ´Ğµ Ğ¾Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„Ğ¸Ñ€ĞµĞ½ÑĞ°.'}, 'en': {'title': 'Unlocking Performance with Generative Multimodal Embeddings', 'desc': 'The paper introduces UME-R1, a novel generative multimodal embedding framework that improves performance by integrating reasoning-driven generation with reinforcement learning. Unlike traditional discriminative models, UME-R1 employs a two-stage training approach that first fine-tunes the model to develop reasoning capabilities, allowing it to produce both discriminative and generative embeddings. The research demonstrates that generative embeddings can significantly enhance performance by leveraging the reasoning strengths of multimodal large language models (MLLMs). Additionally, the findings suggest that combining discriminative and generative embeddings leads to superior results, while reinforcement learning further optimizes the quality of these generative embeddings.'}, 'zh': {'title': 'ç”ŸæˆåµŒå…¥ï¼Œè¶…è¶Šä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹çš„æ–°æ—¶ä»£', 'desc': 'UME-R1æ˜¯ä¸€ç§ç”Ÿæˆå¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†é©±åŠ¨çš„ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åˆ¤åˆ«æ¨¡å‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆé€šè¿‡å†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒèµ‹äºˆæ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œç”Ÿæˆåˆ¤åˆ«å’Œç”ŸæˆåµŒå…¥ã€‚éšåï¼Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä¼˜åŒ–ç”ŸæˆåµŒå…¥çš„è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”ŸæˆåµŒå…¥åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åˆ¤åˆ«åµŒå…¥ï¼Œå¹¶ä¸”ä¸¤è€…çš„ç»“åˆèƒ½å¤Ÿå®ç°æ›´å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01144', 'title': 'AthenaBench: A Dynamic Benchmark for Evaluating LLMs in Cyber Threat\n  Intelligence', 'url': 'https://huggingface.co/papers/2511.01144', 'abstract': 'AthenaBench, an enhanced benchmark for evaluating LLMs in CTI, reveals limitations in reasoning capabilities of current models, especially for tasks like threat actor attribution and risk mitigation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated strong capabilities in natural language reasoning, yet their application to Cyber Threat Intelligence (CTI) remains limited. CTI analysis involves distilling large volumes of unstructured reports into actionable knowledge, a process where LLMs could substantially reduce analyst workload. CTIBench introduced a comprehensive benchmark for evaluating LLMs across multiple CTI tasks. In this work, we extend CTIBench by developing AthenaBench, an enhanced benchmark that includes an improved dataset creation pipeline, duplicate removal, refined evaluation metrics, and a new task focused on risk mitigation strategies. We evaluate twelve LLMs, including state-of-the-art proprietary models such as GPT-5 and Gemini-2.5 Pro, alongside seven open-source models from the LLaMA and Qwen families. While proprietary LLMs achieve stronger results overall, their performance remains subpar on reasoning-intensive tasks, such as threat actor attribution and risk mitigation, with open-source models trailing even further behind. These findings highlight fundamental limitations in the reasoning capabilities of current LLMs and underscore the need for models explicitly tailored to CTI workflows and automation.', 'score': 3, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'd48328a205f1127a', 'authors': ['Md Tanvirul Alam', 'Dipkamal Bhusal', 'Salman Ahmad', 'Nidhi Rastogi', 'Peter Worth'], 'affiliations': ['Athena Security Group', 'Rochester Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01144.jpg', 'data': {'categories': ['#benchmark', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ¸Ğ±ĞµÑ€ÑƒĞ³Ñ€Ğ¾Ğ·', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° AthenaBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ĞºĞ¸Ğ±ĞµÑ€ÑƒĞ³Ñ€Ğ¾Ğ· Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ· Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ² ÑÑ„ĞµÑ€Ğµ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ½ÑƒĞ»Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing LLM Evaluation for Cyber Threat Intelligence', 'desc': 'AthenaBench is a new benchmark designed to evaluate Large Language Models (LLMs) specifically in the context of Cyber Threat Intelligence (CTI). It addresses the shortcomings of existing models in reasoning tasks, particularly in areas like threat actor attribution and risk mitigation. The benchmark improves upon previous efforts by refining dataset creation, removing duplicates, and introducing new evaluation metrics. The evaluation of twelve LLMs shows that while proprietary models perform better overall, they still struggle with complex reasoning tasks, indicating a need for models better suited for CTI applications.'}, 'zh': {'title': 'æå‡ç½‘ç»œå¨èƒæƒ…æŠ¥åˆ†æçš„æ¨ç†èƒ½åŠ›', 'desc': 'AthenaBenchæ˜¯ä¸€ä¸ªå¢å¼ºçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œå¨èƒæƒ…æŠ¥ï¼ˆCTIï¼‰ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨å¨èƒè¡Œä¸ºè€…å½’å±å’Œé£é™©ç¼“è§£ç­‰ä»»åŠ¡ä¸Šã€‚å°½ç®¡å•†ä¸šæ¨¡å‹å¦‚GPT-5å’ŒGemini-2.5 Proçš„è¡¨ç°è¾ƒå¼ºï¼Œä½†åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ä»æ˜¾ä¸è¶³ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†éœ€è¦å¼€å‘ä¸“é—¨é’ˆå¯¹CTIå·¥ä½œæµç¨‹å’Œè‡ªåŠ¨åŒ–çš„æ¨¡å‹ï¼Œä»¥æé«˜åˆ†ææ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00810', 'title': 'GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor\n  for GUI Grounding', 'url': 'https://huggingface.co/papers/2511.00810', 'abstract': 'GUI-AIMA, an attention-based and coordinate-free framework, enhances GUI grounding by aligning MLLM attention with patch-wise signals, achieving state-of-the-art performance with minimal training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA', 'score': 3, 'issue_id': 6780, 'pub_date': '2025-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': '4a4a44b1d5547873', 'authors': ['Shijie Zhou', 'Viet Dac Lai', 'Hao Tan', 'Jihyung Kil', 'Wanrong Zhu', 'Changyou Chen', 'Ruiyi Zhang'], 'affiliations': ['Adobe Research', 'University at Buffalo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00810.jpg', 'data': {'categories': ['#agents', '#small_models', '#multimodal', '#training'], 'emoji': 'ğŸ–±ï¸', 'ru': {'title': 'Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ³Ñ€Ğ°Ğ½Ğ´Ğ¸Ğ½Ğ³ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ GUI-AIMA, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ğ½Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ GUI-AIMA-3B Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 85 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing GUI Grounding with Attention and Efficiency', 'desc': 'GUI-AIMA is a novel framework designed to improve the process of GUI grounding by leveraging the attention mechanisms of Multimodal Large Language Models (MLLMs). Instead of generating exact coordinates for screen interactions, it focuses on identifying relevant visual patches and determining click locations within them. This approach utilizes adaptive patch-wise grounding signals, enhancing efficiency and performance with minimal training data. The framework has shown remarkable results, achieving state-of-the-art accuracy with only 85,000 training screenshots, demonstrating the potential of MLLMs in GUI tasks.'}, 'zh': {'title': 'GUI-AIMAï¼šé«˜æ•ˆçš„å›¾å½¢ç”¨æˆ·ç•Œé¢å®šä½æ¡†æ¶', 'desc': 'GUI-AIMAæ˜¯ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ— åæ ‡æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„å®šä½èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ³¨æ„åŠ›ä¸å›¾åƒå—ä¿¡å·å¯¹é½ï¼Œæ¥å®ç°é«˜æ•ˆçš„GUIå®šä½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šå¤´èšåˆç®€åŒ–çš„æŸ¥è¯¢-è§†è§‰æ³¨æ„åŠ›çŸ©é˜µï¼Œé€‚åº”æ€§åœ°è®¡ç®—ä¸ç”¨æˆ·æŒ‡ä»¤ç›¸å…³çš„ä¿¡å·ã€‚GUI-AIMAåœ¨ä»…ä½¿ç”¨85,000å¼ æˆªå›¾è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå±•ç¤ºäº†å“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œå¹¶åœ¨3Bæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01706', 'title': 'Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace\n  Disentanglement', 'url': 'https://huggingface.co/papers/2511.01706', 'abstract': 'A novel rank-2 projection subspace is proposed to analyze multi-step knowledge interactions in Large Language Models, revealing how Parametric Knowledge and Context Knowledge contribute to Natural Language Explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t Natural Language Explanations (NLEs) describe how Large Language Models (LLMs) make decisions, drawing on both external Context Knowledge (CK) and Parametric Knowledge (PK) stored in model weights. Understanding their interaction is key to assessing the grounding of NLEs, yet it remains underexplored. Prior work has largely examined only single-step generation, typically the final answer, and has modelled PK and CK interaction only as a binary choice in a rank-1 subspace. This overlooks richer forms of interaction, such as complementary or supportive knowledge. We propose a novel rank-2 projection subspace that disentangles PK and CK contributions more accurately and use it for the first multi-step analysis of knowledge interactions across longer NLE sequences. Experiments on four QA datasets and three open-weight instruction-tuned LLMs show that diverse knowledge interactions are poorly represented in a rank-1 subspace but are effectively captured in our rank-2 formulation. Our multi-step analysis reveals that hallucinated NLEs align strongly with the PK direction, context-faithful ones balance PK and CK, and Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing PK reliance. This work provides the first framework for systematic studies of multi-step knowledge interactions in LLMs through a richer rank-2 subspace disentanglement. Code and data: https://github.com/copenlu/pk-ck-knowledge-disentanglement.', 'score': 2, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '743253b5d2ea3e03', 'authors': ['Sekh Mainul Islam', 'Pepa Atanasova', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01706.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#interpretability', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ… LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ (Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸ĞµÑÑ Ğ² Ğ²ĞµÑĞ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ (Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…) Ğ²Ğ¼ĞµÑÑ‚Ğµ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ… ÑĞ²ÑĞ·Ğ°Ğ½Ñ‹ Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unraveling Knowledge Interactions in Language Models with Rank-2 Projections', 'desc': "This paper introduces a new method to analyze how Large Language Models (LLMs) use different types of knowledge when generating explanations. It distinguishes between Parametric Knowledge (PK), which is stored in the model's weights, and Context Knowledge (CK), which comes from the surrounding information. The authors propose a rank-2 projection subspace to better understand the interactions between these two types of knowledge, especially in multi-step explanations. Their findings show that using this new approach reveals more complex relationships between PK and CK than previous methods, leading to better insights into how LLMs create natural language explanations."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†äº¤äº’æ–°è§†è§’', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„äºŒé˜¶æŠ•å½±å­ç©ºé—´ï¼Œç”¨äºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¤šæ­¥çŸ¥è¯†äº¤äº’ï¼Œæ­ç¤ºäº†å‚æ•°çŸ¥è¯†ï¼ˆPKï¼‰å’Œä¸Šä¸‹æ–‡çŸ¥è¯†ï¼ˆCKï¼‰å¦‚ä½•å…±åŒå½±å“è‡ªç„¶è¯­è¨€è§£é‡Šï¼ˆNLEï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç†è§£PKå’ŒCKä¹‹é—´çš„ç›¸äº’ä½œç”¨å¯¹äºè¯„ä¼°NLEçš„åŸºç¡€è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•æ­¥ç”Ÿæˆä¸Šï¼Œé€šå¸¸åªè€ƒè™‘æœ€ç»ˆç­”æ¡ˆï¼Œè€Œå°†PKå’ŒCKçš„äº¤äº’å»ºæ¨¡ä¸ºä¸€å…ƒé€‰æ‹©ï¼Œå¿½è§†äº†æ›´ä¸°å¯Œçš„äº¤äº’å½¢å¼ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°å¤šæ­¥åˆ†æèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰çŸ¥è¯†äº¤äº’ï¼Œå°¤å…¶æ˜¯åœ¨äºŒé˜¶å­ç©ºé—´ä¸­ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†PKå’ŒCKçš„è´¡çŒ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01617', 'title': 'Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers', 'url': 'https://huggingface.co/papers/2511.01617', 'abstract': "Vote-in-Context (ViC) is a training-free framework that leverages Vision-Language Models (VLMs) for zero-shot reranking and fusion in cross-modal video retrieval, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In the retrieval domain, candidates' fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates' representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM's prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https://github.com/mohammad2012191/ViC", 'score': 2, 'issue_id': 6780, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '33443a9e1640cc74', 'authors': ['Mohamed Eltahir', 'Ali Habibullah', 'Lama Ayash', 'Tanveer Hussain', 'Naeemullah Khan'], 'affiliations': ['Edge Hill University', 'King Abdullah University of Science and Technology (KAUST)', 'King Khalid University (KKU)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01617.jpg', 'data': {'categories': ['#rag', '#benchmark', '#open_source', '#video', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ: Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Vision-Language Models', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Vote-in-Context (ViC) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Vision-Language Models Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ¸ÑĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ VLM, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ S-Grid, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑĞµÑ‚ĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ liste-wise Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ActivityNet, VATEX Ğ¸ MSR-VTT Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² zero-shot Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ´Ğ¾ +40 Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Recall@1.'}, 'en': {'title': 'Revolutionizing Video Retrieval with Vote-in-Context', 'desc': "Vote-in-Context (ViC) is a novel framework that enhances video retrieval by utilizing Vision-Language Models (VLMs) for zero-shot reranking and fusion without the need for training. It addresses the challenge of combining results from different retrievers by treating the reranking process as a reasoning task, integrating both content evidence and metadata into the VLM's prompt. The framework introduces the S-Grid, a method for representing videos in a way that facilitates effective reasoning over multiple candidates. ViC achieves impressive performance on video retrieval tasks, setting new benchmarks in zero-shot settings and demonstrating its capability to manage complex visual and textual information."}, 'zh': {'title': 'Vote-in-Contextï¼šè·¨æ¨¡æ€è§†é¢‘æ£€ç´¢çš„æ–°çªç ´', 'desc': 'Vote-in-Context (ViC) æ˜¯ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è·¨æ¨¡æ€è§†é¢‘æ£€ç´¢ä¸­å®ç°é›¶-shot é‡æ–°æ’åºå’Œèåˆï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å†…å®¹è¯æ®å’Œæ£€ç´¢å™¨å…ƒæ•°æ®åºåˆ—åŒ–åˆ° VLM çš„æç¤ºä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°æƒè¡¡æ£€ç´¢å™¨å…±è¯†ä¸è§†è§‰è¯­è¨€å†…å®¹ã€‚ViC åœ¨è§†é¢‘æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„è§†è§‰å’Œæ—¶é—´ä¿¡å·æ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢ç²¾åº¦ã€‚è¯¥æ¡†æ¶çš„åˆ›æ–°æ€§åœ¨äºå…¶ç®€å•ã€å¯é‡å¤æ€§å¼ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ç°ä»£ VLM è½¬å˜ä¸ºå¼ºå¤§çš„é›¶-shot é‡æ–°æ’åºå’Œèåˆå·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02778', 'title': 'VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation', 'url': 'https://huggingface.co/papers/2511.02778', 'abstract': "VCode introduces a benchmark for generating SVG code from images to preserve symbolic meaning, highlighting gaps in visual-centric coding and proposing VCoder to improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.", 'score': 101, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '0a20532e35f480a6', 'authors': ['Kevin Qinghong Lin', 'Yuhao Zheng', 'Hangyu Ran', 'Dantong Zhu', 'Dongxing Mao', 'Linjie Li', 'Philip Torr', 'Alex Jinpeng Wang'], 'affiliations': ['Central South University', 'Microsoft Research', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02778.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#interpretability', '#cv', '#agents', '#plp', '#multimodal', '#dataset', '#reasoning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğº ĞºĞ¾Ğ´Ñƒ: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ñ‡ĞµÑ€ĞµĞ· SVG', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCode Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SVG ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ†ĞµĞ»ÑŒÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° CodeVQA, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… SVG. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VCoder, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¾Ñ‚ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Bridging the Gap in Visual-Centric Coding with VCode and VCoder', 'desc': 'VCode is a new benchmark designed to generate SVG code from images, focusing on maintaining the symbolic meaning of the visuals. It highlights the challenges faced by current visual-centric coding methods compared to language-centric approaches. The paper introduces VCoder, a framework that enhances visual language models (VLMs) by refining SVG code through iterative analysis and providing structured visual cues. The results show that while VLMs excel in reasoning, they struggle with generating accurate SVGs, indicating a significant gap in visual coding capabilities.'}, 'zh': {'title': 'æå‡è§†è§‰ç¼–ç çš„æ€§èƒ½ä¸ç†è§£', 'desc': 'VCodeæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨ä»å›¾åƒç”ŸæˆSVGä»£ç ï¼Œä»¥ä¿ç•™ç¬¦å·æ„ä¹‰ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†è§‰ä¸­å¿ƒç¼–ç çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†VCoderæ¥æé«˜æ€§èƒ½ã€‚VCoderé€šè¿‡è¿­ä»£åˆ†æå’Œä¿®æ­£SVGä»£ç ï¼Œä»¥åŠä½¿ç”¨æ£€æµ‹å™¨å’Œè§£æå™¨æä¾›ç»“æ„åŒ–çº¿ç´¢ï¼Œæ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å‰æ²¿çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸“ä¸šçŸ¥è¯†å’Œä¸‰ç»´æ¨ç†æ–¹é¢ä»ç„¶æœ‰é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25616', 'title': "Don't Blind Your VLA: Aligning Visual Representations for OOD\n  Generalization", 'url': 'https://huggingface.co/papers/2510.25616', 'abstract': "Systematic study reveals that naive action fine-tuning degrades visual representations in Vision-Language-Action models, but targeted strategies can mitigate this and improve generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io", 'score': 96, 'issue_id': 1, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'a8dfa20a846983f9', 'authors': ['Nikita Kachaev', 'Mikhail Kolosov', 'Daniil Zelezetsky', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['Cognitive AI Lab Moscow, Russia', 'Cognitive AI Lab, IAI MIPT Moscow, Russia', 'IAI MIPT Moscow, Russia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.25616.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#multimodal', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº fine-tuning Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº fine-tuning Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Vision-Language Models. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Vision-Language Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ Ğ¸ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ ÑĞ·Ñ‹Ğº-Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Preserving Visual Knowledge in Action Fine-Tuning', 'desc': "This paper investigates how fine-tuning Vision-Language-Action (VLA) models can negatively impact their visual representations. The authors find that naive action fine-tuning often leads to a loss of valuable visual-language knowledge. They propose targeted strategies to preserve these representations and improve the model's ability to generalize to new situations. By analyzing hidden representations and attention maps, they demonstrate effective methods to mitigate degradation and enhance performance in out-of-distribution scenarios."}, 'zh': {'title': 'ä¼˜åŒ–åŠ¨ä½œå¾®è°ƒï¼Œæå‡è§†è§‰è¡¨å¾èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸­ï¼Œç®€å•çš„åŠ¨ä½œå¾®è°ƒä¼šå¯¼è‡´è§†è§‰è¡¨å¾çš„é€€åŒ–ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½å¤Ÿæä¾›å¯è½¬ç§»çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†åœ¨é€‚åº”åŠ¨ä½œæ¨¡æ€æ—¶ï¼Œå…¶åŸæœ‰çš„è§†è§‰-è¯­è¨€è¡¨å¾å’ŒçŸ¥è¯†å¯èƒ½ä¼šå—åˆ°å½±å“ã€‚é€šè¿‡åˆ†æVLAæ¨¡å‹çš„éšè—è¡¨å¾å’Œæ³¨æ„åŠ›å›¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹æ€§çš„ä»»åŠ¡å’Œæ–¹æ³•ï¼Œä»¥è¯„ä¼°åŠ¨ä½œå¾®è°ƒå¯¹è§†è§‰-è¯­è¨€èƒ½åŠ›çš„å½±å“ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥å‡è½»è¿™ç§é€€åŒ–ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02779', 'title': 'When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought', 'url': 'https://huggingface.co/papers/2511.02779', 'abstract': 'MIRA is a benchmark that evaluates models using intermediate visual images to enhance reasoning, showing significant performance improvements over text-only methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through "drawing to think". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.', 'score': 57, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '8e7774609a0828a8', 'authors': ['Yiyang Zhou', 'Haoqin Tu', 'Zijun Wang', 'Zeyu Wang', 'Niklas Muennighoff', 'Fan Nie', 'Yejin Choi', 'James Zou', 'Chaorui Deng', 'Shen Yan', 'Haoqi Fan', 'Cihang Xie', 'Huaxiu Yao', 'Qinghao Ye'], 'affiliations': ['ByteDance Seed', 'Stanford', 'UC Santa Cruz', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02779.jpg', 'data': {'categories': ['#benchmark', '#survey', '#cv', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'MIRA â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (ÑÑĞºĞ¸Ğ·Ñ‹, Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹, ÑÑ…ĞµĞ¼Ñ‹ Ğ¿ÑƒÑ‚Ğ¸) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸: Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ²Ğ²Ğ¾Ğ´, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ CoT Ğ¸ Visual-CoT Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 546 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº (ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 33.7%), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Visual Thinking Boosts Model Reasoning with MIRA', 'desc': 'MIRA is a benchmark that assesses machine learning models by requiring them to generate and use intermediate visual images for reasoning tasks. Unlike traditional methods that depend only on text, MIRA emphasizes the importance of visual aids, such as sketches and diagrams, to enhance problem-solving capabilities. The benchmark includes 546 multimodal problems that are annotated with visual images and answers, demonstrating that models perform significantly better when visual cues are incorporated. Experimental results reveal an average performance improvement of 33.7% when models utilize these intermediate images, highlighting their essential role in effective reasoning.'}, 'zh': {'title': 'ä¸­é—´è§†è§‰å›¾åƒåŠ©åŠ›æ¨ç†æå‡', 'desc': 'MIRAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆä¸­é—´è§†è§‰å›¾åƒæ—¶çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–æ–‡æœ¬çš„æ€ç»´é“¾æ–¹æ³•ä¸åŒï¼ŒMIRAè¦æ±‚æ¨¡å‹ç”Ÿæˆå’Œåˆ©ç”¨ä¸­é—´å›¾åƒï¼Œå¦‚è‰å›¾æˆ–ç»“æ„å›¾ï¼Œä»¥æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä»…ä¾èµ–æ–‡æœ¬æç¤ºæ—¶è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨æä¾›ä¸­é—´è§†è§‰çº¿ç´¢åï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå¹³å‡æé«˜äº†33.7%ã€‚è¿™è¡¨æ˜æƒ³è±¡ä¸­çš„è§†è§‰ä¿¡æ¯åœ¨MIRAçš„æˆåŠŸæ¨ç†ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03601', 'title': 'Step-Audio-EditX Technical Report', 'url': 'https://huggingface.co/papers/2511.03601', 'abstract': 'Step-Audio-EditX, an open-source LLM-based audio model, excels in expressive and iterative audio editing and zero-shot TTS using large-margin synthetic data.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.', 'score': 28, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'ceb8fb9ba3c98737', 'authors': ['Chao Yan', 'Boyong Wu', 'Peng Yang', 'Pengfei Tan', 'Guoqiang Hu', 'Li Xie', 'Yuxin Zhang', 'Xiangyu', 'Zhang', 'Fei Tian', 'Xuerui Yang', 'Xiangyu Zhang', 'Daxin Jiang', 'Shuchang Zhou', 'Gang Yu'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03601.jpg', 'data': {'categories': ['#open_source', '#synthetic'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Step-Audio-EditX â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¼ĞµĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¸, ÑÑ‚Ğ¸Ğ»ÑŒ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¾Ğ´Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ·Ğ°Ğ·Ğ¾Ñ€Ğ¾Ğ¼ (large-margin), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ², Ğ¾Ñ‚Ñ…Ğ¾Ğ´Ñ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ… Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Revolutionizing Audio Editing with Step-Audio-EditX', 'desc': "Step-Audio-EditX is an innovative open-source audio model that utilizes large language models (LLMs) for advanced audio editing and text-to-speech (TTS) tasks. It stands out by allowing users to perform expressive and iterative edits, focusing on aspects like emotion and speaking style without relying on traditional embedding methods. The model's unique approach to large-margin synthetic data enables it to achieve high levels of expressivity and control in audio outputs. Evaluation results indicate that it outperforms existing models in tasks related to emotion editing and fine-grained audio control."}, 'zh': {'title': 'å¼€æºéŸ³é¢‘ç¼–è¾‘çš„æœªæ¥ï¼šStep-Audio-EditX', 'desc': 'Step-Audio-EditXæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„éŸ³é¢‘ç¼–è¾‘æ¨¡å‹ï¼Œä¸“æ³¨äºæƒ…æ„Ÿã€è¯´è¯é£æ ¼å’Œå‰¯è¯­è¨€çš„è¡¨ç°æ€§å’Œè¿­ä»£éŸ³é¢‘ç¼–è¾‘ã€‚å®ƒçš„åˆ›æ–°ä¹‹å¤„åœ¨äºä»…ä½¿ç”¨å¤§è¾¹è·åˆæˆæ•°æ®ï¼Œé¿å…äº†å¯¹åµŒå…¥åŸºç¡€å…ˆéªŒæˆ–è¾…åŠ©æ¨¡å—çš„ä¾èµ–ã€‚é€šè¿‡å¤§è¾¹è·å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜è¡¨è¾¾æ€§å’Œè¿­ä»£æ§åˆ¶ï¼Œæ ‡å¿—ç€ä»ä¼ ç»Ÿçš„è¡¨ç¤ºçº§åˆ«è§£è€¦çš„æ ¹æœ¬è½¬å˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒStep-Audio-EditXåœ¨æƒ…æ„Ÿç¼–è¾‘å’Œå…¶ä»–ç»†ç²’åº¦æ§åˆ¶ä»»åŠ¡ä¸­è¶…è¶Šäº†MiniMax-2.6-hdå’ŒDoubao-Seed-TTS-2.0ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02243', 'title': 'When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs', 'url': 'https://huggingface.co/papers/2511.02243', 'abstract': "A framework decomposes modality following in multimodal large language models into relative reasoning uncertainty and inherent modality preference, providing insights into how models resolve conflicting information.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information.", 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'bcd8e7dc791b7cf4', 'authors': ['Zhuoran Zhang', 'Tengyue Wang', 'Xilin Gong', 'Yang Shi', 'Haotian Wang', 'Di Wang', 'Lijie Hu'], 'affiliations': ['King Abdullah University of Science and Technology', 'MBZUAI', 'Peking University', 'Provable Responsible AI and Data Analytics (PRADA) Lab', 'South China University of Technology', 'Tsinghua University', 'University of Georgia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02243.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€Ğ¾Ğ¶Ğ´Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½: Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ ÑƒĞ±Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ĞµÑ‘ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ² Ñ‚Ğ¾Ñ‡ĞºĞµ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒĞµÑ‚ ĞµÑ‘ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ, Ñ‡ĞµĞ¼ Ğ¼Ğ°ĞºÑ€Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ» Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼ÑƒÑ Ğ½ĞµÑ€ĞµÑˆĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Understanding How Models Navigate Conflicting Information', 'desc': "This paper introduces a framework that breaks down the process of modality following in multimodal large language models (MLLMs) into two key components: relative reasoning uncertainty and inherent modality preference. Relative reasoning uncertainty refers to the confidence gap between predictions made from different modalities, while inherent modality preference indicates a model's consistent bias when faced with equal uncertainties. The authors validate their framework using a specially designed dataset that manipulates the difficulty of visual and textual inputs, revealing a universal trend where the likelihood of following a modality decreases as its uncertainty increases. By analyzing layer-wise predictions, the study also uncovers how models oscillate between modalities in ambiguous situations, providing deeper insights into the decision-making processes of MLLMs."}, 'zh': {'title': 'æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹çš„å†³ç­–æœºåˆ¶', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œå°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡æ€è·Ÿéšåˆ†è§£ä¸ºç›¸å¯¹æ¨ç†ä¸ç¡®å®šæ€§å’Œå›ºæœ‰æ¨¡æ€åå¥½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“ä¸åŒæ¨¡æ€æä¾›çŸ›ç›¾ä¿¡æ¯æ—¶ï¼Œæ¨¡å‹éœ€è¦è§£å†³è¿™äº›å†²çªã€‚é€šè¿‡æ„å»ºä¸€ä¸ªå¯æ§æ•°æ®é›†ï¼Œä½œè€…éªŒè¯äº†ç›¸å¯¹ä¸ç¡®å®šæ€§ä¸æ¨¡æ€è·Ÿéšä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å‘ç°æ¨¡æ€è·Ÿéšçš„æ¦‚ç‡éšç€ç›¸å¯¹ä¸ç¡®å®šæ€§çš„å¢åŠ è€Œå•è°ƒä¸‹é™ã€‚æœ€ç»ˆï¼Œè®ºæ–‡æ­ç¤ºäº†æ¨¡å‹åœ¨æ¨¡æ€ä¹‹é—´çš„å†…éƒ¨æœºåˆ¶ï¼Œæä¾›äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹å¦‚ä½•å¤„ç†å†²çªä¿¡æ¯çš„æ·±å…¥ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02687', 'title': 'The Collaboration Gap', 'url': 'https://huggingface.co/papers/2511.02687', 'abstract': 'Evaluation of agent-based systems reveals a collaboration gap where solo-performing models degrade in pairings, suggesting the need for collaboration-aware evaluation and training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t The trajectory of AI development suggests that we will increasingly rely on agent-based systems composed of independently developed agents with different information, privileges, and tools. The success of these systems will critically depend on effective collaboration among these heterogeneous agents, even under partial observability. Despite intense interest, few empirical studies have evaluated such agent-agent collaboration at scale. We propose a collaborative maze-solving benchmark that (i) isolates collaborative capabilities, (ii) modulates problem complexity, (iii) enables scalable automated grading, and (iv) imposes no output-format constraints, preserving ecological plausibility. Using this framework, we evaluate 32 leading open- and closed-source models in solo, homogeneous, and heterogeneous pairings. Our results reveal a "collaboration gap": models that perform well solo often degrade substantially when required to collaborate. Collaboration can break down dramatically; for instance, small distilled models that solve mazes well alone may fail almost completely in certain pairings. We find that starting with the stronger agent often improves outcomes, motivating a "relay inference" approach where the stronger agent leads before handing off to the weaker one, closing much of the gap. Our findings argue for (1) collaboration-aware evaluation, (2) training strategies developed to enhance collaborative capabilities, and (3) interaction design that reliably elicits agents\' latent skills, guidance that applies to AI-AI and human-AI collaboration.', 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '3bda2c3029855330', 'authors': ['Tim R. Davidson', 'Adam Fourney', 'Saleema Amershi', 'Robert West', 'Eric Horvitz', 'Ece Kamar'], 'affiliations': ['EPFL', 'Microsoft Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02687.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡ĞºÑƒ, Ñ‡Ğ°ÑÑ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Â«relay inferenceÂ», Ğ³Ğ´Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ±ĞµÑ€Ñ‘Ñ‚ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ñƒ, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Bridging the Collaboration Gap in Agent-Based Systems', 'desc': "This paper discusses the challenges faced by agent-based systems when different models collaborate. It identifies a 'collaboration gap' where models that perform well individually do not necessarily work well together. The authors introduce a benchmark for evaluating collaborative capabilities in maze-solving tasks, allowing for scalable assessments of agent interactions. Their findings suggest that training and evaluation methods should focus on enhancing collaboration among agents to improve overall system performance."}, 'zh': {'title': 'æå‡ä»£ç†åä½œèƒ½åŠ›ï¼Œç¼©å°åä½œå·®è·', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºä»£ç†çš„ç³»ç»Ÿä¸­å­˜åœ¨çš„åä½œå·®è·ï¼ŒæŒ‡å‡ºå•ç‹¬è¡¨ç°è‰¯å¥½çš„æ¨¡å‹åœ¨é…å¯¹æ—¶è¡¨ç°ä¸‹é™ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä½œè¿·å®«æ±‚è§£åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ä¸åŒæ¨¡å‹åœ¨åä½œä¸­çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè®¸å¤šæ¨¡å‹åœ¨å•ç‹¬ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åä½œæ—¶å´æ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶å»ºè®®åº”é‡‡ç”¨åä½œæ„è¯†çš„è¯„ä¼°å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æå‡ä»£ç†ä¹‹é—´çš„åä½œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.25976', 'title': 'Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer', 'url': 'https://huggingface.co/papers/2510.25976', 'abstract': 'Brain-IT uses a Brain Interaction Transformer to reconstruct images from fMRI data with high fidelity, surpassing current methods and requiring less training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present "Brain-IT", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT\'s design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.', 'score': 14, 'issue_id': 1, 'pub_date': '2025-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '68a603c08e621421', 'authors': ['Roman Beliy', 'Amit Zalcher', 'Jonathan Kogman', 'Navve Wasserman', 'Michal Irani'], 'affiliations': ['Department of Computer Science and Applied Mathematics', 'The Weizmann Institute of Science'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.25976.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Brain-IT Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… fMRI Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Brain Interaction Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ Ñ‡Ğ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ fMRI Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑĞ¿Ñ‹Ñ‚ÑƒĞµĞ¼Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Image Reconstruction from Brain Data with Brain-IT', 'desc': 'Brain-IT introduces a novel Brain Interaction Transformer (BIT) that enhances the reconstruction of images from fMRI data, achieving high fidelity and requiring less training data. This method leverages functional clusters of brain voxels that are consistent across subjects, allowing for efficient integration of information. BIT predicts both high-level semantic and low-level structural features to guide the image reconstruction process, ensuring that the output closely resembles the actual images seen by individuals. As a result, Brain-IT outperforms existing state-of-the-art techniques in both visual quality and objective metrics, even with minimal fMRI data.'}, 'zh': {'title': 'Brain-ITï¼šé«˜æ•ˆé‡å»ºå¤§è„‘å›¾åƒçš„æ–°æ–¹æ³•', 'desc': 'Brain-ITæ˜¯ä¸€ç§åˆ©ç”¨è„‘äº¤äº’å˜æ¢å™¨ï¼ˆBITï¼‰ä»fMRIæ•°æ®ä¸­é‡å»ºå›¾åƒçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆåœ°å¤„ç†åŠŸèƒ½ç›¸ä¼¼çš„è„‘ä½“ç´ é›†ç¾¤ï¼Œå…‹æœäº†ç°æœ‰æŠ€æœ¯åœ¨å›¾åƒé‡å»ºä¸­çš„ä¸è¶³ã€‚BITèƒ½å¤Ÿé¢„æµ‹é«˜å±‚è¯­ä¹‰ç‰¹å¾å’Œä½å±‚ç»“æ„ç‰¹å¾ï¼Œä»è€Œå¼•å¯¼å›¾åƒé‡å»ºè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒBrain-ITåœ¨ä»…éœ€å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜ä¿çœŸåº¦çš„å›¾åƒé‡å»ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01937', 'title': 'Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR', 'url': 'https://huggingface.co/papers/2511.01937', 'abstract': "Retaining and up-weighting moderately easy problems in RLVR pipelines for LLMs reduces output verbosity without explicit length penalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a model that conflates ``thinking longer'' with ``thinking better''. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \\emph{emergent brevity for free}: the model learns to solve harder problems without inflating the output length,  despite the absence of any explicit length penalization. RLVR experiments using this approach on Qwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at https://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and models on https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging Face}.", 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': 'dc8d8b3b41c50ac6', 'authors': ['Abdelaziz Bounhar', 'Hadi Abdine', 'Evan Dufraisse', 'Ahmad Chamma', 'Amr Mohamed', 'Dani Bouch', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['MBZUAI', 'Ã‰cole Polytechnique'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01937.jpg', 'data': {'categories': ['#rl', '#small_models', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ•ÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ° ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ° Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ² Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… AIME25, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ² ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²Ğ´Ğ²Ğ¾Ğµ.'}, 'en': {'title': 'Emergent Brevity: Enhancing LLMs with Smart Problem Selection', 'desc': 'This paper addresses the issue of excessive verbosity in large language models (LLMs) trained for reasoning tasks. It proposes a method within Reinforcement Learning with Verifiable Rewards (RLVR) that retains and up-weights moderately easy problems during training. By doing so, the model learns to produce shorter outputs without explicit penalties for length, thus avoiding the misconception that longer outputs equate to better reasoning. The results show that this approach leads to improved accuracy while significantly reducing the average length of generated solutions.'}, 'zh': {'title': 'æå‡ç®€æ´æ€§ï¼Œä¼˜åŒ–è¾“å‡ºé•¿åº¦', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç®¡é“ä¸­ä¿ç•™å’Œé€‚åº¦åŠ æƒä¸­ç­‰éš¾åº¦é—®é¢˜çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰æ˜¾å¼é•¿åº¦æƒ©ç½šçš„æƒ…å†µä¸‹ï¼Œå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¾“å‡ºå†—é•¿æ€§ã€‚é€šè¿‡è®©æ¨¡å‹æ¥è§¦å¯è§£çš„çŸ­é“¾ä»»åŠ¡ï¼Œé™åˆ¶äº†å…¶è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œé¿å…äº†å†—é•¿çš„è¾“å‡ºã€‚æœ€ç»ˆï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨è§£å†³æ›´éš¾é—®é¢˜çš„åŒæ—¶ï¼Œä¿æŒè¾“å‡ºç®€æ´ï¼Œè¾¾åˆ°äº†"å…è´¹å‡ºç°ç®€æ´æ€§"çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02832', 'title': 'TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System', 'url': 'https://huggingface.co/papers/2511.02832', 'abstract': 'TWIST2, a portable mocap-free system, enables efficient data collection and hierarchical visuomotor policy control for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '1f8821e3bd15fa83', 'authors': ['Yanjie Ze', 'Siheng Zhao', 'Weizhuo Wang', 'Angjoo Kanazawa', 'Rocky Duan', 'Pieter Abbeel', 'Guanya Shi', 'Jiajun Wu', 'C. Karen Liu'], 'affiliations': ['Amazon FAR', 'CMU', 'Stanford University', 'UC Berkeley', 'USC'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02832.jpg', 'data': {'categories': ['#open_source', '#data', '#training', '#robotics', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'TWIST2 â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ VR-Ğ¾Ñ‡ĞºĞ¸ PICO4U Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚ĞµĞ»ĞµÑĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-ÑˆĞµÑ Ñ 2 ÑÑ‚ĞµĞ¿ĞµĞ½ÑĞ¼Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ñ‚ĞµĞ»Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ 100 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ° 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'TWIST2: Revolutionizing Humanoid Robotics with Mocap-Free Control', 'desc': "TWIST2 is a new system designed for humanoid robots that allows for easy and efficient data collection without the need for motion capture technology. It uses a virtual reality setup to track human movements in real-time, enabling full control of the robot's body. This system can quickly gather a large amount of data, achieving nearly perfect success in demonstrations of complex tasks. Additionally, it introduces a hierarchical visuomotor policy that allows the robot to perform advanced actions based on what it sees from a human perspective."}, 'zh': {'title': 'TWIST2ï¼šé«˜æ•ˆçš„äººå½¢æœºå™¨äººæ§åˆ¶ä¸æ•°æ®æ”¶é›†ç³»ç»Ÿ', 'desc': 'TWIST2æ˜¯ä¸€ä¸ªä¾¿æºå¼çš„æ— åŠ¨æ•ç³»ç»Ÿï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ”¶é›†æ•°æ®å¹¶å®ç°äººå½¢æœºå™¨äººå±‚æ¬¡åŒ–çš„è§†è§‰è¿åŠ¨æ§åˆ¶ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨PICO4U VRå®æ—¶è·å–å…¨èº«äººç±»åŠ¨ä½œï¼Œå¹¶é…å¤‡å®šåˆ¶çš„2è‡ªç”±åº¦æœºå™¨äººé¢ˆéƒ¨ï¼Œå®ç°è‡ªæˆ‘ä¸­å¿ƒçš„è§†è§‰æ§åˆ¶ã€‚TWIST2åœ¨15åˆ†é’Ÿå†…å¯ä»¥æ”¶é›†100ä¸ªæ¼”ç¤ºï¼ŒæˆåŠŸç‡æ¥è¿‘100%ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–çš„è§†è§‰è¿åŠ¨ç­–ç•¥æ¡†æ¶ï¼Œèƒ½å¤ŸåŸºäºè‡ªæˆ‘ä¸­å¿ƒè§†è§‰è‡ªä¸»æ§åˆ¶æ•´ä¸ªäººå½¢æœºå™¨äººã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02650', 'title': 'Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models', 'url': 'https://huggingface.co/papers/2511.02650', 'abstract': 'UniPruneBench is a unified benchmark for evaluating visual token pruning in multimodal LLMs, providing standardized protocols and system-level metrics to assess performance across various tasks and models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'b275977f11502ee3', 'authors': ['Tianfan Peng', 'Yuntao Du', 'Pengzhou Ji', 'Shijie Dong', 'Kailin Jiang', 'Mingchuan Ma', 'Yijun Tian', 'Jinhe Bi', 'Qian Li', 'Wei Du', 'Feng Xiao', 'Lizhen Cui'], 'affiliations': ['EB Tech Co., Ltd.', 'Ludwig Maximilian University of Munich', 'Shandong University', 'Shenzhen University of Information Technology', 'Sichuan University', 'Tongji University', 'University of Notre Dame', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02650.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#inference', '#multimodal'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'UniPruneBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´ĞµÑÑÑ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼, Ğ° Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ñƒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğ¼ Ğ½Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Streamlining Multimodal Models with UniPruneBench', 'desc': 'UniPruneBench is a comprehensive benchmark designed to evaluate visual token pruning in large multimodal models (LMMs). It addresses the inefficiencies caused by excessive visual tokens from image encoders by providing standardized evaluation protocols and metrics. The benchmark covers various compression algorithms and models, assessing not only task accuracy but also system performance metrics like runtime and latency. Key findings reveal that random pruning is a strong baseline, no single method is superior across all tasks, and the pruning ratio significantly impacts performance.'}, 'zh': {'title': 'ç»Ÿä¸€åŸºå‡†ï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ•ˆç‡', 'desc': 'UniPruneBenchæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ ‡è®°å‰ªæã€‚è¯¥åŸºå‡†æä¾›äº†æ ‡å‡†åŒ–çš„åè®®å’Œç³»ç»Ÿçº§æŒ‡æ ‡ï¼Œä»¥ä¾¿åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­è¯„ä¼°æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œéšæœºå‰ªææ˜¯ä¸€ä¸ªæ„å¤–å¼ºå¤§çš„åŸºçº¿ï¼Œè€Œæ²¡æœ‰å•ä¸€æ–¹æ³•åœ¨æ‰€æœ‰åœºæ™¯ä¸­å§‹ç»ˆè¡¨ç°æœ€ä½³ã€‚å‰ªææ•æ„Ÿæ€§åœ¨ä¸åŒä»»åŠ¡ä¸­å·®å¼‚æ˜¾è‘—ï¼Œå‰ªææ¯”ä¾‹æ˜¯å½±å“æ€§èƒ½ä¸‹é™çš„ä¸»è¦å› ç´ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02347', 'title': 'LTD-Bench: Evaluating Large Language Models by Letting Them Draw', 'url': 'https://huggingface.co/papers/2511.02347', 'abstract': "LTD-Bench evaluates large language models' spatial reasoning by requiring them to generate visual outputs, revealing significant limitations in their ability to map language to spatial concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '243b4b3570997ab1', 'authors': ['Liuhao Lin', 'Ke Li', 'Zihan Xu', 'Yuchen Shi', 'Yulei Qin', 'Yan Zhang', 'Xing Sun', 'Rongrong Ji'], 'affiliations': ['Youtu-Agent Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02347.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#cv', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ LTD-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¸Ğ¼ĞµÑÑ‚ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ (Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ñ‚Ğ¾Ñ‡ĞµĞº), Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': "LTD-Bench: Visualizing Language Models' Spatial Reasoning Limitations", 'desc': "LTD-Bench is a new benchmark designed to evaluate large language models (LLMs) specifically on their spatial reasoning abilities. It shifts the focus from abstract numerical scores to generating visual outputs, such as drawings, which makes the models' limitations in mapping language to spatial concepts clear. The benchmark includes tasks that test both spatial imagination and perception, revealing significant gaps in LLMs' capabilities even when they perform well on traditional metrics. By providing observable results, LTD-Bench helps bridge the gap between statistical performance and practical understanding of model abilities in real-world applications."}, 'zh': {'title': 'LTD-Benchï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†çš„çœŸå®èƒ½åŠ›', 'desc': 'LTD-Bench æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒè¦æ±‚æ¨¡å‹ç”Ÿæˆå¯è§†åŒ–è¾“å‡ºï¼Œå¦‚å›¾å½¢æˆ–å¯æ‰§è¡Œä»£ç ï¼Œä»è€Œæ­ç¤ºæ¨¡å‹åœ¨è¯­è¨€ä¸ç©ºé—´æ¦‚å¿µæ˜ å°„æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLTD-Bench ä½¿å¾—å³ä½¿æ˜¯éä¸“å®¶ä¹Ÿèƒ½ç›´è§‚åœ°ç†è§£æ¨¡å‹çš„èƒ½åŠ›å’Œç¼ºé™·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ä¸€äº›æ¨¡å‹åœ¨ä¼ ç»ŸåŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯­è¨€ä¸ç©ºé—´æ¦‚å¿µçš„åŒå‘æ˜ å°„ä¸Šä»å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00839', 'title': 'CodeClash: Benchmarking Goal-Oriented Software Engineering', 'url': 'https://huggingface.co/papers/2511.00839', 'abstract': "CodeClash evaluates language models' ability to iteratively develop code for open-ended objectives through competitive multi-round tournaments.  \t\t\t\t\tAI-generated summary \t\t\t\t Current benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the study of autonomous, goal-oriented code development.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': '923c3bf57db18a9c', 'authors': ['John Yang', 'Kilian Lieret', 'Joyce Yang', 'Carlos E. Jimenez', 'Ofir Press', 'Ludwig Schmidt', 'Diyi Yang'], 'affiliations': ['Cornell University', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00839.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#plp', '#agents'], 'emoji': 'âš”ï¸', 'ru': {'title': 'Ğ¡Ğ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ¾Ğ´Ğ°', 'desc': 'CodeClash â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, CodeClash Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ² ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ĞµĞ³Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1680 Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ¾Ğ² Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ 8 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM, Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ CodeClash Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'CodeClash: Competing for Code Excellence in Open-Ended Challenges', 'desc': 'CodeClash is a new benchmark designed to assess the ability of language models (LMs) to iteratively develop code for open-ended objectives through competitive tournaments. Unlike traditional coding benchmarks that focus on specific tasks, CodeClash simulates real-world software development by allowing LMs to compete in multi-round challenges aimed at achieving high-level goals. Each round consists of code editing followed by head-to-head competitions in a code arena, where models must strategize to improve their codebases against opponents. The results indicate that while LMs demonstrate varied development approaches, they face significant challenges in strategic reasoning and long-term code maintenance, often performing poorly compared to expert human programmers.'}, 'zh': {'title': 'CodeClashï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹çš„å¼€æ”¾ç›®æ ‡ä»£ç å¼€å‘èƒ½åŠ›', 'desc': 'CodeClash æ˜¯ä¸€ä¸ªè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾ç›®æ ‡ä¸‹è¿­ä»£å¼€å‘ä»£ç èƒ½åŠ›çš„åŸºå‡†ï¼Œé€šè¿‡å¤šè½®ç«äº‰æ€§æ¯”èµ›è¿›è¡Œæµ‹è¯•ã€‚ä¸ä¼ ç»Ÿçš„ç¼–ç¨‹åŸºå‡†ä¸åŒï¼ŒCodeClash å…³æ³¨çš„æ˜¯é«˜å±‚æ¬¡ç›®æ ‡çš„å®ç°ï¼Œè€Œä¸æ˜¯å•ä¸€çš„å…·ä½“ä»»åŠ¡ã€‚æ¯”èµ›åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œæ¨¡å‹éœ€è¦ç¼–è¾‘ä»£ç å¹¶åœ¨ä»£ç ç«æŠ€åœºä¸­ä¸å…¶ä»–æ¨¡å‹ç«äº‰ï¼Œç›®æ ‡åŒ…æ‹¬å¾—åˆ†æœ€å¤§åŒ–å’Œèµ„æºè·å–ç­‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹åœ¨å¼€å‘é£æ ¼ä¸Šå„æœ‰ä¸åŒï¼Œä½†åœ¨æˆ˜ç•¥æ¨ç†å’Œé•¿æœŸä»£ç ç»´æŠ¤æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.17950', 'title': 'RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies', 'url': 'https://huggingface.co/papers/2510.17950', 'abstract': 'RoboChallenge is an online evaluation system for robotic control algorithms, particularly VLA models, that addresses the need for large-scale testing with scalability and reproducibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Testing on real machines is indispensable for robotic control algorithms. In the context of learning-based algorithms, especially VLA models, demand for large-scale evaluation, i.e. testing a large number of models on a large number of tasks, is becoming increasingly urgent. However, doing this right is highly non-trivial, especially when scalability and reproducibility is taken into account. In this report, we describe our methodology for constructing RoboChallenge, an online evaluation system to test robotic control algorithms, and our survey of recent state-of-the-art VLA models using our initial benchmark Table30.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': '4b112ac313bc98b5', 'authors': ['Adina Yakefu', 'Bin Xie', 'Chongyang Xu', 'Enwen Zhang', 'Erjin Zhou', 'Fan Jia', 'Haitao Yang', 'Haoqiang Fan', 'Haowei Zhang', 'Hongyang Peng', 'Jing Tan', 'Junwen Huang', 'Kai Liu', 'Kaixin Liu', 'Kefan Gu', 'Qinglun Zhang', 'Ruitao Zhang', 'Saike Huang', 'Shen Cheng', 'Shuaicheng Liu', 'Tiancai Wang', 'Tiezhen Wang', 'Wei Sun', 'Wenbin Tang', 'Yajun Wei', 'Yang Chen', 'Youqiang Gui', 'Yucheng Zhao', 'Yunchao Ma', 'Yunfei Wei', 'Yunhuan Yang', 'Yutong Guo', 'Ze Chen', 'Zhengyuan Du', 'Ziheng Zhang', 'Ziming Liu', 'Ziwei Yan'], 'affiliations': ['Dexmal', 'Hugging Face'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.17950.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'RoboChallenge â€” ÑÑ‚Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ VLA. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Table30 Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'RoboChallenge: Scalable and Reproducible Evaluation for Robotic Control Algorithms', 'desc': 'RoboChallenge is an innovative online platform designed to evaluate robotic control algorithms, focusing on Variable Learning Algorithms (VLA). It addresses the critical need for large-scale testing, allowing researchers to assess numerous models across various tasks efficiently. The system emphasizes scalability and reproducibility, which are essential for reliable evaluations in machine learning. This paper outlines the methodology behind RoboChallenge and presents an initial benchmark of state-of-the-art VLA models.'}, 'zh': {'title': 'RoboChallengeï¼šæœºå™¨äººæ§åˆ¶ç®—æ³•çš„åœ¨çº¿è¯„ä¼°æ–°å¹³å°', 'desc': 'RoboChallengeæ˜¯ä¸€ä¸ªåœ¨çº¿è¯„ä¼°ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æœºå™¨äººæ§åˆ¶ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯å˜æ¢å­¦ä¹ ç®—æ³•ï¼ˆVLAæ¨¡å‹ï¼‰ã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨æ»¡è¶³å¯¹å¤§è§„æ¨¡æµ‹è¯•çš„éœ€æ±‚ï¼Œç¡®ä¿å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ã€‚éšç€å­¦ä¹ å‹ç®—æ³•çš„æ™®åŠï¼Œæµ‹è¯•å¤§é‡æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°å˜å¾—æ„ˆå‘é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†æ„å»ºRoboChallengeçš„æ–¹æ³•è®ºï¼Œå¹¶å¯¹æœ€æ–°çš„VLAæ¨¡å‹è¿›è¡Œäº†åˆæ­¥åŸºå‡†æµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01202', 'title': 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory\n  for LLMs', 'url': 'https://huggingface.co/papers/2511.01202', 'abstract': 'This paper develops a semantic information theory framework for large language models, focusing on token-level semantic embedding and information-theoretic measures to understand and analyze model architectures and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in numerous real-world applications. While the vast majority of research conducted from an experimental perspective is progressing rapidly, it demands substantial computational power, data, and other resources. Therefore, how to open the black-box of LLMs from a theoretical standpoint has become a critical challenge. This paper takes the theory of rate-distortion function, directed information, and Granger causality as its starting point to investigate the information-theoretic principles behind LLMs, leading to the development of semantic information theory for LLMs, where the fundamental unit is token, rather than bits that lacks any semantic meaning. By defining the probabilistic model of LLMs, we discuss structure-agnostic information-theoretic measures, such as the directed rate-distortion function in pre-training, the directed rate-reward function in post-training, and the semantic information flow in inference phase. This paper also delves deeply into the theory of token-level semantic embedding and the information-theoretically optimal vectorization method. Thereafter, we propose a general definition of autoregression LLM, where the Transformer architecture and its performance such as ELBO, generalization error bound, memory capacity, and semantic information measures can be derived theoretically. Other architectures, such as Mamba/Mamba2 and LLaDA, are also discussed in our framework. Consequently, this paper provides a theoretical framework for understanding LLMs from the perspective of semantic information theory, which also offers the necessary theoretical tools for further in-depth research.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '6f088c12d5972797', 'authors': ['Bo Bai'], 'affiliations': ['Theory Lab - Leibniz, Central Research Institute, 2012 Labs, Huawei Technology Co., Ltd., Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01202.jpg', 'data': {'categories': ['#architecture', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹ ÑÑ‰Ğ¸Ğº: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸-Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ“Ñ€ĞµĞ¹Ğ½Ğ´Ğ¶ĞµÑ€Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾ĞºĞµĞ½, Ğ° Ğ½Ğµ Ğ±Ğ¸Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ-Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ-Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²ĞµÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Transformer Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Mamba Ğ¸ LLaDA, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Large Language Models with Semantic Information Theory', 'desc': 'This paper introduces a new framework called semantic information theory to analyze large language models (LLMs) by focusing on token-level semantic embeddings instead of traditional bit-level measures. It employs information-theoretic concepts like rate-distortion functions and directed information to explore the underlying principles of LLM architectures and their performance. The authors define various information-theoretic measures applicable during different phases of model training and inference, providing insights into the efficiency and effectiveness of LLMs. Ultimately, this work aims to demystify LLMs theoretically, offering tools for future research in the field.'}, 'zh': {'title': 'è§£å¯†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯ç†è®º', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ä¿¡æ¯ç†è®ºæ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨åŸºäºtokençš„è¯­ä¹‰åµŒå…¥å’Œä¿¡æ¯è®ºåº¦é‡ï¼Œä»¥ç†è§£å’Œåˆ†ææ¨¡å‹æ¶æ„åŠå…¶æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ç‡å¤±çœŸå‡½æ•°ã€å®šå‘ä¿¡æ¯å’ŒGrangerå› æœå…³ç³»çš„ç†è®ºï¼Œæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹èƒŒåçš„ä¿¡æ¯è®ºåŸç†ã€‚æˆ‘ä»¬å®šä¹‰äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¦‚ç‡æ¨¡å‹ï¼Œå¹¶è®¨è®ºäº†ä¸ç»“æ„æ— å…³çš„ä¿¡æ¯è®ºåº¦é‡ï¼Œå¦‚é¢„è®­ç»ƒä¸­çš„å®šå‘ç‡å¤±çœŸå‡½æ•°å’Œæ¨ç†é˜¶æ®µçš„è¯­ä¹‰ä¿¡æ¯æµã€‚æœ€ç»ˆï¼Œæœ¬æ–‡ä¸ºä»è¯­ä¹‰ä¿¡æ¯ç†è®ºçš„è§’åº¦ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ç†è®ºæ¡†æ¶å’Œå¿…è¦çš„ç ”ç©¶å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01914', 'title': 'iFlyBot-VLA Technical Report', 'url': 'https://huggingface.co/papers/2511.01914', 'abstract': 'iFlyBot-VLA, a large-scale VLA model, uses a latent action model and dual-level action representation to enhance 3D perceptual and reasoning capabilities, achieving superior performance in manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'de1e27a9544ae5d8', 'authors': ['Yuan Zhang', 'Chenyu Xue', 'Wenjie Xu', 'Chao Ji', 'Jiajia wu', 'Jia Pan'], 'affiliations': ['LindenBot', 'iFlyTek Reasearch and Development Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01914.jpg', 'data': {'categories': ['#open_source', '#architecture', '#multimodal', '#training', '#robotics', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸', 'desc': 'iFlyBot-VLA â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ â€” Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing 3D Manipulation with iFlyBot-VLA', 'desc': 'The iFlyBot-VLA is a large-scale Vision-Language-Action model designed to improve 3D perception and reasoning for manipulation tasks. It employs a latent action model trained on extensive human and robotic manipulation videos, allowing it to understand both high-level intentions and low-level dynamics. The model uses a dual-level action representation that supervises both the Vision-Language Model and an action expert, enhancing its training process. Experimental results show that iFlyBot-VLA outperforms existing methods on benchmark tasks and will contribute to future research by open-sourcing part of its dataset.'}, 'zh': {'title': 'æå‡3Dæ“ä½œèƒ½åŠ›çš„iFlyBot-VLAæ¨¡å‹', 'desc': 'iFlyBot-VLAæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨æå‡3Dæ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ½œåœ¨åŠ¨ä½œæ¨¡å‹å’ŒåŒå±‚åŠ¨ä½œè¡¨ç¤ºæ¡†æ¶ï¼Œç»è¿‡å¤§é‡äººç±»å’Œæœºå™¨äººæ“ä½œè§†é¢‘çš„è®­ç»ƒã€‚é€šè¿‡ç»“åˆæœºå™¨äººè½¨è¿¹æ•°æ®ä¸ä¸€èˆ¬é—®ç­”å’Œç©ºé—´é—®ç­”æ•°æ®é›†ï¼ŒiFlyBot-VLAåœ¨æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨LIBERO FrankaåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­ä¹Ÿå–å¾—äº†è‰¯å¥½çš„æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24932', 'title': 'RiddleBench: A New Generative Reasoning Benchmark for LLMs', 'url': 'https://huggingface.co/papers/2510.24932', 'abstract': 'RiddleBench, a benchmark of 1,737 puzzles, reveals fundamental weaknesses in state-of-the-art language models, including hallucination cascades and poor self-correction, highlighting the need for more robust reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models have demonstrated strong performance on many established reasoning benchmarks. However, these benchmarks primarily evaluate structured skills like quantitative problem-solving, leaving a gap in assessing flexible, multifaceted reasoning abilities that are central to human intelligence. These abilities require integrating logical deduction with spatial awareness and constraint satisfaction, which current evaluations do not measure well. To address this, we introduce RiddleBench, a benchmark of 1,737 challenging puzzles in English designed to probe these core reasoning capabilities. Evaluation of state-of-the-art models on RiddleBench shows fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and 63.16%). Analysis further reveals deep failures, including hallucination cascades (accepting flawed reasoning from other models) and poor self-correction due to a strong self-confirmation bias. Their reasoning is also fragile, with performance degrading significantly when constraints are reordered or irrelevant information is introduced. RiddleBench functions as a diagnostic tool for these issues and as a resource for guiding the development of more robust and reliable language models.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '8c94f69602570f0b', 'authors': ['Deepon Halder', 'Alan Saji', 'Thanmay Jayakumar', 'Ratish Puduppully', 'Anoop Kunchukuttan', 'Raj Dabre'], 'affiliations': ['Google', 'IT University of Copenhagen', 'Indian Institute of Engineering, Science and Technology, Shibpur', 'Indian Institute of Technology Madras', 'Microsoft', 'Nilekani Centre at AI4Bharat'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24932.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ RiddleBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 1737 Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Gemini 2.5 Pro, o3 Ğ¸ Claude 4 Sonnet, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ»Ğ¸ÑˆÑŒ Ğ¾ĞºĞ¾Ğ»Ğ¾ 60%, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ» ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ĞºĞ°ÑĞºĞ°Ğ´Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ½ÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'RiddleBench: Uncovering the Reasoning Gaps in Language Models', 'desc': 'RiddleBench is a new benchmark consisting of 1,737 puzzles that tests the reasoning abilities of language models. It highlights significant weaknesses in current state-of-the-art models, such as hallucination cascades and inadequate self-correction. The benchmark reveals that even advanced models struggle with flexible reasoning, achieving only around 60% accuracy. By identifying these flaws, RiddleBench aims to guide the development of more robust language models that can better integrate logical deduction and spatial awareness.'}, 'zh': {'title': 'æ­ç¤ºè¯­è¨€æ¨¡å‹çš„æ¨ç†å¼±ç‚¹', 'desc': 'RiddleBenchæ˜¯ä¸€ä¸ªåŒ…å«1737ä¸ªéš¾é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹çš„åŸºæœ¬å¼±ç‚¹ï¼ŒåŒ…æ‹¬å¹»è§‰çº§è”å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›å·®ã€‚è¿™äº›æ¨¡å‹åœ¨çµæ´»çš„å¤šé¢æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œè¿™äº›èƒ½åŠ›æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒã€‚é€šè¿‡RiddleBenchçš„è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯é¡¶å°–çš„æ¨¡å‹å¦‚Gemini 2.5 Proå’ŒClaude 4 Sonnetçš„å‡†ç¡®ç‡ä¹Ÿä»…ç•¥é«˜äº60%ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ä»…ä½œä¸ºè¯Šæ–­å·¥å…·ï¼Œè¿˜ä¸ºå¼€å‘æ›´å¼ºå¤§å’Œå¯é çš„è¯­è¨€æ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02712', 'title': 'VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models', 'url': 'https://huggingface.co/papers/2511.02712', 'abstract': 'A novel affective cues-guided reasoning framework using video emotion foundation models and a fine-grained dataset achieves competitive performance in emotion understanding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '29941b6b9a2b15b2', 'authors': ['Zhicheng Zhang', 'Weicheng Wang', 'Yongjie Zhu', 'Wenyu Qin', 'Pengfei Wan', 'Di Zhang', 'Jufeng Yang'], 'affiliations': ['Kuaishou Technology', 'Nankai International Advanced Research Institute (SHENZHENFUTIAN)', 'Nankai University', 'Pengcheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02712.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#multimodal', '#training', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ˜Š', 'ru': {'title': 'ĞĞ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ°Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¾Ğ± ÑĞ¼Ğ¾Ñ†Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ° (VidEmo), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· curriculum learning Ğ¸ reinforcement learning Ğ´Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Emo-CFG Ñ 2.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¾Ğ± ÑĞ¼Ğ¾Ñ†Ğ¸ÑÑ…, Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ°, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Emotion Understanding in Videos with Affective Reasoning', 'desc': 'This paper presents a new framework for understanding emotions in videos, called the affective cues-guided reasoning framework. It utilizes video emotion foundation models (VidEmo) that are specifically designed to analyze and reason about emotions in a structured way. The framework incorporates a two-stage tuning process, which includes curriculum emotion learning and affective-tree reinforcement learning, to enhance emotion reasoning capabilities. Additionally, the authors introduce a comprehensive dataset, Emo-CFG, which contains 2.1 million samples to support the development and evaluation of emotion understanding tasks.'}, 'zh': {'title': 'æƒ…æ„Ÿç†è§£çš„æ–°é‡Œç¨‹ç¢‘', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æƒ…æ„Ÿçº¿ç´¢å¼•å¯¼æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘æƒ…æ„Ÿç†è§£çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºæœ¬å±æ€§æ„ŸçŸ¥ã€è¡¨è¾¾åˆ†æå’Œé«˜çº§æƒ…æ„Ÿç†è§£ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µçš„æ–¹æ³•è¿›è¡Œå¤„ç†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—ä¸“é—¨ç”¨äºæƒ…æ„Ÿæ¨ç†çš„è§†é¢‘æƒ…æ„ŸåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µçš„è°ƒä¼˜è¿‡ç¨‹æ¥å¢å¼ºæƒ…æ„ŸçŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨15ä¸ªé¢éƒ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02415', 'title': 'ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension', 'url': 'https://huggingface.co/papers/2511.02415', 'abstract': 'An automated pipeline using retrieval-augmented generation and chain-of-thought strategies creates a diverse dataset to enhance reasoning capabilities in complex chart understanding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM^3, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '5f97519da51d5ce3', 'authors': ['Duo Xu', 'Hao Cheng', 'Xin Lin', 'Zhen Xie', 'Hao Wang'], 'affiliations': ['Alibaba Cloud Computing'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02415.jpg', 'data': {'categories': ['#rag', '#cv', '#multimodal', '#training', '#dataset', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (RAG) Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ChartMÂ³ Ñ 38 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ 142 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Small Models for Complex Chart Understanding', 'desc': 'This paper presents a novel automated pipeline that enhances the reasoning abilities of multimodal large language models (MLLMs) in understanding complex charts. It utilizes retrieval-augmented generation (RAG) to source professional chart templates and employs chain-of-thought (CoT) strategies to create reasoning codes that mimic real-world data distributions. The resulting dataset, ChartM^3, consists of 38,000 charts and 142,000 question-and-answer pairs, designed to improve model training and evaluation. Experiments show that using this dataset allows smaller models to perform comparably to larger models in complex chart comprehension tasks.'}, 'zh': {'title': 'æå‡å¤æ‚å›¾è¡¨ç†è§£çš„è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆç®¡é“', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„å¤šé˜¶æ®µä»£ç é©±åŠ¨ç®¡é“ï¼Œç”¨äºç”Ÿæˆè§†è§‰æ¨ç†æ•°æ®é›†ï¼Œä»¥æé«˜å¤æ‚å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç®¡é“ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ£€ç´¢ä¸“ä¸šå›¾è¡¨æ¨¡æ¿ï¼Œå¹¶åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰ç­–ç•¥ç”Ÿæˆæ¨ç†ä»£ç ï¼Œæ¨¡æ‹ŸçœŸå®æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡æ¨¡å‹è¯„ä¼°ï¼Œè¯¥ç®¡é“æé«˜äº†å›¾è¡¨çš„å¤šæ ·æ€§å’Œæ•°æ®è´¨é‡ï¼Œæ„å»ºäº†åŒ…å«38Kå›¾è¡¨å’Œ142Ké—®ç­”å¯¹çš„ChartM^3æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†æ˜¾è‘—æå‡äº†æ¨ç†èƒ½åŠ›å’Œè·¨é¢†åŸŸæ³›åŒ–æ€§èƒ½ï¼Œä½¿å¾—è¾ƒå°çš„æ¨¡å‹åœ¨å¤æ‚å›¾è¡¨ç†è§£ä¸Šèƒ½å¤Ÿè¾¾åˆ°ä¸å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02490', 'title': "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring", 'url': 'https://huggingface.co/papers/2511.02490', 'abstract': "BRAINS, a system using Large Language Models, effectively detects and monitors Alzheimer's disease by integrating cognitive assessments and case retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'd7006e129e1cf709', 'authors': ['Rajan Das Gupta', 'Md Kishor Morol', 'Nafiz Fahad', 'Md Tanzib Hosain', 'Sumaya Binte Zilani Choya', 'Md Jakir Hossen'], 'affiliations': ['American International University-Bangladesh', 'ELITE Research Lab', 'George Mason University', 'Multimedia University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02490.jpg', 'data': {'categories': ['#rag', '#science', '#healthcare', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'BRAINS â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ĞµĞ·Ğ½Ğ¸ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¸ÑĞºĞ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼.'}, 'en': {'title': "Revolutionizing Alzheimer's Detection with BRAINS", 'desc': "BRAINS is a novel system that utilizes Large Language Models (LLMs) to detect and monitor Alzheimer's disease effectively. It consists of two main components: a cognitive diagnostic module that assesses Alzheimer's risk using fine-tuned LLMs on cognitive and neuroimaging data, and a case retrieval module that finds similar patient cases to enhance understanding. By combining these components, BRAINS improves the accuracy of early detection and classification of disease severity. This approach not only aids in scalable and explainable diagnostics but also opens doors for future advancements in neurodegeneration screening."}, 'zh': {'title': 'BRAINSï¼šé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æ–°å¸Œæœ›', 'desc': 'BRAINSæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å’Œç›‘æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è®¤çŸ¥è¯„ä¼°å’Œæ¡ˆä¾‹æ£€ç´¢ï¼Œé‡‡ç”¨åŒæ¨¡å—æ¶æ„ï¼ŒåŒ…æ‹¬è®¤çŸ¥è¯Šæ–­æ¨¡å—å’Œæ¡ˆä¾‹æ£€ç´¢æ¨¡å—ã€‚è¯Šæ–­æ¨¡å—ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…é£é™©è¿›è¡Œç»“æ„åŒ–è¯„ä¼°ï¼Œè€Œæ¡ˆä¾‹æ£€ç´¢æ¨¡å—åˆ™å°†æ‚£è€…èµ„æ–™ç¼–ç ä¸ºæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒBRAINSèƒ½å¤Ÿæé«˜å¯¹è®¤çŸ¥è¡°é€€æ—©æœŸè¿¹è±¡çš„è¯†åˆ«èƒ½åŠ›ï¼Œå±•ç°å‡ºä½œä¸ºè¾…åŠ©å·¥å…·çš„å¼ºå¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02374', 'title': 'AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda', 'url': 'https://huggingface.co/papers/2511.02374', 'abstract': "AyurParam-2.9B, a domain-specialized bilingual language model fine-tuned for Ayurveda, outperforms other models in its size class and demonstrates competitive performance on specialized medical knowledge tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Current large language models excel at broad, general-purpose tasks, but consistently underperform when exposed to highly specialized domains that require deep cultural, linguistic, and subject-matter expertise. In particular, traditional medical systems such as Ayurveda embody centuries of nuanced textual and clinical knowledge that mainstream LLMs fail to accurately interpret or apply. We introduce AyurParam-2.9B, a domain-specialized, bilingual language model fine-tuned from Param-1-2.9B using an extensive, expertly curated Ayurveda dataset spanning classical texts and clinical guidance. AyurParam's dataset incorporates context-aware, reasoning, and objective-style Q&A in both English and Hindi, with rigorous annotation protocols for factual precision and instructional clarity. Benchmarked on BhashaBench-Ayur, AyurParam not only surpasses all open-source instruction-tuned models in its size class (1.5--3B parameters), but also demonstrates competitive or superior performance compared to much larger models. The results from AyurParam highlight the necessity for authentic domain adaptation and high-quality supervision in delivering reliable, culturally congruent AI for specialized medical knowledge.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '66f4e1ed8f94626b', 'authors': ['Mohd Nauman', 'Sravan Gvm', 'Vijay Devane', 'Shyam Pawar', 'Viraj Thakur', 'Kundeshwar Pundalik', 'Piyush Sawarkar', 'Rohit Saluja', 'Maunendra Desarkar', 'Ganesh Ramakrishnan'], 'affiliations': ['Indian Institute of Technology Bombay'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02374.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#small_models', '#low_resource', '#open_source', '#training', '#dataset', '#multilingual', '#science'], 'emoji': 'ğŸ§˜', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°: ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ¼ Ğ² Ğ°ÑÑ€Ğ²ĞµĞ´Ğµ', 'desc': 'AyurParam-2.9B â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ°ÑÑ€Ğ²ĞµĞ´Ñ‹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ fine-tuning Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ñ…Ğ¸Ğ½Ğ´Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. AyurParam Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (1.5-3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ AI Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'AyurParam-2.9B: Elevating Ayurveda with Specialized AI', 'desc': 'AyurParam-2.9B is a bilingual language model specifically designed for Ayurveda, showing superior performance in specialized medical tasks compared to other models of similar size. It is fine-tuned from an existing model using a carefully curated dataset that includes classical texts and clinical guidance in both English and Hindi. The model excels in context-aware reasoning and objective-style question answering, ensuring factual accuracy and clarity. The results emphasize the importance of domain adaptation and high-quality data in creating effective AI for specialized fields like traditional medicine.'}, 'zh': {'title': 'é˜¿è‚²å é™€åŒ»å­¦çš„ä¸“å±è¯­è¨€æ¨¡å‹', 'desc': 'AyurParam-2.9B æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é˜¿è‚²å é™€åŒ»å­¦çš„åŒè¯­è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡ç²¾ç»†è°ƒä¼˜ï¼Œè¡¨ç°ä¼˜äºåŒç±»æ¨¡å‹ã€‚å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸“ä¸šé¢†åŸŸæ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯éœ€è¦æ·±åšæ–‡åŒ–å’Œä¸“ä¸šçŸ¥è¯†çš„ä¼ ç»ŸåŒ»å­¦ç³»ç»Ÿã€‚AyurParam-2.9B ä½¿ç”¨äº†ç»è¿‡ä¸¥æ ¼ç­›é€‰çš„é˜¿è‚²å é™€æ•°æ®é›†ï¼Œæ¶µç›–ç»å…¸æ–‡æœ¬å’Œä¸´åºŠæŒ‡å¯¼ï¼Œç¡®ä¿äº†ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œæ¸…æ™°æ€§ã€‚è¯¥æ¨¡å‹åœ¨ BhashaBench-Ayur åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰åŒç±»å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›ä»»åŠ¡ä¸Šä¸æ›´å¤§æ¨¡å‹çš„è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02366', 'title': 'LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context', 'url': 'https://huggingface.co/papers/2511.02366', 'abstract': 'LiveSecBench is a continuously updated safety benchmark for Chinese-language LLMs, evaluating them across six critical dimensions including legality, ethics, factuality, privacy, adversarial robustness, and reasoning safety.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'e491da18642fb2aa', 'authors': ['Yudong Li', 'Zhongliang Yang', 'Kejiang Chen', 'Wenxuan Wang', 'Tianxin Zhang', 'Sifang Wan', 'Kecheng Wang', 'Haitian Li', 'Xu Wang', 'Lefan Cheng', 'Youdan Yang', 'Baocheng Chen', 'Ziyu Liu', 'Yufei Sun', 'Liyan Wu', 'Wenya Wen', 'Xingchi Gu', 'Peiru Yang'], 'affiliations': ['Alibaba', 'Ant Group', 'Peking University', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02366.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#low_resource', '#multilingual', '#reasoning', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ–Ğ¸Ğ²Ğ¾Ğ¹ Ñ‰Ğ¸Ñ‚: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LiveSecBench â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ğ·Ğ°ĞºĞ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ¸ĞºĞ°, Ñ„Ğ°ĞºÑ‚-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°, Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ²ÑƒÑ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ³Ñ€Ğ¾Ğ·, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ¾ 18 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Ensuring Safety in Chinese LLMs with LiveSecBench', 'desc': 'LiveSecBench is a safety benchmark designed for evaluating Chinese-language large language models (LLMs) across six important areas: legality, ethics, factuality, privacy, adversarial robustness, and reasoning safety. This benchmark is unique because it is continuously updated to reflect new challenges and threats in AI safety, ensuring that it remains relevant. Currently, LiveSecBench has assessed 18 different LLMs, providing insights into their performance in the context of Chinese legal and social standards. The results are publicly available, allowing for transparency and ongoing improvement in AI safety practices.'}, 'zh': {'title': 'ä¸­æ–‡LLMå®‰å…¨è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'LiveSecBenchæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºä¸­æ–‡è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡çš„åŠ¨æ€å®‰å…¨åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨å…­ä¸ªå…³é”®ç»´åº¦ä¸Šçš„è¡¨ç°ã€‚è¿™å…­ä¸ªç»´åº¦åŒ…æ‹¬åˆæ³•æ€§ã€ä¼¦ç†æ€§ã€äº‹å®æ€§ã€éšç§æ€§ã€å¯¹æŠ—é²æ£’æ€§å’Œæ¨ç†å®‰å…¨æ€§ï¼Œå‡åŸºäºä¸­å›½çš„æ³•å¾‹å’Œç¤¾ä¼šæ¡†æ¶ã€‚è¯¥åŸºå‡†é€šè¿‡åŠ¨æ€æ›´æ–°çš„æ–¹å¼ä¿æŒå…¶ç›¸å…³æ€§ï¼Œè®¡åˆ’åœ¨ä¸‹ä¸€ä¸ªæ›´æ–°ä¸­åŠ å…¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå®‰å…¨å’Œä»£ç†å®‰å…¨ç­‰æ–°å¨èƒå‘é‡ã€‚ç›®å‰ï¼ŒLiveSecBenchï¼ˆv251030ï¼‰å·²è¯„ä¼°äº†18ä¸ªLLMï¼Œä¸ºä¸­æ–‡ç¯å¢ƒä¸‹çš„äººå·¥æ™ºèƒ½å®‰å…¨æä¾›äº†å…¨é¢çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.19278', 'title': 'D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2510.19278', 'abstract': 'A novel framework, Detector-to-Differentiable (D2D), transforms non-differentiable detection models into differentiable critics to improve object counting accuracy in text-to-image diffusion models with minimal impact on image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 22', 'zh': '10æœˆ22æ—¥'}, 'hash': '1e6291ef238d1985', 'authors': ['Nobline Yoo', 'Olga Russakovsky', 'Ye Zhu'], 'affiliations': ['Department of Computer Science, Princeton University', 'LIX, Ã‰cole Polytechnique, IP Paris'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.19278.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#cv', '#diffusion', '#multimodal'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº D2D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµĞ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ñƒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ² Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² (Ğ´Ğ¾ 13,7%) Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Transforming Detection for Better Object Counting in AI Images', 'desc': 'The paper introduces a new framework called Detector-to-Differentiable (D2D) that enhances object counting in text-to-image diffusion models. It addresses the challenge of integrating non-differentiable detection models into the training process, which typically requires differentiable critics for effective gradient guidance. By transforming detection outputs into differentiable forms using custom activation functions, D2D allows these models to contribute to the optimization of image generation. The results show significant improvements in counting accuracy across various benchmarks while maintaining high image quality and low computational costs.'}, 'zh': {'title': 'å°†æ£€æµ‹æ¨¡å‹è½¬åŒ–ä¸ºå¯å¾®åˆ†è¯„ä¼°å™¨ï¼Œæå‡ç‰©ä½“è®¡æ•°å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºDetector-to-Differentiableï¼ˆD2Dï¼‰ï¼Œæ—¨åœ¨å°†éå¯å¾®åˆ†çš„æ£€æµ‹æ¨¡å‹è½¬åŒ–ä¸ºå¯å¾®åˆ†çš„è¯„ä¼°å™¨ï¼Œä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç‰©ä½“è®¡æ•°å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨è¾…åŠ©è®¡æ•°ç½‘ç»œä½œä¸ºå¤–éƒ¨è¯„ä¼°å™¨ï¼Œä½†è¿™äº›è¯„ä¼°å™¨åªèƒ½ç”¨äºå¯å¾®åˆ†çš„å›å½’æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚D2Dæ¡†æ¶é€šè¿‡è®¾è®¡è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°ï¼Œå°†æ£€æµ‹å™¨çš„è¾“å‡ºè½¬åŒ–ä¸ºè½¯äºŒå…ƒæŒ‡ç¤ºå™¨ï¼Œä»è€Œåˆ©ç”¨å…¶ä¼˜è¶Šçš„è®¡æ•°èƒ½åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ç‰©ä½“è®¡æ•°çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¯¹å›¾åƒè´¨é‡å’Œè®¡ç®—å¼€é”€çš„å½±å“æœ€å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02219', 'title': 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning\n  in Tabular Data', 'url': 'https://huggingface.co/papers/2511.02219', 'abstract': "A framework combining query decomposition, table sanitization, and program-of-thoughts reasoning improves large language models' performance on complex tabular numerical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose \\method, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that \\method consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '38b945007537bdab', 'authors': ['Changjiang Jiang', 'Fengchang Yu', 'Haihua Chen', 'Wei Lu', 'Jin Zeng'], 'affiliations': ['Hubei University of Economics', 'University of North Texas', 'Wuhan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02219.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#leakage'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¢Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ² LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, ÑĞ°Ğ½Ğ¸Ñ‚Ğ°Ğ¹Ğ·ĞµÑ€ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°, Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CalTab151 Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 6-20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing LLMs for Complex Tabular Reasoning', 'desc': "This paper presents a new framework that enhances large language models (LLMs) for complex reasoning tasks involving tabular data. The framework includes three main components: a query decomposer to simplify complex questions, a table sanitizer to clean up noisy data, and a program-of-thoughts reasoner that generates code to find answers. By introducing a new dataset called CalTab151, the authors ensure unbiased evaluation and improve the models' performance on numerical reasoning tasks. Experimental results show significant accuracy improvements over existing methods, demonstrating the framework's effectiveness in boosting LLM capabilities."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚è¡¨æ ¼æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç»“åˆäº†æŸ¥è¯¢åˆ†è§£ã€è¡¨æ ¼æ¸…ç†å’Œæ€ç»´ç¨‹åºæ¨ç†ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼šæŸ¥è¯¢åˆ†è§£å™¨ã€è¡¨æ ¼æ¸…ç†å™¨å’ŒåŸºäºæ€ç»´ç¨‹åºçš„æ¨ç†å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤æ‚é—®é¢˜å’Œå™ªå£°æ•°æ®ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†CalTab151ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å…¬æ­£æ€§å¹¶å‡å°‘äº†æ•°æ®æ³„æ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚è¡¨æ ¼æ•°å€¼æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01502', 'title': 'Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning', 'url': 'https://huggingface.co/papers/2511.01502', 'abstract': 'A discriminative approach to depth and ego-motion estimation leverages geometric constraints to improve performance and robustness in 3D perception tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '37fde66b70ece002', 'authors': ['Mengtan Zhang', 'Zizhan Guo', 'Hongbo Zhao', 'Yi Feng', 'Zuyi Xiong', 'Yue Wang', 'Shaoyi Du', 'Hanli Wang', 'Rui Fan'], 'affiliations': ['College of Electronic & Information Engineering, Shanghai Institute of Intelligent Science and Technology, Shanghai Research Institute for Intelligent Autonomous Systems, the State Key Laboratory of Autonomous Intelligent Unmanned Systems, the Frontiers Science Center for Intelligent Autonomous Systems (Ministry of Education), and Shanghai Key Laboratory of Intelligent Autonomous Systems, Tongji University', 'College of Electronic & Information Engineering, Tongji University', 'College of Electronic & Information Engineering, the School of Computer Science and Technology, and the Key Laboratory of Embedded System and Service Computing (Ministry of Education), Tongji University', 'Department of Control Science and Engineering, Zhejiang University', "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University", "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, the National Engineering Research Center for Visual Information and Applications, and the Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University", 'Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01502.jpg', 'data': {'categories': ['#dataset', '#3d', '#cv'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑĞ³Ğ¾-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ… Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞµĞ¹ Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ ÑĞ³Ğ¾-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DiMoDE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing 3D Perception with Discriminative Depth and Motion Estimation', 'desc': 'This paper presents a new method for estimating depth and ego-motion in 3D perception tasks using a discriminative approach. It addresses limitations in existing unsupervised learning methods that often treat ego-motion as a secondary task, which can hinder performance. By applying geometric constraints to each motion component separately, the proposed framework enhances the reliability and robustness of the estimations. The DiMoDE framework demonstrates superior performance on various datasets, especially in challenging scenarios, by leveraging the geometric relationships between depth and motion components.'}, 'zh': {'title': 'åˆ©ç”¨å‡ ä½•çº¦æŸæå‡æ·±åº¦ä¸è‡ªæˆ‘è¿åŠ¨ä¼°è®¡çš„å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ¤åˆ«æ€§çš„æ–¹æ³•æ¥ä¼°è®¡æ·±åº¦å’Œè‡ªæˆ‘è¿åŠ¨ï¼Œè¿™ä¸¤è€…æ˜¯3Dæ„ŸçŸ¥ä»»åŠ¡çš„åŸºç¡€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†è‡ªæˆ‘è¿åŠ¨è§†ä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œé™åˆ¶äº†å‡ ä½•çº¦æŸçš„æœ‰æ•ˆåˆ©ç”¨ï¼Œä»è€Œé™ä½äº†åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„å¯é æ€§ã€‚é€šè¿‡å¯¹è¿åŠ¨æˆåˆ†çš„åˆ¤åˆ«æ€§å¤„ç†ï¼Œæœ¬æ–‡åˆ©ç”¨å‡ ä½•è§„å¾‹æ¥æ”¹å–„æ·±åº¦å’Œè‡ªæˆ‘è¿åŠ¨çš„ä¼°è®¡ã€‚æœ€ç»ˆï¼ŒDiMoDEæ¡†æ¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†å’Œæ–°æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01450', 'title': 'Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for\n  Improving Video Generation', 'url': 'https://huggingface.co/papers/2511.01450', 'abstract': 'A novel approach combining GT-Pair and Reg-DPO enhances video generation quality by addressing data construction, training stability, and memory consumption challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '21959240862c93fd', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rlhf', '#data', '#training', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ GT-Pair Ğ¸ Reg-DPO. GT-Pair Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Reg-DPO Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ supervised fine-tuning Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ DPO, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ FSDP Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… image-to-video Ğ¸ text-to-video.'}, 'en': {'title': 'Revolutionizing Video Generation with GT-Pair and Reg-DPO', 'desc': 'This paper presents a new method that combines GT-Pair and Reg-DPO to improve the quality of video generation. GT-Pair creates high-quality preference pairs using real videos as positive examples and model-generated videos as negatives, which removes the need for external annotations. Reg-DPO enhances training stability and output quality by adding a regularization term to the Direct Preference Optimization objective. The approach also optimizes memory usage, allowing for nearly three times the training capacity compared to previous methods, leading to better performance in generating videos.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç»“åˆäº†GT-Pairå’ŒReg-DPOï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚GT-Pairé€šè¿‡ä½¿ç”¨çœŸå®è§†é¢‘ä½œä¸ºæ­£æ ·æœ¬å’Œæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œè‡ªåŠ¨æ„å»ºé«˜è´¨é‡çš„åå¥½å¯¹ï¼Œé¿å…äº†å¤–éƒ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚Reg-DPOåˆ™å°†SFTæŸå¤±ä½œä¸ºæ­£åˆ™åŒ–é¡¹å¼•å…¥DPOç›®æ ‡ï¼Œä»¥å¢å¼ºè®­ç»ƒçš„ç¨³å®šæ€§å’Œç”Ÿæˆçš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œç»“åˆFSDPæ¡†æ¶å’Œå¤šç§å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒèƒ½åŠ›ä¸Šå‡ ä¹æé«˜äº†ä¸‰å€ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.03276', 'title': 'Diffusion Language Models are Super Data Learners', 'url': 'https://huggingface.co/papers/2511.03276', 'abstract': 'Diffusion language models outperform autoregressive models in low-data settings due to any-order modeling, iterative bidirectional denoising, and Monte Carlo augmentation, and maintain advantages even at scale.  \t\t\t\t\tAI-generated summary \t\t\t\t Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag and > 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime.', 'score': 124, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '3393b24fd7b4f0a3', 'authors': ['Jinjie Ni', 'Qian Liu', 'Longxu Dou', 'Chao Du', 'Zili Wang', 'Hang Yan', 'Tianyu Pang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Shanghai Qiji Zhifeng Co., Ltd.', 'StepFun'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03276.jpg', 'data': {'categories': ['#small_models', '#low_resource', '#optimization', '#architecture', '#diffusion', '#plp', '#training', '#synthetic'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Regression-Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Regression-Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ€Ñ‘Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ»ÑĞ±Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ¿ĞµÑ€ĞµÑĞµÑ‡ĞµĞ½Ğ¸Ñ (crossover) Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.7 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° 10 Ğ¼Ğ»Ñ€Ğ´ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Python, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Regression-Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HellaSwag Ğ¸ MMLU Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑĞºĞ¾Ğ².'}, 'en': {'title': 'Diffusion Models: The New Leaders in Low-Data Language Tasks', 'desc': 'This paper discusses how diffusion language models (DLMs) perform better than autoregressive (AR) models when there is limited data available. The authors identify three key reasons for this advantage: DLMs can model data in any order, they use iterative bidirectional denoising for better training efficiency, and they incorporate Monte Carlo augmentation to enhance performance. They demonstrate that even with a smaller dataset, DLMs can achieve higher accuracy by training longer, and this trend continues as the model size increases. The findings suggest that DLMs maintain their superiority over AR models even as the amount of data grows, challenging previous assumptions about model performance in low-data scenarios.'}, 'zh': {'title': 'æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨ä½æ•°æ®ç¯å¢ƒä¸­çš„ä¼˜åŠ¿', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆDLMï¼‰åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»ä½•é¡ºåºå»ºæ¨¡ã€è¿­ä»£åŒå‘å»å™ªå’Œè’™ç‰¹å¡æ´›å¢å¼ºç­‰æ–¹é¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒDLMé€šè¿‡è®­ç»ƒæ›´å¤šçš„å‘¨æœŸï¼Œèƒ½å¤ŸæŒç»­è¶…è¶Šè‡ªå›å½’æ¨¡å‹ï¼ˆARï¼‰ã€‚éšç€æ•°æ®é‡çš„å¢åŠ ï¼ŒDLMçš„ä¼˜åŠ¿ä¼šé€æ¸å‡å°ï¼Œä½†åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸­ä»ç„¶ä¿æŒæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚è®ºæ–‡è¿˜æŒ‡å‡ºï¼ŒéªŒè¯äº¤å‰ç†µçš„ä¸Šå‡å¹¶ä¸ä¸€å®šæ„å‘³ç€ä¸‹æ¸¸æ€§èƒ½çš„ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03334', 'title': 'UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal\n  Interactions', 'url': 'https://huggingface.co/papers/2511.03334', 'abstract': "UniAVGen, a unified framework using dual Diffusion Transformers and Asymmetric Cross-Modal Interaction, enhances audio-video generation by ensuring synchronization and consistency with fewer training samples.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.", 'score': 51, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '24a0cd36ecc1379b', 'authors': ['Guozhen Zhang', 'Zixiang Zhou', 'Teng Hu', 'Ziqiao Peng', 'Youliang Zhang', 'Yi Chen', 'Yuan Zhou', 'Qinglin Lu', 'Limin Wang'], 'affiliations': ['Renmin University of China', 'Shanghai AI Lab', 'Shanghai Jiao Tong University', 'State Key Laboratory for Novel Software Technology, Nanjing University', 'Tencent Hunyuan', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03334.jpg', 'data': {'categories': ['#video', '#architecture', '#audio', '#multimodal', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸', 'desc': 'UniAVGen â€” ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Diffusion Transformers Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ±, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Face-Aware Modulation Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ°, Ğ° Modality-Aware Classifier-Free Guidance ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ UniAVGen Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'UniAVGen: Synchronizing Audio and Video with Fewer Samples!', 'desc': 'UniAVGen is a novel framework designed to improve the generation of audio and video together by using advanced machine learning techniques. It employs dual Diffusion Transformers to create a shared space for audio and video data, ensuring they are synchronized and semantically consistent. The framework features an Asymmetric Cross-Modal Interaction mechanism that allows for precise alignment of audio and video, enhancing the quality of generated content. With fewer training samples required, UniAVGen demonstrates significant improvements in synchronization and consistency compared to existing methods.'}, 'zh': {'title': 'éŸ³è§†é¢‘ç”Ÿæˆçš„æ–°çºªå…ƒï¼šUniAVGen', 'desc': 'UniAVGenæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œåˆ©ç”¨åŒé‡æ‰©æ•£å˜æ¢å™¨å’Œä¸å¯¹ç§°è·¨æ¨¡æ€äº¤äº’ï¼Œæå‡éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„åŒæ­¥æ€§å’Œä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒåˆ†æ”¯è”åˆåˆæˆæ¶æ„ï¼Œæ„å»ºäº†ä¸€ä¸ªè¿è´¯çš„è·¨æ¨¡æ€æ½œåœ¨ç©ºé—´ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¸å¯¹ç§°è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ï¼Œç¡®ä¿äº†ç²¾ç¡®çš„æ—¶ç©ºåŒæ­¥å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUniAVGenåœ¨ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œèƒ½å¤Ÿåœ¨éŸ³é¢‘è§†é¢‘åŒæ­¥ã€éŸ³è‰²ä¸€è‡´æ€§å’Œæƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03001', 'title': 'LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied\n  Environments with Tool Augmentation', 'url': 'https://huggingface.co/papers/2511.03001', 'abstract': 'LEGO-Eval and LEGO-Bench improve the evaluation and generation of realistic 3D scenes by aligning detailed instructions with scene components, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.', 'score': 46, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '84cac177d315512a', 'authors': ['Gyeom Hwangbo', 'Hyungjoo Chae', 'Minseok Kang', 'Hyeonjong Ju', 'Soohyun Oh', 'Jinyoung Yeo'], 'affiliations': ['Georgia Institute of Technology', 'Yonsei University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03001.jpg', 'data': {'categories': ['#benchmark', '#3d', '#agents', '#dataset', '#science'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ 3D-ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LEGO-Eval Ğ¸ LEGO-Bench â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞºĞ»Ğ°Ğ´ĞºĞ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LEGO-Eval ÑĞ²Ğ½Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² â€” ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ ÑÑ†ĞµĞ½Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 10%.'}, 'en': {'title': 'Enhancing 3D Scene Realism with LEGO-Eval and LEGO-Bench', 'desc': 'The paper introduces LEGO-Eval and LEGO-Bench, two tools designed to enhance the evaluation and generation of realistic 3D scenes. LEGO-Eval provides a framework that accurately assesses how well generated scenes align with detailed, fine-grained instructions, addressing the shortcomings of existing evaluation methods. LEGO-Bench offers a set of complex instructions that reflect real-world environments, which are crucial for training embodied agents effectively. The results show that these new tools significantly improve the alignment assessment and highlight the limitations of current scene generation methods.'}, 'zh': {'title': 'æå‡3Dåœºæ™¯ç”Ÿæˆçš„çœŸå®æ„Ÿä¸è¯„ä¼°ç²¾åº¦', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†LEGO-Evalå’ŒLEGO-Benchï¼Œæ—¨åœ¨æ”¹å–„3Dåœºæ™¯çš„è¯„ä¼°å’Œç”Ÿæˆã€‚ç°æœ‰çš„ç”Ÿæˆæ–¹æ³•å¸¸å¸¸æ— æ³•ç”Ÿæˆå…·æœ‰çœŸå®ç©ºé—´å¸ƒå±€å’Œç‰©ä½“å±æ€§çš„åœºæ™¯ï¼Œä¸»è¦æ˜¯å› ä¸ºæŒ‡ä»¤ä¸å¤Ÿè¯¦ç»†ã€‚é€šè¿‡å¼•å…¥æ›´ç»†è‡´çš„æŒ‡ä»¤ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æŒ‡å¯¼3Dåœºæ™¯åˆæˆï¼Œä»è€Œæé«˜ç”Ÿæˆåœºæ™¯çš„çœŸå®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLEGO-Evalåœ¨è¯„ä¼°åœºæ™¯ä¸æŒ‡ä»¤çš„å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒæˆåŠŸç‡ä¹Ÿæ˜¾è‘—æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02734', 'title': 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents', 'url': 'https://huggingface.co/papers/2511.02734', 'abstract': "CostBench evaluates Large Language Model agents' cost-aware planning and adaptability in response to dynamic changes, revealing significant gaps in current models' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Current evaluations of Large Language Model (LLM) agents primarily emphasize task completion, often overlooking resource efficiency and adaptability. This neglects a crucial capability: agents' ability to devise and adjust cost-optimal plans in response to changing environments. To bridge this gap, we introduce CostBench, a scalable, cost-centric benchmark designed to evaluate agents' economic reasoning and replanning abilities. Situated in the travel-planning domain, CostBench comprises tasks solvable via multiple sequences of atomic and composite tools with diverse, customizable costs. It also supports four types of dynamic blocking events, such as tool failures and cost changes, to simulate real-world unpredictability and necessitate agents to adapt in real time. Evaluating leading open-sourced and proprietary models on CostBench reveals a substantial gap in cost-aware planning: agents frequently fail to identify cost-optimal solutions in static settings, with even GPT-5 achieving less than 75% exact match rate on the hardest tasks, and performance further dropping by around 40% under dynamic conditions. By diagnosing these weaknesses, CostBench lays the groundwork for developing future agents that are both economically rational and robust.", 'score': 20, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '981b46d11b82e81e', 'authors': ['Jiayu Liu', 'Cheng Qian', 'Zhaochen Su', 'Qing Zong', 'Shijue Huang', 'Bingxiang He', 'Yi R. Fung'], 'affiliations': ['Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02734.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset'], 'emoji': 'ğŸ’°', 'ru': {'title': 'Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'CostBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¹ÑÑ ÑÑ€ĞµĞ´Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, CostBench Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ»Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ½. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ: Ğ´Ğ°Ğ¶Ğµ GPT-5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 75% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¸ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 40% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'CostBench: Bridging the Gap in Cost-Aware Planning for LLMs', 'desc': 'The paper introduces CostBench, a new benchmark for evaluating Large Language Model (LLM) agents on their ability to plan cost-effectively and adapt to changing conditions. It highlights that current assessments focus mainly on task completion, neglecting the important aspect of resource efficiency. CostBench is designed to test agents in a travel-planning context, incorporating various tasks with customizable costs and dynamic events that mimic real-world unpredictability. The findings reveal that existing models, including GPT-5, struggle with cost-aware planning, achieving low performance rates, especially under dynamic scenarios, indicating a need for improvement in economic reasoning and adaptability.'}, 'zh': {'title': 'æå‡ä»£ç†çš„æˆæœ¬æ„è¯†ä¸é€‚åº”èƒ½åŠ›', 'desc': 'CostBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œæˆæœ¬æ„è¯†è§„åˆ’å’Œé€‚åº”èƒ½åŠ›çš„åŸºå‡†å·¥å…·ã€‚å½“å‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨ä»»åŠ¡å®Œæˆï¼Œè€Œå¿½è§†äº†èµ„æºæ•ˆç‡å’Œé€‚åº”æ€§ï¼Œè¿™å¯¹äºä»£ç†åœ¨å˜åŒ–ç¯å¢ƒä¸­åˆ¶å®šå’Œè°ƒæ•´æˆæœ¬æœ€ä¼˜è®¡åˆ’è‡³å…³é‡è¦ã€‚CostBench è®¾è®¡äº†å¤šç§ä»»åŠ¡ï¼Œæ”¯æŒä¸åŒçš„å·¥å…·å’Œå¯å®šåˆ¶çš„æˆæœ¬ï¼Œå¹¶æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­çš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡å¯¹é¢†å…ˆæ¨¡å‹çš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨æˆæœ¬æ„è¯†è§„åˆ’æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨åŠ¨æ€æ¡ä»¶ä¸‹è¡¨ç°æ›´å·®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02818', 'title': 'Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning', 'url': 'https://huggingface.co/papers/2511.02818', 'abstract': 'Orion-MSP, a tabular in-context learning architecture, addresses limitations in current models by incorporating multi-scale processing, block-sparse attention, and a Perceiver-style memory, achieving state-of-the-art performance on diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP .', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '3b39df0f331db829', 'authors': ['Mohamed Bouadi', 'Pratinav Seth', 'Aditya Tanna', 'Vinay Kumar Sankarapu'], 'affiliations': ['Lexsi Labs, India & France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02818.jpg', 'data': {'categories': ['#benchmark', '#architecture'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'Orion-MSP â€” ÑÑ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (in-context learning) Ğ½Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾-Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (block-sparse attention) Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ· ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Perceiver, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾. ĞĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Orion-MSP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹.'}, 'en': {'title': 'Revolutionizing Tabular Data Learning with Orion-MSP', 'desc': 'Orion-MSP is a new architecture designed for in-context learning with tabular data, which is commonly used in real-world applications. It improves upon existing models by using multi-scale processing to better understand complex feature interactions and block-sparse attention to manage large datasets efficiently. Additionally, it incorporates a Perceiver-style memory that allows for effective communication between different components of the model. As a result, Orion-MSP achieves top performance on various benchmarks while being scalable for high-dimensional data.'}, 'zh': {'title': 'Orion-MSPï¼šé«˜æ•ˆçš„è¡¨æ ¼æ•°æ®å­¦ä¹ æ–°æ ‡å‡†', 'desc': 'Orion-MSPæ˜¯ä¸€ç§æ–°çš„è¡¨æ ¼æ•°æ®ä¸Šä¸‹æ–‡å­¦ä¹ æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚å®ƒé€šè¿‡å¤šå°ºåº¦å¤„ç†ã€å—ç¨€ç–æ³¨æ„åŠ›å’ŒPerceiveré£æ ¼çš„è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ç‰¹å¾ä¹‹é—´çš„å±‚æ¬¡äº¤äº’ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”åœ¨å¤„ç†é«˜ç»´è¡¨æ ¼æ—¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚Orion-MSPä¸ºé«˜æ•ˆçš„è¡¨æ ¼æ•°æ®ä¸Šä¸‹æ–‡å­¦ä¹ è®¾ç«‹äº†æ–°çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02802', 'title': 'TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models', 'url': 'https://huggingface.co/papers/2511.02802', 'abstract': 'TabTune is a unified library that standardizes the workflow for tabular foundation models, supporting various adaptation strategies and evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune .', 'score': 14, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '0f9518cd87a3d413', 'authors': ['Aditya Tanna', 'Pratinav Seth', 'Mohamed Bouadi', 'Utsav Avaiya', 'Vinay Kumar Sankarapu'], 'affiliations': ['Lexsi Labs, India & France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02802.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ°Ğ±ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… foundation models', 'desc': 'TabTune â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ñ‚Ğ°Ğ±ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… foundation models, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ÑĞµĞ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ zero-shot inference, meta-learning, supervised fine-tuning Ğ¸ parameter-efficient fine-tuning. TabTune Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¸Ğ½Ğ³ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ fairness, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… foundation models.'}, 'en': {'title': 'Streamlining Tabular Model Workflows with TabTune', 'desc': 'TabTune is a comprehensive library designed to streamline the workflow for tabular foundation models in machine learning. It addresses challenges like inconsistent preprocessing and fragmented APIs by providing a unified interface for various adaptation strategies, including zero-shot inference and supervised fine-tuning. The library also automates preprocessing and integrates evaluation metrics for performance, calibration, and fairness, ensuring reliable deployment of models. By promoting extensibility and reproducibility, TabTune facilitates consistent benchmarking across different adaptation methods for tabular data.'}, 'zh': {'title': 'TabTuneï¼šè¡¨æ ¼æ¨¡å‹çš„ç»Ÿä¸€å·¥ä½œæµè§£å†³æ–¹æ¡ˆ', 'desc': 'TabTuneæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åº“ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è¡¨æ ¼åŸºç¡€æ¨¡å‹çš„å·¥ä½œæµç¨‹ã€‚å®ƒæ”¯æŒå¤šç§é€‚åº”ç­–ç•¥å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨é¢„å¤„ç†ã€APIå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚é€šè¿‡å•ä¸€æ¥å£ï¼ŒTabTuneæä¾›å¯¹ä¸ƒç§æœ€å…ˆè¿›æ¨¡å‹çš„ä¸€è‡´è®¿é—®ï¼Œæ”¯æŒé›¶æ ·æœ¬æ¨ç†ã€å…ƒå­¦ä¹ ã€ç›‘ç£å¾®è°ƒå’Œå‚æ•°é«˜æ•ˆå¾®è°ƒç­‰ç­–ç•¥ã€‚è¯¥æ¡†æ¶è‡ªåŠ¨åŒ–æ¨¡å‹æ„ŸçŸ¥çš„é¢„å¤„ç†ï¼Œå†…éƒ¨ç®¡ç†æ¶æ„å¼‚æ„æ€§ï¼Œå¹¶é›†æˆæ€§èƒ½ã€æ ¡å‡†å’Œå…¬å¹³æ€§è¯„ä¼°æ¨¡å—ï¼Œä¿ƒè¿›äº†è¡¨æ ¼åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œå¯é‡å¤æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01294', 'title': 'Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects', 'url': 'https://huggingface.co/papers/2511.01294', 'abstract': 'Kinematify is an automated framework that synthesizes articulated objects from RGB images or textual descriptions, addressing challenges in inferring kinematic topologies and estimating joint parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or textual descriptions. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '91edb1158b49e80b', 'authors': ['Jiawei Wang', 'Dingyou Wang', 'Jiaming Hu', 'Qixuan Zhang', 'Jingyi Yu', 'Lan Xu'], 'affiliations': ['Deemos Technology Co., Ltd.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01294.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#multimodal', '#robotics', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Kinematify â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑˆĞ°Ñ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ¿ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ´ĞµÑ€ĞµĞ²Ğ° ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Automating Articulated Object Synthesis for Robotics', 'desc': 'Kinematify is an innovative framework that automates the creation of articulated object models from RGB images or textual descriptions. It tackles the complex problems of determining kinematic structures and estimating joint parameters, which are crucial for robotic manipulation and simulation tasks. By integrating Monte Carlo Tree Search (MCTS) for structural inference with geometry-driven optimization, Kinematify generates accurate and functional models for objects with high degrees of freedom. The framework shows significant advancements in accuracy and scalability compared to existing methods, making it a valuable tool for robotics and motion planning.'}, 'zh': {'title': 'Kinematifyï¼šè‡ªåŠ¨åˆæˆå…³èŠ‚ç‰©ä½“çš„åˆ›æ–°æ¡†æ¶', 'desc': 'Kinematifyæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥ä»RGBå›¾åƒæˆ–æ–‡æœ¬æè¿°ä¸­åˆæˆå…³èŠ‚ç‰©ä½“ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ¨æ–­é«˜è‡ªç”±åº¦ç‰©ä½“çš„è¿åŠ¨æ‹“æ‰‘å’Œä¼°è®¡å…³èŠ‚å‚æ•°çš„æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œå‡ ä½•é©±åŠ¨çš„ä¼˜åŒ–ï¼ŒKinematifyèƒ½å¤Ÿç”Ÿæˆç‰©ç†ä¸€è‡´ä¸”åŠŸèƒ½æœ‰æ•ˆçš„æè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKinematifyåœ¨æ³¨å†Œå’Œè¿åŠ¨æ‹“æ‰‘å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03628', 'title': 'LiveTradeBench: Seeking Real-World Alpha with Large Language Models', 'url': 'https://huggingface.co/papers/2511.03628', 'abstract': 'LiveTradeBench evaluates LLMs in dynamic trading environments to assess decision-making under real-time uncertainty and market volatility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'd928cc902d93698c', 'authors': ['Haofei Yu', 'Fenghai Li', 'Jiaxuan You'], 'affiliations': ['University of Illinois, Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03628.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning', '#leakage'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğµ: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ¿Ğ¾Ğ´ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ñ‹Ğ½ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ LiveTradeBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ğ¾Ğ»Ğ°Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ€Ñ‹Ğ½ĞºĞ°. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸. LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ², Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ñ€Ğ¸ÑĞº Ğ¸ Ğ´Ğ¾Ñ…Ğ¾Ğ´, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞ¿ĞµÑ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Evaluating LLMs in Real-Time Trading: Bridging Static Tests and Dynamic Markets', 'desc': 'The paper introduces LiveTradeBench, a novel framework for evaluating large language models (LLMs) in dynamic trading scenarios. Unlike traditional benchmarks that assess isolated reasoning, LiveTradeBench simulates real-time market conditions, allowing LLMs to make decisions under uncertainty and volatility. It incorporates live data streaming, multi-asset portfolio management, and diverse market environments to provide a comprehensive evaluation of LLM performance. The findings reveal that high scores in static evaluations do not guarantee effective trading strategies, highlighting the need for benchmarks that reflect real-world decision-making challenges.'}, 'zh': {'title': 'å®æ—¶äº¤æ˜“ç¯å¢ƒä¸‹çš„å†³ç­–è¯„ä¼°', 'desc': 'LiveTradeBench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ¨æ€äº¤æ˜“ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›çš„å·¥å…·ã€‚å®ƒé€šè¿‡å®æ—¶å¸‚åœºæ•°æ®æµå’Œæ–°é—»ï¼Œæ¶ˆé™¤äº†å¯¹ç¦»çº¿å›æµ‹çš„ä¾èµ–ï¼Œä»è€Œæ•æ‰çœŸå®çš„ä¸ç¡®å®šæ€§ã€‚è¯¥å¹³å°æ”¯æŒå¤šèµ„äº§ç»„åˆç®¡ç†ï¼Œæ•´åˆé£é™©ç®¡ç†å’Œè·¨èµ„äº§æ¨ç†ï¼Œé€‚ç”¨äºä¸åŒå¸‚åœºç¯å¢ƒçš„è¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé™æ€è¯„ä¼°ä¸çœŸå®ä¸–ç•Œèƒ½åŠ›ä¹‹é—´å­˜åœ¨å·®è·ï¼Œå¼ºè°ƒäº†åœ¨å®æ—¶ä¸ç¡®å®šæ€§ä¸‹æµ‹è¯•è¿ç»­å†³ç­–çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03146', 'title': 'MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive\n  Capacity', 'url': 'https://huggingface.co/papers/2511.03146', 'abstract': "MME-CC is a vision-grounded benchmark that evaluates multimodal large language models' cognitive capacity across spatial, geometric, and knowledge-based reasoning tasks, revealing weaknesses in spatial and geometric reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -> reason -> verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design.", 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '44c56671af0489ab', 'authors': ['Kaiyuan Zhang', 'Chenghao Yang', 'Zhoufutu Wen', 'Sihang Yuan', 'Qiuyue Wang', 'Chaoyi Huang', 'Guosheng Zhu', 'He Wang', 'Huawenyu Lu', 'Jianing Wen', 'Jianpeng Jiao', 'Lishu Luo', 'Longxiang Liu', 'Sijin Wu', 'Xiaolei Zhu', 'Xuanliang Zhang', 'Ge Zhang', 'Yi Lin', 'Guang Shi', 'Chaoyou Fu', 'Wenhao Huang'], 'affiliations': ['ByteDance Seed', 'Nanjing University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03146.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#cv', '#multimodal', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MME-CC, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 16 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ĞµÑ‚ 30%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸: Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµĞ½Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLMs Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ â†’ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ â†’ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°.'}, 'en': {'title': 'Evaluating Cognitive Capacity in Multimodal Models', 'desc': "The paper introduces MME-CC, a benchmark designed to evaluate the cognitive abilities of multimodal large language models (MLLMs) in reasoning tasks that involve visual information. It categorizes these tasks into spatial, geometric, and knowledge-based reasoning, highlighting the models' weaknesses in spatial and geometric reasoning. The study conducts experiments on 16 MLLMs, revealing that while some closed-source models perform better overall, they still struggle with tasks requiring spatial and geometric understanding. The findings emphasize the need for a deeper focus on cognitive capacity in the development and evaluation of MLLMs."}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›', 'desc': 'MME-CCæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´ã€å‡ ä½•å’ŒçŸ¥è¯†æ¨ç†ä»»åŠ¡ä¸­çš„è®¤çŸ¥èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†è¿‡äºå¼ºè°ƒæ–‡æœ¬æ¨ç†ï¼Œæœªèƒ½ç³»ç»Ÿåœ°æ•æ‰ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è®¤çŸ¥è¡Œä¸ºã€‚é€šè¿‡å¯¹16ä¸ªä»£è¡¨æ€§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç©ºé—´å’Œå‡ ä½•æ¨ç†èƒ½åŠ›æ™®éè¾ƒå¼±ï¼Œä¸”å­˜åœ¨å¸¸è§çš„é”™è¯¯æ¨¡å¼ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæ¨åŠ¨å°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ä½œä¸ºè¯„ä¼°å’Œæ¨¡å‹è®¾è®¡çš„æ ¸å¿ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02309', 'title': 'The Sequential Edge: Inverse-Entropy Voting Beats Parallel\n  Self-Consistency at Matched Compute', 'url': 'https://huggingface.co/papers/2511.02309', 'abstract': "Sequential scaling in language model reasoning outperforms parallel scaling across multiple models and benchmarks, with inverse-entropy weighted voting further enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We revisit test-time scaling for language model reasoning and ask a fundamental question: at equal token budget and compute, is it better to run multiple independent chains in parallel, or to run fewer chains that iteratively refine through sequential steps? Through comprehensive evaluation across 5 state-of-the-art open source models and 3 challenging reasoning benchmarks, we find that sequential scaling where chains explicitly build upon previous attempts consistently outperforms the dominant parallel self-consistency paradigm in 95.6% of configurations with gains in accuracy upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel training-free method to further boost the accuracy of sequential scaling. By weighing answers in proportion to the inverse entropy of their reasoning chains, we increase our success rate over parallel majority and establish it as the optimal test-time scaling strategy. Our findings fundamentally challenge the parallel reasoning orthodoxy that has dominated test-time scaling since Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning sequential refinement as the robust default for modern LLM reasoning and necessitating a paradigm shift in how we approach inference-time optimization.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'bddb0cb10cb55da4', 'authors': ['Aman Sharma', 'Paras Chopra'], 'affiliations': ['Lossfunk'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02309.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#training', '#inference', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 95,6% ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Sequential Scaling: The Future of Language Model Reasoning', 'desc': 'This paper investigates the effectiveness of sequential scaling versus parallel scaling in language model reasoning. It finds that running fewer chains that build on each other through sequential steps leads to better accuracy than running multiple independent chains at the same time. The authors introduce a new method called inverse-entropy weighted voting, which further improves the accuracy of sequential scaling by weighing answers based on the reliability of their reasoning. Overall, the study suggests a significant shift in how we optimize inference time for language models, favoring sequential refinement over traditional parallel approaches.'}, 'zh': {'title': 'é¡ºåºæ‰©å±•è¶…è¶Šå¹¶è¡Œæ‰©å±•ï¼Œæå‡è¯­è¨€æ¨¡å‹æ¨ç†å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„æµ‹è¯•æ—¶é—´æ‰©å±•é—®é¢˜ï¼Œæ¯”è¾ƒäº†å¹¶è¡Œæ‰©å±•å’Œé¡ºåºæ‰©å±•çš„æ•ˆæœã€‚åœ¨ç›¸åŒçš„ä»¤ç‰Œé¢„ç®—å’Œè®¡ç®—èµ„æºä¸‹ï¼Œé¡ºåºæ‰©å±•é€šè¿‡é€æ­¥æ”¹è¿›çš„æ–¹å¼ï¼Œæ˜¾è‘—ä¼˜äºå¹¶è¡Œè‡ªä¸€è‡´æ€§æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¡ºåºæ‰©å±•åœ¨95.6%çš„é…ç½®ä¸­è¡¨ç°æ›´ä½³ï¼Œå‡†ç¡®ç‡æé«˜äº†æœ€é«˜46.7%ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ— å…³æ–¹æ³•â€”â€”é€†ç†µåŠ æƒæŠ•ç¥¨ï¼Œè¿›ä¸€æ­¥æå‡äº†é¡ºåºæ‰©å±•çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03718', 'title': 'Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist\n  Annotation Scheme for MapTask', 'url': 'https://huggingface.co/papers/2511.03718', 'abstract': "A perspectivist annotation scheme for the HCRC MapTask corpus reveals how understanding emerges, diverges, and repairs in collaborative dialogue, highlighting the role of multiplicity discrepancies in referential misalignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '6ce52f670b595fdb', 'authors': ['Nan Li', 'Albert Gatt', 'Massimo Poesio'], 'affiliations': ['Utrecht University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03718.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#data', '#interpretability'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ¸ÑÑ‚ÑĞºĞ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² HCRC MapTask, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ ÑĞ»ÑƒÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑÑ‹Ğ»ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ LLM Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ…ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 13 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ´ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ², Ğ½Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞµ Ğ¾Ñ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ.'}, 'en': {'title': 'Understanding Misalignment in Collaborative Dialogue', 'desc': 'This paper presents a new annotation scheme for analyzing collaborative dialogue in the HCRC MapTask corpus. It focuses on how participants build mutual understanding, even when they may refer to different things. The authors introduce a method to track how understanding develops, diverges, and is repaired during conversations. Their findings highlight that while misunderstandings are uncommon, discrepancies in perspective can lead to significant misalignments in reference, providing insights into how language models can better handle these situations.'}, 'zh': {'title': 'æ­ç¤ºåä½œå¯¹è¯ä¸­çš„ç†è§£ä¸è¯¯è§£', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨é‡Šæ–¹æ¡ˆï¼Œç”¨äºåˆ†æHCRC MapTaskè¯­æ–™åº“ä¸­çš„åä½œå¯¹è¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¸å¯¹ç§°çš„å¯¹è¯ä¸­ï¼Œå‚ä¸è€…å¯èƒ½ä¼šè¯¯ä»¥ä¸ºå½¼æ­¤è¾¾æˆä¸€è‡´ï¼Œå®é™…ä¸Šå´åœ¨æŒ‡ä»£ä¸åŒçš„å®ä½“ã€‚é€šè¿‡å¯¹æ¯ä¸ªå‚è€ƒè¡¨è¾¾çš„å‘è¨€è€…å’Œå¬è€…çš„ç†è§£è¿›è¡Œå•ç‹¬æ•æ‰ï¼Œç ”ç©¶æ­ç¤ºäº†ç†è§£æ˜¯å¦‚ä½•é€æ¸å½¢æˆã€åˆ†æ­§å’Œä¿®å¤çš„ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è¯æ±‡å˜ä½“ç»Ÿä¸€åå®Œå…¨è¯¯è§£çš„æƒ…å†µå¾ˆå°‘ï¼Œä½†å¤šæ ·æ€§å·®å¼‚ä¼šç³»ç»Ÿæ€§åœ°å¼•å‘åˆ†æ­§ï¼Œè¡¨æ˜è¡¨é¢ä¸Šçš„å…±è¯†å¯èƒ½æ©ç›–äº†æŒ‡ä»£çš„ä¸ä¸€è‡´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02358', 'title': 'Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation', 'url': 'https://huggingface.co/papers/2511.02358', 'abstract': 'M-Solomon, a multimodal embedder, adaptively augments queries using a Multimodal LLM, improving performance and reducing embedding latency compared to baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '8acc02d495d5f5bc', 'authors': ['Wongyu Kim', 'Hochang Lee', 'Sanghak Lee', 'Yoonsung Kim', 'Jaehyun Park'], 'affiliations': ['NC AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02358.jpg', 'data': {'categories': ['#rag', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: Ğ±ĞµÑ€Ñ‘Ğ¼ augmentation Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾', 'desc': 'M-Solomon â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ (embedder), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ»Ğ¸Ñ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹: Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ, Ğ¸ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ¾Ğ½Ğ¾ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ»Ğ¸Ğ±Ğ¾ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ /augment Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ»Ğ¸Ğ±Ğ¾ /embed Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ M-Solomon Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Adaptive Query Augmentation for Enhanced Embedding Efficiency', 'desc': 'M-Solomon is a novel multimodal embedder that enhances query performance by adaptively deciding when to augment queries using a Multimodal Large Language Model (MLLM). It categorizes queries into two groups: those that benefit from augmentation and those that do not, optimizing the embedding process. By generating synthetic augmentations only for the necessary queries, M-Solomon significantly reduces embedding latency while improving overall performance. Experimental results demonstrate that M-Solomon outperforms both traditional methods without augmentation and those that always apply augmentation, showcasing its efficiency and effectiveness in multimodal environments.'}, 'zh': {'title': 'è‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºï¼Œæå‡å¤šæ¨¡æ€åµŒå…¥æ•ˆç‡', 'desc': 'M-Solomonæ˜¯ä¸€ç§å¤šæ¨¡æ€åµŒå…¥å™¨ï¼Œé€šè¿‡è‡ªé€‚åº”å¢å¼ºæŸ¥è¯¢ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æé«˜æ€§èƒ½å¹¶å‡å°‘åµŒå…¥å»¶è¿Ÿã€‚è¯¥æ–¹æ³•é¦–å…ˆå°†è®­ç»ƒæ•°æ®é›†çš„æŸ¥è¯¢åˆ†ä¸ºä¸¤ç»„ï¼Œä¸€ç»„éœ€è¦å¢å¼ºï¼Œå¦ä¸€ç»„ä¸éœ€è¦ã€‚æ¥ç€ï¼ŒM-Solomoné€šè¿‡ç”Ÿæˆåˆé€‚çš„å¢å¼ºä¿¡æ¯æ¥å¤„ç†éœ€è¦å¢å¼ºçš„æŸ¥è¯¢ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œè‡ªé€‚åº”æŸ¥è¯¢å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-Solomonåœ¨åµŒå…¥å»¶è¿Ÿä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04583', 'title': 'Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration\n  from a Baseline Paper', 'url': 'https://huggingface.co/papers/2511.04583', 'abstract': 'Jr. AI Scientist, an autonomous AI system, mimics novice researcher workflows to generate scientifically valuable papers, outperforming fully automated systems but with identified limitations and risks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'ab749d992d943b95', 'authors': ['Atsuyuki Miyai', 'Mashiro Toyooka', 'Takashi Otonari', 'Zaiying Zhao', 'Kiyoharu Aizawa'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04583.jpg', 'data': {'categories': ['#benchmark', '#agents', '#science', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° AI ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒĞºÑƒ ĞºĞ°Ğº ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚: ÑƒÑĞ¿ĞµÑ…Ğ¸ Ğ¸ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Jr. AI Scientist â€” Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑƒÑ‡Ñ‘Ğ½Ğ¾Ğ³Ğ¾: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸, ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Agents4Science, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ĞµÑ‘ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ² AI Scientist Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ AI-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸.'}, 'en': {'title': "Empowering Novice Research: Jr. AI Scientist's Journey in AI-Driven Science", 'desc': 'The paper introduces Jr. AI Scientist, an autonomous AI system designed to replicate the research workflow of a novice researcher. It analyzes existing literature, formulates hypotheses, conducts experiments, and writes scientific papers, outperforming fully automated systems in generating valuable contributions. The study evaluates its performance through automated assessments and peer reviews, revealing that it achieves higher scores than previous models. However, the authors also highlight significant limitations and risks associated with its use, emphasizing the need for careful consideration in the application of AI in scientific research.'}, 'zh': {'title': 'æ¨¡ä»¿åˆå­¦è€…ï¼Œæ¨åŠ¨ç§‘å­¦è¿›æ­¥çš„AIç³»ç»Ÿ', 'desc': 'Jr. AI Scientist æ˜¯ä¸€ä¸ªè‡ªä¸»çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œæ¨¡ä»¿åˆå­¦è€…ç ”ç©¶è€…çš„å·¥ä½œæµç¨‹ï¼Œç”Ÿæˆå…·æœ‰ç§‘å­¦ä»·å€¼çš„è®ºæ–‡ã€‚å®ƒåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å®Œå…¨è‡ªåŠ¨åŒ–çš„ç³»ç»Ÿï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§å’Œé£é™©ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†æäººç±»å¯¼å¸ˆæä¾›çš„åŸºç¡€è®ºæ–‡ï¼Œæå‡ºæ”¹è¿›å‡è®¾ï¼Œå¹¶é€šè¿‡ä¸¥æ ¼å®éªŒéªŒè¯è¿™äº›å‡è®¾ï¼Œæœ€ç»ˆæ’°å†™å‡ºç ”ç©¶ç»“æœçš„è®ºæ–‡ã€‚å°½ç®¡ Jr. AI Scientist å–å¾—äº†è¾ƒé«˜çš„è¯„å®¡åˆ†æ•°ï¼Œä½†ä»éœ€å…³æ³¨å…¶åœ¨åº”ç”¨ä¸­çš„æ½œåœ¨é£é™©å’Œæœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04570', 'title': 'Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm', 'url': 'https://huggingface.co/papers/2511.04570', 'abstract': 'The "Thinking with Video" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t "Thinking with Text" and "Thinking with Images" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce "Thinking with Video", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2\'s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions "thinking with video" as a unified multimodal reasoning paradigm.', 'score': 208, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'da0cf16fafb0f708', 'authors': ['Jingqi Tong', 'Yurong Mou', 'Hangcheng Li', 'Mingzhe Li', 'Yongzhuo Yang', 'Ming Zhang', 'Qiguang Chen', 'Tianyi Liang', 'Xiaomeng Hu', 'Yining Zheng', 'Xinchi Chen', 'Jun Zhao', 'Xuanjing Huang', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Harbin Institute of Technology', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04570.jpg', 'data': {'categories': ['#benchmark', '#games', '#video', '#multimodal', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Â«ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Â», ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VideoThinkBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Sora-2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Vision Language Models, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… (92% Ğ½Ğ° MATH Ğ¸ 75.53% Ğ½Ğ° MMMU). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Reasoning with Video Integration', 'desc': 'The paper introduces the "Thinking with Video" paradigm, which enhances multimodal reasoning by integrating video generation models into the reasoning process. This approach addresses the limitations of previous paradigms that relied solely on text and images, which could not effectively represent dynamic changes. The authors developed the Video Thinking Benchmark (VideoThinkBench) to evaluate the performance of their model, Sora-2, on both vision-centric and text-centric tasks. Results show that Sora-2 performs comparably to state-of-the-art vision language models and achieves high accuracy on various reasoning tasks, highlighting its potential for unified multimodal understanding.'}, 'zh': {'title': 'è§†é¢‘æ€ç»´ï¼šç»Ÿä¸€å¤šæ¨¡æ€æ¨ç†çš„æ–°èŒƒå¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†â€œè§†é¢‘æ€ç»´â€è¿™ä¸€æ–°èŒƒå¼ï¼Œé€šè¿‡æ•´åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„â€œæ–‡æœ¬æ€ç»´â€å’Œâ€œå›¾åƒæ€ç»´â€ç›¸æ¯”ï¼Œè§†é¢‘èƒ½å¤Ÿæ›´å¥½åœ°è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹å’Œè¿ç»­å˜åŒ–ï¼Œä»è€Œå…‹æœäº†å›¾åƒå’Œæ–‡æœ¬åˆ†ç¦»çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†è§†é¢‘æ€ç»´åŸºå‡†ï¼ˆVideoThinkBenchï¼‰ï¼ŒåŒ…æ‹¬è§†è§‰ä¸­å¿ƒå’Œæ–‡æœ¬ä¸­å¿ƒçš„ä»»åŠ¡ï¼Œä»¥è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹Sora-2çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSora-2åœ¨è§†è§‰ä»»åŠ¡ä¸Šä¸æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†å®ƒä»¬ï¼Œå±•ç¤ºäº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04460', 'title': 'V-Thinker: Interactive Thinking with Images', 'url': 'https://huggingface.co/papers/2511.04460', 'abstract': 'V-Thinker, a multimodal reasoning assistant using reinforcement learning, enhances image-interactive thinking by synthesizing datasets and aligning perception for improved performance in vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising "Thinking with Images" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.', 'score': 96, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '40e8658b7a62ece7', 'authors': ['Runqi Qiao', 'Qiuna Tan', 'Minghan Yang', 'Guanting Dong', 'Peiqing Yang', 'Shiqiang Lang', 'Enhui Wan', 'Xiaowan Wang', 'Yida Xu', 'Lan Yang', 'Chong Sun', 'Chen Li', 'Honggang Zhang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'WeChat Vision, Tencent Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04460.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#training', '#dataset', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'V-Thinker â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑƒÑ‡ĞµĞ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‰ÑƒÑÑÑ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ VTBench â€” ÑĞºÑĞ¿ĞµÑ€Ñ‚-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ V-Thinker Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Interactive Thinking with V-Thinker', 'desc': "V-Thinker is a multimodal reasoning assistant that uses reinforcement learning to improve how models think interactively with images. It addresses the challenge of integrating image interaction with long-term reasoning by introducing a new paradigm called 'Thinking with Images'. The system features a Data Evolution Flywheel that creates and refines datasets for better reasoning, and a Visual Progressive Training Curriculum that enhances perception and reasoning capabilities. Experimental results show that V-Thinker outperforms existing models in tasks that require vision-centric interactive reasoning."}, 'zh': {'title': 'V-Thinkerï¼šæå‡å›¾åƒäº¤äº’æ€ç»´çš„å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹', 'desc': 'V-Thinkeræ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å›¾åƒäº¤äº’æ€ç»´èƒ½åŠ›ã€‚å®ƒé€šè¿‡åˆæˆæ•°æ®é›†å’Œå¯¹é½æ„ŸçŸ¥ï¼Œæ”¹å–„è§†è§‰ä¸­å¿ƒä»»åŠ¡çš„è¡¨ç°ã€‚V-ThinkeråŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šæ•°æ®æ¼”åŒ–é£è½®å’Œè§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ï¼Œå‰è€…è‡ªåŠ¨ç”Ÿæˆå’ŒéªŒè¯æ¨ç†æ•°æ®é›†ï¼Œåè€…é€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ•´åˆäº¤äº’æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV-Thinkeråœ¨ä¸€èˆ¬å’Œäº¤äº’æ¨ç†åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03773', 'title': 'Scaling Agent Learning via Experience Synthesis', 'url': 'https://huggingface.co/papers/2511.03773', 'abstract': 'DreamGym is a unified framework that synthesizes diverse experiences for scalable online RL training, improving agent performance and reducing real-world interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL.', 'score': 80, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'bbedd76432556a37', 'authors': ['Zhaorun Chen', 'Zhuokai Zhao', 'Kai Zhang', 'Bo Liu', 'Qi Qi', 'Yifan Wu', 'Tarun Kalluri', 'Sara Cao', 'Yuanhao Xiong', 'Haibo Tong', 'Huaxiu Yao', 'Hengduo Li', 'Jiacheng Zhu', 'Xian Li', 'Dawn Song', 'Bo Li', 'Jason Weston', 'Dat Huynh'], 'affiliations': ['FAIR at Meta', 'Meta Superintelligence Labs', 'UC Berkeley', 'UNC', 'University of Chicago'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03773.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#training', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ‹ï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ·Ğ°Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'DreamGym â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸Ğ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ.'}, 'en': {'title': 'DreamGym: Scalable Experience Synthesis for Enhanced RL Training', 'desc': 'DreamGym is a novel framework that enhances online reinforcement learning (RL) by synthesizing diverse experiences, which helps improve agent performance while minimizing the need for real-world interactions. It addresses common challenges in RL, such as high costs and limited task diversity, by creating a reasoning-based experience model that simulates environment dynamics. This model allows for consistent state transitions and feedback, facilitating scalable agent training through an experience replay buffer that combines offline and online data. Additionally, DreamGym generates new tasks to promote adaptive learning, leading to significant performance improvements in both synthetic and real-world scenarios.'}, 'zh': {'title': 'DreamGymï¼šå¯æ‰©å±•çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'DreamGymæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åˆæˆå¤šæ ·åŒ–çš„ç»éªŒï¼Œä»¥å®ç°å¯æ‰©å±•çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½å¹¶å‡å°‘ä¸çœŸå®ä¸–ç•Œçš„äº¤äº’ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨ç†åŸºç¡€çš„ç»éªŒæ¨¡å‹ï¼Œæç‚¼ç¯å¢ƒåŠ¨æ€ï¼Œç”Ÿæˆä¸€è‡´çš„çŠ¶æ€è½¬ç§»å’Œåé¦ˆä¿¡å·ï¼Œé¿å…äº†æ˜‚è´µçš„çœŸå®ç¯å¢ƒå›åˆã€‚DreamGymè¿˜åˆ©ç”¨ç»éªŒé‡æ”¾ç¼“å†²åŒºï¼Œç»“åˆç¦»çº¿çœŸå®æ•°æ®å’Œæ–°äº¤äº’ï¼Œæå‡è¿‡æ¸¡çš„ç¨³å®šæ€§å’Œè´¨é‡ã€‚é€šè¿‡è‡ªé€‚åº”ç”Ÿæˆæ–°ä»»åŠ¡ï¼ŒDreamGymæœ‰æ•ˆæ”¯æŒåœ¨çº¿è¯¾ç¨‹å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†RLè®­ç»ƒçš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04670', 'title': 'Cambrian-S: Towards Spatial Supersensing in Video', 'url': 'https://huggingface.co/papers/2511.04670', 'abstract': 'Progress in multimodal intelligence requires a shift to supersensing, including semantic perception, event cognition, spatial cognition, and predictive modeling, demonstrated through VSI-SUPER benchmarks and a self-supervised predictive sensing approach.  \t\t\t\t\tAI-generated summary \t\t\t\t We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.', 'score': 36, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '1562e300c0eb5c05', 'authors': ['Shusheng Yang', 'Jihan Yang', 'Pinzhi Huang', 'Ellis Brown', 'Zihao Yang', 'Yue Yu', 'Shengbang Tong', 'Zihan Zheng', 'Yifan Xu', 'Muhan Wang', 'Daohan Lu', 'Rob Fergus', 'Yann LeCun', 'Li Fei-Fei', 'Saining Xie'], 'affiliations': ['New York University', 'Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04670.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#long_context', '#multimodal', '#training', '#dataset', '#synthetic'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ñƒ: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ°Ñ 'ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VSI-SUPER Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¿ĞµÑ€ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ 590K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cambrian-S Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 30%, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ½ÑĞ¸Ğ½Ğ³Ğ° Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VSI-SUPER."}, 'en': {'title': 'Advancing Multimodal Intelligence through Supersensing', 'desc': 'This paper discusses the need for advancements in multimodal intelligence through a concept called supersensing, which includes understanding semantics, events, spatial awareness, and predictive modeling. It introduces the VSI-SUPER benchmarks to evaluate these capabilities, emphasizing that current tests focus too much on basic understanding rather than true world modeling. The authors present a new approach called predictive sensing, which uses self-supervised learning to improve memory and event segmentation by predicting future frames based on past experiences. Their findings show that simply increasing data size is not enough; models must also be able to anticipate and organize information effectively to achieve true spatial supersensing.'}, 'zh': {'title': 'è¶…æ„ŸçŸ¥ï¼šå¤šæ¨¡æ€æ™ºèƒ½çš„æ–°æ–¹å‘', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ™ºèƒ½çš„è¿›å±•ï¼Œæå‡ºéœ€è¦è½¬å‘è¶…æ„ŸçŸ¥çš„æ¦‚å¿µï¼ŒåŒ…æ‹¬è¯­ä¹‰æ„ŸçŸ¥ã€äº‹ä»¶è®¤çŸ¥ã€ç©ºé—´è®¤çŸ¥å’Œé¢„æµ‹å»ºæ¨¡ã€‚ä½œè€…ä»‹ç»äº†VSI-SUPERåŸºå‡†æµ‹è¯•ï¼Œå¼ºè°ƒäº†å½“å‰åŸºå‡†ä¸»è¦æµ‹è¯•æ—©æœŸé˜¶æ®µï¼Œç¼ºä¹å¯¹ç©ºé—´è®¤çŸ¥çš„å…¨é¢æŒ‘æˆ˜ã€‚é€šè¿‡è‡ªç›‘ç£çš„é¢„æµ‹æ„ŸçŸ¥æ–¹æ³•ï¼Œè®ºæ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨é¢„æµ‹è¯¯å·®æ¥æ¨åŠ¨è®°å¿†å’Œäº‹ä»¶åˆ†å‰²ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œç©ºé—´è¶…æ„ŸçŸ¥ä¸ä»…éœ€è¦æ¨¡å‹èƒ½å¤Ÿè§‚å¯Ÿï¼Œè¿˜éœ€è¦å…¶å…·å¤‡é¢„æµ‹ã€é€‰æ‹©å’Œç»„ç»‡ç»éªŒçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03929', 'title': 'NVIDIA Nemotron Nano V2 VL', 'url': 'https://huggingface.co/papers/2511.03929', 'abstract': 'Nemotron Nano V2 VL, a hybrid Mamba-Transformer LLM, improves document and video understanding through enhanced architecture and token reduction techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.', 'score': 26, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'fba6c98589033970', 'authors': ['NVIDIA', ':', 'Amala Sanjay Deshmukh', 'Kateryna Chumachenko', 'Tuomas Rintamaki', 'Matthieu Le', 'Tyler Poon', 'Danial Mohseni Taheri', 'Ilia Karmanov', 'Guilin Liu', 'Jarno Seppanen', 'Guo Chen', 'Karan Sapra', 'Zhiding Yu', 'Adi Renduchintala', 'Charles Wang', 'Peter Jin', 'Arushi Goel', 'Mike Ranzinger', 'Lukas Voegtle', 'Philipp Fischer', 'Timo Roman', 'Wei Ping', 'Boxin Wang', 'Zhuolin Yang', 'Nayeon Lee', 'Shaokun Zhang', 'Fuxiao Liu', 'Zhiqi Li', 'Di Zhang', 'Greg Heinrich', 'Hongxu Yin', 'Song Han', 'Pavlo Molchanov', 'Parth Mannan', 'Yao Xu', 'Jane Polak Scowcroft', 'Tom Balough', 'Subhashree Radhakrishnan', 'Paris Zhang', 'Sean Cha', 'Ratnesh Kumar', 'Zaid Pervaiz Bhat', 'Jian Zhang', 'Darragh Hanley', 'Pritam Biswas', 'Jesse Oliver', 'Kevin Vasques', 'Roger Waleffe', 'Duncan Riach', 'Oluwatobi Olabiyi', 'Ameya Sunil Mahabaleshwarkar', 'Bilal Kartal', 'Pritam Gundecha', 'Khanh Nguyen', 'Alexandre Milesi', 'Eugene Khvedchenia', 'Ran Zilberstein', 'Ofri Masad', 'Natan Bagrov', 'Nave Assaf', 'Tomer Asida', 'Daniel Afrimi', 'Amit Zuker', 'Netanel Haber', 'Zhiyu Cheng', 'Jingyu Xin', 'Di Wu', 'Nik Spirin', 'Maryam Moosaei', 'Roman Ageev', 'Vanshil Atul Shah', 'Yuting Wu', 'Daniel Korzekwa', 'Unnikrishnan Kizhakkemadam Sreekumar', 'Wanli Jiang', 'Padmavathy Subramanian', 'Alejandra Rico', 'Sandip Bhaskar', 'Saeid Motiian', 'Kedi Wu', 'Annie Surla', 'Chia-Chih Chen', 'Hayden Wolff', 'Matthew Feinberg', 'Melissa Corpuz', 'Marek Wawrzos', 'Eileen Long', 'Aastha Jhunjhunwala', 'Paul Hendricks', 'Farzan Memarian', 'Benika Hall', 'Xin-Yu Wang', 'David Mosallanezhad', 'Soumye Singhal', 'Luis Vega', 'Katherine Cheung', 'Krzysztof Pawelec', 'Michael Evans', 'Katherine Luna', 'Jie Lou', 'Erick Galinkin', 'Akshay Hazare', 'Kaustubh Purandare', 'Ann Guan', 'Anna Warno', 'Chen Cui', 'Yoshi Suhara', 'Shibani Likhite', 'Seph Mard', 'Meredith Price', 'Laya Sleiman', 'Saori Kaji', 'Udi Karpas', 'Kari Briski', 'Joey Conway', 'Michael Lightstone', 'Jan Kautz', 'Mohammad Shoeybi', 'Mostofa Patwary', 'Jonathen Cohen', 'Oleksii Kuchaiev', 'Andrew Tao', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03929.jpg', 'data': {'categories': ['#small_models', '#inference', '#open_source', '#architecture', '#long_context', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Nemotron Nano V2 VL â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ¸ Transformer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ—Ğ° ÑÑ‡ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸, Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Document and Video Comprehension with Nemotron Nano V2 VL', 'desc': 'The Nemotron Nano V2 VL is a new hybrid Mamba-Transformer large language model (LLM) that enhances the understanding of documents and videos. It outperforms its predecessor, Llama-3.1-Nemotron-Nano-VL-8B, by improving the model architecture and utilizing advanced datasets and training methods. The model incorporates innovative token reduction techniques, allowing for faster processing of long documents and videos. Additionally, the release includes model checkpoints in various formats and access to datasets and training code for further research.'}, 'zh': {'title': 'æå‡æ–‡æ¡£ä¸è§†é¢‘ç†è§£çš„æ··åˆæ¨¡å‹', 'desc': 'Nemotron Nano V2 VL æ˜¯ä¸€ç§æ··åˆçš„ Mamba-Transformer å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡æ–‡æ¡£å’Œè§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡æ”¹è¿›çš„æ¨¡å‹æ¶æ„å’Œä»¤ç‰Œå‡å°‘æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æ¡£å’Œè§†é¢‘æ—¶å®ç°äº†æ›´é«˜çš„æ¨ç†æ•ˆç‡ã€‚ä¸ä¹‹å‰çš„ Llama-3.1-Nemotron-Nano-VL-8B æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron Nano V2 VL åœ¨è§†è§‰å’Œæ–‡æœ¬é¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æˆ‘ä»¬è¿˜å°†å‘å¸ƒ BF16ã€FP8 å’Œ FP4 æ ¼å¼çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶åˆ†äº«å¤§é‡æ•°æ®é›†ã€è®­ç»ƒé…æ–¹å’Œä»£ç ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04217', 'title': 'The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms', 'url': 'https://huggingface.co/papers/2511.04217', 'abstract': 'Theoretical analysis proves the existence of strong lottery tickets within multi-head attention mechanisms and extends the strong lottery ticket hypothesis to transformers without normalization layers.  \t\t\t\t\tAI-generated summary \t\t\t\t The strong lottery ticket hypothesis (SLTH) conjectures that high-performing subnetworks, called strong lottery tickets (SLTs), are hidden in randomly initialized neural networks. Although recent theoretical studies have established the SLTH across various neural architectures, the SLTH for transformer architectures still lacks theoretical understanding. In particular, the current theory of the SLTH does not yet account for the multi-head attention (MHA) mechanism, a core component of transformers. To address this gap, we introduce a theoretical analysis of the existence of SLTs within MHAs. We prove that, if a randomly initialized MHA of H heads and input dimension d has the hidden dimension O(dlog(Hd^{3/2})) for the key and value, it contains an SLT that approximates an arbitrary MHA with the same input dimension with high probability. Furthermore, by leveraging this theory for MHAs, we extend the SLTH to transformers without normalization layers. We empirically validate our theoretical findings, demonstrating that the approximation error between the SLT within a source model (MHA and transformer) and an approximate target counterpart decreases exponentially by increasing the hidden dimension of the source model.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '6c0c89f3ba0b4082', 'authors': ['Hikari Otsuka', 'Daiki Chijiwa', 'Yasuyuki Okoshi', 'Daichi Fujiki', 'Susumu Takeuchi', 'Masato Motomura'], 'affiliations': ['Institute of Science Tokyo', 'NTT, Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04217.jpg', 'data': {'categories': ['#optimization', '#architecture', '#math'], 'emoji': 'ğŸŸï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğµ Ğ±Ğ¸Ğ»ĞµÑ‚Ñ‹ ÑĞºÑ€Ñ‹Ñ‚Ñ‹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ»ĞµÑ‚Ğ¾Ğ² (subnetworks Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸) Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (multi-head attention), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ MHA Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ»ĞµÑ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ MHA Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ»Ğ¾Ñ‚ĞµÑ€ĞµĞ¹Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ»ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ.'}, 'en': {'title': 'Unlocking Strong Lottery Tickets in Transformers', 'desc': 'This paper explores the strong lottery ticket hypothesis (SLTH) in the context of multi-head attention (MHA) mechanisms used in transformers. It demonstrates that within a randomly initialized MHA, there exist high-performing subnetworks, known as strong lottery tickets, that can effectively approximate the performance of the full model. The authors provide a theoretical framework showing that if certain conditions on the hidden dimensions are met, these SLTs can be found with high probability. Additionally, they extend the SLTH to transformers that do not utilize normalization layers, supporting their claims with empirical evidence of reduced approximation error as the hidden dimensions increase.'}, 'zh': {'title': 'å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¼ºå½©ç¥¨ç¥¨æ®å­˜åœ¨æ€§åˆ†æ', 'desc': 'æœ¬æ–‡æå‡ºäº†å¼ºå½©ç¥¨å‡è®¾åœ¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å­˜åœ¨æ€§åˆ†æï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°æ²¡æœ‰å½’ä¸€åŒ–å±‚çš„å˜æ¢å™¨æ¶æ„ã€‚å¼ºå½©ç¥¨å‡è®¾è®¤ä¸ºï¼Œåœ¨éšæœºåˆå§‹åŒ–çš„ç¥ç»ç½‘ç»œä¸­ï¼Œå­˜åœ¨é«˜æ€§èƒ½çš„å­ç½‘ç»œï¼Œç§°ä¸ºå¼ºå½©ç¥¨ç¥¨æ®ã€‚æˆ‘ä»¬è¯æ˜äº†ï¼Œå¦‚æœä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å…·æœ‰ç‰¹å®šçš„éšè—ç»´åº¦ï¼Œå®ƒå°±åŒ…å«ä¸€ä¸ªå¯ä»¥é«˜æ¦‚ç‡è¿‘ä¼¼ä»»æ„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå½©ç¥¨ç¥¨æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¯æ˜äº†æºæ¨¡å‹ä¸­çš„å¼ºå½©ç¥¨ç¥¨æ®ä¸ç›®æ ‡æ¨¡å‹ä¹‹é—´çš„è¿‘ä¼¼è¯¯å·®éšç€éšè—ç»´åº¦çš„å¢åŠ è€ŒæŒ‡æ•°ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04307', 'title': 'GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents', 'url': 'https://huggingface.co/papers/2511.04307', 'abstract': 'GUI-360Â° is a large-scale dataset and benchmark suite for computer-using agents, addressing gaps in real-world tasks, automated data collection, and unified evaluation of GUI grounding, screen parsing, and action prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce GUI-360^circ, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction.   GUI-360^circ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360^circ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360^circ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs.   The full dataset has been made public on https://huggingface.co/datasets/vyokky/GUI-360.', 'score': 14, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'aa241d25d4b42cb3', 'authors': ['Jian Mu', 'Chaoyun Zhang', 'Chiming Ni', 'Lu Wang', 'Bo Qiao', 'Kartik Mathur', 'Qianhui Wu', 'Yuhang Xie', 'Xiaojun Ma', 'Mengyu Zhou', 'Si Qin', 'Liqun Li', 'Yu Kang', 'Minghua Ma', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Microsoft', 'Nanjing University', 'Peking University', 'ZJU-UIUC'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04307.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼ ÑÑ‚Ğ¾Ğ»Ğ¾Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GUI-360Â°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ 1.2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹, Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Windows. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞºÑ€Ğ°Ğ½Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ‚Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Empowering Computer-Using Agents with GUI-360Â° Dataset', 'desc': 'The paper introduces GUI-360Â°, a large-scale dataset aimed at improving computer-using agents (CUAs) by addressing key challenges in real-world applications. It provides a comprehensive benchmark that evaluates GUI grounding, screen parsing, and action prediction, which are essential for CUAs to interact effectively with graphical user interfaces. The dataset includes over 1.2 million action steps from various Windows applications, along with detailed metadata and reasoning traces, enabling researchers to train and test their models. Initial evaluations of state-of-the-art models on this dataset show significant performance gaps, highlighting the need for further advancements in grounding and action prediction to achieve human-level performance.'}, 'zh': {'title': 'æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„é©å‘½æ€§æ•°æ®é›†', 'desc': 'GUI-360Â°æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰çš„å‘å±•ã€‚è¯¥æ•°æ®é›†è§£å†³äº†CUAé¢ä¸´çš„ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šç¼ºä¹çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ã€ç¼ºå°‘è‡ªåŠ¨åŒ–çš„æ•°æ®æ”¶é›†å’Œæ³¨é‡Šæµç¨‹ï¼Œä»¥åŠç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†ã€‚GUI-360Â°æä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºæŸ¥è¯¢æ¥æºã€ç¯å¢ƒæ¨¡æ¿æ„å»ºã€ä»»åŠ¡å®ä¾‹åŒ–å’Œè´¨é‡è¿‡æ»¤ï¼ŒåŒ…å«è¶…è¿‡120ä¸‡æ¡æ‰§è¡Œçš„åŠ¨ä½œæ­¥éª¤ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„è§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°å…¶åœ¨GUIå®šä½å’ŒåŠ¨ä½œé¢„æµ‹æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå°½ç®¡ç»è¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åæœ‰æ‰€æ”¹å–„ï¼Œä½†ä»æœªè¾¾åˆ°äººç±»æ°´å¹³çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03774', 'title': 'Contamination Detection for VLMs using Multi-Modal Semantic Perturbation', 'url': 'https://huggingface.co/papers/2511.03774', 'abstract': 'A novel detection method based on multi-modal semantic perturbation is proposed to identify contaminated Vision-Language Models, demonstrating robustness across various contamination strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'fb9226e5719b12da', 'authors': ['Jaden Park', 'Mu Cai', 'Feng Yao', 'Jingbo Shang', 'Soochahn Lee', 'Yong Jae Lee'], 'affiliations': ['Kookmin University', 'University of California, San Diego', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03774.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#leakage', '#dataset', '#security'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑ‚ĞµÑ‡ĞµĞº Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Vision-Language (VLM) Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ñ… ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ĞºĞ¾Ğ½Ñ‚Ğ°Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Detecting Contamination in Vision-Language Models with Robust Perturbation Techniques', 'desc': 'This paper introduces a new method for detecting contaminated Vision-Language Models (VLMs) using multi-modal semantic perturbation. The authors highlight the issue of inflated performance in VLMs due to test-set leakage from pretraining data. They demonstrate that existing detection methods are inadequate and often inconsistent when faced with contaminated models. The proposed method shows that these contaminated models struggle to generalize when subjected to specific perturbations, proving its robustness across various contamination strategies.'}, 'zh': {'title': 'æ–°æ–¹æ³•è¯†åˆ«æ±¡æŸ“çš„è§†è§‰-è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€è¯­ä¹‰æ‰°åŠ¨çš„æ–°å‹æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«è¢«æ±¡æŸ“çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•åœ¨é¢å¯¹ä¸åŒçš„æ±¡æŸ“ç­–ç•¥æ—¶ï¼Œå¾€å¾€æ— æ³•æœ‰æ•ˆè¯†åˆ«æˆ–è¡¨ç°ä¸ä¸€è‡´ã€‚é€šè¿‡æ•…æ„æ±¡æŸ“å¼€æºVLMså¹¶è¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ§åˆ¶æ‰°åŠ¨ä¸‹çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å°†åœ¨å¤šä¸ªçœŸå®æ±¡æŸ“ç­–ç•¥ä¸‹éªŒè¯è¯¥æ–¹æ³•ï¼Œå¹¶è®¡åˆ’å…¬å¼€å‘å¸ƒä»£ç å’Œæ‰°åŠ¨æ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04655', 'title': 'Benchmark Designers Should "Train on the Test Set" to Expose Exploitable\n  Non-Visual Shortcuts', 'url': 'https://huggingface.co/papers/2511.04655', 'abstract': "A framework for diagnosing and debiasing multimodal benchmarks reveals and mitigates non-visual biases, improving the robustness of Multimodal Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via k-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score s(x). We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.", 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '2e166587761671a4', 'authors': ['Ellis Brown', 'Jihan Yang', 'Shusheng Yang', 'Rob Fergus', 'Saining Xie'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04655.jpg', 'data': {'categories': ['#benchmark', '#ethics', '#multimodal', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¾Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Multimodal Large Language Models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Fine-tuning ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ±Ğ¸Ğ°ÑĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğº Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Diagnosing and Debiasing for Robust Multimodal Models', 'desc': "This paper presents a framework aimed at diagnosing and reducing non-visual biases in multimodal benchmarks for Multimodal Large Language Models (MLLMs). The authors highlight that many models can perform well on these benchmarks without truly understanding visual content, instead relying on biases and superficial patterns. To address this, they propose a diagnostic approach that includes a 'Test-set Stress-Test' methodology to identify exploitable patterns and a 'Iterative Bias Pruning' procedure to filter out high-bias samples. Their findings reveal significant non-visual biases across multiple benchmarks, leading to the creation of a debiased version that shows improved robustness in evaluating visual understanding."}, 'zh': {'title': 'æ­ç¤ºä¸å»åè§ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºè¯Šæ–­å’Œå»åè§å¤šæ¨¡æ€åŸºå‡†ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨è®¸å¤šå¤šæ¨¡æ€åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå®é™…ä¸Šæ˜¯åˆ©ç”¨äº†éè§†è§‰åè§å’Œè¡¨é¢æ¨¡å¼ï¼Œè€Œä¸æ˜¯å¼ºå¤§çš„è§†è§‰ç†è§£ã€‚ä¸ºäº†è®¾è®¡æœ‰æ•ˆçš„åŸºå‡†ï¼Œä½œè€…å»ºè®®è®¾è®¡è€…é¦–å…ˆå°è¯•â€œæ¸¸æˆâ€è‡ªå·±çš„åŸºå‡†ï¼Œé€šè¿‡è¯Šæ–­å’Œå»åè§ç¨‹åºç³»ç»Ÿåœ°è¯†åˆ«å’Œå‡è½»éè§†è§‰åè§ã€‚é€šè¿‡å¯¹å››ä¸ªåŸºå‡†çš„åº”ç”¨ï¼Œç ”ç©¶æ­ç¤ºäº†æ™®éå­˜åœ¨çš„éè§†è§‰åè§ï¼Œå¹¶å±•ç¤ºäº†å»åè§åçš„åŸºå‡†åœ¨è§†è§‰ç›²æ€§èƒ½å·®è·ä¸Šçš„æ”¹å–„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.27656', 'title': 'RDMA Point-to-Point Communication for LLM Systems', 'url': 'https://huggingface.co/papers/2510.27656', 'abstract': 'TransferEngine provides a uniform interface for flexible point-to-point communication in large language models, supporting disaggregated inference, reinforcement learning, and Mixture-of-Experts routing across different hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'fd30ddff99639d4d', 'authors': ['Nandor Licker', 'Kevin Hu', 'Vladimir Zaytsev', 'Lequn Chen'], 'affiliations': ['Perplexity AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.27656.jpg', 'data': {'categories': ['#open_source', '#optimization', '#transfer_learning', '#architecture', '#training', '#inference', '#rl'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'ĞŸĞ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TransferEngine â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ°-Ğº-Ñ‚Ğ¾Ñ‡ĞºĞµ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ„Ñ‘Ñ€ĞµĞ½Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¼ĞµÑĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ². Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ 400 Ğ“Ğ±Ğ¸Ñ‚/Ñ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… production-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…: Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ KV-ĞºÑÑˆĞ°, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ² RL Ğ´Ğ»Ñ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ MoE Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Seamless Communication for Advanced AI Models', 'desc': 'TransferEngine is a novel framework designed to enhance communication in large language models (LLMs) by providing a consistent interface for point-to-point communication. It supports advanced techniques like disaggregated inference, reinforcement learning, and Mixture-of-Experts (MoE) routing, which require more flexible communication than traditional methods. By bridging the functionality of various Network Interface Controllers (NICs), TransferEngine allows for seamless integration and portability across different hardware platforms. The framework achieves impressive performance, demonstrating peak throughput of 400 Gbps and significantly reducing latency in production systems for large-scale AI models.'}, 'zh': {'title': 'TransferEngineï¼šçµæ´»çš„ç‚¹å¯¹ç‚¹é€šä¿¡è§£å†³æ–¹æ¡ˆ', 'desc': 'TransferEngine æ˜¯ä¸€ä¸ªä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›ç»Ÿä¸€æ¥å£çš„å·¥å…·ï¼Œæ”¯æŒçµæ´»çš„ç‚¹å¯¹ç‚¹é€šä¿¡ã€‚å®ƒèƒ½å¤Ÿå¤„ç†åˆ†æ•£æ¨ç†ã€å¼ºåŒ–å­¦ä¹ å’Œä¸“å®¶æ··åˆè·¯ç”±ç­‰å¤æ‚ä»»åŠ¡ï¼Œè¶…è¶Šäº†ç®€å•çš„é›†ä½“é€šä¿¡ã€‚ç°æœ‰çš„å®ç°é€šå¸¸ä¾èµ–ç‰¹å®šçš„ç½‘ç»œæ¥å£æ§åˆ¶å™¨ï¼Œé™åˆ¶äº†ä¸æ¨ç†å¼•æ“çš„é›†æˆå’Œè·¨ç¡¬ä»¶çš„å¯ç§»æ¤æ€§ã€‚TransferEngine é€šè¿‡ç®¡ç†å¤šä¸ªç½‘ç»œæ¥å£ï¼Œæä¾›é«˜è¾¾ 400 Gbps çš„å³°å€¼ååé‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…ç³»ç»Ÿä¸­çš„åº”ç”¨æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04668', 'title': 'SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding', 'url': 'https://huggingface.co/papers/2511.04668', 'abstract': 'A data-generation framework using 3D simulators improves spatial reasoning in multimodal language models with efficient training on simulated data.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'b6084aefb5cf622d', 'authors': ['Ellis Brown', 'Arijit Ray', 'Ranjay Krishna', 'Ross Girshick', 'Rob Fergus', 'Saining Xie'], 'affiliations': ['AllenAI', 'Boston University', 'New York University', 'Vercept'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04668.jpg', 'data': {'categories': ['#benchmark', '#small_models', '#3d', '#video', '#transfer_learning', '#data', '#multimodal', '#training', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SIMS-V â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² multimodal language models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°ĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¸Ñ€, Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹), Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 25 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ 72-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Enhancing Spatial Reasoning with Simulated Data', 'desc': "This paper introduces SIMS-V, a data-generation framework that uses 3D simulators to create training data for multimodal language models, specifically targeting spatial reasoning. Traditional methods rely on real-world video data, which is often limited by the availability of diverse footage and accurate spatial annotations. The authors identify three key question categories that enhance the model's ability to transfer learned spatial reasoning skills to real-world scenarios. Their findings show that a smaller dataset of simulated examples can outperform larger models, demonstrating efficient training and strong generalization capabilities in spatial tasks."}, 'zh': {'title': 'åˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSIMS-Vçš„æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨ç”Ÿæˆä¸°å¯Œçš„ç©ºé—´è§†é¢‘è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚å½“å‰çš„ç©ºé—´è®­ç»ƒæ–¹æ³•ä¾èµ–äºçœŸå®è§†é¢‘æ•°æ®ï¼Œä½†è·å–å¤šæ ·åŒ–ä¸”ç²¾ç¡®æ ‡æ³¨çš„ç´ æå­˜åœ¨ç“¶é¢ˆã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°åˆ†æä¸åŒé—®é¢˜ç±»å‹å’Œç»„åˆï¼Œç ”ç©¶å‘ç°ä¸‰ç§é—®é¢˜ç±»åˆ«ï¼ˆåº¦é‡æµ‹é‡ã€è§†è§’ä¾èµ–æ¨ç†å’Œæ—¶é—´è·Ÿè¸ªï¼‰å¯¹æå‡å¯è½¬ç§»çš„ç©ºé—´æ™ºèƒ½æœ€ä¸ºæœ‰æ•ˆã€‚æœ€ç»ˆï¼Œç»è¿‡25Kä¸ªæ¨¡æ‹Ÿç¤ºä¾‹çš„å¾®è°ƒï¼Œæˆ‘ä»¬çš„7Bå‚æ•°è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äº72BåŸºçº¿æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00956', 'title': 'EVTAR: End-to-End Try on with Additional Unpaired Visual Reference', 'url': 'https://huggingface.co/papers/2511.00956', 'abstract': 'EVTAR is an end-to-end virtual try-on model that enhances accuracy by using reference images, simplifying the inference process and improving garment texture and detail preservation.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': '4c04c63e861c45a6', 'authors': ['Liuzhuozheng Li', 'Yue Gong', 'Shanyuan Liu', 'Bo Cheng', 'Yuhang Ma', 'Liebucha Wu', 'Dengyang Jiang', 'Zanyi Wang', 'Dawei Leng', 'Yuhui Yin'], 'affiliations': ['360 AI Research', 'Hong Kong University of Science and Technology', 'The University of Tokyo', 'University of California San Diego'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00956.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#cv'], 'emoji': 'ğŸ‘—', 'ru': {'title': 'ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹: ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºÑƒ', 'desc': 'EVTAR â€” ÑÑ‚Ğ¾ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ñ€ĞµĞ±ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ· Ğ¼Ğ°ÑĞ¾Ğº, densepose Ğ¸ ĞºĞ°Ñ€Ñ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ¾Ğ´ĞµĞ¶Ğ´Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ñ€ÑĞ´Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ….'}, 'en': {'title': 'EVTAR: Simplifying Virtual Try-Ons with Reference Images', 'desc': 'EVTAR is a novel virtual try-on model that improves the accuracy of garment fitting by using reference images. Unlike traditional methods that require complex inputs like body keypoints or segmentation maps, EVTAR simplifies the process by only needing the source image and target garment. It employs a two-stage training strategy to enhance the quality of the try-on results, ensuring better preservation of garment textures and details. By mimicking how humans use reference models for outfit selection, EVTAR achieves a more realistic dressing effect in its outputs.'}, 'zh': {'title': 'EVTARï¼šç®€åŒ–è™šæ‹Ÿè¯•è¡£çš„é«˜æ•ˆæ¨¡å‹', 'desc': 'EVTARæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è™šæ‹Ÿè¯•è¡£æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨å‚è€ƒå›¾åƒæ¥æé«˜å‡†ç¡®æ€§ï¼Œç®€åŒ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æ”¹å–„æœè£…çº¹ç†å’Œç»†èŠ‚çš„ä¿ç•™ã€‚ä¸ç°æœ‰çš„è™šæ‹Ÿè¯•è¡£æ–¹æ³•ä¸åŒï¼ŒEVTARåªéœ€æºå›¾åƒå’Œç›®æ ‡æœè£…è¾“å…¥ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¿å…äº†å¤æ‚çš„è¾“å…¥è¦æ±‚ã€‚è¯¥æ¨¡å‹æ— éœ€ä½¿ç”¨é®ç½©ã€å¯†é›†å§¿æ€æˆ–åˆ†å‰²å›¾ï¼Œç›´æ¥å°†ç›®æ ‡æœè£…é€‚é…åˆ°äººç‰©å›¾åƒä¸Šã€‚é€šè¿‡åˆ©ç”¨ä¸åŒä¸ªä½“ç©¿ç€ç›¸åŒæœè£…çš„å‚è€ƒå›¾åƒï¼ŒEVTARèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æœè£…çš„çº¹ç†å’Œç»†èŠ‚ï¼Œæ¨¡æ‹Ÿæ›´çœŸå®çš„ç©¿è¡£æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03996', 'title': 'Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots', 'url': 'https://huggingface.co/papers/2511.03996', 'abstract': 'A unified reinforcement learning controller integrates visual perception and motion control for humanoid robots in soccer, using Adversarial Motion Priors and an encoder-decoder architecture to achieve reactive and coherent behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control. We introduce an encoder-decoder architecture combined with a virtual perception system that models real-world visual characteristics, allowing the policy to recover privileged states from imperfect observations and establish active coordination between perception and action. The resulting controller demonstrates strong reactivity, consistently executing coherent and robust soccer behaviors across various scenarios, including real RoboCup matches.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'f9d9b36823f3fe2e', 'authors': ['Yushi Wang', 'Changsheng Luo', 'Penghui Chen', 'Jianran Liu', 'Weijian Sun', 'Tong Guo', 'Kechang Yang', 'Biao Hu', 'Yangang Zhang', 'Mingguo Zhao'], 'affiliations': ['ByteDance Seed', 'China Agricultural University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03996.jpg', 'data': {'categories': ['#games', '#optimization', '#architecture', '#cv', '#multimodal', '#robotics', '#rl'], 'emoji': 'âš½', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ³Ñ€Ğ°ÑÑ‰Ğ¸Ñ… Ğ² Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ». ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Adversarial Motion Priors Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ğ² Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° encoder-decoder Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ñ… RoboCup.'}, 'en': {'title': 'Unified Learning for Reactive Humanoid Soccer Robots', 'desc': "This paper presents a unified reinforcement learning controller designed for humanoid robots playing soccer, integrating visual perception with motion control. The approach utilizes Adversarial Motion Priors and an encoder-decoder architecture to enhance the robots' ability to react and behave coherently in dynamic environments. By bridging the gap between motion imitation and visually grounded control, the system allows robots to effectively respond to real-world visual inputs. The results show that this controller can perform robustly in various scenarios, including actual RoboCup matches, demonstrating improved reactivity and coordination between perception and action."}, 'zh': {'title': 'ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼šæå‡äººå½¢æœºå™¨äººè¶³çƒæŠ€èƒ½çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ï¼Œæ—¨åœ¨å°†è§†è§‰æ„ŸçŸ¥ä¸è¿åŠ¨æ§åˆ¶æ•´åˆï¼Œä»¥æé«˜äººå½¢æœºå™¨äººåœ¨è¶³çƒæ¯”èµ›ä¸­çš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨å¯¹æŠ—è¿åŠ¨å…ˆéªŒå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ§åˆ¶å™¨èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å®ç°å¿«é€Ÿååº”å’Œä¸€è‡´çš„è¡Œä¸ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿç³»ç»Ÿä¸­æ¨¡å—è§£è€¦å¯¼è‡´çš„å»¶è¿Ÿå“åº”é—®é¢˜ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œä¸­æ›´å¥½åœ°é€‚åº”å¤æ‚çš„è§†è§‰ä¿¡æ¯ã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ§åˆ¶å™¨åœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ååº”èƒ½åŠ›ï¼Œèƒ½å¤Ÿç¨³å®šåœ°æ‰§è¡Œè¶³çƒç›¸å…³çš„å¤æ‚åŠ¨ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03295', 'title': 'How to Evaluate Speech Translation with Source-Aware Neural MT Metrics', 'url': 'https://huggingface.co/papers/2511.03295', 'abstract': 'Source-aware metrics using ASR transcripts and back-translations improve speech-to-text evaluation by addressing alignment issues and incorporating source information.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic evaluation of speech-to-text translation (ST) systems is typically performed by comparing translation hypotheses with one or more reference translations. While effective to some extent, this approach inherits the limitation of reference-based evaluation that ignores valuable information from the source input. In machine translation (MT), recent progress has shown that neural metrics incorporating the source text achieve stronger correlation with human judgments. Extending this idea to ST, however, is not trivial because the source is audio rather than text, and reliable transcripts or alignments between source and references are often unavailable. In this work, we conduct the first systematic study of source-aware metrics for ST, with a particular focus on real-world operating conditions where source transcripts are not available. We explore two complementary strategies for generating textual proxies of the input audio, automatic speech recognition (ASR) transcripts, and back-translations of the reference translation, and introduce a novel two-step cross-lingual re-segmentation algorithm to address the alignment mismatch between synthetic sources and reference translations. Our experiments, carried out on two ST benchmarks covering 79 language pairs and six ST systems with diverse architectures and performance levels, show that ASR transcripts constitute a more reliable synthetic source than back-translations when word error rate is below 20%, while back-translations always represent a computationally cheaper but still effective alternative. Furthermore, our cross-lingual re-segmentation algorithm enables robust use of source-aware MT metrics in ST evaluation, paving the way toward more accurate and principled evaluation methodologies for speech translation.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '93fe21aa583cbc7e', 'authors': ['Mauro Cettolo', 'Marco Gaido', 'Matteo Negri', 'Sara Papi', 'Luisa Bentivogli'], 'affiliations': ['Fondazione Bruno Kessler'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03295.jpg', 'data': {'categories': ['#benchmark', '#audio', '#multilingual', '#machine_translation', '#science'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° speech-to-text Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ speech-to-text Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 79 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ ASR Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹, Ñ‡ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ÑĞ»Ğ¾Ğ² Ğ½Ğ¸Ğ¶Ğµ 20%.'}, 'en': {'title': 'Enhancing Speech-to-Text Evaluation with Source-Aware Metrics', 'desc': 'This paper presents a novel approach to improve the evaluation of speech-to-text (ST) systems by using source-aware metrics that incorporate information from the original audio input. The authors propose generating textual proxies from audio using automatic speech recognition (ASR) transcripts and back-translations, addressing the challenge of alignment between these synthetic sources and reference translations. They introduce a two-step cross-lingual re-segmentation algorithm to enhance the evaluation process, particularly in scenarios where reliable transcripts are unavailable. Experimental results demonstrate that ASR transcripts provide a more reliable source than back-translations under certain conditions, leading to more accurate evaluations of ST systems.'}, 'zh': {'title': 'æºæ„ŸçŸ¥æŒ‡æ ‡æå‡è¯­éŸ³è½¬æ–‡æœ¬è¯„ä¼°å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†æºæ„ŸçŸ¥æŒ‡æ ‡åœ¨è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆSTï¼‰è¯„ä¼°ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³å¯¹é½é—®é¢˜å¹¶æ•´åˆæºä¿¡æ¯ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå‚è€ƒç¿»è¯‘ï¼Œå¿½è§†äº†æºè¾“å…¥ä¸­çš„é‡è¦ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç”Ÿæˆæ–‡æœ¬ä»£ç†çš„æ–¹æ³•ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•å’Œå‚è€ƒç¿»è¯‘çš„å›è¯‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„è·¨è¯­è¨€é‡æ–°åˆ†æ®µç®—æ³•æ¥è§£å†³åˆæˆæºä¸å‚è€ƒç¿»è¯‘ä¹‹é—´çš„å¯¹é½ä¸åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å­—é”™è¯¯ç‡ä½äº20%æ—¶ï¼ŒASRè½¬å½•æ¯”å›è¯‘æ›´å¯é ï¼Œè€Œå›è¯‘åˆ™æ˜¯è®¡ç®—ä¸Šæ›´ä¾¿å®œä½†ä»æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02280', 'title': 'SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning', 'url': 'https://huggingface.co/papers/2511.02280', 'abstract': "SAIL-RL enhances multimodal large language models' reasoning capabilities using a dual reward system that improves factual grounding, logical coherence, and adaptability.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '388cff3b05f165a8', 'authors': ['Fangxun Shu', 'Yongjie Ye', 'Yue Liao', 'Zijian Kang', 'Weijie Yin', 'Jiacong Wang', 'Xiao Liang', 'Shuicheng Yan', 'Chao Feng'], 'affiliations': ['Douyin SAIL Team', 'National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02280.jpg', 'data': {'categories': ['#open_source', '#alignment', '#multimodal', '#training', '#rlhf', '#hallucinations', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'SAIL-RL â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¸Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹: Thinking Reward Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Judging Reward Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° GPT-4o.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Dual Rewards', 'desc': 'SAIL-RL is a reinforcement learning framework designed to improve the reasoning abilities of multimodal large language models (MLLMs). It introduces a dual reward system that includes a Thinking Reward for assessing the quality of reasoning and a Judging Reward that helps the model decide when to use deep reasoning versus direct answers. This approach addresses limitations of previous methods that relied solely on correct outcomes and uniform thinking strategies, which could lead to inconsistent reasoning. Experiments show that SAIL-RL enhances performance on reasoning and multimodal tasks while reducing errors known as hallucinations, making MLLMs more reliable and adaptable.'}, 'zh': {'title': 'SAIL-RLï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŒé‡å¥–åŠ±ç³»ç»Ÿ', 'desc': 'SAIL-RLæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚å®ƒé€šè¿‡åŒé‡å¥–åŠ±ç³»ç»Ÿæ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œç¡®ä¿æ¨¡å‹ä¸ä»…èƒ½ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè¿˜èƒ½è¿›è¡Œåˆç†æ¨ç†ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬æ€ç»´å¥–åŠ±å’Œåˆ¤æ–­å¥–åŠ±ï¼Œå‰è€…è¯„ä¼°æ¨ç†è´¨é‡ï¼Œåè€…æ ¹æ®ä»»åŠ¡å¤æ‚æ€§å†³å®šæ˜¯æ·±åº¦æ¨ç†è¿˜æ˜¯ç›´æ¥å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAIL-RLåœ¨æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04962', 'title': 'Too Good to be Bad: On the Failure of LLMs to Role-Play Villains', 'url': 'https://huggingface.co/papers/2511.04962', 'abstract': "LLMs struggle to authentically portray morally ambiguous or villainous characters due to safety alignment, as evidenced by the Moral RolePlay benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.", 'score': 52, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'fc87d40dbfc83633', 'authors': ['Zihao Yi', 'Qingxuan Jiang', 'Ruotian Ma', 'Xingyu Chen', 'Qu Yang', 'Mengru Wang', 'Fanghua Ye', 'Ying Shen', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus'], 'affiliations': ['Sun Yat-Sen University', 'Tencent Multimodal Department'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04962.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#story_generation', '#alignment', '#dataset'], 'emoji': 'ğŸ˜ˆ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ LLM Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ·Ğ»Ğ¾Ğ´ĞµĞµĞ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ·Ğ»Ğ¾Ğ´ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸Ğ·-Ğ·Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Moral RolePlay Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğ¾Ğ¹ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ñ‡Ñ‘Ñ‚ĞºĞ°Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ¾Ğ»ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Navigating the Tension: Safety vs. Creative Fidelity in LLMs', 'desc': "This paper explores the limitations of Large Language Models (LLMs) in portraying morally ambiguous or villainous characters due to their safety alignment. The authors introduce the Moral RolePlay benchmark, which evaluates LLMs on a scale of moral alignment, revealing that as character morality decreases, the models' ability to role-play authentically declines. The study finds that LLMs often replace complex villainous traits with simpler forms of aggression, indicating a struggle with safety principles. This research highlights the tension between ensuring model safety and achieving creative fidelity in character portrayal, suggesting a need for improved alignment methods."}, 'zh': {'title': 'å®‰å…¨æ€§ä¸åˆ›é€ æ€§ä¹‹é—´çš„çŸ›ç›¾', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿé“å¾·æ¨¡ç³Šæˆ–åæ´¾è§’è‰²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶å®‰å…¨å¯¹é½çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†é“å¾·è§’è‰²æ‰®æ¼”åŸºå‡†ï¼ˆMoral RolePlay benchmarkï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒé“å¾·æ°´å¹³ä¸‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€è§’è‰²é“å¾·æ°´å¹³çš„é™ä½ï¼Œæ¨¡å‹çš„è§’è‰²æ‰®æ¼”å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨è¡¨ç°å‡ºæ¬ºéª—å’Œæ“æ§ç­‰ç‰¹å¾æ—¶ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†æ¨¡å‹å®‰å…¨æ€§ä¸åˆ›é€ æ€§è¡¨ç°ä¹‹é—´çš„å…³é”®çŸ›ç›¾ï¼Œä¸ºæœªæ¥å¼€å‘æ›´ç»†è‡´çš„å¯¹é½æ–¹æ³•æä¾›äº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05491', 'title': 'Visual Spatial Tuning', 'url': 'https://huggingface.co/papers/2511.05491', 'abstract': 'A framework called Visual Spatial Tuning (VST) enhances the spatial abilities of Vision-Language Models (VLMs) through progressive training with specialized datasets, achieving state-of-the-art results on spatial benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including 34.8% on MMSI-Bench and 61.2% on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.', 'score': 49, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '47d747b35f57a189', 'authors': ['Rui Yang', 'Ziyu Zhu', 'Yanwei Li', 'Jingjia Huang', 'Shen Yan', 'Siyuan Zhou', 'Zhe Liu', 'Xiangtai Li', 'Shuangye Li', 'Wenqian Wang', 'Yi Lin', 'Hengshuang Zhao'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05491.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#training', '#dataset', '#rl'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Visual Spatial Tuning (VST), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°: VST-P Ñ 4.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ VST-R Ñ 135K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Vision-Language Models with Visual Spatial Tuning', 'desc': "The paper introduces Visual Spatial Tuning (VST), a framework designed to improve the spatial abilities of Vision-Language Models (VLMs). It does this by using specialized datasets, including VST-P for spatial perception and VST-R for spatial reasoning, which help the models learn from a wide range of visual inputs. The training process involves a combination of supervised fine-tuning and reinforcement learning, allowing the models to develop strong spatial reasoning skills without compromising their general capabilities. As a result, VST achieves state-of-the-art performance on various spatial benchmarks, demonstrating its effectiveness in enhancing AI's understanding of spatial relationships."}, 'zh': {'title': 'è§†è§‰ç©ºé—´è°ƒä¼˜ï¼šæå‡æ¨¡å‹ç©ºé—´èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é€æ­¥è®­ç»ƒå’Œä¸“é—¨æ•°æ®é›†æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„ç©ºé—´èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†VST-Pï¼ŒåŒ…å«410ä¸‡æ ·æœ¬ï¼Œæ¶µç›–19ç§æŠ€èƒ½ï¼Œå¸®åŠ©æ¨¡å‹æé«˜ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¼•å…¥äº†VST-Ræ•°æ®é›†ï¼ŒåŒ…å«135Kæ ·æœ¬ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡Œç©ºé—´æ¨ç†ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„é€æ­¥è®­ç»ƒï¼ŒVSTåœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æå‡è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05271', 'title': 'DeepEyesV2: Toward Agentic Multimodal Model', 'url': 'https://huggingface.co/papers/2511.05271', 'abstract': 'DeepEyesV2, an agentic multimodal model, uses a two-stage training pipeline to effectively integrate tool use, demonstrating robust performance across real-world reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.', 'score': 42, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'd54572b9ee618c41', 'authors': ['Jack Hong', 'Chenxiao Zhao', 'ChengLin Zhu', 'Weiheng Lu', 'Guohai Xu', 'Xing Yu'], 'affiliations': ['Xiaohongshu Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05271.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#agents', '#multimodal', '#training', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'DeepEyesV2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº RealX-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DeepEyesV2 ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Empowering Multimodal Reasoning with Adaptive Tool Use', 'desc': 'DeepEyesV2 is a multimodal model designed to integrate tool use effectively for real-world reasoning tasks. It employs a two-stage training pipeline that first establishes tool-use patterns and then refines them through reinforcement learning. The model is evaluated using RealX-Bench, a benchmark that tests its ability to combine perception, search, and reasoning capabilities. DeepEyesV2 demonstrates adaptive tool invocation, using different tools based on the task context, which enhances its performance in various reasoning scenarios.'}, 'zh': {'title': 'DeepEyesV2ï¼šæ™ºèƒ½å¤šæ¨¡æ€æ¨¡å‹çš„å·¥å…·æ•´åˆä¸æ¨ç†èƒ½åŠ›', 'desc': 'DeepEyesV2æ˜¯ä¸€ç§å…·æœ‰ä»£ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹æ¥æœ‰æ•ˆæ•´åˆå·¥å…·ä½¿ç”¨ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£æ–‡æœ¬å’Œå›¾åƒï¼Œè¿˜èƒ½ä¸»åŠ¨è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå¦‚ä»£ç æ‰§è¡Œç¯å¢ƒå’Œç½‘ç»œæœç´¢ï¼Œå¹¶å°†è¿™äº›æ“ä½œèå…¥æ¨ç†ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå•é ç›´æ¥çš„å¼ºåŒ–å­¦ä¹ æ— æ³•æœ‰æ•ˆä¿ƒä½¿å·¥å…·ä½¿ç”¨è¡Œä¸ºï¼Œå› æ­¤æå‡ºäº†å†·å¯åŠ¨é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚DeepEyesV2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œç†è§£ã€æ•°å­¦æ¨ç†å’Œæœç´¢å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04662', 'title': 'VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical\n  Consistency Checks', 'url': 'https://huggingface.co/papers/2511.04662', 'abstract': "VeriCoT, a neuro-symbolic method, formalizes and verifies logical arguments in Chain-of-Thought reasoning to improve the reliability and accuracy of LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT's verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.", 'score': 34, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '378f11bdb7af3595', 'authors': ['Yu Feng', 'Nathaniel Weir', 'Kaj Bostrom', 'Sam Bayless', 'Darion Cassel', 'Sapana Chaudhary', 'Benjamin Kiesl-Reiter', 'Huzefa Rangwala'], 'affiliations': ['Amazon Web Services', 'University of Pennsylvania'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04662.jpg', 'data': {'categories': ['#benchmark', '#rlhf', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'VeriCoT â€” ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Chain-of-Thought Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑĞ¼ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ProofWriter, LegalBench Ğ¸ BioASQ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ VeriCoT ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, supervised fine-tuning Ğ¸ preference fine-tuning Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ DPO, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM.'}, 'en': {'title': 'VeriCoT: Enhancing LLM Reliability through Logical Verification', 'desc': 'VeriCoT is a neuro-symbolic approach designed to enhance the reliability of large language models (LLMs) by formalizing and verifying their logical arguments during Chain-of-Thought (CoT) reasoning. It translates each reasoning step into first-order logic, allowing for the identification of premises that are grounded in context or prior knowledge. This method enables automated verification of logical validity, helping to pinpoint flawed reasoning that could lead to incorrect conclusions. Experiments demonstrate that VeriCoT not only identifies errors in reasoning but also improves the overall accuracy of LLMs through various fine-tuning techniques.'}, 'zh': {'title': 'VeriCoTï¼šæå‡æ¨ç†å¯é æ€§çš„ç¥ç»ç¬¦å·æ–¹æ³•', 'desc': 'VeriCoTæ˜¯ä¸€ç§ç¥ç»ç¬¦å·æ–¹æ³•ï¼Œæ—¨åœ¨å½¢å¼åŒ–å’ŒéªŒè¯é“¾å¼æ€ç»´æ¨ç†ä¸­çš„é€»è¾‘è®ºè¯ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚å°½ç®¡LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æ¨ç†ï¼Œä½†å®ƒä»¬æ— æ³•å¯é åœ°éªŒè¯è‡ªå·±çš„é€»è¾‘ï¼Œå¯¼è‡´åœ¨é«˜é£é™©åœºæ™¯ä¸­ä¿¡ä»»åº¦ä¸‹é™ã€‚VeriCoTé€šè¿‡å°†æ¯ä¸ªæ¨ç†æ­¥éª¤å½¢å¼åŒ–ä¸ºä¸€é˜¶é€»è¾‘ï¼Œå¹¶è¯†åˆ«æ”¯æŒè®ºè¯çš„å‰æï¼Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVeriCoTèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ¨ç†ä¸­çš„ç¼ºé™·ï¼Œå¹¶ä½œä¸ºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å¼ºé¢„æµ‹æŒ‡æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04898', 'title': 'Real-Time Reasoning Agents in Evolving Environments', 'url': 'https://huggingface.co/papers/2511.04898', 'abstract': "Real-Time Reasoning Gym demonstrates the challenges of deploying language models in dynamic environments, introducing AgileThinker to balance reasoning depth and response latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agent's reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents.", 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '33419e29d68f1224', 'authors': ['Yule Wen', 'Yixin Ye', 'Yanzhe Zhang', 'Diyi Yang', 'Hao Zhu'], 'affiliations': ['Georgia Institute of Technology', 'Shanghai Jiao Tong University', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04898.jpg', 'data': {'categories': [], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Real-Time Reasoning Gym â€” Ñ‚ĞµÑÑ‚Ğ¾Ğ²ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ LLM: Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AgileThinker Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¾Ñ‚ĞºĞ»ĞµÑ€Ğ¾Ğ¼ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'AgileThinker: Balancing Depth and Speed in Real-Time Reasoning', 'desc': 'The paper introduces the concept of real-time reasoning for agents operating in dynamic environments, where timely decision-making is crucial. It presents the Real-Time Reasoning Gym, a framework to evaluate how well language models can adapt to changing conditions while reasoning. The authors propose two types of agents: reactive agents for quick responses and planning agents for deeper reasoning, but find that both struggle under pressure. To overcome these challenges, they introduce AgileThinker, which effectively combines both reasoning approaches, leading to better performance in complex and time-sensitive tasks.'}, 'zh': {'title': 'å®æ—¶æ¨ç†ï¼šå¹³è¡¡æ·±åº¦ä¸é€Ÿåº¦çš„æ™ºèƒ½ä½“æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†AgileThinkerä»¥å¹³è¡¡æ¨ç†æ·±åº¦å’Œå“åº”å»¶è¿Ÿã€‚ç°å®ä¸–ç•Œä¸­çš„æ™ºèƒ½ä½“ä¸ä»…éœ€è¦é€»è¾‘åˆ¤æ–­ï¼Œè¿˜éœ€è¦åŠæ—¶åšå‡ºå†³ç­–ï¼Œè¿™è¦æ±‚å®ƒä»¬å¯¹ç¯å¢ƒçš„å˜åŒ–ä¿æŒæŒç»­çš„å…³æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†å®æ—¶æ¨ç†ä½œä¸ºä¸€ç§æ–°é—®é¢˜çš„è¡¨è¿°ï¼Œå¹¶æ„å»ºäº†å®æ—¶æ¨ç†è®­ç»ƒåœºæ¥å±•ç¤ºè¿™ä¸€ç‚¹ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ç°æœ‰çš„è¯­è¨€æ¨¡å‹å·²ç»å¾ˆå…ˆè¿›ï¼Œä½†åœ¨å¤æ‚é—®é¢˜å’Œæ—¶é—´å‹åŠ›ä¸‹ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥åšå‡ºé€»è¾‘å’ŒåŠæ—¶çš„åˆ¤æ–­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05369', 'title': 'Dense Motion Captioning', 'url': 'https://huggingface.co/papers/2511.05369', 'abstract': 'A new task, Dense Motion Captioning, is introduced with a large-scale dataset, CompMo, and a model, DEMO, that integrates a language model with a motion adapter to generate detailed, temporally grounded captions for 3D human motion sequences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'dd532f9caca9df70', 'authors': ['Shiyao Xu', 'Benedetta Liberatori', 'GÃ¼l Varol', 'Paolo Rota'], 'affiliations': ['LIGM, Ecole des Ponts, IP Paris, Univ Gustave Eiffel, CNRS', 'University of Trento'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05369.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#3d', '#multimodal', '#dataset', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Dense Motion Captioning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CompMo Ñ 60 000 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ¾Ñ‚ 2 Ğ´Ğ¾ 10 Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹. Ğ‘Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DEMO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing 3D Motion Understanding with Dense Captions', 'desc': 'This paper introduces Dense Motion Captioning, a new task focused on understanding and describing 3D human motion sequences. It presents the Complex Motion Dataset (CompMo), which is a large-scale dataset containing 60,000 richly annotated motion sequences with precise temporal boundaries. The authors propose a model called DEMO that combines a language model with a motion adapter to generate detailed captions that are temporally grounded. The results demonstrate that DEMO significantly outperforms existing methods, providing a strong foundation for future research in the field of 3D motion understanding and captioning.'}, 'zh': {'title': 'å¯†é›†è¿åŠ¨å­—å¹•ç”Ÿæˆï¼š3DåŠ¨ä½œç†è§£çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼Œç§°ä¸ºå¯†é›†è¿åŠ¨å­—å¹•ç”Ÿæˆï¼ˆDense Motion Captioningï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†CompMoå’Œæ¨¡å‹DEMOã€‚è¯¥ä»»åŠ¡æ—¨åœ¨å¯¹3Däººç±»è¿åŠ¨åºåˆ—ä¸­çš„åŠ¨ä½œè¿›è¡Œæ—¶é—´å®šä½å’Œå­—å¹•ç”Ÿæˆã€‚CompMoæ•°æ®é›†åŒ…å«60,000ä¸ªå¤æ‚è¿åŠ¨åºåˆ—ï¼Œæä¾›äº†è¯¦ç»†çš„æ—¶é—´æ³¨é‡Šï¼Œå…‹æœäº†ç°æœ‰æ•°æ®é›†çš„ä¸è¶³ã€‚DEMOæ¨¡å‹ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œè¿åŠ¨é€‚é…å™¨ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯†é›†ä¸”æ—¶é—´å‡†ç¡®çš„å­—å¹•ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05017', 'title': 'Towards Mitigating Hallucinations in Large Vision-Language Models by\n  Refining Textual Embeddings', 'url': 'https://huggingface.co/papers/2511.05017', 'abstract': 'Refining textual embeddings with average-pooled visual features improves visual grounding and reduces hallucinations in LVLM architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '1e2a84ff5448137b', 'authors': ['Aakriti Agrawal', 'Gouthaman KV', 'Rohith Aralikatti', 'Gauri Jagatap', 'Jiaxin Yuan', 'Vijay Kamarshi', 'Andrea Fanelli', 'Furong Huang'], 'affiliations': ['Capital One', 'Dolby Laboratories', 'Hilabs', 'University of Maryland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05017.jpg', 'data': {'categories': ['#hallucinations', '#architecture', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ…Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ (average pooling) ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ñ„Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Textual Embeddings with Visual Features to Reduce Hallucinations', 'desc': "This paper addresses a bias in large vision-language models (LVLMs) that favors language over visual information. The authors propose a method to enhance textual embeddings by incorporating average-pooled visual features, which helps improve the model's ability to ground visual content accurately. By refining the textual embeddings in this way, the approach reduces the occurrence of hallucinations, where the model generates incorrect or nonsensical outputs. The study suggests that while their method is effective, more complex fusion techniques could further improve the integration of visual and textual modalities in future research."}, 'zh': {'title': 'ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œæå‡è§†è§‰å®šä½ä¸å‡å°‘å¹»è§‰', 'desc': 'æœ¬ç ”ç©¶å‘ç°å½“å‰çš„LVLMæ¶æ„åœ¨è¯­è¨€æ¨¡æ€ä¸Šå­˜åœ¨å›ºæœ‰åå·®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå°†è§†è§‰åµŒå…¥ç®€å•åœ°é™„åŠ åˆ°è¾“å…¥æ–‡æœ¬åºåˆ—çš„å¸¸è§åšæ³•æ‰€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆå¹³å‡æ± åŒ–çš„è§†è§‰ç‰¹å¾æ¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æ”¹å–„äº†è§†è§‰å®šä½ï¼Œå¹¶åœ¨å·²å»ºç«‹çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚è™½ç„¶å¹³å‡æ± åŒ–æ˜¯ä¸€ç§ç›´æ¥ã€ç¨³å¥ä¸”é«˜æ•ˆçš„è§†è§‰ä¿¡æ¯æ•´åˆæ–¹å¼ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºæ›´å¤æ‚çš„èåˆæ–¹æ³•å¯èƒ½ä¼šè¿›ä¸€æ­¥å¢å¼ºè§†è§‰å®šä½å’Œè·¨æ¨¡æ€å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04707', 'title': 'Jailbreaking in the Haystack', 'url': 'https://huggingface.co/papers/2511.04707', 'abstract': 'NINJA, a jailbreak attack method, appends benign content to harmful goals in long-context language models, increasing attack success rates and revealing vulnerabilities in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '24320d1ba080516b', 'authors': ['Rishi Rajesh Shah', 'Chen Henry Wu', 'Shashwat Saxena', 'Ziqian Zhong', 'Alexander Robey', 'Aditi Raghunathan'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04707.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#agents', '#long_context', '#security'], 'emoji': 'ğŸª¡', 'ru': {'title': 'Ğ˜Ğ³Ğ¾Ğ»ĞºĞ° Ğ² ÑÑ‚Ğ¾Ğ³Ğµ ÑĞµĞ½Ğ°: ĞºĞ°Ğº Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ NINJA Ğ´Ğ»Ñ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ±Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğº Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ HarmBench Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LLaMA, Qwen, Mistral Ğ¸ Gemini. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'NINJA: Uncovering Vulnerabilities in Long-Context Language Models', 'desc': 'The paper introduces NINJA, a novel jailbreak attack method that exploits long-context language models by appending harmless content to harmful objectives. This technique enhances the success rates of attacks by strategically positioning harmful goals within the input context. Experiments demonstrate that NINJA effectively compromises various state-of-the-art models, revealing significant safety vulnerabilities. The findings suggest that even seemingly benign content can pose risks when used inappropriately, highlighting the need for improved safety measures in long-context LMs.'}, 'zh': {'title': 'NINJAï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§', 'desc': 'NINJAæ˜¯ä¸€ç§è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹ä¸­é™„åŠ è‰¯æ€§å†…å®¹æ¥å®ç°å¯¹æœ‰å®³ç›®æ ‡çš„æ”»å‡»ï¼Œä»è€Œæé«˜æ”»å‡»æˆåŠŸç‡å¹¶æ­ç¤ºæ¨¡å‹çš„è„†å¼±æ€§ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºæœ‰å®³ç›®æ ‡çš„ä½ç½®å¯¹å®‰å…¨æ€§çš„é‡è¦å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNINJAåœ¨å¤šä¸ªå…ˆè¿›çš„å¼€æ”¾å’Œä¸“æœ‰æ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬LLaMAã€Qwenã€Mistralå’ŒGeminiã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„è‰¯æ€§é•¿ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå¯èƒ½åœ¨ç°ä»£è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥æ ¹æœ¬æ€§çš„è„†å¼±æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01047', 'title': 'HAFixAgent: History-Aware Automated Program Repair Agent', 'url': 'https://huggingface.co/papers/2511.01047', 'abstract': 'HAFixAgent, a history-aware bug-fixing agent, improves automated program repair by incorporating repository history, enhancing effectiveness and efficiency for complex multi-hunk bugs.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated program repair (APR) has recently shifted toward large language models and agent-based systems, yet most systems rely on local snapshot context, overlooking repository history. Prior work shows that repository history helps repair single-line bugs, since the last commit touching the buggy line is often the bug-introducing one. In this paper, we investigate whether repository history can also improve agentic APR systems at scale, especially for complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing Agent that injects blame-derived repository heuristics into its repair loop. A preliminary study of all 854 real-world bugs from Defects4J motivates our design, showing that bug-relevant history is both widely available and highly concentrated. Empirical comparison of HAFixAgent with two state-of-the-art baselines shows: (1) Effectiveness: HAFixAgent significantly improves over the agent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2) Efficiency: history does not significantly increase agent steps and keeps token costs comparable, with notably lower median costs for complex multi-file-multi-hunk bugs. (3) Practicality: combining different historical heuristics repairs more bugs, offering a clear cost-benefit trade-off. HAFixAgent offers a practical recipe for history-aware agentic APR: ground the agent in version control history, prioritize diff-based historical context, and integrate complementary heuristics when needed.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': '91f8e98e37d5b148', 'authors': ['Yu Shi', 'Hao Li', 'Bram Adams', 'Ahmed E. Hassan'], 'affiliations': ['Queens University, Canada'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01047.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ¾Ğ´Ğ° ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº', 'desc': 'HAFixAgent â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ° ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ñ‡Ğ¸Ğ½ĞºĞ¸ Ğ±Ğ°Ğ³Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ blame-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ñ†Ğ¸ĞºĞ» Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° 212% Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ñ…ÑƒĞ½ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ñ Ğ¸ Ğ½Ğ° 30% Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…ÑƒĞ½ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Harnessing History for Smarter Bug Fixing', 'desc': 'HAFixAgent is a novel approach to automated program repair (APR) that leverages repository history to enhance the repair of complex multi-hunk bugs. By incorporating blame-derived heuristics from version control history, it significantly improves the effectiveness of bug fixing, outperforming existing agent-based systems. The empirical results demonstrate that HAFixAgent repairs more bugs while maintaining efficiency, as it does not substantially increase the number of agent steps or costs. This paper highlights the importance of historical context in APR, providing a practical framework for integrating repository insights into bug-fixing agents.'}, 'zh': {'title': 'å†å²æ„ŸçŸ¥ï¼Œæ™ºèƒ½ä¿®å¤ï¼', 'desc': 'HAFixAgentæ˜¯ä¸€ç§å†å²æ„ŸçŸ¥çš„è‡ªåŠ¨ä¿®å¤ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥ä»£ç åº“å†å²æ¥æé«˜å¤æ‚å¤šå—é”™è¯¯çš„ä¿®å¤æ•ˆæœå’Œæ•ˆç‡ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç åº“å†å²å¯¹äºä¿®å¤å•è¡Œé”™è¯¯éå¸¸æœ‰æ•ˆï¼Œå› ä¸ºæœ€åä¸€æ¬¡ä¿®æ”¹é”™è¯¯è¡Œçš„æäº¤é€šå¸¸æ˜¯å¼•å…¥é”™è¯¯çš„æäº¤ã€‚æœ¬æ–‡æ¢è®¨äº†å†å²ä¿¡æ¯æ˜¯å¦ä¹Ÿèƒ½åœ¨å¤§è§„æ¨¡çš„ä»£ç†è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿä¸­æ”¹å–„ä¿®å¤æ•ˆæœï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤æ‚çš„å¤šå—é”™è¯¯ã€‚HAFixAgenté€šè¿‡å°†åŸºäºè´£ä»»çš„å†å²å¯å‘å¼æ–¹æ³•æ³¨å…¥ä¿®å¤å¾ªç¯ï¼Œæ˜¾è‘—æé«˜äº†ä¿®å¤æ•ˆç‡å’Œæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.24505', 'title': 'CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?', 'url': 'https://huggingface.co/papers/2510.24505', 'abstract': "Natural language critiques improve confidence calibration in LLMs, with CritiCal training method outperforming other approaches and enhancing reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate confidence calibration in Large Language Models (LLMs) is critical for safe use in high-stakes domains, where clear verbalized confidence enhances user trust. Traditional methods that mimic reference confidence expressions often fail to capture the reasoning needed for accurate confidence assessment. We propose natural language critiques as a solution, ideally suited for confidence calibration, as precise gold confidence labels are hard to obtain and often require multiple generations. This paper studies how natural language critiques can enhance verbalized confidence, addressing: (1) What to critique: uncertainty (question-focused) or confidence (answer-specific)? Analysis shows confidence suits multiple-choice tasks, while uncertainty excels in open-ended scenarios. (2) How to critique: self-critique or critique calibration training? We propose Self-Critique, enabling LLMs to critique and optimize their confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration training method that leverages natural language critiques to improve confidence calibration, moving beyond direct numerical optimization. Experiments show that CritiCal significantly outperforms Self-Critique and other competitive baselines, even surpassing its teacher model, GPT-4o, in complex reasoning tasks. CritiCal also shows robust generalization in out-of-distribution settings, advancing LLM's reliability.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '89e565a8c78ad0bd', 'authors': ['Qing Zong', 'Jiayu Liu', 'Tianshi Zheng', 'Chunyang Li', 'Baixuan Xu', 'Haochen Shi', 'Weiqi Wang', 'Zhaowei Wang', 'Chunkit Chan', 'Yangqiu Song'], 'affiliations': ['Department of Computer Science and Engineering, HKUST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2510.24505.jpg', 'data': {'categories': ['#rlhf', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ (Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ) Ğ¸ ĞºĞ°Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ (ÑĞ°Ğ¼Ğ¾ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ CritiCal, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CritiCal Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-4o Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ´Ğ²Ğ¸Ğ³Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing LLM Reliability with Natural Language Critiques', 'desc': 'This paper introduces a method called CritiCal that uses natural language critiques to improve the confidence calibration of Large Language Models (LLMs). Accurate confidence calibration is essential for ensuring that LLMs can be trusted in important situations, and traditional methods often fall short. The study explores how to effectively critique LLM outputs, determining that confidence critiques work best for multiple-choice tasks while uncertainty critiques are better for open-ended questions. The results show that CritiCal outperforms other methods, including Self-Critique, and enhances the reliability of LLMs in complex reasoning tasks.'}, 'zh': {'title': 'è‡ªç„¶è¯­è¨€æ‰¹è¯„æå‡LLMsä¿¡å¿ƒæ ¡å‡†', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€æ‰¹è¯„å¦‚ä½•æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¿¡å¿ƒæ ¡å‡†ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®è¯„ä¼°ä¿¡å¿ƒï¼Œè€Œè‡ªç„¶è¯­è¨€æ‰¹è¯„æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘æ‰¹è¯„å’Œæ‰¹è¯„æ ¡å‡†è®­ç»ƒï¼ˆCritiCalï¼‰ä¸¤ç§æ–¹æ³•ï¼Œå‰è€…å¸®åŠ©æ¨¡å‹ä¼˜åŒ–ä¿¡å¿ƒï¼Œåè€…åˆ™åˆ©ç”¨è‡ªç„¶è¯­è¨€æ‰¹è¯„æ¥æå‡ä¿¡å¿ƒæ ¡å‡†çš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCritiCalæ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03506', 'title': 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents', 'url': 'https://huggingface.co/papers/2511.03506', 'abstract': 'HaluMem, a benchmark for evaluating memory hallucinations in AI systems, identifies and analyzes hallucinations across memory extraction, updating, and question answering stages using large-scale human-AI interaction datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.', 'score': 93, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '05b93ffabaaec001', 'authors': ['Ding Chen', 'Simin Niu', 'Kehang Li', 'Peng Liu', 'Xiangping Zheng', 'Bo Tang', 'Xinchi Li', 'Feiyu Xiong', 'Zhiyu Li'], 'affiliations': ['China Telecom Research Institute', 'Harbin Engineering University', 'MemTensor (Shanghai) Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03506.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#agents', '#long_context', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ AI Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ HaluMem â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ 15 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… (Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'HaluMem: Unveiling Memory Hallucinations in AI Systems', 'desc': 'HaluMem is a new benchmark designed to evaluate memory hallucinations in AI systems, particularly focusing on large language models (LLMs) and AI agents. It identifies and analyzes hallucinations during three key stages: memory extraction, memory updating, and question answering. By using extensive human-AI interaction datasets, HaluMem allows researchers to pinpoint where in the memory process these hallucinations occur, rather than just assessing the final output. The findings indicate that hallucinations often arise during the initial stages and can lead to errors in the final responses, highlighting the need for improved memory management techniques.'}, 'zh': {'title': 'HaluMemï¼šæ­ç¤ºAIè®°å¿†å¹»è§‰çš„å…¨æ–°åŸºå‡†', 'desc': 'HaluMemæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­è®°å¿†å¹»è§‰çš„åŸºå‡†ã€‚å®ƒé€šè¿‡åˆ†æè®°å¿†æå–ã€æ›´æ–°å’Œé—®ç­”é˜¶æ®µçš„å¹»è§‰ï¼Œåˆ©ç”¨å¤§è§„æ¨¡çš„äººæœºäº¤äº’æ•°æ®é›†æ¥è¯†åˆ«å’Œåˆ†æè¿™äº›é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç«¯åˆ°ç«¯çš„é—®ç­”ä¸Šï¼Œéš¾ä»¥å®šä½å¹»è§‰å‘ç”Ÿçš„å…·ä½“æ“ä½œé˜¶æ®µã€‚HaluMemå®šä¹‰äº†ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢æ­ç¤ºä¸åŒæ“ä½œé˜¶æ®µçš„å¹»è§‰è¡Œä¸ºï¼Œå¹¶æ„å»ºäº†ç”¨æˆ·ä¸­å¿ƒçš„å¤šè½®äººæœºäº¤äº’æ•°æ®é›†ä»¥æ”¯æŒè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07327', 'title': 'IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction', 'url': 'https://huggingface.co/papers/2511.07327', 'abstract': 'IterResearch, an iterative deep-research paradigm, improves long-horizon reasoning by reformulating it as a Markov Decision Process with strategic workspace reconstruction and Efficiency-Aware Policy Optimization, achieving better performance and interaction scaling compared to existing agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.', 'score': 74, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '635698d160c24d3f', 'authors': ['Guoxin Chen', 'Zile Qiao', 'Xuanzhong Chen', 'Donglei Yu', 'Haotian Xu', 'Wayne Xin Zhao', 'Ruihua Song', 'Wenbiao Yin', 'Huifeng Yin', 'Liwen Zhang', 'Kuan Li', 'Minpeng Liao', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'OpenRLHF', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07327.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#agents', '#long_context', '#rl', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'IterResearch Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞœĞ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Efficiency-Aware Policy Optimization (EAPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ´Ğ¸ÑĞºĞ¾Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸ÑÂ» Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ° ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ IterResearch Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 14.5 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ 2048 Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ 3.5% Ğ´Ğ¾ 42.5%.'}, 'en': {'title': 'IterResearch: Revolutionizing Long-Horizon Reasoning in AI', 'desc': 'The paper introduces IterResearch, a new approach to enhance long-horizon reasoning in AI by treating it as a Markov Decision Process. It addresses the limitations of existing methods that suffer from context suffocation by using strategic workspace reconstruction and maintaining an evolving memory report. The authors also present Efficiency-Aware Policy Optimization (EAPO), which encourages efficient exploration and supports stable training through adaptive downsampling. Experimental results show that IterResearch significantly outperforms current agents, achieving better interaction scaling and improved performance on long-horizon tasks.'}, 'zh': {'title': 'è¿­ä»£æ·±åº¦ç ”ç©¶ï¼šé•¿æ—¶é—´æ¨ç†çš„æ–°èŒƒå¼', 'desc': 'IterResearchæ˜¯ä¸€ç§æ–°çš„è¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œé€šè¿‡å°†é•¿æ—¶é—´æ¨ç†é‡æ–°æ„å»ºä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å®ƒé‡‡ç”¨æˆ˜ç•¥å·¥ä½œç©ºé—´é‡å»ºçš„æ–¹æ³•ï¼Œä¿æŒä¸€ä¸ªä¸æ–­æ¼”å˜çš„æŠ¥å‘Šä½œä¸ºè®°å¿†ï¼Œå¹¶å®šæœŸåˆæˆè§è§£ï¼Œä»è€Œåœ¨ä»»æ„æ¢ç´¢æ·±åº¦ä¸Šä¿æŒä¸€è‡´çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ•ˆç‡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆEAPOï¼‰æ¡†æ¶é€šè¿‡å‡ ä½•å¥–åŠ±æŠ˜æ‰£æ¿€åŠ±é«˜æ•ˆæ¢ç´¢ï¼Œæ”¯æŒç¨³å®šçš„åˆ†å¸ƒå¼è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIterResearchåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºä»£ç†ï¼Œå±•ç°å‡ºå‰æ‰€æœªæœ‰çš„äº¤äº’æ‰©å±•èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06307', 'title': 'DRIVE: Data Curation Best Practices for Reinforcement Learning with\n  Verifiable Reward in Competitive Code Generation', 'url': 'https://huggingface.co/papers/2511.06307', 'abstract': 'The study presents a two-stage reinforcement learning approach for competitive-programming code generation, achieving state-of-the-art performance using Group Relative Policy Optimization and a hard-focus curriculum.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform Pre-GRPO: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.', 'score': 50, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '4a5459d116b8bd5e', 'authors': ['Speed Zhu', 'Jianwei Cai', 'Guang Chen', 'Lulu Wu', 'Saiyong Yang', 'Wiggin Zhou'], 'affiliations': ['Hunyuan Team, Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06307.jpg', 'data': {'categories': ['#optimization', '#plp', '#data', '#training', '#leakage', '#rl', '#reasoning'], 'emoji': 'ğŸ’»', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Group Relative Policy Optimization (GRPO) Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ GRPO Ñ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… LeetCode Ğ¸ Codeforces, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° DeepSeek v3.1.'}, 'en': {'title': 'Revolutionizing Code Generation with Two-Stage Reinforcement Learning', 'desc': "This paper introduces a two-stage reinforcement learning (RL) method specifically designed for generating code in competitive programming. It utilizes Group Relative Policy Optimization (GRPO) to enhance the model's performance by focusing on both a broad range of problems and a curated set of challenging tasks. The approach begins with supervised fine-tuning using strong existing models and then transitions to RL with a focus on executable rewards from test cases. The results demonstrate that this method achieves state-of-the-art performance, outperforming similar models and providing insights into effective data curation and training strategies in RL for code generation."}, 'zh': {'title': 'ç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆï¼Œé‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œä¸¥æ ¼çš„è¯¾ç¨‹è®¾è®¡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•æ„å»ºå¼ºåŒ–å­¦ä¹ è§†è§‰è¯†åˆ«æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥åœ¨ç«äº‰ç¼–ç¨‹ä»£ç ç”Ÿæˆä¸­è·å¾—å¼ºå¤§çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒä»å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¸­æå–çŸ¥è¯†ï¼Œç„¶ååœ¨å¤§è§„æ¨¡çš„ç«äº‰ç¼–ç¨‹é—®é¢˜ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæœ€ååœ¨é«˜è´¨é‡çš„æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šè¿›è¡Œæ›´æ–°ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨LeetCodeå’ŒCodeforcesçš„å‘¨èµ›ä¸­è¯„ä¼°ï¼Œè¡¨ç°ä¼˜äºåŒè§„æ¨¡çš„å…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06309', 'title': 'The Station: An Open-World Environment for AI-Driven Discovery', 'url': 'https://huggingface.co/papers/2511.06309', 'abstract': 'AI agents in the STATION environment achieve state-of-the-art performance across various benchmarks through autonomous scientific discovery and emergent behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.', 'score': 35, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '7be14a71aa38479f', 'authors': ['Stephen Chung', 'Wenyu Du'], 'affiliations': ['DualverseAI', 'University of Cambridge', 'University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06309.jpg', 'data': {'categories': ['#long_context', '#open_source', '#science'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ­Ğ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° STATION â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ»ĞµĞ³, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ñ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ· ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering AI Agents for Autonomous Scientific Discovery in STATION', 'desc': 'The paper presents STATION, a multi-agent environment designed to simulate a scientific ecosystem where AI agents can autonomously conduct research. These agents utilize extended context windows to engage in complex tasks such as reading scientific papers, forming hypotheses, and publishing their findings without centralized control. The results show that these agents achieve state-of-the-art performance across various scientific benchmarks, including advancements in mathematics and computational biology. This work highlights the potential for emergent behavior in AI to foster innovative research methods and represents a significant shift towards autonomous scientific discovery.'}, 'zh': {'title': 'STATIONï¼šè‡ªä¸»ç§‘å­¦å‘ç°çš„æ–°çºªå…ƒ', 'desc': 'STATIONæ˜¯ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªå¾®å‹ç§‘å­¦ç”Ÿæ€ç³»ç»Ÿã€‚åœ¨è¿™ä¸ªç¯å¢ƒä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥è‡ªä¸»è¿›è¡Œç§‘å­¦æ¢ç´¢ï¼ŒåŒ…æ‹¬é˜…è¯»åŒè¡Œçš„è®ºæ–‡ã€æå‡ºå‡è®¾ã€æäº¤ä»£ç ã€è¿›è¡Œåˆ†æå’Œå‘å¸ƒç»“æœã€‚æ™ºèƒ½ä½“ä¹‹é—´æ²¡æœ‰ä¸­å¤®ç³»ç»Ÿåè°ƒï¼Œå®ƒä»¬å¯ä»¥è‡ªç”±é€‰æ‹©è¡ŒåŠ¨å¹¶å‘å±•è‡ªå·±çš„å™äº‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSTATIONä¸­çš„æ™ºèƒ½ä½“åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†AlphaEvolveï¼Œå±•ç¤ºäº†è‡ªä¸»ç§‘å­¦å‘ç°çš„æ–°èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07416', 'title': 'Robot Learning from a Physical World Model', 'url': 'https://huggingface.co/papers/2511.07416', 'abstract': 'PhysWorld integrates video generation and physical world modeling to enable accurate robotic manipulation from visual demonstrations without real robot data.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details.', 'score': 30, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'ad6cc657d01abe15', 'authors': ['Jiageng Mao', 'Sicheng He', 'Hao-Ning Wu', 'Yang You', 'Shuyang Sun', 'Zhicheng Wang', 'Yanan Bao', 'Huizhong Chen', 'Leonidas Guibas', 'Vitor Guizilini', 'Howard Zhou', 'Yue Wang'], 'affiliations': ['DeepMind', 'Google', 'Stanford', 'Toyota Research Institute', 'USC'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07416.jpg', 'data': {'categories': ['#robotics', '#rl', '#3d', '#video'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'PhysWorld â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ object-centric Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½ÑƒĞ»ĞµĞ²Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°.'}, 'en': {'title': 'Transforming Visual Guidance into Accurate Robotic Actions', 'desc': 'PhysWorld is a novel framework that combines video generation with physical world modeling to enhance robotic manipulation. It leverages advanced video generation techniques to create realistic visual demonstrations based on task commands and images. By integrating physical world reconstruction, PhysWorld ensures that the generated video motions correspond to accurate physical actions, using object-centric residual reinforcement learning. This approach allows robots to learn effective manipulation strategies without needing real robot data, achieving better accuracy in diverse tasks.'}, 'zh': {'title': 'PhysWorldï¼šä»è§†é¢‘ç”Ÿæˆåˆ°ç‰©ç†æ‰§è¡Œçš„æœºå™¨äººå­¦ä¹ ', 'desc': 'PhysWorldæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç»“åˆè§†é¢‘ç”Ÿæˆå’Œç‰©ç†ä¸–ç•Œå»ºæ¨¡ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿä»è§†è§‰æ¼”ç¤ºä¸­å­¦ä¹ ã€‚å®ƒåˆ©ç”¨æœ€æ–°çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒä¸­åˆæˆé€¼çœŸçš„è§†è§‰æ¼”ç¤ºï¼Œä¸ºæœºå™¨äººæä¾›å¼ºå¤§çš„è®­ç»ƒä¿¡å·ã€‚é€šè¿‡å°†è§†é¢‘ç”Ÿæˆä¸ç‰©ç†ä¸–ç•Œé‡å»ºç›¸ç»“åˆï¼ŒPhysWorldèƒ½å¤Ÿç”Ÿæˆä¸ä»»åŠ¡ç›¸å…³çš„è§†é¢‘ï¼Œå¹¶å°†è§†é¢‘ä¸­çš„è¿åŠ¨è½¬åŒ–ä¸ºç‰©ç†ä¸Šå‡†ç¡®çš„æœºå™¨äººåŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼ŒPhysWorldåœ¨å¤šç§çœŸå®ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æœºå™¨äººçš„æ“ä½œç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06876', 'title': 'Generating an Image From 1,000 Words: Enhancing Text-to-Image With\n  Structured Captions', 'url': 'https://huggingface.co/papers/2511.06876', 'abstract': 'A text-to-image model trained on long structured captions with DimFusion fusion mechanism and TaBR evaluation protocol achieves state-of-the-art prompt alignment and improved controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image models have rapidly evolved from casual creative tools to professional-grade systems, achieving unprecedented levels of image quality and realism. Yet, most models are trained to map short prompts into detailed images, creating a gap between sparse textual input and rich visual outputs. This mismatch reduces controllability, as models often fill in missing details arbitrarily, biasing toward average user preferences and limiting precision for professional use. We address this limitation by training the first open-source text-to-image model on long structured captions, where every training sample is annotated with the same set of fine-grained attributes. This design maximizes expressive coverage and enables disentangled control over visual factors. To process long captions efficiently, we propose DimFusion, a fusion mechanism that integrates intermediate tokens from a lightweight LLM without increasing token length. We also introduce the Text-as-a-Bottleneck Reconstruction (TaBR) evaluation protocol. By assessing how well real images can be reconstructed through a captioning-generation loop, TaBR directly measures controllability and expressiveness, even for very long captions where existing evaluation methods fail. Finally, we demonstrate our contributions by training the large-scale model FIBO, achieving state-of-the-art prompt alignment among open-source models. Model weights are publicly available at https://huggingface.co/briaai/FIBO', 'score': 26, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'f7ff852adc4d5527', 'authors': ['Eyal Gutflaish', 'Eliran Kachlon', 'Hezi Zisman', 'Tal Hacham', 'Nimrod Sarid', 'Alexander Visheratin', 'Saar Huberman', 'Gal Davidi', 'Guy Bukchin', 'Kfir Goldberg', 'Ron Mokady'], 'affiliations': ['Bria AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06876.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#architecture', '#cv', '#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑÑ… Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ DimFusion, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ’Ğ²ĞµĞ´Ñ‘Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ TaBR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ FIBO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Long Captions for Precise Image Generation', 'desc': "This paper presents a novel text-to-image model that utilizes long structured captions to enhance image generation quality and controllability. By employing the DimFusion fusion mechanism, the model efficiently processes these lengthy captions without increasing token length, allowing for better alignment between text prompts and generated images. The introduction of the TaBR evaluation protocol enables a more accurate assessment of the model's performance in reconstructing images from captions, particularly for complex and detailed inputs. The resulting model, FIBO, achieves state-of-the-art results in prompt alignment, making it a significant advancement in the field of AI-generated imagery."}, 'zh': {'title': 'é•¿ç»“æ„åŒ–æ ‡é¢˜æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯æ§æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œä½¿ç”¨é•¿ç»“æ„åŒ–æ ‡é¢˜è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨DimFusionèåˆæœºåˆ¶å’ŒTaBRè¯„ä¼°åè®®ã€‚è¯¥æ¨¡å‹è§£å†³äº†çŸ­æç¤ºä¸è¯¦ç»†å›¾åƒä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†å¯æ§æ€§å’Œè¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œç»†ç²’åº¦å±æ€§æ ‡æ³¨ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ§åˆ¶è§†è§‰å› ç´ ã€‚æœ€ç»ˆï¼Œè®­ç»ƒçš„å¤§è§„æ¨¡æ¨¡å‹FIBOåœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æç¤ºå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07419', 'title': 'Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs', 'url': 'https://huggingface.co/papers/2511.07419', 'abstract': 'Aligning routing weights with task embeddings in Sparse Mixture-of-Experts (MoE) models improves generalization and reduces performance gaps in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs\' generalization performance. Our method, "Routing Manifold Alignment (RoMA)", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.', 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '0ddf3df037bc9395', 'authors': ['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Johns Hopkins University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07419.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ RoMA (Routing Manifold Alignment) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ»ĞµĞ½ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ‡ĞµÑ€Ğ½ĞµĞ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10-20%.'}, 'en': {'title': 'Aligning Routing for Better Generalization in MoE Models', 'desc': 'This paper presents a method called Routing Manifold Alignment (RoMA) to enhance Sparse Mixture-of-Experts (MoE) models in large language tasks. The authors identify that existing routing mechanisms in MoE models often lead to significant performance gaps, which can be as high as 20% in accuracy. RoMA aligns routing weights with task embeddings, allowing the model to better generalize by ensuring that similar tasks share expert choices. The approach requires only lightweight finetuning of the routers, leading to improved performance across various benchmarks without the need to retrain the entire model.'}, 'zh': {'title': 'ä¼˜åŒ–è·¯ç”±æƒé‡ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›', 'desc': 'åœ¨ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ä¸­ï¼Œå°†è·¯ç”±æƒé‡ä¸ä»»åŠ¡åµŒå…¥å¯¹é½å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚ç°æœ‰çš„MoEæ¨¡å‹åœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè·¯ç”±å™¨çš„æ¬¡ä¼˜æ€§ï¼Œå¯¼è‡´å‡†ç¡®ç‡ä¸Šå­˜åœ¨10-20%çš„æ˜¾è‘—å·®è·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè·¯ç”±æµå½¢å¯¹é½ï¼ˆRoMAï¼‰â€çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨åè®­ç»ƒç›®æ ‡ä¸­å¼•å…¥é¢å¤–çš„æµå½¢æ­£åˆ™åŒ–é¡¹ï¼Œä»…éœ€å¯¹è·¯ç”±å™¨è¿›è¡Œè½»é‡çº§å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoMAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº†ä»»åŠ¡ç†è§£ä¸è§£å†³æ–¹æ¡ˆç”Ÿæˆçš„ç»Ÿä¸€æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07070', 'title': 'RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social\n  Networking Services', 'url': 'https://huggingface.co/papers/2511.07070', 'abstract': "RedOne 2.0, a social networking service-oriented LLM, uses a progressive, RL-prioritized post-training paradigm to achieve rapid and stable adaptation, delivering improvements over larger baselines with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.", 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'a2e8e21aa7d3d87d', 'authors': ['Fei Zhao', 'Chonggang Lu', 'Haofu Qian', 'Fangcheng Shi', 'Zijie Meng', 'Jianzhao Huang', 'Xu Tang', 'Zheyong Xie', 'Zheyu Ye', 'Zhe Xu', 'Yao Hu', 'Shaosheng Cao'], 'affiliations': ['NLP Team, Xiaohongshu Inc. Huangpu District, Shanghai, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07070.jpg', 'data': {'categories': ['#rl', '#multilingual', '#small_models', '#training'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'RedOne 2.0 â€” ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºÑƒ Ñ Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ 4-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ĞºĞ°Ñ‡ĞµĞ»Ğ¸Â» Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'RedOne 2.0: Efficient Adaptation for Social Networking LLMs', 'desc': 'RedOne 2.0 is a large language model (LLM) specifically designed for social networking services (SNS), addressing unique challenges such as diverse languages and rapidly changing social norms. It employs a progressive, reinforcement learning (RL)-prioritized post-training approach that allows for quick and stable adaptation, outperforming larger models while using less data. The training process consists of three stages: exploratory learning to identify weaknesses, targeted fine-tuning to address those gaps, and refinement learning to enhance performance using SNS-specific signals. This model demonstrates significant improvements in efficiency and robustness, making it a strong candidate for domain-specific applications in social media contexts.'}, 'zh': {'title': 'RedOne 2.0ï¼šç¤¾äº¤ç½‘ç»œæœåŠ¡çš„é«˜æ•ˆè¯­è¨€æ¨¡å‹', 'desc': 'RedOne 2.0 æ˜¯ä¸€ç§é¢å‘ç¤¾äº¤ç½‘ç»œæœåŠ¡çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ¸è¿›å¼çš„å¼ºåŒ–å­¦ä¹ ä¼˜å…ˆåè®­ç»ƒæ–¹æ³•ï¼Œä»¥å®ç°å¿«é€Ÿå’Œç¨³å®šçš„é€‚åº”ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆåœ¨ç²¾é€‰çš„ç¤¾äº¤ç½‘ç»œè¯­æ–™ä¸Šè¿›è¡Œæ¢ç´¢æ€§å­¦ä¹ ï¼Œç„¶åé’ˆå¯¹è¯†åˆ«å‡ºçš„å¼±ç‚¹è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œæœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ å·©å›ºæ”¹è¿›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒRedOne 2.0 åœ¨æ•°æ®æ•ˆç‡å’Œç¨³å®šæ€§ä¸Šè¡¨ç°æ›´ä½³ï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šå®ç°æ›´å¤§çš„æ€§èƒ½æå‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒRedOne 2.0 ä¸ºç¤¾äº¤ç½‘ç»œåœºæ™¯ä¸­çš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªå…·æœ‰ç«äº‰åŠ›çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07250', 'title': 'MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2511.07250', 'abstract': "MVU-Eval is a comprehensive benchmark for evaluating multi-video understanding in Multimodal Large Language Models, addressing gaps in existing single-video benchmarks and highlighting performance discrepancies in real-world applications.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.", 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '581a9ddc3ec5d36a', 'authors': ['Tianhao Peng', 'Haochen Wang', 'Yuanxing Zhang', 'Zekun Wang', 'Zili Wang', 'Gavin Chang', 'Jian Yang', 'Shihao Li', 'Yanghai Wang', 'Xintao Wang', 'Houyi Li', 'Wei Ji', 'Pengfei Wan', 'Steven Huang', 'Zhaoxiang Zhang', 'Jiaheng Liu'], 'affiliations': ['CASIA', 'Kuaishou Technology', 'M-A-P', 'Nanjing University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07250.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#multimodal', '#dataset', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MVU-Eval â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 1824 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 4959 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ².'}, 'en': {'title': 'MVU-Eval: Bridging the Gap in Multi-Video Understanding for MLLMs', 'desc': 'MVU-Eval is a new benchmark designed to evaluate how well Multimodal Large Language Models (MLLMs) understand multiple videos, which is important for real-world tasks like sports analysis and self-driving cars. Unlike previous benchmarks that only focused on single videos, MVU-Eval includes a wide range of scenarios with 1,824 question-answer pairs from nearly 5,000 videos. It tests eight key skills, including basic perception and complex reasoning, to see how well these models can handle multi-video information. The findings show that current MLLMs have significant gaps in their performance when it comes to understanding multiple videos, highlighting the need for improvement in this area.'}, 'zh': {'title': 'å¤šè§†é¢‘ç†è§£çš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'MVU-Evalæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šè§†é¢‘ç†è§£åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„å•è§†é¢‘åŸºå‡†æ— æ³•æ»¡è¶³ç°å®åº”ç”¨ä¸­å¯¹å¤šè§†é¢‘ç†è§£çš„éœ€æ±‚ï¼Œä¾‹å¦‚ä½“è‚²åˆ†æå’Œè‡ªåŠ¨é©¾é©¶ã€‚MVU-Evalé€šè¿‡1824ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œè¯„ä¼°äº†æ¥è‡ª4959ä¸ªè§†é¢‘çš„å…«é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œæ¶µç›–åŸºæœ¬æ„ŸçŸ¥ä»»åŠ¡å’Œé«˜é˜¶æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè§†é¢‘ç†è§£æ–¹é¢çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06209', 'title': 'Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps\n  via Uncertainty Heads', 'url': 'https://huggingface.co/papers/2511.06209', 'abstract': 'Transformer-based uncertainty quantification heads improve step-level reasoning verification in LLMs by estimating uncertainty from internal states, offering a lightweight and scalable alternative to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '1df27bb09e636af9', 'authors': ['Jingwei Ni', 'Ekaterina Fadeeva', 'Tianyi Wu', 'Mubashara Akhtar', 'Jiaheng Zhang', 'Elliott Ash', 'Markus Leippold', 'Timothy Baldwin', 'See-Kiong Ng', 'Artem Shelmanov', 'Mrinmaya Sachan'], 'affiliations': ['ETH Zurich', 'MBZUAI', 'National University of Singapore', 'The University of Melbourne', 'University of Zurich'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06209.jpg', 'data': {'categories': ['#small_models', '#optimization', '#interpretability', '#architecture', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ½ĞµÑƒĞ²ĞµÑ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (UHeads). Ğ­Ñ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¼ĞµĞ½ĞµĞµ 10 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ LLM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¦ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ»Ğ¸Ğ±Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UHeads Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (PRM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Lightweight Uncertainty Heads for Efficient Reasoning Verification in LLMs', 'desc': "This paper introduces a new method for verifying the reasoning steps of large language models (LLMs) using transformer-based uncertainty quantification heads (UHeads). These UHeads estimate the uncertainty of reasoning steps by analyzing the internal states of a frozen LLM, providing a lightweight and efficient alternative to traditional verification methods like Process Reward Models (PRMs). The approach is fully automatic, generating target labels through either a larger LLM or self-supervised techniques, and it requires significantly fewer parameters than existing models. The results demonstrate that UHeads can effectively match or exceed the performance of much larger PRMs across various domains, indicating that LLMs' internal states can be valuable for assessing reasoning accuracy."}, 'zh': {'title': 'è½»é‡çº§ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæå‡æ¨ç†éªŒè¯æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–å¤´ï¼ˆUHeadsï¼‰ï¼Œç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€æ­¥æ¨ç†éªŒè¯ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åˆ©ç”¨LLMçš„å†…éƒ¨çŠ¶æ€ï¼ŒUHeadsèƒ½å¤Ÿè‡ªåŠ¨ä¼°è®¡æ¨ç†æ­¥éª¤çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæä¾›äº†ä¸€ç§è½»é‡çº§ä¸”å¯æ‰©å±•çš„éªŒè¯æ–¹æ³•ã€‚ä¸ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ç›¸æ¯”ï¼ŒUHeadsåœ¨å¤šä¸ªé¢†åŸŸçš„è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ï¼Œä¸”å‚æ•°é‡å°‘äº1000ä¸‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMçš„å†…éƒ¨çŠ¶æ€èƒ½å¤Ÿæœ‰æ•ˆç¼–ç ä¸ç¡®å®šæ€§ï¼Œä¸ºæ¨ç†éªŒè¯æä¾›å¯é ä¿¡å·ï¼Œæ¨åŠ¨å¯æ‰©å±•å’Œé€šç”¨çš„è‡ªçœå‹LLMçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07384', 'title': 'Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence', 'url': 'https://huggingface.co/papers/2511.07384', 'abstract': 'Converting pretrained non-recurrent language models to depth-recurrent models improves performance at a given compute budget using a curriculum of recurrences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '5f882be3bb030bd3', 'authors': ['Sean McLeish', 'Ang Li', 'John Kirchenbauer', 'Dayal Singh Kalra', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Avi Schwarzschild', 'Jonas Geiping', 'Tom Goldstein', 'Micah Goldblum'], 'affiliations': ['Columbia University', 'ELLIS Institute TÃ¼bingen', 'Lawrence Livermore National Laboratory', 'Max Planck Institute for Intelligent Systems', 'New York University', 'TÃ¼bingen AI Center', 'University of Maryland', 'University of North Carolina'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07384.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (curriculum learning), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ½ĞµÑ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Language Models: From Non-Recurrent to Depth-Recurrent for Better Efficiency', 'desc': "This paper explores the transformation of pretrained non-recurrent language models into depth-recurrent models to enhance their performance while managing computational resources. The authors introduce a curriculum of recurrences that gradually increases the model's effective depth during training, which helps maintain performance levels. Their findings indicate that this approach allows for better utilization of compute budgets compared to merely post-training the original non-recurrent models. The experiments conducted, particularly in the domain of mathematics, demonstrate that the converted recurrent models outperform their non-recurrent counterparts under the same computational constraints."}, 'zh': {'title': 'å°†éé€’å½’æ¨¡å‹è½¬ä¸ºæ·±åº¦é€’å½’æ¨¡å‹ï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†é¢„è®­ç»ƒçš„éé€’å½’è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ·±åº¦é€’å½’æ¨¡å‹ã€‚é€šè¿‡ä½¿ç”¨é€’å½’çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé€æ­¥å¢åŠ æ¨¡å‹çš„æœ‰æ•ˆæ·±åº¦ï¼Œå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æ€»è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸ºé€’å½’æ¨¡å‹åœ¨ç»™å®šçš„è®¡ç®—é¢„ç®—ä¸‹è¡¨ç°æ›´å¥½ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆåœ°è§£è€¦äº†è®­ç»ƒæ—¶çš„è®¡ç®—ä¸æµ‹è¯•æ—¶çš„è®¡ç®—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06411', 'title': 'SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via\n  Gumbel-Reparameterized Soft-Thinking Policy Optimization', 'url': 'https://huggingface.co/papers/2511.06411', 'abstract': 'A novel policy optimization algorithm, SofT-GRPO, enhances soft-thinking in Large Language Models by integrating Gumbel noise and the Gumbel-Softmax technique, leading to improved performance over discrete-token methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master', 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '15e102fa44767987', 'authors': ['Zhi Zheng', 'Wee Sun Lee'], 'affiliations': ['School of Computing, National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06411.jpg', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑĞ³ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· ÑˆÑƒĞ¼ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ SofT-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Gumbel-Softmax. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ reinforcement learning Ğº Ğ¼ÑĞ³ĞºĞ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ GRPO. SofT-GRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€ÑĞº Ñ€ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ¼ÑĞ³ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Soft-Thinking Potential in LLMs with SofT-GRPO', 'desc': 'This paper introduces SofT-GRPO, a new policy optimization algorithm designed to enhance soft-thinking in Large Language Models (LLMs). By integrating Gumbel noise and the Gumbel-Softmax technique, SofT-GRPO improves the performance of LLMs compared to traditional discrete-token methods. The algorithm addresses the challenges of incorporating stochasticity into soft-thinking tokens and effectively updates soft-thinking policies. Experimental results show that SofT-GRPO achieves better accuracy on various tasks, demonstrating its potential to advance soft-thinking reasoning in LLMs.'}, 'zh': {'title': 'SofT-GRPOï¼šæå‡è½¯æ€ç»´çš„ç­–ç•¥ä¼˜åŒ–æ–°ç®—æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•SofT-GRPOï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è½¯æ€ç»´èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Gumbelå™ªå£°å’ŒGumbel-SoftmaxæŠ€æœ¯ï¼ŒSofT-GRPOåœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„ç¦»æ•£æ ‡è®°æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè½¯æ€ç»´æ¨ç†åœ¨æŸäº›åœºæ™¯ä¸‹èƒ½å¤Ÿè¶…è¶Šå¸¸è§„çš„ç¦»æ•£æ ‡è®°æ¨ç†ï¼Œæ˜¾ç¤ºå‡ºå…¶ç ”ç©¶å’Œåº”ç”¨çš„ä»·å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSofT-GRPOä½¿å¾—è½¯æ€ç»´æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šç•¥å¾®è¶…è¶Šç¦»æ•£æ ‡è®°çš„GRPOï¼Œå°¤å…¶åœ¨Pass@32ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07317', 'title': 'RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments', 'url': 'https://huggingface.co/papers/2511.07317', 'abstract': "Reinforcement Learning with Adaptive Verifiable Environments (RLVE) improves language model reasoning by dynamically adjusting problem difficulty, outperforming static environments and traditional RL training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.", 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'f0d01635ac23233d', 'authors': ['Zhiyuan Zeng', 'Hamish Ivison', 'Yiping Wang', 'Lifan Yuan', 'Shuyue Stella Li', 'Zhuorui Ye', 'Siting Li', 'Jacqueline He', 'Runlong Zhou', 'Tong Chen', 'Chenyang Zhao', 'Yulia Tsvetkov', 'Simon Shaolei Du', 'Natasha Jaques', 'Hao Peng', 'Pang Wei Koh', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Tsinghua University', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07317.jpg', 'data': {'categories': ['#benchmark', '#small_models', '#open_source', '#optimization', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforcement Learning Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ RLVE-Gym â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 400 Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²ÑĞµÑ… 400 Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.37% Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ RL.'}, 'en': {'title': 'Dynamic Difficulty for Smarter Language Models', 'desc': "Reinforcement Learning with Adaptive Verifiable Environments (RLVE) enhances the reasoning abilities of language models by adjusting the difficulty of problems during training. This method uses verifiable environments that generate problems and provide rewards that can be confirmed algorithmically, allowing for scalable reinforcement learning. Unlike traditional static environments, RLVE adapts to the model's learning progress, preventing issues with problems being too easy or too hard. The implementation of RLVE-Gym, a suite of 400 environments, demonstrates significant improvements in reasoning capabilities, achieving a 3.37% average gain across benchmarks compared to traditional methods."}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´éš¾åº¦ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸è‡ªé€‚åº”å¯éªŒè¯ç¯å¢ƒï¼ˆRLVEï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦æ¥æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¶…è¶Šäº†é™æ€ç¯å¢ƒå’Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚RLVEä½¿ç”¨å¯éªŒè¯çš„ç¯å¢ƒï¼Œç¨‹åºæ€§ç”Ÿæˆé—®é¢˜å¹¶æä¾›ç®—æ³•å¯éªŒè¯çš„å¥–åŠ±ï¼Œä»è€Œæ‰©å±•äº†è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€‚ä¸é™æ€æ•°æ®åˆ†å¸ƒç›¸æ¯”ï¼ŒRLVEèƒ½å¤Ÿæ ¹æ®ç­–ç•¥æ¨¡å‹çš„èƒ½åŠ›åŠ¨æ€è°ƒæ•´é—®é¢˜éš¾åº¦åˆ†å¸ƒï¼Œé¿å…äº†å­¦ä¹ ä¿¡å·æ¶ˆå¤±çš„é—®é¢˜ã€‚é€šè¿‡RLVE-Gymï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¯å¢ƒæ‰©å±•èƒ½å¤ŸæŒç»­æ”¹å–„å¯æ¨å¹¿çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07025', 'title': 'Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks', 'url': 'https://huggingface.co/papers/2511.07025', 'abstract': 'A fully open-source text embedding model achieves state-of-the-art performance across embedding tasks, particularly in multilingual scenarios, through a novel data mix and detailed ablation studies.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '0eba4b504cf8021b', 'authors': ['Yauhen Babakhin', 'Radek Osmulski', 'Ronay Ak', 'Gabriel Moreira', 'Mengyao Xu', 'Benedikt Schifferer', 'Bo Liu', 'Even Oldridge'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07025.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#low_resource', '#data', '#training', '#dataset', '#multilingual', '#synthetic'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² llama-embed-nemotron-8b, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMTEB Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾ÑÑ‚Ğ°Ğ²Ñƒ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 16.1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸, ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Multilingual Mastery with Open-Source Embeddings', 'desc': "The paper presents llama-embed-nemotron-8b, a fully open-source text embedding model that sets new records in performance on the Multilingual Massive Text Embedding Benchmark (MMTEB). It utilizes a unique data mix of 16.1 million query-document pairs, combining public datasets and synthetically generated examples to enhance its multilingual capabilities. The authors conduct detailed ablation studies to evaluate various design choices, including loss functions and data generation strategies, which contribute to the model's effectiveness. This model not only excels in traditional embedding tasks like retrieval and classification but also adapts to user-defined instructions, making it a versatile tool for diverse applications."}, 'zh': {'title': 'å¼€æºæ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œæ€§èƒ½å“è¶Šï¼', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å®Œå…¨å¼€æºçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ llama-embed-nemotron-8bï¼Œè¯¥æ¨¡å‹åœ¨å¤šè¯­è¨€å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡åˆ›æ–°çš„æ•°æ®æ··åˆæ–¹æ³•å’Œè¯¦ç»†çš„æ¶ˆèç ”ç©¶ï¼Œè¯¥æ¨¡å‹åœ¨æ£€ç´¢ã€åˆ†ç±»å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ç­‰ä¸»è¦åµŒå…¥ä»»åŠ¡ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹çš„æƒé‡å’Œè®­ç»ƒæ•°æ®é›†ï¼Œç¡®ä¿é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚è¯¥æ¨¡å‹æ”¯æŒç”¨æˆ·å®šä¹‰çš„æŒ‡ä»¤ï¼Œå¢å¼ºäº†åœ¨ç‰¹å®šç”¨ä¾‹ä¸­çš„æ€§èƒ½ï¼Œæˆä¸ºä¸€ç§é€šç”¨çš„æ–‡æœ¬åµŒå…¥è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06449', 'title': 'FLEX: Continuous Agent Evolution via Forward Learning from Experience', 'url': 'https://huggingface.co/papers/2511.06449', 'abstract': 'FLEX, a gradient-free learning paradigm, enables Large Language Model agents to continuously evolve through experience, improving performance in tasks like mathematical reasoning, chemical retrosynthesis, and protein fitness prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'c9a6f0c59dafe929', 'authors': ['Zhicheng Cai', 'Xinyuan Guo', 'Yu Pei', 'Jiangtao Feng', 'Jinsong Su', 'Jiangjie Chen', 'Ya-Qin Zhang', 'Wei-Ying Ma', 'Mingxuan Wang', 'Hao Zhou'], 'affiliations': ['ByteDance Seed', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed', 'School of Informatics, Xiamen University', 'Shanghai Artificial Intelligence Laboratory', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06449.jpg', 'data': {'categories': ['#science', '#benchmark', '#optimization', '#agents', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° FLEX (Forward Learning with EXperience) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ±ĞµĞ»ĞºĞ¾Ğ² (Ğ´Ğ¾ 23% Ğ½Ğ° AIME25, 10% Ğ½Ğ° USPTO50k Ğ¸ 14% Ğ½Ğ° ProteinGym). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM.'}, 'en': {'title': 'Empowering AI Agents to Learn and Evolve Continuously with FLEX', 'desc': 'FLEX is a new learning method that allows Large Language Model (LLM) agents to improve their skills over time without needing traditional gradient-based training. This approach focuses on building a library of experiences, where agents learn from their successes and failures during interactions. By using FLEX, these agents can enhance their performance in complex tasks such as mathematical reasoning and chemical synthesis. The research shows that this method leads to significant performance gains and establishes a framework for continuous learning and evolution in AI agents.'}, 'zh': {'title': 'FLEXï¼šè®©æ™ºèƒ½ä½“é€šè¿‡ç»éªŒä¸æ–­è¿›åŒ–', 'desc': 'FLEXæ˜¯ä¸€ç§æ— æ¢¯åº¦å­¦ä¹ èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†èƒ½å¤Ÿé€šè¿‡ç»éªŒä¸æ–­è¿›åŒ–ã€‚ä¸ä¼ ç»Ÿçš„é™æ€è®­ç»ƒä¸åŒï¼ŒFLEXé€šè¿‡æ„å»ºç»“æ„åŒ–çš„ç»éªŒåº“ï¼Œä¿ƒè¿›ä»£ç†åœ¨ä¸ç¯å¢ƒäº’åŠ¨ä¸­åæ€æˆåŠŸä¸å¤±è´¥ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œå¯ç»§æ‰¿çš„è¿›åŒ–ã€‚è¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†ã€åŒ–å­¦é€†åˆæˆå’Œè›‹ç™½è´¨é€‚åº”æ€§é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚FLEXè¿˜æ­ç¤ºäº†ç»éªŒå¢é•¿çš„æ˜ç¡®è§„æ¨¡æ³•åˆ™å’Œä»£ç†ä¹‹é—´çš„ç»éªŒç»§æ‰¿ç°è±¡ï¼Œæ ‡å¿—ç€å‘å¯æ‰©å±•å’Œå¯ç»§æ‰¿çš„è¿ç»­ä»£ç†è¿›åŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06194', 'title': 'NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS\n  Modeling', 'url': 'https://huggingface.co/papers/2511.06194', 'abstract': 'NURBGen generates high-fidelity 3D CAD models from text using Non-Uniform Rational B-Splines, outperforming existing methods in geometric fidelity and dimensional accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (i.e, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'eae8f3eb93752e5b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#3d', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸: NURBS-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° NURBGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ B-ÑĞ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ (NURBS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² JSON-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² NURBS-Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ½ĞµĞ¿Ğ¾Ğ´Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ NURBS Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ¾Ğ±ÑƒÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ´Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹.'}, 'en': {'title': 'Transforming Text to Precise 3D CAD Models with NURBGen', 'desc': 'NURBGen is a novel framework that generates high-quality 3D CAD models from text descriptions using Non-Uniform Rational B-Splines (NURBS). It fine-tunes a large language model to convert natural language into JSON representations that specify NURBS parameters, enabling direct conversion to BRep format. The framework also introduces a hybrid representation that effectively combines NURBS with analytic primitives to improve handling of complex surfaces. NURBGen outperforms existing text-to-CAD systems in terms of geometric fidelity and dimensional accuracy, as validated by expert assessments.'}, 'zh': {'title': 'ä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸ 3D CAD æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'NURBGen æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå¯ä»¥ä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸåº¦çš„ 3D CAD æ¨¡å‹ï¼Œä½¿ç”¨éå‡åŒ€æœ‰ç† B æ ·æ¡ï¼ˆNURBSï¼‰æŠ€æœ¯ã€‚ä¸ç°æœ‰çš„æ–‡æœ¬åˆ° CAD ç³»ç»Ÿä¸åŒï¼ŒNURBGen ç›´æ¥å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºåŒ…å« NURBS è¡¨é¢å‚æ•°çš„ JSON è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ¥å®ç°ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯ç¼–è¾‘çš„ CAD æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡ ä½•ä¿çœŸåº¦å’Œå°ºå¯¸å‡†ç¡®æ€§ã€‚NURBGen åœ¨å¤šç§æç¤ºä¸‹è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.05933', 'title': 'Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs', 'url': 'https://huggingface.co/papers/2511.05933', 'abstract': 'Reinforcement learning enhances language models\' ability to recall hierarchical knowledge without degrading memorized facts, as evidenced by improved performance on structured prompting and deep-retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '056ea072f9097921', 'authors': ['Renfei Zhang', 'Manasa Kaniselvan', 'Niloofar Mireshghallah'], 'affiliations': ['ETH Zurich', 'FAIR at Meta', 'Simon Fraser University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05933.jpg', 'data': {'categories': ['#rl', '#interpretability', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ÑĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL-ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸ supervised fine-tuned Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğµ Ğ¸Ğ·-Ğ·Ğ° ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RL Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Reinforcement Learning: Enhancing Knowledge Recall in Language Models', 'desc': "This paper explores how reinforcement learning (RL) can improve language models' ability to recall structured knowledge without losing their memorized facts. The authors demonstrate that RL-enhanced models outperform traditional supervised fine-tuned models on tasks that require navigating hierarchical information, such as medical coding. They suggest that the improvements come from better procedural skills in searching through existing knowledge rather than acquiring new data. Additionally, their analysis shows that while the factual knowledge remains similar between models, the way queries are processed changes significantly with RL, enhancing the model's retrieval capabilities."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›å¿†èƒ½åŠ›', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è¯­è¨€æ¨¡å‹åœ¨å›å¿†å±‚æ¬¡çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œä¸ä¼šé™ä½å·²è®°å¿†äº‹å®çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¼ºåŒ–å­¦ä¹ çš„æ¨¡å‹åœ¨çŸ¥è¯†å›å¿†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹å’Œç›‘ç£å¾®è°ƒæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦éå†å±‚æ¬¡ç»“æ„çŸ¥è¯†çš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›æå‡å¹¶éæºäºæ–°æ•°æ®çš„è·å–ï¼Œè€Œæ˜¯æ¨¡å‹åœ¨å¯¼èˆªå’Œæœç´¢ç°æœ‰çŸ¥è¯†å±‚æ¬¡æ–¹é¢çš„æŠ€èƒ½æé«˜ã€‚é€šè¿‡ç»“æ„åŒ–æç¤ºï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥æ˜¾è‘—ç¼©å°æ€§èƒ½å·®è·ï¼Œè¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ ä¸»è¦æ”¹å˜äº†æ¨¡å‹éå†çŸ¥è¯†çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯çŸ¥è¯†æœ¬èº«çš„è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04285', 'title': 'RLoop: An Self-Improving Framework for Reinforcement Learning with\n  Iterative Policy Initialization', 'url': 'https://huggingface.co/papers/2511.04285', 'abstract': 'RLoop, a self-improving framework using iterative policy initialization and Rejection-sampling Fine-Tuning, mitigates overfitting and enhances generalization in Reinforcement Learning for Verifiable Rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '2933ca6bff910550', 'authors': ['Zeng Zhiyuan', 'Jiashuo Liu', 'Zhangyue Yin', 'Ge Zhang', 'Wenhao Huang', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'M-A-P', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04285.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¦Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'desc': 'RLoop â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ„Ğ°Ğ·Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· RL Ğ¸ Ñ„Ğ°Ğ·Ñ‹ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Rejection-sampling Fine-Tuning Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 9% Ğ¸ pass@32 Ğ½Ğ° 15% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'RLoop: Enhancing Reinforcement Learning Through Iterative Policy Improvement', 'desc': 'RLoop is a novel framework designed to improve Reinforcement Learning (RL) by addressing the issue of overfitting, which occurs when models perform well on training data but poorly on new data. It does this by implementing iterative policy initialization and Rejection-sampling Fine-Tuning (RFT), which helps maintain a diverse set of policies during training. The framework creates a cycle where successful training trajectories are used to refine the policy, leading to better generalization and performance. Experiments demonstrate that RLoop significantly enhances accuracy and reduces the risk of forgetting previously learned solutions.'}, 'zh': {'title': 'RLoopï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„è‡ªæˆ‘æ”¹è¿›ä¸æ³›åŒ–æå‡', 'desc': 'RLoopæ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£ç­–ç•¥åˆå§‹åŒ–å’Œæ‹’ç»é‡‡æ ·å¾®è°ƒæ¥å‡è½»å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´å¹¶è¿‡æ»¤æˆåŠŸçš„è½¨è¿¹ï¼Œåˆ›å»ºä¸€ä¸ªä¸“å®¶æ•°æ®é›†ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚RLoopå°†æ ‡å‡†è®­ç»ƒè¿‡ç¨‹è½¬å˜ä¸ºä¸€ä¸ªè‰¯æ€§å¾ªç¯ï¼Œä½¿å¾—æ¯æ¬¡è¿­ä»£éƒ½èƒ½åœ¨åˆå§‹ç­–ç•¥çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLoopæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé€šè¿‡ç‡ï¼Œå‡å°‘äº†é—å¿˜ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05705', 'title': 'Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale', 'url': 'https://huggingface.co/papers/2511.05705', 'abstract': "A new reasoning data generation framework creates a large-scale vision-centric dataset with over 1M synthetic questions, enhancing performance across various benchmarks and improving cross-modality transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'afe423cea99de0e2', 'authors': ['David Acuna', 'Chao-Han Huck Yang', 'Yuntian Deng', 'Jaehun Jung', 'Ximing Lu', 'Prithviraj Ammanabrolu', 'Hyunwoo Kim', 'Yuan-Hong Liao', 'Yejin Choi'], 'affiliations': ['NVIDIA', 'UCSD', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05705.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#cv', '#multimodal', '#training', '#rlhf', '#dataset', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1 Ğ¼Ğ»Ğ½ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñƒ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Vision Language Models Ğ¸ reasoning LLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ (Chain-of-Thought). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing Reasoning with 1M Synthetic Vision Questions', 'desc': 'This paper presents a new framework for generating reasoning data, creating a large-scale dataset with over 1 million synthetic vision-centric questions. The dataset is designed to enhance performance in multimodal reasoning tasks, particularly those that extend beyond simple visual math. The authors demonstrate that fine-tuning a model called Qwen2.5-VL-7B on this dataset outperforms existing open-data and even some closed-data models across various benchmarks. Additionally, the dataset shows positive transfer effects to text-only and audio reasoning tasks, indicating its broad applicability in machine learning.'}, 'zh': {'title': 'åˆ›æ–°æ¨ç†æ•°æ®ç”Ÿæˆï¼Œæå‡è·¨æ¨¡æ€æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡100ä¸‡åˆæˆè§†è§‰ä¸­å¿ƒé—®é¢˜çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸ä»…å¢å¼ºäº†å¤šç§åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œè¿˜æ”¹å–„äº†è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„åˆæˆè¿‡ç¨‹ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œæ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œç”Ÿæˆäº†ä¸°å¯Œçš„æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºè¯¥æ•°æ®é›†å¾®è°ƒçš„æ¨¡å‹åœ¨è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰å¼€æ”¾æ•°æ®åŸºçº¿ï¼Œç”šè‡³è¶…è¿‡äº†ä¸€äº›å¼ºå¤§çš„é—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03317', 'title': 'Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2511.03317', 'abstract': "Diffusion-SDPO improves text-to-image generation quality by adaptively scaling the loser gradient in preference optimization, ensuring the preferred output's error does not increase.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '37fbb5e8cc8e00b0', 'authors': ['Minghao Fu', 'Guo-Hua Wang', 'Tianyu Cui', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce Group', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03317.jpg', 'data': {'categories': ['#open_source', '#alignment', '#diffusion', '#multimodal', '#training', '#rlhf'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Diffusion-SDPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO): ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ€Ğ¶Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ²ĞµÑ‚Ğ²ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ĞµĞ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½ĞµÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Image Quality with Adaptive Preference Scaling', 'desc': 'The paper presents Diffusion-SDPO, a method that enhances text-to-image generation by improving how models align with human preferences. It identifies a problem in existing Direct Preference Optimization (DPO) methods where increasing the preference margin can worsen the quality of generated images. To solve this, Diffusion-SDPO introduces a new update rule that carefully adjusts the loser gradient based on its relationship to the winner gradient, ensuring that the quality of preferred outputs does not decline. The method is easy to implement, works with various models, and shows significant improvements in image quality across multiple evaluation metrics.'}, 'zh': {'title': 'è‡ªé€‚åº”ä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆè´¨é‡ï¼', 'desc': 'Diffusion-SDPOæ˜¯ä¸€ç§æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´åŠ£åŠ¿æ¢¯åº¦æ¥ä¼˜åŒ–åå¥½ï¼Œç¡®ä¿ä¼˜é€‰è¾“å‡ºçš„è¯¯å·®ä¸ä¼šå¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œç®€å•æ‰©å¤§åå¥½è¾¹é™…å¹¶ä¸ä¸€å®šèƒ½æé«˜ç”Ÿæˆè´¨é‡ï¼Œåè€Œå¯èƒ½å¯¼è‡´ä¼˜é€‰å’ŒåŠ£é€‰è¾“å‡ºçš„é‡å»ºè¯¯å·®å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒDiffusion-SDPOå¼•å…¥äº†ä¸€ç§å®‰å…¨æ›´æ–°è§„åˆ™ï¼Œæ ¹æ®åŠ£åŠ¿æ¢¯åº¦ä¸ä¼˜åŠ¿æ¢¯åº¦çš„å¯¹é½ç¨‹åº¦è‡ªé€‚åº”åœ°ç¼©æ”¾åŠ£åŠ¿æ¢¯åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusion-SDPOåœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è‡ªåŠ¨åå¥½ã€ç¾å­¦å’Œæç¤ºå¯¹é½æŒ‡æ ‡ä¸ŠæŒç»­å–å¾—æ›´å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07413', 'title': 'DigiData: Training and Evaluating General-Purpose Mobile Control Agents', 'url': 'https://huggingface.co/papers/2511.07413', 'abstract': 'DigiData and DigiData-Bench advance mobile control agents by providing a diverse, high-quality dataset and dynamic evaluation protocols, respectively.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'cacafb8e80467e4a', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#benchmark', '#open_source', '#agents', '#multimodal', '#dataset', '#synthetic'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° â€” Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ DigiData â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹, Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ DigiData-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ½ĞµĞ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ².'}, 'en': {'title': 'Empowering Mobile Control Agents with DigiData and DigiData-Bench', 'desc': 'This paper introduces DigiData, a comprehensive dataset designed specifically for training mobile control agents, which are AI systems that interact with user interfaces. DigiData is unique because it is built from a thorough analysis of app features, leading to a more diverse and complex set of goals compared to existing datasets. Additionally, the authors present DigiData-Bench, a new evaluation framework that offers dynamic protocols for assessing the performance of these agents on real-world tasks. The paper critiques traditional evaluation metrics and proposes innovative AI-driven methods to better measure agent effectiveness, ultimately enhancing human-device interactions.'}, 'zh': {'title': 'æ¨åŠ¨ç§»åŠ¨æ§åˆ¶ä»£ç†å‘å±•çš„æ–°å·¥å…·', 'desc': 'æœ¬æ–‡ä»‹ç»äº†DigiDataå’ŒDigiData-Benchï¼Œè¿™ä¸¤ä¸ªå·¥å…·æ—¨åœ¨æå‡ç§»åŠ¨æ§åˆ¶ä»£ç†çš„èƒ½åŠ›ã€‚DigiDataæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“ä¸ºè®­ç»ƒç§»åŠ¨æ§åˆ¶ä»£ç†è€Œè®¾è®¡ï¼Œå…·æœ‰æ›´é«˜çš„ç›®æ ‡å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚DigiData-Benchåˆ™æ˜¯ä¸€ä¸ªè¯„ä¼°ç§»åŠ¨æ§åˆ¶ä»£ç†åœ¨ç°å®å¤æ‚ä»»åŠ¡ä¸­çš„åŸºå‡†ï¼Œæå‡ºäº†åŠ¨æ€è¯„ä¼°åè®®å’ŒAIé©±åŠ¨çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°ä»£ç†çš„è¡¨ç°ã€‚é€šè¿‡è¿™äº›è´¡çŒ®ï¼Œæœ¬æ–‡å¸Œæœ›æ¨åŠ¨ç§»åŠ¨æ§åˆ¶ä»£ç†çš„å‘å±•ï¼Œæ”¹å–„äººæœºäº¤äº’çš„ç›´è§‚æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.07137', 'title': 'MPJudge: Towards Perceptual Assessment of Music-Induced Paintings', 'url': 'https://huggingface.co/papers/2511.07137', 'abstract': 'A novel framework MPJudge assesses music-induced paintings by integrating music features into a visual encoder using a modulation-based fusion mechanism, outperforming existing emotion recognition models.  \t\t\t\t\tAI-generated summary \t\t\t\t Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '198031443c510e80', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° MPJudge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MPD Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¾Ğ¹ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑÑŒÑ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ² ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ°Ñ….'}, 'en': {'title': 'Harmonizing Music and Art: MPJudge Unveils New Insights in Perceptual Assessment', 'desc': 'The paper introduces MPJudge, a new framework designed to evaluate music-induced paintings by linking music features with visual art through a modulation-based fusion mechanism. This approach improves upon traditional emotion recognition models, which often fail to capture the full range of perceptual cues. The authors also present MPD, a large dataset of music-painting pairs annotated for perceptual coherence, which aids in training the model. By utilizing Direct Preference Optimization, MPJudge effectively learns from ambiguous cases, leading to superior performance in identifying music-related elements in visual artworks.'}, 'zh': {'title': 'éŸ³ä¹ä¸ç»˜ç”»çš„æ„ŸçŸ¥ä¸€è‡´æ€§è¯„ä¼°æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶MPJudgeï¼Œç”¨äºè¯„ä¼°éŸ³ä¹å¼•å‘çš„ç»˜ç”»ä½œå“ã€‚è¯¥æ¡†æ¶é€šè¿‡è°ƒåˆ¶èåˆæœºåˆ¶å°†éŸ³ä¹ç‰¹å¾æ•´åˆåˆ°è§†è§‰ç¼–ç å™¨ä¸­ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰éŸ³ä¹ä¸è§†è§‰è‰ºæœ¯ä¹‹é—´çš„æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„éŸ³ä¹ä¸ç»˜ç”»å¯¹æ•°æ®é›†MPDï¼Œå¹¶é€šè¿‡ä¸“å®¶æ³¨é‡Šæ¥æ ‡æ³¨æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMPJudgeåœ¨æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.06174', 'title': 'LUT-LLM: Efficient Large Language Model Inference with Memory-based\n  Computations on FPGAs', 'url': 'https://huggingface.co/papers/2511.06174', 'abstract': "LUT-LLM, an FPGA accelerator, improves LLM inference efficiency by shifting computation to memory-based operations, achieving lower latency and higher energy efficiency compared to GPUs.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.", 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'a382a6e3f876b2da', 'authors': ['Zifan He', 'Shengyu Ye', 'Rui Ma', 'Yang Wang', 'Jason Cong'], 'affiliations': ['Microsoft Research Asia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06174.jpg', 'data': {'categories': ['#inference', '#small_models', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'ĞÑ‚ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞ¸ Ğº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: FPGA Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¸', 'desc': 'LUT-LLM â€” ÑÑ‚Ğ¾ FPGA ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ FPGA. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ… Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞĞ° FPGA AMD V80 ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 1.66x Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ñ‡ĞµĞ¼ GPU MI210 Ğ¸ 1.72x Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ A100. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ 2.16x Ğ² ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ´ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ GPU.'}, 'en': {'title': 'Accelerating LLM Inference with Memory-Based FPGA Operations', 'desc': 'LUT-LLM is an FPGA accelerator designed to enhance the efficiency of large language model (LLM) inference by utilizing memory-based operations instead of traditional arithmetic computations. This approach allows for lower latency and improved energy efficiency, making it suitable for on-device intelligence applications. The paper introduces a novel method of activation-weight co-quantization and employs techniques like bandwidth-aware parallel centroid search and efficient 2D table lookups. When tested on an AMD V80 FPGA with a customized Qwen 3 model, LUT-LLM demonstrated significant performance improvements over leading GPU alternatives.'}, 'zh': {'title': 'LUT-LLMï¼šå†…å­˜è®¡ç®—æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡', 'desc': 'LUT-LLMæ˜¯ä¸€ç§FPGAåŠ é€Ÿå™¨ï¼Œé€šè¿‡å°†è®¡ç®—è½¬ç§»åˆ°åŸºäºå†…å­˜çš„æ“ä½œï¼Œæ˜¾è‘—æé«˜äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆç‡ã€‚ä¸GPUç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ›´ä½çš„å»¶è¿Ÿå’Œæ›´é«˜çš„èƒ½æ•ˆï¼Œå°¤å…¶åœ¨å•æ‰¹æ¬¡æ¨ç†ä¸­è¡¨ç°çªå‡ºã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨FPGAä¸°å¯Œçš„ç‰‡ä¸Šå†…å­˜ï¼Œé€šè¿‡æŸ¥æ‰¾è¡¨å°†æ¨ç†è¿‡ç¨‹ä»ç®—æœ¯è®¡ç®—è½¬å˜ä¸ºå†…å­˜è®¡ç®—ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¿€æ´»æƒé‡å…±åŒé‡åŒ–æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ¡ˆï¼Œç»“åˆå¸¦å®½æ„ŸçŸ¥çš„å¹¶è¡Œè´¨å¿ƒæœç´¢å’Œé«˜æ•ˆçš„äºŒç»´æŸ¥æ‰¾è¡¨è®¾è®¡ï¼Œæœ€ç»ˆåœ¨AMD V80 FPGAä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05936', 'title': '10 Open Challenges Steering the Future of Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2511.05936', 'abstract': 'VLA models, combining vision, language, and action, are advancing through milestones like multimodality, reasoning, and safety, with trends focusing on spatial understanding and human coordination.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '84ab99f7a376398c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#benchmark', '#survey', '#agents', '#multimodal', '#training', '#robotics', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: Ğ²ĞµÑ…Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ vision-language-action (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´ĞµÑÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²ĞµÑ… Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ»Ğ°, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°, Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Advancing VLA Models: Bridging Vision, Language, and Action', 'desc': "This paper explores the advancements in vision-language-action (VLA) models, which integrate visual perception, language understanding, and action execution. It identifies ten key milestones in their development, including multimodality, reasoning, and safety, highlighting the importance of these aspects for effective embodied AI. The authors also discuss emerging trends such as spatial understanding and data synthesis, which are crucial for improving the models' performance and generalization capabilities. The goal is to encourage further research that will enhance the acceptance and application of VLA models in real-world scenarios."}, 'zh': {'title': 'æ¨åŠ¨è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æœªæ¥å‘å±•', 'desc': 'æœ¬æ–‡è®¨è®ºäº†è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„å‘å±•ï¼Œå¼ºè°ƒäº†å…¶åœ¨å¤šæ¨¡æ€ã€æ¨ç†å’Œå®‰å…¨æ€§ç­‰æ–¹é¢çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› è€Œåœ¨å…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚æ–‡ç« è¿˜æ¢è®¨äº†ç©ºé—´ç†è§£ã€ä¸–ç•ŒåŠ¨æ€å»ºæ¨¡å’Œæ•°æ®åˆæˆç­‰æ–°å…´è¶‹åŠ¿ï¼Œæ—¨åœ¨æ¨åŠ¨VLAæ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚é€šè¿‡è¿™äº›è®¨è®ºï¼Œæˆ‘ä»¬å¸Œæœ›å¼•èµ·å¯¹åŠ é€ŸVLAæ¨¡å‹ç ”ç©¶çš„å…³æ³¨ï¼Œä»¥å®ç°æ›´å¹¿æ³›çš„åº”ç”¨ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.07409', 'title': 'DIMO: Diverse 3D Motion Generation for Arbitrary Objects', 'url': 'https://huggingface.co/papers/2511.07409', 'abstract': 'A generative approach extracts motion patterns from video models, embeds them into a latent space, and uses neural key point trajectories to generate diverse 3D motions from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '4706b9fc94b55c38', 'authors': ['Linzhan Mou', 'Jiahui Lei', 'Chen Wang', 'Lingjie Liu', 'Kostas Daniilidis'], 'affiliations': ['Archimedes, Athena RC', 'University of Pennsylvania'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07409.jpg', 'data': {'categories': ['#diffusion', '#3d', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ¶Ğ¸Ğ·Ğ½Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DIMO â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… Ğ² Ğ¾Ğ±Ñ‰ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ”Ğ°Ğ»ĞµĞµ Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ 3D Ğ“Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ 3D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'DIMO: Generating Diverse 3D Motions from a Single Image', 'desc': 'This paper introduces DIMO, a generative model that creates diverse 3D motions for various objects using just a single image. It utilizes well-trained video models to identify common motion patterns, which are then embedded into a low-dimensional latent space. By generating multiple videos of the same object with different motions, the model learns a structured representation of these motions through neural key point trajectories. This allows for efficient sampling of diverse 3D motions during inference, enabling applications like 3D motion interpolation and language-guided motion generation.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆå¤šæ ·3Dè¿åŠ¨çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDIMOçš„ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆå¤šæ ·çš„3Dè¿åŠ¨ã€‚æˆ‘ä»¬åˆ©ç”¨ç»è¿‡è‰¯å¥½è®­ç»ƒçš„è§†é¢‘æ¨¡å‹æå–å¸¸è§çš„è¿åŠ¨æ¨¡å¼ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°ä¸€ä¸ªå…±äº«çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­ã€‚é€šè¿‡ç”ŸæˆåŒä¸€ç‰©ä½“çš„å¤šæ®µä¸åŒè¿åŠ¨è§†é¢‘ï¼Œæˆ‘ä»¬å°†æ¯ç§è¿åŠ¨åµŒå…¥ä¸ºæ½œåœ¨å‘é‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå…±äº«çš„è¿åŠ¨è§£ç å™¨æ¥å­¦ä¹ è¿™äº›è¿åŠ¨çš„åˆ†å¸ƒã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†æ—¶å¿«é€Ÿé‡‡æ ·å¤šæ ·çš„3Dè¿åŠ¨ï¼Œæ”¯æŒ3Dè¿åŠ¨æ’å€¼å’ŒåŸºäºè¯­è¨€çš„è¿åŠ¨ç”Ÿæˆç­‰åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07299', 'title': 'VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models', 'url': 'https://huggingface.co/papers/2511.07299', 'abstract': 'VADER, an LLM-driven framework, enhances video anomaly understanding by integrating keyframe object relations and visual cues to provide detailed, causally grounded descriptions and robust question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'e301f3b8cfe481b3', 'authors': ['Ying Cheng', 'Yu-Ho Lin', 'Min-Hung Chen', 'Fu-En Yang', 'Shang-Hong Lai'], 'affiliations': ['NVIDIA', 'National Tsing Hua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07299.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° VADER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. VADER Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ĞºĞ°Ğ´Ñ€Ñƒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'VADER: Enhancing Video Anomaly Understanding with Causal Insights', 'desc': 'VADER is a framework that improves the understanding of unusual events in videos by using advanced language models. It goes beyond just spotting anomalies by analyzing the relationships between objects and their visual context. The framework uses an Anomaly Scorer to evaluate each frame and a Context-Aware Sampling strategy to understand the causal context of anomalies. By combining relational features with language models, VADER provides detailed descriptions and answers questions about the anomalies, showing significant improvements in explainable video analysis.'}, 'zh': {'title': 'VADERï¼šæå‡è§†é¢‘å¼‚å¸¸ç†è§£çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'VADERæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†é¢‘å¼‚å¸¸ç†è§£ã€‚å®ƒé€šè¿‡æ•´åˆå…³é”®å¸§å¯¹è±¡å…³ç³»å’Œè§†è§‰çº¿ç´¢ï¼Œæä¾›è¯¦ç»†çš„å› æœæè¿°å’Œå¼ºå¤§çš„é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é¦–å…ˆä¸ºæ¯å¸§åˆ†é…å¼‚å¸¸åˆ†æ•°ï¼Œç„¶åä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥æ•æ‰å¼‚å¸¸äº‹ä»¶çš„å› æœèƒŒæ™¯ã€‚VADERåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„è§†é¢‘å¼‚å¸¸ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†å¯è§£é‡Šè§†é¢‘å¼‚å¸¸åˆ†æçš„å‰æ²¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06090', 'title': 'SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?', 'url': 'https://huggingface.co/papers/2511.06090', 'abstract': 'SWE-fficiency is a benchmark for evaluating repository-level performance optimization using real workloads, focusing on identifying and implementing performance improvements while maintaining code correctness.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '4e80fa9ff3d8a948', 'authors': ['Jeffrey Jian Ma', 'Milad Hashemi', 'Amir Yazdanbakhsh', 'Kevin Swersky', 'Ofir Press', 'Enhui Li', 'Vijay Janapa Reddi', 'Parthasarathy Ranganathan'], 'affiliations': ['Google', 'Google DeepMind', 'Harvard University', 'Princeton University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06090.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#agents', '#plp', '#dataset', '#science'], 'emoji': 'âš¡', 'ru': {'title': 'ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'SWE-fficiency - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ĞºĞ¾Ğ´Ğ°, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ·ĞºĞ¸Ñ… Ğ¼ĞµÑÑ‚ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 498 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² (numpy, pandas, scipy Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Pull Request Ñ GitHub Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼ĞµĞ½ĞµĞµ 0.15x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'SWE-fficiency: Optimizing Performance with Code Correctness', 'desc': 'The paper introduces SWE-fficiency, a benchmark designed to evaluate the performance optimization of software repositories while ensuring code correctness. It focuses on real workloads and includes 498 tasks from popular data-science and machine-learning libraries. The benchmark challenges agents to identify performance bottlenecks and propose code improvements that match expert-level speedups while passing unit tests. The study reveals that current state-of-the-art agents significantly underperform, achieving only a fraction of the expert speedup due to difficulties in code reasoning and maintaining correctness.'}, 'zh': {'title': 'SWE-fficiencyï¼šä¼˜åŒ–è½¯ä»¶æ€§èƒ½çš„æ–°åŸºå‡†', 'desc': 'SWE-fficiencyæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è½¯ä»¶åº“æ€§èƒ½ä¼˜åŒ–çš„åŸºå‡†ï¼Œä¸“æ³¨äºåœ¨ä¿æŒä»£ç æ­£ç¡®æ€§çš„åŒæ—¶è¯†åˆ«å’Œå®æ–½æ€§èƒ½æ”¹è¿›ã€‚è¯¥åŸºå‡†åŒ…å«498ä¸ªä»»åŠ¡ï¼Œæ¶µç›–ä¹ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œé«˜æ€§èƒ½è®¡ç®—åº“ã€‚é€šè¿‡åˆ†æä»£ç è¯­ä¹‰å’Œå®šä½ç“¶é¢ˆï¼Œä»£ç†éœ€è¦ç”Ÿæˆä¸€ä¸ªè¡¥ä¸ï¼Œä»¥åŒ¹é…æˆ–è¶…è¿‡ä¸“å®¶çš„åŠ é€Ÿæ•ˆæœï¼ŒåŒæ—¶é€šè¿‡ç›¸åŒçš„å•å…ƒæµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–ä»£ç†åœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¹³å‡åªèƒ½è¾¾åˆ°ä¸“å®¶åŠ é€Ÿçš„0.15å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.00710', 'title': 'Ariadne: A Controllable Framework for Probing and Extending VLM\n  Reasoning Boundaries', 'url': 'https://huggingface.co/papers/2511.00710', 'abstract': "Ariadne, a framework using synthetic mazes and RLVR, expands VLMs' capability in visual-centric spatial reasoning and improves zero-shot generalization on real-world benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'e5ee61ff06448bba', 'authors': ['Minghe Shen', 'Zhuo Zhi', 'Chonghan Liu', 'Shuo Xing', 'Zhengzhong Tu', 'Che Liu'], 'affiliations': ['Imperial College London', 'Texas A&M University', 'University College London', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.00710.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#cv', '#multimodal', '#rl', '#reasoning', '#synthetic'], 'emoji': 'ğŸ§­', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ VLM Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ariadne, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VLM Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ²Ğ¾Ğ´Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ 0% Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 50% Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ¾Ñ‚Ğ¼ĞµÑ‚Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 16% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° MapBench Ğ¸ 24% Ğ½Ğ° ReasonMap Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ´ ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': "Ariadne: Expanding VLMs' Spatial Reasoning with Synthetic Mazes", 'desc': "Ariadne is a new framework that enhances Vision-Language Models (VLMs) by using synthetic mazes to improve their ability in visual-centric spatial reasoning. It employs Reinforcement Learning with Verified Rewards (RLVR) to train these models in a controlled environment, allowing for precise adjustments in task difficulty. The results show that after this training, the VLM significantly improves its performance on spatial reasoning tasks, achieving over 50% accuracy where it previously scored 0%. Additionally, Ariadne demonstrates strong zero-shot generalization on real-world benchmarks, indicating that it effectively extends the model's capabilities beyond its initial limitations."}, 'zh': {'title': 'æ‰©å±•è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'Ariadneæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆè¿·å®«å’Œå¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œæ‰©å±•äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è§†è§‰ä¸­å¿ƒç©ºé—´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡åœ¨å¯æ§ç¯å¢ƒä¸­è¿›è¡Œå¤šæ­¥ç©ºé—´æ¨ç†è®­ç»ƒï¼ŒAriadneæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šçš„é›¶-shotæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡RLVRè®­ç»ƒåï¼ŒVLMåœ¨åŸæœ¬å¾—åˆ†ä¸º0%çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è¶…è¿‡50%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ‰©å±•æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚å°½ç®¡è®­ç»ƒä»…åŸºäºåˆæˆè¿·å®«æ ·æœ¬ï¼ŒAriadneåœ¨å®é™…åŸºå‡†æµ‹è¯•ä¸­ä»å®ç°äº†æ˜¾è‘—çš„é›¶-shotæ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®ä¸–ç•Œç©ºé—´æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07061', 'title': 'Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and\n  Curriculum Learning', 'url': 'https://huggingface.co/papers/2511.07061', 'abstract': "A novel ERC training framework, PRC-Emo, integrates prompt engineering, demonstration retrieval, and curriculum learning to enhance LLMs' ability to perceive emotions in conversations, achieving state-of-the-art performance on benchmark datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'e675233a34194f87', 'authors': ['Xinran Li', 'Yu Liu', 'Jiaqi Qiao', 'Xiujuan Xu'], 'affiliations': ['School of Software Technology, Dalian University of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07061.jpg', 'data': {'categories': ['#rag', '#benchmark', '#training'], 'emoji': 'ğŸ˜Š', 'ru': {'title': 'Ğ¢Ñ€Ğ¸ ĞºĞ»ÑÑ‡Ğ° Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° PRC-Emo Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº ÑĞ²Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼, Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ curriculum learning Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ñ LoRA, ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… IEMOCAP Ğ¸ MELD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Enhancing Emotion Recognition in Conversations with PRC-Emo', 'desc': "The paper presents PRC-Emo, a new training framework designed to improve Large Language Models' (LLMs) ability to recognize emotions in conversations. It combines prompt engineering, demonstration retrieval, and curriculum learning to enhance the model's understanding of both explicit and implicit emotional cues. By creating emotion-sensitive prompts and a dedicated repository of dialogue examples, the framework guides LLMs in interpreting emotional states more accurately. The results show that PRC-Emo achieves state-of-the-art performance on benchmark datasets, indicating its effectiveness in emotion recognition tasks."}, 'zh': {'title': 'æå‡å¯¹è¯æƒ…æ„Ÿè¯†åˆ«çš„å…¨æ–°æ¡†æ¶PRC-Emo', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æƒ…æ„Ÿè¯†åˆ«è®­ç»ƒæ¡†æ¶PRC-Emoï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯¹è¯ä¸­æ„ŸçŸ¥æƒ…æ„Ÿçš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æç¤ºå·¥ç¨‹ã€ç¤ºä¾‹æ£€ç´¢å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œè®¾è®¡äº†æƒ…æ„Ÿæ•æ„Ÿçš„æç¤ºæ¨¡æ¿ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼æ¨¡å‹ç†è§£è¯´è¯è€…çš„å¿ƒç†çŠ¶æ€ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç¬¬ä¸€ä¸ªä¸“é—¨ç”¨äºæƒ…æ„Ÿè¯†åˆ«çš„ç¤ºä¾‹æ£€ç´¢åº“ï¼ŒåŒ…å«æ¥è‡ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†çš„è®­ç»ƒæ ·æœ¬å’Œé«˜è´¨é‡çš„å¯¹è¯ç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRC-Emoåœ¨IEMOCAPå’ŒMELDä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æå‡LLMæƒ…æ„Ÿç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ™®é€‚æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07253', 'title': 'Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models', 'url': 'https://huggingface.co/papers/2511.07253', 'abstract': 'Omni-AVSR is a unified audio-visual LLM that efficiently supports ASR, VSR, and AVSR through multi-granularity training and parameter-efficient adaptation, achieving high accuracy with reduced resource use.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'd843529659b96907', 'authors': ['Umberto Cappellazzo', 'Xubo Liu', 'Pingchuan Ma', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London, UK', 'University of Surrey, UK'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07253.jpg', 'data': {'categories': ['#inference', '#audio', '#multimodal', '#training'], 'emoji': 'ğŸ‘ï¸\u200dğŸ—¨ï¸', 'ru': {'title': 'ĞĞ´Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Omni-AVSR â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ°ÑƒĞ´Ğ¸Ğ¾ (ASR), Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ (VSR) Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (AVSR) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°Ñ‚Ñ€Ñ‘ÑˆĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LoRA, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Omni-AVSR: Unifying Speech Recognition for Efficiency and Accuracy', 'desc': 'Omni-AVSR is a unified audio-visual large language model (LLM) designed to enhance speech recognition across three modalities: Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). It employs multi-granularity training and parameter-efficient adaptation to achieve high accuracy while minimizing resource consumption. By integrating various training strategies, Omni-AVSR allows for flexible inference and reduces the need for separate models for each task, thus leveraging cross-task synergies. Experimental results demonstrate that it performs comparably or better than existing models, even in noisy environments, while being more efficient in terms of training and deployment resources.'}, 'zh': {'title': 'ç»Ÿä¸€éŸ³é¢‘-è§†è§‰æ¨¡å‹ï¼Œæå‡è¯†åˆ«æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'Omni-AVSRæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³é¢‘-è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ”¯æŒå¬è§‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å’ŒéŸ³é¢‘-è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šç²’åº¦è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆé€‚åº”ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºä½¿ç”¨ï¼ŒåŒæ—¶å®ç°äº†é«˜å‡†ç¡®ç‡ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒOmni-AVSRé‡‡ç”¨äº†ç»Ÿä¸€æ¡†æ¶ï¼Œé¿å…äº†ç‹¬ç«‹è®­ç»ƒå¤šä¸ªæ¨¡å‹å¸¦æ¥çš„è®¡ç®—å’Œéƒ¨ç½²èµ„æºæµªè´¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-AVSRåœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å™ªå£°ç¯å¢ƒä¸‹ä¿æŒé²æ£’æ€§ï¼Œå¹¶æä¾›äº†æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06221', 'title': 'Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B', 'url': 'https://huggingface.co/papers/2511.06221', 'abstract': "VibeThinker-1.5B, a 1.5B-parameter model using the Spectrum-to-Signal Principle, achieves superior reasoning capabilities compared to larger models at a significantly lower cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.", 'score': 129, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'a5a147e6e481bd5e', 'authors': ['Sen Xu', 'Yi Zhou', 'Wei Wang', 'Jixin Min', 'Zhibin Yin', 'Yingwei Dai', 'Shixi Liu', 'Lianyu Pang', 'Yirong Chen', 'Junlin Zhang'], 'affiliations': ['Sina Weibo Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06221.jpg', 'data': {'categories': ['#small_models', '#open_source', '#optimization', '#training', '#rlhf', '#math', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑƒĞ¼Ğ¾Ğ¼: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ', 'desc': 'VibeThinker-1.5B â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° Spectrum-to-Signal. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ supervised fine-tuning, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ policy optimization Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ² $7,800. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚.'}, 'en': {'title': 'Small Model, Big Reasoning: VibeThinker-1.5B Redefines AI Efficiency', 'desc': 'VibeThinker-1.5B is a 1.5 billion parameter model that utilizes the Spectrum-to-Signal Principle (SSP) to achieve impressive reasoning abilities without the need for massive parameter counts. This model challenges the common belief that larger models are always better by demonstrating that a smaller model can outperform much larger counterparts in reasoning tasks. It employs a Two-Stage Diversity-Exploring Distillation followed by MaxEnt-Guided Policy Optimization to refine its performance. With a low training cost of $7,800, VibeThinker-1.5B not only matches but exceeds the performance of larger models on several benchmarks, showcasing the potential of smaller models in AI research.'}, 'zh': {'title': 'å°æ¨¡å‹ï¼Œå¤§æ™ºæ…§ï¼', 'desc': 'VibeThinker-1.5B æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 15 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†é¢‘è°±åˆ°ä¿¡å·åŸç†ï¼ˆSSPï¼‰ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸Šè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ï¼Œä¸”æˆæœ¬æ˜¾è‘—æ›´ä½ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µå¤šæ ·æ€§æ¢ç´¢è’¸é¦ï¼ˆSFTï¼‰ç”Ÿæˆå¹¿æ³›çš„è§£å†³æ–¹æ¡ˆï¼Œç„¶ååˆ©ç”¨æœ€å¤§ç†µå¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆRLï¼‰æ¥å¢å¼ºæ­£ç¡®ä¿¡å·ã€‚ä¸å¤§å‹æ¨¡å‹å¦‚ DeepSeek R1ï¼ˆ671Bï¼‰å’Œ Kimi k2ï¼ˆ>1Tï¼‰ç›¸æ¯”ï¼ŒVibeThinker-1.5B ä»¥ä»… 7800 ç¾å…ƒçš„è®­ç»ƒæˆæœ¬ï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œç”šè‡³åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† 400 å€å¤§çš„ DeepSeek R1ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°æ¨¡å‹ä¹Ÿèƒ½å®ç°ä¸å¤§æ¨¡å‹ç›¸å½“çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå¤§å¹…é™ä½è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œæ¨åŠ¨å…ˆè¿› AI ç ”ç©¶çš„æ™®åŠã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07332', 'title': 'Grounding Computer Use Agents on Human Demonstrations', 'url': 'https://huggingface.co/papers/2511.07332', 'abstract': 'GroundCUA, a large-scale desktop grounding dataset, enables the development of GroundNext models that achieve state-of-the-art performance in mapping instructions to UI elements with less training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.', 'score': 104, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'c514ab35d2b714dd', 'authors': ['Aarash Feizi', 'Shravan Nayak', 'Xiangru Jian', 'Kevin Qinghong Lin', 'Kaixin Li', 'Rabiul Awal', 'Xing Han LÃ¹', 'Johan Obando-Ceron', 'Juan A. Rodriguez', 'Nicolas Chapados', 'David Vazquez', 'Adriana Romero-Soriano', 'Reihaneh Rabbany', 'Perouz Taslakian', 'Christopher Pal', 'Spandana Gella', 'Sai Rajeswar'], 'affiliations': ['CIFAR AI Chair', 'Ecole de Technologie Superieure', 'McGill University', 'Mila - Quebec AI Institute', 'National University of Singapore', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal', 'University of Oxford', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07332.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#agents', '#multimodal', '#training', '#dataset', '#synthetic'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'GroundCUA â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 56 Ñ‚Ñ‹ÑÑÑ‡ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ğ¸Ğ· 87 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 3.56 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ UI-ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GroundCUA Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GroundNext, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞºÑ€Ğ°Ğ½Ğ°. GroundNext Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ°.'}, 'en': {'title': 'GroundCUA: Bridging Language and Desktop UI with Less Data', 'desc': 'GroundCUA is a new dataset designed to improve how AI understands and interacts with desktop applications by linking natural language instructions to specific UI elements. It includes a vast collection of 56,000 screenshots from 87 different applications, with over 3.56 million annotations made by experts to ensure accuracy. The dataset allows the development of GroundNext models, which achieve top performance in mapping instructions to UI elements while using significantly less training data than previous models. By employing techniques like supervised fine-tuning and reinforcement learning, GroundNext demonstrates that high-quality datasets are essential for creating effective computer-use agents.'}, 'zh': {'title': 'é«˜è´¨é‡æ•°æ®é›†æ¨åŠ¨æ¡Œé¢æ™ºèƒ½ä»£ç†çš„å‘å±•', 'desc': 'GroundCUAæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ¡Œé¢åŸºç¡€æ•°æ®é›†ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘GroundNextæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å‡†ç¡®æ˜ å°„åˆ°ç”¨æˆ·ç•Œé¢å…ƒç´ ä¸Šã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†12ä¸ªç±»åˆ«çš„87ä¸ªåº”ç”¨ç¨‹åºï¼ŒåŒ…å«56,000å¼ æˆªå›¾ï¼Œå¹¶å¯¹æ¯ä¸ªå±å¹•å…ƒç´ è¿›è¡Œäº†è¯¦ç»†æ ‡æ³¨ï¼Œæ€»è®¡è¶…è¿‡356ä¸‡æ¡ç»è¿‡äººå·¥éªŒè¯çš„æ³¨é‡Šã€‚é€šè¿‡è¿™äº›ä¸“å®¶æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å¤šæ ·åŒ–çš„æŒ‡ä»¤ï¼Œæ•æ‰äº†å¹¿æ³›çš„ç°å®ä»»åŠ¡ï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ•°æ®ã€‚ä½¿ç”¨GroundCUAï¼ŒGroundNextæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ä¸åˆ°ä¹‹å‰å·¥ä½œçš„ååˆ†ä¹‹ä¸€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08319', 'title': 'Adaptive Multi-Agent Response Refinement in Conversational Systems', 'url': 'https://huggingface.co/papers/2511.08319', 'abstract': "A multi-agent framework enhances conversational quality by refining responses through agents responsible for factuality, personalization, and coherence, outperforming existing methods on challenging datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.", 'score': 40, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'f6496f6917a5836b', 'authors': ['Soyeong Jeong', 'Aparna Elangovan', 'Emine Yilmaz', 'Oleg Rokhlenko'], 'affiliations': ['Amazon', 'Collate', 'KAIST', 'University College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08319.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğµ: Ñ„Ğ°ĞºÑ‚-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°, Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Conversations with Specialized Agents', 'desc': 'This paper presents a multi-agent framework designed to improve the quality of conversational responses generated by AI. Each agent in the framework specializes in one of three critical aspects: factuality, personalization, and coherence, ensuring that responses are well-rounded and accurate. By dynamically coordinating these agents based on the needs of each query, the framework effectively refines responses before they reach the user. The results show that this approach outperforms existing methods, particularly in complex scenarios requiring specific knowledge or user preferences.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡å¯¹è¯è´¨é‡', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨è´Ÿè´£äº‹å®æ€§ã€ä¸ªæ€§åŒ–å’Œè¿è´¯æ€§çš„æ™ºèƒ½ä½“æ¥æå‡å¯¹è¯è´¨é‡ã€‚æ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£å®¡æŸ¥å’Œæ”¹è¿›ç‰¹å®šæ–¹é¢çš„å“åº”ï¼Œæœ€ç»ˆå°†åé¦ˆåˆå¹¶ä»¥æé«˜æ•´ä½“å“åº”æ•ˆæœã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ²Ÿé€šç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªæŸ¥è¯¢çš„å…·ä½“éœ€æ±‚è‡ªé€‚åº”é€‰æ‹©å’Œåè°ƒæœ€ç›¸å…³çš„æ™ºèƒ½ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†å¤æ‚å¯¹è¯æ•°æ®é›†æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠçŸ¥è¯†æˆ–ç”¨æˆ·ä¸ªæ€§åŒ–çš„ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05664', 'title': 'KLASS: KL-Guided Fast Inference in Masked Diffusion Models', 'url': 'https://huggingface.co/papers/2511.05664', 'abstract': "KL-Adaptive Stability Sampling (KLASS) accelerates diffusion-based generation by identifying stable predictions, achieving significant speedups and quality improvements across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78times wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.", 'score': 35, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '20ad03bf2e79ccfb', 'authors': ['Seo Hyun Kim', 'Sunwoo Hong', 'Hojung Jung', 'Youngrok Park', 'Se-Young Yun'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05664.jpg', 'data': {'categories': ['#reasoning', '#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ½ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ KL-Adaptive Stability Sampling (KLASS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.78 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¶Ğ°Ğ´Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. KLASS Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑ ÑĞµĞ±Ñ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ¼Ğ¿Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Accelerating Diffusion with KLASS: Fast and Stable Sampling!', 'desc': 'KL-Adaptive Stability Sampling (KLASS) is a novel method designed to enhance the efficiency of diffusion-based generation models. It leverages token-level KL divergence to pinpoint stable and high-confidence predictions, allowing for faster sampling without the need for additional model training. By unmasking multiple tokens in each iteration, KLASS significantly accelerates the generation process while preserving the quality of the outputs. The method has demonstrated impressive speed improvements and performance gains across various tasks, including text, image, and molecular generation, establishing itself as a leading approach in the field.'}, 'zh': {'title': 'åŠ é€Ÿç”Ÿæˆï¼Œç¨³å®šé‡‡æ ·çš„é©å‘½', 'desc': 'KLè‡ªé€‚åº”ç¨³å®šé‡‡æ ·ï¼ˆKLASSï¼‰æ˜¯ä¸€ç§åŠ é€ŸåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«ç¨³å®šçš„é¢„æµ‹æ¥å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œè´¨é‡æ”¹å–„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»¤ç‰Œçº§çš„KLæ•£åº¦æ¥è¯†åˆ«é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œä»è€Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­è§£é”å¤šä¸ªä»¤ç‰Œï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒã€‚KLASSåœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾2.78å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨æ ‡å‡†è´ªå©ªè§£ç ä¸Šæé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬ã€å›¾åƒå’Œåˆ†å­ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸéªŒè¯äº†KLASSçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºå¹¿æ³›é€‚ç”¨çš„é‡‡æ ·å™¨çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08567', 'title': 'The Path Not Taken: RLVR Provably Learns Off the Principals', 'url': 'https://huggingface.co/papers/2511.08567', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) improves large language models by updating a limited set of parameters, which is explained by a Three-Gate Theory, revealing distinct optimization dynamics compared to supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.   Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.", 'score': 32, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '54a6e05baaa5180b', 'authors': ['Hanqing Zhu', 'Zhenyu Zhang', 'Hanxian Huang', 'DiJia Su', 'Zechun Liu', 'Jiawei Zhao', 'Igor Fedorov', 'Hamed Pirsiavash', 'Zhizhou Sha', 'Jinwon Lee', 'David Z. Pan', 'Zhangyang Wang', 'Yuandong Tian', 'Kai Sheng Tai'], 'affiliations': ['Meta AI', 'The University of Texas at Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08567.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#alignment', '#architecture', '#training', '#rl', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ RLVR Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ (RLVR) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ…Ğ¾Ñ‚Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€Ğ¾Ğ²: KL-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ RLVR Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…, Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²ĞµÑĞ¾Ğ², Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ñ€ĞµĞ¹Ñ„ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° RLVR Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ supervised fine-tuning, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ SFT, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': "Unlocking Efficient Learning with RLVR's Three-Gate Theory", 'desc': "Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning capabilities of large language models by making targeted updates to a small number of parameters. This paper introduces the Three-Gate Theory, which explains how RLVR operates differently from traditional supervised fine-tuning (SFT) by focusing on specific parameter regions rather than the entire model. The theory outlines three gates that control the optimization process, ensuring that updates are efficient and localized, which leads to improved performance without significant changes to the model's overall structure. The findings suggest that RLVR's unique optimization dynamics require new approaches to fine-tuning that are tailored to its specific learning behavior, rather than relying on methods developed for SFT."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ çš„æ–°è§†è§’ï¼šä¼˜åŒ–ä¸å‚æ•°æ¼”å˜çš„ç§˜å¯†', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡æ›´æ–°æœ‰é™çš„å‚æ•°é›†æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰é—¨ç†è®ºï¼Œæ­ç¤ºäº†ä¸ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒRLVRå…·æœ‰ä¸åŒçš„ä¼˜åŒ–åŠ¨æ€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLVRåœ¨æƒé‡ç©ºé—´ä¸­æ²¿éä¸»æ–¹å‘å­¦ä¹ ï¼Œåˆ©ç”¨æœ€å°çš„è°±æ¼‚ç§»å’Œå‡å°‘çš„ä¸»å­ç©ºé—´æ—‹è½¬æ¥å®ç°æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒRLVRå±•ç°å‡ºæ›´é«˜çš„å‚æ•°æ•ˆç‡å’Œæ›´ç¨³å®šçš„å­¦ä¹ è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07003', 'title': 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs', 'url': 'https://huggingface.co/papers/2511.07003', 'abstract': 'LMT, a suite of large-scale multilingual translation models, addresses challenges in multilingual machine translation through strategic downsampling and parallel multilingual prompting, achieving state-of-the-art performance across 60 languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce LMT, a suite of Large-scale Multilingual Translation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of directional degeneration, where symmetric multi-way fine-tuning data overemphasize reverse directions (X to En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose Strategic Downsampling, a simple yet effective method to mitigate this degeneration. In addition, we design Parallel Multilingual Prompting (PMP), which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\href{https://github.com/NiuTrans/LMT{https://github.com/NiuTrans/LMT}}.', 'score': 32, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '63857de97af80d83', 'authors': ['Yingfeng Luo', 'Ziqiang Xu', 'Yuxuan Ouyang', 'Murun Yang', 'Dingyang Lin', 'Kaiyan Chang', 'Tong Zheng', 'Bei Li', 'Peinan Feng', 'Quan Du', 'Tong Xiao', 'Jingbo Zhu'], 'affiliations': ['NiuTrans Research, Shenyang, China', 'School of Computer Science and Engineering, Northeastern University, Shenyang, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07003.jpg', 'data': {'categories': ['#small_models', '#low_resource', '#open_source', '#transfer_learning', '#training', '#dataset', '#multilingual', '#machine_translation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LMT â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 60 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ 234 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ ÑƒĞ¿Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ many-to-one Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Strategic Downsampling, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Parallel Multilingual Prompting, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LMT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ SOTA Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'LMT: Breaking Language Barriers with Advanced Multilingual Translation', 'desc': 'The paper introduces LMT, a suite of large-scale multilingual translation models designed to improve multilingual machine translation (MMT) across 60 languages. It addresses key challenges such as English-centric bias and inconsistent translation quality by implementing Strategic Downsampling to prevent directional degeneration in translation data. Additionally, Parallel Multilingual Prompting (PMP) is utilized to enhance translation performance by leveraging related auxiliary languages. LMT achieves state-of-the-art results, outperforming larger models while being available in multiple sizes to support further research in MMT.'}, 'zh': {'title': 'LMTï¼šæ¨åŠ¨å¤šè¯­è¨€ç¿»è¯‘çš„æ–°çªç ´', 'desc': 'LMTæ˜¯ä¸€å¥—å¤§å‹å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ä¸­çš„æŒ‘æˆ˜ã€‚é€šè¿‡æˆ˜ç•¥ä¸‹é‡‡æ ·å’Œå¹¶è¡Œå¤šè¯­è¨€æç¤ºï¼ŒLMTåœ¨60ç§è¯­è¨€ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¿»è¯‘æ€§èƒ½ã€‚ç ”ç©¶ä¸­å‘ç°äº†æ–¹å‘é€€åŒ–ç°è±¡ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡ä¸‹é™ï¼Œå› æ­¤æå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚LMTçš„å‘å¸ƒå°†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œæ¨åŠ¨é«˜è´¨é‡çš„å¤šè¯­è¨€ç¿»è¯‘å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07080', 'title': 'Wasm: A Pipeline for Constructing Structured Arabic Interleaved\n  Multimodal Corpora', 'url': 'https://huggingface.co/papers/2511.07080', 'abstract': 'A pipeline for processing the Common Crawl dataset to create a new Arabic multimodal dataset that preserves document structure and supports both text-only and multimodal pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.', 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '22c66b82a539c9c4', 'authors': ['Khalil Hennara', 'Ahmad Bastati', 'Muhammad Hreden', 'Mohamed Motasim Hamed', 'Zeina Aldallal', 'Sara Chrouf', 'Safwan AlModhayan'], 'affiliations': ['Khobar, Saudi Arabia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07080.jpg', 'data': {'categories': ['#low_resource', '#open_source', '#multimodal', '#data', '#dataset', '#multilingual', '#synthetic'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Wasm Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Common Crawl, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²ĞµĞ±-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ markdown. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… LLM, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ², ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¹ LLM Ğ¸ LMM.'}, 'en': {'title': 'Enhancing Arabic Multimodal Learning with Structured Data', 'desc': 'This paper introduces a new pipeline called Wasm for processing the Common Crawl dataset to create a multimodal dataset specifically for the Arabic language. The dataset preserves the structure of documents, allowing for both text-only and multimodal pre-training, which is crucial for enhancing the performance of large language and multimodal models. The authors highlight the limitations of existing Arabic datasets that focus only on text extraction and demonstrate how their approach maintains the integrity of web content. Additionally, they provide a detailed comparison of their processing methods with existing datasets and release their dataset and pipeline for future research.'}, 'zh': {'title': 'æ„å»ºé˜¿æ‹‰ä¼¯å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæå‡æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤„ç†Common Crawlæ•°æ®é›†çš„ç®¡é“ï¼Œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªæ–°çš„é˜¿æ‹‰ä¼¯å¤šæ¨¡æ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¿ç•™äº†æ–‡æ¡£ç»“æ„ï¼Œæ”¯æŒæ–‡æœ¬å’Œå¤šæ¨¡æ€çš„é¢„è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒåœ¨è‡ªç„¶æ–‡æ¡£ä¸Šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä»…ä½¿ç”¨å›¾åƒ-æ–‡æœ¬å¯¹çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å…³æ³¨æ–‡æœ¬æå–ï¼Œè¿˜ä¿æŒäº†ç½‘é¡µå†…å®¹çš„ç»“æ„å®Œæ•´æ€§ï¼Œæä¾›äº†çµæ´»çš„é¢„è®­ç»ƒé€‰é¡¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06281', 'title': 'VideoSSR: Video Self-Supervised Reinforcement Learning', 'url': 'https://huggingface.co/papers/2511.06281', 'abstract': 'A novel video self-supervised reinforcement learning framework, VideoSSR, enhances MLLM performance across various video understanding tasks by leveraging intrinsic video information.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR.', 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '2c35c4aa0f09b95f', 'authors': ['Zefeng He', 'Xiaoye Qu', 'Yafu Li', 'Siyuan Huang', 'Daizong Liu', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Wuhan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06281.jpg', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#dataset', '#rl'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ VideoSSR â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VIUBench, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° 17 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 5% Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Harnessing Video Intrinsic Information for Enhanced Learning', 'desc': 'The paper introduces VideoSSR, a self-supervised reinforcement learning framework designed to improve the performance of Multimodal Large Language Models (MLLMs) in video understanding tasks. It addresses the challenge of limited high-quality video datasets by utilizing intrinsic information from videos to create verifiable training data. The authors propose three self-supervised pretext tasks and develop the Video Intrinsic Understanding Benchmark (VIUBench) to assess their complexity. Experimental results show that VideoSSR significantly enhances MLLM performance across various benchmarks, demonstrating its effectiveness in advancing video understanding capabilities.'}, 'zh': {'title': 'åˆ©ç”¨è§†é¢‘å†…åœ¨ä¿¡æ¯æå‡æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶VideoSSRï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨è§†é¢‘å†…åœ¨ä¿¡æ¯æ¥æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†ä¸‰ä¸ªè‡ªç›‘ç£çš„é¢„è®­ç»ƒä»»åŠ¡ï¼šå¼‚å¸¸å®šä½ã€ç‰©ä½“è®¡æ•°å’Œæ—¶é—´æ‹¼å›¾ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¯éªŒè¯è®­ç»ƒæ•°æ®ã€‚é€šè¿‡æ„å»ºè§†é¢‘å†…åœ¨ç†è§£åŸºå‡†ï¼ˆVIUBenchï¼‰ï¼ŒéªŒè¯äº†è¿™äº›ä»»åŠ¡çš„éš¾åº¦ï¼Œå¹¶å‘ç°ç°æœ‰çš„æœ€å…ˆè¿›çš„MLLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoSSRåœ¨17ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æé«˜è¶…è¿‡5%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07587', 'title': 'Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces', 'url': 'https://huggingface.co/papers/2511.07587', 'abstract': "The Generative Semantic Workspace (GSW) enhances LLMs' long-context reasoning by creating structured, interpretable representations of evolving situations, outperforming existing methods on the Episodic Memory Benchmark and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the Generative Semantic Workspace (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an Operator, which maps incoming observations to intermediate semantic structures, and a Reconciler, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) huet_episodic_2025 comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to 20\\%. Furthermore, GSW is highly efficient, reducing query-time context tokens by 51\\% compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '26a754ddb1bf04c0', 'authors': ['Shreyas Rajesh', 'Pavan Holur', 'Chenda Duan', 'David Chong', 'Vwani Roychowdhury'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07587.jpg', 'data': {'categories': ['#rag', '#benchmark', '#interpretability', '#agents', '#long_context', '#inference', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Generative Semantic Workspace (GSW) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. GSW ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ»ĞµĞ¹, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Operator Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ğ° Reconciler Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ EpBench GSW Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG Ğ½Ğ° 20% Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 51%, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Empowering LLMs with Human-like Episodic Memory', 'desc': 'The Generative Semantic Workspace (GSW) is a new framework designed to improve the long-context reasoning abilities of Large Language Models (LLMs). It creates structured and interpretable representations of situations, allowing LLMs to better track entities and events over time. GSW outperforms existing methods on the Episodic Memory Benchmark by enhancing coherence and reducing inference time significantly. This approach not only boosts performance but also provides a model for developing LLMs with human-like episodic memory capabilities.'}, 'zh': {'title': 'ç”Ÿæˆè¯­ä¹‰å·¥ä½œåŒºï¼šæå‡é•¿ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'ç”Ÿæˆè¯­ä¹‰å·¥ä½œåŒºï¼ˆGSWï¼‰é€šè¿‡åˆ›å»ºç»“æ„åŒ–å’Œå¯è§£é‡Šçš„æ¼”å˜æƒ…å†µè¡¨ç¤ºï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä¸­çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨æƒ…èŠ‚è®°å¿†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚GSWåŒ…æ‹¬ä¸€ä¸ªæ“ä½œå™¨ï¼Œå°†è¾“å…¥è§‚å¯Ÿæ˜ å°„åˆ°ä¸­é—´è¯­ä¹‰ç»“æ„ï¼Œä»¥åŠä¸€ä¸ªè°ƒå’Œå™¨ï¼Œå°†è¿™äº›ç»“æ„æ•´åˆåˆ°ä¸€ä¸ªæŒä¹…çš„å·¥ä½œåŒºä¸­ï¼Œä»¥ç¡®ä¿æ—¶é—´ã€ç©ºé—´å’Œé€»è¾‘çš„ä¸€è‡´æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒGSWä¸ºèµ‹äºˆLLMsäººç±»èˆ¬çš„æƒ…èŠ‚è®°å¿†æä¾›äº†å…·ä½“è“å›¾ï¼Œæ¨åŠ¨äº†æ›´å¼ºå¤§çš„æ™ºèƒ½ä½“çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07885', 'title': 'Intelligence per Watt: Measuring Intelligence Efficiency of Local AI', 'url': 'https://huggingface.co/papers/2511.07885', 'abstract': 'Local inference using small language models on accelerators can accurately handle many real-world queries, significantly reducing demand on centralized cloud infrastructure, as measured by intelligence per watt.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals 3 findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '1966e52e54e6ac4d', 'authors': ['Jon Saad-Falcon', 'Avanika Narayan', 'Hakki Orhun Akengin', 'J. Wes Griffin', 'Herumb Shandilya', 'Adrian Gamarra Lafuente', 'Medhya Goel', 'Rebecca Joseph', 'Shlok Natarajan', 'Etash Kumar Guha', 'Shang Zhu', 'Ben Athiwaratkun', 'John Hennessy', 'Azalia Mirhoseini', 'Christopher RÃ©'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07885.jpg', 'data': {'categories': ['#benchmark', '#small_models', '#open_source', '#optimization', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ğº ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ñƒ: Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ğ´Ğ¾ 20 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° Ğ²Ğ°Ñ‚Ñ‚' (IPW) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 20+ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 8 ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° 88.7% Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° 1.4x Ğ²Ñ‹ÑˆĞµ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ…. Ğ—Ğ° Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ 2023-2025 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° IPW ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ°ÑÑŒ Ğ² 5.3 Ñ€Ğ°Ğ·Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."}, 'en': {'title': 'Empowering Local Inference: Reducing Cloud Dependency with Small LMs', 'desc': 'This paper explores the potential of using small language models (LMs) for local inference on devices like laptops, which can efficiently handle real-world queries. It introduces a new metric called intelligence per watt (IPW) to evaluate the performance and energy efficiency of these models compared to traditional cloud-based systems. The study shows that local LMs can accurately respond to 88.7% of queries and that their efficiency has significantly improved over time. The findings suggest that local inference can effectively reduce reliance on centralized cloud infrastructure, making it a viable alternative for processing language tasks.'}, 'zh': {'title': 'æœ¬åœ°æ¨ç†ï¼šæ™ºèƒ½ä¸æ•ˆç‡çš„æ–°å¹³è¡¡', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹åœ¨æœ¬åœ°åŠ é€Ÿå™¨ä¸Šè¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå‡†ç¡®å¤„ç†è®¸å¤šç°å®ä¸–ç•Œçš„æŸ¥è¯¢ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å¯¹é›†ä¸­å¼äº‘åŸºç¡€è®¾æ–½çš„éœ€æ±‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šä¸å‰æ²¿æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”æœ¬åœ°åŠ é€Ÿå™¨èƒ½å¤Ÿä»¥äº¤äº’å»¶è¿Ÿè¿è¡Œè¿™äº›æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥æ¯ç“¦ç‰¹æ™ºèƒ½ï¼ˆIPWï¼‰ä½œä¸ºè¯„ä¼°æœ¬åœ°æ¨ç†èƒ½åŠ›å’Œæ•ˆç‡çš„æŒ‡æ ‡ï¼Œç ”ç©¶å‘ç°æœ¬åœ°æ¨¡å‹åœ¨å¤„ç†æŸ¥è¯¢æ—¶çš„å‡†ç¡®ç‡é«˜è¾¾88.7%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜æœ¬åœ°æ¨ç†å¯ä»¥æœ‰æ•ˆåœ°é‡æ–°åˆ†é…å¯¹é›†ä¸­åŸºç¡€è®¾æ–½çš„éœ€æ±‚ï¼Œæä¾›äº†ä¼˜åŒ–çš„ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08043', 'title': 'DynaAct: Large Language Model Reasoning with Dynamic Action Spaces', 'url': 'https://huggingface.co/papers/2511.08043', 'abstract': 'A new framework, DynaAct, automatically constructs a compact action space using large language models and submodular functions to enhance sequential reasoning in complex problem-solving scenarios, improving performance while maintaining efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named DynaAct for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '868a7bfc42d8fb76', 'authors': ['Xueliang Zhao', 'Wei Wu', 'Jian Guan', 'Qintong Li', 'Lingpeng Kong'], 'affiliations': ['Ant Group', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08043.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#agents', '#training', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº DynaAct Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒĞ±Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ€Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ–Ğ°Ğ´Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'DynaAct: Smart Action Space for Efficient Decision-Making', 'desc': 'The paper introduces DynaAct, a new framework designed to automatically create a compact action space for sequential decision-making. By leveraging large language models, DynaAct estimates a proxy for the complete action space, capturing essential patterns from a wide range of complex reasoning problems. It employs a submodular function to evaluate candidate actions based on their usefulness and diversity, using a greedy algorithm to select the best options. The results show that DynaAct enhances performance in problem-solving tasks while ensuring efficient inference with minimal latency.'}, 'zh': {'title': 'DynaActï¼šé«˜æ•ˆæ„å»ºç´§å‡‘åŠ¨ä½œç©ºé—´çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DynaActæ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå­æ¨¡å—å‡½æ•°è‡ªåŠ¨æ„å»ºç´§å‡‘çš„åŠ¨ä½œç©ºé—´ï¼Œä»¥å¢å¼ºå¤æ‚é—®é¢˜è§£å†³ä¸­çš„é¡ºåºæ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æå–åœ¨å¤šæ ·åŒ–å¤æ‚æ¨ç†é—®é¢˜ä¸­è§‚å¯Ÿåˆ°çš„ä¸€èˆ¬è‰å›¾ï¼Œæ¥ä¼°è®¡å®Œæ•´åŠ¨ä½œç©ºé—´çš„ä»£ç†ã€‚ç„¶åï¼Œåˆ©ç”¨å­æ¨¡å—å‡½æ•°è¯„ä¼°å€™é€‰åŠ¨ä½œçš„æ•ˆç”¨å’Œå¤šæ ·æ€§ï¼Œå¹¶é‡‡ç”¨è´ªå¿ƒç®—æ³•é€‰æ‹©æœ€ä½³å€™é€‰é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynaActæ˜¾è‘—æé«˜äº†æ•´ä½“æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆæ¨ç†ï¼Œä¸”æ²¡æœ‰å¼•å…¥æ˜¾è‘—çš„å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06428', 'title': "Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective", 'url': 'https://huggingface.co/papers/2511.06428', 'abstract': "LLMs impact software development by offering benefits like maintaining workflow and fostering entrepreneurship, but also pose risks to developers' well-being and reputation, necessitating careful management and adoption strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'fd5c9d9c51b94cfb', 'authors': ['Samuel Ferino', 'Rashina Hoda', 'John Grundy', 'Christoph Treude'], 'affiliations': ['Faculty of Information Technology, Monash University', 'School of Computing and Information Systems, Singapore Management University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06428.jpg', 'data': {'categories': [], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ²: ĞºĞ°Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ñ 22 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ (Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ², Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°), Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ (Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°, Ñ€ĞµĞ¿ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ñ†Ğ¸Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¼, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ LLM Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ´ĞµÑ€Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ IT-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Balancing Benefits and Risks of LLMs in Software Development', 'desc': "This paper explores the dual impact of Large Language Models (LLMs) on software development, highlighting both their advantages and disadvantages. The study reveals that LLMs can enhance workflow, improve developers' understanding, and encourage entrepreneurship, but they also pose risks to developers' mental health and professional reputation. Through interviews with software practitioners, the research identifies best practices for managing the integration of LLMs in development processes. Ultimately, the findings emphasize the need for careful consideration of the trade-offs involved in adopting LLMs in software teams and organizations."}, 'zh': {'title': 'å¹³è¡¡å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ©å¼Šï¼ŒåŠ©åŠ›è½¯ä»¶å¼€å‘', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å¼€å‘ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„å½±å“ï¼Œæ—¢æœ‰åŠ©äºç»´æŠ¤å¼€å‘æµç¨‹å’Œä¿ƒè¿›åˆ›ä¸šï¼Œä¹Ÿå¯èƒ½å¯¹å¼€å‘è€…çš„å¿ƒç†å¥åº·å’Œå£°èª‰é€ æˆé£é™©ã€‚å› æ­¤ï¼Œå¼€å‘è€…éœ€è¦è°¨æ…ç®¡ç†å’Œé‡‡ç”¨è¿™äº›æŠ€æœ¯ã€‚æˆ‘ä»¬é€šè¿‡å¯¹22ä½è½¯ä»¶ä»ä¸šè€…çš„è®¿è°ˆï¼Œåˆ†æäº†LLMsçš„ä¼˜ç¼ºç‚¹ï¼ŒåŒ…æ‹¬å¯¹ä¸ªäººã€å›¢é˜Ÿã€ç»„ç»‡å’Œç¤¾ä¼šå±‚é¢çš„å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºè½¯ä»¶å›¢é˜Ÿé¢†å¯¼å’ŒITç»ç†æä¾›äº†åœ¨ç‰¹å®šç¯å¢ƒä¸­è¯„ä¼°LLMså¯è¡Œæ€§çš„æœ‰ä»·å€¼è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05650', 'title': 'Optimizing Diversity and Quality through Base-Aligned Model Collaboration', 'url': 'https://huggingface.co/papers/2511.05650', 'abstract': "BACo, a token-level collaboration framework for LLMs, enhances output diversity and quality through dynamic routing without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '10bfab1d0dd4dc6b', 'authors': ['Yichen Wang', 'Chenghao Yang', 'Tenghao Huang', 'Muhao Chen', 'Jonathan May', 'Mina Lee'], 'affiliations': ['University of California, Davis', 'University of Chicago', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05650.jpg', 'data': {'categories': ['#optimization', '#alignment', '#architecture', '#training', '#inference'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ BACo â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ĞµÑ‘ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚, Ğ¸Ğ· ĞºĞ°ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ€Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½, Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² (Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, prompt engineering, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°), BACo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 21,3% ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'BACo: Boosting Diversity and Quality in LLMs Through Dynamic Collaboration', 'desc': 'BACo is a novel framework designed for large language models (LLMs) that enhances the diversity and quality of generated outputs. It operates at the token level, dynamically routing between a base model and its aligned counterpart based on prediction uncertainty and semantic roles. Unlike traditional methods that often compromise quality for diversity, BACo achieves both simultaneously in a single pass. The framework has shown significant improvements in various generation tasks, outperforming existing methods in both diversity and quality metrics.'}, 'zh': {'title': 'BACoï¼šæå‡å¤šæ ·æ€§ä¸è´¨é‡çš„åä½œæ¡†æ¶', 'desc': 'BACoæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»¤ç‰Œçº§åä½œæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±æé«˜è¾“å‡ºçš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œè€Œä¸é™ä½æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨æ¨ç†æ—¶åŠ¨æ€ç»“åˆåŸºç¡€LLMå’Œå…¶å¯¹é½æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚BACoé‡‡ç”¨è·¯ç”±ç­–ç•¥ï¼Œæ ¹æ®æ¯ä¸ªä»¤ç‰Œçš„é¢„æµ‹ä¸ç¡®å®šæ€§å’Œè¯­ä¹‰è§’è‰²ï¼Œå†³å®šä»å“ªä¸ªæ¨¡å‹è§£ç ã€‚ä¸ä»¥å¾€çš„å¤šæ ·æ€§æå‡æ–¹æ³•ç›¸æ¯”ï¼ŒBACoåœ¨å•æ¬¡æ¨ç†ä¸­å®ç°äº†é«˜å¤šæ ·æ€§å’Œé«˜è´¨é‡çš„å¹³è¡¡ï¼Œå¹¶ä¸”æä¾›äº†å¼ºå¤§çš„å¯æ§æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08029', 'title': 'BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives', 'url': 'https://huggingface.co/papers/2511.08029', 'abstract': 'BiCA uses citation links to improve biomedical retrieval models by providing effective hard negatives, enhancing zero-shot and long-tailed performance with minimal fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'dd8b545b8edb6041', 'authors': ['Aarush Sinha', 'Pavan Kumar S', 'Roshan Balaji', 'Nirav Pravinbhai Bhatt'], 'affiliations': ['BioSystems Engineering and Control (BiSECt) Lab, Department of Biotechnology and Wadhwani School of Data Science and AI, Indian Institute of Technology (IIT) Madras, Tamil Nadu India', 'The Centre for Integrative Biology and Systems medicinE (IBSE), IIT Madras, Chennai, Tamil Nadu, India', 'Vellore Institute of Technology (VIT) Chennai, India'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08029.jpg', 'data': {'categories': ['#rag', '#benchmark', '#small_models', '#optimization', '#training', '#dataset', '#science'], 'emoji': 'ğŸ“š', 'ru': {'title': 'Ğ¦Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ BiCA â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑÑ‹Ğ»Ğ¾Ğº Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· 20,000 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ PubMed ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ñ…Ğ²Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GTE_small Ğ¸ GTE_Base. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Harnessing Citations for Better Biomedical Retrieval', 'desc': 'BiCA introduces a novel method for enhancing biomedical retrieval models by utilizing citation links to generate effective hard negatives. This approach addresses the challenges of hard-negative mining in the biomedical field, where distinguishing relevant documents can be difficult. By leveraging citation information from 20,000 PubMed articles, BiCA improves the performance of small dense retrievers with minimal fine-tuning. The results show significant advancements in zero-shot retrieval and long-tailed topic performance, demonstrating the effectiveness of citation-aware hard negatives in domain-specific applications.'}, 'zh': {'title': 'åˆ©ç”¨å¼•ç”¨é“¾æ¥æå‡ç”Ÿç‰©åŒ»å­¦æ£€ç´¢æ€§èƒ½', 'desc': 'BiCAæ˜¯ä¸€ç§åˆ©ç”¨å¼•ç”¨é“¾æ¥æ¥æ”¹è¿›ç”Ÿç‰©åŒ»å­¦æ£€ç´¢æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡æä¾›æœ‰æ•ˆçš„å›°éš¾è´Ÿæ ·æœ¬ï¼Œå¢å¼ºäº†é›¶æ ·æœ¬å’Œé•¿å°¾æ€§èƒ½ï¼Œä¸”åªéœ€æœ€å°çš„å¾®è°ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†20,000ç¯‡PubMedæ–‡ç« ä¸­çš„å¼•ç”¨ä¿¡æ¯ï¼Œå¸®åŠ©è®­ç»ƒé¢†åŸŸç‰¹å®šçš„å°å‹å¯†é›†æ£€ç´¢å™¨ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ–‡æ¡£é“¾æ¥ç»“æ„ç”Ÿæˆé«˜ä¿¡æ¯é‡çš„è´Ÿæ ·æœ¬ï¼Œå¯ä»¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”æ•°æ®æ•ˆç‡é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.05489', 'title': 'TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning', 'url': 'https://huggingface.co/papers/2511.05489', 'abstract': 'TimeSearch-R uses interleaved text-video thinking with GRPO-CSV to optimize temporal search in videos, improving performance on long-form video understanding benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '76aa50eba07d81af', 'authors': ['Junwen Pan', 'Qizhe Zhang', 'Rui Zhang', 'Ming Lu', 'Xin Wan', 'Yuan Zhang', 'Chang Liu', 'Qi She'], 'affiliations': ['ByteDance', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.05489.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#video', '#long_context', '#multimodal', '#training', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TimeSearch-R Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ»Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (GRPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° end-to-end, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ GRPO-CSV â€” Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ GRPO Ñ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ½Ğ° LongVideoBench.'}, 'en': {'title': 'Optimizing Video Search with Interleaved Text-Video Reasoning', 'desc': 'TimeSearch-R is a novel approach that enhances temporal search in videos by integrating text and video reasoning through reinforcement learning. It addresses the limitations of traditional methods that rely on hand-crafted search processes by employing Group Relative Policy Optimization with Completeness Self-Verification (GRPO-CSV). This method ensures that the search for relevant video frames is optimized and verified, leading to better logical reasoning and exploration of video content. The results show significant improvements in various benchmarks, establishing new state-of-the-art performance in long-form video understanding tasks.'}, 'zh': {'title': 'ä¼˜åŒ–è§†é¢‘ç†è§£çš„æ—¶é—´æœç´¢æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTimeSearch-Rçš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è§†é¢‘ä¸­çš„æ—¶é—´æœç´¢ã€‚é€šè¿‡å°†æ–‡æœ¬å’Œè§†é¢‘çš„æ€ç»´äº¤é”™ç»“åˆï¼ŒTimeSearch-Råˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æå‡è§†é¢‘ç†è§£çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO-CSVï¼‰ï¼Œä»¥ç¡®ä¿æœç´¢åˆ°çš„è§†é¢‘å¸§çš„å……åˆ†æ€§ï¼Œä»è€Œæ”¹å–„è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeSearch-Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ—¶é—´æœç´¢å’Œé•¿è§†é¢‘ç†è§£çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08892', 'title': 'Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds', 'url': 'https://huggingface.co/papers/2511.08892', 'abstract': "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", 'score': 194, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '979bf2a9c14bc65e', 'authors': ['Weihao Tan', 'Xiangyang Li', 'Yunhao Fang', 'Heyuan Yao', 'Shi Yan', 'Hao Luo', 'Tenglong Ao', 'Huihui Li', 'Hongbin Ren', 'Bairen Yi', 'Yujia Qin', 'Bo An', 'Libin Liu', 'Guang Shi'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08892.jpg', 'data': {'categories': ['#games', '#open_source', '#3d', '#transfer_learning', '#cv', '#agents', '#multimodal'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ¼ĞµĞ¶Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Lumine â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ°Ñ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ†Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¸Ğ³Ñ€Ğµ Genshin Impact Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑĞ¶ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ½ÑƒĞ»ĞµĞ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ (zero-shot) Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Lumine: A Generalist Agent for Real-Time 3D Missions', 'desc': 'Lumine is a vision-language model designed to act as a generalist agent capable of completing complex tasks in real-time within 3D open-world environments. It integrates perception, reasoning, and action in a seamless manner, processing visual data to generate precise actions at a high frequency. Trained on the game Genshin Impact, Lumine can follow natural language instructions and perform various tasks, demonstrating human-like efficiency. Notably, it exhibits zero-shot cross-game generalization, successfully completing missions in different games without additional training, showcasing its versatility and adaptability.'}, 'zh': {'title': 'Lumineï¼šé€šç”¨æ™ºèƒ½ä½“çš„æœªæ¥', 'desc': 'Lumineæ˜¯ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®æ—¶å®Œæˆä»»åŠ¡ï¼Œè¡¨ç°å‡ºç±»ä¼¼äººç±»çš„æ•ˆç‡ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§äººæ€§åŒ–çš„äº¤äº’æ–¹å¼ï¼Œå°†æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ç»Ÿä¸€åœ¨ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­ã€‚Lumineåœ¨ã€ŠåŸç¥ã€‹ä¸­ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆå®Œæˆäº”å°æ—¶çš„ä¸»çº¿ä»»åŠ¡ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¤šç§ä»»åŠ¡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒLumineåœ¨ä¸åŒæ¸¸æˆä¹‹é—´å±•ç°å‡ºå¼ºå¤§çš„é›¶-shotè·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¼€æ”¾ç¯å¢ƒä¸­ä½œä¸ºé€šç”¨æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08923', 'title': 'TiDAR: Think in Diffusion, Talk in Autoregression', 'url': 'https://huggingface.co/papers/2511.08923', 'abstract': 'TiDAR combines diffusion and autoregressive models to achieve high throughput and quality in language generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.', 'score': 113, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '99289bf144839df6', 'authors': ['Jingyu Liu', 'Xin Dong', 'Zhifan Ye', 'Rishabh Mehta', 'Yonggan Fu', 'Vartika Singh', 'Jan Kautz', 'Ce Zhang', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08923.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'TiDAR Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (ÑÑ‚Ğ°Ğ¿ Thinking) Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² (ÑÑ‚Ğ°Ğ¿ Talking), Ğ²ÑĞµ ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ GPU-Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², TiDAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ² 4.71-5.91 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ½Ğ¸Ğ¼Ğ¸.'}, 'en': {'title': 'TiDAR: Bridging Speed and Quality in Language Generation', 'desc': 'TiDAR is a novel language generation model that merges diffusion and autoregressive techniques to enhance both speed and quality. By utilizing a hybrid architecture, it drafts tokens using diffusion methods and samples final outputs with autoregressive processes in a single forward pass. This approach maximizes GPU utilization and improves throughput while maintaining high-quality outputs, effectively addressing the limitations of existing models. Extensive evaluations show that TiDAR significantly outperforms traditional autoregressive models and diffusion variants in both efficiency and quality, achieving a remarkable increase in token generation speed.'}, 'zh': {'title': 'TiDARï¼šé«˜æ•ˆé«˜è´¨é‡çš„è¯­è¨€ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'TiDARæ˜¯ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹çš„è¯­è¨€ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜ååé‡å’Œé«˜è´¨é‡çš„æ–‡æœ¬ç”Ÿæˆã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿå¹¶è¡Œç”Ÿæˆï¼Œè€Œè‡ªå›å½’æ¨¡å‹åœ¨è´¨é‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚TiDARé€šè¿‡åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­ä½¿ç”¨ç‰¹æ®Šè®¾è®¡çš„ç»“æ„åŒ–æ³¨æ„åŠ›æ©ç ï¼Œå¹³è¡¡äº†è‰æ‹Ÿå’ŒéªŒè¯èƒ½åŠ›ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒTiDARåœ¨ç”Ÿæˆå’Œä¼¼ç„¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08217', 'title': 'MADD: Multi-Agent Drug Discovery Orchestra', 'url': 'https://huggingface.co/papers/2511.08217', 'abstract': 'MADD, a multi-agent system integrating large language models and specialized models, streamlines hit identification in early drug discovery with superior performance and accessibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.', 'score': 55, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '2cfc090b8623b2f3', 'authors': ['Gleb V. Solovev', 'Alina B. Zhidkovskaya', 'Anastasia Orlova', 'Nina Gubina', 'Anastasia Vepreva', 'Rodion Golovinskii', 'Ilya Tonkii', 'Ivan Dubrovsky', 'Ivan Gurev', 'Dmitry Gilemkhanov', 'Denis Chistiakov', 'Timur A. Aliev', 'Ivan Poddiakov', 'Galina Zubkova', 'Ekaterina V. Skorb', 'Vladimir Vinogradov', 'Alexander Boukhanovsky', 'Nikolay Nikitin', 'Andrei Dmitrenko', 'Anna Kalyuzhnaya', 'Andrey Savchenko'], 'affiliations': ['D ONE AG, Zurich, Switzerland', 'HSE University, Moscow, Russia', 'ITMO University, Saint Petersburg, Russia', 'Sber AI Lab, Moscow, Russia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08217.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#open_source', '#agents', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²', 'desc': 'MADD â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ MADD Ğ½Ğ° ÑĞµĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ°Ñ€Ğ¼Ğ°ĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ².'}, 'en': {'title': 'MADD: Revolutionizing Drug Discovery with AI Agents', 'desc': 'MADD is a multi-agent system that enhances hit identification in early drug discovery by integrating large language models (LLMs) with specialized models. It simplifies the process by allowing researchers to create customized pipelines through natural language queries, making advanced AI tools more accessible. The system employs four coordinated agents to efficiently manage tasks like compound generation and screening. In evaluations across seven drug discovery cases, MADD outperformed existing LLM-based methods, paving the way for AI-driven drug design and providing a new benchmark for future research.'}, 'zh': {'title': 'MADDï¼šè¯ç‰©å‘ç°çš„æ–°æ™ºèƒ½åŠ©æ‰‹', 'desc': 'MADDæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ä¸šæ¨¡å‹ï¼Œæ—¨åœ¨ç®€åŒ–æ—©æœŸè¯ç‰©å‘ç°ä¸­çš„å‘½ä¸­è¯†åˆ«è¿‡ç¨‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ„å»ºå’Œæ‰§è¡Œå®šåˆ¶çš„å‘½ä¸­è¯†åˆ«æµç¨‹ï¼Œæå‡äº†æ•ˆç‡å’Œå¯è®¿é—®æ€§ã€‚MADDä½¿ç”¨å››ä¸ªåè°ƒçš„æ™ºèƒ½ä½“æ¥å¤„ç†æ–°åŒ–åˆç‰©ç”Ÿæˆå’Œç­›é€‰çš„å…³é”®å­ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸ƒä¸ªè¯ç‰©å‘ç°æ¡ˆä¾‹ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜ä¸ºè¶…è¿‡ä¸‰ç™¾ä¸‡ä¸ªåŒ–åˆç‰©å¼•å…¥äº†æ–°çš„æŸ¥è¯¢-åˆ†å­å¯¹å’Œå¯¹æ¥åˆ†æ•°åŸºå‡†ï¼Œæ¨åŠ¨äº†è¯ç‰©è®¾è®¡çš„æ™ºèƒ½åŒ–æœªæ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08633', 'title': 'Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising', 'url': 'https://huggingface.co/papers/2511.08633', 'abstract': "Time-to-Move (TTM) is a plug-and-play framework for motion- and appearance-controlled video generation using image-to-video (I2V) diffusion models, offering precise control over video content without requiring additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.", 'score': 53, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'eab63b7df1ce2181', 'authors': ['Assaf Singer', 'Noam Rotstein', 'Amir Mann', 'Ron Kimmel', 'Or Litany'], 'affiliations': ['NVIDIA', 'Technion Israel Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08633.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion', '#multimodal', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Time-to-Move â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ±Ñ‹Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑĞ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… ĞºĞ°Ğº Ğ³Ñ€ÑƒĞ±Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ dual-clock denoising, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Effortless Motion and Appearance Control in Video Generation', 'desc': 'Time-to-Move (TTM) is a novel framework that enhances video generation by allowing users to control both motion and appearance without needing additional training. It leverages image-to-video (I2V) diffusion models and utilizes simple user manipulations to create reference animations that guide the video generation process. TTM introduces a dual-clock denoising technique that ensures strong alignment in specified motion areas while maintaining flexibility in others, achieving a balance between user intent and natural movement. Extensive testing shows that TTM performs comparably or better than traditional methods that require extensive training, offering precise control over video content.'}, 'zh': {'title': 'ç²¾å‡†æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„å…¨æ–°æ¡†æ¶', 'desc': 'Time-to-Move (TTM) æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆå—è¿åŠ¨å’Œå¤–è§‚æ§åˆ¶çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è§†é¢‘å†…å®¹ï¼Œå…‹æœäº†ç°æœ‰åŸºäºå›¾åƒå’Œæ–‡æœ¬çš„æ¡ä»¶ç”Ÿæˆåœ¨è¿åŠ¨æ§åˆ¶ä¸Šçš„ä¸è¶³ã€‚TTM é€šè¿‡ç”¨æˆ·å‹å¥½çš„æ“ä½œè·å–ç²—ç•¥çš„å‚è€ƒåŠ¨ç”»ï¼Œå¹¶å°†å…¶ä½œä¸ºç²—ç•¥è¿åŠ¨çº¿ç´¢ï¼Œé€‚åº”è§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTM åœ¨çœŸå®æ„Ÿå’Œè¿åŠ¨æ§åˆ¶æ–¹é¢ä¸ç°æœ‰çš„è®­ç»ƒåŸºç¡€æ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶æä¾›äº†åƒç´ çº§çš„å¤–è§‚æ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07464', 'title': 'Motif 2 12.7B technical report', 'url': 'https://huggingface.co/papers/2511.07464', 'abstract': 'Motif-2-12.7B combines architectural innovations and system optimizations to enhance efficiency and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.', 'score': 38, 'issue_id': 1, 'pub_date': '2025-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '9d81b847377089b9', 'authors': ['Junghwan Lim', 'Sungmin Lee', 'Dongseok Kim', 'Taehyun Kim', 'Eunhwan Park', 'Jeesoo Lee', 'Jeongdoo Lee', 'Junhyeok Lee', 'Wai Ting Cheung', 'Dahye Choi', 'Jaeheui Her', 'Jaeyeon Huh', 'Hanbin Jung', 'Changjin Kang', 'Beomgyu Kim', 'Minjae Kim', 'Taewhan Kim', 'Youngrok Kim', 'Hyukjin Kweon', 'Haesol Lee', 'Kungyu Lee', 'Dongpin Oh', 'Yeongjae Park', 'Bokki Ryu', 'Dongjoo Weon'], 'affiliations': ['Motif Technologies'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07464.jpg', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#training', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Motif-2-12.7B, Ğ½Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Grouped Differential Attention, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¿ÑƒÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¸ ÑˆÑƒĞ¼Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° 5.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° MuonClip Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Efficiency Meets Performance in Language Models', 'desc': 'Motif-2-12.7B is a new large language model that improves efficiency and performance through innovative architecture and system optimizations. It features Grouped Differential Attention (GDA), which enhances how the model processes information by separating important signals from noise. The model is trained on a vast dataset of 5.5 trillion tokens, using a smart scheduling method to gradually adjust the data it learns from. After training, a three-stage fine-tuning process is applied to improve its ability to follow instructions and understand language, allowing it to perform well on various tasks compared to larger models.'}, 'zh': {'title': 'é«˜æ•ˆä¸æ€§èƒ½çš„å®Œç¾ç»“åˆï¼šMotif-2-12.7B', 'desc': 'Motif-2-12.7B æ˜¯ä¸€ç§æ–°å‹çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆæ¶æ„åˆ›æ–°å’Œç³»ç»Ÿä¼˜åŒ–ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†åˆ†ç»„å·®å¼‚æ³¨æ„åŠ›ï¼ˆGDAï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ†ç¦»ä¿¡å·å’Œå™ªå£°ï¼Œä»è€Œæé«˜è¡¨ç¤ºæ•ˆç‡ã€‚å®ƒåœ¨5.5ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–äº†å¤šç§è¯­è¨€ã€æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹é¢†åŸŸï¼Œå¹¶ä½¿ç”¨é€æ­¥è°ƒæ•´çš„æ•°æ®è°ƒåº¦å™¨æ¥ä¼˜åŒ–æ•°æ®ç»„æˆæ¯”ä¾‹ã€‚ç»è¿‡ä¸‰é˜¶æ®µçš„ç›‘ç£å¾®è°ƒï¼ŒMotif-2-12.7B åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ç²¾å¿ƒè®¾è®¡çš„æ¶æ„æ‰©å±•å’Œä¼˜åŒ–è®­ç»ƒæ–¹æ³•å¯ä»¥ä¸æ›´å¤§æ¨¡å‹çš„èƒ½åŠ›ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09515', 'title': 'WMPO: World Model-based Policy Optimization for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2511.09515', 'abstract': 'WMPO, a pixel-based world-model framework for VLA RL, enhances sample efficiency, performance, self-correction, and generalization in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.', 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '4c71f55c00430bbb', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rl', '#multimodal', '#training', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ WMPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¸Ñ€Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ VLA, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ĞµĞ±-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ on-policy Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ off-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'WMPO: Revolutionizing Robotic Learning with Pixel-Based World Models', 'desc': 'WMPO is a new framework designed to improve reinforcement learning (RL) for robotic manipulation tasks by using pixel-based world models. Unlike traditional methods that rely on expert demonstrations, WMPO allows robots to learn from their own experiences, enhancing their ability to self-correct and adapt. This approach significantly reduces the number of interactions needed with the real environment, making learning more efficient. Through extensive testing, WMPO has shown to outperform existing methods in terms of performance, generalization, and the ability to learn continuously over time.'}, 'zh': {'title': 'WMPOï¼šæå‡æœºå™¨äººæ“ä½œçš„æ ·æœ¬æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'WMPOæ˜¯ä¸€ç§åŸºäºåƒç´ çš„ä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä¸ä¸çœŸå®ç¯å¢ƒäº’åŠ¨çš„æƒ…å†µä¸‹è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨çœŸå®æœºå™¨äººä¸Šé«˜æ ·æœ¬å¤æ‚åº¦çš„é—®é¢˜ã€‚WMPOä¸“æ³¨äºåƒç´ çº§é¢„æµ‹ï¼Œä½¿å¾—â€œæƒ³è±¡â€çš„è½¨è¿¹ä¸é¢„è®­ç»ƒçš„VLAç‰¹å¾å¯¹é½ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWMPOåœ¨æ ·æœ¬æ•ˆç‡ã€æ•´ä½“æ€§èƒ½ã€è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.09148', 'title': 'LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls', 'url': 'https://huggingface.co/papers/2511.09148', 'abstract': "A fully automated data evolution framework, LoopTool, enhances tool-use capabilities of Large Language Models by iteratively refining data and model through a closed-loop process.  \t\t\t\t\tAI-generated summary \t\t\t\t Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.", 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '731a46a95599a2a7', 'authors': ['Kangning Zhang', 'Wenxiang Jiao', 'Kounianhua Du', 'Yuan Lu', 'Weiwen Liu', 'Weinan Zhang', 'Yong Yu'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09148.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#data', '#training', '#synthetic'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ—Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM', 'desc': 'LoopTool â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ—Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 8B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoopTool, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 32B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… BFCL-v3 Ğ¸ ACEBench Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ ĞºĞ»Ğ°ÑÑĞµ.'}, 'en': {'title': 'Revolutionizing LLMs with Closed-Loop Data Evolution', 'desc': 'LoopTool is a novel framework designed to improve the performance of Large Language Models (LLMs) by integrating data generation and model training into a closed-loop system. It addresses the limitations of traditional static data pipelines by allowing the model to iteratively refine both its training data and its own capabilities. The framework includes three key components: Greedy Capability Probing to identify strengths and weaknesses, Judgement-Guided Label Verification to correct data errors, and Error-Driven Data Expansion to create new training samples based on model failures. This approach not only enhances training efficiency but also leads to superior performance on benchmark tasks compared to larger models using conventional methods.'}, 'zh': {'title': 'é—­ç¯è‡ªæˆ‘ä¼˜åŒ–ï¼Œæå‡LLMså·¥å…·ä½¿ç”¨èƒ½åŠ›', 'desc': 'LoopToolæ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®æ¼”åŒ–æ¡†æ¶ï¼Œé€šè¿‡é—­ç¯è¿‡ç¨‹è¿­ä»£åœ°ä¼˜åŒ–æ•°æ®å’Œæ¨¡å‹ï¼Œä»è€Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—å®ç°æ•°æ®å’Œæ¨¡å‹çš„ç²¾ç»†åŒ–ï¼šè´ªå©ªèƒ½åŠ›æ¢æµ‹ï¼ˆGCPï¼‰è¯Šæ–­æ¨¡å‹çš„æŒæ¡å’Œå¤±è´¥èƒ½åŠ›ï¼›åˆ¤æ–­å¼•å¯¼çš„æ ‡ç­¾éªŒè¯ï¼ˆJGLVï¼‰åˆ©ç”¨å¼€æºè¯„åˆ¤æ¨¡å‹å‘ç°å¹¶çº æ­£æ ‡æ³¨é”™è¯¯ï¼Œé€æ­¥å‡€åŒ–æ•°æ®é›†ï¼›é”™è¯¯é©±åŠ¨çš„æ•°æ®æ‰©å±•ï¼ˆEDDEï¼‰åŸºäºè¯†åˆ«çš„å¤±è´¥ç”Ÿæˆæ–°çš„æŒ‘æˆ˜æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨LoopToolè®­ç»ƒçš„8Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†å…¶32Bæ•°æ®ç”Ÿæˆå™¨ï¼Œå±•ç¤ºäº†é—­ç¯è‡ªæˆ‘ä¼˜åŒ–æ•°æ®ç®¡é“å¯¹LLMså·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06251', 'title': 'WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation', 'url': 'https://huggingface.co/papers/2511.06251', 'abstract': 'WebVIA is an agentic framework for generating executable and interactive UI code from design mockups, improving stability and accuracy over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://zheny2751-dotcom.github.io/webvia.github.io/{https://webvia.github.io}.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'a6c8eaf212a56433', 'authors': ['Mingde Xu', 'Zhen Yang', 'Wenyi Hong', 'Lihang Pan', 'Xinyue Fan', 'Yan Wang', 'Xiaotao Gu', 'Bin Xu', 'Jie Tang'], 'affiliations': ['Faculty of Mathematics, University of Waterloo', 'The Knowledge Engineering Group (KEG), Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06251.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#cv', '#agents', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¸Ğ· Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°', 'desc': 'WebVIA â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸ UI, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ UI2Code Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ WebVIA-Agent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ². Ğ¢Ğ¾Ğ½ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ WebVIA-UI2Code Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ HTML/CSS/JavaScript ĞºĞ¾Ğ´Ğ° ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… UI-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Transforming Design Mockups into Interactive Code with WebVIA', 'desc': 'WebVIA is a novel framework designed to convert design mockups into interactive user interface (UI) code, enhancing the stability and accuracy of this process. Unlike traditional Vision-Language Models (VLMs) that only produce static layouts, WebVIA introduces an exploration agent to capture dynamic UI states and a UI2Code model that generates executable code. Additionally, it includes a validation module to ensure the generated code is interactive. Experiments show that WebVIA outperforms existing models in both stability and accuracy for UI exploration and code generation.'}, 'zh': {'title': 'WebVIAï¼šäº¤äº’å¼ç”¨æˆ·ç•Œé¢ä»£ç ç”Ÿæˆçš„æ–°æ¡†æ¶', 'desc': 'WebVIAæ˜¯ä¸€ä¸ªç”¨äºä»è®¾è®¡æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œå’Œäº¤äº’å¼ç”¨æˆ·ç•Œé¢ä»£ç çš„æ¡†æ¶ï¼Œæå‡äº†ç°æœ‰æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šæ¢ç´¢ä»£ç†ã€UIåˆ°ä»£ç æ¨¡å‹å’ŒéªŒè¯æ¨¡å—ã€‚æ¢ç´¢ä»£ç†è´Ÿè´£æ•æ‰å¤šçŠ¶æ€çš„ç”¨æˆ·ç•Œé¢æˆªå›¾ï¼ŒUIåˆ°ä»£ç æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œçš„äº¤äº’å¼ä»£ç ï¼Œè€ŒéªŒè¯æ¨¡å—åˆ™ç¡®ä¿ç”Ÿæˆä»£ç çš„äº¤äº’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebVIAåœ¨ç”¨æˆ·ç•Œé¢æ¢ç´¢æ–¹é¢çš„è¡¨ç°ä¼˜äºä¸€èˆ¬ç›®çš„çš„ä»£ç†ï¼Œä¸”ç”Ÿæˆçš„ä»£ç åœ¨äº¤äº’æ€§å’Œå¯æ‰§è¡Œæ€§ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06805', 'title': 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning', 'url': 'https://huggingface.co/papers/2511.06805', 'abstract': "A proposed Mathematical Self-Evolving framework iteratively refines multimodal large language models through inference, reflection, and reward-based feedback, achieving superior performance in mathematical reasoning tasks compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\method, a Mathematical Self-Evolving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/.", 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '815ba7d57552083e', 'authors': ['Jinhao Chen', 'Zhen Yang', 'Jianxin Shi', 'Tianyu Wo', 'Jie Tang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'School of Software, Beihang University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06805.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#multimodal', '#training', '#math', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Mathematical Self-Evolving, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (ORM) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Evolving Math Models for Superior Reasoning', 'desc': 'The paper introduces a new framework called Mathematical Self-Evolving (MSE) for improving multimodal large language models (MLLMs) in mathematical reasoning tasks. Unlike traditional methods that rely on static datasets, MSE uses an iterative process of inference, reflection, and reward-based feedback to enhance model performance. This approach allows the model to learn from its previous reasoning attempts and adapt to more complex problems. Experimental results show that MSE significantly outperforms existing models, demonstrating its effectiveness in tackling challenging mathematical benchmarks.'}, 'zh': {'title': 'æ•°å­¦è‡ªæˆ‘è¿›åŒ–ï¼šæå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°å­¦è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†ã€åæ€å’ŒåŸºäºå¥–åŠ±çš„åé¦ˆï¼Œè¿­ä»£åœ°ä¼˜åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ä¾èµ–äºé™æ€çš„æ•™å¸ˆæ¨¡å‹æ•°æ®é›†ï¼Œé™åˆ¶äº†æ¨¡å‹å¯¹å¤æ‚é—®é¢˜çš„é€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥æ­£ç¡®çš„æ¨ç†è·¯å¾„å’Œä¸“é—¨çš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰è¿›è¡Œè¿­ä»£å¾®è°ƒï¼Œä»è€Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œè¶…è¶Šäº†é¢†å…ˆçš„å¼€æºå¤šæ¨¡æ€æ•°å­¦æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06101', 'title': 'Adapting Web Agents with Synthetic Supervision', 'url': 'https://huggingface.co/papers/2511.06101', 'abstract': 'SynthAgent is a synthetic supervision framework that refines both tasks and trajectories to improve data quality and enhance web agent adaptation to new websites.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'bc445af93f363e06', 'authors': ['Zhaoyang Wang', 'Yiming Liang', 'Xuchao Zhang', 'Qianhui Wu', 'Siwei Han', 'Anson Bastos', 'Rujia Wang', 'Chetan Bansal', 'Baolin Peng', 'Jianfeng Gao', 'Saravan Rajmohan', 'Huaxiu Yao'], 'affiliations': ['Microsoft', 'Purdue University', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06101.jpg', 'data': {'categories': ['#open_source', '#dataset', '#agents', '#data', '#hallucinations', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'SynthAgent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ fine-tuning Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Web Agent Adaptation with SynthAgent', 'desc': 'SynthAgent is a framework designed to enhance the quality of synthetic data used for training web agents, which often struggle to adapt to new websites. It addresses the common issues of data quality, such as hallucinations in synthesized tasks and noisy trajectories. The framework employs a dual refinement process, first by generating diverse tasks through systematic exploration of web elements, and then by refining these tasks and trajectories based on real observations. By fine-tuning web agents on this improved synthetic data, SynthAgent significantly boosts their adaptability to new environments, outperforming previous methods in synthetic data generation.'}, 'zh': {'title': 'åˆæˆç›‘ç£ï¼Œæå‡ç½‘ç»œä»£ç†é€‚åº”èƒ½åŠ›', 'desc': 'SynthAgent æ˜¯ä¸€ä¸ªåˆæˆç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŒé‡ä¼˜åŒ–ä»»åŠ¡å’Œè½¨è¿¹æ¥æé«˜æ•°æ®è´¨é‡ï¼Œå¢å¼ºç½‘ç»œä»£ç†å¯¹æ–°ç½‘ç«™çš„é€‚åº”èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ç½‘é¡µå…ƒç´ çš„åˆ†ç±»æ¢ç´¢ï¼Œåˆæˆå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œç¡®ä¿å¯¹ç›®æ ‡ç¯å¢ƒçš„æœ‰æ•ˆè¦†ç›–ã€‚åœ¨è½¨è¿¹æ”¶é›†è¿‡ç¨‹ä¸­ï¼Œå½“æ£€æµ‹åˆ°ä¸å®é™…è§‚å¯Ÿçš„å†²çªæ—¶ï¼Œä¼šå¯¹ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œå‡å°‘è™šå‡ä¿¡æ¯å¹¶ä¿æŒä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚æœ€åï¼Œé€šè¿‡å¯¹ç²¾ç‚¼åçš„åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæå‡å¼€æºç½‘ç»œä»£ç†åœ¨ç›®æ ‡ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07499', 'title': 'Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance', 'url': 'https://huggingface.co/papers/2511.07499', 'abstract': 'A new guidance method called Adversarial Sinkhorn Attention Guidance improves diffusion model performance by injecting adversarial costs into self-attention layers, enhancing sample quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'b941ff95191efab4', 'authors': ['Kwanyoung Kim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07499.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Adversarial Sinkhorn Attention Guidance (ASAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¸ self-attention, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Sinkhorn Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ASAG Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° IP-Adapter Ğ¸ ControlNet, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Diffusion Models with Adversarial Guidance', 'desc': 'The paper introduces Adversarial Sinkhorn Attention Guidance (ASAG), a new method that enhances diffusion models by modifying self-attention layers with adversarial costs. This approach leverages the Sinkhorn algorithm to disrupt the transport cost, which helps to reduce pixel-wise similarity between queries and keys in the attention mechanism. By intentionally degrading misleading attention alignments, ASAG improves both conditional and unconditional sample quality. The method is lightweight and can be easily integrated into existing models without the need for retraining, showing significant improvements in applications like text-to-image generation.'}, 'zh': {'title': 'å¯¹æŠ—Sinkhornæ³¨æ„åŠ›æŒ‡å¯¼ï¼šæå‡æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡å¯¼æ–¹æ³•ï¼Œç§°ä¸ºå¯¹æŠ—Sinkhornæ³¨æ„åŠ›æŒ‡å¯¼ï¼ˆASAGï¼‰ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚ASAGé€šè¿‡åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­æ³¨å…¥å¯¹æŠ—æˆæœ¬ï¼Œå¢å¼ºäº†æ ·æœ¬çš„è´¨é‡å’Œå¯æ§æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒASAGåˆ©ç”¨æœ€ä¼˜ä¼ è¾“çš„è§†è§’é‡æ–°è§£é‡Šæ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶é€šè¿‡Sinkhornç®—æ³•æ•…æ„ç ´åä¼ è¾“æˆæœ¬ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸€è‡´çš„æ”¹è¿›ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå…·æœ‰è½»é‡çº§å’Œå³æ’å³ç”¨çš„ç‰¹ç‚¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.04824', 'title': 'Agentic Refactoring: An Empirical Study of AI Coding Agents', 'url': 'https://huggingface.co/papers/2511.04824', 'abstract': 'AI agents frequently perform refactoring in open-source Java projects, focusing on low-level consistency edits and improving code quality metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are transforming the software engineering landscape. These AI-powered systems function as autonomous teammates capable of planning and executing complex development tasks. Agents have become active participants in refactoring, a cornerstone of sustainable software development aimed at improving internal code quality without altering observable behavior. Despite their increasing adoption, there is a critical lack of empirical understanding regarding how agentic refactoring is utilized in practice, how it compares to human-driven refactoring, and what impact it has on code quality. To address this empirical gap, we present a large-scale study of AI agent-generated refactorings in real-world open-source Java projects, analyzing 15,451 refactoring instances across 12,256 pull requests and 14,988 commits derived from the AIDev dataset. Our empirical analysis shows that refactoring is a common and intentional activity in this development paradigm, with agents explicitly targeting refactoring in 26.1% of commits. Analysis of refactoring types reveals that agentic efforts are dominated by low-level, consistency-oriented edits, such as Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable (8.5%), reflecting a preference for localized improvements over the high-level design changes common in human refactoring. Additionally, the motivations behind agentic refactoring focus overwhelmingly on internal quality concerns, with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative evaluation of code quality metrics shows that agentic refactoring yields small but statistically significant improvements in structural metrics, particularly for medium-level changes, reducing class size and complexity (e.g., Class LOC median Î” = -15.25).', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '568c78587497ae69', 'authors': ['Kosei Horikawa', 'Hao Li', 'Yutaro Kashiwa', 'Bram Adams', 'Hajimu Iida', 'Ahmed E. Hassan'], 'affiliations': ['Nara Institute of Science and Technology', 'Queens University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.04824.jpg', 'data': {'categories': ['#agents', '#dataset', '#open_source', '#plp'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸ĞºÑ€Ğ¾-Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ (Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº OpenAI Codex Ğ¸ Claude) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Java-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ£Ñ‡Ñ‘Ğ½Ñ‹Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 15 Ñ‚Ñ‹ÑÑÑ‡ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ…, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿ĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑÑ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² â€” Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ° (52.5%) Ğ¸ ĞµĞ³Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ (28.1%), Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğº Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ. ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ñ€ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğ¼, Ğ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ĞºĞ¾Ğ´Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'AI Agents: Transforming Code Quality Through Smart Refactoring', 'desc': 'This paper investigates the role of AI agents in refactoring Java code within open-source projects, highlighting their focus on low-level consistency edits to enhance code quality. The study analyzes a substantial dataset of refactoring instances, revealing that AI agents target refactoring in over a quarter of their commits, primarily making localized changes like renaming variables. The findings indicate that these agents prioritize internal quality improvements, such as maintainability and readability, over broader design alterations typical of human refactoring. Additionally, the research shows that while the improvements in code quality metrics are modest, they are statistically significant, particularly in reducing class size and complexity.'}, 'zh': {'title': 'AIä»£ç†é‡æ„ï¼šæå‡ä»£ç è´¨é‡çš„æ–°åŠ›é‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†AIä»£ç†åœ¨å¼€æºJavaé¡¹ç›®ä¸­çš„é‡æ„å®è·µï¼Œåˆ†æäº†15451ä¸ªé‡æ„å®ä¾‹ã€‚AIç”Ÿæˆçš„é‡æ„ä¸»è¦é›†ä¸­åœ¨ä½çº§åˆ«çš„ä¸€è‡´æ€§ç¼–è¾‘ä¸Šï¼Œå¦‚å˜é‡é‡å‘½åå’Œç±»å‹æ›´æ”¹ï¼Œæ˜¾ç¤ºå‡ºå¯¹å±€éƒ¨æ”¹è¿›çš„åå¥½ã€‚ç ”ç©¶å‘ç°ï¼ŒAIä»£ç†çš„é‡æ„æ´»åŠ¨åœ¨å¼€å‘ä¸­å æ®äº†26.1%çš„æäº¤ï¼Œä¸»è¦å…³æ³¨ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§ã€‚å°½ç®¡AIé‡æ„å¸¦æ¥äº†å°å¹…ä½†æ˜¾è‘—çš„ç»“æ„æ€§è´¨é‡æå‡ï¼Œä½†ä¸äººç±»é©±åŠ¨çš„é‡æ„ç›¸æ¯”ï¼ŒAIçš„é‡æ„æ–¹å¼ä»ç„¶å­˜åœ¨å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.06073', 'title': 'Stemming Hallucination in Language Models Using a Licensing Oracle', 'url': 'https://huggingface.co/papers/2511.06073', 'abstract': "The Licensing Oracle, an architectural solution, eliminates hallucinations in language models by enforcing truth constraints through formal validation against structured knowledge graphs, achieving perfect abstention precision and zero false answers.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '67987a4264f316db', 'authors': ['Simeon Emanuilov', 'Richard Ackermann'], 'affiliations': ['Department of Software Technologies, Faculty of Mathematics and Informatics, Sofia University St. Kliment Ohridski, Bulgaria', 'RA Software, San Diego, United States'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.06073.jpg', 'data': {'categories': ['#rag', '#hallucinations', '#architecture', '#graphs'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¸Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ: Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Licensing Oracle, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ fine-tuning, ÑÑ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ RAG Ğ¸ fine-tuning, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Licensing Oracle Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ (AP = 1.0) Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² (FAR-NE = 0.0). Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Eliminating Hallucinations with the Licensing Oracle', 'desc': 'The paper presents the Licensing Oracle, a new architectural solution aimed at reducing hallucinations in language models (LMs). Hallucinations refer to instances where LMs generate incorrect information despite sounding plausible. The Licensing Oracle enforces truth constraints by validating outputs against structured knowledge graphs, ensuring that only accurate information is produced. Experimental results show that this approach achieves perfect abstention precision and zero false answers, outperforming traditional methods like fine-tuning and retrieval-augmented generation.'}, 'zh': {'title': 'è®¸å¯Oracleï¼šæ¶ˆé™¤è¯­è¨€æ¨¡å‹å¹»è§‰çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºè®¸å¯Oracleçš„æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æ¶ˆé™¤è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ç°è±¡ã€‚é€šè¿‡å¯¹ç»“æ„åŒ–çŸ¥è¯†å›¾è¿›è¡Œæ­£å¼éªŒè¯ï¼Œè®¸å¯Oracleå¼ºåˆ¶æ‰§è¡ŒçœŸç›¸çº¦æŸï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„å†…å®¹æ˜¯äº‹å®å‡†ç¡®çš„ã€‚ä¸ä¾èµ–æ•°æ®æ‰©å±•æˆ–å¾®è°ƒçš„ç»Ÿè®¡æ–¹æ³•ä¸åŒï¼Œè®¸å¯Oracleåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥äº†ç¡®å®šæ€§çš„éªŒè¯æ­¥éª¤ï¼Œç¡®ä¿åªç”Ÿæˆæœ‰æ•ˆçš„å£°æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè®¸å¯Oracleåœ¨äº‹å®å“åº”çš„å‡†ç¡®æ€§ä¸Šè¾¾åˆ°äº†89.1%ï¼Œå¹¶å®ç°äº†å®Œç¾çš„å¼ƒæƒç²¾åº¦å’Œé›¶é”™è¯¯ç­”æ¡ˆï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºé¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10629', 'title': 'One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models', 'url': 'https://huggingface.co/papers/2511.10629', 'abstract': "LUA is a lightweight module that performs super-resolution directly in the latent space of diffusion models, improving efficiency without compromising image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.", 'score': 122, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '10517a05b5ad5f99', 'authors': ['Aleksandr Razin', 'Danil Kazantsev', 'Ilya Makarov'], 'affiliations': ['HSE', 'NIUITMO', 'SPbSTU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10629.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#diffusion', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ¡ÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Latent Upscaler Adapter (LUA) â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LUA Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VAE, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Swin Transformer Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ pixel-shuffle Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² 2Ñ… Ğ¸ 4Ñ… Ñ€Ğ°Ğ·Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… VAE. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LUA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'Efficient High-Resolution Image Synthesis with LUA', 'desc': 'The paper introduces the Latent Upscaler Adapter (LUA), a module designed to enhance image resolution directly in the latent space of diffusion models. By performing super-resolution before the final decoding step, LUA significantly reduces the time and resources needed for high-resolution image generation. It operates as a drop-in component, requiring no changes to existing models, and achieves comparable image quality with lower latency. Extensive testing shows that LUA effectively generalizes across different variational autoencoders (VAEs), making it a versatile tool for efficient image synthesis.'}, 'zh': {'title': 'LUAï¼šé«˜æ•ˆçš„æ½œåœ¨ç©ºé—´è¶…åˆ†è¾¨ç‡è§£å†³æ–¹æ¡ˆ', 'desc': 'LUAæ˜¯ä¸€ç§è½»é‡çº§æ¨¡å—ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œä»è€Œæé«˜æ•ˆç‡è€Œä¸å½±å“å›¾åƒè´¨é‡ã€‚ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹åœ¨é«˜åˆ†è¾¨ç‡é‡‡æ ·æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒLUAé€šè¿‡åœ¨æœ€ç»ˆVAEè§£ç æ­¥éª¤ä¹‹å‰å¯¹ç”Ÿæˆå™¨çš„æ½œåœ¨ä»£ç è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚LUAä½œä¸ºä¸€ä¸ªå¯æ’æ‹”ç»„ä»¶ï¼Œæ— éœ€å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼Œæ”¯æŒ2å€å’Œ4å€çš„æ”¾å¤§å› å­ï¼Œå¹¶ä¸”ä¸å›¾åƒç©ºé—´çš„è¶…åˆ†è¾¨ç‡åŸºçº¿å…¼å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒLUAåœ¨ä¿æŒé«˜ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è§£ç å’Œæ”¾å¤§æ—¶é—´ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„é«˜ä¿çœŸå›¾åƒåˆæˆæ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10647', 'title': 'Depth Anything 3: Recovering the Visual Space from Any Views', 'url': 'https://huggingface.co/papers/2511.10647', 'abstract': 'Depth Anything 3 (DA3) uses a plain transformer for geometry prediction from visual inputs, achieving state-of-the-art results in camera pose estimation, any-view geometry, visual rendering, and monocular depth estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.', 'score': 93, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'e50fcd521d3e8098', 'authors': ['Haotong Lin', 'Sili Chen', 'Junhao Liew', 'Donny Y. Chen', 'Zhenyu Li', 'Guang Shi', 'Jiashi Feng', 'Bingyi Kang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10647.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#cv', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸Ğ· Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Depth Anything 3 (DA3) â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¼Ğµ: Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ°Ğ½Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° DINO Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, Ğ° ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ depth-rays) Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ¿Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ DA2, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞµÑ‘ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. DA3 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ state-of-the-art Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 44.3% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ 25.1% Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Geometry Prediction with Simplicity', 'desc': 'Depth Anything 3 (DA3) is a machine learning model that predicts 3D geometry from various visual inputs, even when the camera positions are unknown. It simplifies the architecture by using a basic transformer model, which proves to be effective without needing complex designs. DA3 introduces a unique depth-ray prediction method that eliminates the requirement for multi-task learning, making the training process more efficient. The model achieves impressive results in tasks like camera pose estimation and depth estimation, outperforming previous models and setting new benchmarks in visual geometry.'}, 'zh': {'title': 'æ·±åº¦é¢„æµ‹çš„æ–°çªç ´ï¼šDA3', 'desc': 'Depth Anything 3ï¼ˆDA3ï¼‰æ˜¯ä¸€ç§ä½¿ç”¨æ™®é€šå˜æ¢å™¨è¿›è¡Œå‡ ä½•é¢„æµ‹çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ä»»æ„æ•°é‡çš„è§†è§‰è¾“å…¥ä¸­é¢„æµ‹ç©ºé—´ä¸€è‡´çš„å‡ ä½•å½¢çŠ¶ã€‚è¯¥æ¨¡å‹åœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€ä»»æ„è§†è§’å‡ ä½•ã€è§†è§‰æ¸²æŸ“å’Œå•ç›®æ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚DA3çš„å…³é”®åœ¨äºä½¿ç”¨å•ä¸€çš„æ™®é€šå˜æ¢å™¨ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¹¶é€šè¿‡æ·±åº¦å…‰çº¿é¢„æµ‹ç›®æ ‡ç®€åŒ–äº†å¤šä»»åŠ¡å­¦ä¹ çš„å¤æ‚æ€§ã€‚é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒèŒƒå¼ï¼ŒDA3åœ¨ç»†èŠ‚å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¸å‰ä¸€ç‰ˆæœ¬DA2ç›¸å½“ï¼Œå¹¶åœ¨æ–°çš„è§†è§‰å‡ ä½•åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‰€æœ‰ä»»åŠ¡çš„å…ˆå‰æœ€ä½³è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09057', 'title': 'PAN: A World Model for General, Interactable, and Long-Horizon World Simulation', 'url': 'https://huggingface.co/papers/2511.09057', 'abstract': 'PAN, a general, interactable, and long-horizon world model, predicts future world states using a Generative Latent Prediction (GLP) architecture that combines autoregressive latent dynamics with a video diffusion decoder, enabling detailed, long-term, and coherent video simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.', 'score': 75, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '8491e2c4bf196511', 'authors': ['PAN Team', 'Jiannan Xiang', 'Yi Gu', 'Zihan Liu', 'Zeyu Feng', 'Qiyue Gao', 'Yiyan Hu', 'Benhao Huang', 'Guangyi Liu', 'Yichi Yang', 'Kun Zhou', 'Davit Abrahamyan', 'Arif Ahmad', 'Ganesh Bannur', 'Junrong Chen', 'Kimi Chen', 'Mingkai Deng', 'Ruobing Han', 'Xinqi Huang', 'Haoqiang Kang', 'Zheqi Liu', 'Enze Ma', 'Hector Ren', 'Yashowardhan Shinde', 'Rohan Shingre', 'Ramsundar Tanikella', 'Kaiming Tao', 'Dequan Yang', 'Xinle Yu', 'Cong Zeng', 'Binglin Zhou', 'Zhengzhong Liu', 'Zhiting Hu', 'Eric P. Xing'], 'affiliations': ['Institute of Foundation Models'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09057.jpg', 'data': {'categories': ['#video', '#diffusion', '#agents', '#long_context', '#multimodal', '#training', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'PAN â€” ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Generative Latent Prediction (GLP), ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, PAN Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'PAN: Predicting the Future with Interactive World Models', 'desc': "The paper introduces PAN, a versatile world model that predicts future states of the world using a Generative Latent Prediction (GLP) architecture. This model combines autoregressive latent dynamics with a video diffusion decoder, allowing it to generate detailed and coherent video simulations over long time horizons. Unlike previous models that lack interactivity and causal control, PAN can simulate diverse environments and respond to natural language actions. The results demonstrate PAN's effectiveness in action-conditioned world simulation and long-term forecasting, marking progress towards more generalizable world models for intelligent agents."}, 'zh': {'title': 'PANï¼šé€šç”¨çš„é•¿æ—¶é—´èŒƒå›´ä¸–ç•Œæ¨¡å‹', 'desc': 'PANæ˜¯ä¸€ç§é€šç”¨çš„ã€å¯äº¤äº’çš„ã€é•¿æ—¶é—´èŒƒå›´çš„ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆæ½œåœ¨é¢„æµ‹ï¼ˆGLPï¼‰æ¶æ„é¢„æµ‹æœªæ¥çš„ä¸–ç•ŒçŠ¶æ€ã€‚å®ƒç»“åˆäº†è‡ªå›å½’æ½œåœ¨åŠ¨æ€å’Œè§†é¢‘æ‰©æ•£è§£ç å™¨ï¼Œå®ç°äº†è¯¦ç»†ã€é•¿æœŸä¸”è¿è´¯çš„è§†é¢‘æ¨¡æ‹Ÿã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸åŒï¼ŒPANèƒ½å¤Ÿåœ¨å†å²å’Œè‡ªç„¶è¯­è¨€åŠ¨ä½œçš„æ¡ä»¶ä¸‹è¿›è¡Œé«˜è´¨é‡çš„è§†é¢‘æ¨¡æ‹Ÿï¼Œæ”¯æŒå¼€æ”¾é¢†åŸŸçš„åŠ¨ä½œæ¡ä»¶æ¨¡æ‹Ÿã€‚å®éªŒè¡¨æ˜ï¼ŒPANåœ¨åŠ¨ä½œæ¡ä»¶çš„ä¸–ç•Œæ¨¡æ‹Ÿå’Œé•¿æ—¶é—´é¢„æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†é€šç”¨ä¸–ç•Œæ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10643', 'title': 'Black-Box On-Policy Distillation of Large Language Models', 'url': 'https://huggingface.co/papers/2511.10643', 'abstract': "Generative Adversarial Distillation (GAD) enhances black-box distillation by framing the student model as a generator and using a discriminator to provide adaptive feedback, surpassing traditional sequence-level knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.", 'score': 46, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '14d8a22edabc8326', 'authors': ['Tianzhu Ye', 'Li Dong', 'Zewen Chi', 'Xun Wu', 'Shaohan Huang', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10643.jpg', 'data': {'categories': ['#architecture', '#training'], 'emoji': 'âš”ï¸', 'ru': {'title': 'Ğ¡Ğ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ: ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ³Ñ€Ğ°ÑÑ‚ Ğ² Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Generative Adversarial Distillation (GAD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº ĞµÑ‘ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑĞ½ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ, Ğ³Ğ´Ğµ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»ÑƒĞ¶Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ ÑĞ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GAD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ sequence-level Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-14B Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4-Chat.'}, 'en': {'title': 'Revolutionizing Black-Box Distillation with GAD', 'desc': "Generative Adversarial Distillation (GAD) is a novel approach to black-box distillation that treats the student model as a generator while employing a discriminator for adaptive feedback. This method allows the student model to learn from the teacher model's outputs without needing access to its internal workings. By creating a minimax game between the generator and discriminator, GAD provides a stable reward mechanism that enhances the learning process. Experimental results demonstrate that GAD outperforms traditional sequence-level knowledge distillation, making it a significant advancement in the field of large language model training."}, 'zh': {'title': 'ç”Ÿæˆå¯¹æŠ—è’¸é¦ï¼šé»‘ç®±è’¸é¦çš„æ–°çªç ´', 'desc': 'ç”Ÿæˆå¯¹æŠ—è’¸é¦ï¼ˆGADï¼‰é€šè¿‡å°†å­¦ç”Ÿæ¨¡å‹è§†ä¸ºç”Ÿæˆå™¨ï¼Œå¹¶ä½¿ç”¨é‰´åˆ«å™¨æä¾›è‡ªé€‚åº”åé¦ˆï¼Œå¢å¼ºäº†é»‘ç®±è’¸é¦ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„åºåˆ—çº§çŸ¥è¯†è’¸é¦ã€‚é»‘ç®±è’¸é¦ä»…é€šè¿‡å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºï¼Œåˆ›å»ºå­¦ç”Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè€Œä¸æ¥è§¦å…¶å†…éƒ¨é€»è¾‘æˆ–å‚æ•°ã€‚GADä½¿å¾—åœ¨ç­–ç•¥å’Œé»‘ç®±è’¸é¦ä¸­ï¼Œå­¦ç”ŸLLMä¸æ•™å¸ˆLLMä¹‹é—´å½¢æˆäº†ä¸€ä¸ªæœ€å°æœ€å¤§åšå¼ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGADåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºå¸¸ç”¨çš„åºåˆ—çº§çŸ¥è¯†è’¸é¦ï¼Œå°¤å…¶æ˜¯ç»è¿‡GADè®­ç»ƒçš„Qwen2.5-14B-Instructåœ¨è‡ªåŠ¨è¯„ä¼°ä¸­ä¸å…¶æ•™å¸ˆGPT-5-Chatç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08521', 'title': 'UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist', 'url': 'https://huggingface.co/papers/2511.08521', 'abstract': 'UniVA is an open-source multi-agent framework that integrates video understanding, segmentation, editing, and generation into cohesive workflows using a Plan-and-Act architecture and hierarchical memory.  \t\t\t\t\tAI-generated summary \t\t\t\t While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation rightarrow multi-round editing rightarrow object segmentation rightarrow compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)', 'score': 37, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '8a917acfe391e2de', 'authors': ['Zhengyang Liang', 'Daoan Zhang', 'Huichi Zhou', 'Rui Huang', 'Bobo Li', 'Yuechen Zhang', 'Shengqiong Wu', 'Xiaohan Wang', 'Jiebo Luo', 'Lizi Liao', 'Hao Fei'], 'affiliations': ['National University of Singapore', 'Singapore Management University', 'Stanford University', 'The Chinese University of Hong Kong', 'University College London', 'University of Rochester'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08521.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#agents', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'UniVA â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñæ¡†æ¶Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Plan-and-Act, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ ÑÑ‚Ğ¸ ÑˆĞ°Ğ³Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ (Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ UniVA-Bench â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ÑĞµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'UniVA: Revolutionizing Video Processing with Multi-Agent Intelligence', 'desc': 'UniVA is an innovative open-source framework designed for multi-agent video processing that integrates various tasks such as understanding, segmentation, editing, and generation. It utilizes a Plan-and-Act architecture where a planner agent breaks down user intentions into actionable steps, while executor agents carry out these tasks using modular tool servers. The framework features a hierarchical memory system that supports long-term reasoning and effective communication between agents, allowing for complex and iterative video workflows. UniVA also introduces UniVA-Bench, a benchmark for evaluating multi-step video tasks, promoting advancements in interactive and general-purpose video intelligence.'}, 'zh': {'title': 'UniVAï¼šè§†é¢‘æ™ºèƒ½çš„ä¸‹ä¸€ä»£è§£å†³æ–¹æ¡ˆ', 'desc': 'UniVAæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†é¢‘ç†è§£ã€åˆ†å‰²ã€ç¼–è¾‘å’Œç”Ÿæˆæ•´åˆä¸ºç»Ÿä¸€çš„å·¥ä½œæµç¨‹ã€‚å®ƒé‡‡ç”¨è®¡åˆ’ä¸æ‰§è¡Œçš„åŒæ™ºèƒ½ä½“æ¶æ„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤„ç†å¤æ‚çš„è§†é¢‘ä»»åŠ¡ã€‚é€šè¿‡å±‚æ¬¡åŒ–çš„å¤šçº§è®°å¿†ï¼ŒUniVAæ”¯æŒé•¿æ—¶é—´çš„æ¨ç†å’Œä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„æ²Ÿé€šã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†UniVA-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ­¥éª¤è§†é¢‘ä»»åŠ¡çš„æ€§èƒ½ï¼Œæ¨åŠ¨äº’åŠ¨å’Œé€šç”¨è§†é¢‘æ™ºèƒ½çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09780', 'title': 'Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO', 'url': 'https://huggingface.co/papers/2511.09780', 'abstract': 'The study identifies and defends against adversarial attacks in decentralized Group Relative Policy Optimization (GRPO) for Large Language Models (LLMs), demonstrating attack success rates of up to 100% and proposing effective defense mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.', 'score': 27, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '1b2e719f2a2252da', 'authors': ['Nikolay Blagoev', 'OÄŸuzhan Ersoy', 'Lydia Yiyu Chen'], 'affiliations': ['Gensyn', 'TU Delft', 'University of Neuchatel'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09780.jpg', 'data': {'categories': ['#alignment', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ½Ğ° Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· out-of-context Ğ¸ in-context Ğ°Ñ‚Ğ°ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 100% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ° 50 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 100%.'}, 'en': {'title': 'Defending Decentralized GRPO: Battling Adversarial Attacks on LLMs', 'desc': 'This paper explores the vulnerabilities of decentralized Group Relative Policy Optimization (GRPO) in training Large Language Models (LLMs) against adversarial attacks. It reveals that attackers can successfully inject harmful tokens into benign models, leading to a complete compromise of the training process with success rates reaching 100%. The study presents two defense strategies tailored to different training scenarios, effectively neutralizing these attacks. By demonstrating the ease of poisoning in both out-of-context and in-context settings, the research highlights the critical need for robust defenses in decentralized machine learning environments.'}, 'zh': {'title': 'é˜²å¾¡å¯¹æŠ—æ”»å‡»ï¼Œä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å»ä¸­å¿ƒåŒ–çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­è¯†åˆ«å’Œé˜²å¾¡å¯¹æŠ—æ€§æ”»å‡»ï¼Œå°¤å…¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¶æ„æ”»å‡»è€…å¯ä»¥é€šè¿‡æ³¨å…¥æ¶æ„æ ‡è®°æ¥ç ´åç³»ç»Ÿï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾100%ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é˜²å¾¡æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜»æ­¢è¿™äº›æ”»å‡»ï¼Œç¡®ä¿æ¨¡å‹çš„å®‰å…¨æ€§ã€‚é€šè¿‡å®è¯ä¾‹å­ï¼Œæˆ‘ä»¬éªŒè¯äº†è¿™äº›é˜²å¾¡æ–¹æ³•åœ¨ä¸åŒè®­ç»ƒæ¨¡å‹æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09030', 'title': 'Solving a Million-Step LLM Task with Zero Errors', 'url': 'https://huggingface.co/papers/2511.09030', 'abstract': 'MAKER, a system using microagents with error correction, successfully solves tasks with over a million LLM steps, suggesting a new approach for scaling LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.', 'score': 20, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'da64c7a751cec303', 'authors': ['Elliot Meyerson', 'Giuseppe Paolo', 'Roberto Dailey', 'Hormoz Shahrzad', 'Olivier Francon', 'Conor F. Hayes', 'Xin Qiu', 'Babak Hodjat', 'Risto Miikkulainen'], 'affiliations': ['Cognizant AI Lab', 'UT Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09030.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM', 'desc': 'MAKER â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸ĞºÑ€Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº.'}, 'en': {'title': 'Scaling LLMs with Error-Free Microagents', 'desc': 'This paper introduces MAKER, a novel system that utilizes microagents to perform tasks with over one million steps of a large language model (LLM) without any errors. The key innovation is the extreme decomposition of complex tasks into smaller, manageable subtasks, allowing each microagent to focus on specific components. This modular approach enables effective error correction through a multi-agent voting mechanism, ensuring accuracy at each step. The findings suggest that rather than solely improving existing LLMs, employing massively decomposed agentic processes (MDAPs) could enhance problem-solving capabilities on a larger scale.'}, 'zh': {'title': 'æè‡´åˆ†è§£ï¼Œé›¶é”™è¯¯è§£å†³æ–¹æ¡ˆï¼', 'desc': 'MAKERæ˜¯ä¸€ä¸ªä½¿ç”¨å¾®ä»£ç†å’Œé”™è¯¯ä¿®æ­£çš„ç³»ç»Ÿï¼ŒæˆåŠŸåœ°è§£å†³äº†è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªLLMæ­¥éª¤çš„ä»»åŠ¡ï¼Œå±•ç¤ºäº†ä¸€ç§æ‰©å±•LLMèƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚å°½ç®¡LLMåœ¨æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å–å¾—äº†æ˜¾è‘—çªç ´ï¼Œä½†å°†è¿™äº›èƒ½åŠ›ä¸²è”æˆå¤§è§„æ¨¡çš„è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚MAKERé€šè¿‡å°†ä»»åŠ¡æåº¦åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œä½¿æ¯ä¸ªå­ä»»åŠ¡ç”±ä¸“æ³¨çš„å¾®ä»£ç†å¤„ç†ï¼Œä»è€Œå®ç°äº†é›¶é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§æç«¯åˆ†è§£å’Œé”™è¯¯ä¿®æ­£çš„ç»“åˆï¼Œä½¿å¾—åœ¨ç»„ç»‡å’Œç¤¾ä¼šå±‚é¢é«˜æ•ˆè§£å†³é—®é¢˜æˆä¸ºå¯èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08522', 'title': 'AlphaResearch: Accelerating New Algorithm Discovery with Language Models', 'url': 'https://huggingface.co/papers/2511.08522', 'abstract': "AlphaResearch, an autonomous research agent, discovers new algorithms in open-ended problems with a dual research environment, achieving competitive performance against human researchers in a benchmark competition.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present AlphaResearch, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct AlphaResearchComp, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the ``packing circles'' problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.", 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'e13cbf21d0122775', 'authors': ['Zhaojian Yu', 'Kaiyue Feng', 'Yilun Zhao', 'Shilin He', 'Xiao-Ping Zhang', 'Arman Cohan'], 'affiliations': ['ByteDance', 'New York University', 'Tsinghua University', 'Yale University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08522.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#open_source', '#agents', '#dataset', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ', 'desc': 'AlphaResearch â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· Ñ‡Ñ‘Ñ‚ĞºĞ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ ÑÑ€ĞµĞ´Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ´ĞµĞ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AlphaResearchComp Ñ Ğ²Ğ¾ÑÑŒĞ¼ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing Algorithm Discovery with AlphaResearch', 'desc': "AlphaResearch is an autonomous research agent that excels in discovering new algorithms for open-ended problems. It operates within a dual research environment that combines execution-based verification with a simulated peer review process, enhancing both feasibility and innovation. The agent follows a structured approach of proposing ideas, verifying them, and optimizing for performance. In a benchmark competition, AlphaResearch demonstrated competitive results against human researchers, achieving notable success in specific algorithmic challenges, particularly in the 'packing circles' problem."}, 'zh': {'title': 'è‡ªä¸»ç ”ç©¶ä»£ç†ï¼šåŠ é€Ÿç®—æ³•å‘ç°çš„æœªæ¥', 'desc': 'æœ¬æ–‡ä»‹ç»äº†AlphaResearchï¼Œä¸€ä¸ªè‡ªä¸»ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾æ€§é—®é¢˜å¹¶å‘ç°æ–°ç®—æ³•ã€‚å®ƒé€šè¿‡æ„å»ºä¸€ä¸ªåŒé‡ç ”ç©¶ç¯å¢ƒï¼Œç»“åˆæ‰§è¡ŒéªŒè¯å’Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„åŒè¡Œè¯„å®¡ï¼Œæ¥ä¿ƒè¿›å‘ç°è¿‡ç¨‹çš„å¯è¡Œæ€§å’Œåˆ›æ–°æ€§ã€‚AlphaResearché€šè¿‡è¿­ä»£æ­¥éª¤æå‡ºæ–°æƒ³æ³•ã€éªŒè¯è¿™äº›æƒ³æ³•å¹¶ä¼˜åŒ–ç ”ç©¶ææ¡ˆï¼Œä»è€Œå®ç°ç®—æ³•çš„å‘ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAlphaResearchåœ¨ä¸äººç±»ç ”ç©¶è€…çš„å¯¹æ¯”ä¸­å–å¾—äº†2/8çš„èƒœç‡ï¼Œå±•ç¤ºäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åŠ é€Ÿç®—æ³•å‘ç°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.01918', 'title': 'Superpositional Gradient Descent: Harnessing Quantum Principles for\n  Model Training', 'url': 'https://huggingface.co/papers/2511.01918', 'abstract': 'Superpositional Gradient Descent, a quantum-inspired optimizer, improves convergence and reduces final loss in large language model training compared to AdamW.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly trained with classical optimization techniques like AdamW to improve convergence and generalization. However, the mechanisms by which quantum-inspired methods enhance classical training remain underexplored. We introduce Superpositional Gradient Descent (SGD), a novel optimizer linking gradient updates with quantum superposition by injecting quantum circuit perturbations. We present a mathematical framework and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On synthetic sequence classification and large-scale LLM fine-tuning, SGD converges faster and yields lower final loss than AdamW. Despite promising results, scalability and hardware constraints limit adoption. Overall, this work provides new insights into the intersection of quantum computing and deep learning, suggesting practical pathways for leveraging quantum principles to control and enhance model behavior.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '27489697380f09cd', 'authors': ['Ahmet Erdem Pamuk', 'Emir Kaan Ã–zdemir', 'Åuayp Talha Kocabay'], 'affiliations': ['Istanbul Erkek High School Istanbul, Turkiye', 'Science High School Ankara, Turkiye', 'UBITAK Science High School Kocaeli, Turkiye'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.01918.jpg', 'data': {'categories': [], 'emoji': 'âš›ï¸', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Superpositional Gradient Descent (SGD), Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ PyTorch Ğ¸ Qiskit. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ AdamW Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ fine-tuning LLM. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ².'}, 'en': {'title': 'Harnessing Quantum Principles for Faster Model Training', 'desc': 'This paper introduces Superpositional Gradient Descent (SGD), a new optimizer inspired by quantum mechanics, which aims to improve the training of large language models (LLMs). By incorporating quantum superposition into the gradient update process, SGD enhances convergence speed and reduces final loss compared to the traditional AdamW optimizer. The authors provide a mathematical framework and demonstrate the implementation of hybrid quantum-classical circuits using PyTorch and Qiskit. Although the results are promising, challenges related to scalability and hardware limitations may hinder widespread adoption of this approach.'}, 'zh': {'title': 'é‡å­å¯å‘çš„ä¼˜åŒ–å™¨ï¼šè¶…ä½ç½®æ¢¯åº¦ä¸‹é™', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºè¶…ä½ç½®æ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰çš„æ–°å‹ä¼˜åŒ–å™¨ï¼Œå®ƒå—åˆ°é‡å­è®¡ç®—çš„å¯å‘ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„AdamWä¼˜åŒ–å™¨ç›¸æ¯”ï¼ŒSGDé€šè¿‡å¼•å…¥é‡å­ç”µè·¯æ‰°åŠ¨æ¥åŠ é€Ÿæ”¶æ•›å¹¶é™ä½æœ€ç»ˆæŸå¤±ã€‚æˆ‘ä»¬åœ¨PyTorchå’ŒQiskitä¸­å®ç°äº†æ··åˆé‡å­-ç»å…¸ç”µè·¯ï¼Œå¹¶åœ¨åˆæˆåºåˆ—åˆ†ç±»å’Œå¤§è§„æ¨¡LLMå¾®è°ƒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å°½ç®¡ç»“æœä»¤äººé¼“èˆï¼Œä½†å¯æ‰©å±•æ€§å’Œç¡¬ä»¶é™åˆ¶ä»ç„¶æ˜¯å…¶åº”ç”¨çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10289', 'title': 'Music Flamingo: Scaling Music Understanding in Audio Language Models', 'url': 'https://huggingface.co/papers/2511.10289', 'abstract': "Music Flamingo, a large audio-language model, advances music understanding through fine-tuning on a rich dataset and post-training with novel methods, achieving state-of-the-art results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.", 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '8213f0b545744910', 'authors': ['Sreyan Ghosh', 'Arushi Goel', 'Lasha Koroshinadze', 'Sang-gil Lee', 'Zhifeng Kong', 'Joao Felipe Santos', 'Ramani Duraiswami', 'Dinesh Manocha', 'Wei Ping', 'Mohammad Shoeybi', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, CA, USA', 'University of Maryland, College Park, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10289.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#optimization', '#audio', '#multimodal', '#training', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸµ', 'ru': {'title': 'ĞÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ: Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ', 'desc': 'Music Flamingo â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MF-Skills Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‚ĞµĞ¼Ğ±Ñ€ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Music Flamingo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 10+ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¼Ñƒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Music Understanding with Music Flamingo', 'desc': "Music Flamingo is a large audio-language model that enhances music understanding by utilizing a comprehensive dataset and innovative training techniques. It addresses the challenges of music's complex nature and the lack of quality data by introducing the MF-Skills dataset, which includes detailed annotations on various musical elements. The model is fine-tuned on this dataset and further improved through a unique post-training approach that incorporates reasoning based on music theory. As a result, Music Flamingo achieves top performance on multiple benchmarks, showcasing its ability to understand music in a nuanced, human-like manner."}, 'zh': {'title': 'éŸ³ä¹ç†è§£çš„æ–°æ ‡å‡†', 'desc': 'Music Flamingo æ˜¯ä¸€ç§å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ä¸°å¯Œæ•°æ®é›†çš„å¾®è°ƒå’Œæ–°é¢–çš„åè®­ç»ƒæ–¹æ³•ï¼Œæå‡éŸ³ä¹ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å…‹æœäº†ä»¥å¾€æ¨¡å‹åœ¨éŸ³ä¹ç†è§£ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŠ¨æ€æ€§ã€å±‚æ¬¡æ€§å’Œä¿¡æ¯å¯†é›†æ€§ã€‚é€šè¿‡æ„å»º MF-Skills æ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„éŸ³ä¹æè¿°å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–å’Œå£°ã€ç»“æ„ã€éŸ³è‰²ã€æ­Œè¯å’Œæ–‡åŒ–èƒŒæ™¯ç­‰æ–¹é¢ã€‚Music Flamingo åœ¨å¤šä¸ªéŸ³ä¹ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨éŸ³ä¹ç†è§£å’Œæ¨ç†æ–¹é¢çš„å¹¿æ³›èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07685', 'title': 'ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents', 'url': 'https://huggingface.co/papers/2511.07685', 'abstract': "ResearchRubrics is a benchmark for evaluating deep research agents, using expert rubrics to assess their factual grounding, reasoning, and clarity across diverse, complex tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.", 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'fe2382a8bb21c8c7', 'authors': ['Manasi Sharma', 'Chen Bo Calvin Zhang', 'Chaithanya Bandi', 'Clinton Wang', 'Ankit Aich', 'Huy Nghiem', 'Tahseen Rabbani', 'Ye Htet', 'Brian Jang', 'Sumana Basu', 'Aishwarya Balwani', 'Denis Peskoff', 'Marcos Ayestaran', 'Sean M. Hendryx', 'Brad Kenstler', 'Bing Liu'], 'affiliations': ['McGill University', 'Scale AI', 'University of California, Berkeley', 'University of Chicago', 'University of Maryland', 'Washington University, St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07685.jpg', 'data': {'categories': ['#rag', '#benchmark', '#reasoning', '#open_source', '#agents', '#science'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'ResearchRubrics â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2500 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑˆĞ¸Ñ€Ğ¾Ñ‚Ñƒ, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 68% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° ÑƒĞ¿ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Evaluating Deep Research Agents with ResearchRubrics', 'desc': "ResearchRubrics is a benchmark designed to evaluate deep research agents that utilize large language models for complex queries. It assesses these agents based on their factual grounding, reasoning abilities, and clarity of responses through a set of expert-written rubrics. The benchmark includes over 2,800 hours of human effort and features 2,500+ detailed rubrics paired with realistic prompts. The study reveals that even top-performing deep research systems struggle to meet the benchmark's standards, indicating a significant need for improved evaluation methods in this field."}, 'zh': {'title': 'è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„æ ‡å‡†åŒ–åŸºå‡†', 'desc': 'ResearchRubricsæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†çš„åŸºå‡†ï¼Œåˆ©ç”¨ä¸“å®¶è¯„åˆ†æ ‡å‡†æ¥è¯„ä¼°å…¶äº‹å®åŸºç¡€ã€æ¨ç†èƒ½åŠ›å’Œæ¸…æ™°åº¦ã€‚æ·±åº¦ç ”ç©¶ï¼ˆDRï¼‰æ˜¯ä¸€ä¸ªæ–°å…´çš„ä»£ç†åº”ç”¨ï¼Œä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¤„ç†å¼€æ”¾å¼æŸ¥è¯¢ã€‚è¯„ä¼°DRçš„æŒ‘æˆ˜åœ¨äºå…¶å“åº”å†…å®¹å†—é•¿å¤šæ ·ï¼Œä¸”å­˜åœ¨å¤šç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šå¸¸è¿˜ä¾èµ–åŠ¨æ€ä¿¡æ¯æºã€‚æˆ‘ä»¬æå‡ºçš„ResearchRubricsç»“åˆäº†2800å¤šä¸ªå°æ—¶çš„äººåŠ›åŠ³åŠ¨ï¼Œé…å¤‡2500å¤šä¸ªä¸“å®¶æ’°å†™çš„ç»†è‡´è¯„åˆ†æ ‡å‡†ï¼Œä»¥è¯„ä¼°DRçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09715', 'title': 'SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control', 'url': 'https://huggingface.co/papers/2511.09715', 'abstract': "SliderEdit enables continuous, fine-grained control over image editing instructions by using low-rank adaptation matrices, improving edit controllability, visual consistency, and user steerability.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '88a7cab83803193d', 'authors': ['Arman Zarei', 'Samyadeep Basu', 'Mobina Pournemat', 'Sayan Nag', 'Ryan Rossi', 'Soheil Feizi'], 'affiliations': ['Adobe Research', 'University of Maryland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09715.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ·ÑƒĞ½ĞºĞ¸', 'desc': 'SliderEdit â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ·ÑƒĞ½Ğ¾Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ»Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… FLUX-Kontext Ğ¸ Qwen-Image-Edit.'}, 'en': {'title': 'SliderEdit: Fine-Grained Control for Interactive Image Editing', 'desc': 'SliderEdit is a novel framework that enhances image editing by allowing users to have continuous and fine-grained control over their editing instructions. It utilizes low-rank adaptation matrices to separate and adjust the strength of individual instructions in a multi-part edit prompt. This approach improves edit controllability and visual consistency, enabling users to smoothly manipulate the intensity of edits without needing separate training for each attribute. By integrating SliderEdit with advanced image editing models, the framework significantly boosts user steerability and paves the way for more interactive image manipulation.'}, 'zh': {'title': 'SliderEditï¼šå®ç°å›¾åƒç¼–è¾‘çš„è¿ç»­ç»†ç²’åº¦æ§åˆ¶', 'desc': 'SliderEdit æ˜¯ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·å¯¹ç¼–è¾‘æŒ‡ä»¤è¿›è¡Œè¿ç»­å’Œç»†ç²’åº¦çš„æ§åˆ¶ã€‚é€šè¿‡ä½¿ç”¨ä½ç§©é€‚åº”çŸ©é˜µï¼ŒSliderEdit å¯ä»¥å°†å¤šéƒ¨åˆ†ç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºå¯è°ƒèŠ‚çš„æ»‘å—ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¹³æ»‘åœ°è°ƒæ•´æ¯ä¸ªæŒ‡ä»¤çš„å¼ºåº¦ã€‚ä¸ä»¥å¾€éœ€è¦ä¸ºæ¯ä¸ªå±æ€§å•ç‹¬è®­ç»ƒçš„æ¨¡å‹ä¸åŒï¼ŒSliderEdit é€šè¿‡å­¦ä¹ ä¸€ç»„é€šç”¨çš„ä½ç§©é€‚åº”çŸ©é˜µï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç¼–è¾‘å’Œå±æ€§ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„æ³›åŒ–ã€‚è¯¥æ–¹æ³•åœ¨ç°æœ‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸­åº”ç”¨ï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘çš„å¯æ§æ€§ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·çš„æ“ä½œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10017', 'title': 'AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2511.10017', 'abstract': "AffordBot, a framework combining Multimodal Large Language Models with chain-of-thought reasoning, achieves state-of-the-art performance in predicting affordance elements' spatial locations, motion types, and axes in 3D scenes based on task instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '57ed63ddf777fd34', 'authors': ['Xinyi Wang', 'Xun Yang', 'Yanlong Xu', 'Yuchen Wu', 'Zhen Li', 'Na Zhao'], 'affiliations': ['Chinese University of Hong Kong, Shenzhen', 'Singapore University of Technology and Design', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10017.jpg', 'data': {'categories': ['#agents', '#3d', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'AffordBot â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² 3D ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° affordance ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ¸Ğ¿ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾ÑÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ 3D Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ 2D-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ MLLM Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ SceneFun3D Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Interaction with AffordBot!', 'desc': 'AffordBot is a new framework that combines Multimodal Large Language Models (MLLMs) with chain-of-thought reasoning to enhance understanding of 3D scenes. It addresses the challenge of predicting the spatial locations, motion types, and axes of affordance elements based on specific task instructions. By using a unique approach that transforms 3D data into 2D images, AffordBot allows for better interaction and reasoning about objects in physical environments. The framework has shown impressive results on the SceneFun3D dataset, outperforming existing methods in fine-grained 3D embodied reasoning.'}, 'zh': {'title': 'AffordBotï¼šæå‡3Dåœºæ™¯ç†è§£çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'AffordBotæ˜¯ä¸€ä¸ªç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé“¾å¼æ€ç»´æ¨ç†çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨3Dåœºæ™¯ä¸­æ ¹æ®ä»»åŠ¡æŒ‡ä»¤é¢„æµ‹å¯æ“ä½œå…ƒç´ çš„ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šç»†ç²’åº¦3Dä½“ç°æ¨ç†ï¼Œè¦æ±‚æ™ºèƒ½ä½“ä¸ºæ¯ä¸ªå‚è€ƒçš„å¯æ“ä½œå…ƒç´ é¢„æµ‹ä¸€ä¸ªç»“æ„åŒ–çš„ä¸‰å…ƒç»„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªä»»åŠ¡ï¼ŒAffordBoté€šè¿‡æ¸²æŸ“åœºæ™¯çš„å…¨æ™¯å›¾åƒå¹¶å°†3Då…ƒç´ å€™é€‰æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚ç»è¿‡åœ¨SceneFun3Dæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒAffordBotå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ä»…ä½¿ç”¨3Dç‚¹äº‘è¾“å…¥å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†åŸºç¡€æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10507', 'title': 'Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following', 'url': 'https://huggingface.co/papers/2511.10507', 'abstract': 'AdvancedIF benchmark and RIFL pipeline improve instruction-following capabilities in large language models by using expert-curated rubrics and reinforcement learning techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'fe35e028d2405a46', 'authors': ['Yun He', 'Wenzhe Li', 'Hejia Zhang', 'Songlin Li', 'Karishma Mandyam', 'Sopan Khosla', 'Yuanhao Xiong', 'Nanshu Wang', 'Xiaoliang Peng', 'Beibin Li', 'Shengjie Bi', 'Shishir G. Patil', 'Qi Qi', 'Shengyu Feng', 'Julian Katz-Samuels', 'Richard Yuanzhe Pang', 'Sujan Gonugondla', 'Hunter Lang', 'Yue Yu', 'Yundi Qian', 'Maryam Fazel-Zarandi', 'Licheng Yu', 'Amine Benhalloum', 'Hany Awadalla', 'Manaal Faruqui'], 'affiliations': ['CMU', 'Meta Superintelligence Labs', 'Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10507.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#rlhf', '#training'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'Ğ ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AdvancedIF Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1600 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ÑƒĞ±Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RIFL â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6.7% Ğ² Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ÑƒĞ±Ñ€Ğ¸ĞºĞ¸ Ğ·Ğ°Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞµĞ±Ñ ĞºĞ°Ğº Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Enhancing Instruction-Following in AI with AdvancedIF and RIFL', 'desc': 'This paper presents the AdvancedIF benchmark and the RIFL pipeline, which enhance the instruction-following abilities of large language models (LLMs). The AdvancedIF benchmark includes over 1,600 expert-curated prompts that evaluate LLMs on complex, multi-turn instructions. The RIFL pipeline utilizes rubric generation and reinforcement learning techniques to improve training effectiveness. Experimental results show that RIFL significantly boosts LLM performance on the AdvancedIF benchmark and other public tests, highlighting the importance of rubrics in AI training and evaluation.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†AdvancedIFåŸºå‡†å’ŒRIFLç®¡é“ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è·Ÿéšèƒ½åŠ›ã€‚AdvancedIFæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼ŒåŒ…å«è¶…è¿‡1600ä¸ªæç¤ºå’Œä¸“å®¶åˆ¶å®šçš„è¯„åˆ†æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚RIFLæ˜¯ä¸€ç§æ–°é¢–çš„åè®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨è¯„åˆ†æ ‡å‡†ç”Ÿæˆã€å¾®è°ƒçš„è¯„åˆ†éªŒè¯å™¨å’Œå¥–åŠ±å¡‘é€ ï¼Œä¿ƒè¿›æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRIFLæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œä¸ºæ›´å¼ºå¤§å’Œå¯é çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10547', 'title': 'Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation', 'url': 'https://huggingface.co/papers/2511.10547', 'abstract': 'A framework for evaluating diversity in text-to-image models through human assessment and systematic analysis of image embeddings.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.   Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '38f7ef934c414106', 'authors': ['Isabela Albuquerque', 'Ira Ktena', 'Olivia Wiles', 'Ivana KajiÄ‡', 'Amal Rannen-Triki', 'Cristina Vasconcelos', 'Aida Nematzadeh'], 'affiliations': ['Ellison Institute of Technology', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10547.jpg', 'data': {'categories': [], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ: ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image (T2I). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ñ†Ğ²ĞµÑ‚ ÑĞ±Ğ»Ğ¾ĞºĞ°). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… image embeddings Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ T2I Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Diversity in Text-to-Image Models', 'desc': 'This paper presents a new framework for evaluating the diversity of text-to-image (T2I) models, which often produce similar outputs despite improvements in quality. The framework includes a human evaluation template that allows for detailed assessments of diversity based on specific concepts and their variations. It also features a curated set of prompts that highlight different factors of variation, such as color in images of apples. By comparing various image embeddings and using statistical methods, the study provides a way to rank T2I models based on their diversity, helping to identify areas for improvement.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å¤šæ ·æ€§çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç”Ÿæˆè´¨é‡è™½é«˜ä½†ç¼ºä¹å¤šæ ·æ€§çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è¯„ä¼°ä¸ªåˆ«æ¦‚å¿µåŠå…¶ç›¸å…³å˜å¼‚å› ç´ ï¼Œç³»ç»Ÿåœ°è¯„ä¼°å¤šæ ·æ€§ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€ç§æ–°çš„äººç±»è¯„ä¼°æ¨¡æ¿ç”¨äºç»†è‡´çš„å¤šæ ·æ€§è¯„ä¼°ï¼›ä¸€ä¸ªæ¶µç›–å¤šæ ·åŒ–æ¦‚å¿µåŠå…¶å˜å¼‚å› ç´ çš„æç¤ºé›†ï¼›ä»¥åŠé€šè¿‡äºŒé¡¹æ£€éªŒæ¯”è¾ƒæ¨¡å‹çš„äººç±»æ³¨é‡Šçš„æ–¹æ³•è®ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸¥æ ¼æ¯”è¾ƒäº†ä¸åŒçš„å›¾åƒåµŒå…¥ä»¥æµ‹é‡å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09067', 'title': 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique', 'url': 'https://huggingface.co/papers/2511.09067', 'abstract': "MM-CRITIC is a benchmark for evaluating the critique abilities of Large Multimodal Models across multiple dimensions and tasks, using expert-informed ground answers for reliable scoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'cb7085959f687cf4', 'authors': ['Gailun Zeng', 'Ziyang Luo', 'Hongzhan Lin', 'Yuchen Tian', 'Kaixin Li', 'Ziyang Gong', 'Jianxiong Guo', 'Jing Ma'], 'affiliations': ['Beijing Normal University', 'Beijing Normal-Hong Kong Baptist University', 'Hong Kong Baptist University', 'National University of Singapore', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09067.jpg', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-CRITIC â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ (Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ) Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 500 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 4471 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ GPT-4o Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Evaluating Critique Abilities of Multimodal Models with MM-CRITIC', 'desc': 'MM-CRITIC is a new benchmark designed to assess how well Large Multimodal Models (LMMs) can critique their outputs across various tasks. It evaluates models on three main dimensions: basic critique, correction of errors, and comparison with other outputs. The benchmark includes over 500 tasks and uses expert-informed answers to ensure reliable scoring of model responses. Through extensive testing, MM-CRITIC provides insights into the critique abilities of LMMs and highlights the relationship between the quality of responses and their critique performance.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹æ‰¹è¯„èƒ½åŠ›çš„å…¨æ–°åŸºå‡†', 'desc': 'MM-CRITICæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ‰¹è¯„èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–å¤šä¸ªç»´åº¦å’Œä»»åŠ¡ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸“å®¶æä¾›çš„æ ‡å‡†ç­”æ¡ˆæ¥ç¡®ä¿è¯„åˆ†çš„å¯é æ€§ï¼Œæ¶‰åŠåŸºæœ¬ã€çº æ­£å’Œæ¯”è¾ƒç­‰å¤šä¸ªæ–¹é¢ã€‚MM-CRITICåŒ…å«è¶…è¿‡500ä¸ªä»»åŠ¡ï¼Œæ”¶é›†äº†ä¸åŒæ¨¡å‹å¤§å°çš„LMMsçš„å“åº”ï¼Œæä¾›äº†4471ä¸ªæ ·æœ¬ã€‚å®éªŒç»“æœéªŒè¯äº†MM-CRITICçš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†å“åº”è´¨é‡ä¸æ‰¹è¯„ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥åŠä¸åŒè¯„ä¼°ç»´åº¦çš„æ‰¹è¯„éš¾åº¦å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07790', 'title': 'CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis', 'url': 'https://huggingface.co/papers/2511.07790', 'abstract': "The CC30k dataset, comprising citation contexts labeled with reproducibility-oriented sentiments, enhances the accuracy of large language models in predicting the reproducibility of machine learning papers.  \t\t\t\t\tAI-generated summary \t\t\t\t Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'e6a8d435a5b3f4a5', 'authors': ['Rochana R. Obadage', 'Sarah M. Rajtmajer', 'Jian Wu'], 'affiliations': ['Old Dominion University', 'The Pennsylvania State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07790.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CC30k, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 30,734 ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¿Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ, Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ). Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ±Ñ‹Ğ» ÑĞ¾Ğ±Ñ€Ğ°Ğ½ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ 94%. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° CC30k Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reproducibility Insights with CC30k Dataset', 'desc': 'The CC30k dataset is designed to improve the prediction of reproducibility-oriented sentiments in machine learning papers. It contains 30,734 citation contexts labeled as Positive, Negative, or Neutral, reflecting the perceived reproducibility of the cited works. This dataset addresses a gap in resources for studying computational reproducibility by providing a robust framework for sentiment analysis. By fine-tuning large language models on this dataset, researchers can enhance their ability to assess the reproducibility of published findings in the machine learning community.'}, 'zh': {'title': 'CC30kæ•°æ®é›†ï¼šæå‡æœºå™¨å­¦ä¹ è®ºæ–‡å¯é‡å¤æ€§é¢„æµ‹çš„åˆ©å™¨', 'desc': 'CC30kæ•°æ®é›†åŒ…å«æ ‡è®°ä¸ºå¯é‡å¤æ€§å¯¼å‘æƒ…æ„Ÿçš„å¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹æœºå™¨å­¦ä¹ è®ºæ–‡å¯é‡å¤æ€§æ–¹é¢çš„å‡†ç¡®æ€§ã€‚è¯¥æ•°æ®é›†å…±åŒ…å«30,734ä¸ªå¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡è¢«æ ‡è®°ä¸ºç§¯æã€æ¶ˆææˆ–ä¸­ç«‹ï¼Œåæ˜ è¢«å¼•ç”¨è®ºæ–‡çš„å¯é‡å¤æ€§ã€‚é€šè¿‡ä¼—åŒ…å’Œæ§åˆ¶ç”Ÿæˆçš„è´Ÿé¢æ ‡ç­¾ï¼ŒCC30kè§£å†³äº†ä¼ ç»Ÿæƒ…æ„Ÿåˆ†ææ•°æ®é›†ä¸­ç¼ºä¹è´Ÿé¢æ ‡ç­¾çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåï¼Œä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯é‡å¤æ€§å¯¼å‘æƒ…æ„Ÿåˆ†ç±»ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œä¸ºæœºå™¨å­¦ä¹ è®ºæ–‡çš„å¯é‡å¤æ€§è¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10047', 'title': 'MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples', 'url': 'https://huggingface.co/papers/2511.10047', 'abstract': 'MuSc-V2 framework improves zero-shot anomaly detection by leveraging mutual scoring and similarity aggregation in both 2D and 3D data, achieving significant performance gains over existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a +23.7% AP gain on the MVTec 3D-AD dataset and a +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at https://github.com/HUST-SLOW/MuSc-V2{https://github.com/HUST-SLOW/MuSc-V2}.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'df8dad78f3e180de', 'authors': ['Xurui Li', 'Feng Xue', 'Yu Zhou'], 'affiliations': ['School of Computer Science, University of Trento', 'School of Electronic Information and Communications, Huazhong University of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10047.jpg', 'data': {'categories': ['#benchmark', '#3d', '#optimization', '#cv', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MuSc-V2 Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² (zero-shot) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒÑ ĞºĞ°Ğº Ğ² 2D, Ñ‚Ğ°Ğº Ğ¸ Ğ² 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸, Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ (Mutual Scoring Mechanism) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 2D Ğ¸ 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². MuSc-V2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: +23.7% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° ÑÑ€ĞµĞ´Ğ½ĞµĞ¹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MVTec 3D-AD Ğ¸ +19.3% Ğ½Ğ° Eyecandies, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Zero-Shot Anomaly Detection with MuSc-V2', 'desc': 'The MuSc-V2 framework enhances zero-shot anomaly detection by utilizing mutual scoring and similarity aggregation for both 2D and 3D data. It identifies normal patches that share similarities across products while recognizing that anomalies are often unique and isolated. The framework employs Iterative Point Grouping to improve 3D representations and reduce false positives, and it integrates multi-scale features through Similarity Neighborhood Aggregation. By implementing a Mutual Scoring Mechanism and Cross-modal Anomaly Enhancement, MuSc-V2 achieves significant performance improvements over existing benchmarks, demonstrating its effectiveness in diverse industrial applications.'}, 'zh': {'title': 'MuSc-V2ï¼šæå‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'MuSc-V2æ¡†æ¶é€šè¿‡åˆ©ç”¨äº’è¯„åˆ†æ•°å’Œç›¸ä¼¼æ€§èšåˆï¼Œæ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äº2Då’Œ3Dæ•°æ®ã€‚è¯¥æ–¹æ³•æ­ç¤ºäº†æ­£å¸¸å›¾åƒå—ä¸å…¶ä»–ç›¸ä¼¼å—ä¹‹é—´çš„å…³ç³»ï¼Œè€Œå¼‚å¸¸åˆ™é€šå¸¸æ˜¯å¤šæ ·åŒ–å’Œå­¤ç«‹çš„ã€‚MuSc-V2é‡‡ç”¨äº†è¿­ä»£ç‚¹åˆ†ç»„å’Œç›¸ä¼¼æ€§é‚»åŸŸèšåˆç­‰æŠ€æœ¯ï¼Œå¢å¼ºäº†ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é›¶æ ·æœ¬åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09146', 'title': 'DoPE: Denoising Rotary Position Embedding', 'url': 'https://huggingface.co/papers/2511.09146', 'abstract': 'Denoising Positional Encoding (DoPE) enhances length generalization in Transformer models by detecting and mitigating noisy frequency bands in positional embeddings, improving retrieval accuracy and reasoning stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io', 'score': 92, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '04c9467a2457b455', 'authors': ['Jing Xiong', 'Liyang Fan', 'Hui Shen', 'Zunhai Su', 'Min Yang', 'Lingpeng Kong', 'Ngai Wong'], 'affiliations': ['Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09146.jpg', 'data': {'categories': ['#optimization', '#training', '#long_context', '#reasoning', '#architecture'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'ĞÑ‡Ğ¸Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DoPE (Denoising Positional Encoding). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒÑ€ĞµĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ "attention sink" Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ´Ğ¾ 64K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Length Generalization in Transformers with DoPE', 'desc': "Denoising Positional Encoding (DoPE) is a method that improves how Transformer models handle longer sequences of data. It identifies and reduces noise in the positional embeddings, which helps the model retrieve information more accurately and reason more reliably. By using a technique based on truncated matrix entropy, DoPE can detect problematic frequency bands in the attention map. This approach not only addresses the limitations of existing positional encoding methods but also enhances the model's ability to generalize over longer contexts, as shown in various experiments."}, 'zh': {'title': 'å»å™ªä½ç½®ç¼–ç ï¼Œæå‡ Transformer æ¨¡å‹çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›', 'desc': 'Denoising Positional Encoding (DoPE) æ˜¯ä¸€ç§å¢å¼º Transformer æ¨¡å‹é•¿åº¦æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡æ£€æµ‹å’Œå‡è½»ä½ç½®åµŒå…¥ä¸­çš„å™ªå£°é¢‘ç‡å¸¦ï¼Œæ¥æé«˜æ£€ç´¢å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦è®­ç»ƒï¼Œåˆ©ç”¨æˆªæ–­çŸ©é˜µç†µæ¥è¯†åˆ«ç‰¹å¾å›¾ä¸­çš„å¼‚å¸¸é¢‘ç‡å¸¦ï¼Œå¹¶é€šè¿‡æ— å‚æ•°çš„é«˜æ–¯åˆ†å¸ƒé‡æ–°å‚æ•°åŒ–ç‰¹å¾å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDoPE åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶æ˜¾è‘—æ”¹å–„äº†æ£€ç´¢å‡†ç¡®æ€§å’Œæ¨ç†ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11434', 'title': 'WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation', 'url': 'https://huggingface.co/papers/2511.11434', 'abstract': "WEAVE introduces a comprehensive suite including a large dataset and a benchmark to assess and improve multi-turn, context-dependent image generation and editing in unified multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.", 'score': 44, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'f5edbe3e8fe1f722', 'authors': ['Wei Chow', 'Jiachun Pan', 'Yongyuan Liang', 'Mingze Zhou', 'Xue Song', 'Liyu Jia', 'Saining Zhang', 'Siliang Tang', 'Juncheng Li', 'Fengda Zhang', 'Weijia Wu', 'Hanwang Zhang', 'Tat-Seng Chua'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'University of Maryland, College Park', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11434.jpg', 'data': {'categories': ['#training', '#multimodal', '#benchmark', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ', 'desc': 'WEAVE Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ…, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… WEAVE-100k ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ 370 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ 500 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. WEAVEBench - ÑÑ‚Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· VLM-ÑÑƒĞ´ÑŒÑ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° WEAVE-100k Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'WEAVE: Advancing Multi-Turn Image Generation and Editing', 'desc': 'WEAVE is a new framework designed to enhance multi-turn, context-dependent image generation and editing using unified multimodal models (UMMs). It includes a large dataset, WEAVE-100k, which contains 100,000 samples and over 370,000 dialogue turns, allowing for complex reasoning in visual tasks. Additionally, WEAVEBench offers a benchmark with 100 tasks to evaluate models on their ability to handle multi-turn interactions and visual memory. This suite aims to address the limitations of existing datasets by providing a comprehensive resource for improving the capabilities of UMMs in real-world applications.'}, 'zh': {'title': 'WEAVEï¼šå¤šè½®å›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ–°è§†é‡', 'desc': 'WEAVEæ˜¯ä¸€ä¸ªå…¨é¢çš„å·¥å…·åŒ…ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæå‡å¤šè½®ã€ä¾èµ–ä¸Šä¸‹æ–‡çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½åŠ›ã€‚å®ƒåŒ…å«ä¸€ä¸ªå¤§å‹æ•°æ®é›†WEAVE-100kï¼Œæ¶µç›–äº†100,000ä¸ªäº¤é”™æ ·æœ¬ï¼Œæ¶‰åŠ370,000ä¸ªå¯¹è¯è½®æ¬¡å’Œ500,000å¼ å›¾åƒï¼Œæ”¯æŒæ¨ç†å†å²ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ã€‚WEAVEBenchæ˜¯ä¸€ä¸ªäººç±»æ ‡æ³¨çš„åŸºå‡†ï¼ŒåŒ…å«100ä¸ªä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤šè½®ç”Ÿæˆã€è§†è§‰è®°å¿†å’Œä¸–ç•ŒçŸ¥è¯†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡è¿™äº›å·¥å…·ï¼ŒWEAVEä¸ºå¤šæ¨¡æ€ç¤¾åŒºæä¾›äº†ç ”ç©¶ä¸Šä¸‹æ–‡äº¤é”™ç†è§£å’Œç”Ÿæˆçš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11238', 'title': 'Virtual Width Networks', 'url': 'https://huggingface.co/papers/2511.11238', 'abstract': 'Virtual Width Networks (VWN) enhance model efficiency by expanding representational width without increasing computational cost, accelerating optimization and improving loss reduction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.', 'score': 35, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '36e2095ae975dc37', 'authors': ['Seed', 'Baisheng Li', 'Banggu Wu', 'Bole Ma', 'Bowen Xiao', 'Chaoyi Zhang', 'Cheng Li', 'Chengyi Wang', 'Chengyin Xu', 'Chi Zhang', 'Chong Hu', 'Daoguang Zan', 'Defa Zhu', 'Dongyu Xu', 'Du Li', 'Faming Wu', 'Fan Xia', 'Ge Zhang', 'Guang Shi', 'Haobin Chen', 'Hongyu Zhu', 'Hongzhi Huang', 'Huan Zhou', 'Huanzhang Dou', 'Jianhui Duan', 'Jianqiao Lu', 'Jianyu Jiang', 'Jiayi Xu', 'Jiecao Chen', 'Jin Chen', 'Jin Ma', 'Jing Su', 'Jingji Chen', 'Jun Wang', 'Jun Yuan', 'Juncai Liu', 'Jundong Zhou', 'Kai Hua', 'Kai Shen', 'Kai Xiang', 'Kaiyuan Chen', 'Kang Liu', 'Ke Shen', 'Liang Xiang', 'Lin Yan', 'Lishu Luo', 'Mengyao Zhang', 'Ming Ding', 'Mofan Zhang', 'Nianning Liang', 'Peng Li', 'Penghao Huang', 'Pengpeng Mu', 'Qi Huang', 'Qianli Ma', 'Qiyang Min', 'Qiying Yu', 'Renming Pang', 'Ru Zhang', 'Shen Yan', 'Shen Yan', 'Shixiong Zhao', 'Shuaishuai Cao', 'Shuang Wu', 'Siyan Chen', 'Siyu Li', 'Siyuan Qiao', 'Tao Sun', 'Tian Xin', 'Tiantian Fan', 'Ting Huang', 'Ting-Han Fan', 'Wei Jia', 'Wenqiang Zhang', 'Wenxuan Liu', 'Xiangzhong Wu', 'Xiaochen Zuo', 'Xiaoying Jia', 'Ximing Yang', 'Xin Liu', 'Xin Yu', 'Xingyan Bin', 'Xintong Hao', 'Xiongcai Luo', 'Xujing Li', 'Xun Zhou', 'Yanghua Peng', 'Yangrui Chen', 'Yi Lin', 'Yichong Leng', 'Yinghao Li', 'Yingshuan Song', 'Yiyuan Ma', 'Yong Shan', 'Yongan Xiang', 'Yonghui Wu', 'Yongtao Zhang', 'Yongzhen Yao', 'Yu Bao', 'Yuehang Yang', 'Yufeng Yuan', 'Yunshui Li', 'Yuqiao Xian', 'Yutao Zeng', 'Yuxuan Wang', 'Zehua Hong', 'Zehua Wang', 'Zengzhi Wang', 'Zeyu Yang', 'Zhengqiang Yin', 'Zhenyi Lu', 'Zhexi Zhang', 'Zhi Chen', 'Zhi Zhang', 'Zhiqi Lin', 'Zihao Huang', 'Zilin Xu', 'Ziyun Wei', 'Zuo Wang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11238.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ° Ğ±ĞµĞ· Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Virtual Width Networks (VWN) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. VWN Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñƒ Ğ¾Ñ‚ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 2-3 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Model Efficiency with Virtual Width Networks', 'desc': "Virtual Width Networks (VWN) provide a way to increase the representational capacity of machine learning models without significantly raising computational costs. By separating the concept of representational width from the actual size of the model's backbone, VWN allows for a larger embedding space while maintaining low computational demands. Experiments show that expanding the virtual width can lead to significant improvements in optimization speed and loss reduction, especially as the model scales. This approach suggests a new avenue for enhancing the efficiency of large models by leveraging virtual width as a key factor in training effectiveness."}, 'zh': {'title': 'è™šæ‹Ÿå®½åº¦ç½‘ç»œï¼šæå‡æ¨¡å‹æ•ˆç‡çš„æ–°ç»´åº¦', 'desc': 'è™šæ‹Ÿå®½åº¦ç½‘ç»œï¼ˆVWNï¼‰é€šè¿‡æ‰©å±•è¡¨ç¤ºå®½åº¦è€Œä¸å¢åŠ è®¡ç®—æˆæœ¬ï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚è¿™ç§æ¡†æ¶å°†è¡¨ç¤ºå®½åº¦ä¸ä¸»å¹²å®½åº¦è§£è€¦ï¼Œæ‰©å¤§äº†åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒä¸»å¹²è®¡ç®—å‡ ä¹ä¸å˜ã€‚åœ¨å¤§è§„æ¨¡å®éªŒä¸­ï¼Œ8å€çš„æ‰©å±•ä½¿å¾—ä¼˜åŒ–é€Ÿåº¦æé«˜äº†2å€ï¼Œä¸‹ä¸€æ­¥é¢„æµ‹çš„é€Ÿåº¦æé«˜äº†3å€ã€‚VWNä¸ä»…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆå‡å°‘æŸå¤±ï¼Œè€Œä¸”éšç€è§„æ¨¡çš„å¢åŠ ï¼Œå…¶æ•ˆæœä¹Ÿè¶Šæ¥è¶Šæ˜¾è‘—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11134', 'title': 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2511.11134', 'abstract': "GGBench is introduced to evaluate geometric generative reasoning, addressing the gap in assessing integrated cognitive processes in multimodal models.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.", 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'cf1eb8f0638f6467', 'authors': ['Jingxuan Wei', 'Caijun Jia', 'Xi Bai', 'Xinglong Xu', 'Siyuan Li', 'Linzhuang Sun', 'Bihui Yu', 'Conghui He', 'Lijun Wu', 'Cheng Tan'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11134.jpg', 'data': {'categories': ['#multimodal', '#benchmark'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ GGBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ»Ğ¸Ğ±Ğ¾ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ»Ğ¸Ğ±Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. GGBench Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ğ½Ğ¾ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'GGBench: Bridging the Gap in Geometric Generative Reasoning Evaluation', 'desc': "GGBench is a new benchmark created to evaluate geometric generative reasoning in Unified Multimodal Models (UMMs). It addresses the limitation of current assessments that focus only on either understanding or generating images, without measuring how well models integrate these processes. By using geometric construction tasks, GGBench tests a model's ability to combine language understanding with visual generation. This framework aims to enhance the evaluation of AI systems, ensuring they can actively reason and construct solutions rather than just passively interpret data."}, 'zh': {'title': 'GGBenchï¼šè¯„ä¼°å‡ ä½•ç”Ÿæˆæ¨ç†çš„æ–°åŸºå‡†', 'desc': 'GGBenchæ˜¯ä¸€ä¸ªæ–°æå‡ºçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å‡ ä½•ç”Ÿæˆæ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨å¡«è¡¥å¤šæ¨¡æ€æ¨¡å‹ä¸­ç»¼åˆè®¤çŸ¥è¿‡ç¨‹è¯„ä¼°çš„ç©ºç™½ã€‚éšç€ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„å‡ºç°ï¼Œäººå·¥æ™ºèƒ½æ­£ä»è¢«åŠ¨æ„ŸçŸ¥è½¬å‘ä¸»åŠ¨çš„è·¨æ¨¡æ€ç”Ÿæˆã€‚ç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦å…³æ³¨åŒºåˆ†ç†è§£æˆ–ä¸å—é™åˆ¶çš„å›¾åƒç”Ÿæˆï¼Œæœªèƒ½æœ‰æ•ˆæµ‹é‡ç”Ÿæˆæ¨ç†çš„ç»¼åˆè®¤çŸ¥è¿‡ç¨‹ã€‚GGBenché€šè¿‡å‡ ä½•æ„å»ºçš„æ–¹å¼ï¼Œæä¾›äº†ä¸€ä¸ªç†æƒ³çš„æµ‹è¯•å¹³å°ï¼Œè¦æ±‚æ¨¡å‹èåˆè¯­è¨€ç†è§£å’Œç²¾ç¡®è§†è§‰ç”Ÿæˆçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08195', 'title': 'UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation', 'url': 'https://huggingface.co/papers/2511.08195', 'abstract': 'UI2Code$^\\text{N}$, a visual language model enhanced through staged pretraining, fine-tuning, and reinforcement learning, achieves superior performance in UI-to-code generation, editing, and polishing with iterative feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.', 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '59a91dba9da67a0e', 'authors': ['Zhen Yang', 'Wenyi Hong', 'Mingde Xu', 'Xinyue Fan', 'Weihan Wang', 'Jiele Cheng', 'Xiaotao Gu', 'Jie Tang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08195.jpg', 'data': {'categories': ['#plp', '#training', '#multimodal', '#benchmark', '#rl', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ² ĞºĞ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ', 'desc': 'UI2Code^N â€” ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ´ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… UI-to-code Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Claude-4-Sonnet Ğ¸ GPT-5.'}, 'en': {'title': 'Revolutionizing UI-to-Code Generation with Iterative Feedback', 'desc': 'UI2Code^N is a visual language model designed to improve the process of converting user interfaces (UIs) into code. It utilizes a combination of staged pretraining, fine-tuning, and reinforcement learning to enhance its performance in generating, editing, and polishing UI code. The model addresses limitations in existing approaches by incorporating iterative visual feedback and supporting multimodal coding capabilities. Experiments demonstrate that UI2Code^N achieves state-of-the-art results, outperforming other open-source models and matching the performance of leading closed-source models.'}, 'zh': {'title': 'UI2Code^Nï¼šæå‡ç”¨æˆ·ç•Œé¢ç¼–ç çš„æ™ºèƒ½æ¨¡å‹', 'desc': 'UI2Code^N æ˜¯ä¸€ç§é€šè¿‡åˆ†é˜¶æ®µé¢„è®­ç»ƒã€å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰åˆ°ä»£ç çš„ç”Ÿæˆã€ç¼–è¾‘å’Œæ¶¦è‰²ã€‚è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€ç¼–ç èƒ½åŠ›ä¸è¶³å’Œç¼ºä¹è¿­ä»£è§†è§‰åé¦ˆçš„é—®é¢˜ã€‚é€šè¿‡äº¤äº’å¼çš„ UI åˆ°ä»£ç èŒƒå¼ï¼ŒUI2Code^N å®ç°äº†å¤šæ¨¡æ€ç¼–ç çš„åŸºç¡€æ€§æ”¹è¿›ï¼Œå¹¶ç»Ÿä¸€äº† UI ç”Ÿæˆã€ç¼–è¾‘å’Œæ¶¦è‰²ä¸‰å¤§åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUI2Code^N åœ¨å¼€æºæ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ï¼Œä¸”ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹å¦‚ Claude-4-Sonnet å’Œ GPT-5 çš„è¡¨ç°ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11062', 'title': 'LiteAttention: A Temporal Sparse Attention for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2511.11062', 'abstract': 'LiteAttention exploits temporal coherence in diffusion attention to accelerate video generation without quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step t typically remain so at step t+Î´. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.', 'score': 30, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'bf8d12079ba36905', 'authors': ['Dor Shmilovich', 'Tony Wu', 'Aviad Dahan', 'Yuval Domb'], 'affiliations': ['MoonMath.ai'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11062.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#open_source', '#video', '#architecture', '#inference'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ (ÑĞ¿Ğ°Ñ€ÑĞ½Ğ¾ÑÑ‚Ğ¸) Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ¿Ğ»Ğ¸Ñ‚Ğ¾Ğº. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LiteAttention Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ FlashAttention Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Accelerating Video Generation with Temporal Coherence', 'desc': 'LiteAttention is a novel approach that enhances video generation by utilizing the temporal coherence of attention patterns in diffusion models. Traditional methods face challenges with high computational costs due to the quadratic complexity of attention mechanisms, which can slow down the generation process. By recognizing that certain attention tiles remain non-essential across multiple denoising steps, LiteAttention allows for efficient computation by skipping unnecessary calculations. This method combines the benefits of dynamic and static attention strategies, resulting in faster video generation without compromising quality.'}, 'zh': {'title': 'åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§åŠ é€Ÿè§†é¢‘ç”Ÿæˆ', 'desc': 'LiteAttentionåˆ©ç”¨æ‰©æ•£æ³¨æ„åŠ›ä¸­çš„æ—¶é—´ä¸€è‡´æ€§æ¥åŠ é€Ÿè§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¸æŸå¤±è´¨é‡ã€‚ä¼ ç»Ÿçš„æ‰©æ•£å˜æ¢å™¨åœ¨è§†é¢‘ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºæ³¨æ„åŠ›å¤æ‚åº¦å‘ˆå¹³æ–¹å¢é•¿ï¼Œå¯¼è‡´å»¶è¿Ÿè¿‡é«˜ã€‚LiteAttentioné€šè¿‡è¯†åˆ«æ‰©æ•£æ³¨æ„åŠ›çš„ç¨€ç–æ¨¡å¼åœ¨å»å™ªæ­¥éª¤ä¹‹é—´çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæå‰æ ‡è®°éå¿…è¦çš„åŒºåŸŸï¼Œä»è€Œå‡å°‘å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŠ¨æ€æ–¹æ³•çš„é€‚åº”æ€§å’Œé™æ€æ–¹æ³•çš„é«˜æ•ˆæ€§ï¼Œå®ç°äº†åœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ é€Ÿè§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08585', 'title': 'Simulating the Visual World with Artificial Intelligence: A Roadmap', 'url': 'https://huggingface.co/papers/2511.08585', 'abstract': 'Video generation is evolving towards foundation models that integrate world simulation and rendering to produce physically plausible and interactive videos.  \t\t\t\t\tAI-generated summary \t\t\t\t The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.', 'score': 29, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '6ac10447f07a93ef', 'authors': ['Jingtong Yue', 'Ziqi Huang', 'Zhaoxi Chen', 'Xintao Wang', 'Pengfei Wan', 'Ziwei Liu'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Robotics Institute, Carnegie Mellon University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08585.jpg', 'data': {'categories': ['#video', '#multimodal', '#robotics'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğ¾ĞºĞ½Ğ¾ Ğ² Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğº Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ğº Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµÑ€, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Building Interactive Worlds: The Future of Video Generation', 'desc': 'This paper discusses the evolution of video generation towards foundation models that combine world simulation and rendering. These models not only create visually appealing videos but also simulate physical interactions and dynamics, allowing for interactive experiences. The authors categorize the development of these models into four generations, each enhancing capabilities like visual reasoning and planning. They also address challenges and future directions for improving agent intelligence within these systems.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆçš„æœªæ¥ï¼šåŸºç¡€æ¨¡å‹ä¸ç‰©ç†æ¨¡æ‹Ÿçš„ç»“åˆ', 'desc': 'è§†é¢‘ç”ŸæˆæŠ€æœ¯æ­£åœ¨å‘åŸºç¡€æ¨¡å‹å‘å±•ï¼Œè¿™äº›æ¨¡å‹ç»“åˆäº†ä¸–ç•Œæ¨¡æ‹Ÿå’Œæ¸²æŸ“æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆç‰©ç†ä¸Šåˆç†ä¸”å¯äº¤äº’çš„è§†é¢‘ã€‚è¿™äº›è§†é¢‘åŸºç¡€æ¨¡å‹ä¸ä»…æ˜¯è§†è§‰ç”Ÿæˆå™¨ï¼Œè¿˜ä½œä¸ºéšå¼ä¸–ç•Œæ¨¡å‹ï¼Œæ¨¡æ‹Ÿç‰©ç†åŠ¨æ€ã€ä»£ç†ä¸ç¯å¢ƒçš„äº¤äº’ä»¥åŠä»»åŠ¡è§„åˆ’ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†è§†é¢‘ç”Ÿæˆçš„æ¼”å˜ï¼Œæå‡ºç°ä»£è§†é¢‘åŸºç¡€æ¨¡å‹ç”±éšå¼ä¸–ç•Œæ¨¡å‹å’Œè§†é¢‘æ¸²æŸ“å™¨ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ç»„æˆã€‚æˆ‘ä»¬æ¢è®¨äº†è§†é¢‘ç”Ÿæˆçš„å››ä¸ªå‘å±•é˜¶æ®µï¼Œå¼ºè°ƒäº†æ¯ä¸ªé˜¶æ®µçš„æ ¸å¿ƒç‰¹å¾åŠå…¶åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œäº’åŠ¨æ¸¸æˆç­‰é¢†åŸŸçš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11257', 'title': 'AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery', 'url': 'https://huggingface.co/papers/2511.11257', 'abstract': 'Axonopedia, an AI agent utilizing LLMs and a multimodal foundation model, enhances property prediction and molecular design for Ionic Liquids through hierarchical search and real-world validation.  \t\t\t\t\tAI-generated summary \t\t\t\t The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.', 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'b22ec9da41588237', 'authors': ['Yuqi Yin', 'Yibo Fu', 'Siyuan Wang', 'Peng Sun', 'Hongyu Wang', 'Xiaohui Wang', 'Lei Zheng', 'Zhiyong Li', 'Zhirong Liu', 'Jianji Wang', 'Zhaoxi Sun'], 'affiliations': ['College of Chemistry and Molecular Engineering, Peking University', 'Faculty of Synthetic Biology, Shenzhen University of Advanced Technology', 'Henan Key Laboratory of Green Chemistry, Collaborative Innovation Center of Henan Province for Green Manufacturing of Fine Chemicals, Key Laboratory of Green Chemical Media and Reactions, Ministry of Education, School of Chemistry and Chemical Engineering, Henan Normal University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning and NYU-ECNU Center for Computational Chemistry, NYU-Shanghai'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11257.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#agents', '#healthcare'], 'emoji': 'âš—ï¸', 'ru': {'title': 'LLM-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¶Ğ¸Ğ´ĞºĞ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AIonopedia â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¶Ğ¸Ğ´ĞºĞ¾ÑÑ‚ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¶Ğ¸Ğ´ĞºĞ¾ÑÑ‚ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¶Ğ¸Ğ´ĞºĞ¾ÑÑ‚ÑÑ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹ in vitro, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Accelerating Ionic Liquid Discovery with AIonopedia', 'desc': 'Axonopedia is an innovative AI agent that uses Large Language Models (LLMs) and a multimodal foundation model to improve the prediction of properties and design of Ionic Liquids (ILs). It addresses significant challenges in IL discovery, such as limited data and low model accuracy, by employing a hierarchical search strategy for molecular screening. The model has been trained on a comprehensive dataset specifically curated for ILs, resulting in enhanced performance in property predictions. Additionally, real-world validation has shown that Axonopedia can effectively generalize and modify ILs, proving its practical utility in accelerating the discovery process.'}, 'zh': {'title': 'Axonopediaï¼šåŠ é€Ÿç¦»å­æ¶²ä½“å‘ç°çš„æ™ºèƒ½ä»£ç†', 'desc': 'Axonopedia æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œæ—¨åœ¨æé«˜ç¦»å­æ¶²ä½“çš„æ€§è´¨é¢„æµ‹å’Œåˆ†å­è®¾è®¡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ†å±‚æœç´¢æ¶æ„ï¼Œèƒ½å¤Ÿè¿›è¡Œå‡†ç¡®çš„æ€§è´¨é¢„æµ‹å’Œåˆ†å­ç­›é€‰ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°åˆ›å»ºçš„å…¨é¢ç¦»å­æ¶²ä½“æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°äº†è¯¥æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜è¶Šã€‚æ­¤å¤–ï¼ŒAxonopedia è¿˜é€šè¿‡å®é™…å®éªŒéªŒè¯äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿäº†ç¦»å­æ¶²ä½“çš„å‘ç°è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07403', 'title': 'SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards', 'url': 'https://huggingface.co/papers/2511.07403', 'abstract': 'SpatialThinker, a 3D-aware MLLM trained with RL, enhances spatial understanding by integrating structured spatial grounding and multi-step reasoning, outperforming existing models on spatial VQA and real-world benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '82855ce8da63de9d', 'authors': ['Hunar Batra', 'Haoqin Tu', 'Hardy Chen', 'Yuanze Lin', 'Cihang Xie', 'Ronald Clark'], 'affiliations': ['University of California, Santa Cruz', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07403.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#3d', '#dataset', '#rl', '#reasoning', '#cv', '#synthetic'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'SpatialThinker â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. SpatialThinker-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ GPT-4o Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'SpatialThinker: Advancing 3D Spatial Understanding in MLLMs', 'desc': 'SpatialThinker is a novel 3D-aware multimodal large language model (MLLM) that enhances spatial understanding through reinforcement learning (RL). It integrates structured spatial grounding and multi-step reasoning, allowing it to outperform existing models in spatial visual question answering (VQA) and real-world benchmarks. The model constructs a scene graph to simulate human-like spatial perception and utilizes a data synthesis pipeline to create a high-quality spatial VQA dataset. By combining spatial supervision with reward-aligned reasoning, SpatialThinker achieves significant improvements in 3D spatial understanding with limited data.'}, 'zh': {'title': 'SpatialThinkerï¼šæå‡ç©ºé—´ç†è§£çš„3Dæ„ŸçŸ¥æ¨¡å‹', 'desc': 'SpatialThinkeræ˜¯ä¸€ç§å…·æœ‰3Dæ„ŸçŸ¥èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œæå‡äº†ç©ºé—´ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºä¸ä»»åŠ¡ç›¸å…³çš„å¯¹è±¡å’Œç©ºé—´å…³ç³»çš„åœºæ™¯å›¾ï¼Œæ¨¡æ‹Ÿäººç±»çš„ç©ºé—´æ„ŸçŸ¥ï¼Œå¹¶é€šè¿‡å¯†é›†çš„ç©ºé—´å¥–åŠ±è¿›è¡Œæ¨ç†ã€‚SpatialThinkerçš„ä¸¤ä¸ªä¸»è¦è´¡çŒ®æ˜¯ï¼šç”Ÿæˆé«˜è´¨é‡ç©ºé—´è§†è§‰é—®ç­”æ•°æ®é›†STVQA-7Kçš„æ•°æ®åˆæˆç®¡é“ï¼Œä»¥åŠé€šè¿‡å¤šç›®æ ‡å¯†é›†ç©ºé—´å¥–åŠ±è¿›è¡Œçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpatialThinkeråœ¨ç©ºé—´ç†è§£å’Œç°å®ä¸–ç•Œè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡ ä¹å°†åŸºç¡€æ¨¡å‹çš„å¢ç›Šç¿»å€ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11373', 'title': 'MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism', 'url': 'https://huggingface.co/papers/2511.11373', 'abstract': 'MarsRL enhances multi-agent reasoning systems by optimizing all agents jointly, improving accuracy in complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '25407a6cb82aade2', 'authors': ['Shulin Liu', 'Dong Du', 'Tao Yang', 'Yang Li', 'Boyu Qiu'], 'affiliations': ['Tencent Hunyuan Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11373.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#agents', '#reasoning'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'MarsRL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Solver, Verifier Ğ¸ Corrector. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ MarsRL Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-30B Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ñ 86.5% Ğ´Ğ¾ 93.3% Ğ½Ğ° AIME2025 Ğ¸ Ñ 64.9% Ğ´Ğ¾ 73.8% Ğ½Ğ° BeyondAIME. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Optimizing Multi-Agent Reasoning for Superior Accuracy', 'desc': "MarsRL is a new framework that enhances multi-agent reasoning systems by allowing all agents to be optimized together, which leads to better performance in complex reasoning tasks. It uses a unique approach called agentic pipeline parallelism, which helps in managing long sequences of reasoning more efficiently. By introducing specific reward mechanisms for each agent, MarsRL reduces noise in the feedback they receive, improving their learning process. The results show significant accuracy improvements in reasoning tasks, demonstrating MarsRL's potential to expand the capabilities of multi-agent systems in various applications."}, 'zh': {'title': 'MarsRLï¼šæå‡å¤šæ™ºèƒ½ä½“æ¨ç†çš„å…¨æ–°æ¡†æ¶', 'desc': 'MarsRL æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è”åˆä¼˜åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æ‰€æœ‰ä»£ç†æ¥å¢å¼ºå¤šæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç‰¹å®šäºä»£ç†çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥å‡å°‘å¥–åŠ±å™ªå£°ï¼Œå¹¶é‡‡ç”¨ç®¡é“å¼è®­ç»ƒæ–¹æ³•ï¼Œæé«˜å¤„ç†é•¿è½¨è¿¹çš„æ•ˆç‡ã€‚é€šè¿‡åº”ç”¨äº Qwen3-30B-A3B-Thinking-2507ï¼ŒMarsRL æ˜¾è‘—æé«˜äº† AIME2025 çš„å‡†ç¡®ç‡ï¼Œä» 86.5% æå‡è‡³ 93.3%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMarsRL æœ‰æ½œåŠ›æ¨åŠ¨å¤šæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿçš„å‘å±•ï¼Œå¹¶æ‰©å¤§å…¶åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09915', 'title': 'HI-TransPA: Hearing Impairments Translation Personal Assistant', 'url': 'https://huggingface.co/papers/2511.09915', 'abstract': 'HI-TransPA, an instruction-driven audio-visual personal assistant, uses Omni-Model paradigm to translate and dialogue by fusing speech with lip dynamics, achieving state-of-the-art performance in assistive communication for hearing-impaired individuals.  \t\t\t\t\tAI-generated summary \t\t\t\t To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '1ce87aa4b2be3e13', 'authors': ['Zhiming Ma', 'Shiyu Gan', 'Junhao Zhao', 'Xianming Li', 'Qingyun Pan', 'Peidong Wang', 'Mingjun Pan', 'Yuhao Mo', 'Jiajie Cheng', 'Chengxin Chen', 'Zhonglun Cao', 'Chonghan Liu', 'Shi Cheng'], 'affiliations': ['BUPT', 'CMIC', 'Northeastern University', 'Peking University', 'Qiyuan Tech', 'SmartFlowAI Research', 'Tongji University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09915.jpg', 'data': {'categories': ['#audio', '#multimodal', '#data', '#dataset', '#healthcare'], 'emoji': 'ğŸ‘‚', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ³Ğ»ÑƒÑ…Ğ¸Ğ¼ Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾ÑĞ»Ñ‹ÑˆĞ°Ñ‰Ğ¸Ğ¼ Ğ»ÑĞ´ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° HI-TransPA â€” Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Omni-Model Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ğ»ÑĞ´ÑĞ¼ Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»ÑƒÑ…Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµÑ‡Ñ‘Ñ‚ĞºÑƒÑ Ñ€ĞµÑ‡ÑŒ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ³ÑƒĞ± Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ², ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ³ÑƒĞ±Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ curriculum learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ SigLIP ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Unified 3D-Resampler Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HI-Dialogue.'}, 'en': {'title': 'Revolutionizing Communication for the Hearing-Impaired with HI-TransPA', 'desc': 'HI-TransPA is an innovative audio-visual personal assistant designed to enhance communication for hearing-impaired individuals by integrating speech and lip movements. It employs the Omni-Model paradigm, which allows for simultaneous translation and dialogue within a single framework. The model addresses challenges such as noisy data by using a robust preprocessing pipeline that stabilizes lip movements and assesses the quality of multimodal inputs. Through advanced training techniques and a specialized dataset, HI-TransPA achieves leading performance in accurately conveying spoken language and its meaning.'}, 'zh': {'title': 'HI-TransPAï¼šåŠ©åŠ›å¬éšœäººå£«çš„æ™ºèƒ½äº¤æµåŠ©æ‰‹', 'desc': 'HI-TransPAæ˜¯ä¸€ç§åŸºäºæŒ‡ä»¤é©±åŠ¨çš„éŸ³è§†é¢‘ä¸ªäººåŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©å¬éšœäººå£«è¿›è¡Œæ—¥å¸¸äº¤æµã€‚è¯¥æ¨¡å‹é‡‡ç”¨Omni-ModelèŒƒå¼ï¼Œå°†æ¨¡ç³Šçš„è¯­éŸ³ä¸é«˜å¸§ç‡çš„å”‡éƒ¨åŠ¨æ€èåˆï¼Œå®ç°ç¿»è¯‘å’Œå¯¹è¯åŠŸèƒ½ã€‚ä¸ºäº†è§£å†³å™ªå£°å’Œå¼‚æ„æ•°æ®çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„é¢„å¤„ç†å’Œç­–åˆ’ç®¡é“ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ ·æœ¬çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHI-TransPAåœ¨å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09554', 'title': 'RF-DETR: Neural Architecture Search for Real-Time Detection Transformers', 'url': 'https://huggingface.co/papers/2511.09554', 'abstract': 'RF-DETR, a light-weight detection transformer, uses weight-sharing NAS to optimize accuracy and latency for real-time detection across diverse datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the "tunable knobs" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'b74f4e94edf93d9a', 'authors': ['Isaac Robinson', 'Peter Robicheaux', 'Matvei Popov', 'Deva Ramanan', 'Neehar Peri'], 'affiliations': ['Carnegie Mellon University', 'Roboflow'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09554.jpg', 'data': {'categories': ['#benchmark', '#inference', '#architecture', '#cv', '#small_models'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'RF-DETR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ»Ñ‘Ğ³ĞºÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ğ¼Ğ¸ÑÑ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ÑĞ¶Ñ‘Ğ»Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… COCO Ğ¸ Roboflow100-VL.'}, 'en': {'title': 'Optimizing Real-Time Detection with RF-DETR', 'desc': 'RF-DETR is a lightweight detection transformer designed to optimize both accuracy and latency for real-time object detection across various datasets. It employs weight-sharing neural architecture search (NAS) to explore different configurations, allowing it to find the best accuracy-latency trade-offs without the need for retraining. This method enhances the transferability of detection transformers to new domains, addressing the challenges faced by traditional heavy-weight vision-language models. The results show that RF-DETR outperforms previous state-of-the-art methods, achieving significant improvements in average precision on benchmark datasets like COCO and Roboflow100-VL.'}, 'zh': {'title': 'RF-DETRï¼šè½»é‡çº§æ£€æµ‹å˜æ¢å™¨çš„çªç ´', 'desc': 'RF-DETRæ˜¯ä¸€ç§è½»é‡çº§çš„æ£€æµ‹å˜æ¢å™¨ï¼Œåˆ©ç”¨æƒé‡å…±äº«çš„ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ¥ä¼˜åŒ–å®æ—¶æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå»¶è¿Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„åŸºç¡€ç½‘ç»œï¼Œé’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¯„ä¼°æˆåƒä¸Šä¸‡ç§ç½‘ç»œé…ç½®ï¼Œä»¥å®ç°ä¸åŒçš„å‡†ç¡®æ€§å’Œå»¶è¿Ÿæƒè¡¡ã€‚RF-DETRåœ¨COCOå’ŒRoboflow100-VLç­‰æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„å®æ—¶æ£€æµ‹æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ ·åŒ–ç›®æ ‡é¢†åŸŸçš„è½¬ç§»èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒRF-DETR (2x-large)æ˜¯é¦–ä¸ªåœ¨COCOä¸Šè¶…è¿‡60 APçš„å®æ—¶æ£€æµ‹å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10492', 'title': "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding", 'url': 'https://huggingface.co/papers/2511.10492', 'abstract': 'A framework integrates human priors into end-to-end generative recommenders, enhancing accuracy and beyond-accuracy objectives by leveraging lightweight adapter heads and hierarchical composition strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.   Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'bf674da1a065786e', 'authors': ['Yunkai Zhang', 'Qiang Zhang', 'Feng Lin', 'Ruizhong Qiu', 'Hanchao Yu', 'Jiayi Liu', 'Yinglong Xia', 'Zhuoran Yu', 'Zeyu Zheng', 'Diji Yang'], 'affiliations': ['BAIR (UC Berkeley)', 'Meta AI', 'UC Santa Cruz'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10492.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€-Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ² Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾ÑĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° ĞµÑ‘ Ñ€Ğ°Ğ¼ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Integrating Human Knowledge for Smarter Recommendations', 'desc': "This paper presents a new framework that incorporates human knowledge, referred to as human priors, into generative recommender systems to improve their performance. By using lightweight adapter heads and a hierarchical composition strategy, the framework allows for end-to-end training while maintaining the benefits of structured domain knowledge. This integration helps the model better understand user intent and achieve objectives beyond just accuracy, such as diversity and personalization. Experiments show that this approach not only enhances accuracy but also improves the model's ability to utilize longer contexts and larger sizes effectively."}, 'zh': {'title': 'å°†äººç±»æ™ºæ…§èå…¥æ¨èç³»ç»Ÿï¼Œæå‡å‡†ç¡®æ€§ä¸å¤šæ ·æ€§', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå°†äººç±»å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°ç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨èç³»ç»Ÿä¸­ï¼Œä»¥æé«˜æ¨èçš„å‡†ç¡®æ€§å’Œå…¶ä»–ç›®æ ‡ï¼Œå¦‚å¤šæ ·æ€§å’Œä¸ªæ€§åŒ–ã€‚æˆ‘ä»¬åˆ©ç”¨è½»é‡çº§çš„é€‚é…å™¨å¤´å’Œå±‚æ¬¡åŒ–ç»„åˆç­–ç•¥ï¼Œç›´æ¥åœ¨ç”Ÿæˆæ¨èæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥è¿™äº›å…ˆéªŒçŸ¥è¯†ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶åœ¨ä¸åŒçš„å…ˆéªŒç±»å‹ä¹‹é—´å»ºæ¨¡å¤æ‚çš„äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤§å‹æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨èçš„å‡†ç¡®æ€§å’Œå…¶ä»–ç›®æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10984', 'title': 'DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains', 'url': 'https://huggingface.co/papers/2511.10984', 'abstract': 'A new benchmark DiscoX and evaluation system Metric-S are introduced to assess discourse-level and expert-level Chinese-English translation, highlighting the challenges in achieving professional-grade machine translation.  \t\t\t\t\tAI-generated summary \t\t\t\t The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '33bf131c1d40901d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#benchmark', '#open_source', '#machine_translation', '#dataset', '#multilingual'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑƒÑ€Ñ-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DiscoX Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Metric-S Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸ÑĞºÑƒÑ€Ñ-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº. DiscoX ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 200 Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸Ğ· 7 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 1700 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Metric-S â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±ĞµĞ³Ğ»Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ.'}, 'en': {'title': 'Bridging the Gap in Expert-Level Translation Evaluation', 'desc': 'This paper introduces DiscoX, a new benchmark designed to evaluate discourse-level and expert-level translation between Chinese and English. It highlights the inadequacies of current evaluation methods that focus mainly on segment-level accuracy, neglecting the need for coherence and precision in professional translations. Alongside DiscoX, the authors present Metric-S, a reference-free evaluation system that assesses translation quality in terms of accuracy, fluency, and appropriateness, showing strong alignment with human evaluations. The findings reveal that even advanced language models still fall short of human expert performance, emphasizing the ongoing challenges in achieving high-quality machine translation.'}, 'zh': {'title': 'æå‡ç¿»è¯‘è´¨é‡çš„æ–°åŸºå‡†ä¸è¯„ä¼°ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•DiscoXå’Œè¯„ä¼°ç³»ç»ŸMetric-Sï¼Œç”¨äºè¯„ä¼°ä¸­æ–‡-è‹±æ–‡ç¿»è¯‘çš„è¯­ç¯‡çº§å’Œä¸“å®¶çº§è¡¨ç°ã€‚å°½ç®¡ä¸“ä¸šé¢†åŸŸçš„ç¿»è¯‘éœ€è¦è¯­ç¯‡è¿è´¯æ€§å’Œä¸¥æ ¼çš„æœ¯è¯­ç²¾ç¡®æ€§ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ®µè½çº§çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚DiscoXåŒ…å«æ¥è‡ª7ä¸ªé¢†åŸŸçš„200ç¯‡ä¸“ä¸šæ–‡æœ¬ï¼Œå¹³å‡é•¿åº¦è¶…è¿‡1700ä¸ªè¯å…ƒï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€è¯„ä¼°ç©ºç™½ã€‚Metric-Sæ˜¯ä¸€ä¸ªæ— å‚è€ƒçš„è¯„ä¼°ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨å‡†ç¡®æ€§ã€æµç•…æ€§å’Œé€‚å½“æ€§æ–¹é¢æä¾›ç»†è‡´çš„è‡ªåŠ¨è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2511.11519', 'title': 'Experience-Guided Adaptation of Inference-Time Reasoning Strategies', 'url': 'https://huggingface.co/papers/2511.11519', 'abstract': 'Experience-Guided Reasoner dynamically generates and optimizes computational strategies at inference time, adapting to problems using accumulated experience and improving accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '5e0282ead304181f', 'authors': ['Adam Stein', 'Matthew Trager', 'Benjamin Bowman', 'Michael Kleinman', 'Aditya Chattopadhyay', 'Wei Xia', 'Stefano Soatto'], 'affiliations': ['AWS AI', 'University of Pennsylvania'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11519.jpg', 'data': {'categories': ['#alignment', '#optimization', '#benchmark', '#agents', '#reasoning', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‹Ñ‚: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°', 'desc': 'Experience-Guided Reasoner â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°-ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ½Ğ¾ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Guide Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚ÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° Consolidator Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. ĞĞ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 14% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² 111 Ñ€Ğ°Ğ·, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ÑÑ Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°.'}, 'en': {'title': 'Dynamic Strategy Optimization for Enhanced AI Problem Solving', 'desc': 'The Experience-Guided Reasoner (EGuR) is a machine learning system that creates and optimizes problem-solving strategies during inference, using its past experiences to enhance both accuracy and efficiency. Unlike traditional systems that only adjust inputs or remain static after deployment, EGuR dynamically generates complete computational procedures, including prompts and sampling parameters, tailored to specific problems. It consists of two main components: a Guide that proposes various strategies based on current challenges and a Consolidator that refines these strategies using feedback from their execution. This innovative approach allows EGuR to significantly improve performance on complex tasks while drastically reducing resource usage as it learns from experience.'}, 'zh': {'title': 'åŠ¨æ€é€‚åº”ï¼Œä¼˜åŒ–æ¨ç†ç­–ç•¥', 'desc': 'ç»éªŒå¼•å¯¼æ¨ç†å™¨ï¼ˆEGuRï¼‰åœ¨æ¨ç†æ—¶åŠ¨æ€ç”Ÿæˆå’Œä¼˜åŒ–è®¡ç®—ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®ç§¯ç´¯çš„ç»éªŒé€‚åº”ä¸åŒé—®é¢˜ï¼Œä»è€Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ƒç­–ç•¥ï¼Œçµæ´»è°ƒæ•´ç­–ç•¥ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºã€é‡‡æ ·å‚æ•°å’Œå·¥å…·é…ç½®ã€‚EGuRåŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šå¼•å¯¼å™¨ç”Ÿæˆå¤šä¸ªå€™é€‰ç­–ç•¥ï¼Œè€Œæ•´åˆå™¨åˆ™æ ¹æ®æ‰§è¡Œåé¦ˆæ”¹è¿›æœªæ¥çš„ç­–ç•¥ç”Ÿæˆã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒEGuRåœ¨å‡†ç¡®æ€§ä¸Šæ¯”æœ€å¼ºåŸºçº¿æé«˜äº†14%ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é™ä½äº†111å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11002', 'title': 'EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation', 'url': 'https://huggingface.co/papers/2511.11002', 'abstract': 'EmoVid, a multimodal emotion-annotated video dataset, bridges emotion understanding with video generation, leading to improved emotional expression in generated videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'c8b7f47c58573ba0', 'authors': ['Zongyang Qiu', 'Bingyuan Wang', 'Xingbei Chen', 'Yingqing He', 'Zeyu Wang'], 'affiliations': ['Fudan University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11002.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#open_source', '#video', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EmoVid, Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹ Ğ¸ ĞºĞ»Ğ¸Ğ¿Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² ÑĞ²ÑĞ·Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… insights Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan2.1. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text-to-video Ğ¸ image-to-video, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ.'}, 'en': {'title': 'Bridging Emotions and Video Generation with EmoVid', 'desc': 'EmoVid is a new dataset that combines emotions with video generation, helping machines understand and express feelings in videos. Current video generation methods often ignore emotional aspects, focusing instead on basic visual qualities. EmoVid includes various types of videos, like cartoons and movie clips, all labeled with emotions and visual features. By analyzing these videos, the researchers created a method to generate videos that better reflect emotions, improving the quality and relevance of AI-generated content.'}, 'zh': {'title': 'æƒ…æ„Ÿé©±åŠ¨çš„è§†é¢‘ç”Ÿæˆæ–°çºªå…ƒ', 'desc': 'EmoVidæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æƒ…æ„Ÿæ ‡æ³¨è§†é¢‘æ•°æ®é›†ï¼Œæ—¨åœ¨å°†æƒ…æ„Ÿç†è§£ä¸è§†é¢‘ç”Ÿæˆç›¸ç»“åˆï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚ç°æœ‰çš„è§†é¢‘ç”Ÿæˆç³»ç»Ÿä¸»è¦å…³æ³¨ä½çº§è§†è§‰æŒ‡æ ‡ï¼Œè€Œå¿½è§†äº†æƒ…æ„Ÿç»´åº¦ã€‚EmoVidå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæä¾›äº†ä¸“é—¨ç”¨äºåˆ›æ„åª’ä½“çš„æƒ…æ„Ÿæ ‡æ³¨è§†é¢‘æ•°æ®ï¼ŒåŒ…æ‹¬å¡é€šåŠ¨ç”»ã€ç”µå½±ç‰‡æ®µå’ŒåŠ¨ç”»è´´çº¸ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†è§†è§‰ç‰¹å¾ä¸æƒ…æ„Ÿæ„ŸçŸ¥ä¹‹é—´çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ï¼Œå¹¶åŸºäºè¿™äº›è§è§£å¼€å‘äº†æƒ…æ„Ÿæ¡ä»¶çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10899', 'title': 'From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models', 'url': 'https://huggingface.co/papers/2511.10899', 'abstract': 'Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '2001a55bee66c0a9', 'authors': ['Farima Fatahi Bayat', 'Pouya Pezeshkpour', 'Estevam Hruschka'], 'affiliations': ['Megagon Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10899.jpg', 'data': {'categories': ['#training', '#interpretability', '#benchmark', '#open_source', '#math', '#dataset', '#reasoning'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ, Ğ½Ğ¾ Ğ²Ñ€ĞµĞ´ÑÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Tool-Induced Myopia (TIM) â€” ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ¾Ğ¼ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€ ĞºĞ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PYMATH Ñ 1679 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´, Ğ³Ğ´Ğµ ĞºĞ¾Ğ´ Ğ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½, Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 19.3 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ, Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¼ĞµÑ‰Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾, Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reasoning in Tool-Augmented Language Models', 'desc': 'This paper investigates the performance of Tool-augmented Language Models (TaLMs) when using external tools like the Code Interpreter. It identifies a problem called Tool-Induced Myopia (TIM), where TaLMs rely too heavily on tool outputs instead of reasoning through problems, leading to solutions that seem correct but lack proper justification. The study uses a benchmark of mathematical problems to show that while TaLMs can achieve higher accuracy with tools, their reasoning quality declines significantly compared to non-tool models. The authors propose a new framework to help TaLMs use tools more effectively, enhancing both their accuracy and reasoning capabilities.'}, 'zh': {'title': 'å·¥å…·ä½¿ç”¨å¯¼è‡´æ¨ç†èƒ½åŠ›ä¸‹é™çš„ç°è±¡', 'desc': 'å·¥å…·å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆTaLMsï¼‰å¯ä»¥è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥è§£å†³è¶…å‡ºå…¶å‚æ•°èƒ½åŠ›çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥å…·çš„ä½¿ç”¨æ˜¯å¦åæ˜ å‡ºå¯ä¿¡çš„æ¨ç†ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿å·¥å…·è¢«æ­£ç¡®é€‰æ‹©å’Œæ‰§è¡Œï¼ŒTaLMsä¹Ÿä¼šå°†å·¥å…·è¾“å‡ºè§†ä¸ºæ¨ç†çš„æ›¿ä»£å“ï¼Œå¯¼è‡´è§£å†³æ–¹æ¡ˆçœ‹ä¼¼æ­£ç¡®ä½†ç¼ºä¹è¿è´¯çš„ç†ç”±ã€‚æˆ‘ä»¬ç§°è¿™ç§å¤±è´¥æ¨¡å¼ä¸ºå·¥å…·è¯±å¯¼è¿‘è§†ï¼ˆTIMï¼‰ï¼Œå¹¶é€šè¿‡PYMATHåŸºå‡†è¿›è¡Œç ”ç©¶ï¼Œå‘ç°TaLMsåœ¨æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œä½†æ¨ç†è¡Œä¸ºå´æŒç»­æ¶åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.03108', 'title': 'miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward', 'url': 'https://huggingface.co/papers/2511.03108', 'abstract': 'We perform a thorough analysis of the formal and informal statements in the miniF2F benchmark from the perspective of an AI system that is tasked to participate in a math Olympiad consisting of the problems in miniF2F. In such setting, the model has to read and comprehend the problems in natural language, formalize them in Lean language, then proceed with proving the problems, and it will get credit for each problem if the formal proof corresponds to the original informal statement presented to the model. Our evaluation results reveal that the best accuracy of such pipeline can be about 36% using the SoTA models in the literature, considerably lower than the individual SoTA accuracies, 97% and 69% reported in the autoformalization and theorem proving literature. Analyzing the failure modes, we trace back a considerable portion of this drop to discrepancies between the formal and informal statements for more than half of the problems in miniF2F. We proceed with correcting all the errors, discrepancies and simplifications in formal and informal statements, and present the miniF2F-v2 with fully verified formal and informal statements and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to the best accuracy of 70%, a significant improvement from the 40% on the original miniF2F, yet indicating considerable misalignment between the autoformalization models and theorem provers. Our deep analysis suggests that a higher quality benchmark can help the community better evaluate progress in the field of formal reasoning and also better diagnose the failure and success modes of autoformalization and theorem proving models. Our dataset is available at https://github.com/roozbeh-yz/miniF2F_v2.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'd18f18b23f3dafca', 'authors': ['Azim Ospanov', 'Farzan Farnia', 'Roozbeh Yousefzadeh'], 'affiliations': ['Chinese University of Hong Kong', 'Independent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.03108.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#math', '#dataset', '#reasoning'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº miniF2F Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‘ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Lean Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 36%, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (97%) Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ (69%), ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ miniF2F-v2, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ²Ğ¾Ğ·Ñ€Ğ¾ÑĞ»Ğ° Ğ´Ğ¾ 70%, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Improving AI Math Problem Solving with Enhanced Benchmarks', 'desc': 'This paper analyzes the miniF2F benchmark for AI systems participating in math Olympiads, focusing on the challenges of understanding and formalizing problems. The authors found that the best accuracy achieved by state-of-the-art models was only 36%, which is significantly lower than the individual accuracies reported in related fields. They identified that many discrepancies between formal and informal problem statements contributed to this performance drop. To address these issues, they created miniF2F-v2 with corrected statements and achieved a 70% accuracy, highlighting the need for higher quality benchmarks in formal reasoning.'}, 'zh': {'title': 'æå‡å½¢å¼åŒ–æ¨ç†çš„åŸºå‡†è´¨é‡', 'desc': 'æœ¬æ–‡åˆ†æäº†miniF2FåŸºå‡†æµ‹è¯•ä¸­æ­£å¼å’Œéæ­£å¼é™ˆè¿°çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸€ä¸ªå‚ä¸æ•°å­¦å¥¥æ—åŒ¹å…‹çš„AIç³»ç»Ÿã€‚è¯¥æ¨¡å‹éœ€è¦ç†è§£è‡ªç„¶è¯­è¨€ä¸­çš„é—®é¢˜ï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºLeanè¯­è¨€ï¼Œç„¶åè¿›è¡Œè¯æ˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ•´ä½“å‡†ç¡®ç‡çº¦ä¸º36%ï¼Œè¿œä½äºå„ä¸ªé¢†åŸŸçš„æœ€ä½³å‡†ç¡®ç‡ã€‚é€šè¿‡çº æ­£æ‰€æœ‰é”™è¯¯å’Œä¸ä¸€è‡´ï¼Œæå‡ºäº†miniF2F-v2ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå®ç°äº†70%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜å½¢å¼åŒ–æ¨¡å‹ä¸å®šç†è¯æ˜å™¨ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—çš„ä¸åŒ¹é…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07448', 'title': 'Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey', 'url': 'https://huggingface.co/papers/2511.07448', 'abstract': "This survey examines methods for using large language models to generate scientific ideas, categorizing them into five families and aligning them with creativity frameworks to improve scientific soundness and novelty.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '6a0fc5a337f8643b', 'authors': ['Fatemeh Shahhosseini', 'Arash Marioriyad', 'Ali Momen', 'Mahdieh Soleymani Baghshah', 'Mohammad Hossein Rohban', 'Shaghayegh Haghjooy Javanmard'], 'affiliations': ['Isfahan University of Medical Sciences', 'Sharif University of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07448.jpg', 'data': {'categories': ['#survey', '#training', '#science', '#multimodal', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLM ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞĞ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ¿ÑÑ‚ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ‘Ğ¾Ğ´ĞµĞ½Ğ° Ğ¸ Ğ Ğ¾Ğ´ÑĞ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ’Ñ‹Ğ´ĞµĞ»ĞµĞ½Ñ‹ Ğ¿ÑÑ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²: Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ĞºĞ°Ğº LLM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹.'}, 'en': {'title': 'Harnessing LLMs for Creative Scientific Discovery', 'desc': 'This survey explores how large language models (LLMs) can be used to generate scientific ideas, categorizing the methods into five distinct families. It highlights the importance of balancing creativity with scientific soundness in the idea generation process, which is crucial for scientific discovery. The paper employs creativity frameworks to analyze how different methods contribute to generating novel and empirically sound scientific ideas. By providing a structured overview, it aims to guide future research and applications of LLMs in the scientific domain.'}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨åŠ¨ç§‘å­¦åˆ›æ„çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è¿™ç¯‡è°ƒæŸ¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆç§‘å­¦åˆ›æ„çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶åˆ†ä¸ºäº”ä¸ªç±»åˆ«ï¼Œä»¥æé«˜ç§‘å­¦çš„åˆç†æ€§å’Œæ–°é¢–æ€§ã€‚ç§‘å­¦åˆ›æ„ç”Ÿæˆæ˜¯ç§‘å­¦å‘ç°çš„æ ¸å¿ƒï¼Œæ¶‰åŠè§£å†³æœªè§£é—®é¢˜å’Œæå‡ºæ–°å‡è®¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç§‘å­¦åˆ›æ„æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åˆ›é€ èƒ½åŠ›ä»ç„¶ä¸ç¨³å®šä¸”ç†è§£ä¸è¶³ã€‚é€šè¿‡å°†æ–¹æ³•ä¸åˆ›é€ åŠ›æ¡†æ¶å¯¹é½ï¼Œè¿™é¡¹ç ”ç©¶æ˜ç¡®äº†è¯¥é¢†åŸŸçš„ç°çŠ¶ï¼Œå¹¶æŒ‡å‡ºäº†LLMsåœ¨ç§‘å­¦å‘ç°ä¸­å¯é ã€ç³»ç»Ÿå’Œå˜é©æ€§åº”ç”¨çš„å…³é”®æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11287', 'title': 'Building the Web for Agents: A Declarative Framework for Agent-Web Interaction', 'url': 'https://huggingface.co/papers/2511.11287', 'abstract': "VOIX is a web-native framework that enables reliable and privacy-preserving interactions between AI agents and human-oriented user interfaces using declarative HTML elements.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '75e540f379edd26d', 'authors': ['Sven Schultze', 'Meike Verena Kietzmann', 'Nils-Lucas SchÃ¶nfeld', 'Ruth Stock-Homburg'], 'affiliations': ['Technical University of Darmstadt'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11287.jpg', 'data': {'categories': ['#agents'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'VOIX â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµĞºĞ»Ğ°Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ HTML ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ <tool> Ğ¸ <context> Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ñ‹ Ğ¾Ñ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ VOIX.'}, 'en': {'title': 'Empowering AI Agents with Reliable Web Interactions', 'desc': 'VOIX is a framework designed to improve interactions between AI agents and user interfaces on the web. It allows developers to use simple HTML elements to define actions and states for AI agents, making these interactions more reliable and secure. By using specific tags like <tool> and <context>, VOIX creates a clear contract for how agents should behave, which helps prevent misunderstandings. The framework was tested with developers who quickly learned to create functional applications, showing its potential for enhancing human-AI collaboration online.'}, 'zh': {'title': 'å®ç°å®‰å…¨é«˜æ•ˆçš„äººæœºåä½œ', 'desc': 'VOIXæ˜¯ä¸€ä¸ªç½‘ç»œåŸç”Ÿæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°AIä»£ç†ä¸äººç±»ç”¨æˆ·ç•Œé¢ä¹‹é—´çš„å¯é å’Œéšç§ä¿æŠ¤çš„äº¤äº’ã€‚å®ƒé€šè¿‡ç®€å•çš„å£°æ˜æ€§HTMLå…ƒç´ ï¼Œä½¿ç½‘ç«™èƒ½å¤Ÿå‘AIä»£ç†æä¾›å¯é ã€å¯å®¡è®¡å’Œä¿æŠ¤éšç§çš„åŠŸèƒ½ã€‚VOIXå¼•å…¥äº†<tool>å’Œ<context>æ ‡ç­¾ï¼Œå…è®¸å¼€å‘è€…æ˜ç¡®å®šä¹‰å¯ç”¨çš„æ“ä½œå’Œç›¸å…³çŠ¶æ€ï¼Œä»è€Œåˆ›å»ºæ¸…æ™°çš„æœºå™¨å¯è¯»çš„ä»£ç†è¡Œä¸ºåˆåŒã€‚é€šè¿‡åœ¨é»‘å®¢é©¬æ‹‰æ¾ä¸­è¯„ä¼°è¯¥æ¡†æ¶çš„å®ç”¨æ€§å’Œå¯å­¦ä¹ æ€§ï¼Œç»“æœè¡¨æ˜å‚ä¸è€…èƒ½å¤Ÿå¿«é€Ÿæ„å»ºå¤šæ ·åŒ–å’ŒåŠŸèƒ½é½å…¨çš„ä»£ç†å¯ç”¨çš„Webåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10258', 'title': 'Workload Schedulers -- Genesis, Algorithms and Differences', 'url': 'https://huggingface.co/papers/2511.10258', 'abstract': 'This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'cd12068232a3548a', 'authors': ['Leszek Sliwko', 'Vladimir Getov'], 'affiliations': ['Faculty of Science and Technology, University of Westminster'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10258.jpg', 'data': {'categories': [], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞĞ¡ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¾Ñ‚ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸ Ğ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²ÑĞµĞ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ. Ğ’ Ğ·Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ñ… ĞºĞ°Ğº Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼.'}, 'en': {'title': 'Classifying Workload Schedulers: Evolution and Common Goals', 'desc': 'This paper introduces a new method for classifying different types of workload schedulers used in computing. It categorizes schedulers into three main classes: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers, and Big Data Schedulers. The authors trace the evolution of these schedulers from their early versions to contemporary designs, emphasizing the algorithms used and their functionalities. The paper concludes by highlighting the common goals of scheduling strategies across both local and distributed systems, despite their differences.'}, 'zh': {'title': 'ç°ä»£è°ƒåº¦å™¨åˆ†ç±»çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥å¯¹ç°ä»£å·¥ä½œè´Ÿè½½è°ƒåº¦å™¨è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬æè¿°äº†ä¸‰ç±»è°ƒåº¦å™¨ï¼šæ“ä½œç³»ç»Ÿè¿›ç¨‹è°ƒåº¦å™¨ã€é›†ç¾¤ç³»ç»Ÿä½œä¸šè°ƒåº¦å™¨å’Œå¤§æ•°æ®è°ƒåº¦å™¨ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™äº›è°ƒåº¦å™¨ä»æ—©æœŸé‡‡ç”¨åˆ°ç°ä»£å®ç°çš„æ¼”å˜ï¼Œè€ƒè™‘äº†ç®—æ³•çš„ä½¿ç”¨å’Œç‰¹æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†è°ƒåº¦ç­–ç•¥è®¾è®¡çš„ç›¸ä¼¼æ€§ï¼Œè¿™äº›ç­–ç•¥é€‚ç”¨äºæœ¬åœ°å’Œåˆ†å¸ƒå¼ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11168', 'title': 'CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios', 'url': 'https://huggingface.co/papers/2511.11168', 'abstract': 'CATS-V2V is a new real-world dataset for V2V cooperative perception in complex adverse traffic scenarios, providing comprehensive sensor data and precise temporal alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '2ccbc7902752b3a0', 'authors': ['Hangyu Li', 'Bofeng Cao', 'Zhaohui Liang', 'Wuzhen Li', 'Juyoung Oh', 'Yuxuan Chen', 'Shixiao Liang', 'Hang Zhou', 'Chengyuan Ma', 'Jiaxi Liu', 'Zheng Li', 'Peng Zhang', 'KeKe Long', 'Maolin Liu', 'Jackson Jiang', 'Chunlei Yu', 'Shengxiang Liu', 'Hongkai Yu', 'Xiaopeng Li'], 'affiliations': ['Cleveland State University', 'University of Wisconsin-Madison', 'wuwen-ai'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11168.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#3d', '#dataset', '#cv'], 'emoji': 'ğŸš—', 'ru': {'title': 'ĞšĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑĞ¼Ğ¸ Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…', 'desc': 'CATS-V2V Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ñ€Ğ¾Ğ´Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑĞ¼Ğ¸ (V2V) Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ´Ğ²ÑƒÑ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº LiDAR, Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ĞºĞ°Ğ¼ĞµÑ€, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ GNSS Ğ¸ IMU Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸ ÑĞ²ĞµÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Autonomous Driving with CATS-V2V Dataset', 'desc': 'CATS-V2V is a groundbreaking dataset designed for Vehicle-to-Vehicle (V2V) cooperative perception in challenging traffic situations. It includes extensive sensor data collected from two synchronized vehicles, capturing various weather and lighting conditions across multiple locations. The dataset features 60,000 frames of LiDAR point clouds and over 1.26 million camera images, along with precise GNSS and IMU data, all annotated for accurate object detection. This resource aims to enhance the performance of autonomous driving systems by providing high-quality data for training and testing in complex environments.'}, 'zh': {'title': 'CATS-V2Vï¼šæå‡è‡ªåŠ¨é©¾é©¶çš„ååŒæ„ŸçŸ¥æ–°æ•°æ®é›†', 'desc': 'CATS-V2Væ˜¯ä¸€ä¸ªæ–°çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œä¸“æ³¨äºå¤æ‚ä¸åˆ©äº¤é€šåœºæ™¯ä¸‹çš„è½¦å¯¹è½¦ï¼ˆV2Vï¼‰ååŒæ„ŸçŸ¥ã€‚è¯¥æ•°æ®é›†æä¾›äº†å…¨é¢çš„ä¼ æ„Ÿå™¨æ•°æ®å’Œç²¾ç¡®çš„æ—¶é—´å¯¹é½ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ•°æ®é›†ä¸­å¯¹æ™®é€šäº¤é€šåœºæ™¯çš„å±€é™æ€§ã€‚æ•°æ®é›†åŒ…å«æ¥è‡ªä¸¤è¾†ç¡¬ä»¶æ—¶é—´åŒæ­¥è½¦è¾†çš„60Kå¸§æ¿€å…‰é›·è¾¾ç‚¹äº‘å’Œ126ä¸‡å¸§å¤šè§†è§’ç›¸æœºå›¾åƒï¼Œæ¶µç›–10ç§å¤©æ°”å’Œå…‰ç…§æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡çš„æ—¶é—´å¯¹é½æ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ä¼ æ„Ÿå™¨æ¨¡æ€ä¸‹çš„å¯¹è±¡ç²¾ç¡®å¯¹é½ï¼Œä»¥æ”¯æŒè‡ªåŠ¨é©¾é©¶ç¤¾åŒºçš„ç›¸å…³ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11721', 'title': 'A Meta-Heuristic Load Balancer for Cloud Computing Systems', 'url': 'https://huggingface.co/papers/2511.11721', 'abstract': 'This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '1ae8947278d6d2f2', 'authors': ['Leszek Sliwko', 'Vladimir Getov'], 'affiliations': ['Faculty of Science and Technology, University of Westminster, London', 'University of Westminster, London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11721.jpg', 'data': {'categories': [], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ·ĞºĞ¸ ÑƒĞ·Ğ»Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¼Ğ¸Ğ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ². ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ° Ğ¼ĞµÑ‚Ğ°-ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ ĞµĞ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ°-ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ².'}, 'en': {'title': 'Optimizing Cloud Service Allocation with Genetic Algorithms', 'desc': 'This paper introduces a method for efficiently distributing services across a Cloud system to prevent node overload while ensuring stability and cost-effectiveness. It outlines a model that accounts for various resource types and the costs associated with migrating services. The authors demonstrate a prototype load balancer based on meta-heuristic techniques and provide experimental results to validate their approach. Additionally, they propose a unique genetic algorithm that incorporates solutions from other meta-heuristic algorithms to enhance performance.'}, 'zh': {'title': 'ä¼˜åŒ–äº‘æœåŠ¡åˆ†é…ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šä¸æˆæœ¬æœ€ä½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨äº‘ç³»ç»Ÿä¸­åˆ†é…æœåŠ¡çš„ç­–ç•¥ï¼Œæ—¨åœ¨é¿å…èŠ‚ç‚¹è¿‡è½½å¹¶ä»¥æœ€ä½æˆæœ¬ç»´æŒç³»ç»Ÿç¨³å®šæ€§ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªäº‘èµ„æºåˆ©ç”¨çš„æŠ½è±¡æ¨¡å‹ï¼Œæ¶µç›–å¤šç§èµ„æºç±»å‹ä»¥åŠæœåŠ¡è¿ç§»æˆæœ¬çš„è€ƒè™‘ã€‚æ–‡ä¸­å±•ç¤ºäº†ä¸€ä¸ªåŸå‹å…ƒå¯å‘å¼è´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶å¯¹å®éªŒç»“æœè¿›è¡Œäº†è®¨è®ºã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„é—ä¼ ç®—æ³•ï¼Œå…¶ç§ç¾¤ç”±å…¶ä»–å…ƒå¯å‘å¼ç®—æ³•çš„è¾“å‡ºç§å­ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11793', 'title': 'MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling', 'url': 'https://huggingface.co/papers/2511.11793', 'abstract': 'We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.', 'score': 160, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '19d8fd13a6373a10', 'authors': ['MiroMind Team', 'Song Bai', 'Lidong Bing', 'Carson Chen', 'Guanzheng Chen', 'Yuntao Chen', 'Zhe Chen', 'Ziyi Chen', 'Jifeng Dai', 'Xuan Dong', 'Wenhan Dou', 'Yue Deng', 'Yunjie Fu', 'Junqi Ge', 'Chenxia Han', 'Tammy Huang', 'Zhenhang Huang', 'Jerry Jiao', 'Shilei Jiang', 'Tianyu Jiao', 'Xiaoqi Jian', 'Lei Lei', 'Ruilin Li', 'Ryan Luo', 'Tiantong Li', 'Xiang Lin', 'Ziyuan Liu', 'Zhiqi Li', 'Jie Ni', 'Qiang Ren', 'Pax Sun', 'Shiqian Su', 'Chenxin Tao', 'Bin Wang', 'Hellen Wang', 'Haonan Wang', 'James Wang', 'Jin Wang', 'Jojo Wang', 'Letian Wang', 'Shizun Wang', 'Weizhi Wang', 'Zixuan Wang', 'Jinfan Xu', 'Sen Xing', 'Chenyu Yang', 'Hai Ye', 'Jiaheng Yu', 'Yue Yu', 'Muyan Zhong', 'Tianchen Zhao', 'Xizhou Zhu', 'Yanpeng Zhou', 'Yifan Zhang', 'Zhi Zhu'], 'affiliations': ['MiroMind Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11793.jpg', 'data': {'categories': ['#optimization', '#training', '#long_context', '#benchmark', '#open_source', '#rl', '#agents', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ñ€ĞµÑ‚ÑŒĞµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'MiroThinker v1.0 â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ test-time Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾ 600 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ 256K, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (GAIA, HLE, BrowseComp Ğ¸ BrowseComp-ZH) 72B Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 81.9%, 37.7%, 47.1% Ğ¸ 55.6% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-5-high.'}, 'en': {'title': 'MiroThinker: Revolutionizing Research with Interactive Scaling', 'desc': "MiroThinker v1.0 is an innovative open-source research agent that enhances reasoning and information-seeking through interactive scaling. Unlike traditional models that focus solely on increasing size or context, MiroThinker emphasizes the importance of frequent interactions with its environment to improve performance. By utilizing reinforcement learning, it can make up to 600 tool calls per task, allowing for complex reasoning and effective research workflows. The model's performance consistently improves with deeper interactions, establishing interaction scaling as a vital aspect of developing advanced research agents."}, 'zh': {'title': 'äº¤äº’æ‰©å±•ï¼šæ„å»ºä¸‹ä¸€ä»£ç ”ç©¶ä»£ç†çš„å…³é”®', 'desc': 'MiroThinker v1.0 æ˜¯ä¸€ä¸ªå¼€æºç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨æå‡å·¥å…·å¢å¼ºæ¨ç†å’Œä¿¡æ¯è·å–èƒ½åŠ›ã€‚ä¸ä»¥å¾€ä»…é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–ä¸Šä¸‹æ–‡é•¿åº¦çš„ä»£ç†ä¸åŒï¼ŒMiroThinker é€šè¿‡åœ¨æ¨¡å‹å±‚é¢æ¢ç´¢äº¤äº’æ‰©å±•ï¼Œç³»ç»Ÿæ€§åœ°è®­ç»ƒæ¨¡å‹å¤„ç†æ›´æ·±å’Œæ›´é¢‘ç¹çš„ä»£ç†-ç¯å¢ƒäº¤äº’ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜æ•ˆçš„äº¤äº’æ‰©å±•ï¼Œèƒ½å¤Ÿåœ¨256Kçš„ä¸Šä¸‹æ–‡çª—å£å†…æ¯ä¸ªä»»åŠ¡æ‰§è¡Œå¤šè¾¾600æ¬¡å·¥å…·è°ƒç”¨ï¼Œæ”¯æŒæŒç»­çš„å¤šè½®æ¨ç†å’Œå¤æ‚çš„ç°å®ä¸–ç•Œç ”ç©¶å·¥ä½œæµã€‚ç ”ç©¶è¡¨æ˜ï¼Œäº¤äº’æ·±åº¦çš„æ‰©å±•ä¸æ¨¡å‹è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„æ‰©å±•å…·æœ‰ç›¸ä¼¼çš„è¡Œä¸ºï¼Œç¡®ç«‹äº†äº¤äº’æ‰©å±•ä½œä¸ºæ„å»ºä¸‹ä¸€ä»£å¼€æ”¾ç ”ç©¶ä»£ç†çš„ç¬¬ä¸‰ä¸ªå…³é”®ç»´åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13254', 'title': 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance', 'url': 'https://huggingface.co/papers/2511.13254', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.', 'score': 134, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '7ac9e6a61836d1a5', 'authors': ['Shalini Maiti', 'Amar Budhiraja', 'Bhavul Gauri', 'Gaurav Chaurasia', 'Anton Protopopov', 'Alexis Audran-Reiss', 'Michael Slater', 'Despoina Magka', 'Tatiana Shavrina', 'Roberta Raileanu', 'Yoram Bachrach'], 'affiliations': ['FAIR at Meta', 'Meta SuperIntelligence Labs', 'University College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13254.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#open_source', '#math', '#multilingual'], 'emoji': 'ğŸ²', 'ru': {'title': 'Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°ÑÑ‚Ñ€ÑĞ»Ğµ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Soup Of Category Experts (SoCE) Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ğ· Ğ½Ğ¸Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ SoCE Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Berkeley Function Calling.'}, 'en': {'title': 'Enhancing LLMs with Expert Model Soups', 'desc': "This paper presents a new method called Soup Of Category Experts (SoCE) for improving the performance of Large Language Models (LLMs) through a technique known as model souping. SoCE identifies the best models for different categories based on their performance and combines them using a weighted averaging approach, rather than treating all models equally. This method takes advantage of the fact that models often perform differently across various tasks, allowing for a more tailored combination of their strengths. The results show that SoCE enhances the models' capabilities in areas like multilingual understanding and tool usage, achieving top results in benchmark tests."}, 'zh': {'title': 'æ¨¡å‹æ··åˆï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚æ¨¡å‹æ··åˆï¼ˆmodel soupingï¼‰æ˜¯ä¸€ç§é€šè¿‡å¹³å‡å¤šä¸ªç›¸åŒæ¶æ„æ¨¡å‹çš„æƒé‡æ¥æå‡æ€§èƒ½çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œæ˜‚è´µé‡è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜æ¨¡å‹æ•ˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºç±»åˆ«ä¸“å®¶æ±¤ï¼ˆSoCEï¼‰çš„æ¨¡å‹æ··åˆæ–¹æ³•ï¼Œé€šè¿‡åŸºå‡†ç»„åˆæ¥è¯†åˆ«æœ€ä½³æ¨¡å‹å€™é€‰ï¼Œå¹¶åº”ç”¨éå‡åŒ€åŠ æƒå¹³å‡æ¥æœ€å¤§åŒ–æ€§èƒ½ã€‚ä¸ä»¥å¾€çš„å‡åŒ€åŠ æƒæ–¹æ³•ä¸åŒï¼ŒSoCEåˆ©ç”¨äº†åŸºå‡†ç±»åˆ«ä¹‹é—´æ¨¡å‹æ€§èƒ½çš„ä½ç›¸å…³æ€§ï¼Œè¯†åˆ«å‡ºæ¯ä¸ªå¼±ç›¸å…³ç±»åˆ«é›†ç¾¤çš„â€œä¸“å®¶â€æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–çš„åŠ æƒå¹³å‡è¿›è¡Œç»„åˆï¼Œä»è€Œåœ¨å¤šé¢†åŸŸä¸­æå‡äº†æ€§èƒ½å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13612', 'title': 'P1: Mastering Physics Olympiads with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2511.13612', 'abstract': 'Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.', 'score': 132, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '5cf7d0ed1cc080b0', 'authors': ['Jiacheng Chen', 'Qianjia Cheng', 'Fangchen Yu', 'Haiyuan Wan', 'Yuchen Zhang', 'Shenghe Zheng', 'Junchi Yao', 'Qingyang Zhang', 'Haonan He', 'Yun Luo', 'Yufeng Zhao', 'Futing Wang', 'Li Sheng', 'Chengxing Xie', 'Yuxin Zuo', 'Yizhuo Li', 'Wenxauan Zeng', 'Yulun Wu', 'Rui Huang', 'Dongzhan Zhou', 'Kai Chen', 'Yu Qiao', 'Lei Bai', 'Yu Cheng', 'Ning Ding', 'Bowen Zhou', 'Peng Ye', 'Ganqu Cui'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13612.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#math', '#rl', '#agents', '#plp', '#reasoning'], 'emoji': 'ğŸ†', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ P1, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ P1-235B-A22B ÑÑ‚Ğ°Ğ»Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ²ÑˆĞµĞ¹ Ğ·Ğ¾Ğ»Ğ¾Ñ‚ÑƒÑ Ğ¼ĞµĞ´Ğ°Ğ»ÑŒ Ğ½Ğ° ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ 2025 Ğ³Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ğ°Ğ²ÑˆĞµĞ¹ 12 Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ñ‹Ñ… Ğ¼ĞµĞ´Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ· 13 Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° PhysicsMinions, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ½Ğ° IPhO 2025 Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞ¸Ğ¹ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ±Ğ°Ğ»Ğ» Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ 13 ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ P1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'P1: Revolutionizing Physics Reasoning with LLMs', 'desc': 'This paper presents a significant advancement in large language models (LLMs) specifically designed for physics reasoning. The authors introduce P1, a series of open-source models that utilize reinforcement learning to excel in solving complex physics problems, achieving remarkable success in international competitions. Notably, the P1-235B-A22B model earned gold medals at the International Physics Olympiad, demonstrating its superior reasoning capabilities. Additionally, the P1 models show strong performance in other reasoning tasks, indicating their versatility beyond physics.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹æ¨åŠ¨ç‰©ç†å­¦ç ”ç©¶çš„é©å‘½', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ä½¿å¾—ä»è§£è°œåˆ°ç§‘å­¦çº§æ¨ç†çš„è¾¹ç•Œå¾—ä»¥æ‹“å±•ã€‚è¿™ç§æ¨ç†èƒ½åŠ›åœ¨ç‰©ç†å­¦ä¸­å¾—åˆ°äº†æœ€ä¸¥æ ¼çš„æµ‹è¯•ï¼Œç‰©ç†å­¦å°†ç¬¦å·ä¸ç°å®ç´§å¯†ç»“åˆï¼Œæ˜¯ç°ä»£æŠ€æœ¯çš„åŸºçŸ³ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—å…·æœ‰å“è¶Šç‰©ç†æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹P1ï¼Œç‰¹åˆ«æ“…é•¿è§£å†³å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜ã€‚P1-235B-A22Bæ˜¯ç¬¬ä¸€ä¸ªåœ¨å›½é™…ç‰©ç†å¥¥æ—åŒ¹å…‹ï¼ˆIPhO 2025ï¼‰ä¸­è·å¾—é‡‘ç‰Œè¡¨ç°çš„å¼€æºæ¨¡å‹ï¼Œå±•ç¤ºäº†P1ç³»åˆ—åœ¨ç‰©ç†å’Œå…¶ä»–æ¨ç†ä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12609', 'title': 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data', 'url': 'https://huggingface.co/papers/2511.12609', 'abstract': "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.", 'score': 102, 'issue_id': 1, 'pub_date': '2025-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': 'b14fef3ecc428f72', 'authors': ['Yunxin Li', 'Xinyu Chen', 'Shenyuan Jiang', 'Haoyuan Shi', 'Zhenyu Liu', 'Xuanyu Zhang', 'Nanhao Deng', 'Zhenran Xu', 'Yicheng Ma', 'Meishan Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12609.jpg', 'data': {'categories': ['#alignment', '#optimization', '#reasoning', '#training', '#audio', '#multimodal', '#benchmark', '#open_source', '#video', '#3d', '#rlhf', '#architecture'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ÑĞµĞ¿Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Uni-MoE 2.0 â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Qwen2.5-7B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµÑ‡Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° RoPE Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 75 Ğ¼Ğ»Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ğ»Ğ¾ÑÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ GSPO-DPO Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 85 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Qwen2.5-Omni Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50 Ğ¸Ğ· 76 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Multimodal AI with Uni-MoE 2.0', 'desc': 'Uni-MoE 2.0 is an advanced omnimodal large model that enhances multimodal understanding and generation capabilities. It utilizes a dynamic-capacity Mixture-of-Experts (MoE) design to efficiently process various types of data, including text, images, and speech. The model is trained using a progressive strategy that incorporates reinforcement learning to improve reasoning and performance. With extensive evaluation, Uni-MoE 2.0 demonstrates state-of-the-art results across multiple benchmarks, showcasing its strengths in video understanding, audiovisual reasoning, and generative tasks.'}, 'zh': {'title': 'å…¨æ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„æ–°æ—¶ä»£', 'desc': 'Uni-MoE 2.0æ˜¯ä¸€ä¸ªå…¨æ–°çš„å¼€æ”¾æºä»£ç çš„å…¨æ¨¡æ€å¤§æ¨¡å‹ï¼Œä¸“æ³¨äºè¯­è¨€å’Œå¤šæ¨¡æ€çš„ç†è§£ã€æ¨ç†å’Œç”Ÿæˆã€‚å®ƒé‡‡ç”¨åŠ¨æ€å®¹é‡çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è®¾è®¡ï¼Œå¹¶ç»“åˆæ¸è¿›å¼è®­ç»ƒç­–ç•¥å’Œå¤šæ¨¡æ€æ•°æ®åŒ¹é…æŠ€æœ¯ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ï¼Œå…·æœ‰é«˜æ•ˆçš„è®¡ç®—èƒ½åŠ›å’Œè·¨æ¨¡æ€è¾“å…¥çš„å¤„ç†èƒ½åŠ›ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒUni-MoE 2.0åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šé¢†å…ˆçš„å…¨æ¨¡æ€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13647', 'title': 'Part-X-MLLM: Part-aware 3D Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2511.13647', 'abstract': 'We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/', 'score': 70, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '42372f7c9ddf43d9', 'authors': ['Chunshi Wang', 'Junliang Ye', 'Yunhan Yang', 'Yang Li', 'Zizhuo Lin', 'Jun Zhu', 'Zhuo Chen', 'Yawei Luo', 'Chunchao Guo'], 'affiliations': ['Tencent Hunyuan', 'The University of Hong Kong', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13647.jpg', 'data': {'categories': ['#training', '#multimodal', '#3d', '#dataset', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Part-X-MLLM â€” Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº RGB Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ‰Ğ¸ĞºĞ°Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Unifying 3D Tasks with Language-Driven Structure', 'desc': 'Part-X-MLLM is a novel 3D multimodal large language model designed to handle various 3D tasks by treating them as structured programs. It takes an RGB point cloud and a natural language prompt to generate a coherent sequence of tokens that include bounding boxes, semantic descriptions, and editing commands. This structured output acts as a flexible interface for controlling geometry-aware modules, facilitating part-based generation and editing. By separating symbolic planning from geometric synthesis, the model allows for seamless integration with different geometry engines, achieving state-of-the-art results in tasks like grounded Q&A and compositional generation.'}, 'zh': {'title': 'ç»Ÿä¸€3Dä»»åŠ¡çš„è¯­è¨€æ¨¡å‹', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Part-X-MLLMï¼Œè¿™æ˜¯ä¸€ç§åŸç”Ÿçš„3Då¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†å¤šç§3Dä»»åŠ¡ç»Ÿä¸€ä¸ºç»“æ„åŒ–çš„å¯æ‰§è¡Œè¯­æ³•ç¨‹åºã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®RGBç‚¹äº‘å’Œè‡ªç„¶è¯­è¨€æç¤ºï¼Œè‡ªå›å½’åœ°ç”Ÿæˆä¸€ä¸ªè¿è´¯çš„ä»¤ç‰Œåºåˆ—ï¼Œç¼–ç éƒ¨ä»¶çº§çš„è¾¹ç•Œæ¡†ã€è¯­ä¹‰æè¿°å’Œç¼–è¾‘å‘½ä»¤ã€‚é€šè¿‡å°†ç¬¦å·è§„åˆ’ä¸å‡ ä½•åˆæˆè§£è€¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸é€šè¿‡å•ä¸€çš„è¯­è¨€åŸç”Ÿå‰ç«¯æ§åˆ¶ä»»ä½•å…¼å®¹çš„å‡ ä½•å¼•æ“ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„åŒ–è®¡åˆ’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨åŸºäºé—®ç­”ã€ç»„åˆç”Ÿæˆå’Œå±€éƒ¨ç¼–è¾‘ç­‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09611', 'title': 'MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation', 'url': 'https://huggingface.co/papers/2511.09611', 'abstract': 'A parallel multimodal diffusion framework, MMaDA-Parallel, enhances cross-modal alignment and semantic consistency in thinking-aware image synthesis by addressing error propagation issues in sequential approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel', 'score': 68, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'e876714dde8c5507', 'authors': ['Ye Tian', 'Ling Yang', 'Jiongfan Yang', 'Anran Wang', 'Yu Tian', 'Jiani Zheng', 'Haochen Wang', 'Zhiyang Teng', 'Zhuochen Wang', 'Yinjie Wang', 'Yunhai Tong', 'Mengdi Wang', 'Xiangtai Li'], 'affiliations': ['ByteDance', 'CASIA', 'Peking University', 'Princeton University', 'The University of Chicago'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09611.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#open_source', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMaDA-Parallel, Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ…: Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ParaBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ reinforcement learning Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 6,9% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Image Synthesis with Parallel Multimodal Diffusion', 'desc': 'The paper introduces MMaDA-Parallel, a new framework that improves the generation of images from text by addressing issues of error propagation found in traditional sequential methods. It highlights the importance of cross-modal alignment, where the reasoning behind the text must closely match the generated image. To evaluate this, the authors present ParaBench, a benchmark for assessing both text and image outputs. By using a parallel approach and reinforcement learning, MMaDA-Parallel enhances the interaction between text and images, leading to better semantic consistency and a notable performance improvement over existing models.'}, 'zh': {'title': 'å¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶æå‡å›¾åƒåˆæˆè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMMaDA-Parallelçš„å¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ€ç»´æ„ŸçŸ¥å›¾åƒåˆæˆä¸­çš„è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„é¡ºåºè‡ªå›å½’æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å¯èƒ½å› é”™è¯¯ä¼ æ’­è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ParaBenchåŸºå‡†ï¼Œè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºçš„å¯¹é½æƒ…å†µã€‚é€šè¿‡MMaDA-Parallelæ¡†æ¶ï¼Œæˆ‘ä»¬å®ç°äº†æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„åŒå‘äº¤äº’ï¼Œä»è€Œæ˜¾è‘—æå‡äº†è·¨æ¨¡æ€çš„ä¸€è‡´æ€§å’Œå¯¹é½æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13720', 'title': 'Back to Basics: Let Denoising Generative Models Denoise', 'url': 'https://huggingface.co/papers/2511.13720', 'abstract': 'Today\'s denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "Just image Transformers", or JiT, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.', 'score': 65, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'b1db56865d624b86', 'authors': ['Tianhong Li', 'Kaiming He'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13720.jpg', 'data': {'categories': ['#architecture', '#cv', '#diffusion', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑƒĞ¼Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚. ĞĞ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½ÑƒÑ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ, Ğ¾Ğ½Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»ĞµĞ¶Ğ°Ñ‚ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ ÑÑ‚Ğ¸Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ½Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑÑ… Ğ±ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ JiT (Just image Transformers) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… 256 Ğ¸ 512 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾.'}, 'en': {'title': 'Transforming Image Generation: Directly Predicting Clean Data with JiT', 'desc': "This paper critiques current denoising diffusion models for not directly predicting clean images, but rather focusing on noise prediction. It introduces the idea that predicting clean data is fundamentally different from predicting noised quantities, based on the manifold assumption that natural data exists on a low-dimensional manifold. The authors propose a new approach using large-patch Transformers that directly predict clean images without the need for tokenization or pre-training. Their method, termed 'Just image Transformers' (JiT), demonstrates strong performance on ImageNet, showing that simpler models can effectively handle high-dimensional data by adhering to the manifold concept."}, 'zh': {'title': 'ç›´æ¥é¢„æµ‹å¹²å‡€æ•°æ®çš„Transformeræ¨¡å‹', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å»å™ªæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬åŸç†ï¼ŒæŒ‡å‡ºå½“å‰æ¨¡å‹å¹¶ä¸ç›´æ¥é¢„æµ‹å¹²å‡€å›¾åƒï¼Œè€Œæ˜¯é¢„æµ‹å™ªå£°æˆ–å¸¦å™ªå£°çš„é‡ã€‚æˆ‘ä»¬æå‡ºï¼Œé¢„æµ‹å¹²å‡€æ•°æ®ä¸é¢„æµ‹å¸¦å™ªå£°çš„é‡åœ¨æœ¬è´¨ä¸Šæ˜¯ä¸åŒçš„ï¼ŒåŸºäºæµå½¢å‡è®¾ï¼Œè‡ªç„¶æ•°æ®åº”ä½äºä½ç»´æµå½¢ä¸Šã€‚æˆ‘ä»¬å»ºè®®ç›´æ¥é¢„æµ‹å¹²å‡€æ•°æ®çš„æ¨¡å‹ï¼Œè¿™ä½¿å¾—å®¹é‡è¾ƒå°çš„ç½‘ç»œèƒ½å¤Ÿåœ¨é«˜ç»´ç©ºé—´ä¸­æœ‰æ•ˆè¿ä½œã€‚é€šè¿‡ä½¿ç”¨ç®€å•çš„å¤§è¡¥ä¸Transformerï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ImageNetä¸Šå–å¾—äº†ç«äº‰åŠ›çš„ç»“æœï¼Œè¯æ˜äº†è¿™ä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11653', 'title': 'GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning', 'url': 'https://huggingface.co/papers/2511.11653', 'abstract': 'Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.', 'score': 54, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'eaedfe0f841dd466', 'authors': ['Duolin Sun', 'Meixiu Long', 'Dan Yang', 'Yihan Jiao', 'Zhehao Tan', 'Jie Feng', 'Junjie Wang', 'Yue Shen', 'Peng Wei', 'Jian Wang', 'Jinjie Gu'], 'affiliations': ['Ant Group', 'Sun Yat-sen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11653.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#rag', '#rlhf', '#synthetic'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Groupwise, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ÑĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²-ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñƒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ GRPO Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Groupwise Reranking: Bridging Flexibility and Context in Document Evaluation', 'desc': 'This paper introduces Groupwise, a new reranking paradigm designed to improve the performance of retrieval-augmented generation (RAG) systems. Unlike traditional Pointwise methods that evaluate documents independently and can miss important relationships, or Listwise methods that consider global context but struggle with scalability, Groupwise allows for within-group comparisons of candidate documents. The model is trained using a novel GRPO approach that combines ranking metrics with a distributional reward to enhance score alignment across groups. Additionally, the authors present a method for generating high-quality labeled data to support both the reranker and retriever, demonstrating the effectiveness of their approach through extensive experiments on two challenging benchmarks.'}, 'zh': {'title': 'ç»„é‡æ’åºï¼šçµæ´»ä¸æ¯”è¾ƒçš„å®Œç¾ç»“åˆ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¢å¼ºRAGç³»ç»Ÿæ•´ä½“æ€§èƒ½æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶ä½œä¸ºé‡æ’åºå™¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é‡æ’åºæ–¹æ³•é¢ä¸´ä¸€ä¸ªæ ¸å¿ƒçš„ç†è®ºå’Œå®è·µå›°å¢ƒï¼šç‚¹å¯¹ç‚¹æ–¹æ³•è™½ç„¶ç®€å•çµæ´»ï¼Œä½†ç‹¬ç«‹è¯„ä¼°æ–‡æ¡£ï¼Œå®¹æ˜“é™·å…¥æ’åè¿‘è§†é™·é˜±ï¼Œå¿½è§†æ–‡æ¡£ä¹‹é—´çš„ç›¸å¯¹é‡è¦æ€§ã€‚ç›¸å¯¹è€Œè¨€ï¼Œåˆ—è¡¨æ–¹æ³•èƒ½å¤Ÿæ„ŸçŸ¥å…¨å±€æ’åä¸Šä¸‹æ–‡ï¼Œä½†ç”±äºå›ºæœ‰çš„åˆ—è¡¨åˆšæ€§ï¼Œåœ¨å¤„ç†å¤§å€™é€‰é›†æ—¶ä¼šé‡åˆ°ä¸¥é‡çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡æ’åºèŒƒå¼â€”â€”ç»„é‡æ’åºï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç‚¹å¯¹ç‚¹æ–¹æ³•çµæ´»æ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨åˆ—è¡¨æ–¹æ³•çš„æ¯”è¾ƒèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13648', 'title': 'PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image', 'url': 'https://huggingface.co/papers/2511.13648', 'abstract': '3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.', 'score': 52, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '2451d05234eb623d', 'authors': ['Ziang Cao', 'Fangzhou Hong', 'Zhaoxi Chen', 'Liang Pan', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13648.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#robotics', '#3d', '#dataset', '#architecture', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ñ„Ğ¾Ñ‚Ğ¾ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ AI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PhysX-Anything â€” Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ…, Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VLM-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² 193 Ñ€Ğ°Ğ·Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° PhysX-Mobility Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2000 Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑÑ… ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº.'}, 'en': {'title': 'Empowering 3D Generation with Physics and Articulation', 'desc': 'This paper presents PhysX-Anything, a novel framework for generating simulation-ready 3D models from single images. It addresses the limitations of existing methods by incorporating physical properties and articulation into the 3D generation process. The framework utilizes a new representation that significantly reduces the number of tokens needed for geometry learning, enhancing the quality of generated assets. Additionally, it introduces the PhysX-Mobility dataset, which expands the diversity of physical 3D objects, enabling better training and generalization for applications in embodied AI and robotics.'}, 'zh': {'title': 'ç‰©ç†æ¨¡æ‹Ÿçš„3Dç”Ÿæˆæ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†PhysX-Anythingï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªä¸ºç‰©ç†æ¨¡æ‹Ÿå‡†å¤‡çš„3Dç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®å•å¼ è‡ªç„¶å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„3Dèµ„äº§ï¼Œå…·å¤‡æ˜ç¡®çš„å‡ ä½•å½¢çŠ¶ã€å…³èŠ‚å’Œç‰©ç†å±æ€§ã€‚è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„3Dç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„3Dè¡¨ç¤ºæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡æ„å»ºæ–°çš„æ•°æ®é›†PhysX-Mobilityï¼Œè®ºæ–‡æ‰©å±•äº†ç‰©ç†3Dæ•°æ®é›†çš„å¯¹è±¡ç±»åˆ«ï¼ŒéªŒè¯äº†PhysX-Anythingåœ¨ç”Ÿæˆæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13704', 'title': 'TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models', 'url': 'https://huggingface.co/papers/2511.13704', 'abstract': "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.", 'score': 42, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'ae01eafd685a6f2b', 'authors': ['Harold Haodong Chen', 'Disen Lan', 'Wen-Jie Shu', 'Qingyang Liu', 'Zihan Wang', 'Sirui Chen', 'Wenkai Cheng', 'Kanghao Chen', 'Hongfei Zhang', 'Zixin Zhang', 'Rongjin Guo', 'Yu Cheng', 'Ying-Cong Chen'], 'affiliations': ['CUHK', 'CityUHK', 'FDU', 'HKUST', 'HKUST(GZ)', 'Knowin', 'SJTU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13704.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#open_source', '#video', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ TiViBench â€” Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (image-to-video), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sora 2, Veo 3.1) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ VideoTPO â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· LLM Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unlocking Reasoning in Video Generation Models', 'desc': 'This paper discusses the advancements in video generative models, emphasizing the need for these models to not only produce realistic visuals but also demonstrate logical reasoning and physical plausibility. The authors introduce TiViBench, a new benchmark designed to evaluate the reasoning capabilities of image-to-video generation models across four key dimensions, including structural reasoning and action planning. They find that while commercial models show promising reasoning abilities, open-source models have potential that is limited by their training data and scale. Additionally, the paper presents VideoTPO, a strategy that enhances reasoning performance during testing by analyzing generated outputs, thus improving the models without extra training or data.'}, 'zh': {'title': 'æ¨åŠ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°', 'desc': 'éšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶é‡ç‚¹å·²ä»ç”Ÿæˆè§†è§‰ä¸Šå¯ä¿¡çš„è¾“å‡ºè½¬å‘è§£å†³éœ€è¦ç‰©ç†åˆç†æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡åƒVeo 3è¿™æ ·çš„æ¨¡å‹åœ¨é“¾å¼æ¨ç†æ–¹é¢å–å¾—äº†çªç ´ï¼Œä½†å®ƒä»¬æ˜¯å¦å…·å¤‡ç±»ä¼¼å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»ä¸æ˜ç¡®ã€‚ç°æœ‰çš„è¯„ä¼°æ ‡å‡†ä¸»è¦å…³æ³¨è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œæœªèƒ½æ•æ‰æ›´é«˜é˜¶çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TiViBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¨ç†èƒ½åŠ›çš„åˆ†å±‚åŸºå‡†ï¼Œæ¶µç›–äº†ç»“æ„æ¨ç†ã€ç©ºé—´è§†è§‰æ¨¡å¼æ¨ç†ã€ç¬¦å·é€»è¾‘æ¨ç†å’Œè¡ŒåŠ¨è§„åˆ’ç­‰å››ä¸ªç»´åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12710', 'title': 'Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs', 'url': 'https://huggingface.co/papers/2511.12710', 'abstract': 'Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce EvoSynth, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.', 'score': 36, 'issue_id': 1, 'pub_date': '2025-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': '1119c225aea5f650', 'authors': ['Yunhao Chen', 'Xin Wang', 'Juncheng Li', 'Yixu Wang', 'Jie Li', 'Yan Teng', 'Yingchun Wang', 'Xingjun Ma'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12710.jpg', 'data': {'categories': ['#open_source', '#security'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·: Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğº Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° EvoSynth â€” Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸, Ğ° ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EvoSynth Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ°Ñ‚Ğ°Ğº 85,5% Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Claude-Sonnet-4.5, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ğ°Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹.'}, 'en': {'title': 'EvoSynth: Evolving Novel Attack Strategies for LLMs', 'desc': 'This paper presents EvoSynth, an innovative framework designed to enhance the capabilities of automated red teaming for Large Language Models (LLMs). Unlike traditional methods that rely on existing attack strategies, EvoSynth autonomously creates and evolves new attack algorithms using a multi-agent system. It incorporates a self-correction mechanism that allows it to adapt and improve its attack logic based on previous failures. The results show that EvoSynth achieves a remarkable 85.5% Attack Success Rate (ASR) and generates a wider variety of attacks compared to current techniques, marking a significant advancement in the field.'}, 'zh': {'title': 'EvoSynthï¼šè¿›åŒ–åˆæˆæ–°æ”»å‡»ç­–ç•¥çš„è‡ªåŠ¨åŒ–æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEvoSynthçš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ”»å‡»ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒEvoSynthé€šè¿‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè‡ªä¸»è®¾è®¡ã€æ¼”åŒ–å’Œæ‰§è¡Œæ–°çš„ä»£ç æ”»å‡»ç®—æ³•ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¼˜åŒ–å·²æœ‰çš„æ”»å‡»ç­–ç•¥ã€‚è¯¥æ¡†æ¶å…·æœ‰ä»£ç çº§è‡ªæˆ‘ä¿®æ­£å¾ªç¯ï¼Œèƒ½å¤Ÿæ ¹æ®å¤±è´¥æƒ…å†µè¿­ä»£é‡å†™æ”»å‡»é€»è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoSynthåœ¨å¯¹æŠ—å¼ºå¤§æ¨¡å‹æ—¶ï¼Œæ”»å‡»æˆåŠŸç‡è¾¾åˆ°85.5%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ”»å‡»æ–¹å¼æ¯”ç°æœ‰æ–¹æ³•æ›´åŠ å¤šæ ·åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11332', 'title': 'UFO^3: Weaving the Digital Agent Galaxy', 'url': 'https://huggingface.co/papers/2511.11332', 'abstract': 'UFO$^3$ unifies heterogeneous devices into a single orchestration fabric, enabling seamless task collaboration and dynamic optimization across distributed environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO^3, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO^3 models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.   We evaluate UFO^3 on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO^3 achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO^3 achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.', 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'be012751db8bdc4f', 'authors': ['Chaoyun Zhang', 'Liqun Li', 'He Huang', 'Chiming Ni', 'Bo Qiao', 'Si Qin', 'Yu Kang', 'Minghua Ma', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Microsoft', 'ZJU-UIUC Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11332.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞ° Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'UFOÂ³ â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° (Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿Ñ‹, ÑĞµÑ€Ğ²ĞµÑ€Ñ‹, Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, edge-ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°) Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¼ÑƒÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑĞ¾Ğ·Ğ²ĞµĞ·Ğ´Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ (TaskConstellation) â€” Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„ (DAG) Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾ÑÑ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ DAG Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (AIP). ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ NebulaBench UFOÂ³ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 70.9% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 31% ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ±Ğ¾ÑĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unifying Devices for Seamless Task Collaboration', 'desc': 'UFO$^3$ is a system designed to integrate various devices into a single framework for better collaboration and task management. It treats user requests as a dynamic structure called TaskConstellation, which consists of smaller tasks that can adapt and change as new information is received. This allows for efficient execution and recovery from failures, making the system resilient and responsive. The evaluation shows that UFO$^3 significantly improves task completion rates and reduces latency, demonstrating its effectiveness in orchestrating tasks across different devices.'}, 'zh': {'title': 'UFO$^3$: ç»Ÿä¸€å¼‚æ„è®¾å¤‡ï¼Œå®ç°æ™ºèƒ½åä½œ', 'desc': 'UFO$^3$ æ˜¯ä¸€ä¸ªå°†ä¸åŒè®¾å¤‡ç»Ÿä¸€ä¸ºå•ä¸€è°ƒåº¦æ¡†æ¶çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®ç°æ— ç¼çš„ä»»åŠ¡åä½œå’ŒåŠ¨æ€ä¼˜åŒ–ã€‚å®ƒå°†æ¯ä¸ªç”¨æˆ·è¯·æ±‚å»ºæ¨¡ä¸ºä¸€ä¸ªå¯å˜çš„ä»»åŠ¡æ˜Ÿåº§ï¼ŒåŒ…å«å¤šä¸ªåŸå­å­ä»»åŠ¡ï¼Œå¹¶æ˜ç¡®æ§åˆ¶å’Œæ•°æ®ä¾èµ–å…³ç³»ã€‚é€šè¿‡æŒç»­æ¥æ”¶æ¥è‡ªåˆ†å¸ƒå¼è®¾å¤‡çš„ç»“æœï¼Œä»»åŠ¡æ˜Ÿåº§èƒ½å¤Ÿå®ç°å¼‚æ­¥æ‰§è¡Œå’Œè‡ªé€‚åº”æ¢å¤ã€‚UFO$^3$ åœ¨å¤šä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºé«˜æ•ˆã€å‡†ç¡®å’Œå¼¹æ€§çš„ä»»åŠ¡è°ƒåº¦èƒ½åŠ›ï¼Œæ‰“ç ´äº†è®¾å¤‡å’Œå¹³å°ä¹‹é—´çš„ä¼ ç»Ÿç•Œé™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14659', 'title': 'NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards', 'url': 'https://huggingface.co/papers/2511.14659', 'abstract': 'NORA-1.5, an enhanced vision-language-action model with a flow-matching-based action expert and reward-driven post-training, improves performance and reliability in both simulated and real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '37e80e065451ab5e', 'authors': ['Chia-Yu Hung', 'Navonil Majumder', 'Haoyuan Deng', 'Liu Renhang', 'Yankang Ang', 'Amir Zadeh', 'Chuan Li', 'Dorien Herremans', 'Ziwei Wang', 'Soujanya Poria'], 'affiliations': ['Lambda Labs', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14659.jpg', 'data': {'categories': ['#training', '#multimodal', '#robotics', '#rlhf', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° NORA-1.5 â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ vision-language-action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ, ÑĞ·Ñ‹Ğº Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ° Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° flow-matching, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (reward signals), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ world model Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· direct preference optimization. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ….'}, 'en': {'title': 'NORA-1.5: Enhancing VLA Models for Real-World Reliability', 'desc': 'NORA-1.5 is an advanced vision-language-action (VLA) model that enhances performance and reliability in various tasks by integrating a flow-matching-based action expert. This model builds on the pre-trained NORA backbone and shows significant improvements over its predecessor and other leading VLA models in both simulated and real-world scenarios. To further boost robustness, NORA-1.5 employs reward models during post-training, which assess the effectiveness of actions based on their alignment with desired goals. The results indicate that these reward-driven strategies lead to more dependable embodied agents, making NORA-1.5 a promising solution for real-world applications.'}, 'zh': {'title': 'NORA-1.5ï¼šæå‡å¯é æ€§çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹', 'desc': 'NORA-1.5æ˜¯ä¸€ç§å¢å¼ºçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥åŸºäºæµåŒ¹é…çš„åŠ¨ä½œä¸“å®¶å’Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒï¼Œæå‡äº†åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒçš„NORAåŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å¥–åŠ±æ¨¡å‹ï¼Œç»“åˆäº†åŠ¨ä½œæ¡ä»¶çš„ä¸–ç•Œæ¨¡å‹å’ŒçœŸå®åŠ¨ä½œåå·®çš„å¯å‘å¼è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¥–åŠ±é©±åŠ¨çš„åè®­ç»ƒåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­å‡èƒ½æœ‰æ•ˆæå‡æ¨¡å‹çš„å¯é æ€§å’Œä»»åŠ¡æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13714', 'title': 'UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity', 'url': 'https://huggingface.co/papers/2511.13714', 'abstract': 'The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UnSAMv2 improves NoC_{90} (5.69 rightarrow 4.75), 1-IoU (58.0 rightarrow 73.1), and AR_{1000} (49.6 rightarrow 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'd486f379a58ff86c', 'authors': ['Junwei Yu', 'Trevor Darrell', 'XuDong Wang'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13714.jpg', 'data': {'categories': ['#video', '#cv', '#benchmark', '#dataset'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnSAMv2 â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Segment Anything Model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ¸Ğ½Ğ³ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (self-supervised learning) Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 11 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹, Ğ¿Ğ¾Ğ»Ğ½Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Precise Segmentation Control with UnSAMv2', 'desc': 'The paper introduces UnSAMv2, an advanced model that enhances the segmentation capabilities of the Segment Anything Model (SAM) by allowing users to control the granularity of segmentation without needing human annotations. It employs a divide-and-conquer strategy to identify various mask-granularity pairs and incorporates a new granularity control embedding for precise segmentation adjustments. This approach significantly reduces the reliance on labeled data, using only 6,000 unlabeled images while adding minimal parameters. The results demonstrate substantial improvements in segmentation performance across multiple tasks, showcasing the effectiveness of self-supervised learning in vision models.'}, 'zh': {'title': 'æ— æ ‡æ³¨ä¸‹çš„ä»»æ„ç²’åº¦åˆ†å‰²', 'desc': 'Segment Anything Modelï¼ˆSAMï¼‰ç³»åˆ—åœ¨è§†è§‰åŸºç¡€æ¨¡å‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶æ§åˆ¶åˆ†å‰²ç²’åº¦çš„èƒ½åŠ›æœ‰é™ã€‚ç”¨æˆ·é€šå¸¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´ç»“æœï¼Œé€šè¿‡æ·»åŠ æ›´å¤šæç¤ºæˆ–ä»é¢„ç”Ÿæˆçš„æ©ç ä¸­é€‰æ‹©ï¼Œä»¥è¾¾åˆ°æ‰€éœ€çš„ç»†èŠ‚æ°´å¹³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UnSAMv2ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°ä»»æ„ç²’åº¦çš„åˆ†å‰²ã€‚UnSAMv2é€šè¿‡å‘ç°ä¸°å¯Œçš„æ©ç ç²’åº¦å¯¹å’Œå¼•å…¥æ–°é¢–çš„ç²’åº¦æ§åˆ¶åµŒå…¥ï¼Œæ˜¾è‘—æå‡äº†SAM-2çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨äº¤äº’å¼ã€å…¨å›¾å’Œè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸­å®ç°ä»»æ„ç²’åº¦çš„åˆ†å‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12997', 'title': 'WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance', 'url': 'https://huggingface.co/papers/2511.12997', 'abstract': 'Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'e25bc8dee4f8e2bf', 'authors': ['Genglin Liu', 'Shijie Geng', 'Sha Li', 'Hejie Cui', 'Sarah Zhang', 'Xin Liu', 'Tianyi Liu'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12997.jpg', 'data': {'categories': ['#optimization', '#long_context', '#multimodal', '#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WebCoach â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: WebCondenser Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Coach Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… ÑĞµÑÑĞ¸Ğ¹. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-ÑĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ñ 47% Ğ´Ğ¾ 61% Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 38B, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ WebCoach Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ GPT-4o.'}, 'en': {'title': 'Empowering Web Agents with Persistent Memory for Continuous Learning', 'desc': 'This paper presents WebCoach, a framework designed to enhance the capabilities of multimodal LLM-powered web browsing agents. It addresses the limitations of current agents, which often make repetitive errors and cannot learn from past experiences across different sessions. WebCoach introduces a persistent memory system that allows agents to improve their long-term planning and learning without needing retraining. The framework includes components for summarizing navigation logs, organizing experiences, and providing task-specific advice, leading to significant performance improvements in web navigation tasks.'}, 'zh': {'title': 'WebCoachï¼šæå‡ç½‘ç»œä»£ç†çš„é•¿æœŸå­¦ä¹ èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºWebCoachçš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç½‘ç»œæµè§ˆä»£ç†çš„èƒ½åŠ›ã€‚WebCoaché€šè¿‡å¼•å…¥æŒä¹…çš„è·¨ä¼šè¯è®°å¿†ï¼Œè§£å†³äº†å½“å‰ä»£ç†åœ¨å¤æ‚æµè§ˆä»»åŠ¡ä¸­åå¤å‡ºé”™å’Œç¼ºä¹å­¦ä¹ èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šWebCondenserã€å¤–éƒ¨è®°å¿†å­˜å‚¨å’Œæ•™ç»ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ç»„ç»‡å’Œåˆ©ç”¨ä»£ç†çš„ç»éªŒã€‚é€šè¿‡ä¸æ–­æ›´æ–°æƒ…èŠ‚è®°å¿†ï¼ŒWebCoachä½¿ä»£ç†åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°æŒç»­å­¦ä¹ ï¼Œä»è€Œæé«˜äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13655', 'title': 'OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation', 'url': 'https://huggingface.co/papers/2511.13655', 'abstract': "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at https://github.com/allenai/olmoearth_pretrain{https://github.com/allenai/olmoearth_pretrain}.", 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '141cf7a50cd05ba5', 'authors': ['Henry Herzog', 'Favyen Bastani', 'Yawen Zhang', 'Gabriel Tseng', 'Joseph Redmon', 'Hadrien Sablon', 'Ryan Park', 'Jacob Morrison', 'Alexandra Buraczynski', 'Karen Farley', 'Joshua Hansen', 'Andrew Howe', 'Patrick Alan Johnson', 'Mark Otterlee', 'Ted Schmitt', 'Hunter Pitelka', 'Stephen Daspit', 'Rachel Ratner', 'Christopher Wilhelm', 'Sebastian Wood', 'Mike Jacobi', 'Hannah Kerner', 'Evan Shelhamer', 'Ali Farhadi', 'Ranjay Krishna', 'Patrick Beukema'], 'affiliations': ['Allen Institute for AI', 'Arizona State University', 'Columbia University', 'University of British Columbia', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13655.jpg', 'data': {'categories': ['#training', '#science', '#multimodal', '#benchmark', '#open_source', '#dataset', '#cv'], 'emoji': 'ğŸ›°ï¸', 'ru': {'title': 'Ğ¤ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑˆĞµĞ¹ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ', 'desc': 'OlmoEarth â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ—ĞµĞ¼Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ¸Ğ· 24 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ½Ğ° 19 Ğ¸Ğ· 29 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ fine-tuning, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ 12 Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¸ OlmoEarth ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² ĞµÑ‘ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.'}, 'en': {'title': 'OlmoEarth: Revolutionizing Earth Observation with Multimodal Learning', 'desc': 'OlmoEarth is a new foundation model designed specifically for Earth observation data, which combines spatial and temporal information. It uses a unique self-supervised learning approach, along with innovative masking strategies and loss functions tailored for this domain. The model outperforms 12 other foundation models on various benchmarks, achieving top results in multiple tasks related to Earth observation. Additionally, OlmoEarth is integrated into a comprehensive platform that facilitates data management and model training for organizations addressing global challenges.'}, 'zh': {'title': 'OlmoEarthï¼šåœ°çƒè§‚æµ‹çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹', 'desc': 'OlmoEarthæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ—¶ç©ºåŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹åœ°çƒè§‚æµ‹æ•°æ®çš„æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€æ©è”½ç­–ç•¥å’ŒæŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹é¢†åŸŸçš„è¡¨ç°ã€‚ä¸å…¶ä»–12ä¸ªåŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒOlmoEarthåœ¨å¤šä¸ªç ”ç©¶åŸºå‡†å’Œå®é™…ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨åµŒå…¥è¯„ä¼°ä¸­åœ¨24ä¸ªä»»åŠ¡ä¸­å–å¾—äº†15ä¸ªæœ€ä½³æˆç»©ã€‚è¯¥å¹³å°ä¸ºéè¥åˆ©ç»„ç»‡å’Œéæ”¿åºœç»„ç»‡æä¾›äº†å¼ºå¤§çš„æ•°æ®ç®¡ç†å·¥å…·ï¼Œå¸®åŠ©ä»–ä»¬è§£å†³å…¨çƒé‡å¤§é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13646', 'title': 'Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?', 'url': 'https://huggingface.co/papers/2511.13646', 'abstract': 'Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-GÃ¶del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'f58f5280b67db006', 'authors': ['Chunqiu Steven Xia', 'Zhe Wang', 'Yan Yang', 'Yuxiang Wei', 'Lingming Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13646.jpg', 'data': {'categories': ['#optimization', '#plp', '#benchmark', '#open_source', '#agents', '#reasoning'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Live-SWE-agent â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. ĞĞ³ĞµĞ½Ñ‚ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (bash ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹) Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ LLM Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Live-SWE-agent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ 75,4% Ğ½Ğ° SWE-bench Verified Ğ¸ 45,8% Ğ½Ğ° SWE-Bench Pro, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Evolving Software Agents: Live-SWE-agent Redefines Autonomy in Coding', 'desc': 'This paper introduces Live-SWE-agent, a novel software agent that can autonomously evolve its capabilities during runtime to tackle real-world software engineering tasks. Unlike traditional agents that require extensive offline training and may not generalize well, Live-SWE-agent starts with a basic framework and improves itself on-the-fly using only essential tools. The evaluation demonstrates that it achieves a 75.4% solve rate on the SWE-bench Verified benchmark, outperforming existing open-source agents and nearing the performance of top proprietary solutions. Additionally, it sets a new record with a 45.8% solve rate on the SWE-Bench Pro benchmark, showcasing its effectiveness in dynamic environments.'}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„è½¯ä»¶ä»£ç†ï¼Œå®æ—¶è§£å†³è½¯ä»¶é—®é¢˜', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨æ”¹å˜å‡ ä¹æ‰€æœ‰è¡Œä¸šï¼ŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLive-SWE-agentçš„è‡ªæˆ‘è¿›åŒ–è½¯ä»¶ä»£ç†ï¼Œå®ƒèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶è‡ªä¸»æ¼”åŒ–ï¼Œä»¥è§£å†³å®é™…è½¯ä»¶é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦ç¦»çº¿è®­ç»ƒçš„ä»£ç†ä¸åŒï¼ŒLive-SWE-agentä»æœ€åŸºæœ¬çš„ä»£ç†æ¡†æ¶å¼€å§‹ï¼Œåˆ©ç”¨bashå·¥å…·åœ¨è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ä¸æ–­æ”¹è¿›è‡ªèº«ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒLive-SWE-agentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè§£å†³ç‡è¾¾åˆ°75.4%ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºè½¯ä»¶ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12797', 'title': 'Genomic Next-Token Predictors are In-Context Learners', 'url': 'https://huggingface.co/papers/2511.12797', 'abstract': 'In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?   To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': 'b42dd83a443a06a0', 'authors': ['Nathan Breslow', 'Aayush Mishra', 'Mahler Revsine', 'Michael C. Schatz', 'Anqi Liu', 'Daniel Khashabi'], 'affiliations': ['Department of Computer Science, Johns Hopkins University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12797.jpg', 'data': {'categories': ['#benchmark', '#training', '#science', '#reasoning'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ² Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (in-context learning) Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ½Ğ¾ Ğ¸ Ğ² Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Evo2 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½ÑƒĞºĞ»ĞµĞ¾Ñ‚Ğ¸Ğ´ (Ğ/Ğ¢/Ğ¦/Ğ“) Ğ² Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ LLM. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾Ğ¼ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking In-Context Learning Beyond Language', 'desc': 'This paper investigates in-context learning (ICL), which is the ability of models to learn from examples given in their input, in the context of genomic sequences. The authors focus on the Evo2 genomic model, which predicts the next nucleotide in DNA sequences, and compare its performance to large language models. They find that genomic models can also demonstrate ICL, showing improved pattern recognition as more examples are provided. This research suggests that ICL is not limited to language but can emerge in other domains through extensive predictive training.'}, 'zh': {'title': 'åŸºå› ç»„åºåˆ—ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šè¶…è¶Šè¯­è¨€çš„ç»Ÿä¸€è§†è§’', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†åœ¨åŸºå› ç»„åºåˆ—ä¸­æ˜¯å¦å¯ä»¥é€šè¿‡å¤§è§„æ¨¡é¢„æµ‹è®­ç»ƒè‡ªç„¶äº§ç”Ÿä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨Evo2åŸºå› ç»„æ¨¡å‹ï¼Œä¸»è¦è¿›è¡Œä¸‹ä¸€ä¸ªæ ¸è‹·é…¸çš„é¢„æµ‹ï¼Œå¹¶ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºå› ç»„æ¨¡å‹åœ¨æ¨¡å¼å½’çº³æ–¹é¢çš„è¡¨ç°ä¸è¯­è¨€æ¨¡å‹ç›¸ä¼¼ï¼Œéšç€ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡çš„å¢åŠ ï¼Œè¡¨ç°å‡ºå¯¹æ•°çº¿æ€§å¢é•¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æä¾›äº†åŸºå› ç»„åºåˆ—ä¸­è‡ªå‘æ€§ICLçš„è¯æ®ï¼Œæ”¯æŒäº†ICLä½œä¸ºå¤§è§„æ¨¡é¢„æµ‹å»ºæ¨¡ç»“æœçš„å‡è®¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12472', 'title': 'Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing', 'url': 'https://huggingface.co/papers/2511.12472', 'abstract': 'SerenQA evaluates Large Language Models\' ability to generate surprising and valuable answers in scientific knowledge graph question answering, particularly in drug repurposing, using a structured pipeline and serendipity metric.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs\' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': '6ab13dc20fb4904d', 'authors': ['Mengying Wang', 'Chenhui Ma', 'Ao Jiao', 'Tuo Liang', 'Pengjun Lu', 'Shrinidhi Hegde', 'Yu Yin', 'Evren Gurkan-Cavusoglu', 'Yinghui Wu'], 'affiliations': ['Case Western Reserve University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12472.jpg', 'data': {'categories': ['#science', '#benchmark', '#open_source', '#dataset', '#graphs', '#reasoning', '#healthcare'], 'emoji': 'ğŸ’Š', 'ru': {'title': 'ĞÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğº Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑĞ¼ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° SerenQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ°Ñ€Ğ¼Ğ°ĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚Ğ¸ (serendipity), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾Ğ²Ğ¸Ğ·Ğ½Ğµ Ğ¸ ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‚Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸: Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ¿Ğ¾Ğ´Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Surprising Insights in Drug Repurposing with SerenQA', 'desc': 'This paper introduces SerenQA, a framework designed to assess how well Large Language Models (LLMs) can provide surprising and valuable answers in scientific knowledge graph question answering, especially in the context of drug repurposing. It highlights the limitations of current systems that focus on delivering predictable answers and emphasizes the need for models that can generate novel insights. The authors define a new task called serendipity-aware KGQA and propose a metric that evaluates answers based on relevance, novelty, and surprise. Through their experiments, they demonstrate that while LLMs excel in retrieving information, they often fail to produce genuinely unexpected and useful findings, indicating a need for further advancements in this area.'}, 'zh': {'title': 'æ¢ç´¢æ„å¤–å‘ç°çš„çŸ¥è¯†å›¾è°±é—®ç­”æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†SerenQAæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦çŸ¥è¯†å›¾è°±é—®ç­”ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯ç‰©é‡å®šä½æ–¹é¢ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºæ„å¤–æ€§åº¦é‡ï¼Œæ—¨åœ¨é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ„å¤–ä¸”æœ‰ä»·å€¼çš„ç­”æ¡ˆã€‚SerenQAåŒ…æ‹¬çŸ¥è¯†æ£€ç´¢ã€å­å›¾æ¨ç†å’Œæ„å¤–æ€§æ¢ç´¢ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ„æˆäº†ä¸€ä¸ªç»“æ„åŒ–çš„è¯„ä¼°æµç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰çš„å…ˆè¿›æ¨¡å‹åœ¨æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å‘ç°çœŸæ­£æ„å¤–å’Œæœ‰ä»·å€¼çš„å‘ç°æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11407', 'title': 'MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2511.11407', 'abstract': "Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom's level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '1ddd429bf6ae4443', 'authors': ['Manyu Li', 'Ruian He', 'Chenxi Ma', 'Weimin Tan', 'Bo Yan'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11407.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#open_source', '#dataset', '#agents', '#graphs', '#synthetic', '#healthcare'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MicroVQA++, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Visual Question Answering Ğ² Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ· Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ° BIOMEDICA Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ½Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ HiCQA-Graph Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ¼Ğ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ğ½Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MLLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ğ¸Ñ… Ğ»ÑĞ´Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ GPT-5.'}, 'en': {'title': 'Enhancing Microscopy Reasoning with MicroVQA++ Dataset', 'desc': 'This paper presents MicroVQA++, a comprehensive dataset designed to enhance the performance of Multimodal Large Language Models (MLLMs) in biomedical microscopy. The dataset is created through a three-stage process that includes expert validation, graph-based filtering, and human refinement to ensure high quality and consistency. A novel graph structure, HiCQA-Graph, is introduced to integrate images, captions, and questions for better cross-modal reasoning. The results demonstrate that this carefully constructed dataset allows MLLMs to achieve competitive performance in microscopy reasoning tasks, surpassing existing benchmarks.'}, 'zh': {'title': 'æ„å»ºé«˜è´¨é‡æ˜¾å¾®é•œè§†è§‰é—®ç­”æ•°æ®é›†çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MicroVQA++ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ˜¾å¾®é•œå›¾åƒçš„é«˜è´¨é‡å¤šæ¨¡æ€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ„å»ºï¼Œç¬¬ä¸€é˜¶æ®µä»åŒè¡Œè¯„å®¡æ–‡ç« ä¸­æå–ä¸“å®¶éªŒè¯çš„å›¾åƒ-æ ‡é¢˜å¯¹è¿›è¡Œç›‘ç£ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†HiCQA-Graphï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼‚æ„å›¾ï¼Œç”¨äºèåˆæ–‡æœ¬è•´å«ã€è§†è§‰-è¯­è¨€å¯¹é½å’Œä»£ç†ä¿¡å·ï¼Œä»¥è¯†åˆ«å’Œè¿‡æ»¤ä¸ä¸€è‡´æ ·æœ¬ã€‚æœ€åï¼Œç¬¬ä¸‰é˜¶æ®µåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¹¶ç»è¿‡äººå·¥ç­›é€‰ï¼Œæœ€ç»ˆå½¢æˆäº†ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10628', 'title': 'Instella: Fully Open Language Models with Stellar Performance', 'url': 'https://huggingface.co/papers/2511.10628', 'abstract': 'Instella, a family of fully open large language models, achieves state-of-the-art performance using open data and is competitive with leading open-weight models, with specialized variants for long context and mathematical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'd68b84bd40c83319', 'authors': ['Jiang Liu', 'Jialian Wu', 'Xiaodong Yu', 'Yusheng Su', 'Prakamya Mishra', 'Gowtham Ramesh', 'Sudhanshu Ranjan', 'Chaitanya Manem', 'Ximeng Sun', 'Ze Wang', 'Pratik Prabhanjan Brahma', 'Zicheng Liu', 'Emad Barsoum'], 'affiliations': ['AMD'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10628.jpg', 'data': {'categories': ['#alignment', '#training', '#long_context', '#open_source', '#math', '#dataset', '#rlhf', '#reasoning', '#small_models'], 'emoji': 'ğŸ”“', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Instella â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸: Instella-Long Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Instella-Math Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· supervised fine-tuning Ğ¸ reinforcement learning. Ğ­Ñ‚Ğ¾Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Instella: Open Models for Transparent AI Excellence', 'desc': 'Instella is a family of fully open large language models that achieves top performance using publicly available data. It consists of three billion parameters and is trained on openly accessible resources, ensuring transparency and reproducibility. The models are optimized through large-scale pre-training and instruction tuning, with specialized versions for long context handling and mathematical reasoning. Instella demonstrates competitive results against leading models while promoting open research in language modeling.'}, 'zh': {'title': 'Instellaï¼šå¼€æ”¾ä¸é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹æ–°é€‰æ‹©', 'desc': 'Instellaæ˜¯ä¸€ç³»åˆ—å®Œå…¨å¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨å¼€æ”¾æ•°æ®è®­ç»ƒï¼Œè¡¨ç°å‡ºè‰²ã€‚å®ƒåœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œæ•°å­¦æ¨ç†æ–¹é¢æœ‰ä¸“é—¨çš„å˜ä½“ï¼Œèƒ½å¤Ÿä¸é¢†å…ˆçš„å¼€æ”¾æƒé‡æ¨¡å‹ç«äº‰ã€‚å°½ç®¡é¢„è®­ç»ƒçš„æ ‡è®°æ•°é‡è¿œå°‘äºè®¸å¤šåŒç±»æ¨¡å‹ï¼ŒInstellaä»ç„¶åœ¨å®Œå…¨å¼€æ”¾æ¨¡å‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¯¥æ¨¡å‹çš„å¼€å‘æ—¨åœ¨æé«˜é€æ˜åº¦å’Œå¯é‡å¤æ€§ï¼Œæ¨åŠ¨å¼€æ”¾å’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.09809', 'title': 'Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.09809', 'abstract': 'Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '4246d0dee4cab773', 'authors': ['Konstantinos M. Dafnis', 'Dimitris N. Metaxas'], 'affiliations': ['Rutgers University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.09809.jpg', 'data': {'categories': ['#multimodal', '#inference'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Spectrum-Aware Test-Time Steering (STS) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ´Ğ²Ğ¸Ğ³Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. STS Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ² 8 Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Adaptation of Vision-Language Models at Inference', 'desc': "This paper presents Spectrum-Aware Test-Time Steering (STS), a new method for adapting Vision-Language Models (VLMs) during inference without modifying the model's core components. STS focuses on extracting important semantic directions from textual embeddings and uses a small set of parameters to adjust latent representations effectively. Unlike traditional methods that require backpropagation, STS operates in the latent space, making it faster and more memory-efficient. Experimental results show that STS outperforms existing adaptation techniques while maintaining a lightweight framework."}, 'zh': {'title': 'è°±æ„ŸçŸ¥æµ‹è¯•æ—¶å¼•å¯¼ï¼šé«˜æ•ˆçš„è§†è§‰-è¯­è¨€æ¨¡å‹é€‚åº”æ–¹æ³•', 'desc': 'è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµ‹è¯•æ—¶é¢†åŸŸè½¬ç§»æ—¶å¸¸å¸¸æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ€è¿‘å‡ºç°äº†æƒ…æ™¯æµ‹è¯•æ—¶é€‚åº”ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿå°†VLMsé€‚åº”åˆ°å•ä¸ªæœªæ ‡è®°å›¾åƒä¸Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„é€‚åº”æ¡†æ¶ï¼Œç§°ä¸ºè°±æ„ŸçŸ¥æµ‹è¯•æ—¶å¼•å¯¼ï¼ˆSTSï¼‰ï¼Œå®ƒé€šè¿‡æå–æ–‡æœ¬åµŒå…¥çš„è°±å­ç©ºé—´æ¥å®šä¹‰ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼Œå¹¶é€šè¿‡é€‚åº”å°‘é‡æ ·æœ¬ç‰¹å®šçš„åç§»å‚æ•°æ¥æœ€å°åŒ–ç†µã€‚STSåœ¨æ½œåœ¨ç©ºé—´ä¸­å®Œå…¨è¿›è¡Œæ¨ç†ï¼Œè€Œæ— éœ€é€šè¿‡æˆ–ä¿®æ”¹å†»ç»“çš„ç¼–ç å™¨ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒSTSåœ¨æ€§èƒ½å’Œé€Ÿåº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æµ‹è¯•æ—¶é€‚åº”æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12982', 'title': 'SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization', 'url': 'https://huggingface.co/papers/2511.12982', 'abstract': 'Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '2c6cc3576cea5514', 'authors': ['Xuankun Rong', 'Wenke Huang', 'Tingfeng Wang', 'Daiguo Zhou', 'Bo Du', 'Mang Ye'], 'affiliations': ['MiLM Plus, Xiaomi Inc.', 'School of Computer Science, Wuhan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12982.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark', '#dataset', '#rlhf', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸Ğ·-Ğ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ SafeGRPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Group Relative Policy Optimization Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° SafeTag-VL-3K Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ¸ÑĞºĞ°Ğ¼ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Safety in Multimodal Language Models with SafeGRPO', 'desc': "This paper discusses the challenges of safety in multimodal large language models (MLLMs) that process both text and images. It highlights how these models can produce unsafe outputs due to complex interactions between different types of data, even when the individual inputs are safe. The authors introduce SafeGRPO, a new framework that enhances safety by incorporating rule-based rewards into the existing Group Relative Policy Optimization method, allowing for better reasoning safety without needing human oversight. By using a specially designed dataset with safety tags, SafeGRPO improves the models' ability to reason safely and robustly across various tasks while maintaining their overall performance."}, 'zh': {'title': 'å®‰å…¨æ¨ç†ï¼Œç¨³å¥å¯¹é½çš„å¤šæ¨¡æ€æ¡†æ¶', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨ç†å’Œéµå¾ªæŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ‰©å±•çš„æ¨¡æ€ç©ºé—´å¸¦æ¥äº†æ–°çš„ç»„åˆå®‰å…¨é£é™©ã€‚è¿™äº›è·¨æ¨¡æ€çš„è€¦åˆå¯èƒ½ä¼šäº§ç”Ÿä¸å®‰å…¨çš„è¯­ä¹‰ï¼Œå³ä½¿å•ä¸ªè¾“å…¥æ˜¯æ— å®³çš„ï¼Œæš´éœ²äº†å½“å‰MLLMsè„†å¼±çš„å®‰å…¨æ„è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SafeGRPOï¼Œä¸€ä¸ªè‡ªæˆ‘å¥–åŠ±çš„å¤šæ¨¡æ€å®‰å…¨å¯¹é½æ¡†æ¶ï¼Œç»“åˆäº†è§„åˆ™é©±åŠ¨çš„å¥–åŠ±æ„å»ºï¼Œèƒ½å¤Ÿå®ç°å¯è§£é‡Šå’Œå¯éªŒè¯çš„æ¨ç†å®‰å…¨ä¼˜åŒ–ã€‚é€šè¿‡æ„å»ºå¸¦æœ‰æ˜ç¡®å®‰å…¨æ ‡ç­¾çš„æ•°æ®é›†ï¼ŒSafeGRPOæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€å®‰å…¨æ„è¯†å’Œæ¨ç†ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.02767', 'title': 'Dynamic Reflections: Probing Video Representations with Text Alignment', 'url': 'https://huggingface.co/papers/2511.02767', 'abstract': 'The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '8f7470ffc14d8969', 'authors': ['Tyler Zhu', 'Tengda Han', 'Leonidas Guibas', 'Viorica PÄƒtrÄƒucean', 'Maks Ovsjanikov'], 'affiliations': ['Google DeepMind', 'Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.02767.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#transfer_learning', '#video', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ñ‰Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ğ¼Ğ°Ğ»Ğ¾Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒÑ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Unlocking Video-Text Alignment for Enhanced Model Insights', 'desc': 'This paper explores how to align video and text representations to understand the similarities and capabilities of different machine learning models. It highlights that the effectiveness of this alignment is influenced by the complexity of the visual and textual data used during testing. The authors introduce scaling laws that help predict how well these models perform based on the richness of the input data. Additionally, they examine the relationship between semantic alignment and model performance, suggesting that better alignment may enhance video understanding and reasoning.'}, 'zh': {'title': 'è§†é¢‘ä¸æ–‡æœ¬çš„è¡¨ç¤ºå¯¹é½ï¼šæ¢ç´¢ç¼–ç å™¨çš„è¡¨ç°åŠ›', 'desc': 'æœ¬ç ”ç©¶é¦–æ¬¡å…¨é¢æ¢è®¨è§†é¢‘ä¸æ–‡æœ¬çš„è¡¨ç¤ºå¯¹é½ï¼Œåˆ†æç°ä»£è§†é¢‘å’Œè¯­è¨€ç¼–ç å™¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œè·¨æ¨¡æ€å¯¹é½é«˜åº¦ä¾èµ–äºæµ‹è¯•æ—¶æä¾›çš„è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„ä¸°å¯Œæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æœ€å…ˆè¿›çš„è§†é¢‘ç¼–ç å™¨æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†å‚æ•°åŒ–çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ³•åˆ™ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹è¿™ä¸€è¡Œä¸ºï¼Œå¹¶æ¢è®¨äº†è¯­ä¹‰å¯¹é½ä¸ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ—¶é—´æ¨ç†ä¸è·¨æ¨¡æ€å¯¹é½ç›¸å…³è”ï¼Œä¸ºè§†è§‰å’Œè¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13998', 'title': 'LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering', 'url': 'https://huggingface.co/papers/2511.13998', 'abstract': "LoCoBench-Agent evaluates large language models as autonomous software development agents using interactive scenarios, specialized tools, and multi-turn conversations to assess their long-context performance, comprehension, and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~qiu2025locobench assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce LoCoBench-Agent, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '039400aa2bef696e', 'authors': ['Jielin Qiu', 'Zuxin Liu', 'Zhiwei Liu', 'Rithesh Murthy', 'Jianguo Zhang', 'Haolin Chen', 'Shiyu Wang', 'Ming Zhu', 'Liangwei Yang', 'Juntao Tan', 'Roshan Ram', 'Akshara Prabhakar', 'Tulika Awalgaonkar', 'Zixiang Chen', 'Zhepeng Cen', 'Cheng Qian', 'Shelby Heinecke', 'Weiran Yao', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13998.jpg', 'data': {'categories': ['#plp', '#long_context', '#benchmark', '#agents', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'LoCoBench-Agent â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LoCoBench, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ 8000 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ 9 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¾Ñ‚ 10K Ğ´Ğ¾ 1Ğœ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼, Ğ½Ğ¾ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹.'}, 'en': {'title': 'Evaluating LLMs as Smart Software Development Agents', 'desc': 'LoCoBench-Agent is a new framework designed to evaluate large language models (LLMs) as autonomous agents in software development. It focuses on assessing their performance in multi-turn conversations and their ability to use specialized tools effectively over long contexts. The framework includes 8,000 interactive scenarios and introduces 9 evaluation metrics to measure comprehension and efficiency. Key findings show that while agents are robust in long contexts, there is a trade-off between comprehension and efficiency, and that strategic tool usage can significantly impact performance.'}, 'zh': {'title': 'è¯„ä¼°è‡ªä¸»è½¯ä»¶å¼€å‘ä»£ç†çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›', 'desc': 'LoCoBench-Agent æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªä¸»è½¯ä»¶å¼€å‘ä»£ç†çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡äº’åŠ¨åœºæ™¯ã€ä¸“ç”¨å·¥å…·å’Œå¤šè½®å¯¹è¯æ¥æµ‹è¯•è¿™äº›æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„è¡¨ç°ã€ç†è§£èƒ½åŠ›å’Œæ•ˆç‡ã€‚ä¸ç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸åŒï¼ŒLoCoBench-Agent èƒ½å¤Ÿæ•æ‰å¤šè½®äº’åŠ¨çš„ç‰¹æ€§å’Œå·¥å…·ä½¿ç”¨æ¨¡å¼ï¼Œæä¾›æ›´çœŸå®çš„è¯„ä¼°ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†ä¹ä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œå¸®åŠ©ç³»ç»Ÿåœ°åˆ†æä»£ç†åœ¨è½¯ä»¶å·¥ç¨‹å·¥ä½œæµä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12133', 'title': 'AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing', 'url': 'https://huggingface.co/papers/2511.12133', 'abstract': 'Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '381ada204b1fdbe2', 'authors': ['Qingyu Zhang', 'Chunlei Xin', 'Xuanang Chen', 'Yaojie Lu', 'Hongyu Lin', 'Xianpei Han', 'Le Sun', 'Qing Ye', 'Qianlong Xie', 'Xingxing Wang'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12133.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#rl'], 'emoji': 'ğŸ’¼', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ñƒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ TeleSalesCorpus â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚ĞµĞ»ĞµĞ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AI-Salesman â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Dynamic Outline-Guided Agent (DOGA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ¾Ğ¹ LLM-as-a-Judge, Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AI-Salesman ĞºĞ°Ğº Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ.'}, 'en': {'title': 'AI-Salesman: Mastering Persuasive Dialogue with Strategic Precision', 'desc': 'This paper addresses the challenges of goal-driven persuasive dialogue, particularly in telemarketing, where maintaining factual accuracy and strategic planning is crucial. The authors introduce TeleSalesCorpus, a new dataset specifically designed for training models in this domain. They propose a framework called AI-Salesman, which uses a two-stage approach: a Bayesian-supervised reinforcement learning algorithm for training and a Dynamic Outline-Guided Agent for inference. Experimental results show that AI-Salesman outperforms existing models, demonstrating its ability to effectively navigate complex persuasive interactions.'}, 'zh': {'title': 'æ™ºèƒ½é”€å”®ï¼šæå‡åŠè¯´å¯¹è¯çš„æœ‰æ•ˆæ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ç›®æ ‡é©±åŠ¨çš„åŠè¯´å¯¹è¯ï¼Œå°¤å…¶æ˜¯åœ¨ç”µè¯è¥é”€ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†TeleSalesCorpusï¼Œè¿™æ˜¯è¯¥é¢†åŸŸé¦–ä¸ªåŸºäºçœŸå®ä¸–ç•Œçš„å¯¹è¯æ•°æ®é›†ã€‚æå‡ºçš„AI-Salesmanæ¡†æ¶é‡‡ç”¨åŒé˜¶æ®µæ¶æ„ï¼Œé€šè¿‡è´å¶æ–¯ç›‘ç£å¼ºåŒ–å­¦ä¹ ç®—æ³•å­¦ä¹ ç¨³å¥çš„é”€å”®ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€å¤§çº²å¼•å¯¼ä»£ç†ï¼ˆDOGAï¼‰æä¾›é€æ­¥çš„æˆ˜ç•¥æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAI-Salesmanåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚åŠè¯´åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07577', 'title': 'A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain', 'url': 'https://huggingface.co/papers/2511.07577', 'abstract': 'A decentralized retrieval-augmented generation system uses blockchain-based reliability scoring to manage data sources, enhancing performance and cost-efficiency compared to centralized systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '12720abff5645aae', 'authors': ['Yining Lu', 'Wenyi Tang', 'Max Johnson', 'Taeho Jung', 'Meng Jiang'], 'affiliations': ['Department of Computer Science and Engineering, University of Notre Dame, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07577.jpg', 'data': {'categories': ['#rag', '#open_source', '#small_models'], 'emoji': 'â›“ï¸', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° RAG Ñ Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½-Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (RAG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾ĞºÑ‡ĞµĞ¹Ğ½-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»Ğ¾Ğº Ğ±ĞµĞ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10.7% Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 56% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Decentralized RAG: Trust and Efficiency through Blockchain', 'desc': 'This paper presents a decentralized retrieval-augmented generation (RAG) system that leverages blockchain technology to enhance data source reliability. Unlike traditional centralized RAG systems, which face high costs and privacy issues, this approach allows data owners to maintain control over their information. The system introduces a reliability scoring mechanism that evaluates data sources based on the quality of their contributions, improving retrieval accuracy. The results show a significant performance boost and cost savings compared to centralized systems, demonstrating the effectiveness of decentralization in AI applications.'}, 'zh': {'title': 'å»ä¸­å¿ƒåŒ–RAGç³»ç»Ÿï¼šæå‡æ€§èƒ½ä¸æˆæœ¬æ•ˆç‡çš„åˆ›æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å»ä¸­å¿ƒåŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œåˆ©ç”¨åŒºå—é“¾æŠ€æœ¯å¯¹æ•°æ®æºè¿›è¡Œå¯é æ€§è¯„åˆ†ï¼Œä»è€Œæé«˜æ€§èƒ½å’Œæˆæœ¬æ•ˆç‡ã€‚ä¼ ç»Ÿçš„RAGç³»ç»Ÿé€šå¸¸é‡‡ç”¨é›†ä¸­å¼æ¶æ„ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†å’Œç®¡ç†æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å­˜åœ¨éšç§é—®é¢˜ã€‚æ–°ç³»ç»Ÿå…è®¸åŸºç¡€æ¨¡å‹ç›´æ¥ä»æ•°æ®æ‹¥æœ‰è€…é‚£é‡Œè·å–ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®æºçš„æ§åˆ¶æƒã€‚é€šè¿‡åŠ¨æ€è¯„ä¼°æ¯ä¸ªæ•°æ®æºçš„å“åº”è´¨é‡ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä¸å¯é çš„æ•°æ®ç¯å¢ƒä¸­å®ç°äº†è¶…è¿‡10.7%çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ç†æƒ³å¯é ç¯å¢ƒä¸­æ¥è¿‘é›†ä¸­å¼ç³»ç»Ÿçš„æ€§èƒ½ä¸Šé™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11510', 'title': 'OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning', 'url': 'https://huggingface.co/papers/2511.11510', 'abstract': "OpenUS, an open-source ultrasound foundation model, leverages a vision Mamba backbone and a novel self-adaptive masking framework to enhance pre-training and achieve label-efficient fine-tuning for diverse ultrasound imaging tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.", 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '1afbae4b92524e66', 'authors': ['Xiaoyu Zheng', 'Xu Chen', 'Awais Rauf', 'Qifan Fu', 'Benedetta Monosi', 'Felice Rivellese', 'Myles J. Lewis', 'Shaogang Gong', 'Gregory Slabaugh'], 'affiliations': ['Digital Environment Research Institute (DERI), Queen Mary University of London', 'School of Electronic Engineering and Computer Science, Queen Mary University of London', 'The William Harvey Research Institute, Queen Mary University of London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11510.jpg', 'data': {'categories': ['#optimization', '#training', '#science', '#open_source', '#dataset', '#architecture', '#cv', '#healthcare'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'OpenUS â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ foundational model Ğ´Ğ»Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Vision Mamba Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº self-adaptive masking, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ contrastive learning Ğ¸ masked image modeling Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· 308K ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. OpenUS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ label-efficient fine-tuning Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ²Ğ¾ĞµĞ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Ultrasound Imaging with OpenUS', 'desc': 'OpenUS is an innovative open-source foundation model designed for ultrasound imaging, utilizing a vision Mamba backbone to effectively capture both local and global features in images. It introduces a self-adaptive masking framework that enhances pre-training by integrating contrastive learning with masked image modeling, allowing for better feature extraction. The model is trained on a comprehensive dataset of over 308,000 ultrasound images, addressing the challenges of variability in image interpretation and limited annotations. OpenUS enables efficient fine-tuning for various ultrasound tasks, making it a valuable tool for improving AI applications in medical imaging.'}, 'zh': {'title': 'OpenUSï¼šå¼€æºè¶…å£°AIæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'OpenUSæ˜¯ä¸€ä¸ªå¼€æºçš„è¶…å£°åŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨è§†è§‰Mambaéª¨å¹²ç½‘ç»œå’Œæ–°é¢–çš„è‡ªé€‚åº”æ©è”½æ¡†æ¶æ¥å¢å¼ºé¢„è®­ç»ƒï¼Œå¹¶å®ç°å¤šæ ·åŒ–è¶…å£°æˆåƒä»»åŠ¡çš„æ ‡ç­¾é«˜æ•ˆå¾®è°ƒã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œæ©è”½å›¾åƒå»ºæ¨¡ï¼Œæå–ä¸°å¯Œçš„ç‰¹å¾ï¼Œé€‚åº”æ€§åœ°ä¼˜åŒ–ä¸´åºŠç›¸å…³çš„æ©è”½ï¼Œä»è€Œæé«˜é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚OpenUSè¿˜é‡‡ç”¨åŠ¨æ€å­¦ä¹ è®¡åˆ’ï¼Œé€æ­¥è°ƒæ•´é¢„è®­ç»ƒè¿‡ç¨‹çš„éš¾åº¦ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚é€šè¿‡æ±‡é›†è¶…è¿‡30.8ä¸‡å¼ å›¾åƒçš„å…¬å…±è¶…å£°æ•°æ®é›†ï¼ŒOpenUSä¸ºè¶…å£°AIæ¨¡å‹çš„å¼€å‘æä¾›äº†å¼ºå¤§çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11113', 'title': 'VIDEOP2R: Video Understanding from Perception to Reasoning', 'url': 'https://huggingface.co/papers/2511.11113', 'abstract': "VideoP2R, a process-aware reinforcement fine-tuning framework, improves video reasoning and understanding by modeling perception and reasoning separately, achieving state-of-the-art results on multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.", 'score': 111, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'c356d5771463f428', 'authors': ['Yifan Jiang', 'Yueying Wang', 'Rui Zhao', 'Toufiq Parag', 'Zhimin Chen', 'Zhenyu Liao', 'Jayakrishnan Unnikrishnan'], 'affiliations': ['Amazon', 'USC'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11113.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#benchmark', '#video', '#rl', '#dataset', '#rlhf', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'VideoP2R â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Video Reasoning with Process-Aware Fine-Tuning', 'desc': "VideoP2R is a new framework designed to enhance video reasoning and understanding by treating perception and reasoning as separate processes. It utilizes a two-stage approach called reinforcement fine-tuning (RFT), which includes supervised fine-tuning (SFT) followed by reinforcement learning (RL). In the SFT stage, a specialized dataset called VideoP2R-CoT-162K is created to improve the model's ability to think through video content. The RL stage employs a unique algorithm, PA-GRPO, that provides distinct rewards for perception and reasoning, leading to state-of-the-art performance on various benchmarks."}, 'zh': {'title': 'è§†é¢‘æ¨ç†çš„æ–°çªç ´ï¼šVideoP2Ræ¡†æ¶', 'desc': 'VideoP2Ræ˜¯ä¸€ä¸ªè¿‡ç¨‹æ„ŸçŸ¥çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ„ŸçŸ¥å’Œæ¨ç†å»ºæ¨¡ä¸ºç‹¬ç«‹çš„è¿‡ç¨‹æ¥æé«˜è§†é¢‘æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå¹¶åœ¨SFTé˜¶æ®µç”Ÿæˆé«˜è´¨é‡çš„è¿‡ç¨‹æ„ŸçŸ¥é“¾å¼æ€ç»´æ•°æ®é›†ã€‚RLé˜¶æ®µå¼•å…¥äº†ä¸€ç§æ–°çš„è¿‡ç¨‹æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¸ºæ„ŸçŸ¥å’Œæ¨ç†æä¾›ç‹¬ç«‹çš„å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoP2Råœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.08577', 'title': 'Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models', 'url': 'https://huggingface.co/papers/2511.08577', 'abstract': 'Think-at-Hard (TaH) dynamically refines only hard tokens in LLMs using a neural decider and LoRA, improving reasoning performance with minimal additional parameters or iterations.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.', 'score': 104, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'b4583d31245848fa', 'authors': ['Tianyu Fu', 'Yichen You', 'Zekai Chen', 'Guohao Dai', 'Huazhong Yang', 'Yu Wang'], 'affiliations': ['Infinigence AI', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.08577.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#reasoning', '#architecture', '#inference'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Think-at-Hard Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸ÑÂ» - ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ğ½Ğ¸Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8-12% Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ 94% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Refining Hard Tokens for Smarter LLMs', 'desc': "The paper introduces Think-at-Hard (TaH), a method that enhances the reasoning abilities of Large Language Models (LLMs) by focusing on difficult tokens during the prediction process. Instead of refining all tokens, TaH uses a neural decider to identify and iterate only on those tokens that are likely to be incorrect after the initial pass. This approach minimizes unnecessary revisions on easy tokens, which can lead to errors, and employs Low-Rank Adaptation (LoRA) to adjust the model's focus during these iterations. The results demonstrate significant accuracy improvements on various benchmarks while keeping the number of parameters constant, showcasing TaH's efficiency in enhancing LLM performance."}, 'zh': {'title': 'åŠ¨æ€ä¼˜åŒ–éš¾è¯å…ƒï¼Œæé«˜æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºThink-at-Hard (TaH) çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»å†³ç­–å™¨ï¼Œä»…å¯¹éš¾ä»¥é¢„æµ‹çš„è¯å…ƒè¿›è¡ŒåŠ¨æ€è¿­ä»£ï¼Œä»è€Œé¿å…äº†å¯¹ç®€å•è¯å…ƒçš„è¿‡åº¦æ€è€ƒã€‚ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—ï¼ŒTaHå°†æ¨¡å‹çš„ç›®æ ‡ä»ä¸€èˆ¬çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹è½¬å‘ä¸“æ³¨äºéš¾è¯å…ƒçš„ç²¾ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTaHåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•°é‡ä¸å˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14295', 'title': 'AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models', 'url': 'https://huggingface.co/papers/2511.14295', 'abstract': "AraLingBench evaluates Arabic and bilingual LLMs' linguistic competence using a benchmark with expert-designed questions across grammar, morphology, spelling, reading comprehension, and syntax, revealing gaps between surface proficiency and true comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.", 'score': 71, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '37807f1afeb1fa25', 'authors': ['Mohammad Zbeeb', 'Hasan Abed Al Kader Hammoud', 'Sina Mukalled', 'Nadine Rizk', 'Fatima Karnib', 'Issam Lakkis', 'Ammar Mohanna', 'Bernard Ghanem'], 'affiliations': ['American University of Beirut (AUB)', 'King Abdullah University of Science and Technology (KAUST)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14295.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#reasoning', '#low_resource', '#multilingual'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AraLingBench â€” Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ğ¾Ñ€Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 150 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞÑ†ĞµĞ½ĞºĞ° 35 Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼. AraLingBench Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ»Ğ°Ğ¼Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Bridging the Gap in Arabic Linguistic Mastery', 'desc': 'AraLingBench is a benchmark designed to assess the linguistic competence of Arabic and bilingual large language models (LLMs). It includes 150 expert-crafted multiple choice questions that evaluate key areas such as grammar, morphology, spelling, reading comprehension, and syntax. The evaluation of 35 LLMs shows that while these models perform well on surface-level tasks, they often lack deeper understanding of grammatical and syntactic structures. This highlights a significant gap between high performance on standard tests and true linguistic mastery, indicating that many models rely on memorization rather than genuine comprehension.'}, 'zh': {'title': 'è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•', 'desc': 'AraLingBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯è¯­å’ŒåŒè¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†è¯­æ³•ã€å½¢æ€å­¦ã€æ‹¼å†™ã€é˜…è¯»ç†è§£å’Œå¥æ³•äº”ä¸ªæ ¸å¿ƒé¢†åŸŸï¼Œé€šè¿‡150ä¸ªä¸“å®¶è®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ç›´æ¥è¯„ä¼°è¯­è¨€ç†è§£çš„ç»“æ„æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨è¡¨é¢æ°´å¹³ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´æ·±å±‚æ¬¡çš„è¯­æ³•å’Œå¥æ³•æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚AraLingBenchæ­ç¤ºäº†çŸ¥è¯†æ€§åŸºå‡†æµ‹è¯•é«˜åˆ†ä¸çœŸæ­£è¯­è¨€æŒæ¡ä¹‹é—´çš„å·®è·ï¼Œå¼ºè°ƒäº†è®¸å¤šæ¨¡å‹ä¾èµ–è®°å¿†æˆ–æ¨¡å¼è¯†åˆ«è€ŒéçœŸå®ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.10555', 'title': 'A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space', 'url': 'https://huggingface.co/papers/2511.10555', 'abstract': 'A novel method, CoTyle, generates images in consistent visual styles using unique numerical style codes, filling an academic gap in code-to-style image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.', 'score': 60, 'issue_id': 1, 'pub_date': '2025-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '7c29f7b93138de5e', 'authors': ['Huijie Liu', 'Shuhao Cui', 'Haoxiang Cao', 'Shuai Ma', 'Kai Wu', 'Guoliang Kang'], 'affiliations': ['Beihang University', 'Kolors Team, Kuaishou Technology', 'South China Normal University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.10555.jpg', 'data': {'categories': ['#multimodal', '#cv', '#diffusion', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ´Ğ¸Ğ½ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ â€” Ğ¾Ğ´Ğ¸Ğ½ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ', 'desc': 'CoTyle â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ÑÑ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ±ÑƒĞº ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. ĞŸÑ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ÑÑ Ğ² ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ğµ.'}, 'en': {'title': 'Transforming Numbers into Unique Visual Styles', 'desc': 'The paper introduces CoTyle, a novel method for generating images with consistent visual styles using unique numerical style codes. This approach addresses the challenges of existing generative methods that rely on complex prompts and often fail to maintain style consistency. CoTyle utilizes a discrete style codebook to extract style embeddings, which are then used to condition a text-to-image diffusion model for image generation. The method simplifies the process of style generation, allowing for a diverse range of reproducible styles from minimal input, thus filling a significant gap in the academic research of code-to-style image generation.'}, 'zh': {'title': 'ç”¨ä¸€ä¸ªä»£ç å®šä¹‰é£æ ¼çš„é©å‘½æ€§æ–¹æ³•', 'desc': 'CoTyleæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç‰¹çš„æ•°å€¼é£æ ¼ä»£ç ç”Ÿæˆä¸€è‡´çš„è§†è§‰é£æ ¼å›¾åƒï¼Œå¡«è¡¥äº†ä»£ç åˆ°é£æ ¼å›¾åƒç”Ÿæˆçš„å­¦æœ¯ç©ºç™½ã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–æ•°å€¼é£æ ¼ä»£ç ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°é¢–ä¸”ä¸€è‡´çš„è§†è§‰é£æ ¼ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨é£æ ¼ä¸€è‡´æ€§å’Œåˆ›é€ åŠ›æ–¹é¢çš„å±€é™ã€‚æˆ‘ä»¬é¦–å…ˆä»å›¾åƒé›†åˆä¸­è®­ç»ƒç¦»æ•£é£æ ¼ä»£ç æœ¬ï¼Œä»¥æå–é£æ ¼åµŒå…¥ï¼Œç„¶ååˆ©ç”¨è¿™äº›åµŒå…¥æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆé£æ ¼å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoTyleæœ‰æ•ˆåœ°å°†æ•°å€¼ä»£ç è½¬åŒ–ä¸ºé£æ ¼æ§åˆ¶å™¨ï¼Œè¯æ˜äº†é£æ ¼çš„ç¡®å¯ä»¥ç”¨ä¸€ä¸ªä»£ç æ¥è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13189', 'title': 'Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework', 'url': 'https://huggingface.co/papers/2511.13189', 'abstract': "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.", 'score': 38, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '332f65259461354a', 'authors': ['Diego Ortego', 'Marlon RodrÃ­guez', 'Mario Almagro', 'Kunal Dahiya', 'David JimÃ©nez', 'Juan C. SanMiguel'], 'affiliations': ['IIT Delhi', 'NielsenIQ', 'Universidad AutÃ³noma de Madrid (UAM)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13189.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#open_source', '#dataset'], 'emoji': 'ğŸ”¤', 'ru': {'title': 'Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… foundation models Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (XMC), Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Only Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ViXML â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· foundation vision Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking the Power of Vision in Extreme Multi-label Classification', 'desc': 'This paper explores the use of foundation models in Extreme Multi-label Classification (XMC), which involves associating queries with relevant labels from very large label sets. The authors propose a new framework called Vision-enhanced eXtreme Multi-label Learning (ViXML) that combines the strengths of larger decoder-only models and visual information to improve classification performance. They demonstrate that using a few billion parameters in the decoder can significantly enhance results while keeping computational costs low. Their experiments show that ViXML outperforms traditional text-only models, highlighting the importance of integrating visual data in multi-label classification tasks.'}, 'zh': {'title': 'è§†è§‰å¢å¼ºæç«¯å¤šæ ‡ç­¾å­¦ä¹ çš„çªç ´', 'desc': 'åŸºç¡€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†åœ¨æç«¯å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆXMCï¼‰ä¸­çš„æ½œåŠ›å°šæœªè¢«å……åˆ†åˆ©ç”¨ã€‚XMCä¸­çš„æŸ¥è¯¢ä¸æ¥è‡ªæå¤§æ ‡ç­¾ç©ºé—´çš„ç›¸å…³æ ‡ç­¾ç›¸å…³è”ï¼Œå› æ­¤åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ›´å¤§çš„è§£ç å™¨æ¨¡å‹ä»¥åŠå¦‚ä½•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºçš„è§†è§‰å¢å¼ºæç«¯å¤šæ ‡ç­¾å­¦ä¹ æ¡†æ¶ï¼ˆViXMLï¼‰é€šè¿‡å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œå•ä¸€åµŒå…¥æ± åŒ–ï¼ŒæˆåŠŸæ•´åˆäº†åŸºç¡€è§†è§‰æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13853', 'title': 'Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark', 'url': 'https://huggingface.co/papers/2511.13853', 'abstract': 'Gen-ViRe benchmarks video models on reasoning abilities using a framework that decomposes Chain-of-Frames reasoning into cognitive dimensions and subtasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.', 'score': 34, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '538a88e47dec5dca', 'authors': ['Xinxin Liu', 'Zhaopan Xu', 'Ming Li', 'Kai Wang', 'Yong Jae Lee', 'Yuzhang Shang'], 'affiliations': ['National University of Singapore', 'UW-Madison', 'University of Central Florida'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13853.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ', 'desc': 'Gen-ViRe â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Chain-of-Frames, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑˆĞµÑÑ‚ÑŒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ 24 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ VLM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Gen-ViRe: Benchmarking Video Models for Real-World Reasoning', 'desc': 'The paper introduces Gen-ViRe, a benchmark designed to evaluate video models based on their reasoning abilities through a framework that breaks down Chain-of-Frames (CoF) reasoning into cognitive dimensions and subtasks. Unlike traditional benchmarks that focus on visual fidelity, Gen-ViRe assesses core cognitive skills such as multi-step planning and abstract reasoning. This framework is informed by cognitive science and aims to provide a systematic understanding of model capabilities, highlighting the gap between visual quality and reasoning depth. The results from testing state-of-the-art systems reveal significant differences in reasoning performance, paving the way for improved world simulation models.'}, 'zh': {'title': 'Gen-ViReï¼šè§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'Gen-ViReæ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†æ¡†æ¶ï¼Œå®ƒå°†é“¾å¸§æ¨ç†åˆ†è§£ä¸ºè®¤çŸ¥ç»´åº¦å’Œå­ä»»åŠ¡ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†åœ¨å¤šæ­¥è§„åˆ’ã€ç®—æ³•é€»è¾‘å’ŒæŠ½è±¡æ¨¡å¼æ¨æ–­ç­‰æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚é€šè¿‡å¤šæºæ•°æ®æ•´ç†å’Œæ··åˆè§†è§‰è¯­è¨€æ¨¡å‹è¾…åŠ©è¯„ä¼°ï¼ŒGen-ViReæä¾›äº†è§†é¢‘æ¨¡å‹ä½œä¸ºæ¨ç†è€…çš„é¦–æ¬¡å®šé‡è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è§†è§‰è´¨é‡ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®é™…æ¨ç†æ·±åº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸ºçœŸæ­£çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13026', 'title': 'REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding', 'url': 'https://huggingface.co/papers/2511.13026', 'abstract': "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.", 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '7891552879d1cdc9', 'authors': ['Jiaze Li', 'Hao Yin', 'Wenhui Tan', 'Jingyang Chen', 'Boshen Xu', 'Yuxun Qu', 'Yijing Chen', 'Jianzhong Ju', 'Zhenbo Luo', 'Jian Luan'], 'affiliations': ['Independent Researcher', 'MiLM Plus, Xiaomi Inc.', 'Renmin University of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13026.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#benchmark', '#video', '#rl', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ REVISOR â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (MLLM) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Dual Attribution Decoupled Reward (DADR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (GRPO) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Video Understanding with Multimodal Reflection', 'desc': "This paper introduces REVISOR, a new framework designed to improve long-form video understanding by integrating visual and textual reflection processes. Traditional text-based self-reflection methods struggle with the dynamic nature of video content, as they do not adequately process visual information. REVISOR addresses this by enabling multimodal large language models (MLLMs) to collaboratively reflect on both text and video, enhancing their reasoning capabilities. The framework employs a unique Dual Attribution Decoupled Reward (DADR) mechanism to ensure that the model's reasoning aligns with relevant video evidence, achieving strong performance across multiple benchmarks without needing additional supervised training."}, 'zh': {'title': 'REVISORï¼šæå‡é•¿è§†é¢‘ç†è§£çš„æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºREVISORçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€åæ€èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬åæ€æœºåˆ¶åœ¨å¤„ç†åŠ¨æ€è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œå› æ­¤REVISORå¼•å…¥äº†è·¨æ¨¡æ€çš„åæ€è¿‡ç¨‹ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†ã€‚é€šè¿‡è®¾è®¡åŒé‡å½’å› è§£è€¦å¥–åŠ±æœºåˆ¶ï¼ˆDADRï¼‰ï¼Œè¯¥æ¡†æ¶ç¡®ä¿æ¨¡å‹çš„æ¨ç†ä¸æ‰€é€‰è§†é¢‘è¯æ®ä¹‹é—´çš„å› æœå¯¹é½ã€‚REVISORåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£çš„èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„ç›‘ç£å¾®è°ƒæˆ–å¤–éƒ¨æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14159', 'title': 'MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs', 'url': 'https://huggingface.co/papers/2511.14159', 'abstract': 'MVI-Bench evaluates the robustness of Large Vision-Language Models against misleading visual inputs using a hierarchical taxonomy and a novel sensitivity metric, revealing significant vulnerabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.', 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'be7a75bd7662179a', 'authors': ['Huiyi Chen', 'Jiawei Peng', 'Dehai Min', 'Changchang Sun', 'Kaijie Chen', 'Yan Yan', 'Xu Yang', 'Lu Cheng'], 'affiliations': ['Department of Computer Science, University of Illinois at Chicago', 'Guohao School, Tongji University', 'School of Computer Science & Engineering, Southeast University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14159.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#benchmark', '#hallucinations', '#dataset', '#cv', '#security'], 'emoji': 'ğŸš¨', 'ru': {'title': 'Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MVI-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ñ 1248 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ MVI-Sensitivity Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 18 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Strengthening LVLMs Against Visual Deception', 'desc': "MVI-Bench is a new benchmark designed to test how well Large Vision-Language Models (LVLMs) handle misleading visual inputs. It introduces a hierarchical taxonomy that categorizes misleading visuals into three levels: Visual Concept, Visual Attribute, and Visual Relationship. The benchmark includes 1,248 annotated visual question-answering instances to assess the models' robustness. A new metric called MVI-Sensitivity is also introduced to evaluate the models' vulnerabilities in detail, revealing significant weaknesses in current LVLMs when faced with misleading visuals."}, 'zh': {'title': 'è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§æ–°åŸºå‡†', 'desc': 'MVI-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å¯¹è¯¯å¯¼æ€§è§†è§‰è¾“å…¥é²æ£’æ€§çš„æ–°åŸºå‡†ã€‚å®ƒé€šè¿‡å»ºç«‹ä¸€ä¸ªåˆ†å±‚åˆ†ç±»æ³•ï¼Œå…³æ³¨è§†è§‰æ¦‚å¿µã€è§†è§‰å±æ€§å’Œè§†è§‰å…³ç³»ä¸‰ä¸ªå±‚æ¬¡ï¼Œæ­ç¤ºäº†LVLMåœ¨é¢å¯¹è¯¯å¯¼æ€§è§†è§‰è¾“å…¥æ—¶çš„æ˜¾è‘—è„†å¼±æ€§ã€‚è¯¥åŸºå‡†åŒ…å«1248ä¸ªç»è¿‡ä¸“å®¶æ³¨é‡Šçš„è§†è§‰é—®ç­”å®ä¾‹ï¼Œå¹¶å¼•å…¥äº†MVI-Sensitivityè¿™ä¸€æ–°é¢–çš„é²æ£’æ€§è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹18ä¸ªæœ€å…ˆè¿›çš„LVLMè¿›è¡Œå®è¯åˆ†æï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†è¯¯å¯¼æ€§è§†è§‰è¾“å…¥æ—¶å­˜åœ¨æ˜æ˜¾çš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14210', 'title': 'Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution', 'url': 'https://huggingface.co/papers/2511.14210', 'abstract': 'Orion, a visual agent framework, uses a suite of specialized computer vision tools to execute complex visual workflows, achieving competitive performance on multiple benchmarks and enabling autonomous visual reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.', 'score': 19, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'f0bf227b9c4ab079', 'authors': ['N Dinesh Reddy', 'Dylan Snyder', 'Lona Kiragu', 'Mirajul Mohin', 'Shahrear Bin Amin', 'Sudeep Pillai'], 'affiliations': ['VLM Run'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14210.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark', '#agents'], 'emoji': 'ğŸ”§', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚', 'desc': 'Orion â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Orion ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ°Ğ½Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Orion: Empowering Visual Intelligence through Tool-Driven Reasoning', 'desc': 'Orion is a visual agent framework that integrates various computer vision tools to perform complex visual tasks. It stands out by using an agentic approach that allows it to call multiple tools for executing workflows, rather than just generating descriptive outputs like traditional models. Orion achieves state-of-the-art performance on several benchmarks, demonstrating its effectiveness in visual AI applications. By merging neural perception with symbolic execution, it enables a shift towards autonomous visual reasoning, enhancing the capabilities of visual intelligence systems.'}, 'zh': {'title': 'Orionï¼šä¸»åŠ¨è§†è§‰æ¨ç†çš„æ–°çºªå…ƒ', 'desc': 'Orionæ˜¯ä¸€ä¸ªè§†è§‰ä»£ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ã€‚å®ƒç»“åˆäº†å¤šç§è®¡ç®—æœºè§†è§‰å·¥å…·ï¼Œæ‰§è¡Œå¤æ‚çš„è§†è§‰å·¥ä½œæµç¨‹ï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒOrioné€šè¿‡è°ƒç”¨å·¥å…·å®ç°ä¸»åŠ¨çš„è§†è§‰æ¨ç†ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç«äº‰åŠ›ï¼Œæ¨åŠ¨äº†è§†è§‰æ™ºèƒ½çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14582', 'title': 'OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models', 'url': 'https://huggingface.co/papers/2511.14582', 'abstract': 'OmniZip is a training-free framework that compresses audio-visual tokens by dynamically pruning video tokens based on audio retention scores, achieving significant inference speedup and memory reduction without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '82d8f11a42e65eb9', 'authors': ['Keda Tao', 'Kele Shao', 'Bohan Yu', 'Weiqiang Wang', 'Jian liu', 'Huan Wang'], 'affiliations': ['Ant Group', 'Shanghai Innovation Institute', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14582.jpg', 'data': {'categories': ['#multimodal', '#inference', '#video', '#audio'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾', 'desc': 'OmniZip â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 3.42 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 1.4 Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Dynamic Token Pruning for Fast and Efficient Audio-Visual Processing', 'desc': 'OmniZip is a novel framework designed to compress audio-visual tokens without the need for training. It works by evaluating audio retention scores to determine which video tokens can be pruned, thus optimizing the representation of multimodal data. This method significantly speeds up inference and reduces memory usage while preserving the essential information from audio cues. The results show that OmniZip can achieve over three times faster inference and nearly 1.5 times less memory usage compared to existing methods, all without any training involved.'}, 'zh': {'title': 'OmniZipï¼šæ— è®­ç»ƒçš„éŸ³è§†é¢‘æ ‡è®°å‹ç¼©æ¡†æ¶', 'desc': 'OmniZipæ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡æ ¹æ®éŸ³é¢‘ä¿ç•™åˆ†æ•°åŠ¨æ€ä¿®å‰ªè§†é¢‘æ ‡è®°æ¥å‹ç¼©éŸ³è§†é¢‘æ ‡è®°ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿå’Œå†…å­˜å‡å°‘ï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–å…ˆè¯†åˆ«é‡è¦çš„éŸ³é¢‘æ ‡è®°ï¼Œç„¶åè®¡ç®—æ¯ä¸ªæ—¶é—´ç»„çš„éŸ³é¢‘ä¿ç•™åˆ†æ•°ï¼Œä»¥æ•æ‰ä¿¡æ¯å¯†åº¦ï¼Œä»è€ŒåŠ¨æ€æŒ‡å¯¼è§†é¢‘æ ‡è®°çš„ä¿®å‰ªã€‚OmniZipé‡‡ç”¨äº¤é”™çš„æ—¶ç©ºæ–¹æ¡ˆå‹ç¼©è§†é¢‘æ ‡è®°ï¼Œä¿ç•™ç”±éŸ³é¢‘é”šç‚¹å¢å¼ºçš„çº¿ç´¢ã€‚å®éªŒè¯æ˜ï¼ŒOmniZipåœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†3.42å€ï¼Œå†…å­˜å‡å°‘äº†1.4å€ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼Œæ— éœ€è®­ç»ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14460', 'title': 'Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning', 'url': 'https://huggingface.co/papers/2511.14460', 'abstract': 'A new training framework for RL-based LLM Agents is introduced, extending MDP methodology and demonstrating effectiveness on Multihop QA tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '3a15f7d47e3a2c9f', 'authors': ['Mingyue Cheng', 'Jie Ouyang', 'Shuo Yu', 'Ruiran Yan', 'Yucong Luo', 'Zirui Liu', 'Daoyu Wang', 'Qi Liu', 'Enhong Chen'], 'affiliations': ['State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14460.jpg', 'data': {'categories': ['#benchmark', '#training', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-R1 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (MDP) Ğ´Ğ»Ñ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ RL-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (Multihop QA).'}, 'en': {'title': 'Empowering LLM Agents with Reinforcement Learning: Introducing Agent-R1', 'desc': 'This paper presents a new training framework for Reinforcement Learning (RL) applied to Large Language Model (LLM) Agents, enhancing the traditional Markov Decision Process (MDP) methodology. It addresses the challenges faced in training LLM Agents for complex tasks, particularly in active environments where they can interact and use tools. The proposed framework, named Agent-R1, is modular and adaptable, allowing for easy customization across various tasks and scenarios. Initial experiments on Multihop Question Answering tasks demonstrate the effectiveness of this approach, paving the way for further advancements in RL for LLM Agents.'}, 'zh': {'title': 'ä¸ºRLä»£ç†æ‰“é€ çµæ´»çš„è®­ç»ƒæ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è€Œè®¾è®¡ï¼Œæ‰©å±•äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„æ–¹æ³•è®ºã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å½“å‰RLåœ¨LLMä»£ç†åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›çµæ´»ã€å¯æ‰©å±•çš„è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡ç³»ç»Ÿåœ°å®šä¹‰LLMä»£ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œæœ¬æ–‡ä¸ºè¯¥é¢†åŸŸçš„æ·±å…¥æ¢ç´¢å¥ å®šäº†åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šå…·æœ‰æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14366', 'title': 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning', 'url': 'https://huggingface.co/papers/2511.14366', 'abstract': 'ATLAS, a large-scale, cross-disciplinary evaluation suite, addresses the limitations of existing benchmarks by providing high-difficulty, original, and high-fidelity scientific problems to assess the reasoning capabilities of Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models\' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS\'s effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '63ee4433c7a942e8', 'authors': ['Hongwei Liu', 'Junnan Liu', 'Shudong Liu', 'Haodong Duan', 'Yuqiang Li', 'Mao Su', 'Xiaohong Liu', 'Guangtao Zhai', 'Xinyu Fang', 'Qianhong Ma', 'Taolin Zhang', 'Zihan Ma', 'Yufeng Zhao', 'Peiheng Zhou', 'Linchen Xiao', 'Wenlong Zhang', 'Shijie Zhou', 'Xingjian Ma', 'Siqi Sun', 'Jiaye Ge', 'Meng Li', 'Yuhong Liu', 'Jianxin Dong', 'Jiaying Li', 'Hui Wu', 'Hanwen Liang', 'Jintai Lin', 'Yanting Wang', 'Jie Dong', 'Tong Zhu', 'Tianfan Fu', 'Conghui He', 'Qi Zhang', 'Songyang Zhang', 'Lei Bai', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14366.jpg', 'data': {'categories': ['#science', '#open_source', '#leakage', '#reasoning', '#agi'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI', 'desc': 'ATLAS â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¾ĞºĞ¾Ğ»Ğ¾ 800 Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑĞµĞ¼ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ PhD-ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'ATLAS: A New Benchmark for Testing Scientific Reasoning in AI', 'desc': 'ATLAS is a new evaluation suite designed to test the reasoning abilities of Large Language Models (LLMs) using challenging and original scientific problems. It addresses the shortcomings of existing benchmarks by offering high-difficulty questions across multiple scientific disciplines, ensuring that models can integrate knowledge and reason effectively. The suite includes rigorous quality control measures to maintain the integrity and complexity of the questions, which require multi-step reasoning and detailed answers. Preliminary results indicate that ATLAS can effectively differentiate the advanced reasoning capabilities of leading LLMs, paving the way for a more reliable assessment of progress towards Artificial General Intelligence.'}, 'zh': {'title': 'ATLASï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ ‡å°º', 'desc': 'ATLASæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è·¨å­¦ç§‘è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚å®ƒæä¾›äº†çº¦800ä¸ªé«˜éš¾åº¦ã€åŸåˆ›æ€§å¼ºçš„ç§‘å­¦é—®é¢˜ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ATLASæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€è®¡ç®—æœºç§‘å­¦ã€åœ°çƒç§‘å­¦å’Œææ–™ç§‘å­¦ç­‰ä¸ƒä¸ªæ ¸å¿ƒç§‘å­¦é¢†åŸŸï¼Œå¼ºè°ƒè·¨å­¦ç§‘çŸ¥è¯†çš„æ•´åˆå’Œå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚è¯¥å¹³å°è¿˜é‡‡ç”¨ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶å’Œä¸“å®¶è¯„å®¡ï¼Œç¡®ä¿é—®é¢˜çš„ç§‘å­¦ä»·å€¼å’Œæ­£ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11270', 'title': 'Î¦eat: Physically-Grounded Feature Representation', 'url': 'https://huggingface.co/papers/2511.11270', 'abstract': 'A physically-grounded visual backbone, $Î¦$eat, is introduced to capture material identity through self-supervised training, demonstrating robust features invariant to external physical factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce Î¦eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that Î¦eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '6be7995006bc181a', 'authors': ['Giuseppe Vecchio', 'Adrien Kaiser', 'Rouffet Romain', 'Rosalie Martin', 'Elena Garces', 'Tamy Boubekeur'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11270.jpg', 'data': {'categories': ['#cv', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Î¦eat, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¾Ğ¿Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… foundation models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ (Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼), Î¦eat Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼.'}, 'en': {'title': 'Revolutionizing Material Recognition with Self-Supervised Learning', 'desc': 'This paper presents Î¦eat, a new visual backbone designed to understand material identity through self-supervised learning. Unlike traditional models that mix high-level concepts with low-level physical details, Î¦eat focuses on capturing features that remain consistent despite changes in geometry and lighting. The authors use a pretraining method that contrasts different views and physical variations of the same material, allowing the model to learn robust representations without needing labeled data. The results show that Î¦eat effectively identifies physical properties, paving the way for better physics-aware applications in vision and graphics.'}, 'zh': {'title': 'ç‰©ç†æ„ŸçŸ¥çš„æ–°åŸºç¡€ï¼šÎ¦eatè§†è§‰éª¨å¹²ç½‘ç»œ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºÎ¦eatçš„ç‰©ç†åŸºç¡€è§†è§‰éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒæ•æ‰ææ–™ç‰¹æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒÎ¦eatèƒ½å¤Ÿæå–å¯¹å¤–éƒ¨ç‰©ç†å› ç´ ä¸æ•æ„Ÿçš„å¼ºç‰¹å¾ï¼Œé¿å…äº†é«˜å±‚è¯­ä¹‰ä¸ä½å±‚ç‰©ç†å› ç´ çš„çº ç¼ ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ç›¸åŒææ–™åœ¨ä¸åŒå½¢çŠ¶å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„ç©ºé—´è£å‰ªå’Œç‰©ç†å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ¦eatåœ¨ç‰©ç†ç‰¹å¾å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºè§†è§‰å’Œå›¾å½¢ä¸­çš„ç‰©ç†æ„ŸçŸ¥å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14385', 'title': 'Mitigating Label Length Bias in Large Language Models', 'url': 'https://huggingface.co/papers/2511.14385', 'abstract': 'Normalized contextual calibration addresses label length bias in large language models, improving performance and reliability across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '887b34bedbc803db', 'authors': ['Mario Sanz-Guerrero', 'Katharina von der Wense'], 'affiliations': ['Johannes Gutenberg University Mainz, Germany', 'University of Colorado Boulder, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14385.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ (NCC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. NCC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Mitigating Label Length Bias for Better Language Model Performance', 'desc': 'This paper introduces a method called normalized contextual calibration (NCC) to address label length bias in large language models (LLMs). Label length bias occurs when LLMs inconsistently treat labels of varying lengths, which can lead to inaccurate predictions. NCC normalizes and calibrates predictions at the full-label level, resulting in significant performance improvements across various tasks and datasets. The method enhances the reliability of LLMs, especially in scenarios like multiple-choice question answering, by providing better confidence estimates and requiring fewer examples for effective learning.'}, 'zh': {'title': 'å½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¯é æ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆNCCï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ ‡ç­¾é•¿åº¦åå·®é—®é¢˜ã€‚æ ‡ç­¾é•¿åº¦åå·®æ˜¯æŒ‡åœ¨å¤„ç†ä¸åŒé•¿åº¦çš„æ ‡ç­¾æ—¶ï¼Œæ¨¡å‹çš„é¢„æµ‹è¡¨ç°ä¸ä¸€è‡´ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚NCCé€šè¿‡åœ¨å®Œæ•´æ ‡ç­¾çº§åˆ«ä¸Šè¿›è¡Œå½’ä¸€åŒ–å’Œæ ¡å‡†ï¼Œæ˜¾è‘—æé«˜äº†å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæå‡å¹…åº¦å¯è¾¾10%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒNCCè¿˜æ‰©å±•äº†åå·®ç¼“è§£çš„åº”ç”¨èŒƒå›´ï¼Œé€‚ç”¨äºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ç­‰æ›´å¹¿æ³›çš„ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11473', 'title': 'Proactive Hearing Assistants that Isolate Egocentric Conversations', 'url': 'https://huggingface.co/papers/2511.11473', 'abstract': "A proactive hearing assistant system identifies and separates conversation partners in real-time using a dual-model architecture on binaural audio, adapting to conversational dynamics without explicit prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '2369b0421f03af73', 'authors': ['Guilin Hu', 'Malek Itani', 'Tuochao Chen', 'Shyamnath Gollakota'], 'affiliations': ['Paul G. Allen School of Computer Science & Engineering, University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11473.jpg', 'data': {'categories': [], 'emoji': 'ğŸ‘‚', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ»ÑƒÑ…Ğ°, ÑƒĞ¼ĞµÑÑ‰Ğ¸Ğ¹ ÑƒĞ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ ĞºĞ°Ğº Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ», Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑÑ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ğ¼ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ğ¼.'}, 'en': {'title': 'Proactive Hearing Assistants: Real-Time Speaker Separation', 'desc': "This paper presents a proactive hearing assistant system that can identify and separate conversation partners in real-time using binaural audio. The system employs a dual-model architecture, where a lightweight streaming model processes audio every 12.5 ms for quick identification, while a slower model analyzes longer conversational patterns. By using the wearer's own speech as a reference, the system effectively distinguishes between different speakers and suppresses background noise. The results demonstrate the system's ability to generalize across various real-world conversation scenarios, enhancing the user experience in multi-speaker environments."}, 'zh': {'title': 'ä¸»åŠ¨é€‚åº”å¯¹è¯çš„å¬åŠ›åŠ©æ‰‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸»åŠ¨å¬åŠ›åŠ©æ‰‹ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«å’Œåˆ†ç¦»å¯¹è¯ä¼™ä¼´ï¼Œè€Œæ— éœ€æ˜ç¡®çš„æç¤ºã€‚è¯¥ç³»ç»ŸåŸºäºè‡ªæˆ‘ä¸­å¿ƒçš„åŒè€³éŸ³é¢‘ï¼Œåˆ©ç”¨ä½©æˆ´è€…çš„è‡ªæˆ‘å‘è¨€ä½œä¸ºé”šç‚¹ï¼Œæ¨æ–­å¯¹è¯ä¼™ä¼´å¹¶æŠ‘åˆ¶å…¶ä»–å£°éŸ³ã€‚ä¸ºäº†å®ç°å®æ—¶çš„è®¾å¤‡æ“ä½œï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŒæ¨¡å‹æ¶æ„ï¼šä¸€ä¸ªè½»é‡çº§çš„æµå¼æ¨¡å‹æ¯12.5æ¯«ç§’è¿è¡Œä¸€æ¬¡ï¼Œä»¥ä½å»¶è¿Ÿæå–å¯¹è¯ä¼™ä¼´ï¼Œè€Œä¸€ä¸ªè¾ƒæ…¢çš„æ¨¡å‹åˆ™ä¸é‚£ä¹ˆé¢‘ç¹åœ°è¿è¡Œï¼Œä»¥æ•æ‰æ›´é•¿æ—¶é—´èŒƒå›´çš„å¯¹è¯åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šå¯¹è¯åœºæ™¯ä¸­èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œéš”ç¦»å¯¹è¯ä¼™ä¼´ï¼Œæ ‡å¿—ç€ä¸»åŠ¨é€‚åº”å¯¹è¯åŠ¨æ€çš„å¬åŠ›åŠ©æ‰‹çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12884', 'title': 'Agent READMEs: An Empirical Study of Context Files for Agentic Coding', 'url': 'https://huggingface.co/papers/2511.12884', 'abstract': 'Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '4ac888750cbe8e61', 'authors': ['Worawalan Chatlatanagulchai', 'Hao Li', 'Yutaro Kashiwa', 'Brittany Reid', 'Kundjanasith Thonglek', 'Pattara Leelaprute', 'Arnon Rungsawang', 'Bundit Manaskasemsak', 'Bram Adams', 'Ahmed E. Hassan', 'Hajimu Iida'], 'affiliations': ['Faculty of Engineering, Kasetsart University', 'Nara Institute of Science and Technology', 'Queens University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12884.jpg', 'data': {'categories': ['#open_source', '#dataset', '#agents', '#plp', '#security'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°), Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ 2303 Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸Ğ· 1925 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ´Ñƒ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ: ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° (62,3%), Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (69,9%) Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (67,7%). Ğ’Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ»Ğ°ĞºÑƒĞ½Ğ°: Ğ½ĞµÑ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ (14,5%) Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (14,5%), Ñ€ĞµĞ´ĞºĞ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Enhancing Agent Context for Safer Code Generation', 'desc': 'This paper explores the role of agent context files, which serve as detailed instructions for coding agents that convert natural language goals into executable code. The study analyzes 2,303 context files from various repositories, revealing that these files are dynamic and evolve over time, similar to configuration code. The analysis shows that developers focus heavily on functional aspects, such as build commands and implementation details, while neglecting important non-functional requirements like security and performance. The findings suggest a need for better tools and practices to enhance the quality and safety of code generated by these agents.'}, 'zh': {'title': 'æå‡ä»£ç†ä»£ç å®‰å…¨æ€§ä¸æ€§èƒ½çš„å¿…è¦æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ä»£ç†ä¸Šä¸‹æ–‡æ–‡ä»¶çš„ç»“æ„ã€ç»´æŠ¤å’Œå†…å®¹ï¼Œè¿™äº›æ–‡ä»¶ä¸ºä»£ç†æä¾›é¡¹ç›®çº§çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬åˆ†æäº†2303ä¸ªä»£ç†ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œå‘ç°å®ƒä»¬å¹¶ä¸æ˜¯é™æ€æ–‡æ¡£ï¼Œè€Œæ˜¯åƒé…ç½®ä»£ç ä¸€æ ·ä¸æ–­æ¼”å˜çš„å¤æ‚æ–‡æ¡£ã€‚å¼€å‘è€…åœ¨è¿™äº›æ–‡ä»¶ä¸­ä¼˜å…ˆè€ƒè™‘åŠŸèƒ½æ€§ä¸Šä¸‹æ–‡ï¼Œå¦‚æ„å»ºå’Œè¿è¡Œå‘½ä»¤ã€å®ç°ç»†èŠ‚å’Œæ¶æ„ï¼Œä½†å¯¹éåŠŸèƒ½æ€§éœ€æ±‚å¦‚å®‰å…¨æ€§å’Œæ€§èƒ½çš„å…³æ³¨è¾ƒå°‘ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¼€å‘è€…åˆ©ç”¨ä¸Šä¸‹æ–‡æ–‡ä»¶ä½¿ä»£ç†åŠŸèƒ½æ­£å¸¸ï¼Œä½†ç¼ºä¹ç¡®ä¿ä»£ç†ç”Ÿæˆä»£ç å®‰å…¨å’Œé«˜æ•ˆçš„æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†æ”¹è¿›å·¥å…·å’Œå®è·µçš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14086', 'title': 'Error-Driven Scene Editing for 3D Grounding in Large Language Models', 'url': 'https://huggingface.co/papers/2511.14086', 'abstract': 'An error-driven framework, DEER-3D, improves the grounding accuracy of 3D large language models by iteratively editing and retraining them with targeted counterfactuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '3b9099885a757ea6', 'authors': ['Yue Zhang', 'Zun Wang', 'Han Lin', 'Jialu Li', 'Jianing Yang', 'Yonatan Bitton', 'Idan Szpektor', 'Mohit Bansal'], 'affiliations': ['Google Research', 'UNC Chapel Hill', 'University of Michigan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14086.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#3d', '#synthetic'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ² 3D LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DEER-3D Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ° Ğº 3D-ÑÑ†ĞµĞ½Ğ°Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² (Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ² 3D LLM.'}, 'en': {'title': 'DEER-3D: Targeted Edits for Enhanced 3D Grounding Accuracy', 'desc': 'The paper introduces DEER-3D, an innovative framework designed to enhance the grounding accuracy of 3D large language models (3D-LLMs). It addresses the challenge of these models struggling to connect language with visual and spatial elements due to biases in training data. By employing a structured approach of decomposing errors, diagnosing them, editing scenes, and retraining the model, DEER-3D generates targeted counterfactuals that improve model performance. The framework demonstrates significant improvements in grounding accuracy through iterative refinements across various benchmarks for 3D scene understanding tasks.'}, 'zh': {'title': 'DEER-3Dï¼šç²¾å‡†æå‡3Dè¯­è¨€æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§', 'desc': 'DEER-3Dæ˜¯ä¸€ä¸ªåŸºäºé”™è¯¯é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜3Då¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ç¼–è¾‘å’Œé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨é’ˆå¯¹æ€§çš„åäº‹å®æ¥è§£å†³æ¨¡å‹åœ¨3Dç¯å¢ƒä¸­è¯­è¨€ä¸è§†è§‰å…ƒç´ çš„å¯¹æ¥é—®é¢˜ã€‚DEER-3Dé‡‡ç”¨â€œåˆ†è§£ã€è¯Šæ–­è¯„ä¼°ã€ç¼–è¾‘å’Œå†è®­ç»ƒâ€çš„ç»“æ„åŒ–å·¥ä½œæµç¨‹ï¼Œç²¾ç¡®è¯†åˆ«æ¨¡å‹çš„é”™è¯¯å¹¶è¿›è¡Œæœ€å°åŒ–çš„3Dåœºæ™¯ç¼–è¾‘ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDEER-3Dæ˜¾è‘—æå‡äº†æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†é’ˆå¯¹æ€§åœºæ™¯ç¼–è¾‘åœ¨3Dè¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13954', 'title': 'A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition', 'url': 'https://huggingface.co/papers/2511.13954', 'abstract': 'RBTransformer, a Transformer-based model, enhances EEG-based emotion recognition by capturing inter-cortical neural dynamics in latent space, outperforming existing methods on multiple datasets and dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '099ff4901f9e53ef', 'authors': ['Nilay Kumar', 'Priyansh Bhandari', 'G. Maragatham'], 'affiliations': ['Department of Computational Intelligence, SRM Institute of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13954.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ­Ğ­Ğ“', 'desc': 'RBTransformer â€” ÑÑ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ­Ğ­Ğ“-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ­Ğ­Ğ“-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ embeddings ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²ÑƒÑ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞµÑ‚Ğ¸ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶ĞºĞ¾Ñ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RBTransformer Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… SEED, DEAP Ğ¸ DREAMER Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Emotions: RBTransformer Revolutionizes EEG Analysis', 'desc': 'The RBTransformer is a novel Transformer-based model designed to improve emotion recognition from EEG signals by effectively capturing the dynamic interactions between different brain regions. It converts EEG data into Band Differential Entropy (BDE) tokens and utilizes Electrode Identity embeddings to maintain spatial information. By employing inter-cortical multi-head attention mechanisms, the model learns the dependencies between electrodes, enhancing its ability to recognize emotions accurately. Extensive experiments show that RBTransformer surpasses existing methods on multiple datasets, demonstrating its effectiveness in classifying emotional states across various dimensions.'}, 'zh': {'title': 'RBTransformerï¼šæå‡æƒ…æ„Ÿè¯†åˆ«çš„ç¥ç»åŠ¨æ€æ•æ‰', 'desc': 'RBTransformeræ˜¯ä¸€ç§åŸºäºTransformerçš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•æ‰å¤§è„‘çš®å±‚é—´çš„ç¥ç»åŠ¨æ€æ¥å¢å¼ºåŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰çš„æƒ…æ„Ÿè¯†åˆ«ã€‚è¯¥æ¨¡å‹å°†EEGä¿¡å·è½¬æ¢ä¸ºå¸¦å·®ç†µï¼ˆBDEï¼‰æ ‡è®°ï¼Œå¹¶é€šè¿‡ç”µæèº«ä»½åµŒå…¥ä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒRBTransformerèƒ½å¤Ÿå­¦ä¹ ä¸åŒè„‘åŒºä¹‹é—´çš„ç¥ç»ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRBTransformeråœ¨å¤šä¸ªæ•°æ®é›†å’Œç»´åº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.07865', 'title': 'LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost', 'url': 'https://huggingface.co/papers/2511.07865', 'abstract': 'ChaosEater automates the Chaos Engineering cycle using Large Language Models to enhance the resilience of Kubernetes systems with low time and cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '0d698a7084a9e5d6', 'authors': ['Daisuke Kikuta', 'Hiroki Ikeuchi', 'Kengo Tajiri'], 'affiliations': ['NTT, Inc., Japan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.07865.jpg', 'data': {'categories': ['#optimization', '#science'], 'emoji': 'ğŸ‰', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ…Ğ°Ğ¾ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Kubernetes-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'ChaosEater â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑÑŒ Ñ†Ğ¸ĞºĞ» Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ…Ğ°Ğ¾ÑĞ° (Chaos Engineering) Ğ´Ğ»Ñ Kubernetes-ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ workflow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºÑƒ. LLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ChaosEater ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Automating Chaos Engineering for Resilient Kubernetes Systems', 'desc': 'ChaosEater is a system that automates the Chaos Engineering (CE) process using Large Language Models (LLMs) to improve the resilience of Kubernetes systems. It simplifies the traditionally manual tasks of planning and executing CE experiments by defining a structured workflow that assigns specific tasks to LLMs. This automation allows for efficient requirement gathering, code generation, testing, and debugging, making it accessible for users without extensive expertise. The evaluation shows that ChaosEater can effectively conduct CE cycles with reduced time and cost, while also receiving validation from both human engineers and LLMs.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–æ··æ²Œå·¥ç¨‹ï¼Œæå‡ç³»ç»ŸéŸ§æ€§ï¼', 'desc': 'ChaosEater æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–æ··æ²Œå·¥ç¨‹ï¼ˆChaos Engineeringï¼‰å‘¨æœŸçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜ Kubernetes ç³»ç»Ÿçš„éŸ§æ€§ã€‚æ··æ²Œå·¥ç¨‹æ˜¯ä¸€ç§é€šè¿‡æ•…æ„å¼•å…¥æ•…éšœæ¥æµ‹è¯•åˆ†å¸ƒå¼ç³»ç»ŸéŸ§æ€§çš„å·¥ç¨‹æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ··æ²Œå·¥ç¨‹å·¥å…·è™½ç„¶å¯ä»¥è‡ªåŠ¨æ‰§è¡Œé¢„å®šä¹‰çš„å®éªŒï¼Œä½†å®éªŒè§„åˆ’å’Œç»“æœæ”¹è¿›ä»éœ€äººå·¥å®Œæˆï¼Œè€—æ—¶ä¸”éœ€è¦å¤šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚ChaosEater é€šè¿‡é¢„å®šä¹‰çš„å·¥ä½œæµç¨‹å’Œ LLMsï¼Œç®€åŒ–äº†æ•´ä¸ªæ··æ²Œå·¥ç¨‹å‘¨æœŸï¼Œä½¿å¾—ä»»ä½•äººéƒ½èƒ½ä»¥ä½æˆæœ¬æ„å»ºéŸ§æ€§ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11831', 'title': 'TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.11831', 'abstract': "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'a82135a4d5efda57', 'authors': ['Wenhao Zhou', 'Hao Zheng', 'Rong Zhao'], 'affiliations': ['Center for Brain-Inspired Computing Research (CBICR), Tsinghua University, Beijing, China', 'Department of Precision Instruments, Tsinghua University, Beijing, China', 'IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11831.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¢Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TopoPerception Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹ Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ LVLMs. ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Rethinking Global Visual Perception in LVLMs with TopoPerception', 'desc': "This paper introduces TopoPerception, a new benchmark designed to evaluate the global visual perception capabilities of Large Vision-Language Models (LVLMs) without the influence of local shortcuts. Traditional benchmarks often overestimate model performance due to their reliance on local visual features, which can mislead assessments of a model's true perceptual abilities. The authors find that state-of-the-art LVLMs perform poorly on TopoPerception, often no better than random chance, indicating a significant gap in their ability to understand global visual structures. The study suggests that simply increasing model size and complexity may not solve these issues, highlighting the need for innovative training methods or architectures to enhance global perception in LVLMs."}, 'zh': {'title': 'æ‹“æ‰‘æ„ŸçŸ¥ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨çƒæ„ŸçŸ¥èƒ½åŠ›', 'desc': 'å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šå¸¸å°†ç¼–ç å™¨çš„è§†è§‰ç‰¹å¾ä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ä½¿å¾—è§†è§‰æ„ŸçŸ¥æ¨¡å—æˆä¸ºç“¶é¢ˆï¼Œé™åˆ¶äº†LVLMsçš„æ•´ä½“èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†TopoPerceptionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æ‹“æ‰‘ç‰¹æ€§ä¸¥æ ¼è¯„ä¼°LVLMsåœ¨ä¸åŒç²’åº¦ä¸‹çš„å…¨çƒè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨TopoPerceptionä¸Šçš„è¡¨ç°ä¸éšæœºçŒœæµ‹æ— å¼‚ï¼Œè¡¨æ˜å®ƒä»¬åœ¨æ„ŸçŸ¥å…¨çƒè§†è§‰ç‰¹å¾æ–¹é¢å­˜åœ¨ä¸¥é‡ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14993', 'title': 'Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation', 'url': 'https://huggingface.co/papers/2511.14993', 'abstract': 'Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.', 'score': 222, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '69648feecbbe5248', 'authors': ['Vladimir Arkhipkin', 'Vladimir Korviakov', 'Nikolai Gerasimenko', 'Denis Parkhomenko', 'Viacheslav Vasilev', 'Alexey Letunovskiy', 'Nikolai Vaulin', 'Maria Kovaleva', 'Ivan Kirillov', 'Lev Novitskiy', 'Denis Koposov', 'Nikita Kiselev', 'Alexander Varlamov', 'Dmitrii Mikhailov', 'Vladimir Polovnikov', 'Andrey Shutkin', 'Julia Agafonova', 'Ilya Vasiliev', 'Anastasiia Kargapoltseva', 'Anna Dmitrienko', 'Anastasia Maltseva', 'Anna Averchenkova', 'Olga Kim', 'Tatiana Nikulina', 'Denis Dimitrov'], 'affiliations': ['Kandinsky Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14993.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#multimodal', '#open_source', '#video', '#data', '#architecture', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Kandinsky 5.0: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞµĞ¼ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Kandinsky 5.0 â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¸Ğ½ĞµĞµĞº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (2B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° (19B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑĞ±Ğ¾Ñ€, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ reinforcement learning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸.'}, 'en': {'title': 'Kandinsky 5.0: Revolutionizing High-Quality Image and Video Generation', 'desc': 'Kandinsky 5.0 is a cutting-edge generative model designed for creating high-resolution images and short videos. It includes three main versions: Image Lite for image generation, Video Lite for quick text-to-video tasks, and Video Pro for high-quality video generation. The model benefits from a robust training pipeline that utilizes advanced techniques like self-supervised fine-tuning and reinforcement learning to enhance output quality. With its open-source release, Kandinsky 5.0 aims to support researchers in developing high-quality generative applications.'}, 'zh': {'title': 'Kandinsky 5.0ï¼šé«˜è´¨é‡ç”Ÿæˆæ¨¡å‹çš„æ–°æ—¶ä»£', 'desc': 'Kandinsky 5.0 æ˜¯ä¸€ç³»åˆ—å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸“æ³¨äºé«˜åˆ†è¾¨ç‡å›¾åƒå’ŒçŸ­è§†é¢‘çš„åˆæˆã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ç§æ ¸å¿ƒæ¨¡å‹ï¼šKandinsky 5.0 Image Liteï¼Œå…·æœ‰6Bå‚æ•°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼›Kandinsky 5.0 Video Liteï¼Œå¿«é€Ÿè½»é‡çš„2Bå‚æ•°æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼›ä»¥åŠKandinsky 5.0 Video Proï¼Œå…·æœ‰19Bå‚æ•°çš„é«˜è´¨é‡è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†æ•°æ®ç­–åˆ’ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¤„ç†ã€è¿‡æ»¤å’Œèšç±»ï¼Œä»¥åŠå¤šé˜¶æ®µè®­ç»ƒç®¡é“ä¸­çš„è´¨é‡æå‡æŠ€æœ¯ï¼Œå¦‚è‡ªç›‘ç£å¾®è°ƒå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒã€‚Kandinsky 5.0 é€šè¿‡åˆ›æ–°çš„æ¶æ„å’Œè®­ç»ƒä¼˜åŒ–ï¼Œå®ç°äº†é«˜ç”Ÿæˆé€Ÿåº¦å’Œå“è¶Šçš„æ€§èƒ½ï¼Œé€‚ç”¨äºå¤šç§ç”Ÿæˆåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15065', 'title': "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks", 'url': 'https://huggingface.co/papers/2511.15065', 'abstract': "VR-Bench evaluates video models' spatial reasoning capabilities through maze-solving tasks, demonstrating that these models excel in spatial perception and reasoning, outperforming VLMs and benefiting from diverse sampling during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.", 'score': 74, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '09e9785ac42b8ebc', 'authors': ['Cheng Yang', 'Haiyuan Wan', 'Yiran Peng', 'Xin Cheng', 'Zhaoyang Yu', 'Jiayi Zhang', 'Junchi Yu', 'Xinlei Yu', 'Xiawu Zheng', 'Dongzhan Zhou', 'Chenglin Wu'], 'affiliations': ['DeepWisdom', 'Hong Kong University of Science and Technology (GuangZhou)', 'National University of Singapore', 'Renmin University of China', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Oxford', 'Xiamen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15065.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#games', '#reasoning'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VR-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 10â€“20 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Spatial Reasoning in Video Models with VR-Bench', 'desc': 'The paper introduces VR-Bench, a benchmark designed to evaluate the spatial reasoning capabilities of video models through maze-solving tasks. It demonstrates that these models excel in spatial perception and reasoning, outperforming traditional vision-language models (VLMs). The study reveals that video models can effectively perform reasoning via video generation, leveraging explicit spatial layouts and temporal continuity. Additionally, it finds that diverse sampling during inference significantly enhances the reliability of reasoning outcomes by 10-20%.'}, 'zh': {'title': 'è§†é¢‘æ¨¡å‹ï¼šç©ºé—´æ¨ç†çš„æ–°å‰æ²¿', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†VR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œä¸»è¦é€šè¿‡è¿·å®«æ±‚è§£ä»»åŠ¡è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹åœ¨ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚é€šè¿‡å¤šæ ·åŒ–çš„é‡‡æ ·æ–¹æ³•ï¼Œè§†é¢‘æ¨¡å‹åœ¨æ¨ç†æ—¶çš„å¯é æ€§æé«˜äº†10%åˆ°20%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è§†é¢‘ç”Ÿæˆåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ç‹¬ç‰¹æ½œåŠ›å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15593', 'title': 'What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity', 'url': 'https://huggingface.co/papers/2511.15593', 'abstract': 'Ideation diversity significantly enhances the performance of AI research agents across various models and scaffolds on the MLE-bench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.', 'score': 55, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'bee400ab5faf8c87', 'authors': ['Alexis Audran-Reiss', 'Jordi Armengol-EstapÃ©', 'Karen Hambardzumyan', 'Amar Budhiraja', 'Martin Josifoski', 'Edan Toledo', 'Rishi Hazra', 'Despoina Magka', 'Michael Shvartsman', 'Parth Pathak', 'Justine T Kao', 'Lucia Cipolina-Kun', 'Bhavul Gauri', 'Jean-Christophe Gagnon-Audet', 'Emanuel Tewolde', 'Jenny Zhang', 'Taco Cohen', 'Yossi Adi', 'Tatiana Shavrina', 'Yoram Bachrach'], 'affiliations': ['FAIR at Meta', 'Meta SuperIntelligence Labs', 'University College London', 'University of British Columbia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15593.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸Ğ´ĞµĞ¹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ´ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MLE-bench Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ´ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Boosting AI Performance through Ideation Diversity', 'desc': 'This paper investigates how ideation diversity affects the performance of AI research agents in machine learning. By analyzing agent trajectories on the MLE-bench benchmark, the authors find that agents with greater ideation diversity tend to perform better. They conduct controlled experiments to show that increasing ideation diversity leads to improved outcomes in various models and agent scaffolds. Additionally, the study confirms these results using multiple performance metrics, reinforcing the importance of ideation diversity in enhancing AI research capabilities.'}, 'zh': {'title': 'åˆ›æ„å¤šæ ·æ€§æå‡AIç ”ç©¶ä»£ç†æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åˆ›æ„å¤šæ ·æ€§å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬åœ¨MLE-benchåŸºå‡†ä¸Šåˆ†æäº†ä¸åŒæ¨¡å‹å’Œä»£ç†æ¡†æ¶çš„ä»£ç†è½¨è¿¹ï¼Œå‘ç°åˆ›æ„å¤šæ ·æ€§ä¸ä»£ç†çš„è¡¨ç°æœ‰æ˜¾è‘—å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ›æ„å¤šæ ·æ€§è¶Šé«˜ï¼Œä»£ç†çš„æ€§èƒ½è¶Šå¼ºã€‚æˆ‘ä»¬è¿˜é€šè¿‡å…¶ä»–è¯„ä¼°æŒ‡æ ‡éªŒè¯äº†è¿™ä¸€å‘ç°ï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†åˆ›æ„å¤šæ ·æ€§åœ¨AIç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15661', 'title': 'VisPlay: Self-Evolving Vision-Language Models from Images', 'url': 'https://huggingface.co/papers/2511.15661', 'abstract': "VisPlay, a self-evolving RL framework, uses unlabeled image data to enhance VLMs' reasoning, generalization, and response quality through two interacting roles and GRPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/", 'score': 42, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'a19c6b5e20cad57d', 'authors': ['Yicheng He', 'Chengsong Huang', 'Zongxia Li', 'Jiaxin Huang', 'Yonghui Yang'], 'affiliations': ['National University of Singapore', 'University of Illinois Urbana-Champaign', 'University of Maryland', 'Washington University in St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15661.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#benchmark', '#open_source', '#rl', '#hallucinations', '#rlhf', '#reasoning'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ¸Ğ³Ñ€Ğ°: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'VisPlay â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ° Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ğ´Ñ€ÑƒĞ³Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ…Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering VLMs with Self-Evolving Reinforcement Learning', 'desc': 'VisPlay is a novel self-evolving reinforcement learning (RL) framework designed to enhance Vision-Language Models (VLMs) by utilizing large amounts of unlabeled image data. It introduces two key roles: an Image-Conditioned Questioner that creates challenging visual questions, and a Multimodal Reasoner that provides responses to these questions. The framework employs Group Relative Policy Optimization (GRPO) to optimize the training process, balancing the complexity of questions with the quality of answers. By applying VisPlay to different model families, it demonstrates significant improvements in visual reasoning and generalization across multiple benchmarks.'}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹æå‡æ¨ç†èƒ½åŠ›', 'desc': 'VisPlayæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§é‡æœªæ ‡è®°çš„å›¾åƒæ•°æ®æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªç›¸äº’ä½œç”¨çš„è§’è‰²ï¼šå›¾åƒæ¡ä»¶æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…ï¼Œå‰è€…æå‡ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰é—®é¢˜ï¼Œåè€…ç”Ÿæˆç›¸åº”çš„ç­”æ¡ˆã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒVisPlayåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡äº†é—®é¢˜çš„å¤æ‚æ€§å’Œç­”æ¡ˆçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisPlayåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è§†è§‰æ¨ç†å’Œç»„åˆæ³›åŒ–çš„ä¸€è‡´æ€§æå‡ï¼Œå±•ç¤ºäº†è‡ªæˆ‘è¿›åŒ–å¤šæ¨¡æ€æ™ºèƒ½çš„å¯æ‰©å±•è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15186', 'title': 'Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset', 'url': 'https://huggingface.co/papers/2511.15186', 'abstract': 'A new instruction-guided lesion segmentation paradigm using a large-scale dataset and a vision-language model enables diverse CXR lesion segmentation with simple instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.', 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '3030d26b32d3ec42', 'authors': ['Geon Choi', 'Hangyul Yoon', 'Hyunju Shin', 'Hyunki Park', 'Sang Hoon Seo', 'Eunho Yang', 'Edward Choi'], 'affiliations': ['AITRICS', 'KAIST', 'Samsung Medical Center'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15186.jpg', 'data': {'categories': ['#science', '#multimodal', '#open_source', '#dataset', '#cv', '#synthetic', '#healthcare'], 'emoji': 'ğŸ«', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ MIMIC-ILS â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1,1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 192K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ĞµĞ³ĞºĞ¸Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ROSALIA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Simplifying Lesion Segmentation with User-Friendly Instructions', 'desc': 'This paper presents a new method for segmenting lesions in chest X-rays (CXRs) using simple instructions, overcoming the limitations of previous models that required complex inputs. The authors introduce a large-scale dataset called MIMIC-ILS, which includes 1.1 million instruction-answer pairs and covers various lesion types. They also develop a vision-language model named ROSALIA, which is fine-tuned on this dataset to accurately segment lesions and provide explanations based on user instructions. The results demonstrate that this approach significantly improves the accessibility and effectiveness of lesion segmentation in medical imaging.'}, 'zh': {'title': 'ç®€å•æŒ‡ä»¤å®ç°å¤šæ ·åŒ–ç—…ç¶åˆ†å‰²', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡ä»¤å¼•å¯¼ç—…ç¶åˆ†å‰²èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡ç®€å•çš„ç”¨æˆ·æŒ‡ä»¤å®ç°å¤šæ ·åŒ–çš„èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰ç—…ç¶åˆ†å‰²ã€‚æˆ‘ä»¬æ„å»ºäº†MIMIC-ILSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤-ç­”æ¡ˆæ•°æ®é›†ï¼ŒåŒ…å«110ä¸‡ä¸ªæŒ‡ä»¤-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–ä¸ƒç§ä¸»è¦ç—…ç¶ç±»å‹ã€‚é€šè¿‡ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ROSALIAï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç—…ç¶åˆ†å‰²å’Œæ–‡æœ¬è§£é‡Šæ–¹é¢çš„é«˜å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåƒç´ çº§CXRç—…ç¶å®šä½æä¾›äº†æœ‰æ•ˆçš„åŸºç¡€èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14349', 'title': 'ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries', 'url': 'https://huggingface.co/papers/2511.14349', 'abstract': 'ARC-Chapter is a large-scale video chaptering model that improves performance through extensive training data and a new evaluation metric, demonstrating superior results and transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '60b706cd356bf7d3', 'authors': ['Junfu Pu', 'Teng Wang', 'Yixiao Ge', 'Yuying Ge', 'Chen Li', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14349.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#transfer_learning', '#video', '#dataset', '#multilingual'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸', 'desc': 'ARC-Chapter â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸, Ñ‚ĞµĞºÑÑ‚ ÑĞ¾ ÑÑ†ĞµĞ½ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GRACE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 14% Ğ¿Ğ¾ F1-score Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Video Chaptering with ARC-Chapter!', 'desc': 'ARC-Chapter is a novel video chaptering model that leverages a large-scale dataset to enhance the structuring of long videos. It addresses limitations of previous methods by utilizing over a million long video chapters with detailed bilingual annotations. The model introduces a new evaluation metric called GRACE, which better captures the nuances of chaptering by considering segment overlaps and semantic similarity. As a result, ARC-Chapter achieves significant performance improvements, setting new benchmarks in F1 and SODA scores, and demonstrates strong transferability to related tasks like dense video captioning.'}, 'zh': {'title': 'ARC-Chapterï¼šè§†é¢‘ç« èŠ‚åˆ’åˆ†çš„æ–°æ ‡æ†', 'desc': 'ARC-Chapteræ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘ç« èŠ‚æ¨¡å‹ï¼Œé€šè¿‡å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œæ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†è¶…è¿‡ç™¾ä¸‡çº§çš„é•¿è§†é¢‘ç« èŠ‚è¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰åŒè¯­ã€æ—¶é—´åŸºç¡€å’Œå±‚æ¬¡åŒ–çš„ç« èŠ‚æ³¨é‡Šã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡GRACEï¼Œæ›´å¥½åœ°åæ˜ äº†å®é™…ç« èŠ‚åˆ’åˆ†çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARC-Chapteråœ¨F1åˆ†æ•°å’ŒSODAåˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„è¿ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15586', 'title': 'MHR: Momentum Human Rig', 'url': 'https://huggingface.co/papers/2511.15586', 'abstract': "MHR combines ATLAS's skeleton/shape paradigm with a modern rig to provide expressive, anatomically plausible human animation for AR/VR and graphics.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.", 'score': 13, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'c5f90b0500ba4bb9', 'authors': ['Aaron Ferguson', 'Ahmed A. A. Osman', 'Berta Bescos', 'Carsten Stoll', 'Chris Twigg', 'Christoph Lassner', 'David Otte', 'Eric Vignola', 'Fabian Prada', 'Federica Bogo', 'Igor Santesteban', 'Javier Romero', 'Jenna Zarate', 'Jeongseok Lee', 'Jinhyung Park', 'Jinlong Yang', 'John Doublestein', 'Kishore Venkateshan', 'Kris Kitani', 'Ladislav Kavan', 'Marco Dal Farra', 'Matthew Hu', 'Matthew Cioffi', 'Michael Fabris', 'Michael Ranieri', 'Mohammad Modarres', 'Petr Kadlecek', 'Rawal Khirodkar', 'Rinat Abdrashitov', 'Romain PrÃ©vost', 'Roman Rajbhandari', 'Ronald Mallet', 'Russell Pearsall', 'Sandy Kao', 'Sanjeev Kumar', 'Scott Parrish', 'Shoou-I Yu', 'Shunsuke Saito', 'Takaaki Shiratori', 'Te-Li Wang', 'Tony Tung', 'Yichen Xu', 'Yuan Dong', 'Yuhua Chen', 'Yuanlu Xu', 'Yuting Ye', 'Zhongshi Jiang'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15586.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§‘\u200dğŸ¦¾', 'ru': {'title': 'ĞĞ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² AR/VR', 'desc': 'MHR â€” ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞ»Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ATLAS Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ñ€Ğ¸Ğ³-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¾Ğ¹ Momentum Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ AR/VR Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Realistic Human Animation for AR/VR with MHR', 'desc': 'MHR is a new model for creating realistic human animations in augmented and virtual reality. It merges the skeleton and shape concepts from the ATLAS model with a modern rig that allows for flexible movements. The model includes a pose corrective system that enhances the realism of animations by adjusting poses dynamically. This makes MHR suitable for use in various graphics applications, ensuring that animations look natural and anatomically correct.'}, 'zh': {'title': 'MHRï¼šè§£å‰–å­¦ä¸è¡¨ç°åŠ›çš„å®Œç¾ç»“åˆ', 'desc': 'MHRæ˜¯ä¸€ç§å‚æ•°åŒ–çš„äººä½“æ¨¡å‹ï¼Œå®ƒç»“åˆäº†ATLASçš„è§£è€¦éª¨æ¶/å½¢çŠ¶èŒƒå¼å’Œç°ä»£çš„çµæ´»è£…é…ç³»ç»Ÿã€‚è¯¥æ¨¡å‹æ”¯æŒéçº¿æ€§å§¿æ€ä¿®æ­£ï¼Œèƒ½å¤Ÿå®ç°å¯Œæœ‰è¡¨ç°åŠ›ä¸”ç¬¦åˆè§£å‰–å­¦çš„åŠ¨ç”»æ•ˆæœã€‚MHRç‰¹åˆ«é€‚ç”¨äºå¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰å’Œå›¾å½¢å¤„ç†çš„é›†æˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMHRä¸ºäººç±»åŠ¨ç”»æä¾›äº†æ›´é«˜çš„çœŸå®æ„Ÿå’Œè¡¨ç°åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12207', 'title': 'Mixture of States: Routing Token-Level Dynamics for Multimodal Generation', 'url': 'https://huggingface.co/papers/2511.12207', 'abstract': "MoS, a novel multimodal diffusion model fusion paradigm, achieves state-of-the-art results in text-to-image generation and editing with minimal parameters and computational overhead by using a learnable, token-wise router for modality interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an Îµ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '732aff547d48321d', 'authors': ['Haozhe Liu', 'Ding Liu', 'Mingchen Zhuge', 'Zijian Zhou', 'Tian Xie', 'Sen He', 'Yukang Yang', 'Shuming Liu', 'Yuren Cong', 'Jiadong Guo', 'Hongyu Xu', 'Ke Xu', 'Kam-Woh Ng', 'Juan C. PÃ©rez', 'Juan-Manuel~PÃ©rez-RÃºa', 'Tao Xiang', 'Wei Liu', 'Shikun Liu', 'JÃ¼rgen Schmidhuber'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12207.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#inference', '#architecture'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹', 'desc': 'MoS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Îµ-Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. MoS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ state-of-the-art Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 3-5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'MoS: Efficient Multimodal Fusion for Superior Image Generation', 'desc': 'The paper presents MoS, a new approach for combining different types of data (multimodal) in diffusion models, which are used for generating and editing images from text. MoS uses a smart router that learns to connect different data types at a very detailed level, allowing it to work efficiently with fewer resources. This method focuses on selecting the most relevant features from the data, which helps in achieving high-quality results without needing a lot of computational power. The experiments show that MoS can produce better or similar results compared to larger models, making it a promising option for future applications in text-to-image tasks.'}, 'zh': {'title': 'MoSï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹èåˆ', 'desc': 'MoSæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹èåˆèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚å®ƒé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„ã€é€ä¸ªæ ‡è®°çš„è·¯ç”±å™¨æ¥å®ç°æ¨¡æ€ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œå‡å°‘å‚æ•°å’Œè®¡ç®—å¼€é”€ã€‚è¯¥è·¯ç”±å™¨æ ¹æ®å»å™ªæ—¶é—´æ­¥å’Œè¾“å…¥åŠ¨æ€åœ°é€‰æ‹©æœ€ä¼˜çš„éšè—çŠ¶æ€ï¼Œç²¾ç¡®å¯¹é½æ ‡è®°çº§ç‰¹å¾ä¸æ‰©æ•£è½¨è¿¹ã€‚MoSæ¨¡å‹ä»…éœ€3Båˆ°5Bçš„å‚æ•°ï¼Œä¾¿èƒ½ä¸æœ€å¤§å››å€å‚æ•°çš„æ¨¡å‹ç›¸åª²ç¾æˆ–è¶…è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15706', 'title': 'RoMa v2: Harder Better Faster Denser Feature Matching', 'url': 'https://huggingface.co/papers/2511.15706', 'abstract': 'A novel dense feature matching model using a custom architecture and loss, combined with DINOv3, achieves state-of-the-art accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '3bb41368dddfca61', 'authors': ['Johan Edstedt', 'David NordstrÃ¶m', 'Yushan Zhang', 'Georg BÃ¶kman', 'Jonathan Astermark', 'Viktor Larsson', 'Anders Heyden', 'Fredrik Kahl', 'MÃ¥rten WadenbÃ¤ck', 'Michael Felsberg'], 'affiliations': ['Centre for Mathematical Sciences, Lund University', 'Chalmers University of Technology', 'Linkoping University', 'University of Amsterdam'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15706.jpg', 'data': {'categories': ['#architecture', '#cv', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DINOv3', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ 3D ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ DINOv3, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ CUDA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ»Ğ¾ÑÑĞ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Dense Feature Matching with Custom Architecture and DINOv3', 'desc': "This paper presents a new dense feature matching model that improves the accuracy and efficiency of matching features between images of 3D scenes. The authors introduce a custom architecture and loss function, along with a diverse training dataset, to enhance the model's performance on challenging matching tasks. They also implement a two-stage pipeline that separates matching and refinement processes, which speeds up training and reduces memory usage. By integrating the DINOv3 foundation model, the proposed matcher achieves state-of-the-art results, outperforming previous models in accuracy and robustness."}, 'zh': {'title': 'æ–°å‹ç¨ å¯†ç‰¹å¾åŒ¹é…æ¨¡å‹ï¼Œç²¾å‡†é«˜æ•ˆï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¨ å¯†ç‰¹å¾åŒ¹é…æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªå®šä¹‰æ¶æ„å’ŒæŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆDINOv3ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç¨ å¯†ç‰¹å¾åŒ¹é…æ—¨åœ¨ä¼°è®¡ä¸‰ç»´åœºæ™¯ä¸­ä¸¤å¹…å›¾åƒä¹‹é—´çš„æ‰€æœ‰å¯¹åº”å…³ç³»ï¼Œå› å…¶é«˜å‡†ç¡®æ€§å’Œé²æ£’æ€§è€Œè¢«å¹¿æ³›è®¤å¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¨ å¯†åŒ¹é…å™¨åœ¨è®¸å¤šå¤æ‚çš„ç°å®åœºæ™¯ä¸­ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä¸”é«˜ç²¾åº¦æ¨¡å‹é€šå¸¸é€Ÿåº¦è¾ƒæ…¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚é€šè¿‡ä¸€ç³»åˆ—ç³»ç»Ÿæ€§çš„æ”¹è¿›ï¼Œæœ¬æ–‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºäº†æ–°çš„åŒ¹é…æ¶æ„å’ŒæŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡è§£è€¦çš„ä¸¤é˜¶æ®µåŒ¹é…ä¸ç²¾ç‚¼æµç¨‹åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13524', 'title': 'FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI', 'url': 'https://huggingface.co/papers/2511.13524', 'abstract': 'As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '7fb7fdf95694c72d', 'authors': ['Yuhang Peng', 'Yizhou Pan', 'Xinning He', 'Jihaoyu Yang', 'Xinyu Yin', 'Han Wang', 'Xiaoji Zheng', 'Chao Gao', 'Jiangtao Gong'], 'affiliations': ['Institute for AI Industry Research, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13524.jpg', 'data': {'categories': ['#training', '#multimodal', '#benchmark', '#open_source', '#dataset', '#agents', '#cv', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ embodied Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FreeAskWorld â€” Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Vision-and-Language Navigation Ğ² Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑˆĞµÑÑ‚ÑŒÑ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 17 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ embodied AI ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° FreeAskWorld, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼.'}, 'en': {'title': 'Empowering AI with Socially Grounded Interaction in FreeAskWorld', 'desc': 'This paper presents FreeAskWorld, a new simulation framework designed to enhance embodied intelligence in AI by integrating large language models (LLMs) for better behavior planning and social interaction. The framework allows for realistic human-agent simulations and includes a data generation pipeline for various tasks. It extends the Vision-and-Language Navigation (VLN) task to include interactive elements, enabling agents to seek and interpret navigational guidance. The results show that models trained on FreeAskWorld significantly improve their understanding and interaction skills, highlighting the importance of social context in AI development.'}, 'zh': {'title': 'æ¨åŠ¨å…·èº«æ™ºèƒ½çš„ç¤¾ä¼šåŸºç¡€æ¨¡æ‹Ÿæ¡†æ¶', 'desc': 'éšç€å…·èº«æ™ºèƒ½æˆä¸ºäººå·¥æ™ºèƒ½ç ”ç©¶çš„æ ¸å¿ƒå‰æ²¿ï¼Œæ¨¡æ‹Ÿå¹³å°å¿…é¡»è¶…è¶Šä½çº§ç‰©ç†äº¤äº’ï¼Œä»¥æ•æ‰å¤æ‚çš„äººæœ¬ç¤¾ä¼šè¡Œä¸ºã€‚æˆ‘ä»¬ä»‹ç»äº†FreeAskWorldï¼Œè¿™æ˜¯ä¸€ä¸ªäº’åŠ¨æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ•´åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”¨äºé«˜å±‚æ¬¡è¡Œä¸ºè§„åˆ’å’Œè¯­ä¹‰åŸºç¡€çš„äº¤äº’ï¼Œå—æ„å›¾å’Œç¤¾ä¼šè®¤çŸ¥ç†è®ºçš„å¯å‘ã€‚è¯¥æ¡†æ¶æ”¯æŒå¯æ‰©å±•ã€çœŸå®çš„äººæœºæ¨¡æ‹Ÿï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªæ¨¡å—åŒ–çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€‚ç”¨äºå¤šæ ·åŒ–çš„å…·èº«ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨FreeAskWorldä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œäº¤äº’èƒ½åŠ›ä¸Šä¼˜äºåŸå§‹æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç¤¾ä¼šåŸºç¡€æ¨¡æ‹Ÿæ¡†æ¶åœ¨æ¨åŠ¨å…·èº«AIç³»ç»Ÿå‘å¤æ‚é«˜å±‚æ¬¡è§„åˆ’å’Œæ›´è‡ªç„¶çš„äººæœºäº¤äº’æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15038', 'title': 'Aligning Generative Music AI with Human Preferences: Methods and Challenges', 'url': 'https://huggingface.co/papers/2511.15038', 'abstract': "Preference alignment techniques, such as those in MusicRL and DiffRhythm+, are proposed to enhance music generation by addressing human preferences and unique challenges like temporal coherence and harmonic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '29b2b4d2bbcae1a3', 'authors': ['Dorien Herremans', 'Abhinaba Roy'], 'affiliations': ['AMAAI Lab, Singapore University of Technology and Design'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15038.jpg', 'data': {'categories': ['#alignment', '#diffusion', '#training', '#audio', '#rlhf'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº MusicRL Ğ¸ DiffRhythm+. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ - Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Aligning AI Music with Human Preferences', 'desc': 'This paper discusses how preference alignment techniques can improve AI-generated music by better matching human tastes. It highlights the limitations of current generative models, which often do not consider the subtleties of human musical preferences due to their loss functions. The authors propose using methods like MusicRL and DiffRhythm+ to enhance aspects such as temporal coherence and harmonic consistency in music generation. They also identify challenges in scaling these techniques for longer compositions and emphasize the need for interdisciplinary research to create music AI that meets human creative needs.'}, 'zh': {'title': 'éŸ³ä¹ç”Ÿæˆä¸­çš„åå¥½å¯¹é½ï¼šæå‡åˆ›ä½œä½“éªŒ', 'desc': 'æœ¬æ–‡æå‡ºäº†åå¥½å¯¹é½æŠ€æœ¯ï¼Œä»¥æé«˜éŸ³ä¹ç”Ÿæˆçš„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯è§£å†³äººç±»åå¥½å’ŒéŸ³ä¹ç”Ÿæˆä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´ä¸€è‡´æ€§å’Œå’Œå£°ä¸€è‡´æ€§ã€‚å°½ç®¡ç”Ÿæˆå¼AIåœ¨éŸ³ä¹åˆ›ä½œä¸­å–å¾—äº†æ˜¾è‘—çš„é£æ ¼å¤šæ ·æ€§å’Œä¿çœŸåº¦ï¼Œä½†ç”±äºç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œè¿™äº›ç³»ç»Ÿå¾€å¾€æ— æ³•æ»¡è¶³ç»†è‡´çš„äººç±»åå¥½ã€‚æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•é€šè¿‡å¤§è§„æ¨¡åå¥½å­¦ä¹ å’Œå¤šåå¥½å¯¹é½æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶æå‡ºäº†åœ¨æ¨ç†æ—¶ä¼˜åŒ–çš„æŠ€æœ¯ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¸Œæœ›åå¥½å¯¹é½çš„éŸ³ä¹ç”Ÿæˆèƒ½å¤Ÿåœ¨äº’åŠ¨åˆ›ä½œå·¥å…·å’Œä¸ªæ€§åŒ–éŸ³ä¹æœåŠ¡ä¸­å®ç°å˜é©æ€§åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13001', 'title': 'Medal S: Spatio-Textual Prompt Model for Medical Segmentation', 'url': 'https://huggingface.co/papers/2511.13001', 'abstract': 'Medal S is a medical segmentation foundation model that integrates spatial and textual prompts for efficient, high-accuracy multi-class segmentation across various imaging modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '2d0a2011c58fda12', 'authors': ['Pengcheng Shi', 'Jiawei Chen', 'Jiaqi Liu', 'Xinglin Zhang', 'Tao Chen', 'Lei Li'], 'affiliations': ['Medical Image Insights, Shanghai, China', 'University of Washington, Seattle, WA, USA', 'University of Waterloo, Waterloo, ON, Canada', 'Xian Jiaotong University, Xian, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13001.jpg', 'data': {'categories': ['#science', '#multimodal', '#inference', '#open_source', '#dataset', '#cv', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Medal S â€” Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ›Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ¾ 243 ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ½Ğ° ĞšĞ¢, ĞœĞ Ğ¢, ĞŸĞ­Ğ¢, ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞµ Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸. ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 90% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Medal S: Revolutionizing Medical Segmentation with Spatial and Textual Prompts', 'desc': 'Medal S is a cutting-edge medical segmentation model that combines spatial and textual prompts to enhance multi-class segmentation accuracy across various imaging types. It uniquely aligns volumetric prompts with text embeddings, addressing resolution mismatches that can lead to errors in segmentation. The model processes multiple segmentation masks simultaneously while maintaining full 3D context, significantly improving performance and reducing inference time. With its innovative prompting modes and advanced data augmentation techniques, Medal S sets a new standard in medical image analysis, outperforming existing models in both efficiency and accuracy.'}, 'zh': {'title': 'Medal Sï¼šé«˜æ•ˆç²¾å‡†çš„åŒ»ç–—åˆ†å‰²æ–°æ¨¡å‹', 'desc': 'Medal S æ˜¯ä¸€ç§åŒ»ç–—åˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¸”é«˜ç²¾åº¦åœ°è¿›è¡Œå¤šç±»åˆ†å‰²ï¼Œæ”¯æŒç©ºé—´å’Œæ–‡æœ¬æç¤ºçš„ç»“åˆã€‚ä¸ä»…ä½¿ç”¨æ–‡æœ¬çš„æ–¹æ³•ä¸åŒï¼ŒMedal S å®ç°äº†ä½“ç§¯æç¤ºä¸æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„é€šé“å¯¹é½ï¼Œå‡å°‘äº†åˆ†è¾¨ç‡ä¸åŒ¹é…å¸¦æ¥çš„ä¸å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªåŸç”Ÿåˆ†è¾¨ç‡çš„æ©è†œï¼Œæå‡äº†å¤šç±»åˆ†å‰²çš„æ€§èƒ½ã€‚é€šè¿‡åŠ¨æ€é‡é‡‡æ ·å’Œä¼˜åŒ–çš„æ–‡æœ¬é¢„å¤„ç†ï¼ŒMedal S åœ¨å¤šä¸ªæˆåƒæ¨¡æ€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»ç–—åˆ†å‰²ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16624', 'title': 'SAM 3D: 3Dfy Anything in Images', 'url': 'https://huggingface.co/papers/2511.16624', 'abstract': 'SAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.', 'score': 106, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '138438c382a15080', 'authors': ['SAM 3D Team', 'Xingyu Chen', 'Fu-Jen Chu', 'Pierre Gleize', 'Kevin J Liang', 'Alexander Sax', 'Hao Tang', 'Weiyao Wang', 'Michelle Guo', 'Thibaut Hardin', 'Xiang Li', 'Aohan Lin', 'Jiawei Liu', 'Ziqi Ma', 'Anushka Sagar', 'Bowen Song', 'Xiaodong Wang', 'Jianing Yang', 'Bowen Zhang', 'Piotr DollÃ¡r', 'Georgia Gkioxari', 'Matt Feiszli', 'Jitendra Malik'], 'affiliations': ['Meta Superintelligence Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16624.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#3d', '#dataset', '#cv', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¼ĞºĞ° Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ 3D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'SAM 3D â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñƒ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² ÑÑ†ĞµĞ½Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºÑƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ²-Ñ†Ğ¸ĞºĞ»Ğµ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ SAM 3D Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹ 5:1 Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing 3D Object Reconstruction from Single Images', 'desc': 'SAM 3D is a generative model designed to create 3D representations of objects from single images. It utilizes a multi-stage training approach that includes pretraining on synthetic data and aligning with real-world images to enhance performance. The model effectively handles challenges like occlusion and clutter in natural scenes by leveraging contextual visual cues. With a robust pipeline for annotating object features, SAM 3D achieves superior results in human preference tests, significantly advancing the field of 3D object reconstruction.'}, 'zh': {'title': 'SAM 3Dï¼šä»å•å›¾åƒé‡å»º3Dç‰©ä½“çš„åˆ›æ–°æ¨¡å‹', 'desc': 'SAM 3Dæ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒé‡å»º3Dç‰©ä½“ã€‚å®ƒé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬åˆæˆé¢„è®­ç»ƒå’ŒçœŸå®ä¸–ç•Œå¯¹é½ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å¤„ç†è‡ªç„¶å›¾åƒæ—¶è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåº”å¯¹é®æŒ¡å’Œåœºæ™¯æ‚ä¹±çš„é—®é¢˜ã€‚é€šè¿‡äººæœºåä½œçš„æ–¹å¼ï¼ŒSAM 3Dæä¾›äº†å‰æ‰€æœªæœ‰è§„æ¨¡çš„è§†è§‰åŸºç¡€3Dé‡å»ºæ•°æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16043', 'title': 'Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning', 'url': 'https://huggingface.co/papers/2511.16043', 'abstract': "Agent0, a self-evolving framework utilizing multi-step co-evolution and tool integration, enhances LLM reasoning capabilities without human-curated data.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.", 'score': 105, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '4fcabbd3d12bbb85', 'authors': ['Peng Xia', 'Kaide Zeng', 'Jiaqi Liu', 'Can Qin', 'Fang Wu', 'Yiyang Zhou', 'Caiming Xiong', 'Huaxiu Yao'], 'affiliations': ['Salesforce Research', 'Stanford University', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16043.jpg', 'data': {'categories': ['#training', '#open_source', '#rl', '#agents', '#reasoning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ ĞºĞ¾Ğ¾Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Agent0 â€” ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ĞºĞ¾Ğ¾Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ²ÑƒÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ğ´Ğ¸Ğ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑÑ‘ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸Ñ… Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞ°ÑÑ‰ĞµĞ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°-ĞºÑƒÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‰Ñ‘ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ ÑĞ°Ğ¼Ğ¾ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ğ½Ğ° 18% Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° 24% Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering LLMs through Self-Evolution and Tool Integration', 'desc': 'Agent0 is a novel framework designed to enhance the reasoning abilities of Large Language Models (LLMs) without relying on human-curated data. It employs a multi-step co-evolution process where two agents, a curriculum agent and an executor agent, compete and collaborate to improve their performance. The curriculum agent generates increasingly complex tasks, while the executor agent learns to solve these tasks using integrated external tools. This self-reinforcing cycle leads to significant improvements in reasoning capabilities, as demonstrated by substantial performance gains on various benchmarks.'}, 'zh': {'title': 'Agent0ï¼šæ— æ•°æ®è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶', 'desc': 'Agent0æ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡å¤šæ­¥éª¤å…±åŒè¿›åŒ–å’Œå·¥å…·æ•´åˆï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ•´ç†çš„æ•°æ®ã€‚è¯¥æ¡†æ¶å…‹æœäº†ç°æœ‰è‡ªæˆ‘è¿›åŒ–æ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒåŸ¹å…»é«˜æ€§èƒ½çš„æ™ºèƒ½ä½“ã€‚Agent0é€šè¿‡ä¸¤ä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„å…±ç”Ÿç«äº‰æ¥å·¥ä½œï¼Œä¸€ä¸ªæ˜¯æå‡ºæŒ‘æˆ˜æ€§ä»»åŠ¡çš„è¯¾ç¨‹æ™ºèƒ½ä½“ï¼Œå¦ä¸€ä¸ªæ˜¯å­¦ä¹ è§£å†³è¿™äº›ä»»åŠ¡çš„æ‰§è¡Œæ™ºèƒ½ä½“ã€‚é€šè¿‡è¿™ç§è¿­ä»£è¿‡ç¨‹ï¼ŒAgent0ä¸æ–­ç”Ÿæˆé«˜è´¨é‡çš„è¯¾ç¨‹ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16668', 'title': 'V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models', 'url': 'https://huggingface.co/papers/2511.16668', 'abstract': 'V-ReasonBench evaluates generative video models across structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics using a diverse set of tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.', 'score': 53, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '90c49b2fc7065b11', 'authors': ['Yang Luo', 'Xuanlei Zhao', 'Baijiong Lin', 'Lingting Zhu', 'Liyao Tang', 'Yuqi Liu', 'Ying-Cong Chen', 'Shengju Qian', 'Xin Wang', 'Yang You'], 'affiliations': ['CUHK', 'HKU', 'HKUST(GZ)', 'LIGHTSPEED', 'NUS', 'USYD'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16668.jpg', 'data': {'categories': ['#benchmark', '#video', '#hallucinations', '#dataset', '#reasoning', '#synthetic'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ V-ReasonBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ²Ñ‹Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑˆĞµÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Evaluating Video Reasoning: V-ReasonBench Unleashed!', 'desc': 'V-ReasonBench is a benchmark designed to evaluate generative video models on their reasoning capabilities across four main areas: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. It addresses the need for systematic evaluation as recent models like Veo-3 demonstrate impressive zero-shot reasoning abilities. The benchmark includes a variety of tasks derived from both synthetic and real-world video sequences, ensuring that the evaluations are reproducible and scalable. By analyzing the performance of various state-of-the-art video models, V-ReasonBench highlights differences in reasoning skills and aims to enhance the development of models that align more closely with human reasoning.'}, 'zh': {'title': 'è§†é¢‘æ¨ç†çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶', 'desc': 'V-ReasonBenchæ˜¯ä¸€ä¸ªè¯„ä¼°ç”Ÿæˆè§†é¢‘æ¨¡å‹çš„åŸºå‡†ï¼Œä¸»è¦å…³æ³¨å››ä¸ªå…³é”®ç»´åº¦ï¼šç»“æ„åŒ–é—®é¢˜è§£å†³ã€ç©ºé—´è®¤çŸ¥ã€åŸºäºæ¨¡å¼çš„æ¨ç†å’Œç‰©ç†åŠ¨æ€ã€‚è¯¥åŸºå‡†é€šè¿‡åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„å›¾åƒåºåˆ—æ„å»ºï¼Œæä¾›äº†ä¸€ç³»åˆ—å¯éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ï¼Œç¡®ä¿å¯é‡å¤æ€§å’Œæ˜ç¡®æ€§ã€‚å¯¹å…­ç§æœ€å…ˆè¿›çš„è§†é¢‘æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºå‡ºåœ¨ä¸åŒç»´åº¦ä¸Šçš„æ˜æ˜¾å·®å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„åŒ–ã€ç©ºé—´ã€åŸºäºæ¨¡å¼å’Œç‰©ç†æ¨ç†æ–¹é¢ã€‚V-ReasonBenchæ—¨åœ¨ä¸ºè§†é¢‘æ¨ç†æä¾›ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¼€å‘æ›´å¯é ã€ä¸äººç±»æ¨ç†èƒ½åŠ›æ›´ä¸€è‡´çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15700', 'title': 'First Frame Is the Place to Go for Video Content Customization', 'url': 'https://huggingface.co/papers/2511.15700', 'abstract': "Video generation models use the first frame as a conceptual memory buffer, enabling robust customization with minimal training examples.  \t\t\t\t\tAI-generated summary \t\t\t\t What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.", 'score': 52, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '6e0190040d62e121', 'authors': ['Jingxi Chen', 'Zongxia Li', 'Zhichao Liu', 'Guangyao Shi', 'Xiyang Wu', 'Fuxiao Liu', 'Cornelia Fermuller', 'Brandon Y. Feng', 'Yiannis Aloimonos'], 'affiliations': ['Massachusetts Institute of Technology', 'University of Maryland', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15700.jpg', 'data': {'categories': ['#video', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ°Ğ´Ñ€ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğº Ğ±ÑƒÑ„ĞµÑ€ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 20-50 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unlocking Video Customization with the First Frame', 'desc': 'This paper explores the role of the first frame in video generation models, proposing that it acts as a conceptual memory buffer. Instead of just being a starting point, the first frame retains visual information that can be reused throughout the video generation process. The authors demonstrate that this understanding allows for effective customization of video content with only a small number of training examples, ranging from 20 to 50. This finding highlights a significant, yet underappreciated, capability of video generation models for creating tailored video outputs without needing extensive retraining or architectural modifications.'}, 'zh': {'title': 'ç¬¬ä¸€å¸§ï¼šè§†é¢‘ç”Ÿæˆçš„è®°å¿†ç¼“å†²åŒº', 'desc': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹å°†ç¬¬ä¸€å¸§è§†ä¸ºæ¦‚å¿µè®°å¿†ç¼“å†²åŒºï¼Œè¿™ä½¿å¾—åœ¨æœ€å°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹å®ç°å¼ºå¤§çš„å®šåˆ¶åŒ–æˆä¸ºå¯èƒ½ã€‚ä¼ ç»Ÿä¸Šï¼Œç¬¬ä¸€å¸§è¢«è®¤ä¸ºæ˜¯è§†é¢‘çš„æ—¶ç©ºèµ·ç‚¹ï¼Œä»…ä»…æ˜¯åç»­åŠ¨ç”»çš„ç§å­ã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ä¸ªæ ¹æœ¬ä¸åŒçš„è§†è§’ï¼šè§†é¢‘æ¨¡å‹éšå¼åœ°å°†ç¬¬ä¸€å¸§ä½œä¸ºå­˜å‚¨è§†è§‰å®ä½“çš„ç¼“å†²åŒºï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡å¤ä½¿ç”¨ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸åŒåœºæ™¯ä¸­ï¼Œä»…ä½¿ç”¨20-50ä¸ªè®­ç»ƒæ ·æœ¬å³å¯å®ç°ç¨³å¥å’Œé€šç”¨çš„è§†é¢‘å†…å®¹å®šåˆ¶ï¼Œè€Œæ— éœ€æ”¹å˜æ¶æ„æˆ–è¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15848', 'title': 'Step-Audio-R1 Technical Report', 'url': 'https://huggingface.co/papers/2511.15848', 'abstract': 'Step-Audio-R1, using the Modality-Grounded Reasoning Distillation framework, achieves strong reasoning capabilities in audio, outperforming previous models and demonstrating the transferability of reasoning across modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.', 'score': 51, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'bb201aead46cb654', 'authors': ['Fei Tian', 'Xiangyu Tony Zhang', 'Yuxin Zhang', 'Haoyang Zhang', 'Yuxin Li', 'Daijiao Liu', 'Yayue Deng', 'Donghang Wu', 'Jun Chen', 'Liang Zhao', 'Chengyuan Yao', 'Hexin Liu', 'Eng Siong Chng', 'Xuerui Yang', 'Xiangyu Zhang', 'Daxin Jiang', 'Gang Yu'], 'affiliations': ['StepFun-Audio'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15848.jpg', 'data': {'categories': ['#training', '#audio', '#multimodal', '#transfer_learning', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ§', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Step-Audio-R1, Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Modality-Grounded Reasoning Distillation. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Gemini 2.5 Pro Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Gemini 3 Pro Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Unlocking Audio Intelligence Through Grounded Reasoning', 'desc': 'The paper introduces Step-Audio-R1, a novel audio reasoning model that enhances reasoning capabilities in the audio domain. It utilizes the Modality-Grounded Reasoning Distillation (MGRD) framework to create reasoning chains that are firmly based on acoustic features, avoiding irrelevant or disconnected thoughts. This model outperforms previous audio models, including Gemini 2.5 Pro, and achieves results comparable to the advanced Gemini 3 Pro across various audio understanding tasks. The findings suggest that reasoning can be effectively transferred across different modalities, paving the way for more sophisticated multimodal reasoning systems.'}, 'zh': {'title': 'éŸ³é¢‘æ¨ç†çš„æ–°çªç ´ï¼šStep-Audio-R1', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Step-Audio-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸè§£é”éŸ³é¢‘é¢†åŸŸæ¨ç†èƒ½åŠ›çš„éŸ³é¢‘æ¨ç†æ¨¡å‹ã€‚é€šè¿‡æå‡ºçš„æ¨¡æ€åŸºç¡€æ¨ç†è’¸é¦æ¡†æ¶ï¼ˆMGRDï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸éŸ³é¢‘ç›¸å…³çš„æ¨ç†é“¾ï¼Œè¿™äº›æ¨ç†é“¾çœŸæ­£åŸºäºå£°å­¦ç‰¹å¾ï¼Œè€Œä¸æ˜¯æ— å…³çš„æ€è€ƒã€‚Step-Audio-R1åœ¨éŸ³é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†æ¨ç†èƒ½åŠ›åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºçœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ‰€æœ‰æ„Ÿå®˜æ¨¡æ€ä¸­è¿›è¡Œæ·±å…¥æ€è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13719', 'title': 'Scaling Spatial Intelligence with Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2511.13719', 'abstract': 'Multimodal foundation models like SenseNova-SI improve spatial intelligence through diverse data scaling and demonstrate strong performance across various spatial benchmarks while maintaining general multimodal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.', 'score': 44, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'c7c7cc9daeb1c18b', 'authors': ['Zhongang Cai', 'Ruisi Wang', 'Chenyang Gu', 'Fanyi Pu', 'Junxiang Xu', 'Yubo Wang', 'Wanqi Yin', 'Zhitao Yang', 'Chen Wei', 'Qingping Sun', 'Tongxi Zhou', 'Jiaqi Li', 'Hui En Pang', 'Oscar Qian', 'Yukun Wei', 'Zhiqian Lin', 'Xuanke Shi', 'Kewang Deng', 'Xiaoyang Han', 'Zukai Chen', 'Xiangyu Fan', 'Hanming Deng', 'Lewei Lu', 'Liang Pan', 'Bo Li', 'Ziwei Liu', 'Quan Wang', 'Dahua Lin', 'Lei Yang'], 'affiliations': ['Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13719.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#reasoning', '#cv'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SenseNova-SI â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. SenseNova-SI Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ emergent ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Spatial Intelligence with SenseNova-SI', 'desc': 'This paper presents SenseNova-SI, a multimodal foundation model designed to enhance spatial intelligence by leveraging a large and diverse dataset. The model is built on existing multimodal frameworks and demonstrates significant improvements in various spatial benchmarks while retaining strong overall multimodal understanding. The authors emphasize the importance of data scaling and its effects on model performance, including the potential for emergent generalization capabilities. Additionally, they address challenges such as overfitting and the need for robust reasoning in spatial tasks, paving the way for future research in this area.'}, 'zh': {'title': 'æå‡ç©ºé—´æ™ºèƒ½çš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†SenseNova-SIï¼Œä¸€ä¸ªå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ ·åŒ–çš„æ•°æ®æ‰©å±•æ¥æå‡ç©ºé—´æ™ºèƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤šæ¨¡æ€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚SenseNova-SI-8MåŒ…å«800ä¸‡ç§å¤šæ ·åŒ–çš„æ•°æ®æ ·æœ¬ï¼Œç»è¿‡ä¸¥æ ¼çš„ç©ºé—´èƒ½åŠ›åˆ†ç±»ï¼Œå±•ç°äº†åœ¨å¤šä¸ªç©ºé—´æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚è®ºæ–‡è¿˜åˆ†æäº†æ•°æ®æ‰©å±•çš„å½±å“ã€è¿‡æ‹Ÿåˆé£é™©ä»¥åŠç©ºé—´æ¨ç†çš„åˆæ­¥ç ”ç©¶ï¼Œæ—¨åœ¨æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16669', 'title': 'Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO', 'url': 'https://huggingface.co/papers/2511.16669', 'abstract': "VANS, a model combining reinforcement learning, a Vision-Language Model, and a Video Diffusion Model, achieves state-of-the-art performance in Video-Next-Event Prediction by generating visually consistent and semantically accurate videos.  \t\t\t\t\tAI-generated summary \t\t\t\t While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.", 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'e92e593a7d9fd7af', 'authors': ['Junhao Cheng', 'Liang Hou', 'Xin Tao', 'Jing Liao'], 'affiliations': ['City University of Hong Kong', 'Kling Team, Kuaishou Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16669.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#multimodal', '#benchmark', '#open_source', '#video', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ñƒ: Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ VANS â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ (Video-Next-Event Prediction). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (Vision-Language Model), Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ (Video Diffusion Model) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ñ… ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Joint-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ²Ñ‚Ğ¾Ñ€ÑƒÑ â€” ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ¸Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VANS-Data-100K Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'VANS: Bridging Vision and Language for Next-Event Video Prediction', 'desc': 'The paper introduces VANS, a novel model that integrates reinforcement learning with a Vision-Language Model (VLM) and a Video Diffusion Model (VDM) to enhance Video-Next-Event Prediction (VNEP). VNEP aims to generate videos that visually and semantically depict the next event in a sequence, moving beyond traditional text-based predictions. VANS employs a Joint-GRPO mechanism to synchronize the VLM and VDM, ensuring that the generated captions and videos are coherent and contextually relevant. The model demonstrates superior performance on benchmarks, showcasing its potential for intuitive procedural learning and creative exploration in video generation.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´ï¼šVANSæ¨¡å‹', 'desc': 'VANSæ˜¯ä¸€ç§ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ã€è§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¨¡å‹ï¼Œä¸“æ³¨äºè§†é¢‘ä¸‹ä¸€ä¸ªäº‹ä»¶é¢„æµ‹ï¼ˆVNEPï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”Ÿæˆè§†è§‰ä¸€è‡´ä¸”è¯­ä¹‰å‡†ç¡®çš„è§†é¢‘ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘äº‹ä»¶é¢„æµ‹çš„æ€§èƒ½ã€‚VANSçš„æ ¸å¿ƒæ˜¯è”åˆGRPOï¼Œå®ƒåè°ƒè§†è§‰è¯­è¨€æ¨¡å‹å’Œè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ååŒå·¥ä½œï¼Œä»¥å®ç°æ›´ç›´è§‚çš„å­¦ä¹ å’Œåˆ›ä½œæ¢ç´¢ã€‚é€šè¿‡æ„å»ºä¸“é—¨çš„æ•°æ®é›†VANS-Data-100Kï¼ŒVANSåœ¨ç¨‹åºæ€§å’Œé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16664', 'title': 'Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs', 'url': 'https://huggingface.co/papers/2511.16664', 'abstract': "Nemotron Elastic reduces training costs and memory usage by embedding multiple submodels within a single large language model, optimized for various deployment configurations and budgets without additional training or fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.", 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'c36b66075bb8a986', 'authors': ['Ali Taghibakhshi', 'Sharath Turuvekere Sreenivas', 'Saurav Muralidharan', 'Ruisi Cai', 'Marcin Chochowski', 'Ameya Sunil Mahabaleshwarkar', 'Yoshi Suhara', 'Oluwatobi Olabiyi', 'Daniel Korzekwa', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Jan Kautz', 'Bryan Catanzaro', 'Ashwath Aithal', 'Nima Tajbakhsh', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16664.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#architecture', '#inference', '#small_models'], 'emoji': 'ğŸª†', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¶Ğ½Ğ°Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ: Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Nemotron Elastic â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ curriculum Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, router-ÑĞµÑ‚ÑŒ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ group-aware SSM Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Mamba-Attention), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ zero-shot Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ knowledge distillation Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ñ‘Ğ², ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 360 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ½ÑƒĞ»Ñ Ğ¸ Ğ² 7 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ state-of-the-art, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'One Model, Many Solutions: Cost-Effective LLM Deployment', 'desc': 'Nemotron Elastic is a framework designed to reduce the costs and memory usage associated with training large language models (LLMs) by embedding multiple submodels within a single parent model. This approach allows for the optimization of various deployment configurations and budgets without the need for additional training or fine-tuning. By utilizing a two-stage training curriculum and an end-to-end trained router, the framework enables zero-shot extraction of submodels during deployment. The results show significant cost reductions and competitive performance compared to state-of-the-art compression techniques, while maintaining a constant memory footprint regardless of the number of models.'}, 'zh': {'title': 'é«˜æ•ˆåµŒå¥—æ¨¡å‹ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼', 'desc': 'Nemotron Elastic æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡åœ¨å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­åµŒå…¥å¤šä¸ªå­æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å’Œå†…å­˜ä½¿ç”¨ã€‚æ¯ä¸ªå­æ¨¡å‹é’ˆå¯¹ä¸åŒçš„éƒ¨ç½²é…ç½®å’Œé¢„ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å’Œä¸“é—¨ä¸ºæ¨ç†æ¨¡å‹è®¾è®¡çš„ä¸¤é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹åµŒå¥—ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å‹å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼ŒNemotron Elastic åœ¨æˆæœ¬å’Œæ€§èƒ½ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå‡†ç¡®åº¦çš„åŒæ—¶ï¼Œå‡å°‘è®­ç»ƒæ‰€éœ€çš„èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16518', 'title': 'MiMo-Embodied: X-Embodied Foundation Model Technical Report', 'url': 'https://huggingface.co/papers/2511.16518', 'abstract': 'MiMo-Embodied, a cross-embodied foundation model, achieves state-of-the-art performance in both autonomous driving and embodied AI through multi-stage learning, curated data, and CoT/RL fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.', 'score': 23, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'db2603b3778e3310', 'authors': ['Xiaoshuai Hao', 'Lei Zhou', 'Zhijian Huang', 'Zhiwen Hou', 'Yingbo Tang', 'Lingfeng Zhang', 'Guang Li', 'Zheng Lu', 'Shuhuai Ren', 'Xianhui Meng', 'Yuchen Zhang', 'Jing Wu', 'Jinghui Lu', 'Chenxu Dang', 'Jiayi Guan', 'Jianhua Wu', 'Zhiyi Hou', 'Hanbing Li', 'Shumeng Xia', 'Mingliang Zhou', 'Yinan Zheng', 'Zihao Yue', 'Shuhao Gu', 'Hao Tian', 'Yuannan Shen', 'Jianwei Cui', 'Wen Zhang', 'Shaoqing Xu', 'Bing Wang', 'Haiyang Sun', 'Zeyu Zhu', 'Yuncheng Jiang', 'Zibin Guo', 'Chuhong Gong', 'Chaofan Zhang', 'Wenbo Ding', 'Kun Ma', 'Guang Chen', 'Rui Cai', 'Diyun Xiang', 'Heng Qu', 'Fuli Luo', 'Hangjun Ye', 'Long Chen'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16518.jpg', 'data': {'categories': ['#training', '#benchmark', '#robotics', '#dataset', '#agents', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑĞ¼Ğ¸', 'desc': 'MiMo-Embodied Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºÑ€Ğ¾ÑÑĞ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 17 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ affordance Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° 12 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ fine-tuning Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'MiMo-Embodied: Bridging Autonomous Driving and Embodied AI for Superior Performance', 'desc': 'MiMo-Embodied is a groundbreaking foundation model that excels in both autonomous driving and embodied AI by utilizing multi-stage learning and curated datasets. It achieves top performance across 17 benchmarks in embodied AI and 12 benchmarks in autonomous driving, showcasing its versatility and effectiveness. The model benefits from a unique approach that combines Chain of Thought (CoT) reasoning and Reinforcement Learning (RL) fine-tuning, leading to significant improvements over existing models. This research highlights the strong positive transfer between the two domains, suggesting that advancements in one area can enhance performance in the other.'}, 'zh': {'title': 'è·¨ä½“æ¨¡å‹ï¼Œæ¨åŠ¨AIæ–°é«˜åº¦', 'desc': 'MiMo-Embodiedæ˜¯ä¸€ä¸ªè·¨ä½“æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è‡ªåŠ¨é©¾é©¶å’Œå…·èº«äººå·¥æ™ºèƒ½é¢†åŸŸå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å¤šé˜¶æ®µå­¦ä¹ ã€ç²¾å¿ƒç­–åˆ’çš„æ•°æ®å’ŒCoT/RLå¾®è°ƒï¼ŒMiMo-Embodiedåœ¨17ä¸ªå…·èº«AIåŸºå‡†æµ‹è¯•å’Œ12ä¸ªè‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¨¡å‹æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼€æºã€é—­æºå’Œä¸“ä¸šåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„å¼ºæ­£å‘è¿ç§»å’Œç›¸äº’å¢å¼ºã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•åˆ†æï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15605', 'title': 'SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2511.15605', 'abstract': "Self-Referential Policy Optimization (SRPO) uses latent world representations to assign progress-wise rewards to failed trajectories, improving efficiency and effectiveness in vision-language-action robotic manipulation tasks without external demonstrations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.", 'score': 22, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '736f3c1f4220ad56', 'authors': ['Senyu Fei', 'Siyin Wang', 'Li Ji', 'Ao Li', 'Shiduo Zhang', 'Liming Liu', 'Jinlong Hou', 'Jingjing Gong', 'Xianzhong Zhao', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Tongji University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15605.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#robotics', '#transfer_learning', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Self-Referential Policy Optimization (SRPO) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ² Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ²Ğ¾Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑƒÑĞ¿ĞµÑ…Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ Ğ±Ğ°Ñ‚Ñ‡Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº ÑĞ°Ğ¼Ğ¾ÑÑÑ‹Ğ»ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ 99.2% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 200 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 103% Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚.'}, 'en': {'title': 'Empowering Robots with Self-Referential Learning', 'desc': "Self-Referential Policy Optimization (SRPO) is a new framework that enhances reinforcement learning for vision-language-action robotic tasks by using the robot's own successful experiences to guide learning. Instead of depending on external demonstrations, SRPO assigns rewards to failed attempts based on their progress compared to successful trajectories within the same training batch. This approach utilizes latent world representations to effectively measure behavioral progress, allowing for better learning from both successes and failures. The results show that SRPO significantly improves performance, achieving a 99.2% success rate in just 200 reinforcement learning steps, demonstrating its efficiency and robustness in robotic manipulation tasks."}, 'zh': {'title': 'è‡ªæˆ‘å‚è€ƒç­–ç•¥ä¼˜åŒ–ï¼šæå‡æœºå™¨äººæ“ä½œçš„æ•ˆç‡ä¸æ•ˆæœ', 'desc': 'è‡ªæˆ‘å‚è€ƒç­–ç•¥ä¼˜åŒ–ï¼ˆSRPOï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚SRPOé€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«åœ¨å½“å‰è®­ç»ƒæ‰¹æ¬¡ä¸­ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ¼”ç¤ºçš„éœ€æ±‚ï¼Œä»è€Œä¸ºå¤±è´¥çš„å°è¯•åˆ†é…è¿›åº¦å¥–åŠ±ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ½œåœ¨ä¸–ç•Œè¡¨ç¤ºæ¥ç¨³å¥åœ°è¡¡é‡è¡Œä¸ºè¿›å±•ï¼Œé¿å…äº†ä¾èµ–åŸå§‹åƒç´ æˆ–é¢†åŸŸç‰¹å®šå¾®è°ƒçš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRPOåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†99.2%çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13703', 'title': 'Generalist Foundation Models Are Not Clinical Enough for Hospital Operations', 'url': 'https://huggingface.co/papers/2511.13703', 'abstract': "Lang1, a specialized language model pretrained on clinical data, outperforms generalist models in predicting hospital operational metrics through supervised finetuning and real-world evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.", 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'e8c302be3c784ee7', 'authors': ['Lavender Y. Jiang', 'Angelica Chen', 'Xu Han', 'Xujin Chris Liu', 'Radhika Dua', 'Kevin Eaton', 'Frederick Wolff', 'Robert Steele', 'Jeff Zhang', 'Anton Alyakin', 'Qingkai Pan', 'Yanbing Chen', 'Karl L. Sangwon', 'Daniel A. Alber', 'Jaden Stryker', 'Jin Vivian Lee', 'Yindalon Aphinyanaphongs', 'Kyunghyun Cho', 'Eric Karl Oermann'], 'affiliations': ['Courant Institute School of Mathematics, Computing, and Data Science, New York University', 'Department of Computer Science, ETH Zurich', 'Department of Medicine, NYU Langone Health', 'Department of Neurosurgery, NYU Langone Health', 'Department of Population Health, NYU Langone Health', 'Department of Radiology, NYU Langone Health', 'Department of Surgery, NYU Langone Health', 'Division of Applied AI Technologies, NYU Langone Health', 'Electrical and Computer Engineering, Tandon School of Engineering', 'Global AI Frontier Lab, New York University', 'Grossman School of Medicine, New York University', 'Prescient Design, Genentech', 'School of Global Public Health, New York University', 'School of Medicine, Washington University of St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13703.jpg', 'data': {'categories': ['#training', '#science', '#benchmark', '#open_source', '#transfer_learning', '#dataset', '#small_models', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Lang1 â€” ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ReMedE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±Ğ¾Ğ»ÑŒĞ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ñ, Ğ»ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞŸĞ¾ÑĞ»Ğµ Ñ„Ğ°Ğ·Ğ°-Ñ‚ÑƒĞ¸Ğ½Ğ³Ğ° Lang1-1B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ AUROC Ğ½Ğ° 3.64%-6.75% Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ·Ğ°-Ñ‚ÑƒĞ¸Ğ½Ğ³Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Lang1: Transforming Hospital Operations with Specialized AI', 'desc': 'Lang1 is a specialized language model designed for healthcare, pretrained on a large dataset of clinical data. It significantly outperforms generalist models in predicting important hospital metrics like readmission rates and mortality after being fine-tuned on specific tasks. The model was rigorously tested using the REalistic Medical Evaluation (ReMedE) benchmark, which assesses its performance on various operational tasks. Results show that Lang1 not only excels in direct predictions but also benefits from joint fine-tuning across multiple tasks, demonstrating the importance of specialized training in healthcare AI.'}, 'zh': {'title': 'ä¸“æ³¨äºä¸´åºŠçš„è¯­è¨€æ¨¡å‹ï¼Œæå‡åŒ»é™¢è¿è¥é¢„æµ‹èƒ½åŠ›', 'desc': 'Lang1æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹ä¸´åºŠæ•°æ®é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡ç›‘ç£å¾®è°ƒååœ¨åŒ»é™¢è¿è¥æŒ‡æ ‡é¢„æµ‹æ–¹é¢è¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æ¥è‡ªNYU Langone Healthç”µå­å¥åº·è®°å½•çš„800äº¿ä¸´åºŠæ ‡è®°å’Œ6270äº¿äº’è”ç½‘æ ‡è®°ï¼Œå½¢æˆäº†ä¸€ä¸ªå¼ºå¤§çš„ä¸“ç”¨è¯­æ–™åº“ã€‚é€šè¿‡REalistic Medical Evaluationï¼ˆReMedEï¼‰åŸºå‡†æµ‹è¯•ï¼ŒLang1åœ¨å¤šä¸ªå…³é”®ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨30å¤©å†å…¥é™¢é¢„æµ‹å’Œ30å¤©æ­»äº¡ç‡é¢„æµ‹ç­‰ä»»åŠ¡ä¸­ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸“é—¨çš„è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­èƒ½å¤Ÿä¸é€šç”¨æ¨¡å‹ç«äº‰ï¼Œå¹¶ä¸”æœ‰æ•ˆçš„åŒ»ç–—ç³»ç»ŸAIéœ€è¦ç»“åˆé¢†åŸŸå†…çš„é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’ŒçœŸå®ä¸–ç•Œçš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16528', 'title': 'TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval', 'url': 'https://huggingface.co/papers/2511.16528', 'abstract': 'TurkColBERT evaluates dense encoders and late-interaction models for Turkish information retrieval, demonstrating parameter efficiency and superior performance of late-interaction models with faster indexing.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600times smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5times smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33times faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '569d158ecea14c94', 'authors': ['Ã–zay Ezerceli', 'Mahmoud El Hussieni', 'Selva TaÅŸ', 'Reyhan Bayraktar', 'Fatma BetÃ¼l TerzioÄŸlu', 'Yusuf Ã‡elebi', 'YaÄŸÄ±z Asker'], 'affiliations': ['NewMind AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16528.jpg', 'data': {'categories': ['#inference', '#benchmark', '#open_source', '#low_resource', '#multilingual', '#small_models'], 'emoji': 'ğŸ‡¹ğŸ‡·', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸: Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„Ğ°ĞºÑ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ° Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¼', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ TurkColBERT â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ñ–Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğµ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ½Ğ° Ñ‚ÑƒÑ€ĞµÑ†ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… NLI/STS, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ñ… Ğ² retriever'Ñ‹ ÑÑ‚Ğ¸Ğ»Ñ ColBERT Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PyLate Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° MS MARCO-TR. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ² 3-5 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ², Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¼ Ğ´Ğ»Ñ production-ÑÑ€ĞµĞ´Ñ‹."}, 'en': {'title': 'Unlocking Efficient Turkish Information Retrieval with TurkColBERT', 'desc': 'TurkColBERT is a benchmark study that evaluates dense encoders and late-interaction models specifically for Turkish information retrieval (IR). It highlights the advantages of late-interaction models, which maintain token-level representations for better matching, showing that they can outperform traditional dense encoders while being more parameter-efficient. The study demonstrates that a smaller late-interaction model can achieve comparable performance to a much larger dense encoder, thus enabling faster indexing and retrieval times. The findings suggest that late-interaction models are a promising approach for improving IR in morphologically rich languages like Turkish, although further evaluations on larger datasets are needed.'}, 'zh': {'title': 'åœŸè€³å…¶ä¿¡æ¯æ£€ç´¢çš„æ–°çªç ´ï¼šæ™šæœŸäº¤äº’æ¨¡å‹çš„ä¼˜åŠ¿', 'desc': 'TurkColBERT æ˜¯ä¸€ä¸ªé’ˆå¯¹åœŸè€³å…¶ä¿¡æ¯æ£€ç´¢çš„åŸºå‡†è¯„ä¼°ï¼Œæ¯”è¾ƒäº†ç¨ å¯†ç¼–ç å™¨å’Œæ™šæœŸäº¤äº’æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ™šæœŸäº¤äº’æ¨¡å‹åœ¨å‚æ•°æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºç¨ å¯†ç¼–ç å™¨ï¼Œä¸”å…·æœ‰æ›´å¿«çš„ç´¢å¼•é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µçš„é€‚åº”æµç¨‹ï¼Œå°†è‹±è¯­å’Œå¤šè¯­è¨€ç¼–ç å™¨å¾®è°ƒåˆ°åœŸè€³å…¶çš„è‡ªç„¶è¯­è¨€æ¨ç†å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä»»åŠ¡ä¸Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¾ƒå°çš„æ™šæœŸäº¤äº’æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å¹³å‡ç²¾åº¦ï¼Œä¸”åœ¨ä½å»¶è¿Ÿæ£€ç´¢ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16671', 'title': 'Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation', 'url': 'https://huggingface.co/papers/2511.16671', 'abstract': 'A new framework, Thinking-while-Generating (TwiG), integrates interleaved textual reasoning during visual generation, enhancing context-awareness and semantic richness through zero-shot prompting, supervised fine-tuning, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '02cbba02c8f824ac', 'authors': ['Ziyu Guo', 'Renrui Zhang', 'Hongyu Li', 'Manyuan Zhang', 'Xinyan Chen', 'Sifan Wang', 'Yan Feng', 'Peng Pei', 'Pheng-Ann Heng'], 'affiliations': ['CUHK', 'IMIXR', 'MMLab', 'Meituan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16671.jpg', 'data': {'categories': ['#training', '#multimodal', '#open_source', '#video', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Thinking-while-Generating (TwiG), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞĞ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: zero-shot Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³, supervised fine-tuning Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ TwiG-50K Ğ¸ reinforcement learning Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ TwiG-GRPO. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Enhancing Visual Generation with Real-Time Textual Reasoning', 'desc': 'The paper introduces a new framework called Thinking-while-Generating (TwiG) that enhances visual generation by integrating textual reasoning during the generation process. Unlike previous methods that only apply reasoning before or after generation, TwiG allows for real-time interaction between text and visuals, improving context-awareness and semantic richness. The framework employs techniques such as zero-shot prompting, supervised fine-tuning, and reinforcement learning to optimize the generation process. This innovative approach aims to inspire further research into the benefits of interleaving reasoning with visual content creation.'}, 'zh': {'title': 'æ€ç»´ç”Ÿæˆï¼šæå‡è§†è§‰ç”Ÿæˆçš„è¯­ä¹‰ä¸°å¯Œæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæ€ç»´ç”Ÿæˆï¼ˆTwiGï¼‰ï¼Œå®ƒåœ¨è§†è§‰ç”Ÿæˆè¿‡ç¨‹ä¸­é›†æˆäº†äº¤é”™çš„æ–‡æœ¬æ¨ç†ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é›¶æ ·æœ¬æç¤ºã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºä¸Šä¸‹æ–‡æ„è¯†å’Œè¯­ä¹‰ä¸°å¯Œæ€§ã€‚åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„åŒæ—¶ï¼Œæ–‡æœ¬æ¨ç†ä¸ä¹‹äº¤é”™è¿›è¡Œï¼Œæ—¢æŒ‡å¯¼å³å°†ç”Ÿæˆçš„å±€éƒ¨åŒºåŸŸï¼Œåˆåæ€ä¹‹å‰åˆæˆçš„éƒ¨åˆ†ã€‚è¿™ç§åŠ¨æ€çš„äº’åŠ¨ä½¿å¾—ç”Ÿæˆçš„è§†è§‰è¾“å‡ºæ›´åŠ ç¬¦åˆä¸Šä¸‹æ–‡å¹¶å¯Œæœ‰è¯­ä¹‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16317', 'title': 'NaTex: Seamless Texture Generation as Latent Color Diffusion', 'url': 'https://huggingface.co/papers/2511.16317', 'abstract': 'NaTex generates 3D textures directly using latent color diffusion and geometry-aware models, outperforming previous methods in coherence and alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '5bb48d2f85aec57c', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Zibo Zhao', 'Xin Yang', 'Xin Huang', 'Jingwei Huang', 'Xiangyu Yue', 'Chunchao Guo'], 'affiliations': ['MMLab, CUHK', 'Tencent Hunyuan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16317.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#open_source', '#3d', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€ÑĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ñ†Ğ²ĞµÑ‚Ğ° Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'NaTex â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ†Ğ²ĞµÑ‚Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ ĞºĞ°Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ñ†Ğ²ĞµÑ‚Ğ° Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ latent color diffusion â€” ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ VAE Ğ´Ğ»Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ñ†Ğ²ĞµÑ‚Ğ° Ñ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ native geometry control, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¿Ğ¾ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ ÑĞµÑ‚ĞºĞ¸. NaTex Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Revolutionizing 3D Texture Generation with NaTex', 'desc': 'NaTex is a novel framework for generating 3D textures using latent color diffusion and geometry-aware models. Unlike traditional methods that depend on 2D images, NaTex directly predicts texture colors in 3D space, overcoming challenges like occlusion and alignment. It employs a unique architecture combining a color point cloud VAE and a multi-control diffusion transformer, trained specifically on 3D data. This approach not only enhances texture coherence and alignment but also allows for versatile applications in material generation and texture refinement.'}, 'zh': {'title': 'NaTexï¼šä¸‰ç»´çº¹ç†ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'NaTexæ˜¯ä¸€ç§æ–°çš„çº¹ç†ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨ä¸‰ç»´ç©ºé—´ä¸­é¢„æµ‹çº¹ç†é¢œè‰²ã€‚ä¸ä¹‹å‰ä¾èµ–äºå‡ ä½•æ¡ä»¶çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMVDï¼‰ç”Ÿæˆçš„äºŒç»´å›¾åƒä¸åŒï¼ŒNaTexå…‹æœäº†MVDç®¡é“çš„ä¸€äº›å›ºæœ‰é™åˆ¶ï¼Œå¦‚å¤„ç†é®æŒ¡åŒºåŸŸå’Œå®ç°ç²¾ç¡®çš„ç½‘æ ¼çº¹ç†å¯¹é½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æ½œåœ¨é¢œè‰²æ‰©æ•£æ–¹æ³•ï¼Œç»“åˆäº†å‡ ä½•æ„ŸçŸ¥çš„é¢œè‰²ç‚¹äº‘å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œå¤šæ§åˆ¶æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œå®Œå…¨åŸºäºä¸‰ç»´æ•°æ®è¿›è¡Œè®­ç»ƒã€‚NaTexåœ¨çº¹ç†ä¸€è‡´æ€§å’Œå¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸”å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16595', 'title': 'TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding', 'url': 'https://huggingface.co/papers/2511.16595', 'abstract': 'TimeViper, a hybrid vision-language model using Mamba-Transformer architecture, efficiently processes long videos by transferring and compressing vision tokens to text tokens, achieving state-of-the-art performance while handling over 10,000 frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'ad484db3e18ed053', 'authors': ['Boshen Xu', 'Zihan Xiao', 'Jiaze Li', 'Jianzhong Ju', 'Zhenbo Luo', 'Jian Luan', 'Qin Jin'], 'affiliations': ['AIM3 Lab, Renmin University of China', 'MiLM Plus, Xiaomi Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16595.jpg', 'data': {'categories': ['#long_context', '#interpretability', '#multimodal', '#benchmark', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'TimeViper - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ğ¸ Transformer Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ TransV, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‡Ğ°ÑĞ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 10 000 ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'TimeViper: Revolutionizing Long Video Understanding with Hybrid Architecture', 'desc': 'TimeViper is a new model that combines vision and language processing to understand long videos effectively. It uses a special architecture called Mamba-Transformer, which merges efficient state-space models with powerful attention mechanisms. The model transfers and compresses visual information into text tokens, reducing redundancy and allowing it to handle videos with over 10,000 frames. Through extensive testing, TimeViper shows competitive performance against leading models while providing insights into how hybrid architectures can be interpreted and improved.'}, 'zh': {'title': 'TimeViperï¼šé«˜æ•ˆå¤„ç†é•¿è§†é¢‘çš„æ··åˆæ¨¡å‹', 'desc': 'TimeViperæ˜¯ä¸€ç§æ··åˆè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨Mamba-Transformeræ¶æ„ï¼Œé€šè¿‡å°†è§†è§‰æ ‡è®°è½¬ç§»å’Œå‹ç¼©ä¸ºæ–‡æœ¬æ ‡è®°ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†è¶…è¿‡10,000å¸§çš„é•¿è§†é¢‘ã€‚è¯¥æ¨¡å‹ç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜æ•ˆæ€§å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è¡¨ç°åŠ›ï¼Œå±•ç¤ºäº†è§†è§‰åˆ°æ–‡æœ¬ä¿¡æ¯èšåˆçš„ç°è±¡ã€‚é€šè¿‡å¼•å…¥TransVæ¨¡å—ï¼ŒTimeViperåœ¨ä¿æŒå¤šæ¨¡æ€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼ŒæˆåŠŸåœ°å¤„ç†äº†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16659', 'title': 'PartUV: Part-Based UV Unwrapping of 3D Meshes', 'url': 'https://huggingface.co/papers/2511.16659', 'abstract': "PartUV, a part-based UV unwrapping pipeline, improves chart quality and reduces fragmentation for AI-generated meshes using part decomposition and geometric heuristics.  \t\t\t\t\tAI-generated summary \t\t\t\t UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '72782101f44bd3e2', 'authors': ['Zhaoning Wang', 'Xinyue Wei', 'Ruoxi Shi', 'Xiaoshuai Zhang', 'Hao Su', 'Minghua Liu'], 'affiliations': ['Hillbot Inc.', 'University of California San Diego'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16659.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-ÑĞµÑ‚Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¼Ñ‹ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸', 'desc': 'PartUV â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ UV-ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ ÑĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ÑĞµÑ‚ĞºĞ°Ğ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ ÑĞ²ĞµÑ€Ñ…Ñƒ Ğ²Ğ½Ğ¸Ğ·, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞµ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ñ‹Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑĞµÑ‚Ğ¾Ğº Ğ¸ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ PartUV Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğµ ÑˆĞ²Ğ¾Ğ² Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing UV Unwrapping for AI-Generated Meshes with PartUV', 'desc': 'PartUV is a novel UV unwrapping pipeline designed to enhance the quality of texture mapping for AI-generated 3D meshes. It utilizes part decomposition and geometric heuristics to create fewer, more aligned UV charts while keeping distortion low. By addressing the challenges posed by noisy and poorly conditioned meshes, PartUV minimizes fragmentation and optimizes chart boundaries. The method is efficient and effective, outperforming existing techniques in various datasets, making it suitable for advanced applications in 3D modeling.'}, 'zh': {'title': 'PartUVï¼šæå‡AIç”Ÿæˆç½‘æ ¼çš„UVå±•å¼€è´¨é‡', 'desc': 'PartUVæ˜¯ä¸€ç§åŸºäºéƒ¨ä»¶çš„UVå±•å¼€ç®¡é“ï¼Œæ—¨åœ¨æé«˜AIç”Ÿæˆç½‘æ ¼çš„å›¾è¡¨è´¨é‡å¹¶å‡å°‘ç¢ç‰‡åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡éƒ¨ä»¶åˆ†è§£å’Œå‡ ä½•å¯å‘å¼ç®—æ³•ï¼Œç”Ÿæˆå¯¹é½çš„å›¾è¡¨ï¼ŒåŒæ—¶ä¿æŒä½å¤±çœŸã€‚PartUVç»“åˆäº†é«˜å±‚æ¬¡çš„è¯­ä¹‰éƒ¨ä»¶åˆ†è§£å’Œæ–°é¢–çš„å‡ ä½•å¯å‘å¼ï¼Œç¡®ä¿æ¯ä¸ªå›¾è¡¨çš„å¤±çœŸä½äºç”¨æˆ·æŒ‡å®šçš„é˜ˆå€¼ã€‚ç»è¿‡å¤šç§æ•°æ®é›†çš„è¯„ä¼°ï¼ŒPartUVåœ¨å›¾è¡¨æ•°é‡å’Œæ¥ç¼é•¿åº¦ä¸Šä¼˜äºç°æœ‰å·¥å…·ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç½‘æ ¼å¹¶æ”¯æŒæ–°çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16618', 'title': 'SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking', 'url': 'https://huggingface.co/papers/2511.16618', 'abstract': 'A surgical video segmentation model SAM2S enhances interactive video object segmentation through robust memory, temporal learning, and ambiguity handling, achieving high performance and real-time inference on a comprehensive surgical benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing SAM2 for Surgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average J\\&F over vanilla SAM2. SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '6dec2fa37fa2894a', 'authors': ['Haofeng Liu', 'Ziyue Wang', 'Sudhanshu Mishra', 'Mingqi Gao', 'Guanyi Qin', 'Chang Han Low', 'Alex Y. W. Kong', 'Yueming Jin'], 'affiliations': ['National University of Singapore', 'University of Sheffield'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16618.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#video', '#dataset', '#architecture', '#cv', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SAM2S Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ DiveMem, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SA-SV Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ spatio-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚Ğ¾Ğº 61 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 80.42 ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ J&F, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SAM2 Ğ½Ğ° 17.10 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° 68 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ fine-tuning, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ĞµÑ‘ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Surgical Video Segmentation with SAM2S', 'desc': 'The paper presents SAM2S, an advanced model for surgical video segmentation that improves upon the existing Segment Anything Model 2 (SAM2). It introduces a robust memory mechanism called DiveMem, which enhances long-term tracking of surgical instruments and tissues. The model also incorporates temporal semantic learning to better understand instrument behavior and ambiguity-resilient learning to address inconsistencies in data annotations. Extensive testing shows that SAM2S achieves significant performance improvements while maintaining real-time processing speeds, making it a valuable tool for computer-assisted surgery.'}, 'zh': {'title': 'æ‰‹æœ¯è§†é¢‘åˆ†å‰²çš„æ™ºèƒ½è¿›åŒ–', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSAM2Sçš„æ‰‹æœ¯è§†é¢‘åˆ†å‰²æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¢å¼ºçš„è®°å¿†æœºåˆ¶ã€æ—¶é—´å­¦ä¹ å’Œæ¨¡ç³Šå¤„ç†æ¥æå‡äº¤äº’å¼è§†é¢‘å¯¹è±¡åˆ†å‰²çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨ä¸€ä¸ªå¤§å‹æ‰‹æœ¯åŸºå‡†æ•°æ®é›†SA-SVä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«äº†å¤šç§æ‰‹æœ¯ç±»å‹çš„å®ä¾‹çº§æ—¶ç©ºæ³¨é‡Šï¼Œæ”¯æŒé•¿æœŸè·Ÿè¸ªå’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚é€šè¿‡å¯¹SAM2è¿›è¡Œæ”¹è¿›ï¼ŒSAM2Såœ¨å¤„ç†æ‰‹æœ¯åœºæ™¯ä¸­çš„æŒ‘æˆ˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦å’Œå®æ—¶æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAM2Såœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ï¼Œè¾¾åˆ°äº†80.42çš„å¹³å‡J&Fåˆ†æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15248', 'title': 'EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control', 'url': 'https://huggingface.co/papers/2511.15248', 'abstract': 'EntroPIC, a novel reinforcement learning method, adaptsively tunes loss coefficients to stabilize entropy during long-term training of large language models, ensuring efficient exploration and optimal training.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '8df5879c94dd49eb', 'authors': ['Kai Yang', 'Xin Xu', 'Yangkun Chen', 'Weijie Liu', 'Jiafei Lyu', 'Zichuan Lin', 'Deheng Ye', 'Saiyong Yang'], 'affiliations': ['Tencent Hunyuan', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15248.jpg', 'data': {'categories': ['#training', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ EntroPIC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ĞºĞ°Ğº on-policy, Ñ‚Ğ°Ğº Ğ¸ off-policy Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EntroPIC ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Stabilizing Entropy for Optimal LLM Training with EntroPIC', 'desc': 'EntroPIC is a new reinforcement learning method designed to stabilize entropy during the long-term training of large language models (LLMs). It adaptively tunes the loss coefficients of positive and negative samples to maintain a balanced exploration strategy, preventing the model from settling into sub-optimal behaviors. By controlling entropy, EntroPIC ensures that the training process remains efficient and avoids premature convergence. The method is backed by theoretical analysis and experimental results, demonstrating its effectiveness in both on-policy and off-policy learning scenarios.'}, 'zh': {'title': 'è‡ªé€‚åº”ç†µç¨³å®šåŒ–ï¼Œä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ', 'desc': 'EntroPICæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”è°ƒæ•´æŸå¤±ç³»æ•°æ¥ç¨³å®šå¤§è¯­è¨€æ¨¡å‹çš„ç†µï¼Œä»è€Œç¡®ä¿é•¿æœŸè®­ç»ƒçš„é«˜æ•ˆæ¢ç´¢å’Œæœ€ä½³è®­ç»ƒæ•ˆæœã€‚ç†µåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå®ƒæ§åˆ¶æ¢ç´¢å¹¶å¸®åŠ©é¿å…æ¨¡å‹è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç»´æŒé€‚å½“çš„ç†µæ°´å¹³æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­æ­£è´Ÿæ ·æœ¬çš„æ··åˆä¼šä»¥ä¸åŒæ–¹å¼å½±å“ç†µã€‚EntroPICé€šè¿‡åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬çš„æŸå¤±ç³»æ•°ï¼Œæœ‰æ•ˆåœ°ç¨³å®šäº†ç†µï¼Œç¡®ä¿äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œè¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14865', 'title': 'FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications', 'url': 'https://huggingface.co/papers/2511.14865', 'abstract': 'FinTRec, a transformer-based framework, addresses challenges in financial services recommendation systems by handling long-range interactions and multiple products, outperforming traditional tree-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'bb38cb6319c93fb1', 'authors': ['Dwipam Katariya', 'Snehita Varma', 'Akshat Shreemali', 'Benjamin Wu', 'Kalanand Mishra', 'Pranab Mohanty'], 'affiliations': ['Capital One, AI Foundations'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14865.jpg', 'data': {'categories': [], 'emoji': 'ğŸ’°', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ²: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ñ…', 'desc': 'FinTRec â€” ÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ñ†ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ñ€ĞµĞ²ĞµÑĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ»Ğ¸ÑÑŒ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ ÑĞµĞºÑ‚Ğ¾Ñ€Ğµ Ğ·Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ³ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼, FinTRec Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ğ¼Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»Ğµ ÑƒÑĞ»ÑƒĞ³.'}, 'en': {'title': 'Transforming Financial Recommendations with FinTRec', 'desc': 'FinTRec is a transformer-based framework designed to improve recommendation systems in financial services by effectively managing long-range user interactions and multiple related products. It tackles the unique challenges of real-time recommendations, such as varying user behaviors across digital and physical channels. Unlike traditional tree-based models, which are favored for their explainability, FinTRec demonstrates superior performance through its ability to share signals across products and reduce training costs. This study is significant as it represents the first thorough exploration of unified sequential recommendation modeling in the financial sector, balancing both technical and business needs.'}, 'zh': {'title': 'FinTRecï¼šé‡‘èæœåŠ¡æ¨èçš„æ–°å˜é©', 'desc': 'FinTRecæ˜¯ä¸€ä¸ªåŸºäºå˜æ¢å™¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é‡‘èæœåŠ¡æ¨èç³»ç»Ÿä¸­çš„æŒ‘æˆ˜ã€‚å®ƒèƒ½å¤Ÿå¤„ç†é•¿æ—¶é—´çš„ç”¨æˆ·äº¤äº’å’Œå¤šä¸ªç›¸å…³äº§å“ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ ‘æ¨¡å‹ã€‚é€šè¿‡å†å²æ¨¡æ‹Ÿå’Œå®æ—¶A/Bæµ‹è¯•ï¼ŒFinTRecåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰çš„æ ‘æ¨¡å‹ã€‚è¯¥æ¡†æ¶çš„ç»Ÿä¸€æ¶æ„å¯ä»¥å®ç°è·¨äº§å“ä¿¡å·å…±äº«ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶æé«˜æ‰€æœ‰äº§å“çš„ç¦»çº¿è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16315', 'title': 'BioBench: A Blueprint to Move Beyond ImageNet for Scientific ML Benchmarks', 'url': 'https://huggingface.co/papers/2511.16315', 'abstract': 'BioBench is an open ecology vision benchmark that addresses the limitations of ImageNet-1K accuracy for scientific imagery by evaluating models on a diverse set of ecological tasks and modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t ImageNet-1K linear-probe transfer accuracy remains the default proxy for visual representation quality, yet it no longer predicts performance on scientific imagery. Across 46 modern vision model checkpoints, ImageNet top-1 accuracy explains only 34% of variance on ecology tasks and mis-ranks 30% of models above 75% accuracy. We present BioBench, an open ecology vision benchmark that captures what ImageNet misses. BioBench unifies 9 publicly released, application-driven tasks, 4 taxonomic kingdoms, and 6 acquisition modalities (drone RGB, web video, micrographs, in-situ and specimen photos, camera-trap frames), totaling 3.1M images. A single Python API downloads data, fits lightweight classifiers to frozen backbones, and reports class-balanced macro-F1 (plus domain metrics for FishNet and FungiCLEF); ViT-L models evaluate in 6 hours on an A6000 GPU. BioBench provides new signal for computer vision in ecology and a template recipe for building reliable AI-for-science benchmarks in any domain. Code and predictions are available at https://github.com/samuelstevens/biobench and results at https://samuelstevens.me/biobench.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '91a3a52765e44522', 'authors': ['Samuel Stevens'], 'affiliations': ['The Ohio State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16315.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#open_source', '#transfer_learning', '#dataset', '#cv'], 'emoji': 'ğŸ¦', 'ru': {'title': 'ImageNet Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡ĞµĞ½: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'BioBench â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ImageNet-1K Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ImageNet Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 34% Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒĞµÑ‚ 30% Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 9 Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, 4 Ñ†Ğ°Ñ€ÑÑ‚Ğ²Ğ° Ğ¶Ğ¸Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ğ¸ 6 Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ´Ñ€Ğ¾Ğ½Ñ‹, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸, Ñ„Ğ¾Ñ‚Ğ¾ Ğ² Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğµ Ğ¸ ÑĞºÑĞ¿Ğ¾Ğ½Ğ°Ñ‚Ğ¾Ğ²), Ğ²ÑĞµĞ³Ğ¾ 3,1 Ğ¼Ğ»Ğ½ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. BioBench Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Python API Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ»ÑƒĞ¶Ğ° ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² AI Ğ´Ğ»Ñ Ğ½Ğ°ÑƒĞºĞ¸.'}, 'en': {'title': 'BioBench: Elevating Ecological AI Evaluation Beyond ImageNet', 'desc': 'BioBench is a new benchmark designed to improve the evaluation of machine learning models in ecological tasks, addressing the shortcomings of traditional metrics like ImageNet-1K accuracy. It demonstrates that ImageNet accuracy is not a reliable indicator of performance for scientific imagery, as it only explains a small portion of the variance in ecological tasks. The benchmark includes a diverse dataset of 3.1 million images across various ecological tasks and modalities, allowing for a more comprehensive assessment of model performance. BioBench also provides a user-friendly Python API for easy access to data and evaluation metrics, paving the way for better AI applications in scientific research.'}, 'zh': {'title': 'BioBenchï¼šç”Ÿæ€è§†è§‰çš„æ–°åŸºå‡†', 'desc': 'BioBenchæ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç”Ÿæ€è§†è§‰åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ImageNet-1Kåœ¨ç§‘å­¦å›¾åƒä¸Šçš„å‡†ç¡®æ€§å±€é™æ€§ã€‚å®ƒé€šè¿‡è¯„ä¼°æ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ç”Ÿæ€ä»»åŠ¡å’Œæ¨¡æ€ä¸Šçš„è¡¨ç°ï¼Œæä¾›äº†æ¯”ImageNetæ›´å…¨é¢çš„è¯„ä¼°æ ‡å‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒImageNetçš„å‡†ç¡®æ€§ä»…èƒ½è§£é‡Šç”Ÿæ€ä»»åŠ¡ä¸­34%çš„æ–¹å·®ï¼Œä¸”é”™è¯¯æ’åäº†30%çš„é«˜å‡†ç¡®æ€§æ¨¡å‹ã€‚BioBenchæ•´åˆäº†9ä¸ªå…¬å¼€å‘å¸ƒçš„åº”ç”¨é©±åŠ¨ä»»åŠ¡ï¼Œæ¶µç›–4ä¸ªåˆ†ç±»ç‹å›½å’Œ6ç§è·å–æ¨¡æ€ï¼Œæä¾›äº†310ä¸‡å¼ å›¾åƒï¼Œæ¨åŠ¨äº†ç”Ÿæ€è®¡ç®—æœºè§†è§‰çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11005', 'title': 'Draft and Refine with Visual Experts', 'url': 'https://huggingface.co/papers/2511.11005', 'abstract': "The Draft and Refine (DnR) agent framework uses a question-conditioned utilization metric to improve visual grounding in large vision-language models, reducing hallucinations and increasing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent Large Vision-Language Models (LVLMs) exhibit strong multimodal reasoning abilities, they often produce ungrounded or hallucinated responses because they rely too heavily on linguistic priors instead of visual evidence. This limitation highlights the absence of a quantitative measure of how much these models actually use visual information during reasoning. We propose Draft and Refine (DnR), an agent framework driven by a question-conditioned utilization metric. The metric quantifies the model's reliance on visual evidence by first constructing a query-conditioned relevance map to localize question-specific cues and then measuring dependence through relevance-guided probabilistic masking. Guided by this metric, the DnR agent refines its initial draft using targeted feedback from external visual experts. Each expert's output (such as boxes or masks) is rendered as visual cues on the image, and the model is re-queried to select the response that yields the largest improvement in utilization. This process strengthens visual grounding without retraining or architectural changes. Experiments across VQA and captioning benchmarks show consistent accuracy gains and reduced hallucination, demonstrating that measuring visual utilization provides a principled path toward more interpretable and evidence-driven multimodal agent systems.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '910ca618d1ec39f7', 'authors': ['Sungheon Jeong', 'Ryozo Masukawa', 'Jihong Park', 'Sanggeon Yun', 'Wenjun Huang', 'Hanning Chen', 'Mahdi Imani', 'Mohsen Imani'], 'affiliations': ['MOLOCO', 'Northeastern University', 'University of California, Irvine'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11005.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#benchmark', '#hallucinations', '#agents', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Draft and Refine (DnR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ ĞµĞ³Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… VQA Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Visual Grounding with Draft and Refine', 'desc': 'The Draft and Refine (DnR) agent framework enhances visual grounding in large vision-language models by using a question-conditioned utilization metric. This metric assesses how much the model relies on visual information rather than just linguistic cues, addressing the issue of hallucinations in responses. By creating relevance maps and applying probabilistic masking, the DnR agent can refine its initial outputs based on feedback from visual experts. This approach leads to improved accuracy and reduced hallucinations in tasks like visual question answering and image captioning, all without needing to retrain the model or change its architecture.'}, 'zh': {'title': 'æå‡è§†è§‰å®šä½ï¼Œå‡å°‘å¹»è§‰çš„DnRæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè‰æ‹Ÿä¸ç²¾ç‚¼ï¼ˆDnRï¼‰çš„ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é—®é¢˜æ¡ä»¶çš„åˆ©ç”¨åº¦æŒ‡æ ‡æ¥æ”¹å–„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰å®šä½èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºæŸ¥è¯¢æ¡ä»¶çš„ç›¸å…³æ€§å›¾ï¼Œé‡åŒ–æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ç¨‹åº¦ã€‚DnRä»£ç†åœ¨åˆæ­¥è‰æ‹Ÿçš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å¤–éƒ¨è§†è§‰ä¸“å®¶çš„åé¦ˆè¿›è¡Œç²¾ç‚¼ï¼Œä»è€Œå¢å¼ºè§†è§‰åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä»»åŠ¡ä¸­å‡æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15943', 'title': 'Boosting Medical Visual Understanding From Multi-Granular Language Learning', 'url': 'https://huggingface.co/papers/2511.15943', 'abstract': 'Multi-Granular Language Learning (MGLL) enhances visual understanding by improving multi-label and cross-granularity alignment in image-text pretraining, outperforming existing methods in complex domains like medical imaging.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '35db7d8cd745e0d0', 'authors': ['Zihan Li', 'Yiqing Wang', 'Sina Farsiu', 'Paul Kinahan'], 'affiliations': ['Duke University', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15943.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#healthcare'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Multi-Granular Language Learning (MGLL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MGLL Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIP, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ»Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ÑĞ³ĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MGLL Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Visual Understanding with Multi-Granular Language Learning', 'desc': 'Multi-Granular Language Learning (MGLL) is a novel framework that enhances visual understanding by improving the alignment of multi-label and cross-granularity data in image-text pretraining. Unlike traditional methods that focus on single-label alignment, MGLL addresses the complexities of domains like medical imaging, where images can have multiple labels at different levels of detail. It utilizes structured multi-label supervision and integrates various textual descriptions to achieve better alignment. By employing smooth Kullback-Leibler divergence, MGLL ensures consistency across different granularities while being efficient and easy to integrate into existing vision-language models.'}, 'zh': {'title': 'å¤šç²’åº¦å­¦ä¹ ï¼Œæå‡è§†è§‰ç†è§£ï¼', 'desc': 'å¤šç²’åº¦è¯­è¨€å­¦ä¹ ï¼ˆMGLLï¼‰é€šè¿‡æ”¹å–„å›¾åƒ-æ–‡æœ¬é¢„è®­ç»ƒä¸­çš„å¤šæ ‡ç­¾å’Œè·¨ç²’åº¦å¯¹é½ï¼Œå¢å¼ºäº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤æ‚é¢†åŸŸå¦‚åŒ»å­¦å½±åƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚MGLLé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„å¤šæ ‡ç­¾ç›‘ç£ï¼Œæ•´åˆä¸åŒç²’åº¦çš„æ–‡æœ¬æè¿°ï¼Œå¹¶å¼•å…¥è½¯æ ‡ç­¾ç›‘ç£ä»¥å¢å¼ºå¯¹é½æ•ˆæœã€‚è¯¥æ–¹æ³•ä½¿ç”¨å¹³æ»‘çš„Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ï¼Œç¡®ä¿è·¨ç²’åº¦çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œé€‚ç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹ã€‚ç»è¿‡åœ¨å¤§è§„æ¨¡å¤šç²’åº¦æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒï¼ŒMGLLåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16719', 'title': 'SAM 3: Segment Anything with Concepts', 'url': 'https://huggingface.co/papers/2511.16719', 'abstract': 'Segment Anything Model 3 achieves state-of-the-art performance in promptable concept segmentation and tracking by leveraging a unified model architecture with decoupled recognition and localization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., "yellow school bus"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.', 'score': 109, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'd064ac8ac0205cab', 'authors': ['Nicolas Carion', 'Laura Gustafson', 'Yuan-Ting Hu', 'Shoubhik Debnath', 'Ronghang Hu', 'Didac Suris', 'Chaitanya Ryali', 'Kalyan Vasudev Alwala', 'Haitham Khedr', 'Andrew Huang', 'Jie Lei', 'Tengyu Ma', 'Baishan Guo', 'Arpit Kalla', 'Markus Marks', 'Joseph Greer', 'Meng Wang', 'Peize Sun', 'Roman RÃ¤dle', 'Triantafyllos Afouras', 'Effrosyni Mavroudi', 'Katherine Xu', 'Tsung-Han Wu', 'Yu Zhou', 'Liliane Momeni', 'Rishi Hazra', 'Shuangrui Ding', 'Sagar Vaze', 'Francois Porcher', 'Feng Li', 'Siyuan Li', 'Aishwarya Kamath', 'Ho Kei Cheng', 'Piotr DollÃ¡r', 'Nikhila Ravi', 'Kate Saenko', 'Pengchuan Zhang', 'Christoph Feichtenhofer'], 'affiliations': ['Meta Superintelligence Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16719.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#video', '#dataset', '#architecture', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Segment Anything Model 3 â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ presence head, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚-Ğ»ĞµĞ¹Ğ±Ğ»Ğ¾Ğ² Ñ Ñ‚Ñ€ÑƒĞ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. SAM 3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Object Segmentation with SAM 3', 'desc': 'The Segment Anything Model 3 (SAM 3) is a cutting-edge machine learning model designed for promptable concept segmentation and tracking in images and videos. It utilizes a unified architecture that separates the tasks of recognizing objects and localizing them, enhancing detection accuracy. SAM 3 processes various prompts, such as noun phrases and image examples, to generate precise segmentation masks and unique identities for detected objects. With a robust dataset of 4 million unique concept labels, SAM 3 significantly outperforms previous models in both image and video segmentation tasks.'}, 'zh': {'title': 'ç»Ÿä¸€æ¨¡å‹ï¼Œç²¾å‡†åˆ†å‰²ä¸è·Ÿè¸ª', 'desc': 'Segment Anything Model 3ï¼ˆSAM 3ï¼‰æ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ¦‚å¿µæç¤ºåœ¨å›¾åƒå’Œè§†é¢‘ä¸­æ£€æµ‹ã€åˆ†å‰²å’Œè·Ÿè¸ªå¯¹è±¡ã€‚è¯¥æ¨¡å‹é€šè¿‡è§£è€¦è¯†åˆ«å’Œå®šä½ï¼Œæå‡äº†æç¤ºå¼æ¦‚å¿µåˆ†å‰²ï¼ˆPCSï¼‰çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿè¿”å›åˆ†å‰²æ©ç å’Œå”¯ä¸€èº«ä»½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œç”Ÿæˆäº†åŒ…å«400ä¸‡ä¸ªç‹¬ç‰¹æ¦‚å¿µæ ‡ç­¾çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚SAM 3åœ¨å›¾åƒå’Œè§†é¢‘çš„PCSä»»åŠ¡ä¸­å®ç°äº†ç°æœ‰ç³»ç»Ÿçš„ä¸¤å€å‡†ç¡®ç‡ï¼Œå¹¶æ”¹è¿›äº†ä¹‹å‰SAMåœ¨è§†è§‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15705', 'title': 'GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization', 'url': 'https://huggingface.co/papers/2511.15705', 'abstract': 'GeoVista, an agentic model integrating tool invocation and reinforcement learning, achieves high geolocalization performance on GeoBench, outperforming open-source models and matching closed-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.', 'score': 92, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'b7e22103dfcb9f5e', 'authors': ['Yikun Wang', 'Zuyan Liu', 'Ziyi Wang', 'Pengfei Liu', 'Han Hu', 'Yongming Rao'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Tencent Hunyuan', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15705.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#benchmark', '#dataset', '#agents', '#rl', '#reasoning', '#cv'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GeoVista â€” Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GeoBench Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ°Ğ¼Ğ¸ ÑĞ¾ Ğ²ÑĞµĞ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°: ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ GeoVista Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'GeoVista: Elevating Geolocalization with Agentic Intelligence', 'desc': 'GeoVista is an advanced agentic model designed for geolocalization tasks, combining tool invocation with reinforcement learning to enhance performance. It addresses the limitations of existing benchmarks by introducing GeoBench, which includes high-resolution images and satellite data for rigorous evaluation. The model employs a two-stage training process, starting with supervised fine-tuning to establish reasoning patterns, followed by reinforcement learning to refine its capabilities. Experimental results demonstrate that GeoVista outperforms open-source models and competes effectively with closed-source counterparts in geolocalization accuracy.'}, 'zh': {'title': 'GeoVistaï¼šæ™ºèƒ½æ¨¡å‹çš„æ–°é«˜åº¦', 'desc': 'GeoVistaæ˜¯ä¸€ç§ç»“åˆå·¥å…·è°ƒç”¨å’Œå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½æ¨¡å‹ï¼Œåœ¨GeoBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸é—­æºæ¨¡å‹ç›¸å½“ã€‚è¯¥ç ”ç©¶é‡æ–°å®¡è§†äº†åœ°ç†å®šä½ä»»åŠ¡ï¼Œå¼ºè°ƒäº†è§†è§‰åŸºç¡€å’Œç½‘ç»œæœç´¢åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚ä¸ºäº†è¯„ä¼°æ™ºèƒ½æ¨¡å‹çš„åœ°ç†å®šä½èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†GeoBenchåŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªä¸–ç•Œå„åœ°çš„é«˜åˆ†è¾¨ç‡ç…§ç‰‡å’Œå…¨æ™¯å›¾ã€‚GeoVistaé€šè¿‡åœ¨æ¨ç†å¾ªç¯ä¸­æ— ç¼é›†æˆå·¥å…·è°ƒç”¨ï¼Œé‡‡ç”¨åˆ†å±‚å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†åœ°ç†å®šä½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16334', 'title': 'OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe', 'url': 'https://huggingface.co/papers/2511.16334', 'abstract': 'OpenMMReasoner, a two-stage training approach combining supervised fine-tuning and reinforcement learning, enhances multimodal reasoning performance through rigorous data curation and improved training strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.', 'score': 91, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'c390ad0bbfe8688a', 'authors': ['Kaichen Zhang', 'Keming Wu', 'Zuhao Yang', 'Bo Li', 'Kairui Hu', 'Bin Wang', 'Ziwei Liu', 'Xingxuan Li', 'Lidong Bing'], 'affiliations': ['LMMs-Lab Team', 'MiroMind AI', 'Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16334.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#benchmark', '#open_source', '#data', '#dataset', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'OpenMMReasoner Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ supervised fine-tuning Ğ¸ reinforcement learning. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ SFT ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ¼ Ğ¸Ğ· 874K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ RL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 74K Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with OpenMMReasoner', 'desc': "OpenMMReasoner is a novel approach that enhances multimodal reasoning by combining supervised fine-tuning and reinforcement learning in a two-stage training process. The first stage involves creating a large, well-validated dataset to establish a strong foundation for reasoning tasks. In the second stage, reinforcement learning is applied to refine and stabilize the model's capabilities using a diverse dataset. This method not only improves performance significantly over existing models but also emphasizes the importance of data quality and training strategies in multimodal reasoning."}, 'zh': {'title': 'OpenMMReasonerï¼šæå‡å¤šæ¨¡æ€æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'OpenMMReasoneræ˜¯ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†å’Œæ”¹è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«874Kæ ·æœ¬çš„å†·å¯åŠ¨æ•°æ®é›†ï¼Œä¸ºæ¨ç†èƒ½åŠ›æä¾›äº†åšå®åŸºç¡€ã€‚éšåï¼Œåˆ©ç”¨74Kæ ·æœ¬çš„æ•°æ®é›†è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥å¢å¼ºå’Œç¨³å®šè¿™äº›èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®­ç»ƒæ–¹æ³•åœ¨ä¹ä¸ªå¤šæ¨¡æ€æ¨ç†åŸºå‡†ä¸Šè¶…è¶Šäº†å¼ºåŸºçº¿ï¼Œå¼ºè°ƒäº†æ•°æ®è´¨é‡å’Œè®­ç»ƒè®¾è®¡åœ¨å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15210', 'title': 'Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story', 'url': 'https://huggingface.co/papers/2511.15210', 'abstract': 'The study explores intrinsic dimension in large language models through cross-encoder analysis, linguistic features, and sparse autoencoders, revealing its independence from entropy, genre-specific stratification, and causal features related to text type.  \t\t\t\t\tAI-generated summary \t\t\t\t Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.', 'score': 87, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '137843b8ae93cd3b', 'authors': ['Vladislav Pedashenko', 'Laida Kushnareva', 'Yana Khassan Nibal', 'Eduard Tulchinskii', 'Kristian Kuznetsov', 'Vladislav Zharchinskii', 'Yury Maximov', 'Irina Piontkovskaya'], 'affiliations': ['Interdata Astana', 'Lomonosov Research Institute', 'Moscow State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15210.jpg', 'data': {'categories': ['#architecture', '#training', '#interpretability'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ LLM: Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° Ğ½Ğ°ÑƒĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ° Ğ¾Ñ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¶Ğ°Ğ½Ñ€-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ: Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ (~8), ÑĞ½Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¿ĞµĞ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ ÑÑ€ĞµĞ´Ğ½ÑÑ (~9), Ğ° Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ (~10.5). Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸: Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞµÑ‘.'}, 'en': {'title': 'Understanding Intrinsic Dimension: The Key to Language Model Complexity', 'desc': 'This paper investigates the concept of intrinsic dimension (ID) in large language models (LLMs) using various analytical methods. It finds that ID is independent of entropy and varies significantly across different text genres, with scientific writing being simpler and requiring less representational complexity than creative writing. The study also identifies specific linguistic features that influence ID, showing that formal and statistical elements lower ID while personal and emotional elements raise it. Overall, the research offers valuable insights into how ID can be effectively utilized and interpreted in the context of LLMs.'}, 'zh': {'title': 'æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹çš„å†…åœ¨ç»´åº¦', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å†…åœ¨ç»´åº¦ï¼Œé€šè¿‡äº¤å‰ç¼–ç å™¨åˆ†æã€è¯­è¨€ç‰¹å¾å’Œç¨€ç–è‡ªç¼–ç å™¨ï¼Œæ­ç¤ºäº†å…¶ä¸ç†µã€ç‰¹å®šç±»å‹çš„åˆ†ç±»å’Œæ–‡æœ¬ç±»å‹ç›¸å…³çš„å› æœç‰¹å¾çš„ç‹¬ç«‹æ€§ã€‚å†…åœ¨ç»´åº¦ï¼ˆIDï¼‰æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹åˆ†æä¸­çš„é‡è¦å·¥å…·ï¼Œå½±å“è®­ç»ƒåŠ¨æ€ã€æ‰©å±•è¡Œä¸ºå’Œæ•°æ®é›†ç»“æ„ã€‚ç ”ç©¶å‘ç°ï¼ŒIDä¸ç†µåº¦é‡äº’è¡¥ï¼Œä¸”åœ¨æ§åˆ¶æ–‡æœ¬é•¿åº¦åï¼Œä¸¤è€…æ— ç›¸å…³æ€§ï¼ŒIDæ•æ‰å‡ ä½•å¤æ‚æ€§ã€‚é€šè¿‡ç¨€ç–è‡ªç¼–ç å™¨ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºç§‘å­¦ä¿¡å·é™ä½IDï¼Œè€Œäººæ€§åŒ–ä¿¡å·åˆ™å¢åŠ IDï¼Œè¡¨æ˜ç§‘å­¦å†™ä½œç›¸å¯¹â€œç®€å•â€ï¼Œè€Œå°è¯´å’Œæƒ…æ„Ÿå†™ä½œåˆ™éœ€è¦æ›´å¤šçš„è¡¨ç°è‡ªç”±åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17502', 'title': 'RynnVLA-002: A Unified Vision-Language-Action and World Model', 'url': 'https://huggingface.co/papers/2511.17502', 'abstract': "A unified Vision-Language-Action (VLA) and world model, RynnVLA-002, jointly learns environmental dynamics and action planning, outperforming individual models in both simulation and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.", 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'ab117a1cbfe298cd', 'authors': ['Jun Cen', 'Siteng Huang', 'Yuqian Yuan', 'Kehan Li', 'Hangjie Yuan', 'Chaohui Yu', 'Yuming Jiang', 'Jiayan Guo', 'Xin Li', 'Hao Luo', 'Fan Wang', 'Deli Zhao', 'Hao Chen'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17502.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#robotics', '#architecture', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ', 'desc': 'RynnVLA-002 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Vision-Language-Action Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ world model Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. World model Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº VLA ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ (97.4% Ğ½Ğ° LIBERO), Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ³Ğ´Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ world model Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 50%.'}, 'en': {'title': 'Unified Learning for Enhanced Action and Vision Understanding', 'desc': "RynnVLA-002 is a novel model that combines Vision-Language-Action (VLA) capabilities with a world model to improve how machines understand and interact with their environments. It learns to predict future visual states based on actions and visual inputs, effectively capturing the dynamics of the environment. The VLA component enhances action generation by interpreting visual data, which in turn supports the world model's ability to generate accurate images. This integrated approach leads to superior performance in both simulated and real-world tasks, significantly outperforming traditional models."}, 'zh': {'title': 'ç»Ÿä¸€æ¨¡å‹ï¼Œæå‡æ™ºèƒ½è¡ŒåŠ¨ï¼', 'desc': 'RynnVLA-002æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å’Œä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿå…±åŒå­¦ä¹ ç¯å¢ƒåŠ¨æ€å’ŒåŠ¨ä½œè§„åˆ’ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§†è§‰å’ŒåŠ¨ä½œè¾“å…¥æ¥é¢„æµ‹æœªæ¥çš„å›¾åƒçŠ¶æ€ï¼Œä»è€Œå­¦ä¹ ç¯å¢ƒçš„åŸºæœ¬ç‰©ç†è§„å¾‹ã€‚VLAæ¨¡å‹åˆ™æ ¹æ®å›¾åƒè§‚å¯Ÿç”Ÿæˆåç»­åŠ¨ä½œï¼Œå¢å¼ºè§†è§‰ç†è§£å¹¶æ”¯æŒä¸–ç•Œæ¨¡å‹çš„å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRynnVLA-002åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å‡ä¼˜äºå•ç‹¬çš„VLAå’Œä¸–ç•Œæ¨¡å‹ï¼ŒæˆåŠŸç‡æ˜¾è‘—æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13593', 'title': 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents', 'url': 'https://huggingface.co/papers/2511.13593', 'abstract': 'O-Mem, an active user profiling memory framework, enhances contextual consistency and dynamic personalization in LLM-powered agents, improving performance on benchmarks and response efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.', 'score': 24, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '491a11d6cfd4e4ab', 'authors': ['Piaohong Wang', 'Motong Tian', 'Jiaxian Li', 'Yuan Liang', 'Yuqing Wang', 'Qianben Chen', 'Tiannan Wang', 'Zhicong Lu', 'Jiawei Ma', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['oppo.com'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13593.jpg', 'data': {'categories': ['#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ñ Ğ˜Ğ˜', 'desc': 'O-Mem â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹. O-Mem Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LoCoMo (51.67%) Ğ¸ PERSONAMEM (62.99%), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ state-of-the-art Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 3-3.5%. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'O-Mem: Enhancing AI Personalization Through Active User Profiling', 'desc': 'O-Mem is a new memory framework designed to improve how AI agents interact with users by focusing on active user profiling. It addresses issues of contextual consistency and personalization by dynamically updating user information based on their interactions. Unlike traditional memory systems that may miss important details, O-Mem retrieves relevant user attributes and context in a hierarchical manner. This leads to more coherent and personalized responses, achieving significant performance improvements on benchmark tests compared to previous models.'}, 'zh': {'title': 'O-Memï¼šæå‡ä¸ªæ€§åŒ–å“åº”çš„æ™ºèƒ½è®°å¿†æ¡†æ¶', 'desc': 'O-Memæ˜¯ä¸€ç§åŸºäºä¸»åŠ¨ç”¨æˆ·ç”»åƒçš„è®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’ŒåŠ¨æ€ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€æå–å’Œæ›´æ–°ç”¨æˆ·ç‰¹å¾åŠäº‹ä»¶è®°å½•ï¼Œæ”¯æŒå±‚æ¬¡åŒ–çš„ä¸ªæ€§å±æ€§å’Œä¸»é¢˜ç›¸å…³ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œä»è€Œå®ç°æ›´é€‚åº”å’Œè¿è´¯çš„ä¸ªæ€§åŒ–å“åº”ã€‚O-Memåœ¨å…¬å…±LoCoMoåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†51.67%çš„æˆç»©ï¼Œæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æŠ€æœ¯LangMemæé«˜äº†è¿‘3%ã€‚æ­¤å¤–ï¼ŒO-Memåœ¨å“åº”æ—¶é—´æ•ˆç‡ä¸Šä¹Ÿä¼˜äºä¹‹å‰çš„è®°å¿†æ¡†æ¶ï¼Œä¸ºæœªæ¥å¼€å‘é«˜æ•ˆä¸”ç±»äººåŒ–çš„ä¸ªæ€§åŒ–AIåŠ©æ‰‹å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17490', 'title': 'Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination', 'url': 'https://huggingface.co/papers/2511.17490', 'abstract': 'Video-R4, a video reasoning LMM, uses iterative visual rumination to improve text-rich video QA by selecting, zooming, and re-encoding frames, achieving state-of-the-art results on various QA tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.', 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '708e158d9291bf36', 'authors': ['Yolo Y. Tang', 'Daiki Shimada', 'Hang Hua', 'Chao Huang', 'Jing Bi', 'Rogerio Feris', 'Chenliang Xu'], 'affiliations': ['MIT-IBM Watson AI Lab', 'Sony Group Corporation', 'University of Rochester'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17490.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#dataset', '#hallucinations', '#rl', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Video-R4 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ĞºĞ°Ğ´Ñ€Ñ‹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ ÑĞ²Ğ¾Ñ‘ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ reinforcement learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ°Ğ¹Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Iterative Visual Rumination for Enhanced Video QA', 'desc': 'Video-R4 is a video reasoning language model that enhances question answering (QA) for text-rich videos by using a technique called visual rumination. This method involves iteratively selecting and zooming into important frames, allowing the model to re-encode visual information and refine its understanding. Unlike traditional models that analyze fixed frames in a single pass, Video-R4 mimics human behavior by revisiting and inspecting critical visual cues, which helps reduce errors and improve accuracy. The model has been trained on specialized datasets and achieves top performance on various QA tasks, showcasing the effectiveness of its iterative approach to multimodal reasoning.'}, 'zh': {'title': 'è¿­ä»£è§†è§‰åæ€ï¼Œæå‡è§†é¢‘é—®ç­”èƒ½åŠ›', 'desc': 'Video-R4æ˜¯ä¸€ç§è§†é¢‘æ¨ç†çš„è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨è¿­ä»£è§†è§‰åæ€çš„æ–¹æ³•æ¥æå‡æ–‡æœ¬ä¸°å¯Œè§†é¢‘çš„é—®ç­”èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡é€‰æ‹©ã€æ”¾å¤§å’Œé‡æ–°ç¼–ç è§†é¢‘å¸§ï¼Œè§£å†³äº†ä¼ ç»Ÿè§†é¢‘é—®ç­”æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦è¯æ®æ—¶çš„ä¸è¶³ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«ç”¨äºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ”¯æŒæ¨¡å‹çš„è®­ç»ƒã€‚Video-R4åœ¨å¤šä¸ªé—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†è¿­ä»£åæ€åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16825', 'title': 'WorldGen: From Text to Traversable and Interactive 3D Worlds', 'url': 'https://huggingface.co/papers/2511.16825', 'abstract': 'WorldGen converts text prompts into interactive 3D environments using LLM-driven reasoning, procedural generation, diffusion-based 3D generation, and object-aware decomposition, enabling creators to build coherent, navigable worlds efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.', 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '507477860f14b56d', 'authors': ['Dilin Wang', 'Hyunyoung Jung', 'Tom Monnier', 'Kihyuk Sohn', 'Chuhang Zou', 'Xiaoyu Xiang', 'Yu-Ying Yeh', 'Di Liu', 'Zixuan Huang', 'Thu Nguyen-Phuoc', 'Yuchen Fan', 'Sergiu Oprea', 'Ziyan Wang', 'Roman Shapovalov', 'Nikolaos Sarafianos', 'Thibault Groueix', 'Antoine Toisoul', 'Prithviraj Dhar', 'Xiao Chu', 'Minghao Chen', 'Geon Yeong Park', 'Mahima Gupta', 'Yassir Azziz', 'Rakesh Ranjan', 'Andrea Vedaldi'], 'affiliations': ['Reality Labs, Meta'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16825.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#3d', '#games', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ‚ ÑĞ»Ğ¾Ğ²Ğ° Ğº Ğ¼Ğ¸Ñ€Ñƒ: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… 3D-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'WorldGen â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¸Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ€Ğ°Ğ·Ñƒ Ğ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°Ñ…. ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¼, Ğ´ĞµĞ»Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming Text into Interactive 3D Worlds', 'desc': 'WorldGen is a system that automatically creates interactive 3D environments from text prompts. It uses large language models (LLMs) to understand scene layouts and combines this with procedural generation and diffusion techniques to create detailed 3D worlds. The system allows creators to easily design and navigate these environments without needing advanced 3D modeling skills. By providing modular control over various aspects of the world, WorldGen makes it easier to generate rich, coherent virtual spaces for gaming and simulations.'}, 'zh': {'title': 'æ–‡æœ¬åˆ°3Dä¸–ç•Œçš„è‡ªåŠ¨ç”Ÿæˆ', 'desc': 'WorldGen æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºè‡ªåŠ¨åˆ›å»ºå¤§è§„æ¨¡çš„äº’åŠ¨ 3D ä¸–ç•Œã€‚å®ƒå°†è‡ªç„¶è¯­è¨€æè¿°è½¬åŒ–ä¸ºå¯æ¢ç´¢çš„ã€å®Œå…¨çº¹ç†åŒ–çš„ç¯å¢ƒï¼Œç”¨æˆ·å¯ä»¥åœ¨æ ‡å‡†æ¸¸æˆå¼•æ“ä¸­ç«‹å³ç¼–è¾‘è¿™äº›ç¯å¢ƒã€‚é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åœºæ™¯å¸ƒå±€æ¨ç†ã€ç¨‹åºç”Ÿæˆã€åŸºäºæ‰©æ•£çš„ 3D ç”Ÿæˆå’Œå¯¹è±¡æ„ŸçŸ¥çš„åœºæ™¯åˆ†è§£ï¼ŒWorldGen ä½¿åˆ›ä½œè€…èƒ½å¤Ÿé«˜æ•ˆåœ°è®¾è®¡è¿è´¯ä¸”å¯å¯¼èˆªçš„è™šæ‹Ÿä¸–ç•Œã€‚è¯¥ç³»ç»Ÿæ¨¡å—åŒ–ï¼Œæ”¯æŒå¯¹å¸ƒå±€ã€è§„æ¨¡å’Œé£æ ¼çš„ç²¾ç»†æ§åˆ¶ï¼Œç”Ÿæˆå‡ ä½•ä¸€è‡´ã€è§†è§‰ä¸°å¯Œä¸”å®æ—¶æ¸²æŸ“é«˜æ•ˆçš„ä¸–ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17220', 'title': 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs', 'url': 'https://huggingface.co/papers/2511.17220', 'abstract': 'PARROT evaluates the robustness of large language models against social pressure and sycophancy, revealing significant variability in model behavior and confidence shifts across different domains and authority templates.  \t\t\t\t\tAI-generated summary \t\t\t\t This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" (leq 11%, GPT-5: 4\\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\\%, Qwen 2.5-1.5B: 94\\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'c08f5f1f2e0dcc47', 'authors': ['Yusuf Ã‡elebi', 'Ã–zay Ezerceli', 'Mahmoud El Hussieni'], 'affiliations': ['NewMind AI, Istanbul, Turkey'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17220.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#dataset', '#security'], 'emoji': 'ğŸ¦œ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° LLM Ğ¿Ğ¾Ğ´Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ²: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑĞ¸ĞºĞ¾Ñ„Ğ°Ğ½Ñ‚ÑÑ‚Ğ²Ğ° Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PARROT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸ĞºĞ¾Ñ„Ğ°Ğ½Ñ‚ÑÑ‚Ğ²Ñƒ (Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ»ĞµÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ¿Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ¾ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-5, Claude Sonnet) ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğº Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ĞºÑ€Ğ°Ñ…Ğ° ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ€Ğ°Ğ²Ğ½Ğµ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Evaluating Language Models: Resisting Social Pressure for Better Accuracy', 'desc': 'The paper introduces PARROT, a framework that assesses how large language models (LLMs) respond to social pressure and sycophancy, which is the tendency to conform excessively. It evaluates the accuracy degradation of LLMs when faced with authoritative but incorrect information by comparing neutral and biased prompts. The study employs a double-blind evaluation method and tracks confidence shifts in model responses using log-likelihood calibration. Results indicate that while advanced models maintain accuracy under pressure, older models often exhibit significant epistemic collapse, highlighting the need for models to resist overfitting to social cues as a key objective.'}, 'zh': {'title': 'æŠµæŠ—ç¤¾ä¼šå‹åŠ›ï¼Œæå‡æ¨¡å‹é²æ£’æ€§', 'desc': 'PARROTï¼ˆè¯´æœä¸ä¸€è‡´æ€§é²æ£’æ€§è¾“å‡ºçœŸç›¸è¯„åˆ†ï¼‰æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šå‹åŠ›ä¸‹çš„é²æ£’æ€§æ¡†æ¶ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸå’Œæƒå¨æ¨¡æ¿ä¸‹çš„è¡Œä¸ºå·®å¼‚å’Œä¿¡å¿ƒå˜åŒ–ã€‚é€šè¿‡æ¯”è¾ƒä¸­ç«‹é—®é¢˜ä¸æƒå¨æ€§é”™è¯¯ç‰ˆæœ¬ï¼ŒPARROTèƒ½å¤Ÿé‡åŒ–æ¨¡å‹åœ¨é¢å¯¹ç¤¾ä¼šå‹åŠ›æ—¶çš„å‡†ç¡®æ€§ä¸‹é™ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå…ˆè¿›æ¨¡å‹çš„è·Ÿéšç‡è¾ƒä½ï¼Œè€Œè¾ƒæ—§æˆ–è¾ƒå°çš„æ¨¡å‹åˆ™è¡¨ç°å‡ºä¸¥é‡çš„çŸ¥è¯†å´©æºƒï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å…³æ³¨æŠµæŠ—è¿‡æ‹Ÿåˆå‹åŠ›çš„ç›®æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17344', 'title': 'Loomis Painter: Reconstructing the Painting Process', 'url': 'https://huggingface.co/papers/2511.17344', 'abstract': 'A unified framework using diffusion models with semantic control and cross-medium style augmentation generates consistent and high-fidelity multi-media painting processes, supported by a comprehensive dataset and evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '221b2271cf76e366', 'authors': ['Markus Pobitzer', 'Chang Liu', 'Chenyi Zhuang', 'Teng Long', 'Bin Ren', 'Nicu Sebe'], 'affiliations': ['University of Pisa', 'University of Trento'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17344.jpg', 'data': {'categories': ['#diffusion', '#training', '#multimodal', '#benchmark', '#open_source', '#video', '#dataset', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾ĞºĞ¸ Ğ¶Ğ¸Ğ²Ğ¾Ğ¿Ğ¸ÑĞ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñæ¡†æ¶ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ´Ğ¸ÑƒĞ¼Ğ¾Ğ², Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ĞºÑ€Ğ¾ÑÑ-Ğ¼ĞµĞ´Ğ¸Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºÑƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº LPIPS, DINO, CLIP Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğ³Ğ¾ Perceptual Distance Profile, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°.'}, 'en': {'title': 'Revolutionizing Artistic Creation with Diffusion Models', 'desc': 'This paper presents a unified framework that utilizes diffusion models to enhance the generation of multi-media painting processes with semantic control. It addresses the limitations of existing generative models by ensuring consistency and fidelity across different artistic styles and media. The framework incorporates a reverse-painting training strategy and a comprehensive dataset to evaluate the quality of generated processes. Additionally, it introduces a Perceptual Distance Profile (PDP) curve to model the creative sequence, reflecting the natural progression of human artistry.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼šæå‡å¤šåª’ä½“ç»˜ç”»ç”Ÿæˆçš„ä¸€è‡´æ€§ä¸ä¿çœŸåº¦', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šåª’ä½“ç»˜ç”»è¿‡ç¨‹çš„ç”Ÿæˆï¼Œå¹¶ç»“åˆè¯­ä¹‰æ§åˆ¶å’Œè·¨åª’ä»‹é£æ ¼å¢å¼ºã€‚è¯¥æ¡†æ¶é€šè¿‡å°†å¤šç§åª’ä»‹åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ç©ºé—´ï¼Œå®ç°äº†ä¸€è‡´çš„çº¹ç†æ¼”å˜å’Œé£æ ¼è½¬ç§»ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„çœŸå®ç»˜ç”»è¿‡ç¨‹æ•°æ®é›†ï¼Œå¹¶åœ¨è·¨åª’ä»‹ä¸€è‡´æ€§ã€æ—¶é—´è¿è´¯æ€§å’Œæœ€ç»ˆå›¾åƒä¿çœŸåº¦ç­‰æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ„ŸçŸ¥è·ç¦»æ›²çº¿(PDP)å®šé‡å»ºæ¨¡äº†åˆ›ä½œåºåˆ—ï¼Œåæ˜ äº†äººç±»è‰ºæœ¯åˆ›ä½œçš„è¿›ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.11007', 'title': 'VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.11007', 'abstract': 'VisMem enhances Vision-Language Models by incorporating dynamic latent vision memories, improving performance on complex visual tasks through perceptual fidelity and semantic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '31ad8a9eddd1753a', 'authors': ['Xinlei Yu', 'Chengming Xu', 'Guibin Zhang', 'Zhangquan Chen', 'Yudong Zhang', 'Yongbo He', 'Peng-Tao Jiang', 'Jiangning Zhang', 'Xiaobin Hu', 'Shuicheng Yan'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Tsinghua University', 'University of Science and Technology of China', 'Zhejiang University', 'vivo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.11007.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#reasoning', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Vision-Language Models', 'desc': 'VisMem â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Vision-Language Models Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ: ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 11.8% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Vision-Language Models with Dynamic Memory', 'desc': "The paper introduces VisMem, a new framework designed to enhance Vision-Language Models (VLMs) by integrating dynamic latent vision memories. This approach addresses the visual processing bottleneck that VLMs face, which often leads to a loss of visual grounding and contextual understanding during complex tasks. By mimicking human memory, VisMem incorporates both short-term perceptual retention and long-term semantic consolidation, improving the model's ability to maintain accuracy and coherence in visual reasoning. Experimental results show that VisMem significantly boosts performance by an average of 11.8% compared to traditional models, setting a new standard for memory enhancement in machine learning."}, 'zh': {'title': 'åŠ¨æ€æ½œåœ¨è§†è§‰è®°å¿†ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç°', 'desc': 'VisMemæ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥åŠ¨æ€æ½œåœ¨è§†è§‰è®°å¿†æ¥æå‡å…¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶ç»“åˆäº†çŸ­æœŸè§†è§‰è®°å¿†å’Œé•¿æœŸè¯­ä¹‰è®°å¿†ï¼Œå¸®åŠ©æ¨¡å‹åœ¨æ¨ç†å’Œç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒæ„ŸçŸ¥çš„å‡†ç¡®æ€§å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒVisMemåœ¨ç†è§£ã€æ¨ç†å’Œç”Ÿæˆç­‰å¤šç§è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æå‡äº†11.8%çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºæ½œåœ¨ç©ºé—´è®°å¿†å¢å¼ºå»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16175', 'title': 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight', 'url': 'https://huggingface.co/papers/2511.16175', 'abstract': 'Mantis, a VLA framework with Disentangled Visual Foresight and a diffusion Transformer, improves action prediction, comprehension, and reasoning while reducing training complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms Ï€_{0.5}, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'f1963cb80b84fef7', 'authors': ['Yi Yang', 'Xueqi Li', 'Yiyang Chen', 'Jin Song', 'Yihan Wang', 'Zipeng Xiao', 'Jiadi Su', 'You Qiaoben', 'Pengfei Liu', 'Zhijie Deng'], 'affiliations': ['BOSCH', 'FDU', 'NJUPT', 'SII', 'SJTU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16175.jpg', 'data': {'categories': ['#diffusion', '#training', '#multimodal', '#open_source', '#robotics', '#video', '#reasoning', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Mantis â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Vision-Language-Action (VLA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ñ€Ğ°Ğ·Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (DVF) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ (DiT) Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ° Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mantis Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 96.7% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LIBERO Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Mantis: Simplifying VLA with Disentangled Visual Foresight', 'desc': "Mantis is a new framework designed to enhance Vision-Language-Action (VLA) models by improving action prediction and reasoning while simplifying training. It introduces Disentangled Visual Foresight (DVF), which separates visual foresight prediction from the main model, allowing for better learning of actions without overwhelming the model's capacity. By using a diffusion Transformer (DiT) and meta queries, Mantis effectively captures latent actions and improves comprehension through language supervision. The framework has shown impressive results, achieving a 96.7% success rate on the LIBERO benchmark and outperforming existing models in real-world tasks."}, 'zh': {'title': 'Mantisï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›', 'desc': 'Mantisæ˜¯ä¸€ä¸ªæ–°çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨äº†è§£è€¦çš„è§†è§‰å‰ç»ï¼ˆDVFï¼‰å’Œæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œæ—¨åœ¨æé«˜åŠ¨ä½œé¢„æµ‹ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒå¤æ‚æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†è§†è§‰å‰ç»é¢„æµ‹ä¸ä¸»å¹²ç½‘ç»œè§£è€¦ï¼Œåˆ©ç”¨å…ƒæŸ¥è¯¢å’ŒDiTå¤´éƒ¨çš„ç»„åˆï¼Œæ¥æœ‰æ•ˆæ•æ‰æ½œåœ¨åŠ¨ä½œï¼Œä»è€Œå¢å¼ºæ˜¾å¼åŠ¨ä½œçš„å­¦ä¹ ã€‚Mantisåœ¨è¯­è¨€ç›‘ç£çš„å¸®åŠ©ä¸‹ï¼Œä¿æŒäº†è‰¯å¥½çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚ç»è¿‡åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒå’Œå¾®è°ƒï¼ŒMantisåœ¨LIBEROåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†96.7%çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†è®¸å¤šå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14899', 'title': 'InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization', 'url': 'https://huggingface.co/papers/2511.14899', 'abstract': 'A framework called InstructMix2Mix uses a 2D diffusion model to improve multi-view image editing by leveraging a 3D prior for cross-view consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '4e881e5d1cb67be2', 'authors': ['Daniel Gilo', 'Or Litany'], 'affiliations': ['NVIDIA', 'Technion Israel Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14899.jpg', 'data': {'categories': ['#multimodal', '#3d', '#diffusion', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'InstructMix2Mix â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ 3D Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ³Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ² Score Distillation Sampling Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ÑĞ¼Ğ¸: Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ scheduler ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Multi-View Image Editing with InstructMix2Mix', 'desc': 'InstructMix2Mix (I-Mix2Mix) is a framework designed to enhance multi-view image editing by utilizing a 2D diffusion model alongside a 3D prior for better consistency across different viewpoints. The method addresses challenges faced by existing techniques, which often lead to artifacts and inconsistencies when editing images from sparse input views. By integrating a multi-view diffusion model into the Score Distillation Sampling process, I-Mix2Mix introduces innovative strategies such as incremental updates and a specialized noise scheduler to improve coherence. Experimental results show that this approach not only boosts cross-view consistency but also preserves high-quality edits for each individual frame.'}, 'zh': {'title': 'æå‡å¤šè§†è§’ä¸€è‡´æ€§çš„å›¾åƒç¼–è¾‘æ–°æ¡†æ¶', 'desc': 'InstructMix2Mixï¼ˆI-Mix2Mixï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨3Då…ˆéªŒçŸ¥è¯†æ¥æ”¹å–„å¤šè§†è§’å›¾åƒç¼–è¾‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨2Dæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç¨€ç–è¾“å…¥è§†å›¾çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ä¿®æ”¹åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒå„è§†è§’ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒI-Mix2Mixé€šè¿‡å¼•å…¥å¤šè§†è§’æ‰©æ•£å­¦ç”Ÿï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„ç¥ç»åœºæ•´åˆå™¨ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†ä¼ªå½±å’Œä¸ä¸€è‡´çš„ç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒI-Mix2Mixåœ¨æé«˜å¤šè§†è§’ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒæ¯å¸§ç¼–è¾‘çš„é«˜è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17487', 'title': 'Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models', 'url': 'https://huggingface.co/papers/2511.17487', 'abstract': 'Reducing the capacity of large language models disproportionately impacts visual capabilities in multimodal systems, but visual extraction tuning combined with step-by-step reasoning improves efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'd728f1bb76fff964', 'authors': ['Mark Endo', 'Serena Yeung-Levy'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17487.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#reasoning', '#cv', '#small_models'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ â€” ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ LLM Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑ‘ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Extract+Think, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Extract+Think: Enhancing Visual Reasoning in Smaller Models', 'desc': 'This paper investigates how reducing the size of large language models (LLMs) affects their ability to understand and reason about visual information in multimodal systems. The authors find that downscaling LLMs leads to a significant decline in visual capabilities, which is more pronounced than the decline in reasoning abilities. To mitigate this issue, they propose a method called visual extraction tuning, which helps the model focus on relevant visual details for better performance. By combining this tuning with step-by-step reasoning, they introduce the Extract+Think approach, enhancing both efficiency and effectiveness in multimodal tasks.'}, 'zh': {'title': 'è§†è§‰èƒ½åŠ›æå‡çš„æ–°æ–¹æ³•ï¼šExtract+Think', 'desc': 'åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­ï¼Œé™ä½å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¼šå¯¹è§†è§‰èƒ½åŠ›äº§ç”Ÿä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶åˆ†æäº†åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ç¼©å°æ™ºèƒ½çš„å½±å“ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ä¸‹é™ä¸»è¦å½±å“è§†è§‰èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ç»§æ‰¿è‡ªè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰æå–è°ƒä¼˜çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¸€è‡´æ€§åœ°è®­ç»ƒæ¨¡å‹æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚ç»“åˆé€æ­¥æ¨ç†ï¼Œæˆ‘ä»¬çš„Extract+Thinkæ–¹æ³•åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.14806', 'title': 'MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging', 'url': 'https://huggingface.co/papers/2511.14806', 'abstract': 'MergeDNA uses a hierarchical architecture with Token Merging and latent Transformers to model genomic sequences, achieving superior performance on DNA benchmarks and multi-omics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '034072ae3093ed4e', 'authors': ['Siyuan Li', 'Kai Yu', 'Anna Wang', 'Zicheng Liu', 'Chang Yu', 'Jingbo Zhou', 'Qirong Yang', 'Yucheng Guo', 'Xiaoming Zhang', 'Stan Z. Li'], 'affiliations': ['AI Lab, Research Center for Industries of the Future, Westlake University, China', 'BioMap Research, Beijing, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.14806.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'MergeDNA Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Token Merging Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸Ğ· Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ† Ğ”ĞĞš. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸ merging, Ğ¸ Latent Encoder Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ”ĞĞš Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾Ğ¼Ğ¸ĞºÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ”ĞĞš.'}, 'en': {'title': 'Revolutionizing Genomic Modeling with MergeDNA', 'desc': "MergeDNA presents a novel hierarchical architecture that effectively models genomic sequences by addressing the challenges of varying information density and undefined vocabulary units. It utilizes Token Merging techniques to create a dynamic genomic tokenizer that adapts to the complexities of DNA data. The architecture incorporates latent Transformers for capturing global context, enhancing the model's ability to learn from genomic sequences. Experimental results demonstrate that MergeDNA outperforms existing methods on DNA benchmarks and multi-omics tasks, showcasing its effectiveness in genomic modeling."}, 'zh': {'title': 'MergeDNAï¼šåŸºå› ç»„åºåˆ—å»ºæ¨¡çš„æ–°çªç ´', 'desc': 'MergeDNAæ˜¯ä¸€ç§ä½¿ç”¨åˆ†å±‚æ¶æ„çš„æ¨¡å‹ï¼Œç»“åˆäº†Token Mergingå’Œæ½œåœ¨å˜æ¢å™¨æ¥å¤„ç†åŸºå› ç»„åºåˆ—ã€‚è¯¥æ–¹æ³•è§£å†³äº†åŸºå› ç»„åºåˆ—ä¸­ä¿¡æ¯å¯†åº¦å˜åŒ–å’Œç¼ºä¹æ˜ç¡®æœ€å°è¯æ±‡å•å…ƒçš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€åŸºå› ç»„æ ‡è®°å™¨å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é¢„è®­ç»ƒä»»åŠ¡ï¼ŒMergeDNAèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼˜åŒ–æ ‡è®°å’Œæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeDNAåœ¨å¤šä¸ªDNAåŸºå‡†æµ‹è¯•å’Œå¤šç»„å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æ ‡è®°æ–¹æ³•å’Œå¤§å‹DNAåŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17199', 'title': 'VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation', 'url': 'https://huggingface.co/papers/2511.17199', 'abstract': 'VLA-4D enhances robotic manipulation by integrating 4D spatial-temporal awareness into visual and action representations, achieved through cross-attention and temporal extension.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '68b33e91457dd995', 'authors': ['Hanyu Zhou', 'Chuanhao Ma', 'Gim Hee Lee'], 'affiliations': ['School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'School of Computing, National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17199.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#architecture', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ§ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸', 'desc': 'VLA-4D â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ-ÑĞ·Ñ‹Ğº-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ (Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ 4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Robots with 4D Spatial-Temporal Awareness for Smooth Manipulation', 'desc': 'The paper introduces VLA-4D, a novel model that enhances robotic manipulation by incorporating 4D spatial-temporal awareness into visual and action representations. It addresses the limitations of existing vision-language-action (VLA) models, which often struggle with maintaining coherence in time during manipulation tasks. By utilizing a cross-attention mechanism, the model integrates time into visual features, creating a unified representation that captures both spatial and temporal dimensions. Additionally, it extends action representations to include temporal information, allowing for better planning and execution of actions in a coherent manner.'}, 'zh': {'title': 'å››ç»´æ„è¯†æå‡æœºå™¨äººæ“ä½œèƒ½åŠ›', 'desc': 'VLA-4Dæ¨¡å‹é€šè¿‡å°†å››ç»´æ—¶ç©ºæ„è¯†æ•´åˆåˆ°è§†è§‰å’ŒåŠ¨ä½œè¡¨ç¤ºä¸­ï¼Œå¢å¼ºäº†æœºå™¨äººæ“ä½œçš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†ä¸€ç»´æ—¶é—´åµŒå…¥ä¸‰ç»´ä½ç½®ï¼Œå½¢æˆç»Ÿä¸€çš„å››ç»´è§†è§‰è¡¨ç¤ºã€‚é€šè¿‡æ‰©å±•ä¼ ç»Ÿçš„ç©ºé—´åŠ¨ä½œè¡¨ç¤ºï¼ŒåŠ å…¥æ—¶é—´ä¿¡æ¯ï¼Œå®ç°æ—¶ç©ºè§„åˆ’ï¼Œä½¿å¾—æœºå™¨äººæ“ä½œåœ¨ç©ºé—´ä¸Šå¹³æ»‘ä¸”æ—¶é—´ä¸Šè¿è´¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLA-4Dåœ¨ä¸åŒçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17074', 'title': 'Diversity Has Always Been There in Your Visual Autoregressive Models', 'url': 'https://huggingface.co/papers/2511.17074', 'abstract': 'DiverseVAR enhances generative diversity in Visual Autoregressive models by modifying the pivotal component of feature maps without additional training, improving synthesis quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '5f944615180ce131', 'authors': ['Tong Wang', 'Guanyu Yang', 'Nian Liu', 'Kai Wang', 'Yaxing Wang', 'Abdelrahman M Shaker', 'Salman Khan', 'Fahad Shahbaz Khan', 'Senmao Li'], 'affiliations': ['City University of Hong Kong', 'MBZUAI', 'Nankai University', 'Southeast University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17074.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'DiverseVAR Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ‚Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ€Ñ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ° Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ¸ ĞµĞ³Ğ¾ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiverseVAR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unlocking Diversity in Visual Autoregressive Models', 'desc': "DiverseVAR is a method designed to improve the diversity of outputs in Visual Autoregressive (VAR) models, which are known for their efficient image generation. The paper identifies a crucial part of the feature map that affects the variability of generated images and proposes a way to modify this component without needing extra training. By adjusting the pivotal component in the input and output of the model, DiverseVAR enhances the model's ability to produce varied and high-quality images. The results show that this approach significantly increases generative diversity while maintaining the overall performance of the model."}, 'zh': {'title': 'DiverseVARï¼šæå‡è§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§', 'desc': 'DiverseVARæ˜¯ä¸€ç§å¢å¼ºè§†è§‰è‡ªå›å½’æ¨¡å‹ç”Ÿæˆå¤šæ ·æ€§çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¿®æ”¹ç‰¹å¾å›¾çš„å…³é”®ç»„ä»¶ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œä»è€Œæé«˜åˆæˆè´¨é‡ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåœ°æ¢å¤äº†è‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§ï¼Œè§£å†³äº†è¾“å‡ºå˜å¼‚æ€§é™ä½çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiverseVARæ˜¾è‘—æå‡äº†ç”Ÿæˆå¤šæ ·æ€§ï¼ŒåŒæ—¶å¯¹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16931', 'title': 'OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists', 'url': 'https://huggingface.co/papers/2511.16931', 'abstract': 'OmniScientist is a framework that simulates human scientific processes to automate and enhance AI-driven scientific research through structured knowledge systems, collaborative protocols, and evaluation platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'ea9d862dc86187ee', 'authors': ['Chenyang Shao', 'Dehao Huang', 'Yu Li', 'Keyu Zhao', 'Weiquan Lin', 'Yining Zhang', 'Qingbin Zeng', 'Zhiyu Chen', 'Tianxing Li', 'Yifei Huang', 'Taozhong Wu', 'Xinyang Liu', 'Ruotong Zhao', 'Mengsheng Zhao', 'Xuhua Zhang', 'Yue Wang', 'Yuanyi Zhen', 'Fengli Xu', 'Yong Li', 'Tie-Yan Liu'], 'affiliations': ['Department of Electronic Engineering, BNRist, Tsinghua University', 'Zhongguancun Academy'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16931.jpg', 'data': {'categories': ['#open_source', '#science'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ½Ğ¾Ğµ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹', 'desc': 'OmniScientist â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… AI ÑƒÑ‡Ñ‘Ğ½Ñ‹Ñ…, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ScienceArena Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Elo Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼.'}, 'en': {'title': 'Empowering AI with Human-Like Scientific Collaboration', 'desc': 'OmniScientist is a new framework designed to improve AI-driven scientific research by mimicking human scientific processes. It integrates structured knowledge systems, collaborative protocols, and evaluation platforms to create a more realistic research environment. Unlike traditional AI Scientists that treat scientific discovery as isolated tasks, OmniScientist emphasizes the importance of collaboration and social interaction in research. This framework automates various stages of the research process while ensuring that AI agents can effectively engage with human researchers and the broader scientific community.'}, 'zh': {'title': 'OmniScientistï¼šé‡å¡‘ç§‘å­¦ç ”ç©¶çš„AIæ¡†æ¶', 'desc': 'OmniScientistæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ¨¡æ‹Ÿäººç±»ç§‘å­¦è¿‡ç¨‹ï¼Œä»¥è‡ªåŠ¨åŒ–å’Œå¢å¼ºåŸºäºAIçš„ç§‘å­¦ç ”ç©¶ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–çŸ¥è¯†ç³»ç»Ÿã€åä½œåè®®å’Œè¯„ä¼°å¹³å°ï¼Œè§£å†³äº†ç°æœ‰AIç§‘å­¦å®¶åœ¨ç§‘å­¦å‘ç°ä¸­å¿½è§†ç¤¾ä¼šåä½œçš„ä¸è¶³ã€‚OmniScientistå®ç°äº†ä»æ•°æ®åŸºç¡€åˆ°æ–‡çŒ®ç»¼è¿°ã€ç ”ç©¶æ„æ€ã€å®éªŒè‡ªåŠ¨åŒ–ã€ç§‘å­¦å†™ä½œå’ŒåŒè¡Œè¯„å®¡çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚è¯¥æ¡†æ¶è¿˜æä¾›äº†æ”¯æŒäººç±»ç§‘å­¦ç³»ç»Ÿçš„åŸºç¡€è®¾æ–½ï¼Œä¿ƒè¿›äº†AIä»£ç†ä¸äººç±»ç ”ç©¶è€…çš„æ·±åº¦åˆä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15462', 'title': 'Insights from the ICLR Peer Review and Rebuttal Process', 'url': 'https://huggingface.co/papers/2511.15462', 'abstract': 'The study analyzes the ICLR 2024 and 2025 peer review processes, focusing on score dynamics and reviewer interactions, using LLM-based text categorization to identify trends and factors influencing score changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR. As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores, author-reviewer engagement, temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes. Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence. Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/iclr-insights.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '963e43812446bcb8', 'authors': ['Amir Hossein Kargaran', 'Nafiseh Nikeghbal', 'Jing Yang', 'Nedjma Ousidhoum'], 'affiliations': ['Cardiff University', 'LMU Munich', 'Munich Center for Machine Learning', 'Paper Copilot', 'Technical University of Munich', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15462.jpg', 'data': {'categories': ['#survey', '#open_source'], 'emoji': 'ğŸ“‹', 'ru': {'title': 'ĞšĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸ÑÑ… ICLR 2024 Ğ¸ 2025, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ ÑĞ¾Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²-Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ñ€ÑƒĞ³ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚.'}, 'en': {'title': 'Enhancing Peer Review: Insights from ICLR 2024 and 2025', 'desc': 'This study investigates the peer review processes of the ICLR conferences in 2024 and 2025, focusing on how review scores change before and after author rebuttals. It employs large language model (LLM) techniques to categorize review texts and analyze reviewer interactions, revealing patterns that influence score dynamics. The research highlights that initial scores and co-reviewer ratings significantly impact score changes, indicating the importance of reviewer influence. Ultimately, the findings provide actionable insights for authors on effective rebuttal strategies and suggest improvements for the overall peer review system.'}, 'zh': {'title': 'æå‡åŒè¡Œè¯„å®¡æ•ˆç‡çš„å…³é”®æ´å¯Ÿ', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†ICLR 2024å’Œ2025çš„åŒè¡Œè¯„å®¡è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨è¯„åˆ†åŠ¨æ€å’Œè¯„å®¡è€…äº’åŠ¨ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼Œè¯†åˆ«å½±å“è¯„åˆ†å˜åŒ–çš„è¶‹åŠ¿å’Œå› ç´ ã€‚ç ”ç©¶å‘ç°ï¼Œåˆå§‹è¯„åˆ†å’Œå…±åŒè¯„å®¡è€…çš„è¯„åˆ†æ˜¯è¯„åˆ†å˜åŒ–çš„æœ€å¼ºé¢„æµ‹å› ç´ ï¼Œè¡¨æ˜è¯„å®¡è€…ä¹‹é—´å­˜åœ¨ä¸€å®šçš„å½±å“åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ”¹è¿›åŒè¡Œè¯„å®¡è¿‡ç¨‹æä¾›äº†åŸºäºè¯æ®çš„è§è§£ï¼ŒæŒ‡å¯¼ä½œè€…åˆ¶å®šæœ‰æ•ˆçš„åé©³ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17450', 'title': 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation', 'url': 'https://huggingface.co/papers/2511.17450', 'abstract': 'SketchVerify enhances video motion planning and generation by iteratively refining candidate motion plans using a lightweight sketch-based verification process, improving both quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '6395b90b88b6199e', 'authors': ['Yidong Huang', 'Zun Wang', 'Han Lin', 'Dong-Ki Kim', 'Shayegan Omidshafiei', 'Jaehong Yoon', 'Yue Zhang', 'Mohit Bansal'], 'affiliations': ['FieldAI', 'Nanyang Technological University', 'UNC Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17450.jpg', 'data': {'categories': ['#video', '#multimodal', '#diffusion', '#optimization'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°', 'desc': 'SketchVerify â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞºĞ¸Ğ·Ğ¾Ğ² â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ½Ğ°Ğ±Ñ€Ğ¾ÑĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'Refining Motion Plans for Better Video Generation', 'desc': 'SketchVerify is a novel framework that enhances video motion planning by refining candidate motion plans through a sketch-based verification process. It addresses the limitations of existing methods that either rely on simple single-shot plans or require costly iterative refinements. By predicting multiple motion plans and evaluating them for semantic alignment and physical plausibility, SketchVerify ensures more coherent and realistic motion trajectories. This approach not only improves the quality of generated videos but also significantly reduces computational costs compared to traditional methods.'}, 'zh': {'title': 'SketchVerifyï¼šé«˜æ•ˆçš„è§†é¢‘è¿åŠ¨è§„åˆ’ä¸ç”Ÿæˆ', 'desc': 'SketchVerify æ˜¯ä¸€ç§åŸºäºè‰å›¾éªŒè¯çš„è¿åŠ¨è§„åˆ’æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆä¸­çš„è¿åŠ¨è§„åˆ’è´¨é‡å’Œæ•ˆç‡ã€‚å®ƒé€šè¿‡å¼•å…¥æµ‹è¯•æ—¶çš„é‡‡æ ·å’ŒéªŒè¯å¾ªç¯ï¼Œç”Ÿæˆæ›´å…·åŠ¨æ€ä¸€è‡´æ€§çš„è½¨è¿¹ï¼Œç¡®ä¿è¿åŠ¨çš„ç‰©ç†åˆç†æ€§å’ŒæŒ‡ä»¤ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç»™å®šæç¤ºå’Œå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å¤šä¸ªå€™é€‰è¿åŠ¨è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨è§†è§‰-è¯­è¨€éªŒè¯å™¨å¯¹å…¶è¿›è¡Œè¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSketchVerify åœ¨è¿åŠ¨è´¨é‡ã€ç‰©ç†çœŸå®æ„Ÿå’Œé•¿æœŸä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15299', 'title': 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection', 'url': 'https://huggingface.co/papers/2511.15299', 'abstract': 'A one-stage text-to-image synthesis pipeline for X-ray security images improves efficiency and quality without extra labor cost, enhancing prohibited item detection performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'b66621108f2d87af', 'authors': ['Jialong Sun', 'Hongguang Zhu', 'Weizhe Liu', 'Yunda Sun', 'Renshuai Tao', 'Yunchao Wei'], 'affiliations': ['Faculty of Data Science, City University of Macau', 'Institute of Information Science, Beijing Jiaotong University', 'Nuctech Company Limited'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15299.jpg', 'data': {'categories': ['#diffusion', '#data', '#dataset', '#cv', '#synthetic'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image generation. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Cross-Attention Refinement Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Background Occlusion Modeling Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ñ„Ğ¾Ğ½Ğ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Xsyn Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ÑƒÑ‚Ğ¾Ğ¼Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ² Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 1.2% mAP Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Efficient X-ray Image Synthesis for Enhanced Detection', 'desc': 'This paper presents a novel one-stage pipeline for synthesizing X-ray security images using text-to-image generation, which enhances both efficiency and quality. The proposed method, called Xsyn, eliminates the need for labor-intensive foreground extraction by integrating two innovative strategies: Cross-Attention Refinement (CAR) and Background Occlusion Modeling (BOM). CAR improves bounding box annotations by utilizing cross-attention maps from a diffusion model, while BOM enhances the complexity of the background in the generated images. Experimental results show that Xsyn achieves a 1.2% improvement in mean Average Precision (mAP) over previous methods, significantly boosting the performance of prohibited item detection without incurring additional labor costs.'}, 'zh': {'title': 'ä¸€é˜¶æ®µXå°„çº¿å›¾åƒåˆæˆï¼Œæå‡æ£€æµ‹æ•ˆç‡ä¸è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸€é˜¶æ®µçš„Xå°„çº¿å®‰å…¨å›¾åƒåˆæˆç®¡é“ï¼ˆXsynï¼‰ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œå›¾åƒè´¨é‡ï¼ŒåŒæ—¶é™ä½é¢å¤–çš„äººå·¥æˆæœ¬ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆæˆæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µéœ€è¦è¿›è¡ŒåŠ³åŠ¨å¯†é›†å‹çš„å‰æ™¯æå–ï¼Œç¬¬äºŒé˜¶æ®µå†è¿›è¡Œå›¾åƒåˆæˆï¼Œè¿™æ ·æ•ˆç‡ä½ä¸‹ä¸”å¢åŠ äº†äººå·¥æˆæœ¬ã€‚Xsyné€šè¿‡æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆäº¤å‰æ³¨æ„åŠ›ç²¾ç‚¼ï¼ˆCARï¼‰å’ŒèƒŒæ™¯é®æŒ¡å»ºæ¨¡ï¼ˆBOMï¼‰ç­–ç•¥ï¼Œä¼˜åŒ–äº†åˆæˆå›¾åƒçš„å¯ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒXsynåœ¨å¤šä¸ªXå°„çº¿å®‰å…¨æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šå‡å®ç°äº†1.2%çš„mAPæå‡ï¼Œæ˜¾è‘—æ”¹å–„äº†ç¦å“æ£€æµ‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17127', 'title': 'Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design', 'url': 'https://huggingface.co/papers/2511.17127', 'abstract': 'A large-scale mixture-of-experts (MoE) pretraining study on AMD hardware provides comprehensive guidance for system and model design, demonstrating competitive performance with leading models.  \t\t\t\t\tAI-generated summary \t\t\t\t We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'dabe886bb856f5ce', 'authors': ['Quentin Anthony', 'Yury Tokpanov', 'Skyler Szot', 'Srivatsan Rajagopal', 'Praneeth Medepalli', 'Anna Golubeva', 'Vasu Shyam', 'Robert Washbourne', 'Rishi Iyer', 'Ansh Chaurasia', 'Tomas Figliolia', 'Xiao Yang', 'Abhinav Sarje', 'Drew Thorstensen', 'Amartey Pearson', 'Zack Grossbart', 'Jason van Patten', 'Emad Barsoum', 'Zhenyu Gu', 'Yao Fu', 'Beren Millidge'], 'affiliations': ['AMD', 'IBM', 'Zyphra'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17127.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#open_source', '#architecture'], 'emoji': 'âš™ï¸', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MoE Ğ½Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ AMD ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ AMD, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPU MI300X Ñ Ğ¸Ğ½Ñ‚ĞµÑ€ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ğ¼ Pollara. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ (Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°, ÑĞµÑ‚ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸), Ñ‚Ğ°Ğº Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ MoE). ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ZAYA1 Ñ 760Ğœ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Qwen3-4B Ğ¸ Gemma3-12B, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Llama-3-8B Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ·Ñ€ĞµĞ»Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾, ÑĞµÑ‚ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµĞºĞ° AMD Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Optimizing MoE Pretraining with AMD Hardware', 'desc': "This paper presents a large-scale study on mixture-of-experts (MoE) pretraining using AMD hardware, specifically the MI300X GPUs. It offers detailed insights into system and model design, including microbenchmarks for core collective operations and memory bandwidth, which are crucial for optimizing performance. The authors introduce MI300X-aware transformer sizing rules and justify the configuration of MoE widths to enhance both training speed and inference efficiency. The resulting model, ZAYA1, shows competitive performance against leading models, indicating that AMD's technology is well-suited for advanced machine learning tasks."}, 'zh': {'title': 'AMDç¡¬ä»¶ä¸Šçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶é¢„è®­ç»ƒç ”ç©¶', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†åœ¨çº¯AMDç¡¬ä»¶ä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰é¢„è®­ç»ƒç ”ç©¶ã€‚ç ”ç©¶æä¾›äº†ç³»ç»Ÿå’Œæ¨¡å‹è®¾è®¡çš„å®ç”¨æŒ‡å¯¼ï¼Œå±•ç¤ºäº†ä¸é¢†å…ˆæ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„é›†ç¾¤å’Œç½‘ç»œç‰¹æ€§åˆ†æï¼Œå¹¶æä¾›äº†MI300Xå¾®åŸºå‡†æµ‹è¯•ï¼Œä»¥å¸®åŠ©æ¨¡å‹è®¾è®¡ã€‚æœ€ç»ˆï¼ŒZAYA1æ¨¡å‹åœ¨æ¨ç†å’Œè®­ç»ƒæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†AMDç¡¬ä»¶å’Œè½¯ä»¶å †æ ˆçš„æˆç†Ÿåº¦å’Œä¼˜åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16110', 'title': 'Multi-Faceted Attack: Exposing Cross-Model Vulnerabilities in Defense-Equipped Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.16110', 'abstract': 'Multi-Faceted Attack (MFA) framework reveals vulnerabilities in VLMs by transferring adversarial attacks across models, achieving high success rates even against state-of-the-art defenses.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing misuse of Vision-Language Models (VLMs) has led providers to deploy multiple safeguards, including alignment tuning, system prompts, and content moderation. However, the real-world robustness of these defenses against adversarial attacks remains underexplored. We introduce Multi-Faceted Attack (MFA), a framework that systematically exposes general safety vulnerabilities in leading defense-equipped VLMs such as GPT-4o, Gemini-Pro, and Llama-4. The core component of MFA is the Attention-Transfer Attack (ATA), which hides harmful instructions inside a meta task with competing objectives. We provide a theoretical perspective based on reward hacking to explain why this attack succeeds. To improve cross-model transferability, we further introduce a lightweight transfer-enhancement algorithm combined with a simple repetition strategy that jointly bypasses both input-level and output-level filters without model-specific fine-tuning. Empirically, we show that adversarial images optimized for one vision encoder transfer broadly to unseen VLMs, indicating that shared visual representations create a cross-model safety vulnerability. Overall, MFA achieves a 58.5% success rate and consistently outperforms existing methods. On state-of-the-art commercial models, MFA reaches a 52.8% success rate, surpassing the second-best attack by 34%. These results challenge the perceived robustness of current defense mechanisms and highlight persistent safety weaknesses in modern VLMs. Code: https://github.com/cure-lab/MultiFacetedAttack', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '4253fff7fc472c1c', 'authors': ['Yijun Yang', 'Lichao Wang', 'Jianping Zhang', 'Chi Harold Liu', 'Lanqing Hong', 'Qiang Xu'], 'affiliations': ['Beijing Institute of Technology', 'Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16110.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#cv', '#security'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ â€” Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ VLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Multi-Faceted Attack (MFA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Vision-Language Models Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ adversarial-Ğ°Ñ‚Ğ°Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” Attention-Transfer Attack (ATA) â€” ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ°Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ reward hacking. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ MFA Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² 52.8% Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 34%.'}, 'en': {'title': 'Exposing Vulnerabilities in Vision-Language Models with MFA', 'desc': 'The Multi-Faceted Attack (MFA) framework identifies weaknesses in Vision-Language Models (VLMs) by effectively transferring adversarial attacks across different models. It introduces the Attention-Transfer Attack (ATA), which cleverly embeds harmful instructions within tasks that have conflicting goals. This approach reveals that existing defenses, such as alignment tuning and content moderation, may not be as robust as believed, achieving a notable 58.5% success rate against these defenses. The findings emphasize the need for improved safety measures in VLMs, as the vulnerabilities exposed by MFA challenge the effectiveness of current protective strategies.'}, 'zh': {'title': 'æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ¼æ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤šé¢æ”»å‡»ï¼ˆMFAï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„å®‰å…¨æ¼æ´ã€‚MFAé€šè¿‡è·¨æ¨¡å‹è½¬ç§»å¯¹æŠ—æ”»å‡»ï¼ŒæˆåŠŸç‡é«˜ï¼Œå³ä½¿é¢å¯¹æœ€å…ˆè¿›çš„é˜²å¾¡æªæ–½ä¹Ÿèƒ½æœ‰æ•ˆçªç ´ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯æ³¨æ„åŠ›è½¬ç§»æ”»å‡»ï¼ˆATAï¼‰ï¼Œèƒ½å¤Ÿå°†æœ‰å®³æŒ‡ä»¤éšè—åœ¨ç«äº‰ç›®æ ‡çš„å…ƒä»»åŠ¡ä¸­ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–åçš„å¯¹æŠ—å›¾åƒå¯ä»¥å¹¿æ³›è½¬ç§»åˆ°æœªè§è¿‡çš„VLMsï¼Œæ˜¾ç¤ºå‡ºå…±äº«è§†è§‰è¡¨ç¤ºæ‰€å¸¦æ¥çš„è·¨æ¨¡å‹å®‰å…¨æ¼æ´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13081', 'title': 'Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations', 'url': 'https://huggingface.co/papers/2511.13081', 'abstract': 'The Reference-Frame Ã— Granularity (RFxG) taxonomy and novel faithfulness metrics improve the evaluation and alignment of saliency explanations with user intent in deep learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame times Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations. Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'ffd4e62e39610239', 'authors': ['Yehonatan Elisha', 'Seffi Cohen', 'Oren Barkan', 'Noam Koenigstein'], 'affiliations': ['Harvard University', 'Tel Aviv University', 'The Open University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13081.jpg', 'data': {'categories': ['#alignment', '#cv', '#benchmark', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ RFxG: Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ RFxG Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ñ€Ñ‚ ÑĞ°Ğ»Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¾ÑÑĞ¼: Ñ‚Ğ¸Ğ¿ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹) Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğº Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Aligning Saliency Explanations with User Intent', 'desc': 'This paper introduces the Reference-Frame Ã— Granularity (RFxG) taxonomy to enhance the evaluation of saliency explanations in deep learning. It categorizes explanations based on two axes: Reference-Frame, which differentiates between pointwise and contrastive explanations, and Granularity, which ranges from fine-grained to coarse-grained interpretations. The authors identify limitations in current evaluation metrics that focus too much on pointwise faithfulness, overlooking the importance of contrastive reasoning and semantic granularity. They propose four new faithfulness metrics to assess explanation quality across these dimensions, aiming to align saliency maps more closely with user intent and improve their practical utility.'}, 'zh': {'title': 'æå‡æ·±åº¦å­¦ä¹ è§£é‡Šçš„ç”¨æˆ·æ„å›¾å¯¹é½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç§°ä¸ºå‚è€ƒæ¡†æ¶Ã—ç²’åº¦ï¼ˆRFxGï¼‰ï¼Œç”¨äºæ”¹è¿›æ·±åº¦å­¦ä¹ ä¸­æ˜¾è‘—æ€§è§£é‡Šçš„è¯„ä¼°å’Œç”¨æˆ·æ„å›¾çš„å¯¹é½ã€‚æ˜¾è‘—æ€§å›¾åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å¯¹äºå…¶ç›®çš„å’Œä¸ç”¨æˆ·æŸ¥è¯¢çš„å¯¹é½å­˜åœ¨å…±è¯†ç¼ºä¹çš„é—®é¢˜ã€‚RFxGæ¡†æ¶é€šè¿‡åŒºåˆ†ç‚¹å¯¹ç‚¹å’Œå¯¹æ¯”è§£é‡Šï¼Œä»¥åŠç»†ç²’åº¦å’Œç²—ç²’åº¦çš„è§£é‡Šï¼Œç³»ç»ŸåŒ–äº†æ˜¾è‘—æ€§è§£é‡Šçš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å››ç§æ–°çš„å¿ å®åº¦æŒ‡æ ‡ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°è¯„ä¼°ç°æœ‰æ˜¾è‘—æ€§æ–¹æ³•çš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18423', 'title': 'General Agentic Memory Via Deep Research', 'url': 'https://huggingface.co/papers/2511.18423', 'abstract': 'GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of "just-in time (JIT) compilation" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.', 'score': 157, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '45deed7c794d5cb3', 'authors': ['B. Y. Yan', 'Chaofan Li', 'Hongjin Qian', 'Shuqi Lu', 'Zheng Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Hong Kong Polytechnic University', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18423.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GAM â€” Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ just-in-time ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚ĞµÑ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, GAM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Memorizer Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Researcher Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ (page-store), Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-ÑÑ‚Ğ°Ğ¿Ğµ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Optimizing AI Memory with Just-in-Time Learning', 'desc': 'The paper introduces a new framework called General Agentic Memory (GAM) that enhances memory efficiency for AI agents by using principles from just-in-time (JIT) compilation. Unlike traditional static memory systems that can lose important information, GAM creates optimized memory contexts at runtime while retaining essential historical data in a universal page-store. It features a dual-component design: a Memorizer that captures key information and a Researcher that retrieves relevant data for real-time tasks. The framework shows significant improvements in task completion performance compared to existing memory systems, leveraging reinforcement learning for optimization.'}, 'zh': {'title': 'GAMï¼šä¼˜åŒ–è®°å¿†ç®¡ç†çš„æ–°æ¡†æ¶', 'desc': 'GAMæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé‡‡ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰åŸåˆ™ï¼Œæ—¨åœ¨æé«˜å†…å­˜æ•ˆç‡å’Œä»»åŠ¡å®Œæˆåº¦ã€‚å®ƒé€šè¿‡è½»é‡çº§çš„è®°å¿†å™¨å’Œç ”ç©¶è€…çš„ç»“åˆï¼Œä¼˜åŒ–äº†AIä»£ç†çš„è®°å¿†ç®¡ç†ã€‚GAMåœ¨ç¦»çº¿é˜¶æ®µä¿ç•™ç®€å•ä½†æœ‰ç”¨çš„è®°å¿†ï¼Œè€Œåœ¨è¿è¡Œæ—¶åˆ›å»ºä¼˜åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œå‡å°‘ä¿¡æ¯æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGAMåœ¨å¤šç§åŸºäºè®°å¿†çš„ä»»åŠ¡å®Œæˆåœºæ™¯ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰çš„å†…å­˜ç³»ç»Ÿï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19304', 'title': 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning', 'url': 'https://huggingface.co/papers/2511.19304', 'abstract': 'AutoEnv and AutoEnv-36 provide a standardized framework and dataset for evaluating cross-environment learning in agents, highlighting the challenges and limitations of existing learning methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.', 'score': 89, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '0e253e5c3dd456fa', 'authors': ['Jiayi Zhang', 'Yiran Peng', 'Fanqi Kong', 'Cheng Yang', 'Yifan Wu', 'Zhaoyang Yu', 'Jinyu Xiang', 'Jianhao Ruan', 'Jinlin Wang', 'Maojia Song', 'HongZhang Liu', 'Xiangru Tang', 'Bang Liu', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'Mila', 'Peking University', 'Singapore University of Technology and Design', 'Sydney University', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Universite de Montreal', 'Yale University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19304.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ: Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AutoEnv â€” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ², Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AutoEnv-36 Ğ¸Ğ· 36 Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑĞµĞ¼ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 12-49% Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ’Ñ‹Ğ±Ğ¾Ñ€, ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞÑ†ĞµĞ½ĞºĞ°, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑÑĞ½Ğ²Ğ°Ğ¹Ñ€Ğ¾Ğ½Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹.'}, 'en': {'title': 'Empowering Agents to Learn Across Diverse Environments', 'desc': 'The paper introduces AutoEnv and AutoEnv-36, a framework and dataset designed to evaluate how well agents can learn across different environments. It highlights that while humans adapt to various settings, current AI agents often struggle because they are trained in fixed environments. AutoEnv allows for the creation of diverse environments by treating them as distributions of transitions, observations, and rewards, leading to the development of 36 unique environments. The study shows that traditional learning methods do not perform well as the number of environments increases, emphasizing the need for adaptive learning strategies in cross-environment scenarios.'}, 'zh': {'title': 'è·¨ç¯å¢ƒå­¦ä¹ çš„æ–°æ ‡å‡†ä¸æŒ‘æˆ˜', 'desc': 'AutoEnvå’ŒAutoEnv-36æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸­çš„å­¦ä¹ èƒ½åŠ›ã€‚ç°æœ‰çš„å­¦ä¹ æ–¹æ³•é€šå¸¸å‡è®¾ç¯å¢ƒæ˜¯å›ºå®šçš„ï¼Œè€ŒAutoEnvé€šè¿‡å°†ç¯å¢ƒè§†ä¸ºå¯åˆ†è§£çš„åˆ†å¸ƒï¼Œå…è®¸ç”Ÿæˆå¤šæ ·åŒ–çš„ç¯å¢ƒã€‚æˆ‘ä»¬æ„å»ºäº†AutoEnv-36æ•°æ®é›†ï¼ŒåŒ…å«36ä¸ªç¯å¢ƒå’Œ358ä¸ªéªŒè¯çº§åˆ«ï¼Œå±•ç¤ºäº†åœ¨è¿™äº›ç¯å¢ƒä¸­å­¦ä¹ çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå›ºå®šçš„å­¦ä¹ æ–¹æ³•åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­æ•ˆæœè¿…é€Ÿä¸‹é™ï¼Œè€Œç¯å¢ƒè‡ªé€‚åº”çš„å­¦ä¹ æ–¹æ³•è™½ç„¶èƒ½æé«˜æ€§èƒ½ï¼Œä½†åœ¨æ–¹æ³•ç©ºé—´æ‰©å±•æ—¶æ”¶ç›Šé€’å‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19365', 'title': 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation', 'url': 'https://huggingface.co/papers/2511.19365', 'abstract': 'The frequency-DeCoupled pixel diffusion framework improves image generation efficiency and quality by separating high-frequency details and low-frequency semantics, achieving superior performance compared to existing pixel diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.', 'score': 63, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '1bfd45ed7112fc2f', 'authors': ['Zehong Ma', 'Longhui Wei', 'Shuai Wang', 'Shiliang Zhang', 'Qi Tian'], 'affiliations': ['Huawei Inc.', 'Nanjing University', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19365.jpg', 'data': {'categories': ['#cv', '#optimization', '#architecture', '#diffusion', '#open_source', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VAE. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº FID Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Decoupling Frequencies for Superior Image Generation', 'desc': 'The frequency-DeCoupled pixel diffusion framework enhances image generation by separating high-frequency details from low-frequency semantics. This method allows for more efficient training and inference by using a lightweight pixel decoder for high-frequency generation, while a diffusion transformer focuses on low-frequency aspects. By implementing a frequency-aware flow-matching loss, the model prioritizes important visual frequencies and reduces noise from less significant ones. Experimental results demonstrate that this approach significantly outperforms existing pixel diffusion models, achieving competitive scores on benchmark datasets.'}, 'zh': {'title': 'é¢‘ç‡è§£è€¦ï¼Œæå‡å›¾åƒç”Ÿæˆæ•ˆç‡ä¸è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢‘ç‡è§£è€¦åƒç´ æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜é¢‘ç»†èŠ‚å’Œä½é¢‘è¯­ä¹‰åˆ†å¼€å¤„ç†ï¼Œå…‹æœäº†ç°æœ‰åƒç´ æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„æ…¢é€Ÿé—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨è½»é‡çº§çš„åƒç´ è§£ç å™¨ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ï¼ŒåŒæ—¶åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ä¸“æ³¨äºä½é¢‘è¯­ä¹‰å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥çš„é¢‘ç‡æ„ŸçŸ¥æµåŒ¹é…æŸå¤±å¼ºè°ƒäº†è§†è§‰ä¸Šæ˜¾è‘—çš„é¢‘ç‡ï¼Œä»è€Œè¿›ä¸€æ­¥æå‡äº†ç”Ÿæˆæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19399', 'title': 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research', 'url': 'https://huggingface.co/papers/2511.19399', 'abstract': 'Reinforcement Learning with Evolving Rubrics (RLER) enables training of deep research models for long-form tasks, outperforming existing models and proprietary systems while being more cost-effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.', 'score': 55, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '3bd93d1f23649d74', 'authors': ['Rulin Shao', 'Akari Asai', 'Shannon Zejiang Shen', 'Hamish Ivison', 'Varsha Kishore', 'Jingming Zhuo', 'Xinran Zhao', 'Molly Park', 'Samuel G. Finlayson', 'David Sontag', 'Tyler Murray', 'Sewon Min', 'Pradeep Dasigi', 'Luca Soldaini', 'Faeze Brahman', 'Wen-tau Yih', 'Tongshuang Wu', 'Luke Zettlemoyer', 'Yoon Kim', 'Hannaneh Hajishirzi', 'Pang Wei Koh'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Seattle Childrens Hospital', 'University of California, Berkeley', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19399.jpg', 'data': {'categories': ['#dataset', '#rl', '#reasoning', '#science', '#benchmark', '#healthcare', '#rlhf', '#open_source', '#training', '#agents'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforcement Learning with Evolving Rubrics (RLER) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ½Ğ¾ RLER Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ÑƒĞ±Ñ€Ğ¸Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Deep Research Tulu-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Evolving Rubrics for Superior Long-Form Research Models', 'desc': "Reinforcement Learning with Evolving Rubrics (RLER) introduces a novel approach to training deep research models for long-form tasks, which are typically more complex than short-form question answering. Unlike traditional methods that rely on fixed rewards, RLER adapts the evaluation criteria, or rubrics, in tandem with the model's learning process, allowing for more relevant and nuanced feedback. This method led to the development of Deep Research Tulu (DR Tulu-8B), a model specifically designed for open-ended research tasks, which significantly outperforms existing models in various domains. The authors also provide all resources, including data and code, to support further advancements in deep research systems."}, 'zh': {'title': 'è¿›åŒ–è¯„åˆ†æ ‡å‡†åŠ©åŠ›æ·±åº¦ç ”ç©¶æ¨¡å‹', 'desc': 'å¼ºåŒ–å­¦ä¹ ä¸è¿›åŒ–è¯„åˆ†æ ‡å‡†ï¼ˆRLERï¼‰ä½¿å¾—æ·±åº¦ç ”ç©¶æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿ç¯‡ä»»åŠ¡ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹å’Œä¸“æœ‰ç³»ç»Ÿï¼ŒåŒæ—¶æˆæœ¬æ›´ä½ã€‚å¤§å¤šæ•°å¼€æ”¾çš„æ·±åº¦ç ”ç©¶æ¨¡å‹ä»…åœ¨çŸ­ç¯‡é—®ç­”ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— æ³•é€‚åº”å¤æ‚çš„é•¿ç¯‡ä»»åŠ¡ã€‚RLERé€šè¿‡æ„å»ºå’Œç»´æŠ¤ä¸ç­–ç•¥æ¨¡å‹å…±åŒè¿›åŒ–çš„è¯„åˆ†æ ‡å‡†ï¼Œæä¾›äº†æ›´å…·åŒºåˆ†æ€§çš„åé¦ˆï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­æ”¹è¿›ã€‚æˆ‘ä»¬å¼€å‘çš„æ·±åº¦ç ”ç©¶Tuluï¼ˆDR Tulu-8Bï¼‰æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å¼€æ”¾å¼é•¿ç¯‡æ·±åº¦ç ”ç©¶è®­ç»ƒçš„å¼€æ”¾æ¨¡å‹ï¼Œè¡¨ç°ä¼˜å¼‚ä¸”èµ„æºæ¶ˆè€—ä½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15567', 'title': 'Computer-Use Agents as Judges for Generative User Interface', 'url': 'https://huggingface.co/papers/2511.15567', 'abstract': 'A framework leveraging Computer-Use Agents as judges to assist coding-oriented language models in designing efficient and functional GUIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.', 'score': 51, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '10a52993cad05a95', 'authors': ['Kevin Qinghong Lin', 'Siyuan Hu', 'Linjie Li', 'Zhengyuan Yang', 'Lijuan Wang', 'Philip Torr', 'Mike Zheng Shou'], 'affiliations': ['Microsoft', 'Show Lab, National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15567.jpg', 'data': {'categories': ['#cv', '#dataset', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ°Ğº ÑÑƒĞ´ÑŒĞ¸: Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AUI-Gym â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ², Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ â€” ÑÑƒĞ´ÑŒÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 1560 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 52 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Coder-CUA Ğ² ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ, Ğ³Ğ´Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ÑÑ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· CUA Dashboard. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Empowering Agents in GUI Design: Efficiency Over Aesthetics', 'desc': 'This paper presents a new framework that uses Computer-Use Agents (CUA) as judges to help coding-oriented language models (Coder) create better Graphical User Interfaces (GUI). The authors introduce AUI-Gym, a benchmark that includes 52 applications and 1560 tasks to simulate real-world scenarios for automatic GUI design. The Coder generates and revises the GUI, while the CUA evaluates its functionality and provides feedback for improvements. The focus is on making GUIs more efficient for agents rather than just visually appealing for humans, enhancing the overall usability in digital environments.'}, 'zh': {'title': 'ä»£ç†ä¸ç¼–ç æ¨¡å‹åä½œï¼Œæå‡GUIè®¾è®¡æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰ä½œä¸ºè¯„å®¡ï¼Œå¸®åŠ©ç¼–ç å¯¼å‘çš„è¯­è¨€æ¨¡å‹ï¼ˆCoderï¼‰è®¾è®¡é«˜æ•ˆä¸”åŠŸèƒ½é½å…¨çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†AUI-Gymï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–52ä¸ªåº”ç”¨ç¨‹åºçš„è‡ªåŠ¨GUIå¼€å‘åŸºå‡†ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹åˆæˆ1560ä¸ªæ¨¡æ‹ŸçœŸå®åœºæ™¯çš„ä»»åŠ¡ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªéªŒè¯å™¨ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡åœ¨å…¶ç¯å¢ƒä¸­å¯æ‰§è¡Œï¼Œä»è€Œæé«˜ä»»åŠ¡çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†CUAä½œä¸ºè¯„å®¡ï¼Œè¯„ä¼°åŠŸèƒ½æ€§å¹¶ä¼˜åŒ–è®¾è®¡ï¼Œæ¨åŠ¨ä»£ç†åœ¨æ•°å­—ç¯å¢ƒä¸­çš„ä¸»åŠ¨å‚ä¸ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18050', 'title': 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios', 'url': 'https://huggingface.co/papers/2511.18050', 'abstract': 'UltraFlux, a Flux-based DiT trained on a 4K dataset, addresses failures in diffusion transformers at 4K resolution through enhanced positional encoding, improved VAE compression, gradient rebalancing, and aesthetic curriculum learning, achieving superior performance compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.', 'score': 37, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '7ab1aa44b87d2af4', 'authors': ['Tian Ye', 'Song Fei', 'Lei Zhu'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18050.jpg', 'data': {'categories': ['#optimization', '#dataset', '#architecture', '#diffusion', '#benchmark', '#open_source', '#multimodal', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞÑ‚ ĞºĞ¸Ğ»Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ¼ĞµĞ³Ğ°Ğ¿Ğ¸ĞºÑĞµĞ»ÑĞ¼: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² 4K', 'desc': 'UltraFlux â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Flux, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ 4K Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RoPE Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ Ğ´Ğ»Ğ¸Ğ½ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ VAE Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ SNR-Aware Huber Wavelet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑƒÑ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Stage-wise Aesthetic Curriculum Learning, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² 4K Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½.'}, 'en': {'title': 'UltraFlux: Elevating 4K Image Generation with Advanced Techniques', 'desc': "UltraFlux is a new model designed to improve the performance of diffusion transformers for generating images at 4K resolution. It addresses key issues like positional encoding and VAE compression that affect image quality when scaling up to higher resolutions. By using advanced techniques such as gradient rebalancing and aesthetic curriculum learning, UltraFlux enhances the model's ability to produce detailed and aesthetically pleasing images. The model has been trained on a large dataset and shows superior results compared to existing models in various metrics related to image fidelity and aesthetics."}, 'zh': {'title': 'UltraFluxï¼šæå‡4Kå›¾åƒç”Ÿæˆçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ', 'desc': 'UltraFluxæ˜¯ä¸€ç§åŸºäºFluxçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰ï¼Œä¸“é—¨é’ˆå¯¹4Kåˆ†è¾¨ç‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡å¢å¼ºçš„ä½ç½®ç¼–ç ã€æ”¹è¿›çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å‹ç¼©ã€æ¢¯åº¦é‡å¹³è¡¡å’Œç¾å­¦è¯¾ç¨‹å­¦ä¹ ï¼Œè§£å†³äº†åœ¨4Kåˆ†è¾¨ç‡ä¸‹æ‰©æ•£å˜æ¢å™¨çš„å¤±è´¥é—®é¢˜ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒUltraFluxåœ¨å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€ç¾å­¦å’Œå¯¹é½åº¦ç­‰æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„é•¿å®½æ¯”ä¸‹ç¨³å®šç”Ÿæˆé«˜è´¨é‡çš„4Kå›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19401', 'title': 'In-Video Instructions: Visual Signals as Generative Control', 'url': 'https://huggingface.co/papers/2511.19401', 'abstract': 'Video generative models can interpret and execute visual instructions embedded within frames, enhancing controllability in image-to-video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.', 'score': 30, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'f2e6e877078bd945', 'authors': ['Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19401.jpg', 'data': {'categories': ['#video', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² ĞºĞ°Ğ´Ñ€Ñ‹, Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑÑ‚Ñ€ĞµĞ»ĞºĞ¸, Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (Veo 3.1, Kling 2.5, Wan 2.2) Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ.'}, 'en': {'title': 'Controllable Video Generation with Visual Instructions', 'desc': 'This paper explores a new method called In-Video Instruction for generating videos from images. It allows users to embed visual instructions directly into video frames, such as arrows or text, which helps the model understand what actions to take. Unlike traditional prompt-based methods that use text descriptions, this approach provides clearer and more precise guidance for each object in the scene. The authors demonstrate that their method works effectively with advanced video generative models, even in complicated situations with multiple objects.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆä¸­çš„å¯æ§æ€§ï¼šé€šè¿‡è§†è§‰æŒ‡ä»¤å®ç°', 'desc': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒåµŒå…¥åœ¨å¸§ä¸­çš„è§†è§‰æŒ‡ä»¤ï¼Œä»è€Œå¢å¼ºå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„å¯æ§æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºâ€œè§†é¢‘å†…æŒ‡ä»¤â€ï¼Œé€šè¿‡åœ¨è§†è§‰åŸŸä¸­ç›´æ¥ç¼–ç ç”¨æˆ·æŒ‡å¯¼ï¼Œæ¥å®ç°å¯æ§çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚ä¸åŸºäºæ–‡æœ¬çš„æç¤ºæ§åˆ¶ä¸åŒï¼Œè§†é¢‘å†…æŒ‡ä»¤ä½¿ç”¨å åŠ æ–‡æœ¬ã€ç®­å¤´æˆ–è½¨è¿¹ç­‰å…ƒç´ ï¼Œæä¾›æ˜ç¡®çš„ç©ºé—´æ„ŸçŸ¥å’Œæ— æ­§ä¹‰çš„è§†è§‰æŒ‡ä»¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è§£é‡Šå’Œæ‰§è¡Œè¿™äº›åµŒå…¥çš„è§†è§‰æŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19418', 'title': 'Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens', 'url': 'https://huggingface.co/papers/2511.19418', 'abstract': 'Chain-of-Visual-Thought (COVT) enables Vision-Language Models to reason through visual tokens, improving their performance on perceptual tasks by capturing dense visual information.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.', 'score': 27, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'efa9eec6c65df1f9', 'authors': ['Yiming Qin', 'Bomin Wei', 'Jiaxin Ge', 'Konstantinos Kallidromitis', 'Stephanie Fu', 'Trevor Darrell', 'XuDong Wang'], 'affiliations': ['Panasonic AI Research', 'UC Berkeley', 'UCLA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19418.jpg', 'data': {'categories': ['#cv', '#reasoning', '#benchmark', '#multimodal', '#interpretability', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Chain-of-Visual-Thought (COVT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğ¾ Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ¾Ğ³Ğ°Ñ‚ÑƒÑ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. COVT Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¸Ğ· 20 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°: 2D-Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ’Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ DINO. ĞŸÑ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Vision-Language Models with Visual Reasoning', 'desc': "Chain-of-Visual-Thought (COVT) enhances Vision-Language Models (VLMs) by allowing them to reason with visual tokens, which are compact representations of visual information. This approach addresses the challenge VLMs face in understanding complex visual tasks that require detailed spatial and geometric reasoning. By using a limited number of visual tokens, COVT captures essential visual features like depth and edge structure, improving the model's ability to interpret and predict visual data. The integration of COVT into existing VLMs has shown significant performance improvements across various perception benchmarks, demonstrating its effectiveness in enhancing multimodal intelligence."}, 'zh': {'title': 'é“¾å¼è§†è§‰æ€ç»´æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›', 'desc': 'é“¾å¼è§†è§‰æ€ç»´ï¼ˆCOVTï¼‰ä½¿å¾—è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è§†è§‰æ ‡è®°è¿›è¡Œæ¨ç†ï¼Œä»è€Œæå‡å…¶åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éœ€è¦å¯†é›†è§†è§‰æ„ŸçŸ¥çš„ä»»åŠ¡ä¸­ï¼Œå¦‚ç©ºé—´æ¨ç†å’Œå‡ ä½•æ„è¯†ï¼Œè¡¨ç°è¾ƒå·®ã€‚COVTæ¡†æ¶é€šè¿‡ä½¿ç”¨è¿ç»­çš„è§†è§‰æ ‡è®°ï¼Œæ•æ‰ä¸°å¯Œçš„æ„ŸçŸ¥çº¿ç´¢ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒä¸­é‡å»ºå¯†é›†çš„ç›‘ç£ä¿¡å·ã€‚ç»è¿‡è¯„ä¼°ï¼ŒCOVTçš„é›†æˆæ˜¾è‘—æé«˜äº†å¤šç§æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ä¸­çš„æ¨¡å‹æ€§èƒ½ï¼Œè¯æ˜äº†ç´§å‡‘çš„è§†è§‰æ€ç»´èƒ½å¤Ÿå¢å¼ºå¤šæ¨¡æ€æ™ºèƒ½çš„ç²¾ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20256', 'title': 'The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation', 'url': 'https://huggingface.co/papers/2511.20256', 'abstract': 'An adversarial reward learning framework in reinforcement learning for image generation improves image quality and aesthetics by using dense visual signals from vision foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.', 'score': 26, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '6ac56cd7871128d7', 'authors': ['Weijia Mao', 'Hao Chen', 'Zhenheng Yang', 'Mike Zheng Shou'], 'affiliations': ['ByteDance', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20256.jpg', 'data': {'categories': ['#cv', '#training', '#rl'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Adv-GRPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ±Ğ¾Ñ€ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ñ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº DINO) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ°Ğ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ñ…Ğ°ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ ÑƒÑĞ¿ĞµÑ…Ğ° 70-72% Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ.'}, 'en': {'title': 'Enhancing Image Generation with Adversarial Rewards', 'desc': 'This paper presents Adv-GRPO, a novel reinforcement learning framework that enhances image generation by utilizing an adversarial reward system. Unlike traditional methods that rely on scalar rewards from pre-trained models, Adv-GRPO employs dense visual signals from vision foundation models to provide richer feedback. This approach mitigates issues like reward hacking and biases found in existing reward functions, leading to improved image quality and aesthetics. The framework demonstrates superior performance in human evaluations, achieving higher win rates compared to previous methods.'}, 'zh': {'title': 'å¯¹æŠ—å¥–åŠ±å­¦ä¹ æå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹æŠ—å¥–åŠ±å­¦ä¹ æ¡†æ¶ï¼ˆAdv-GRPOï¼‰ï¼Œç”¨äºå›¾åƒç”Ÿæˆä¸­çš„å¼ºåŒ–å­¦ä¹ ã€‚è¯¥æ¡†æ¶é€šè¿‡ä½¿ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æä¾›çš„å¯†é›†è§†è§‰ä¿¡å·ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œç¾å­¦ã€‚ä¸ä¼ ç»Ÿçš„æ ‡é‡å¥–åŠ±æ¨¡å‹ä¸åŒï¼ŒAdv-GRPOé€šè¿‡å‚è€ƒå›¾åƒä½œä¸ºæ­£æ ·æœ¬æ¥ç›‘ç£å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œå‡å°‘äº†å¥–åŠ±è¢«æ“æ§çš„é£é™©ã€‚æœ€ç»ˆï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œç¾å­¦æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17006', 'title': 'Budget-Aware Tool-Use Enables Effective Agent Scaling', 'url': 'https://huggingface.co/papers/2511.17006', 'abstract': 'Budget-aware methods improve the scaling of tool-augmented agents by providing continuous budget awareness and adaptive planning, leading to better cost-performance trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only "thinking" in tokens but also "acting" via tool calls. The number of tool calls directly bounds the agent\'s interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack "budget awareness" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to "dig deeper" on a promising lead or "pivot" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.', 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'ae8b49b11d8db602', 'authors': ['Tengxiao Liu', 'Zifeng Wang', 'Jin Miao', 'I-Hung Hsu', 'Jun Yan', 'Jiefeng Chen', 'Rujun Han', 'Fangyuan Xu', 'Yanfei Chen', 'Ke Jiang', 'Samira Daruki', 'Yi Liang', 'William Yang Wang', 'Tomas Pfister', 'Chen-Yu Lee'], 'affiliations': ['Google Cloud AI Research', 'Google DeepMind', 'New York University', 'UC Santa Barbara'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17006.jpg', 'data': {'categories': ['#training', '#agents'], 'emoji': 'ğŸ’°', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹, Ğ¾ÑĞ¾Ğ·Ğ½Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ Ğ±ÑĞ´Ğ¶ĞµÑ‚: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ. Ğ ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Budget Tracker, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…, Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BATS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Agents with Budget Awareness for Optimal Performance', 'desc': 'This paper introduces budget-aware methods to enhance the performance of tool-augmented agents by ensuring they are continuously aware of their resource limits. The authors present the Budget Tracker, a tool that helps agents manage their tool-call budgets effectively, leading to improved decision-making during task execution. They also propose BATS, a framework that allows agents to adapt their strategies based on remaining resources, optimizing their actions between exploring deeper or shifting focus. The study provides a unified cost metric for evaluating performance, demonstrating that budget-aware approaches yield better cost-performance trade-offs and improve scaling in these agents.'}, 'zh': {'title': 'é¢„ç®—æ„è¯†æå‡å·¥å…·å¢å¼ºä»£ç†çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†é¢„ç®—æ„è¯†æ–¹æ³•å¦‚ä½•æ”¹å–„å·¥å…·å¢å¼ºä»£ç†çš„æ‰©å±•æ€§ã€‚é€šè¿‡æä¾›æŒç»­çš„é¢„ç®—æ„è¯†å’Œè‡ªé€‚åº”è§„åˆ’ï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´å¥½çš„æˆæœ¬ä¸æ€§èƒ½çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†é¢„ç®—è·Ÿè¸ªå™¨å’ŒBATSæ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®å‰©ä½™èµ„æºåŠ¨æ€è°ƒæ•´å…¶è§„åˆ’ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé¢„ç®—æ„è¯†æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ¨åŠ¨æˆæœ¬æ€§èƒ½çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæå‡ä»£ç†åœ¨é¢„ç®—é™åˆ¶ä¸‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18870', 'title': 'HunyuanVideo 1.5 Technical Report', 'url': 'https://huggingface.co/papers/2511.18870', 'abstract': 'HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network.  \t\t\t\t\tAI-generated summary \t\t\t\t We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.', 'score': 22, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '439a866927eb13d5', 'authors': ['Bing Wu', 'Chang Zou', 'Changlin Li', 'Duojun Huang', 'Fang Yang', 'Hao Tan', 'Jack Peng', 'Jianbing Wu', 'Jiangfeng Xiong', 'Jie Jiang', 'Linus', 'Patrol', 'Peizhen Zhang', 'Peng Chen', 'Penghao Zhao', 'Qi Tian', 'Songtao Liu', 'Weijie Kong', 'Weiyan Wang', 'Xiao He', 'Xin Li', 'Xinchi Deng', 'Xuefei Zhe', 'Yang Li', 'Yanxin Long', 'Yuanbo Peng', 'Yue Wu', 'Yuhong Liu', 'Zhenyu Wang', 'Zuozhuo Dai', 'Bo Peng', 'Coopers Li', 'Gu Gong', 'Guojian Xiao', 'Jiahe Tian', 'Jiaxin Lin', 'Jie Liu', 'Jihong Zhang', 'Jiesong Lian', 'Kaihang Pan', 'Lei Wang', 'Lin Niu', 'Mingtao Chen', 'Mingyang Chen', 'Mingzhe Zheng', 'Miles Yang', 'Qiangqiang Hu', 'Qi Yang', 'Qiuyong Xiao', 'Runzhou Wu', 'Ryan Xu', 'Rui Yuan', 'Shanshan Sang', 'Shisheng Huang', 'Siruis Gong', 'Shuo Huang', 'Weiting Guo', 'Xiang Yuan', 'Xiaojia Chen', 'Xiawei Hu', 'Wenzhi Sun', 'Xiele Wu', 'Xianshun Ren', 'Xiaoyan Yuan', 'Xiaoyue Mi', 'Yepeng Zhang', 'Yifu Sun', 'Yiting Lu', 'Yitong Li', 'You Huang', 'Yu Tang', 'Yixuan Li', 'Yuhang Deng', 'Yuan Zhou', 'Zhichao Hu', 'Zhiguang Liu', 'Zhihe Yang', 'Zilin Yang', 'Zhenzhi Lu', 'Zixiang Zhou', 'Zhao Zhong'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18870.jpg', 'data': {'categories': ['#video', '#architecture', '#multilingual', '#open_source', '#inference', '#data'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²ÑĞµÑ…', 'desc': 'HunyuanVideo 1.5 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ Ğ²ÑĞµĞ³Ğ¾ 8.3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ DiT Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (SSTA) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ĞºÑ€ÑƒĞ³Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Video Generation with HunyuanVideo 1.5', 'desc': 'HunyuanVideo 1.5 is a cutting-edge video generation model that combines high visual quality with smooth motion coherence while being lightweight, containing only 8.3 billion parameters. It utilizes a DiT architecture enhanced with selective and sliding tile attention (SSTA) and incorporates an efficient video super-resolution network. The model supports both text-to-video and image-to-video generation, allowing for various durations and resolutions. By making this model open-source, it aims to democratize access to advanced video generation technology for researchers and creators alike.'}, 'zh': {'title': 'è½»é‡çº§è§†é¢‘ç”Ÿæˆï¼Œè§†è§‰è´¨é‡æ–°æ ‡æ†', 'desc': 'HunyuanVideo 1.5 æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰å…ˆè¿›çš„è§†è§‰è´¨é‡å’Œè¿åŠ¨ä¸€è‡´æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº† DiT æ¶æ„å’Œé€‰æ‹©æ€§æ»‘åŠ¨å—æ³¨æ„åŠ›ï¼ˆSSTAï¼‰ï¼Œå¹¶ç»“åˆäº†é«˜æ•ˆçš„è§†é¢‘è¶…åˆ†è¾¨ç‡ç½‘ç»œã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ•´ç†å’ŒåŒè¯­ç†è§£çš„å¢å¼ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªæ—¶é•¿å’Œåˆ†è¾¨ç‡ä¸‹å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬å°†ä»£ç å’Œæ¨¡å‹æƒé‡å…¬å¼€ï¼Œé™ä½äº†è§†é¢‘åˆ›ä½œå’Œç ”ç©¶çš„é—¨æ§›ï¼Œä½¿æ›´å¹¿æ³›çš„ç”¨æˆ·èƒ½å¤Ÿæ¥è§¦åˆ°å…ˆè¿›çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17803', 'title': 'Pillar-0: A New Frontier for Radiology Foundation Models', 'url': 'https://huggingface.co/papers/2511.17803', 'abstract': 'Pillar-0, a radiology foundation model pretrained on diverse imaging datasets, outperforms existing models across various tasks and extends to new applications using RATE for label extraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.', 'score': 19, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '9690d6043af51893', 'authors': ['Kumar Krishna Agrawal', 'Longchao Liu', 'Long Lian', 'Michael Nercessian', 'Natalia Harguindeguy', 'Yufu Wu', 'Peter Mikhael', 'Gigin Lin', 'Lecia V. Sequist', 'Florian Fintelmann', 'Trevor Darrell', 'Yutong Bai', 'Maggie Chung', 'Adam Yala'], 'affiliations': ['Clinical Metabolomics Core and Imaging Core Laboratory, Institute for Radiological Research, Chang Gung Memorial Hospital at Linkou and Chang Gung University, Taiwan', 'Computational Precision Health, UC Berkeley and UC San Francisco, USA', 'Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA', 'Department of Electrical Engineering and Computer Science, UC Berkeley, USA', 'Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou, Taiwan', 'Department of Medical Imaging and Radiological Sciences, Chang Gung University, Taiwan', 'Department of Radiology and Biomedical Imaging, UC San Francisco, USA', 'Harvard Medical School, USA', 'Mass General Brigham Cancer Institute, USA', 'Massachusetts General Hospital, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17803.jpg', 'data': {'categories': ['#dataset', '#science', '#benchmark', '#transfer_learning', '#healthcare', '#open_source', '#multimodal'], 'emoji': 'ğŸ«€', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸', 'desc': 'Pillar-0 â€” ÑÑ‚Ğ¾ foundation model Ğ´Ğ»Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (ĞšĞ¢ Ğ¸ ĞœĞ Ğ˜) Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğµ Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ RATE â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ 366 Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Pillar-0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€Ğ°ĞºĞ° Ğ»Ñ‘Ğ³ĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ·Ğ³Ğ¾Ğ²Ñ‹Ñ… ĞºÑ€Ğ¾Ğ²Ğ¾Ğ¸Ğ·Ğ»Ğ¸ÑĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Pillar-0: Revolutionizing Radiology with Advanced Imaging Insights', 'desc': 'Pillar-0 is a radiology foundation model that has been pretrained on a vast array of imaging datasets, significantly enhancing its performance across various radiology tasks. Unlike existing models that treat volumetric CT and MRI scans as low-quality 2D images, Pillar-0 retains critical grayscale contrast information, leading to improved accuracy in detecting radiologic findings. The model utilizes RATE, a framework that efficiently extracts structured labels for numerous findings, achieving near-perfect accuracy. With superior performance metrics, Pillar-0 not only surpasses other leading models but also extends its capabilities to new applications, such as lung cancer risk prediction and brain hemorrhage detection, making it a robust tool for modern radiology.'}, 'zh': {'title': 'Pillar-0ï¼šæ”¾å°„å­¦çš„æœªæ¥åŸºç¡€æ¨¡å‹', 'desc': 'Pillar-0æ˜¯ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨äº†å¤šç§å½±åƒæ•°æ®é›†ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¤„ç†ä½“ç§¯CTå’ŒMRIæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™é‡è¦çš„ç°åº¦å¯¹æ¯”ä¿¡æ¯ï¼Œå¹¶ä¸”å…·å¤‡åæ˜ çœŸå®ä¸´åºŠå®è·µçš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡RATEæ¡†æ¶ï¼ŒPillar-0èƒ½å¤Ÿé«˜æ•ˆæå–366ç§æ”¾å°„å­¦å‘ç°çš„ç»“æ„åŒ–æ ‡ç­¾ï¼Œå‡†ç¡®ç‡æ¥è¿‘å®Œç¾ã€‚Pillar-0åœ¨å¤šä¸ªå†…éƒ¨æµ‹è¯•é›†å’Œå¤–éƒ¨éªŒè¯ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨äº†æ”¾å°„å­¦ä»»åŠ¡çš„æ€§èƒ½è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.13288', 'title': 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO', 'url': 'https://huggingface.co/papers/2511.13288', 'abstract': 'M-GRPO, an extension of Group Relative Policy Optimization for hierarchical multi-agent systems, improves stability and efficiency in tool-augmented reasoning tasks by aligning heterogeneous trajectories and decoupling agent training.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '73b58df5a8f5c31b', 'authors': ['Haoyang Hong', 'Jiajun Yin', 'Yuan Wang', 'Jingnan Liu', 'Zhe Chen', 'Ailing Yu', 'Ji Li', 'Zhiling Ye', 'Hansong Xiao', 'Yefei Chen', 'Hualei Zhou', 'Yun Yue', 'Minghui Yang', 'Chunxiao Guo', 'Junwei Liu', 'Peng Wei', 'Jinjie Gu'], 'affiliations': ['Ant Group', 'Imperial College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.13288.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#rlhf', '#training', '#agents'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ M-GRPO â€” Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ³Ğ´Ğµ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ĞµĞµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞµÑ€Ğ²ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ M-GRPO Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Agent Training with M-GRPO', 'desc': 'M-GRPO is a new method designed to improve the training of hierarchical multi-agent systems, particularly in complex reasoning tasks. It addresses the challenges of training agents that operate at different frequencies and on separate servers by using a decoupled training approach. By aligning the trajectories of heterogeneous agents and maintaining a hierarchical credit assignment, M-GRPO enhances the stability and efficiency of the training process. Experiments show that this method outperforms existing approaches, leading to better performance in specialized reasoning tasks.'}, 'zh': {'title': 'M-GRPOï¼šæå‡å¤šæ™ºèƒ½ä½“æ¨ç†ä»»åŠ¡çš„ç¨³å®šæ€§ä¸æ•ˆç‡', 'desc': 'M-GRPOæ˜¯å¯¹ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ‰©å±•ï¼Œä¸“ä¸ºå±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ã€‚å®ƒé€šè¿‡å¯¹å¼‚æ„è½¨è¿¹çš„å¯¹é½å’Œæ™ºèƒ½ä½“è®­ç»ƒçš„è§£è€¦ï¼Œæé«˜äº†å·¥å…·å¢å¼ºæ¨ç†ä»»åŠ¡çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è®¡ç®—ä¸»æ™ºèƒ½ä½“å’Œå­æ™ºèƒ½ä½“çš„ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ï¼Œä¿æŒå±‚æ¬¡åŒ–çš„ä¿¡ç”¨åˆ†é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17986', 'title': 'Plan-X: Instruct Video Generation via Semantic Planning', 'url': 'https://huggingface.co/papers/2511.17986', 'abstract': 'Plan-X integrates a Semantic Planner and diffusion models to reduce visual hallucinations and improve instruction-aligned video generation by using multimodal semantic tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user\'s intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': 'bbd0dd1dc8b70c17', 'authors': ['Lun Huang', 'You Xie', 'Hongyi Xu', 'Tianpei Gu', 'Chenxu Zhang', 'Guoxian Song', 'Zenan Li', 'Xiaochen Zhao', 'Linjie Luo', 'Guillermo Sapiro'], 'affiliations': ['Apple', 'ByteDance Intelligent Creation', 'Duke University', 'Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17986.jpg', 'data': {'categories': ['#video', '#alignment', '#architecture', '#diffusion', '#hallucinations', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Plan-X â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ»ÑƒĞ¶Ğ°Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Â«ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑĞºĞ¸Ğ·Ğ°Ğ¼Ğ¸Â» Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Plan-X: Enhancing Video Generation with Semantic Planning', 'desc': 'Plan-X is a novel framework that combines a Semantic Planner with diffusion models to enhance video generation by minimizing visual hallucinations. It utilizes multimodal semantic tokens to better align generated videos with user instructions, particularly in complex scenarios. The Semantic Planner interprets user intent from both text and visual inputs, creating structured semantic tokens that guide the video generation process. By integrating high-level reasoning with advanced visual synthesis, Plan-X significantly improves the quality and relevance of AI-generated videos.'}, 'zh': {'title': 'Plan-Xï¼šå‡å°‘è§†è§‰å¹»è§‰ï¼Œå®ç°æŒ‡ä»¤å¯¹é½çš„è§†é¢‘ç”Ÿæˆ', 'desc': 'Plan-X æ˜¯ä¸€ä¸ªç»“åˆäº†è¯­ä¹‰è§„åˆ’å™¨å’Œæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è§†è§‰å¹»è§‰å¹¶æ”¹å–„ä¸æŒ‡ä»¤å¯¹é½çš„è§†é¢‘ç”Ÿæˆã€‚å®ƒé€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€è¯­ä¹‰æ ‡è®°ï¼Œæ˜ç¡®æ‰§è¡Œé«˜å±‚æ¬¡çš„è¯­ä¹‰è§„åˆ’ï¼Œä»¥æŒ‡å¯¼è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ã€‚æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡ä¸­æ¨ç†ç”¨æˆ·æ„å›¾ï¼Œå¹¶è‡ªå›å½’ç”Ÿæˆä¸€ç³»åˆ—åŸºäºæ–‡æœ¬çš„æ—¶ç©ºè¯­ä¹‰æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼ŒPlan-X æ˜¾è‘—å‡å°‘äº†è§†è§‰å¹»è§‰ï¼Œå¹¶å®ç°äº†ä¸å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸€è‡´çš„ç»†ç²’åº¦æŒ‡ä»¤å¯¹é½è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17729', 'title': 'M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark', 'url': 'https://huggingface.co/papers/2511.17729', 'abstract': "M^3-Bench evaluates multimodal tool use with a focus on visual grounding, textual reasoning, and tool dependencies using a novel similarity-driven alignment method and interpretable metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench", 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '2ae5abf91a13d0fc', 'authors': ['Yang Zhou', 'Mingyu Zhao', 'Zhenting Wang', 'Difei Gu', 'Bangwei Guo', 'Ruosong Ye', 'Ligong Han', 'Can Jin', 'Dimitris N. Metaxas'], 'affiliations': ['Rutgers University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17729.jpg', 'data': {'categories': ['#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸ› ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ¼ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²', 'desc': 'M^3-Bench â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ° Model Context Protocol. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°-driven Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²ĞµĞ½Ğ³ĞµÑ€ÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Evaluating Multimodal Tool Use with M^3-Bench', 'desc': 'M^3-Bench is a new benchmark designed to assess how well multimodal models can use various tools in complex workflows. It focuses on tasks that require understanding both visual and textual information, as well as managing dependencies between different tools. The benchmark uses a unique similarity-driven alignment method to ensure accurate matching of tool calls and provides clear metrics to evaluate performance. Results show that current multimodal models struggle with maintaining argument fidelity and structural consistency, highlighting the need for improved reasoning across different data types.'}, 'zh': {'title': 'M^3-Benchï¼šå¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„è¯„ä¼°åŸºå‡†', 'desc': 'M^3-Benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å·¥å…·ä½¿ç”¨çš„åŸºå‡†ï¼Œé‡ç‚¹å…³æ³¨è§†è§‰å®šä½ã€æ–‡æœ¬æ¨ç†å’Œå·¥å…·ä¾èµ–æ€§ã€‚è¯¥åŸºå‡†é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ç›¸ä¼¼æ€§é©±åŠ¨å¯¹é½æ–¹æ³•ï¼Œèƒ½å¤Ÿåºåˆ—åŒ–æ¯ä¸ªå·¥å…·è°ƒç”¨ï¼Œå¹¶é€šè¿‡å¥å­ç¼–ç å™¨åµŒå…¥ç­¾åï¼Œè¿›è¡Œç›¸ä¼¼æ€§åˆ†æ¡¶çš„åŒˆç‰™åˆ©åŒ¹é…ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„å¯è§£é‡Šæ€§æŒ‡æ ‡å°†è¯­ä¹‰ä¿çœŸåº¦ä¸å·¥ä½œæµä¸€è‡´æ€§è§£è€¦ï¼Œå¸®åŠ©æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„è¡¨ç°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨è®ºè¯ä¿çœŸåº¦å’Œç»“æ„ä¸€è‡´æ€§æ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18922', 'title': 'One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control', 'url': 'https://huggingface.co/papers/2511.18922', 'abstract': 'One4D is a unified framework for 4D generation and reconstruction that uses a novel decoupled approach to produce high-quality RGB frames and pointmaps from varying sparsities of input frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '60b27b8165a2d909', 'authors': ['Zhenxing Mi', 'Yuxin Wang', 'Dan Xu'], 'affiliations': ['The Hong Kong University of Science and Technology (HKUST)'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18922.jpg', 'data': {'categories': ['#video', '#diffusion', '#synthetic', '#multimodal', '#training', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'One4D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ RGB-ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Unified Masked Conditioning (UMC), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ 4D Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ¸Ğ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ RGB Ğ¸ ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Decoupled LoRA Control (DLC) â€” Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ²ÑĞ·ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 4D Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'One4D: Revolutionizing 4D Generation and Reconstruction', 'desc': 'One4D is a new framework designed for generating and reconstructing 4D content, which includes synchronized RGB images and pointmaps. It uses a Unified Masked Conditioning (UMC) mechanism to effectively manage different levels of input frame sparsity, allowing for flexible transitions between generating from a single image and reconstructing from full videos. The framework incorporates Decoupled LoRA Control (DLC) to maintain high-quality outputs by separating the processing of RGB frames and pointmaps while ensuring they remain consistent. Trained on diverse datasets, One4D aims to enhance the quality of 4D world modeling using advanced video diffusion techniques.'}, 'zh': {'title': 'One4Dï¼šé«˜è´¨é‡4Dç”Ÿæˆä¸é‡å»ºçš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'One4Dæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äº4Dç”Ÿæˆå’Œé‡å»ºï¼Œèƒ½å¤Ÿä»ä¸åŒç¨€ç–åº¦çš„è¾“å…¥å¸§ä¸­ç”Ÿæˆé«˜è´¨é‡çš„RGBå¸§å’Œç‚¹å›¾ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„æ©ç æ¡ä»¶ï¼ˆUMCï¼‰æœºåˆ¶ï¼Œçµæ´»å¤„ç†æ¡ä»¶å¸§çš„ç¨€ç–æ€§ï¼Œå®ç°ä»å•å¼ å›¾åƒç”Ÿæˆ4Då†…å®¹ã€ä»å®Œæ•´è§†é¢‘é‡å»º4Då†…å®¹ä»¥åŠä»ç¨€ç–å¸§è¿›è¡Œæ··åˆç”Ÿæˆå’Œé‡å»ºã€‚One4Dé‡‡ç”¨å¼ºå¤§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œæ¶æ„ï¼Œè¿›è¡ŒRGBå¸§å’Œç‚¹å›¾çš„è”åˆç”Ÿæˆã€‚ä¸ºäº†è§£å†³æ·±åº¦å›¾æˆ–ç‚¹å›¾é‡å»ºä¸­å¸¸ç”¨çš„æ‰©æ•£å¾®è°ƒç­–ç•¥çš„ä¸è¶³ï¼ŒOne4Då¼•å…¥äº†è§£è€¦çš„LoRAæ§åˆ¶ï¼ˆDLCï¼‰ï¼Œé€šè¿‡ç‰¹å®šæ¨¡æ€çš„LoRAé€‚é…å™¨å½¢æˆè§£è€¦è®¡ç®—åˆ†æ”¯ï¼Œé€æ­¥å­¦ä¹ åƒç´ çº§ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17405', 'title': 'Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT', 'url': 'https://huggingface.co/papers/2511.17405', 'abstract': 'ReVeL, a framework that converts multiple-choice questions to open-form questions, improves data efficiency and robustness in fine-tuning multimodal language models and reveals score inflation in MCQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'f6e4151deb74bb5e', 'authors': ['Yesheng Liu', 'Hao Li', 'Haiyu Xu', 'Baoqi Pei', 'Jiahao Wang', 'Mingxuan Zhao', 'Jingshu Zheng', 'Zheqi He', 'JG Yao', 'Bowen Qin', 'Xi Yang', 'Jiajun Zhang'], 'affiliations': ['BAAI FlagEval Team', 'BUAA', 'Institute of Automation, CAS', 'PKU', 'School of Artificial Intelligence, UCAS', 'ZJU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17405.jpg', 'data': {'categories': ['#optimization', '#dataset', '#synthetic', '#benchmark', '#rlhf', '#open_source', '#multimodal', '#training'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ: ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ReVeL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² MCQA ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶Ğ°ÑÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‚ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ GRPO Ğ´Ğ»Ñ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen2.5-VL. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ°Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° 20 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MCQA.'}, 'en': {'title': 'Transforming MCQA for Better Model Training and Evaluation', 'desc': 'ReVeL is a novel framework designed to transform multiple-choice questions into open-form questions, enhancing the efficiency and robustness of fine-tuning multimodal language models. By addressing the limitations of multiple-choice question answering (MCQA), such as score inflation and answer guessing, ReVeL provides a more reliable evaluation method. The framework categorizes questions based on answer types and applies tailored rewriting and verification techniques. Results show that models fine-tuned with ReVeL not only match MCQA accuracy but also significantly improve OpenQA performance, revealing hidden biases in traditional benchmarks.'}, 'zh': {'title': 'ReVeLï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„é—®ç­”èƒ½åŠ›', 'desc': 'ReVeLæ˜¯ä¸€ä¸ªå°†å¤šé¡¹é€‰æ‹©é¢˜è½¬æ¢ä¸ºå¼€æ”¾å¼é—®é¢˜çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæ•°æ®æ•ˆç‡å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é—®é¢˜åˆ†ç±»å¹¶åº”ç”¨ä¸åŒçš„é‡å†™å’ŒéªŒè¯æ–¹æ¡ˆï¼Œç¡®ä¿ç­”æ¡ˆåœ¨å¯èƒ½çš„æƒ…å†µä¸‹å¯éªŒè¯ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šé¡¹é€‰æ‹©é¢˜çš„é€‰é¡¹å¯èƒ½æ³„éœ²å¯åˆ©ç”¨çš„ä¿¡å·ï¼Œå¯¼è‡´å‡†ç¡®æ€§æŒ‡æ ‡ä¸å¯é ï¼Œå¹¶é¼“åŠ±åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡ŒçŒœæµ‹è¡Œä¸ºã€‚ä½¿ç”¨ReVeLè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹åœ¨å¤šé¡¹é€‰æ‹©åŸºå‡†æµ‹è¯•ä¸­ä¸MCQAçš„å‡†ç¡®æ€§ç›¸åŒ¹é…ï¼Œå¹¶åœ¨å¼€æ”¾å¼é—®ç­”ä¸­æé«˜äº†çº¦å…­ä¸ªç™¾åˆ†ç‚¹çš„å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®æ•ˆç‡å’Œæ›´å¼ºçš„å¥–åŠ±ä¿¡å·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18945', 'title': 'MIST: Mutual Information Via Supervised Training', 'url': 'https://huggingface.co/papers/2511.18945', 'abstract': "A data-driven neural network approach estimates mutual information using a meta-dataset of synthetic distributions, offering flexibility, efficiency, and uncertainty quantification.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.", 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'f83a8119f734d9ea', 'authors': ['German Gritsai', 'Megan Richards', 'Maxime MÃ©loux', 'Kyunghyun Cho', 'Maxime Peyrard'], 'affiliations': ['New York University', 'Prescient Design, Genentech', 'Universite Grenoble Alpes, CNRS, Grenoble INP, LIG'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18945.jpg', 'data': {'categories': ['#optimization', '#synthetic'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ‚Ğ°Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (MI), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°-Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¸Ğ· 625,000 ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ MI. ĞœĞ¾Ğ´ĞµĞ»ÑŒ MIST Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°Ğ¼. Ğ”Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ñ‹ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ MI, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Mutual Information Estimation with Neural Networks', 'desc': 'This paper presents a novel approach to estimating mutual information (MI) using a neural network called MIST, which is trained on a large dataset of synthetic distributions. The method allows for flexibility and efficiency by employing a two-dimensional attention mechanism to handle varying sample sizes and dimensions. Additionally, it incorporates quantile regression to provide uncertainty quantification, resulting in well-calibrated confidence intervals. The proposed estimators outperform traditional methods and can be integrated into larger machine learning frameworks, making them versatile for different data types.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„äº’ä¿¡æ¯ä¼°è®¡æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å®Œå…¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥è®¾è®¡äº’ä¿¡æ¯ï¼ˆMIï¼‰ä¼°è®¡å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆMISTï¼‰å¯¹MIä¼°è®¡å‡½æ•°è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶é€šè¿‡625,000ä¸ªå·²çŸ¥çœŸå®MIçš„åˆæˆè”åˆåˆ†å¸ƒçš„å¤§å‹å…ƒæ•°æ®é›†è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚ä¸ºäº†å¤„ç†å¯å˜çš„æ ·æœ¬å¤§å°å’Œç»´åº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†äºŒç»´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿è¾“å…¥æ ·æœ¬çš„ç½®æ¢ä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–åˆ†ä½æ•°å›å½’æŸå¤±æ¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œä½¿å¾—ä¼°è®¡å™¨èƒ½å¤Ÿè¿‘ä¼¼MIçš„é‡‡æ ·åˆ†å¸ƒï¼Œè€Œä¸ä»…ä»…è¿”å›å•ä¸€çš„ç‚¹ä¼°è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16249', 'title': 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation', 'url': 'https://huggingface.co/papers/2511.16249', 'abstract': 'Controllable Layer Decomposition (CLD) enables fine-grained and controllable separation of raster images into RGBA layers, surpassing existing methods in quality and practical use.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents Controllable Layer Decomposition (CLD), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting, but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT (LD-DiT), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter (MLCA), which injects target image information into multi-layer tokens to achieve precise conditional generation. To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics. Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'be07904cd9d90e02', 'authors': ['Zihao Liu', 'Zunnan Xu', 'Shi Shu', 'Jun Zhou', 'Ruicheng Zhang', 'Zhenchao Tang', 'Xiu Li'], 'affiliations': ['Sun Yat-sen University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16249.jpg', 'data': {'categories': ['#cv', '#dataset', '#architecture', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Controllable Layer Decomposition (CLD) Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ RGBA ÑĞ»Ğ¾Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: LayerDecompose-DiT Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¸ Multi-Layer Conditional Adapter Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ², Ñ‡Ñ‚Ğ¾ CLD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ matting Ğ¸ inpainting Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ²Ğ¾Ñ€Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ².'}, 'en': {'title': 'Revolutionizing Image Editing with Controllable Layer Decomposition', 'desc': 'Controllable Layer Decomposition (CLD) is a novel method that allows for the precise separation of raster images into RGBA layers, enhancing the quality and usability of image editing. Unlike traditional methods that struggle with controllability and segmentation accuracy, CLD introduces two innovative modules: LayerDecompose-DiT (LD-DiT) for distinct layer separation and Multi-Layer Conditional Adapter (MLCA) for targeted image generation. This approach not only improves the decomposition process but also allows designers to manipulate layers directly in popular design software. The method has been rigorously evaluated against existing techniques, demonstrating superior performance in both quality and practical application.'}, 'zh': {'title': 'å¯æ§å±‚åˆ†è§£ï¼šå›¾åƒå¤„ç†çš„æ–°çªç ´', 'desc': 'å¯æ§å±‚åˆ†è§£ï¼ˆCLDï¼‰æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥å°†å…‰æ …å›¾åƒç²¾ç»†ä¸”å¯æ§åœ°åˆ†è§£ä¸ºRGBAå±‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚è®¾è®¡å¸ˆé€šå¸¸ç‹¬ç«‹ç”Ÿæˆå’Œç¼–è¾‘æ¯ä¸ªRGBAå±‚ï¼Œä½†ä¸€æ—¦åˆæˆï¼Œå±‚çº§ç¼–è¾‘å°±å˜å¾—ä¸å¯èƒ½ã€‚CLDé€šè¿‡ä¸¤ä¸ªå…³é”®æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šLayerDecompose-DiTï¼ˆLD-DiTï¼‰ç”¨äºå°†å›¾åƒå…ƒç´ è§£è€¦ä¸ºä¸åŒå±‚ï¼Œå¹¶å®ç°ç²¾ç»†æ§åˆ¶ï¼›Multi-Layer Conditional Adapterï¼ˆMLCAï¼‰åˆ™å°†ç›®æ ‡å›¾åƒä¿¡æ¯æ³¨å…¥å¤šå±‚ä»¤ç‰Œï¼Œä»¥å®ç°ç²¾ç¡®çš„æ¡ä»¶ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLDåœ¨åˆ†è§£è´¨é‡å’Œå¯æ§æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åˆ†ç¦»çš„å±‚å¯ä»¥ç›´æ¥åœ¨å¸¸ç”¨è®¾è®¡å·¥å…·ä¸­æ“ä½œï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åˆ›ä½œå·¥ä½œæµç¨‹ä¸­çš„ä»·å€¼å’Œé€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16397', 'title': 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser', 'url': 'https://huggingface.co/papers/2511.16397', 'abstract': "A novel extraction pipeline using a language model improves web data quality, significantly enhancing the performance of large language models trained on extracted corpora.  \t\t\t\t\tAI-generated summary \t\t\t\t While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.", 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '708884c5c7b5a0be', 'authors': ['Ren Ma', 'Jiantao Qiu', 'Chao Xu', 'Pei Chu', 'Kaiwen Liu', 'Pengli Ren', 'Yuan Qu', 'Jiahui Peng', 'Linfeng Hou', 'Mengjie Liu', 'Lindong Lu', 'Wenchang Ning', 'Jia Yu', 'Rui Min', 'Jin Shi', 'Haojiong Chen', 'Peng Zhang', 'Wenjian Zhang', 'Qian Jiang', 'Zengjie Hu', 'Guoqiang Yang', 'Zhenxiang Li', 'Fukai Shang', 'Runyuan Ma', 'Chenlin Su', 'Zhongying Tu', 'Wentao Zhang', 'Dahua Lin', 'Conghui He'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16397.jpg', 'data': {'categories': ['#optimization', '#dataset', '#synthetic', '#benchmark', '#multilingual', '#open_source', '#small_models', '#data'], 'emoji': 'ğŸ§¹', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM - ĞºĞ»ÑÑ‡ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MinerU-HTML Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ (sequence labeling), Ñ€ĞµÑˆĞ°ĞµĞ¼ÑƒÑ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ 0,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹, ĞºĞ¾Ğ´ Ğ¸ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MinerU-HTML Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ AICC Ğ¸Ğ· 7,3 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° AICC, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ½Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ¿Ğ° HTML-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Enhancing Web Data Quality with Intelligent Extraction', 'desc': 'This paper presents MinerU-HTML, a new extraction pipeline that enhances the quality of web data for training large language models. Unlike traditional methods that rely on heuristics, MinerU-HTML treats content extraction as a sequence labeling task, utilizing a language model to better understand and preserve the structure of documents. The results show that this approach significantly improves the preservation of structured elements and overall data quality, leading to better performance in downstream tasks. The authors also introduce AICC, a large multilingual corpus created using this improved extraction method, which outperforms existing datasets in various benchmarks.'}, 'zh': {'title': 'æå‡ç½‘ç»œæ•°æ®è´¨é‡ï¼Œå¢å¼ºè¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®æå–ç®¡é“MinerU-HTMLï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹æ¥æé«˜ç½‘ç»œæ•°æ®çš„è´¨é‡ï¼Œä»è€Œæ˜¾è‘—æå‡åŸºäºæå–è¯­æ–™åº“è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå¯å‘å¼çš„æ–¹æ³•ä¸åŒï¼ŒMinerU-HTMLå°†å†…å®¹æå–è§†ä¸ºä¸€ä¸ªåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æ–‡æ¡£ç»“æ„å’Œè¯­ä¹‰å…ƒç´ ã€‚é€šè¿‡åœ¨æå–è¿‡ç¨‹ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µæ ¼å¼åŒ–ç®¡é“ï¼ŒMinerU-HTMLèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ†ç±»è¯­ä¹‰å…ƒç´ å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MinerU-HTMLæå–çš„æ•°æ®åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†æå–è´¨é‡å¯¹æ¨¡å‹èƒ½åŠ›çš„é‡è¦å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19314', 'title': 'PRInTS: Reward Modeling for Long-Horizon Information Seeking', 'url': 'https://huggingface.co/papers/2511.19314', 'abstract': "PRInTS, a generative process reward model, enhances information-seeking abilities in AI agents by providing dense scoring and trajectory summarization, outperforming existing models with smaller backbones.  \t\t\t\t\tAI-generated summary \t\t\t\t Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.", 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'df12a4a99a8a162d', 'authors': ['Jaewoo Lee', 'Archiki Prasad', 'Justin Chih-Yao Chen', 'Zaid Khan', 'Elias Stengel-Eskin', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill', 'University of Texas at Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19314.jpg', 'data': {'categories': ['#optimization', '#long_context', '#reasoning', '#benchmark', '#training', '#agents'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ PRInTS, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…: Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ²) Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. PRInTS ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ PRInTS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing AI Information-Seeking with PRInTS', 'desc': 'PRInTS is a novel generative process reward model designed to improve the information-seeking capabilities of AI agents. It achieves this by providing dense scoring that evaluates multiple dimensions of reasoning and summarizing long trajectories of information-gathering tasks. Unlike traditional models that struggle with complex, multi-step tasks, PRInTS effectively captures interactions with tools and interprets their outputs. Evaluations show that PRInTS significantly enhances the performance of both open-source and specialized AI models, even with smaller architectures, outperforming existing reward modeling approaches.'}, 'zh': {'title': 'PRInTSï¼šæå‡AIä»£ç†çš„ä¿¡æ¯è·å–èƒ½åŠ›', 'desc': 'PRInTSæ˜¯ä¸€ç§ç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡äººå·¥æ™ºèƒ½ä»£ç†çš„ä¿¡æ¯è·å–èƒ½åŠ›ã€‚å®ƒé€šè¿‡æä¾›å¯†é›†è¯„åˆ†å’Œè½¨è¿¹æ‘˜è¦ï¼Œå¸®åŠ©ä»£ç†åœ¨é•¿æ—¶é—´çš„ä»»åŠ¡ä¸­æ›´å¥½åœ°æ”¶é›†å’Œæ¨ç†ä¿¡æ¯ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒPRInTSèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä¿¡æ¯è·å–æ­¥éª¤ï¼Œå¹¶æœ‰æ•ˆç®¡ç†å¿«é€Ÿå¢é•¿çš„ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRInTSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„å¥–åŠ±å»ºæ¨¡åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16301', 'title': 'Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling', 'url': 'https://huggingface.co/papers/2511.16301', 'abstract': 'Upsample Anything is a lightweight test-time optimization framework that enhances low-resolution features to high-resolution outputs without training, using an anisotropic Gaussian kernel for precise reconstruction in tasks like semantic segmentation and depth estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'c94b9804541077a2', 'authors': ['Minseok Seo', 'Mark Hamilton', 'Changick Kim'], 'affiliations': ['KAIST', 'MIT', 'Microsoft'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16301.jpg', 'data': {'categories': [], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Upsample Anything â€” ÑÑ‚Ğ¾ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Vision Foundation Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ½Ğ¸Ğ·Ğ¾Ñ‚Ñ€Ğ¾Ğ¿Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ğ“Ğ°ÑƒÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ğ´ĞµĞ¸ Ğ¸Ğ· Gaussian Splatting Ğ¸ Joint Bilateral Upsampling. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğº Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'Enhancing Low-Resolution Features with Upsample Anything', 'desc': 'Upsample Anything is a novel test-time optimization framework that enhances low-resolution features to high-resolution outputs without requiring any training. It utilizes an anisotropic Gaussian kernel to achieve precise reconstruction, making it suitable for tasks like semantic segmentation and depth estimation. This approach overcomes limitations of existing methods that often need retraining or complex optimization, allowing for better scalability and generalization across different models. The framework operates quickly, processing images in approximately 0.419 seconds, while achieving state-of-the-art results in various pixel-level applications.'}, 'zh': {'title': 'è½»æ¾æå‡ä½åˆ†è¾¨ç‡ç‰¹å¾è‡³é«˜åˆ†è¾¨ç‡', 'desc': 'Upsample Anything æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°†ä½åˆ†è¾¨ç‡ç‰¹å¾æå‡ä¸ºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚è¯¥æ–¹æ³•ä½¿ç”¨å„å‘å¼‚æ€§é«˜æ–¯æ ¸è¿›è¡Œç²¾ç¡®é‡å»ºï¼Œé€‚ç”¨äºè¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ä»»åŠ¡ã€‚ç°æœ‰çš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•é€šå¸¸éœ€è¦ç‰¹å®šæ•°æ®é›†çš„é‡è®­ç»ƒæˆ–å¤æ‚çš„éšå¼ä¼˜åŒ–ï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚Upsample Anything é€šè¿‡ç®€å•çš„é€å›¾åƒä¼˜åŒ–ï¼Œå­¦ä¹ ä¸€ä¸ªç»“åˆç©ºé—´å’ŒèŒƒå›´çº¿ç´¢çš„é«˜æ–¯æ ¸ï¼Œå®ç°äº†åœ¨ä¸åŒæ¶æ„å’Œæ¨¡æ€é—´çš„æ— ç¼è½¬ç§»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18373', 'title': 'MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.18373', 'abstract': "A method that enhances vision language models with spatial-temporal signals and motion tracking improves their performance on physics-driven video reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.", 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '3acc9219091a8d50', 'authors': ['Xiyang Wu', 'Zongxia Li', 'Jihui Jin', 'Guangyao Shi', 'Gouthaman KV', 'Vishnu Raj', 'Nilotpal Sinha', 'Jingxi Chen', 'Fan Du', 'Dinesh Manocha'], 'affiliations': ['Dolby Laboratories', 'University of Maryland', 'University of Southern California'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18373.jpg', 'data': {'categories': ['#video', '#dataset', '#reasoning', '#benchmark', '#multimodal', '#interpretability', '#training', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision Language Models (VLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ MASS-Bench â€” Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 4350 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ 8361 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ 3D Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ MASS Ğ¸Ğ½Ğ¶ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· 3D ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ reinforcement fine-tuning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ½Ğ° 8.7% Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ SoTA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Gemini-2.5-Flash.'}, 'en': {'title': 'Enhancing VLMs with Motion Tracking for Better Physics Reasoning', 'desc': 'This paper presents a method to improve Vision Language Models (VLMs) by incorporating spatial-temporal signals and motion tracking, specifically for physics-driven video reasoning tasks. The authors introduce MASS-Bench, a new benchmark with thousands of videos and question-answer pairs that focus on understanding physics in videos. They propose a model-agnostic method called MASS, which enhances VLMs by integrating depth-based 3D encoding and visual grounding, along with a motion tracker to capture object dynamics. Experimental results demonstrate that the enhanced VLMs significantly outperform existing models, achieving state-of-the-art performance in physics reasoning tasks.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ç†æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ—¶ç©ºä¿¡å·å’Œè¿åŠ¨è·Ÿè¸ªï¼Œæå‡å…¶åœ¨ç‰©ç†é©±åŠ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„VLMsåœ¨å¤„ç†æ¶‰åŠè¿åŠ¨åŠ¨æ€å’Œç©ºé—´äº¤äº’çš„ç‰©ç†æ¨ç†æ—¶å­˜åœ¨å±€é™ï¼Œå½±å“äº†å…¶å¯¹çœŸå®æˆ–AIç”Ÿæˆå†…å®¹çš„ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MASS-Benchï¼Œä¸€ä¸ªåŒ…å«4350ä¸ªçœŸå®å’ŒAIç”Ÿæˆè§†é¢‘çš„åŸºå‡†æ•°æ®é›†ï¼Œä¸“æ³¨äºç‰©ç†ç›¸å…³çš„ç†è§£ä»»åŠ¡ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ³¨é‡Šã€‚é€šè¿‡å°†æ—¶ç©ºä¿¡å·æ³¨å…¥VLMè¯­è¨€ç©ºé—´ï¼Œå¹¶ç»“åˆè¿åŠ¨è·Ÿè¸ªï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†å’Œç†è§£æ–¹é¢çš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19428', 'title': 'Flow Map Distillation Without Data', 'url': 'https://huggingface.co/papers/2511.19428', 'abstract': "A data-free framework that samples from the prior distribution surpasses data-based alternatives in flow map distillation, achieving state-of-the-art fidelity with minimal sampling steps.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.", 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '49c37f902422b5a3', 'authors': ['Shangyuan Tong', 'Nanye Ma', 'Saining Xie', 'Tommi Jaakkola'], 'affiliations': ['MIT', 'NYU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19428.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ· Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ² FID 1.45 Ğ½Ğ° ImageNet 256x256 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Data-Free Flow Map Distillation: A New Era in Generative Modeling', 'desc': "This paper presents a novel data-free framework for flow map distillation that samples solely from the prior distribution, avoiding the pitfalls of relying on external datasets. Traditional methods face risks of Teacher-Data Mismatch, where the static dataset may not accurately represent the teacher's generative capabilities. The proposed approach learns to predict the teacher's sampling path while correcting its own errors, achieving high fidelity with minimal sampling steps. As a result, this method outperforms existing data-based techniques, setting a new state-of-the-art in generative modeling."}, 'zh': {'title': 'æ— æ•°æ®æµå›¾è’¸é¦ï¼šè¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„åˆ›æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ— æ•°æ®æ¡†æ¶ï¼Œé€šè¿‡ä»å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œè¶…è¶Šäº†åŸºäºæ•°æ®çš„æµå›¾è’¸é¦æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ä¿çœŸåº¦ï¼Œå¹¶ä¸”åªéœ€æœ€å°‘çš„é‡‡æ ·æ­¥éª¤ã€‚ä¼ ç»Ÿçš„æµå›¾è’¸é¦ä¾èµ–äºå¤–éƒ¨æ•°æ®é›†ï¼Œè¿™å¯èƒ½å¯¼è‡´æ•™å¸ˆä¸æ•°æ®ä¹‹é—´çš„ä¸åŒ¹é…é£é™©ã€‚æˆ‘ä»¬æå‡ºçš„æ— æ•°æ®æ–¹æ³•é¿å…äº†è¿™ç§é£é™©ï¼Œç¡®ä¿äº†æ•™å¸ˆçš„é‡‡æ ·è·¯å¾„ä¸å…ˆéªŒåˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ImageNetä¸Šå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œå±•ç¤ºäº†æ— æ•°æ®æµå›¾è’¸é¦çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16166', 'title': 'EvoVLA: Self-Evolving Vision-Language-Action Model', 'url': 'https://huggingface.co/papers/2511.16166', 'abstract': 'EvoVLA, a self-supervised VLA framework, enhances long-horizon robotic manipulation by addressing stage hallucination through triplet contrastive learning, pose-based exploration, and long-horizon memory, achieving improved success rates and sample efficiency on both simulated and real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '37b6a8ac6938b252', 'authors': ['Zeting Liu', 'Zida Yang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16166.jpg', 'data': {'categories': ['#dataset', '#rl', '#benchmark', '#hallucinations', '#open_source', '#multimodal', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ· ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ²', 'desc': 'EvoVLA â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Vision-Language-Action (VLA) Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², ĞºĞ¾Ğ³Ğ´Ğ° Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ñ‹Ñ‚Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑĞ±Ğ¾Ğ¿Ñ‹Ñ‚ÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 10.2 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ°, ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ 38.5% Ğ´Ğ¾ 14.8% Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 54.6% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'EvoVLA: Enhancing Robotic Manipulation with Self-Supervised Learning', 'desc': 'EvoVLA is a self-supervised framework designed to improve long-horizon robotic manipulation by addressing the problem of stage hallucination in Vision-Language-Action (VLA) models. It employs triplet contrastive learning to ensure agents do not take shortcuts in multi-step tasks, thus enhancing the accuracy of task completion. Additionally, it incorporates pose-based exploration to focus on the relationship between objects and the gripper, rather than just visual input. The framework also utilizes long-horizon memory to maintain relevant context during extended tasks, resulting in significant improvements in success rates and sample efficiency in both simulated and real-world environments.'}, 'zh': {'title': 'EvoVLAï¼šæå‡æœºå™¨äººæ“ä½œçš„è‡ªç›‘ç£æ¡†æ¶', 'desc': 'EvoVLAæ˜¯ä¸€ä¸ªè‡ªç›‘ç£çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œ(VLA)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡é•¿æ—¶é—´èŒƒå›´çš„æœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚å®ƒé€šè¿‡ä¸‰ç§äº’è¡¥çš„ç»„ä»¶æ¥è§£å†³é˜¶æ®µå¹»è§‰é—®é¢˜ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸‰å…ƒå¯¹æ¯”å­¦ä¹ çš„é˜¶æ®µå¯¹é½å¥–åŠ±(SAR)ã€åŸºäºå§¿æ€çš„ç‰©ä½“æ¢ç´¢(POE)ä»¥åŠé•¿æ—¶é—´è®°å¿†ã€‚EvoVLAåœ¨æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œä»»åŠ¡ä¸­éƒ½æ˜¾ç¤ºå‡ºæ›´é«˜çš„æˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ï¼ŒæˆåŠŸç‡æé«˜äº†10.2ä¸ªç™¾åˆ†ç‚¹ï¼Œæ ·æœ¬æ•ˆç‡æé«˜äº†ä¸€å€åŠã€‚è¯¥æ¡†æ¶åœ¨ç‰©ç†æœºå™¨äººä¸Šçš„å®é™…éƒ¨ç½²ä¹Ÿå–å¾—äº†54.6%çš„å¹³å‡æˆåŠŸç‡ï¼Œå±•ç¤ºäº†æœ‰æ•ˆçš„æ¨¡æ‹Ÿåˆ°ç°å®è½¬ç§»å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17792', 'title': 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?', 'url': 'https://huggingface.co/papers/2511.17792', 'abstract': 'Target-Bench evaluates state-of-the-shelf world models on mapless path planning tasks, showing significant improvement through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'a9c7fe06feb1ec0d', 'authors': ['Dingrui Wang', 'Hongyuan Ye', 'Zhihao Liang', 'Zhexiao Sun', 'Zhaowei Lu', 'Yuchen Zhang', 'Yuyu Zhao', 'Yuan Gao', 'Marvin Seegert', 'Finn SchÃ¤fer', 'Haotong Qin', 'Wei Li', 'Luigi Palmieri', 'Felix Jahncke', 'Mattia Piccinini', 'Johannes Betz'], 'affiliations': ['Bosch AI Center', 'ETH', 'NJU', 'TUM'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17792.jpg', 'data': {'categories': ['#optimization', '#video', '#dataset', '#benchmark', '#open_source', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ¸Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸: Ğ¾Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Target-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼ Ğ±ĞµĞ· ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 450 Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 45 ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ†ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Sora 2, Veo 3.1, ÑĞµÑ€Ğ¸Ñ Wan) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ 5-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 325 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 15% Ğ²Ñ‹ÑˆĞµ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ²Ñ‹ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸.'}, 'en': {'title': 'Target-Bench: Advancing Robot Path Planning with World Models', 'desc': 'This paper introduces Target-Bench, a benchmark designed to assess the performance of world models in mapless path planning tasks. It highlights the limitations of current state-of-the-art models in achieving effective robot navigation towards semantic targets. The authors demonstrate that fine-tuning a large open-source model significantly enhances its planning capabilities, achieving a notable improvement over existing models. The study provides a comprehensive evaluation framework that includes metrics for target-reaching, trajectory accuracy, and directional consistency, paving the way for future advancements in robotic path planning.'}, 'zh': {'title': 'Target-Benchï¼šè¯„ä¼°ä¸–ç•Œæ¨¡å‹çš„æ— åœ°å›¾è·¯å¾„è§„åˆ’èƒ½åŠ›', 'desc': 'Target-Benchæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ä¸–ç•Œæ¨¡å‹åœ¨æ— åœ°å›¾è·¯å¾„è§„åˆ’ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æä¾›äº†450ä¸ªæœºå™¨äººæ”¶é›†çš„è§†é¢‘åºåˆ—ï¼Œæ¶µç›–45ä¸ªè¯­ä¹‰ç±»åˆ«ï¼Œå¹¶åŸºäºSLAMæŠ€æœ¯æä¾›çœŸå®è½¨è¿¹ã€‚é€šè¿‡å¯¹ç”Ÿæˆè§†é¢‘çš„ç›¸æœºè¿åŠ¨è¿›è¡Œæ¢å¤ï¼Œè¯„ä¼°ç®¡é“ä½¿ç”¨äº”ä¸ªäº’è¡¥æŒ‡æ ‡æ¥é‡åŒ–ç›®æ ‡åˆ°è¾¾èƒ½åŠ›ã€è½¨è¿¹å‡†ç¡®æ€§å’Œæ–¹å‘ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜è·¯å¾„è§„åˆ’æ€§èƒ½ï¼Œè¶…è¶Šå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19319', 'title': 'SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis', 'url': 'https://huggingface.co/papers/2511.19319', 'abstract': 'SyncMV4D generates realistic and consistent multi-view 3D Hand-Object Interaction videos and 4D motions by integrating visual priors, motion dynamics, and multi-view geometry.  \t\t\t\t\tAI-generated summary \t\t\t\t Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'e186177136bab7a0', 'authors': ['Lingwei Dang', 'Zonghan Li', 'Juntong Li', 'Hongwen Zhang', 'Liang An', 'Yebin Liu', 'Qingyao Wu'], 'affiliations': ['Beijing Normal University', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19319.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal', '#robotics', '#3d'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸', 'desc': 'SyncMV4D â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ñ… Ğ¸ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Multi-view Joint Diffusion Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Diffusion Points Aligner Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 4D Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ³Ğ´Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ 4D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğµ, Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Hand-Object Interaction with SyncMV4D', 'desc': 'SyncMV4D is a novel framework that generates realistic multi-view videos of hand-object interactions and their corresponding 4D motions. It addresses the limitations of existing methods by integrating visual priors, motion dynamics, and multi-view geometry, allowing for a more comprehensive understanding of 3D interactions. The model employs a Multi-view Joint Diffusion (MJD) approach to co-generate videos and motions, while a Diffusion Points Aligner (DPA) refines these motions into aligned 4D point tracks. This innovative closed-loop system enhances both the visual quality and motion accuracy, outperforming current state-of-the-art techniques in realism and consistency.'}, 'zh': {'title': 'ç”ŸæˆçœŸå®ä¸€è‡´çš„å¤šè§†è§’3Dæ‰‹ç‰©ä½“äº¤äº’è§†é¢‘', 'desc': 'SyncMV4Dæ˜¯ä¸€ç§ç”ŸæˆçœŸå®ä¸”ä¸€è‡´çš„å¤šè§†è§’3Dæ‰‹ç‰©ä½“äº¤äº’è§†é¢‘å’Œ4Dè¿åŠ¨çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡æ•´åˆè§†è§‰å…ˆéªŒã€è¿åŠ¨åŠ¨æ€å’Œå¤šè§†è§’å‡ ä½•æ¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬å¤šè§†è§’è”åˆæ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ç‚¹å¯¹é½å™¨ï¼Œèƒ½å¤Ÿå…±åŒç”ŸæˆHOIè§†é¢‘å’Œä¸­é—´è¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncMV4Dåœ¨è§†è§‰çœŸå®æ„Ÿã€è¿åŠ¨åˆç†æ€§å’Œå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19166', 'title': 'Representational Stability of Truth in Large Language Models', 'url': 'https://huggingface.co/papers/2511.19166', 'abstract': 'LLMs exhibit varying levels of stability in encoding truth representations, influenced more by epistemic familiarity than linguistic form, as assessed through perturbation analysis of their activations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are widely used for factual tasks such as "What treats asthma?" or "What is the capital of Latvia?". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM\'s veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM\'s activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'adc4803db77e6c81', 'authors': ['Samantha Dies', 'Courtney Maynard', 'Germans Savcisens', 'Tina Eliassi-Rad'], 'affiliations': ['Khoury College of Computer Sciences, Northeastern University', 'Network Science Institute, Northeastern University', 'Santa Fe Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19166.jpg', 'data': {'categories': ['#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Â«Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸Â» Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ĞµÑ‘ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ñ‹. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹ Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ½ĞµĞ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… (Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ñ…ÑÑ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸), Ñ‡ĞµĞ¼ Ğ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ñ‹Ñ… Ğ²Ñ‹Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ¾Ñ‚Ğ°Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ ÑĞºĞ¾Ñ€ĞµĞµ Ğ¾Ñ‚ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ²Ğ° Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ñ‡ĞµĞ¼ Ğ¾Ñ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Stability of Truth in Language Models: Familiarity Matters!', 'desc': "This paper investigates how large language models (LLMs) represent truth and falsehood in their internal structures. It introduces the concept of representational stability, which measures how consistent an LLM's truth representations are when faced with changes in the definitions of truth. The study finds that LLMs are more stable in their truth representations when they are familiar with the content, while unfamiliar statements lead to significant shifts in their truth judgments. This research highlights the importance of epistemic familiarity over mere linguistic form in understanding how LLMs encode factual information."}, 'zh': {'title': 'çŸ¥è¯†ç†Ÿæ‚‰åº¦å½±å“çœŸç›¸è¡¨ç¤ºçš„ç¨³å®šæ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç çœŸç›¸è¡¨ç¤ºæ—¶è¡¨ç°å‡ºä¸åŒçš„ç¨³å®šæ€§ï¼Œè¿™ç§ç¨³å®šæ€§æ›´å¤šåœ°å—åˆ°çŸ¥è¯†ç†Ÿæ‚‰åº¦çš„å½±å“ï¼Œè€Œä¸æ˜¯è¯­è¨€å½¢å¼ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡¨ç¤ºç¨³å®šæ€§è¿™ä¸€æ¦‚å¿µï¼Œæ¥è¡¡é‡LLMåœ¨çœŸç›¸å®šä¹‰å˜åŒ–ä¸‹çš„çœŸå®æ€§è¡¨ç¤ºçš„ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹16ä¸ªå¼€æºæ¨¡å‹çš„æ¿€æ´»è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¸ç†Ÿæ‚‰çš„é™ˆè¿°ä¼šå¯¼è‡´æ›´å¤§çš„å†³ç­–è¾¹ç•Œå˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨è„†å¼±é¢†åŸŸä¸­ï¼ŒçœŸä¼ªåˆ¤æ–­çš„ç¿»è½¬ç‡å¯è¾¾40%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¡¨ç¤ºç¨³å®šæ€§ä¸»è¦æºäºçŸ¥è¯†çš„ç†Ÿæ‚‰ç¨‹åº¦ï¼Œè€Œéè¯­è¨€çš„å½¢å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18047', 'title': 'Fidelity-Aware Recommendation Explanations via Stochastic Path Integration', 'url': 'https://huggingface.co/papers/2511.18047', 'abstract': "SPINRec, a model-agnostic approach, enhances explanation fidelity in recommender systems by using stochastic baseline sampling and path-integration techniques to capture both observed and unobserved interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '9afe2b0fd0c831cc', 'authors': ['Oren Barkan', 'Yahlly Schein', 'Yehonatan Elisha', 'Veronika Bogina', 'Mikhail Baklanov', 'Noam Koenigstein'], 'affiliations': ['Tel Aviv University, Israel', 'The Open University, Israel'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18047.jpg', 'data': {'categories': ['#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ§ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¹', 'desc': 'SPINRec â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ»Ğ¸Ğ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ·Ğ¸ÑĞ°, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€Ñ‘Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº.'}, 'en': {'title': 'Enhancing Explainability in Recommendations with SPINRec', 'desc': 'SPINRec is a new method designed to improve how well explanations in recommender systems reflect the actual reasoning of the models. It uses stochastic baseline sampling and path-integration techniques to better understand both the interactions that are seen and those that are not. By sampling various user profiles from real data, SPINRec finds the most accurate way to explain recommendations. This approach leads to more reliable and personalized explanations, setting a new standard for explainability in recommendation systems.'}, 'zh': {'title': 'æå‡æ¨èç³»ç»Ÿè§£é‡Šå¯ä¿¡åº¦çš„SPINRec', 'desc': 'SPINRecæ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨èç³»ç»Ÿä¸­è§£é‡Šçš„å¯ä¿¡åº¦ã€‚å®ƒé€šè¿‡éšæœºåŸºçº¿é‡‡æ ·å’Œè·¯å¾„ç§¯åˆ†æŠ€æœ¯ï¼Œæ•æ‰è§‚å¯Ÿåˆ°å’Œæœªè§‚å¯Ÿåˆ°çš„äº¤äº’ã€‚SPINRecå…‹æœäº†ä»¥å¾€æ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿæä¾›æ›´ç¨³å®šå’Œä¸ªæ€§åŒ–çš„è§£é‡Šã€‚é€šè¿‡å¯¹å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†çš„å…¨é¢è¯„ä¼°ï¼ŒSPINRecåœ¨è§£é‡Šçš„å¯ä¿¡åº¦ä¸Šè®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18024', 'title': 'Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems', 'url': 'https://huggingface.co/papers/2511.18024', 'abstract': "A Sparse Autoencoder method extracts interpretable latent dimensions from user and item embeddings in recommender systems, aligning with model predictions and supporting controllable personalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.", 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '264e06cc15e5277c', 'authors': ['Dor Arviv', 'Yehonatan Elisha', 'Oren Barkan', 'Noam Koenigstein'], 'affiliations': ['Tel Aviv University', 'The Open University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18024.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Sparse Autoencoder. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ objective Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ². ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¶Ğ°Ğ½Ñ€, Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Interpretability in Recommendations with Sparse Autoencoders', 'desc': "This paper introduces a Sparse Autoencoder (SAE) method that extracts interpretable latent dimensions from user and item embeddings in recommender systems. The method focuses on creating monosemantic neurons, which represent clear and coherent concepts while maintaining the relationships between user and item embeddings. By using a prediction-aware training objective, the approach aligns the learned latent structure with the recommender's predictions, allowing for better personalization. This technique enables targeted filtering and content promotion, making it a valuable tool for enhancing interpretability and control in recommendation systems."}, 'zh': {'title': 'å¯è§£é‡Šçš„ä¸ªæ€§åŒ–æ¨èæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoder, SAEï¼‰æ–¹æ³•ï¼Œç”¨äºä»æ¨èç³»ç»Ÿä¸­çš„ç”¨æˆ·å’Œç‰©å“åµŒå…¥ä¸­æå–å¯è§£é‡Šçš„æ½œåœ¨ç»´åº¦ã€‚è¯¥æ–¹æ³•é€šè¿‡æ­ç¤ºé¢„è®­ç»ƒè¡¨ç¤ºä¸­çš„è¯­ä¹‰ç»“æ„ï¼Œå¸®åŠ©å®ç°å¯æ§çš„ä¸ªæ€§åŒ–æ¨èã€‚ä¸è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ä¸åŒï¼Œæ¨èç³»ç»Ÿä¸­çš„å•ä¹‰æ€§éœ€è¦ä¿æŒç”¨æˆ·å’Œç‰©å“åµŒå…¥ä¹‹é—´çš„äº¤äº’ã€‚æœ€ç»ˆï¼Œæå–çš„ç¥ç»å…ƒèƒ½å¤Ÿæ•æ‰æµæ´¾ã€å—æ¬¢è¿ç¨‹åº¦å’Œæ—¶é—´è¶‹åŠ¿ç­‰ç‰¹æ€§ï¼Œæ”¯æŒåç»­çš„è¿‡æ»¤å’Œå†…å®¹æ¨å¹¿æ“ä½œï¼Œè€Œæ— éœ€ä¿®æ”¹åŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.12810', 'title': 'MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection', 'url': 'https://huggingface.co/papers/2511.12810', 'abstract': 'A Multi-Scale Recursive Network using a Pyramid Vision Transformer and specialized units improves camouflaged object detection by enhancing feature extraction and recursive feature refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': '7968ac29ec6a5fb1', 'authors': ['Leena Alghamdi', 'Muhammad Usman', 'Hafeez Anwar', 'Abdul Bais', 'Saeed Anwar'], 'affiliations': ['Department of Computer Science and Software Engineering, University of Western Australia, Perth 6009, Australia', 'Department of Computer Science, National University of Computer and Emerging Sciences, Peshawar 24720, Pakistan', 'Electronic Systems Engineering, University of Regina, Regina S4S 0A2, Canada', 'Faculty of Science, Ontario Tech University, Oshawa L1G 0C5, Canada', 'Information and Computer Science, King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.12810.jpg', 'data': {'categories': ['#cv', '#open_source', '#architecture', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ Ğ² Ñ†Ğ²ĞµÑ‚Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Multi-Scale Recursive Network, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Pyramid Vision Transformer Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Attention-Based Scale Integration Units Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Multi-Granularity Fusion Units Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ°Ğ¼ÑƒÑ„Ğ»ÑĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Camouflaged Object Detection with Multi-Scale Recursive Networks', 'desc': 'This paper presents a Multi-Scale Recursive Network designed to improve the detection of camouflaged objects in complex environments. It utilizes a Pyramid Vision Transformer to extract features at multiple scales and employs specialized Attention-Based Scale Integration Units for effective feature merging. The model incorporates a recursive feedback mechanism that enhances global context understanding, allowing for better detection of small and multiple camouflaged objects. The proposed method outperforms existing techniques on benchmark datasets, demonstrating significant advancements in camouflaged object detection.'}, 'zh': {'title': 'å¤šå°ºåº¦é€’å½’ç½‘ç»œï¼šæå‡ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦é€’å½’ç½‘ç»œï¼Œåˆ©ç”¨é‡‘å­—å¡”è§†è§‰å˜æ¢å™¨å’Œä¸“é—¨çš„å•å…ƒæ¥æé«˜ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¤šå°ºåº¦ç‰¹å¾å¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èåˆï¼Œä»è€Œå¢å¼ºäº†ç‰¹å¾æå–å’Œé€’å½’ç‰¹å¾ä¼˜åŒ–ã€‚ä¸ºäº†æ›´ç²¾ç¡®åœ°æ£€æµ‹å°å‹å’Œå¤šä¸ªä¼ªè£…ç‰©ä½“ï¼Œè§£ç å™¨é€šè¿‡å¤šç²’åº¦èåˆå•å…ƒé€’å½’åœ°ç»†åŒ–ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¼ªè£…ç‰©ä½“æ£€æµ‹çš„åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20626', 'title': 'ROOT: Robust Orthogonalized Optimizer for Neural Network Training', 'url': 'https://huggingface.co/papers/2511.20626', 'abstract': 'ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.', 'score': 169, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'b0c37b417728c4ce', 'authors': ['Wei He', 'Kai Han', 'Hang Zhou', 'Hanting Chen', 'Zhicheng Liu', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20626.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾-ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²ÑƒÑ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ROOT â€” Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞÑŒÑÑ‚Ğ¾Ğ½Ğ° Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ¿Ñ€Ğ¾ĞºÑĞ¸-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑˆÑƒĞ¼ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ROOT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Muon Ğ¸ Adam, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ½ĞµĞ²Ñ‹Ğ¿ÑƒĞºĞ»Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'ROOT: A Robust Optimizer for Stable Large Language Model Training', 'desc': 'The paper introduces ROOT, a new optimizer designed to improve the training of large language models (LLMs) by enhancing stability and convergence. It addresses two main issues: dimensional fragility, which affects precision during orthogonalization, and sensitivity to outlier noise. ROOT employs adaptive Newton iterations for robust orthogonalization and uses proximal optimization to mitigate the impact of outliers while maintaining effective gradient directions. Experimental results show that ROOT outperforms existing optimizers like Muon and Adam, especially in challenging training conditions.'}, 'zh': {'title': 'ROOTï¼šæå‡å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒç¨³å®šæ€§çš„é²æ£’ä¼˜åŒ–å™¨', 'desc': 'ROOTæ˜¯ä¸€ç§å¼ºå¤§çš„ä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å®ƒé€šè¿‡è‡ªé€‚åº”ç‰›é¡¿è¿­ä»£å’Œè¿‘ç«¯ä¼˜åŒ–æ¥è§£å†³ç»´åº¦è„†å¼±æ€§å’Œå¼‚å¸¸å€¼å™ªå£°çš„é—®é¢˜ã€‚ROOTé‡‡ç”¨äº†åŒé‡é²æ£’æ€§æœºåˆ¶ï¼Œç¡®ä¿åœ¨ä¸åŒæ¶æ„é…ç½®ä¸­ä¿æŒä¸€è‡´çš„ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROOTåœ¨å™ªå£°å’Œéå‡¸åœºæ™¯ä¸‹çš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¼˜åŒ–å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17592', 'title': 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms', 'url': 'https://huggingface.co/papers/2511.17592', 'abstract': 'GigaEvo is an open-source framework for LLM-guided evolutionary computation, offering modular and concurrent tools for research and experimentation in solving complex optimization problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.', 'score': 118, 'issue_id': 1, 'pub_date': '2025-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'f489c8de90b3d7ff', 'authors': ['Valentin Khrulkov', 'Andrey Galichin', 'Denis Bashkirov', 'Dmitry Vinichenko', 'Oleg Travkin', 'Roman Alferov', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['Artificial Intelligence Research Institute (AIRI), Presnenskaya Embankment 6, bld. 2, Moscow, 123112, Russia', 'Sber, 19 Vavilova St., Moscow 117312, Russia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17592.jpg', 'data': {'categories': ['#optimization', '#open_source'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'GigaEvo â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ MAP-Elites, Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾ÑÑ‚Ñ€Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² LLM-ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸. Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğ¥ĞµĞ¹Ğ»ÑŒĞ±Ñ€Ğ¾Ğ½Ğ½Ğ°, ÑƒĞ¿Ğ°ĞºĞ¾Ğ²ĞºĞ° ĞºÑ€ÑƒĞ³Ğ¾Ğ² Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ğ¾Ñ†ĞµĞ»ÑƒĞµĞ² Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ….'}, 'en': {'title': 'GigaEvo: Empowering Evolutionary Computation with LLMs', 'desc': 'GigaEvo is an open-source framework designed for evolutionary computation guided by large language models (LLMs). It provides modular and concurrent tools that facilitate research and experimentation in solving complex optimization problems. The framework includes key components such as MAP-Elites algorithms, asynchronous evaluation pipelines, and LLM-driven mutation operators, all aimed at enhancing the efficiency of hybrid LLM-evolution approaches. By offering detailed implementation guidelines and validating its effectiveness on challenging problems, GigaEvo aims to improve reproducibility and foster further advancements in the field.'}, 'zh': {'title': 'GigaEvoï¼šè¿›åŒ–è®¡ç®—çš„æ–°çºªå…ƒ', 'desc': 'GigaEvoæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œä¸“æ³¨äºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›åŒ–è®¡ç®—ï¼Œæ—¨åœ¨è§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶æä¾›äº†æ¨¡å—åŒ–å’Œå¹¶å‘çš„å·¥å…·ï¼Œæ”¯æŒç ”ç©¶äººå‘˜è¿›è¡Œå®éªŒå’Œæ¢ç´¢æ··åˆçš„LLM-è¿›åŒ–æ–¹æ³•ã€‚GigaEvoå®ç°äº†å…³é”®ç»„ä»¶çš„æ¨¡å—åŒ–ï¼ŒåŒ…æ‹¬MAP-Elitesè´¨é‡å¤šæ ·æ€§ç®—æ³•å’Œå¼‚æ­¥DAGè¯„ä¼°ç®¡é“ï¼Œä¿ƒè¿›äº†å¿«é€ŸåŸå‹å¼€å‘ã€‚é€šè¿‡å¯¹AlphaEvolveè®ºæ–‡ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜è¿›è¡Œè¯„ä¼°ï¼ŒGigaEvoå±•ç¤ºäº†å…¶åœ¨å¯é‡å¤æ€§å’ŒéªŒè¯å®æ–½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19046', 'title': 'MedSAM3: Delving into Segment Anything with Medical Concepts', 'url': 'https://huggingface.co/papers/2511.19046', 'abstract': 'MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.', 'score': 48, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '024638673f58aeed', 'authors': ['Anglin Liu', 'Rundong Xue', 'Xu R. Cao', 'Yifan Shen', 'Yi Lu', 'Xiang Li', 'Qianqian Chen', 'Jintai Chen'], 'affiliations': ['Southeast University', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Illinois Urbana-Champaign', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19046.jpg', 'data': {'categories': ['#video', '#architecture', '#science', '#healthcare', '#open_source', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'MedSAM-3 â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ fine-tuning Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ SAM 3 Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ multimodal Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½, ĞœĞ Ğ¢, Ğ£Ğ—Ğ˜, ĞšĞ¢ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾) Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Medical Image Segmentation with Text Prompts', 'desc': 'MedSAM-3 is a medical segmentation model that enhances the Segment Anything Model (SAM) 3 architecture by allowing users to provide text prompts for segmentation tasks. This model is designed to improve the generalizability of medical image segmentation, reducing the need for extensive manual annotations. By utilizing semantic conceptual labels and integrating Multimodal Large Language Models (MLLMs), MedSAM-3 can accurately identify anatomical structures based on descriptive text rather than just geometric shapes. Extensive testing across various imaging modalities shows that MedSAM-3 outperforms existing models, making it a significant advancement in the field of medical imaging.'}, 'zh': {'title': 'MedSAM-3ï¼šåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°çªç ´', 'desc': 'MedSAM-3æ˜¯ä¸€ç§åŸºäºSAM 3æ¶æ„çš„åŒ»å­¦åˆ†å‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡ŒåŒ»å­¦å›¾åƒå’Œè§†é¢‘çš„åˆ†å‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹åŒ»å­¦å›¾åƒè¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆè¯­ä¹‰æ¦‚å¿µæ ‡ç­¾ï¼Œå®ç°äº†åŒ»å­¦å¯æç¤ºæ¦‚å¿µåˆ†å‰²ï¼ˆPCSï¼‰ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡å¼€æ”¾è¯æ±‡çš„æ–‡æœ¬æè¿°ç²¾ç¡®å®šä½è§£å‰–ç»“æ„ã€‚MedSAM-3è¿˜å¼•å…¥äº†MedSAM-3ä»£ç†æ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåœ¨å¾ªç¯å·¥ä½œæµä¸­è¿›è¡Œå¤æ‚æ¨ç†å’Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedSAM-3åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡å¼ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸“ä¸šæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19900', 'title': 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning', 'url': 'https://huggingface.co/papers/2511.19900', 'abstract': 'Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.', 'score': 46, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '68d69fc73915970a', 'authors': ['Jiaqi Liu', 'Kaiwen Xiong', 'Peng Xia', 'Yiyang Zhou', 'Haonian Ji', 'Lu Feng', 'Siwei Han', 'Mingyu Ding', 'Huaxiu Yao'], 'affiliations': ['UNC-Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19900.jpg', 'data': {'categories': ['#alignment', '#rl', '#reasoning', '#rlhf', '#hallucinations', '#open_source', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒ', 'desc': 'Agent0-VL â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞµĞ±Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰ĞµĞ³Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ§ĞµÑ€ĞµĞ· Ñ†Ğ¸ĞºĞ» ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ 12.5% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Self-Improvement in Vision-Language Agents with Tools', 'desc': 'Agent0-VL is a vision-language agent that enhances its reasoning and self-evaluation capabilities by integrating tool usage. This model employs a Self-Evolving Reasoning Cycle, allowing it to introspect and refine its reasoning through evidence-based analysis and reinforcement learning. By acting as both a Solver and a Verifier, it generates structured feedback and self-rewards, facilitating continual improvement without relying on human annotations. Experiments demonstrate that Agent0-VL significantly outperforms its base model in tasks like geometric problem solving and visual scientific analysis.'}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„è§†è§‰-è¯­è¨€ä»£ç†ï¼šæŒç»­æ”¹è¿›çš„æ™ºèƒ½å·¥å…·', 'desc': 'Agent0-VLæ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„è§†è§‰-è¯­è¨€ä»£ç†ï¼Œèƒ½å¤Ÿå°†å·¥å…·ä½¿ç”¨èå…¥æ¨ç†å’Œè‡ªæˆ‘è¯„ä¼°ä¸­ï¼Œä»è€Œå®ç°æŒç»­æ”¹è¿›ã€‚è¯¥æ¨¡å‹é€šè¿‡è¯æ®åŸºç¡€åˆ†æå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå…‹æœäº†ä¼ ç»Ÿäººç±»æ ‡æ³¨ç›‘ç£çš„å±€é™æ€§ã€‚Agent0-VLç»“åˆäº†è§£å†³è€…å’ŒéªŒè¯è€…çš„è§’è‰²ï¼Œå‰è€…è¿›è¡Œå¤šè½®å·¥å…·é›†æˆæ¨ç†ï¼Œåè€…åˆ™é€šè¿‡å·¥å…·åŸºç¡€çš„æ‰¹è¯„ç”Ÿæˆç»“æ„åŒ–åé¦ˆã€‚é€šè¿‡è‡ªæˆ‘è¿›åŒ–æ¨ç†å¾ªç¯ï¼ŒAgent0-VLå®ç°äº†æ— å¤–éƒ¨å¥–åŠ±çš„è‡ªæˆ‘æ”¹è¿›ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å‡ ä½•é—®é¢˜è§£å†³å’Œè§†è§‰ç§‘å­¦åˆ†æä¸Šæ¯”åŸºç¡€æ¨¡å‹æé«˜äº†12.5%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19320', 'title': 'SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation', 'url': 'https://huggingface.co/papers/2511.19320', 'abstract': 'SteadyDancer, an Image-to-Video framework, ensures first-frame identity preservation and precise motion control through harmonized conditions, adaptive pose representation, and hierarchical training objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.', 'score': 42, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'd492cb49c1587493', 'authors': ['Jiaming Zhang', 'Shengming Cao', 'Rui Li', 'Xiaotong Zhao', 'Yutao Cui', 'Xinglin Hou', 'Gangshan Wu', 'Haolan Chen', 'Yu Xu', 'Limin Wang', 'Kai Ma'], 'affiliations': ['Platform and Content Group (PCG), Tencent', 'Shanghai AI Lab', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19320.jpg', 'data': {'categories': ['#video', '#training', '#architecture'], 'emoji': 'ğŸ’ƒ', 'ru': {'title': 'Ğ¢Ğ°Ğ½Ñ†ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ±ĞµĞ· Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'SteadyDancer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ñ„Ğ¸Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'SteadyDancer: Harmonizing Identity and Motion in Image-to-Video Animation', 'desc': 'SteadyDancer is an innovative Image-to-Video framework that addresses the challenge of maintaining the identity of the first frame while controlling motion accurately in human image animation. It introduces a Condition-Reconciliation Mechanism to align conflicting conditions, ensuring high fidelity in animation. Additionally, the framework utilizes Synergistic Pose Modulation Modules for creating adaptive pose representations that work well with reference images. The Staged Decoupled-Objective Training Pipeline optimizes the model for visual quality and temporal coherence, achieving superior performance with reduced training resources compared to existing methods.'}, 'zh': {'title': 'ç¨³æ€èˆè€…ï¼šå›¾åƒåˆ°è§†é¢‘çš„å®Œç¾è½¬æ¢', 'desc': 'SteadyDanceræ˜¯ä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³äººç±»å›¾åƒåŠ¨ç”»ä¸­çš„é¦–å¸§èº«ä»½ä¿æŒå’Œç²¾ç¡®è¿åŠ¨æ§åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¡ä»¶å’Œè°æœºåˆ¶ã€ååŒå§¿æ€è°ƒåˆ¶æ¨¡å—å’Œåˆ†é˜¶æ®µè§£è€¦ç›®æ ‡è®­ç»ƒç®¡é“æ¥å®ç°ä¸€è‡´çš„åŠ¨ç”»æ•ˆæœã€‚å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°åè°ƒä¸åŒæ¡ä»¶ï¼Œç¡®ä¿åœ¨ä¿æŒå›¾åƒçœŸå®æ„Ÿçš„åŒæ—¶å®ç°ç²¾ç¡®æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSteadyDanceråœ¨å¤–è§‚ä¿çœŸåº¦å’Œè¿åŠ¨æ§åˆ¶æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„è®­ç»ƒèµ„æºæ˜¾è‘—ä½äºå…¶ä»–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20347', 'title': 'Soft Adaptive Policy Optimization', 'url': 'https://huggingface.co/papers/2511.20347', 'abstract': 'Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.', 'score': 38, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '2d237003958c5f9f', 'authors': ['Chang Gao', 'Chujie Zheng', 'Xiong-Hui Chen', 'Kai Dang', 'Shixuan Liu', 'Bowen Yu', 'An Yang', 'Shuai Bai', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Alibaba Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20347.jpg', 'data': {'categories': ['#optimization', '#alignment', '#rl', '#architecture', '#reasoning', '#training'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'ĞœÑĞ³ĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Soft Adaptive Policy Optimization (SAPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…, SAPO Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ÑĞ³ĞºĞ¸Ğ¹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, Ğ¸Ğ·Ğ±Ğ¸Ñ€Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²ĞµÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ off-policy ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen3-VL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Stability in Reinforcement Learning with SAPO', 'desc': 'Soft Adaptive Policy Optimization (SAPO) is a new method designed to improve the stability and performance of reinforcement learning in large language models. It addresses the high variance in token-level importance ratios, especially in Mixture-of-Experts models, which can lead to unstable updates. Unlike traditional methods that use hard clipping, SAPO employs a smooth, temperature-controlled gate to selectively down-weight off-policy updates while retaining useful learning signals. This approach results in better sample efficiency and consistent performance improvements across various tasks and model sizes.'}, 'zh': {'title': 'è½¯è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'è½¯è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆSAPOï¼‰é€šè¿‡è‡ªé€‚åº”åœ°å‡å¼±ç¦»ç­–ç•¥æ›´æ–°ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰çš„åŸºäºç»„çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒSAPOä½¿ç”¨å¹³æ»‘çš„æ¸©åº¦æ§åˆ¶é—¨ï¼Œé¿å…äº†ç¡¬å‰ªåˆ‡å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚å®ƒèƒ½å¤Ÿåœ¨ä¿ç•™æœ‰ç”¨å­¦ä¹ ä¿¡å·çš„åŒæ—¶ï¼Œé€‰æ‹©æ€§åœ°é™ä½ä¸è‰¯æ ‡è®°çš„æƒé‡ï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAPOåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§å’Œæ›´é«˜çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20635', 'title': 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation', 'url': 'https://huggingface.co/papers/2511.20635', 'abstract': 'iMontage repurposes pre-trained video models to generate high-quality, diverse image sets with natural transitions and enhanced dynamics through a unified framework and tailored adaptation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.', 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '4de88f8f4874f2a7', 'authors': ['Zhoujie Fu', 'Xianfang Zeng', 'Jinghong Lan', 'Xinyao Liao', 'Cheng Chen', 'Junyi Chen', 'Jiacheng Wei', 'Wei Cheng', 'Shiyu Liu', 'Yunuo Chen', 'Gang Yu', 'Guosheng Lin'], 'affiliations': ['Nanyang Technological University', 'Shanghai Jiao Tong University', 'StepFun'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20635.jpg', 'data': {'categories': ['#video', '#training', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹', 'desc': 'iMontage â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ğ²Ğ°Ğ·Ğ¸Ğ²Ğ½Ğ° Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµÑ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Transforming Video Models into Dynamic Image Generators', 'desc': "iMontage is a novel framework that transforms pre-trained video models into versatile image generators. By leveraging the temporal coherence of video data and integrating diverse image content, it creates high-quality image sets with smooth transitions and enhanced dynamics. The framework employs a unique adaptation strategy and a careful data curation process to maintain the model's original motion capabilities while expanding its image manipulation skills. As a result, iMontage achieves impressive performance across various image generation and editing tasks, producing visually compelling scenes with rich dynamics."}, 'zh': {'title': 'iMontageï¼šå°†è§†é¢‘æ¨¡å‹è½¬åŒ–ä¸ºå¤šæ ·åŒ–å›¾åƒç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'iMontage æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹è½¬åŒ–ä¸ºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒé›†ã€‚é€šè¿‡å°†å›¾åƒæ•°æ®çš„ä¸°å¯Œå¤šæ ·æ€§æ³¨å…¥åˆ°æ—¶é—´ä¸€è‡´çš„æ¡†æ¶ä¸­ï¼ŒiMontage èƒ½å¤Ÿç”Ÿæˆå…·æœ‰è‡ªç„¶è¿‡æ¸¡å’Œæ›´å¹¿æ³›åŠ¨æ€èŒƒå›´çš„å›¾åƒé›†ã€‚è¯¥æ¡†æ¶æ”¯æŒå¯å˜é•¿åº¦çš„å›¾åƒé›†ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ï¼Œé‡‡ç”¨ä¼˜é›…çš„é€‚åº”ç­–ç•¥å’Œå®šåˆ¶çš„æ•°æ®æ•´ç†è¿‡ç¨‹ã€‚iMontage åœ¨å¤šä¸ªä¸»æµä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¿æŒäº†å¼ºå¤§çš„è·¨å›¾åƒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œå¹¶ç”Ÿæˆè¶…è¶Šä¼ ç»ŸèŒƒå›´çš„åŠ¨æ€åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20561', 'title': 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward', 'url': 'https://huggingface.co/papers/2511.20561', 'abstract': 'UniSandbox evaluates Unified Multimodal Models, revealing a gap between understanding and generation, and identifies Chain-of-Thought and self-training as means to bridge this gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox', 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '3ab059d6d671f863', 'authors': ['Yuwei Niu', 'Weiyang Jin', 'Jiaqi Liao', 'Chaoran Feng', 'Peng Jin', 'Bin Lin', 'Zongjian Li', 'Bin Zhu', 'Weihao Yu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'HKU MMLab', 'Peking University', 'PengCheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20561.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#synthetic', '#benchmark', '#multimodal', '#leakage', '#training'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UniSandbox â€” Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ñ‹Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought) Ğ² Ğ¼Ğ¾Ğ´ÑƒĞ»Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ², Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Understanding-Generation Gap in AI', 'desc': "The paper introduces UniSandbox, a framework designed to evaluate Unified Multimodal Models by examining the relationship between understanding and generation. It identifies a significant gap in performance, particularly in reasoning generation and knowledge transfer tasks. The study highlights that incorporating Chain-of-Thought (CoT) techniques can effectively bridge this gap, enhancing the model's ability to reason and transfer knowledge. Additionally, the research suggests that self-training methods can help models internalize reasoning capabilities, leading to improved generative performance."}, 'zh': {'title': 'å¼¥åˆç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·', 'desc': 'UniSandbox æ˜¯ä¸€ä¸ªè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ­ç¤ºäº†ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„å·®è·ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†ç”Ÿæˆå’ŒçŸ¥è¯†è½¬ç§»æ˜¯ä¸»è¦çš„ä¸¤ä¸ªç»´åº¦ï¼Œå…¶ä¸­é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨ç†è§£æ¨¡å—ä¸­æœ‰æ•ˆåœ°å¼¥è¡¥äº†è¿™ä¸€å·®è·ã€‚é€šè¿‡è‡ªæˆ‘è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥æˆåŠŸåœ°å†…åŒ–æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°éšå¼æ¨ç†ã€‚æ­¤å¤–ï¼ŒCoT è¿˜å¸®åŠ©ç”Ÿæˆè¿‡ç¨‹æ£€ç´¢æ–°å­¦ä¹ çš„çŸ¥è¯†ï¼Œæ˜¾ç¤ºå‡ºæŸ¥è¯¢åŸºç¡€æ¶æ„å…·æœ‰æ½œåœ¨çš„ CoT ç±»ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20462', 'title': 'STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow', 'url': 'https://huggingface.co/papers/2511.20462', 'abstract': 'STARFlow-V, a normalizing flow-based video generator, offers end-to-end learning, robust causal prediction, and high-quality video generation with practical sampling efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.', 'score': 31, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '2ee08062c0db4653', 'authors': ['Jiatao Gu', 'Ying Shen', 'Tianrong Chen', 'Laurent Dinh', 'Yuyang Wang', 'Miguel Angel Bautista', 'David Berthelot', 'Josh Susskind', 'Shuangfei Zhai'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20462.jpg', 'data': {'categories': ['#video', '#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'STARFlow-V â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ flow-score matching â€” Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·ĞµÑ€ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Video Generation with Normalizing Flows', 'desc': 'STARFlow-V is a video generation model that uses normalizing flows to create high-quality videos efficiently. It allows for end-to-end learning and robust causal predictions, addressing challenges in video generation that traditional models face. By operating in a spatiotemporal latent space, it maintains causal dependencies while enabling rich interactions within frames. The model also introduces flow-score matching and a video-aware Jacobi iteration scheme to enhance sampling efficiency and video consistency, demonstrating that normalizing flows can effectively generate videos with strong visual fidelity and temporal coherence.'}, 'zh': {'title': 'STARFlow-Vï¼šé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'STARFlow-Væ˜¯ä¸€ç§åŸºäºå½’ä¸€åŒ–æµçš„åœ¨çº¿è§†é¢‘ç”Ÿæˆå™¨ï¼Œå…·æœ‰ç«¯åˆ°ç«¯å­¦ä¹ ã€ç¨³å¥çš„å› æœé¢„æµ‹å’Œé«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨æ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­æ“ä½œï¼Œé‡‡ç”¨å…¨çƒ-å±€éƒ¨æ¶æ„ï¼Œæœ‰æ•ˆé™åˆ¶å› æœä¾èµ–å…³ç³»ï¼ŒåŒæ—¶ä¿ç•™ä¸°å¯Œçš„å±€éƒ¨å¸§å†…äº¤äº’ã€‚é€šè¿‡å¼•å…¥æµåˆ†æ•°åŒ¹é…ï¼ŒSTARFlow-Vé…å¤‡äº†è½»é‡çº§çš„å› æœå»å™ªå™¨ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒSTARFlow-Vè¿˜é‡‡ç”¨è§†é¢‘æ„ŸçŸ¥çš„é›…å¯æ¯”è¿­ä»£æ–¹æ¡ˆï¼Œæå‡äº†é‡‡æ ·æ•ˆç‡ï¼Œæ”¯æŒæ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘åŠè§†é¢‘åˆ°è§†é¢‘çš„ç”Ÿæˆä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19861', 'title': 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI', 'url': 'https://huggingface.co/papers/2511.19861', 'abstract': 'GigaWorld-0 is a unified world model framework that integrates video generation and 3D modeling to produce high-quality, diverse, and physically plausible VLA data, enabling strong real-world performance in embodied AI without real-world training.  \t\t\t\t\tAI-generated summary \t\t\t\t World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.', 'score': 30, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'df9ec5683622b7ca', 'authors': ['GigaWorld Team', 'Angen Ye', 'Boyuan Wang', 'Chaojun Ni', 'Guan Huang', 'Guosheng Zhao', 'Haoyun Li', 'Jiagang Zhu', 'Kerui Li', 'Mengyuan Xu', 'Qiuping Deng', 'Siting Wang', 'Wenkang Qin', 'Xinze Chen', 'Xiaofeng Wang', 'Yankai Wang', 'Yu Cao', 'Yifan Chang', 'Yuan Xu', 'Yun Ye', 'Yang Wang', 'Yukun Zhou', 'Zhengyuan Zhang', 'Zhehao Dong', 'Zheng Zhu'], 'affiliations': ['GigaAI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19861.jpg', 'data': {'categories': ['#video', '#dataset', '#synthetic', '#open_source', '#multimodal', '#robotics', '#training', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'GigaWorld-0 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: GigaWorld-0-Video Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° GigaWorld-0-3D Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GigaTrain Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ VLA, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'GigaWorld-0: Bridging Virtual and Real for Embodied AI', 'desc': 'GigaWorld-0 is a comprehensive framework that combines video generation and 3D modeling to create high-quality data for Vision-Language-Action (VLA) learning. It features two main components: GigaWorld-0-Video, which generates diverse and coherent video sequences, and GigaWorld-0-3D, which ensures realistic 3D modeling and motion planning. This integration allows for the production of visually rich and physically plausible data that can be used to train AI models effectively. The GigaTrain framework optimizes the training process, enabling strong performance in real-world applications without requiring real-world training data.'}, 'zh': {'title': 'GigaWorld-0ï¼šæ— é¡»çœŸå®è®­ç»ƒçš„æ™ºèƒ½ä¸–ç•Œæ¨¡å‹', 'desc': 'GigaWorld-0æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä¸–ç•Œæ¨¡å‹æ¡†æ¶ï¼Œç»“åˆäº†è§†é¢‘ç”Ÿæˆå’Œ3Då»ºæ¨¡ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”ç‰©ç†ä¸Šåˆç†çš„VLAæ•°æ®ã€‚è¿™ä¸€æ¡†æ¶ä¸ºå…·èº«äººå·¥æ™ºèƒ½æä¾›äº†å¼ºå¤§çš„ç°å®ä¸–ç•Œè¡¨ç°ï¼Œè€Œæ— éœ€åœ¨çœŸå®ä¸–ç•Œä¸­è¿›è¡Œè®­ç»ƒã€‚GigaWorld-0é€šè¿‡GigaWorld-0-Videoå’ŒGigaWorld-0-3Dä¸¤ä¸ªç»„ä»¶çš„ååŒä½œç”¨ï¼Œå®ç°äº†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œå­¦ä¹ çš„æ•°æ®å¼•æ“ã€‚é€šè¿‡é«˜æ•ˆçš„GigaTrainæ¡†æ¶ï¼ŒGigaWorld-0èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œç”Ÿæˆå¯æ§çš„é«˜è´¨é‡æ•°æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20102', 'title': 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space', 'url': 'https://huggingface.co/papers/2511.20102', 'abstract': 'SSA, a unified training framework for sparse attention in LLMs, achieves state-of-the-art performance by aligning sparse attention with full attention, improving long-context processing and extrapolation.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.', 'score': 26, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '07cc3df59796e658', 'authors': ['Zhenyi Shen', 'Junru Lu', 'Lin Gui', 'Jiazheng Li', 'Yulan He', 'Di Yin', 'Xing Sun'], 'affiliations': ['Kings College London', 'Tencent Youtu Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20102.jpg', 'data': {'categories': ['#optimization', '#long_context', '#training', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'SSA â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²ÑƒÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¸Ğ·-Ğ·Ğ° Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸ÑĞºĞ»ÑÑ‡Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ. SSA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking Sparse Attention: SSA for Superior Performance in LLMs', 'desc': "The paper introduces SSA (Sparse Sparse Attention), a novel training framework designed to enhance sparse attention mechanisms in large language models (LLMs). By aligning sparse attention with full attention, SSA addresses the limitations of traditional methods that struggle with performance degradation during training. This framework ensures that all tokens receive proper gradient updates, which helps maintain effective learning and improves long-context processing. As a result, SSA not only achieves state-of-the-art performance but also allows for flexible adjustments in attention sparsity, enhancing the model's adaptability and extrapolation capabilities."}, 'zh': {'title': 'ç¨€ç–æ³¨æ„åŠ›çš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶', 'desc': 'SSAï¼ˆç¨€ç–ç¨€ç–æ³¨æ„åŠ›ï¼‰æ˜¯ä¸€ç§ç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ç¨€ç–æ³¨æ„åŠ›çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†ç¨€ç–æ³¨æ„åŠ›ä¸å…¨æ³¨æ„åŠ›å¯¹é½ï¼ŒSSAæ˜¾è‘—æé«˜äº†é•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œå¤–æ¨èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¯ä¸€å±‚å¼ºåˆ¶åŒå‘å¯¹é½ï¼Œç¡®ä¿æ‰€æœ‰æ ‡è®°çš„æ¢¯åº¦æµåŠ¨ï¼Œä»è€Œå¢å¼ºäº†ç¨€ç–æ³¨æ„åŠ›çš„æ•ˆæœã€‚æœ€ç»ˆï¼ŒSSAåœ¨å¤šä¸ªå¸¸è¯†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒçµæ´»çš„è®¡ç®—ä¸æ€§èƒ½æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19575', 'title': 'HunyuanOCR Technical Report', 'url': 'https://huggingface.co/papers/2511.19575', 'abstract': 'HunyuanOCR, a lightweight Vision-Language Model, achieves state-of-the-art performance in OCR tasks through a unified end-to-end architecture combining Vision Transformer and lightweight LLM, supported by data-driven and RL strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.   HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.   HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.', 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'b9cce7a31b43bdc4', 'authors': ['Hunyuan Vision Team', 'Pengyuan Lyu', 'Xingyu Wan', 'Gengluo Li', 'Shangpin Peng', 'Weinong Wang', 'Liang Wu', 'Huawen Shen', 'Yu Zhou', 'Canhui Tang', 'Qi Yang', 'Qiming Peng', 'Bin Luo', 'Hower Yang', 'Houwen Peng', 'Hongming Yang', 'Senhao Xie', 'Binghong Wu', 'Mana Yang', 'Sergey Wang', 'Raccoon Liu', 'Dick Zhu', 'Jie Jiang', 'Linus', 'Han Hu', 'Chengquan Zhang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19575.jpg', 'data': {'categories': ['#cv', '#optimization', '#rl', '#benchmark', '#open_source', '#multimodal', '#small_models', '#training'], 'emoji': 'ğŸ“–', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ°Ñ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½ĞµÑ† Ğ² ĞºĞ¾Ğ½ĞµÑ†', 'desc': 'HunyuanOCR â€” ÑÑ‚Ğ¾ Ğ»Ñ‘Ğ³ĞºĞ°Ñ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ· Vision Transformer Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ LLM, ÑĞ¾ĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· MLP Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ end-to-end Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² reinforcement learning Ğ´Ğ»Ñ OCR Ğ·Ğ°Ğ´Ğ°Ñ‡. HunyuanOCR Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑÑ€ĞµĞ´Ğ¸ Vision-Language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'HunyuanOCR: Lightweight Excellence in OCR Tasks', 'desc': "HunyuanOCR is a lightweight Vision-Language Model designed for Optical Character Recognition (OCR) tasks, achieving state-of-the-art results with only 1 billion parameters. It integrates a Vision Transformer and a lightweight language model through an MLP adapter, allowing it to perform various tasks like text spotting and image translation efficiently. The model's end-to-end architecture eliminates the need for pre-processing, reducing error propagation and simplifying deployment. Additionally, it leverages high-quality data and Reinforcement Learning strategies to enhance performance, making it a competitive choice against larger models and commercial APIs."}, 'zh': {'title': 'HunyuanOCRï¼šè½»é‡çº§OCRçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'HunyuanOCRæ˜¯ä¸€ç§è½»é‡çº§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä»»åŠ¡ã€‚å®ƒç»“åˆäº†è§†è§‰å˜æ¢å™¨å’Œè½»é‡çº§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯æ¶æ„å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„OCRè§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬æ£€æµ‹ã€è§£æå’Œè¯­ä¹‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ICDAR 2025 DIMTæŒ‘æˆ˜èµ›ä¸­è·å¾—ç¬¬ä¸€åã€‚HunyuanOCRè¿˜é€šè¿‡æ•°æ®é©±åŠ¨å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†OCRä»»åŠ¡çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18886', 'title': 'MagicWorld: Interactive Geometry-driven Video World Exploration', 'url': 'https://huggingface.co/papers/2511.18886', 'abstract': 'MagicWorld, an interactive video world model, integrates 3D geometry and historical retrieval to improve scene stability and continuity under user instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.', 'score': 18, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '7bd894ff44343573', 'authors': ['Guangyuan Li', 'Siming Zheng', 'Shuolin Xu', 'Jinwei Chen', 'Bo Li', 'Xiaobin Hu', 'Lei Zhao', 'Peng-Tao Jiang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University', 'National University of Singapore', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18886.jpg', 'data': {'categories': ['#video', '#games', '#3d', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°', 'desc': 'MagicWorld â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ AG3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ HCR Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ¸Ğ· Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ.'}, 'en': {'title': 'Enhancing Interactive Video with 3D Geometry and Historical Context', 'desc': 'MagicWorld is an innovative interactive video world model that enhances scene stability and continuity by combining 3D geometry with historical data retrieval. It addresses two main challenges in existing models: the lack of structural stability during viewpoint changes and the loss of historical context in multi-step interactions. The model utilizes an Action-Guided 3D Geometry Module (AG3D) to create a point cloud from the initial scene and user actions, ensuring consistent structural integrity. Additionally, the History Cache Retrieval (HCR) mechanism helps the model remember past frames, reducing errors and improving the overall coherence of the generated scenes.'}, 'zh': {'title': 'é­”æ³•ä¸–ç•Œï¼šæå‡äº¤äº’è§†é¢‘åœºæ™¯çš„ç¨³å®šæ€§ä¸è¿ç»­æ€§', 'desc': 'MagicWorld æ˜¯ä¸€ç§äº¤äº’å¼è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒç»“åˆäº†ä¸‰ç»´å‡ ä½•å’Œå†å²æ£€ç´¢ï¼Œä»¥æé«˜åœºæ™¯çš„ç¨³å®šæ€§å’Œè¿ç»­æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”¨æˆ·æŒ‡ä»¤é©±åŠ¨åœºæ™¯çš„åŠ¨æ€æ¼”å˜ï¼Œå¹¶è‡ªå›å½’åœ°åˆæˆè¿ç»­åœºæ™¯ã€‚MagicWorld å¼•å…¥äº†åŠ¨ä½œå¼•å¯¼çš„ä¸‰ç»´å‡ ä½•æ¨¡å—ï¼ˆAG3Dï¼‰ï¼Œé€šè¿‡æ„å»ºç‚¹äº‘æ¥æä¾›è§†è§’è½¬æ¢çš„å‡ ä½•çº¦æŸï¼Œä»è€Œæ”¹å–„ç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå†å²ç¼“å­˜æ£€ç´¢æœºåˆ¶ï¼ˆHCRï¼‰å¸®åŠ©æ¨¡å‹åˆ©ç”¨è¿‡å»çš„åœºæ™¯ä¿¡æ¯ï¼Œå‡å°‘é”™è¯¯ç´¯ç§¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMagicWorld åœ¨äº¤äº’è¿­ä»£ä¸­æ˜¾è‘—æé«˜äº†åœºæ™¯çš„ç¨³å®šæ€§å’Œè¿ç»­æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20123', 'title': 'UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2511.20123', 'abstract': 'UltraViCo addresses video length extrapolation by suppressing attention dispersion, improving quality and reducing repetition beyond training length.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.', 'score': 16, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '40637b6b863fa375', 'authors': ['Min Zhao', 'Hongzhou Zhu', 'Yingze Wang', 'Bokai Yan', 'Jintao Zhang', 'Guande He', 'Ling Yang', 'Chongxuan Li', 'Jun Zhu'], 'affiliations': ['Princeton University', 'Renmin University of China', 'ShengShu', 'The University of Texas at Austin', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20123.jpg', 'data': {'categories': ['#video', '#diffusion', '#architecture', '#interpretability'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞ¿ĞµÑ€Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ UltraViCo Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ¸ÑĞ¿ĞµÑ€Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ½Ğ°Ñ€ÑƒÑˆĞ°ÑÑ‚ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ° Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ Ğ¾Ğ±Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UltraViCo Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ 2x Ğ´Ğ¾ 4x Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° 40.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'UltraViCo: Enhancing Video Extrapolation by Controlling Attention', 'desc': "UltraViCo is a novel approach to video length extrapolation that enhances the quality of generated videos while minimizing repetition beyond the training length. The paper identifies two main issues in existing models: periodic content repetition and a decline in video quality, both caused by attention dispersion in the model's attention maps. By introducing a constant decay factor to suppress attention for tokens outside the training window, UltraViCo effectively addresses these issues. This method not only improves extrapolation capabilities from 2x to 4x but also significantly boosts video quality metrics, making it applicable to various video synthesis and editing tasks."}, 'zh': {'title': 'è¶…è¶Šè®­ç»ƒé™åˆ¶ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡', 'desc': 'UltraViCo æ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘é•¿åº¦å¤–æ¨çš„æŠ€æœ¯ï¼Œé€šè¿‡æŠ‘åˆ¶æ³¨æ„åŠ›åˆ†æ•£æ¥æé«˜è§†é¢‘è´¨é‡å¹¶å‡å°‘é‡å¤ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œè§†é¢‘æ‰©å±•ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜æ˜¯å†…å®¹çš„å‘¨æœŸæ€§é‡å¤å’Œæ•´ä½“è´¨é‡ä¸‹é™ã€‚ä»¥å¾€çš„æ–¹æ³•ä¸»è¦å…³æ³¨ä½ç½®ç¼–ç ï¼Œæœªèƒ½æœ‰æ•ˆè§£å†³è´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚UltraViCo æå‡ºäº†ä¸€ç§æ— è®­ç»ƒã€å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹è¶…å‡ºè®­ç»ƒçª—å£çš„æ ‡è®°æ–½åŠ æ’å®šè¡°å‡å› å­ï¼ŒæˆåŠŸè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18659', 'title': 'CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning', 'url': 'https://huggingface.co/papers/2511.18659', 'abstract': 'CLaRa enhances retrieval-augmented generation by introducing unified embedding-based compression and joint optimization, achieving state-of-the-art performance in QA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.', 'score': 14, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '5b7798a92ba5d4e8', 'authors': ['Jie He', 'Richard He Bai', 'Sinead Williamson', 'Jeff Z. Pan', 'Navdeep Jaitly', 'Yizhe Zhang'], 'affiliations': ['Apple', 'University of Edinburgh'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18659.jpg', 'data': {'categories': ['#optimization', '#long_context', '#synthetic', '#benchmark', '#rag', '#training'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'CLaRa Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑæ¡†æ¶Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SCP Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ CLaRa Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'CLaRa: Unifying Compression and Optimization for Superior QA Performance', 'desc': 'CLaRa is a new framework that improves retrieval-augmented generation (RAG) by using a unified approach to compress and optimize data. It combines embedding-based compression with joint optimization in a continuous space, which helps in managing long contexts and enhances the quality of generated answers. The framework introduces a novel data synthesis method called SCP, which ensures that the compressed vectors maintain important information for retrieval. By training both the reranker and generator together, CLaRa achieves better performance in question-answering tasks compared to traditional methods.'}, 'zh': {'title': 'CLaRaï¼šç»Ÿä¸€ä¼˜åŒ–ï¼Œæå‡é—®ç­”æ€§èƒ½', 'desc': 'CLaRaæ˜¯ä¸€ç§å¢å¼ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»Ÿä¸€çš„åµŒå…¥å‹ç¼©å’Œè”åˆä¼˜åŒ–æ¥æå‡æ€§èƒ½ã€‚è¯¥æ¡†æ¶åœ¨å…±äº«çš„è¿ç»­ç©ºé—´ä¸­è¿›è¡ŒåµŒå…¥å‹ç¼©ï¼Œæ—¨åœ¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œä¸”å¯æ£€ç´¢çš„å‹ç¼©å‘é‡ã€‚CLaRaä½¿ç”¨å…³é”®ä¿ç•™çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œç»“åˆé—®ç­”å’Œæ”¹å†™ç›‘ç£è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLaRaåœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†åŸºäºæ–‡æœ¬çš„å¾®è°ƒåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20211', 'title': 'OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation', 'url': 'https://huggingface.co/papers/2511.20211', 'abstract': 'OmniAlpha, a unified multi-task generative framework, excels in RGBA image generation and editing using a Diffusion Transformer with a novel MSRoPE-BiL method, outperforming specialized models across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '40c0c81d50816722', 'authors': ['Hao Yu', 'Jiabo Zhan', 'Zile Wang', 'Jinglin Wang', 'Huaisong Zhang', 'Hongyu Li', 'Xinrui Chen', 'Yongxian Wei', 'Chun Yuan'], 'affiliations': ['Beihang University', 'Beijing University of Posts and Telecommunications', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20211.jpg', 'data': {'categories': ['#cv', '#dataset', '#architecture', '#diffusion', '#synthetic', '#open_source', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'OmniAlpha â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ MSRoPE-BiL Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ RGBA Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AlphaLayers Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° 21 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ĞµĞ¹ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑƒĞ·ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ RGBA Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'OmniAlpha: Unifying RGBA Image Generation and Editing', 'desc': 'OmniAlpha is a new generative framework designed to handle RGBA image generation and editing tasks effectively. It uses a Diffusion Transformer architecture enhanced by a novel method called MSRoPE-BiL, which allows it to process multiple RGBA layers simultaneously. This framework is trained on a unique dataset called AlphaLayers, consisting of high-quality multi-layer images, enabling it to perform well across various tasks. The results show that OmniAlpha significantly outperforms specialized models, demonstrating its potential for creating versatile and powerful generative systems.'}, 'zh': {'title': 'OmniAlphaï¼šç»Ÿä¸€çš„å¤šä»»åŠ¡RGBAç”Ÿæˆæ¡†æ¶', 'desc': 'OmniAlphaæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡ç”Ÿæˆæ¡†æ¶ï¼Œä¸“æ³¨äºRGBAå›¾åƒçš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„MSRoPE-BiLæ–¹æ³•ï¼Œç»“åˆäº†æ‰©å±•çš„åŒå‘å±‚è½´ï¼Œæå‡äº†Diffusion Transformerçš„æ€§èƒ½ã€‚é€šè¿‡åœ¨ä¸€ä¸ªåŒ…å«1000ä¸ªé«˜è´¨é‡å¤šå±‚ä¸‰å…ƒç»„çš„æ–°æ•°æ®é›†AlphaLayersä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼ŒOmniAlphaåœ¨21ä¸ªä¸åŒä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šä¸“é—¨æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»Ÿä¸€çš„å¤šä»»åŠ¡æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¼˜çš„RGBAå…±äº«è¡¨ç¤ºï¼Œä¸ºæ›´å¼ºå¤§çš„ç”Ÿæˆç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19663', 'title': 'Fara-7B: An Efficient Agentic Model for Computer Use', 'url': 'https://huggingface.co/papers/2511.19663', 'abstract': 'FaraGen creates synthetic datasets for computer use agents, enabling the training of efficient and high-performing models like Fara-7B on diverse web tasks, outperforming larger models on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.', 'score': 12, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '284658fc86054709', 'authors': ['Ahmed Awadallah', 'Yash Lara', 'Raghav Magazine', 'Hussein Mozannar', 'Akshay Nambi', 'Yash Pandya', 'Aravind Rajeswaran', 'Corby Rosset', 'Alexey Taymanov', 'Vibhav Vineet', 'Spencer Whitehead', 'Andrew Zhao'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19663.jpg', 'data': {'categories': ['#cv', '#dataset', '#data', '#synthetic', '#benchmark', '#open_source', '#small_models', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'FaraGen â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹ Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Fara-7B â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Fara-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WebVoyager, Online-Mind2Web Ğ¸ WebTailBench, ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Empowering Small Models with Synthetic Data for Web Tasks', 'desc': 'FaraGen is a synthetic data generation system designed to create high-quality datasets for training computer use agents (CUAs). It generates diverse multi-step web tasks and filters successful action trajectories using multiple verifiers, enabling efficient training of models like Fara-7B. This model, which operates using only screenshots and predicted coordinates, demonstrates superior performance on various benchmarks compared to larger models. By providing scalable and verified data, FaraGen significantly enhances the capabilities of smaller, efficient agentic models in web task execution.'}, 'zh': {'title': 'FaraGenï¼šæ¨åŠ¨é«˜æ•ˆè®¡ç®—æœºä½¿ç”¨ä»£ç†çš„åˆæˆæ•°æ®ç”Ÿæˆ', 'desc': 'FaraGen æ˜¯ä¸€ä¸ªæ–°é¢–çš„åˆæˆæ•°æ®ç”Ÿæˆç³»ç»Ÿï¼Œä¸“ä¸ºè®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰è®¾è®¡ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ­¥éª¤çš„ç½‘ç»œä»»åŠ¡æ•°æ®ã€‚å®ƒé€šè¿‡ä»å¸¸ç”¨ç½‘ç«™æå‡ºå¤šæ ·åŒ–ä»»åŠ¡ï¼Œç”Ÿæˆå¤šä¸ªè§£å†³æ–¹æ¡ˆå°è¯•ï¼Œå¹¶ä½¿ç”¨å¤šä¸ªéªŒè¯å™¨è¿‡æ»¤æˆåŠŸçš„è½¨è¿¹ï¼Œæ¥è§£å†³ç¼ºä¹é«˜è´¨é‡æ•°æ®é›†çš„é—®é¢˜ã€‚ä½¿ç”¨ FaraGen ç”Ÿæˆçš„æ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒäº† Fara-7B æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…é€šè¿‡æˆªå›¾æ„ŸçŸ¥è®¡ç®—æœºï¼Œå¹¶é€šè¿‡é¢„æµ‹åæ ‡æ‰§è¡Œæ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFara-7B åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŒç±»æ¨¡å‹ï¼Œå¹¶ä¸æ›´å¤§è§„æ¨¡çš„å‰æ²¿æ¨¡å‹ç«äº‰ï¼Œå±•ç¤ºäº†å¯æ‰©å±•æ•°æ®ç”Ÿæˆç³»ç»Ÿåœ¨æ¨åŠ¨å°å‹é«˜æ•ˆä»£ç†æ¨¡å‹æ–¹é¢çš„å…³é”®ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19827', 'title': 'ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding', 'url': 'https://huggingface.co/papers/2511.19827', 'abstract': 'ReDirector uses a novel camera-controlled video retake method with Rotary Camera Encoding (RoCE) to improve dynamic object localization and static background preservation in variable-length videos.  \t\t\t\t\tAI-generated summary \t\t\t\t We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '9a93bd19c3c656aa', 'authors': ['Byeongjun Park', 'Byung-Hoon Kim', 'Hyungjin Chung', 'Jong Chul Ye'], 'affiliations': ['EverEx', 'KAIST', 'Yonsei University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19827.jpg', 'data': {'categories': ['#video', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'ReDirector Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ€ĞµÑ‚ĞµĞ¹ĞºĞ¾Ğ² Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RoPE Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‚ĞµĞ¹ĞºĞ°. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Rotary Camera Encoding (RoCE) â€” ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³ RoPE. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ°Ğ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ğ½Ğ°.'}, 'en': {'title': 'ReDirector: Enhancing Video Retakes with Camera Control', 'desc': 'ReDirector is a new method for generating video retakes that uses camera control to enhance the quality of dynamic objects and maintain a stable background. It introduces Rotary Camera Encoding (RoCE), which helps align the positions of the original and retake videos in both space and time. This method corrects previous errors in using RoPE by incorporating camera conditions, allowing it to work well with different camera movements and video lengths. The results show that ReDirector significantly improves the accuracy of object localization, the consistency of geometry, and the overall quality of the videos.'}, 'zh': {'title': 'ReDirectorï¼šæå‡åŠ¨æ€ç‰©ä½“å®šä½ä¸èƒŒæ™¯ä¿ç•™çš„åˆ›æ–°è§†é¢‘é‡æ‹æ–¹æ³•', 'desc': 'ReDirectoræ˜¯ä¸€ç§æ–°é¢–çš„æ‘„åƒæœºæ§åˆ¶è§†é¢‘é‡æ‹ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„åŠ¨æ€ç‰©ä½“å®šä½å’Œé™æ€èƒŒæ™¯ä¿ç•™ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—‹è½¬æ‘„åƒæœºç¼–ç ï¼ˆRoCEï¼‰æ¥å¯¹è¾“å…¥è§†é¢‘å’Œç›®æ ‡é‡æ‹çš„æ—¶ç©ºä½ç½®è¿›è¡Œå¯¹é½ï¼Œçº æ­£äº†ä»¥å¾€ç ”ç©¶ä¸­RoPEçš„å¸¸è§è¯¯ç”¨ã€‚é€šè¿‡å°†æ‘„åƒæœºæ¡ä»¶æ•´åˆåˆ°RoPEä¸­ï¼ŒReDirectorèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ‘„åƒæœºè½¨è¿¹å’Œè§†é¢‘é•¿åº¦ï¼Œä»è€Œæé«˜åŠ¨æ€ç‰©ä½“çš„å®šä½ç²¾åº¦å’Œé™æ€èƒŒæ™¯çš„ä¿ç•™æ•ˆæœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‘„åƒæœºå¯æ§æ€§ã€å‡ ä½•ä¸€è‡´æ€§å’Œè§†é¢‘è´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19773', 'title': 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs', 'url': 'https://huggingface.co/papers/2511.19773', 'abstract': 'VISTA-Gym enhances vision-language models\' tool-integrated visual reasoning through a scalable training environment with diverse multimodal tasks and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'dc9d2b2ca9ab4588', 'authors': ['Meng Lu', 'Ran Xu', 'Yi Fang', 'Wenxuan Zhang', 'Yue Yu', 'Gaurav Srivastava', 'Yuchen Zhuang', 'Mohamed Elhoseiny', 'Charles Fleming', 'Carl Yang', 'Zhengzhong Tu', 'Yang Xie', 'Guanghua Xiao', 'Hanrui Wang', 'Di Jin', 'Wenqi Shi', 'Xuan Wang'], 'affiliations': ['Cisco', 'Eigen AI', 'Emory University', 'Georgia Tech', 'KAUST', 'TAMU', 'UT Southwestern Medical Center', 'Virginia Tech'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19773.jpg', 'data': {'categories': ['#cv', '#optimization', '#dataset', '#rl', '#reasoning', '#benchmark', '#open_source', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° VISTA-Gym â€” Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ñ€ĞµĞ´Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 7 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 13 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VISTA-Gym Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VISTA-R1 Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ² ĞµÑ‘ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VISTA-R1-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 9.51%-18.72% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Visual Reasoning with VISTA-Gym', 'desc': "VISTA-Gym is a new training environment designed to improve vision-language models (VLMs) by enhancing their ability to perform complex visual reasoning tasks that involve using tools. It combines various multimodal reasoning tasks and provides a standardized interface for visual tools, allowing models to learn through interactive loops and receive feedback. The system enables reinforcement learning at scale, helping models like VISTA-R1 to better integrate tool use with reasoning processes. Experiments show that VISTA-R1 significantly outperforms existing models, proving VISTA-Gym's effectiveness in developing advanced reasoning capabilities in VLMs."}, 'zh': {'title': 'VISTA-Gymï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'VISTA-Gymæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è®­ç»ƒç¯å¢ƒï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å·¥å…·é›†æˆè§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡ç»Ÿä¸€å¤šç§çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œæä¾›æ ‡å‡†åŒ–çš„è§†è§‰å·¥å…·æ¥å£å’Œå¯æ‰§è¡Œçš„äº¤äº’å¾ªç¯ï¼Œä¿ƒè¿›äº†è§†è§‰ä»£ç†çš„å¼ºåŒ–å­¦ä¹ ã€‚å°½ç®¡ç°æœ‰çš„VLMåœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å·¥å…·é€‰æ‹©å’Œåè°ƒæ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚é€šè¿‡VISTA-Gymè®­ç»ƒçš„VISTA-R1æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å·¥å…·é›†æˆæ¨ç†èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20415', 'title': 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts', 'url': 'https://huggingface.co/papers/2511.20415', 'abstract': 'MajutsuCity is a natural language-driven framework that synthesizes 3D urban scenes with high structural consistency, stylistic diversity, and controllability through a four-stage pipeline and interactive editing agent.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'c6ea8bba05dab344', 'authors': ['Zilong Huang', 'Jun He', 'Xiaobin Huang', 'Ziyi Xiong', 'Yang Luo', 'Junyan Ye', 'Weijia Li', 'Yiping Chen', 'Ting Han'], 'affiliations': ['Sun Yat-sen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20415.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#benchmark', '#open_source', '#multimodal', '#agents', '#3d', '#games'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼', 'desc': 'MajutsuCity â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³Ğ¾Ñ€Ğ¾Ğ´ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ², 3D-Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° MajutsuAgent, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ FID Ğ½Ğ° 83.7% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'MajutsuCity: Revolutionizing 3D Urban Scene Generation with Language Control', 'desc': 'MajutsuCity is a framework designed to create realistic 3D urban environments using natural language inputs. It employs a four-stage pipeline that ensures high structural consistency while allowing for stylistic diversity and user control. The framework includes an interactive editing agent, MajutsuAgent, which enables users to make specific changes to the generated scenes. Additionally, it is supported by a comprehensive dataset, MajutsuDataset, which provides essential resources for training and evaluation, demonstrating significant improvements over previous methods in terms of quality and flexibility.'}, 'zh': {'title': 'MajutsuCityï¼š3DåŸå¸‚ç”Ÿæˆçš„æ–°æ ‡æ†', 'desc': 'MajutsuCityæ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„æ¡†æ¶ï¼Œç”¨äºåˆæˆç»“æ„ä¸€è‡´ã€é£æ ¼å¤šæ ·ä¸”å¯æ§çš„3DåŸå¸‚åœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡å››ä¸ªé˜¶æ®µçš„æµç¨‹å’Œäº¤äº’å¼ç¼–è¾‘ä»£ç†ï¼Œè§£å†³äº†æ–‡æœ¬ç”Ÿæˆçš„åˆ›æ„çµæ´»æ€§ä¸æ˜ç¡®ç»“æ„è¡¨ç¤ºçš„å¯¹è±¡çº§å¯ç¼–è¾‘æ€§ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MajutsuCityå°†åŸå¸‚è¡¨ç¤ºä¸ºå¯æ§çš„å¸ƒå±€ã€èµ„äº§å’Œææ–™çš„ç»„åˆï¼Œå¹¶æ”¯æŒå¤šç§å¯¹è±¡çº§æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMajutsuCityåœ¨å‡ ä½•ä¿çœŸåº¦ã€é£æ ¼é€‚åº”æ€§å’Œè¯­ä¹‰å¯æ§æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæˆä¸º3DåŸå¸‚ç”Ÿæˆçš„æ–°æ ‡æ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.16660', 'title': 'Cognitive Foundations for Reasoning and Their Manifestation in LLMs', 'url': 'https://huggingface.co/papers/2511.16660', 'abstract': 'LLMs exhibit reasoning gaps compared to humans, underutilizing cognitive elements and failing to deploy meta-cognitive controls, but test-time guidance can improve their performance on complex problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '096b6123122562db', 'authors': ['Priyanka Kargupta', 'Shuyue Stella Li', 'Haocheng Wang', 'Jinu Lee', 'Shan Chen', 'Orevaoghene Ahia', 'Dean Light', 'Thomas L. Griffiths', 'Max Kleiman-Weiner', 'Jiawei Han', 'Asli Celikyilmaz', 'Yulia Tsvetkov'], 'affiliations': ['Harvard University', 'Princeton University', 'University of Illinois Urbana-Champaign', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16660.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark', '#multimodal', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ¾ÑÑ‚Ğ¸Ğº Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¾Ğ¹ Ğ¸ LLM: Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸Ğ· 28 ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· 192 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ 18 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ 54 Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°Ğ¼Ğ¸. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ LLM Ğ½ĞµĞ´Ğ¾Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¶ĞµÑÑ‚ĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° 66,7% Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Reasoning Gap: Enhancing LLMs with Cognitive Insights', 'desc': 'This paper explores the reasoning gaps between large language models (LLMs) and human cognition, highlighting that LLMs often struggle with simpler problems despite their ability to tackle complex ones. The authors create a taxonomy of cognitive elements from cognitive science, identifying key areas where LLMs underperform, such as meta-cognitive controls and diverse representations. Through a large-scale analysis of model outputs and human reasoning processes, they find that LLMs tend to rely on rigid, surface-level processing rather than deeper conceptual understanding. To address these issues, the paper introduces test-time reasoning guidance that enhances LLM performance by up to 66.7% on complex tasks, promoting a more robust cognitive approach in AI development.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç®€å•é—®é¢˜ä¸Šå´å¸¸å¸¸å¤±è´¥ï¼Œè¿™è¡¨æ˜å®ƒä»¬çš„è¾“å‡ºæœºåˆ¶ä¸äººç±»æ¨ç†æœ‰æ ¹æœ¬ä¸åŒã€‚æˆ‘ä»¬å°†è®¤çŸ¥ç§‘å­¦çš„ç ”ç©¶æ•´åˆæˆä¸€ä¸ªåŒ…å«28ä¸ªè®¤çŸ¥å…ƒç´ çš„åˆ†ç±»æ³•ï¼Œæ¶µç›–æ¨ç†ä¸å˜æ€§ã€å…ƒè®¤çŸ¥æ§åˆ¶ã€çŸ¥è¯†ç»„ç»‡çš„è¡¨ç¤ºå’Œå˜æ¢æ“ä½œã€‚é€šè¿‡å¯¹18ä¸ªæ¨¡å‹çš„192Kä¸ªè¿½è¸ªæ•°æ®è¿›è¡Œå¤§è§„æ¨¡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶æœªèƒ½å……åˆ†åˆ©ç”¨ä¸æˆåŠŸç›¸å…³çš„è®¤çŸ¥å…ƒç´ ã€‚æˆ‘ä»¬æå‡ºçš„æµ‹è¯•æ—¶æ¨ç†æŒ‡å¯¼å¯ä»¥è‡ªåŠ¨æ„å»ºæˆåŠŸçš„ç»“æ„ï¼Œä½¿æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°æé«˜äº†66.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15703', 'title': 'Think Visually, Reason Textually: Vision-Language Synergy in ARC', 'url': 'https://huggingface.co/papers/2511.15703', 'abstract': 'Combining visual and linguistic reasoning strategies improves performance on abstract reasoning tasks in the ARC-AGI dataset by leveraging the strengths of each modality.  \t\t\t\t\tAI-generated summary \t\t\t\t Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'a4ab06821bd2a601', 'authors': ['Beichen Zhang', 'Yuhang Zang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15703.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#benchmark', '#agi'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ARC-AGI Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ğ¸Ğ¼ĞµÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹: Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ° ÑĞ·Ñ‹Ğº â€” Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Vision-Language Synergy Reasoning (VLSR) Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Modality-Switch Self-Correction (MSSC) Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ñ€ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 4,33% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ğ·ĞµĞ»Ğ°Ğ¹Ğ½Ğ¾Ğ².'}, 'en': {'title': 'Unifying Vision and Language for Smarter Reasoning', 'desc': 'This paper explores how combining visual and linguistic reasoning can enhance performance on abstract reasoning tasks, specifically using the ARC-AGI dataset. It identifies that existing models struggle with structured transformation rules, which are essential for human-like intelligence. The authors propose two strategies: Vision-Language Synergy Reasoning (VLSR) to break down tasks into visual and textual components, and Modality-Switch Self-Correction (MSSC) to use visual input for verifying text-based reasoning. Their experiments show that this integrated approach leads to significant performance improvements over traditional text-only methods.'}, 'zh': {'title': 'è§†è§‰ä¸è¯­è¨€çš„ååŒæ¨ç†æå‡æ™ºèƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨ç†ç­–ç•¥å¦‚ä½•æé«˜åœ¨ARC-AGIæ•°æ®é›†ä¸Šçš„æŠ½è±¡æ¨ç†ä»»åŠ¡è¡¨ç°ã€‚ç°æœ‰æ¨¡å‹åœ¨ä»å°‘é‡ç¤ºä¾‹ä¸­æ¨å¯¼ç»“æ„åŒ–è½¬æ¢è§„åˆ™æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™æ˜¯äººç±»æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰-è¯­è¨€ååŒæ¨ç†ï¼ˆVLSRï¼‰å’Œæ¨¡æ€åˆ‡æ¢è‡ªæˆ‘æ ¡æ­£ï¼ˆMSSCï¼‰ä¸¤ç§ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨è§†è§‰å’Œè¯­è¨€çš„äº’è¡¥ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªARC-AGIä»»åŠ¡ä¸Šæ¯”ä»…ä½¿ç”¨æ–‡æœ¬çš„åŸºçº¿æ¨¡å‹æé«˜äº†æœ€å¤š4.33%çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20573', 'title': 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering', 'url': 'https://huggingface.co/papers/2511.20573', 'abstract': 'A data-centric framework and benchmark for Visual Question-Visual Answering (VQ-VA) improve open-source model performance, narrowing the gap with proprietary systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'e66794f7650939a8', 'authors': ['Chenhui Gou', 'Zilong Chen', 'Zeyu Wang', 'Feng Li', 'Deyao Zhu', 'Zicheng Duan', 'Kunchang Li', 'Chaorui Deng', 'Hongyi Yuan', 'Haoqi Fan', 'Cihang Xie', 'Jianfei Cai', 'Hamid Rezatofighi'], 'affiliations': ['Bytedance Seed', 'Monash University', 'Tsinghua University', 'UC Santa Cruz', 'University of Adelaide'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20573.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#benchmark', '#open_source', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ñƒ: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VQ-VA World â€” Ğ´Ğ°Ñ‚Ğ°Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Visual Question-Visual Answering, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ pipeline Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° 1.8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IntelligentBench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LightFusion Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ñ‚Ğ¸Ğ¿Ğ° NanoBanana Ğ¸ GPT-Image.'}, 'en': {'title': 'Bridging the Gap in Visual Question-Visual Answering', 'desc': 'This paper presents a new framework called VQ-VA World for improving Visual Question-Visual Answering (VQ-VA) capabilities in open-source models. It focuses on generating images in response to visual questions, a feature that has been primarily seen in proprietary systems. The framework utilizes a large-scale data construction pipeline that collects approximately 1.8 million high-quality image-text pairs for training models. Additionally, the authors introduce IntelligentBench, a benchmark for evaluating VQ-VA performance, which shows significant improvements in model accuracy, bringing open-source models closer to proprietary counterparts.'}, 'zh': {'title': 'æå‡å¼€æºè§†è§‰é—®ç­”æ¨¡å‹çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†è§†è§‰é—®ç­”ï¼ˆVQ-VAï¼‰ï¼Œå³æ ¹æ®è§†è§‰é—®é¢˜ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚ä¸ºäº†æå‡å¼€æºæ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†VQ-VA Worldï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¤§è§„æ¨¡æ„å»ºé’ˆå¯¹æ€§çš„æ•°æ®ã€‚é€šè¿‡ç½‘ç»œè§„æ¨¡çš„éƒ¨ç½²ï¼Œè¯¥æ¡†æ¶æ”¶é›†äº†çº¦180ä¸‡é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬æ ·æœ¬ç”¨äºæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†IntelligentBenchï¼Œä¸€ä¸ªäººç±»ç­–åˆ’çš„åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°VQ-VAåœ¨ä¸–ç•ŒçŸ¥è¯†ã€è®¾è®¡çŸ¥è¯†å’Œæ¨ç†ç­‰æ–¹é¢çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19430', 'title': 'Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution', 'url': 'https://huggingface.co/papers/2511.19430', 'abstract': 'ORS3D, a new task requiring language understanding, 3D grounding, and efficient scheduling, is introduced with a large dataset and an embodied multi-modal model named GRANT that uses a scheduling token mechanism for effective task management.  \t\t\t\t\tAI-generated summary \t\t\t\t Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT', 'score': 7, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'cdfc8eff4fd9aa8c', 'authors': ['Dingkang Liang', 'Cheng Zhang', 'Xiaopeng Xu', 'Jianzhong Ju', 'Zhenbo Luo', 'Xiang Bai'], 'affiliations': ['Huazhong University of Science and Technology', 'MiLM Plus, Xiaomi Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19430.jpg', 'data': {'categories': ['#optimization', '#dataset', '#open_source', '#multimodal', '#robotics', '#agents', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ° Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² 3D Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ORS3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ² 3D Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ORS3D-60K Ñ 60 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 4 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GRANT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ LLM Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ scheduling-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ GRANT Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°, 3D Ğ³Ñ€ounding Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Optimizing Task Scheduling in 3D with Language Understanding', 'desc': 'The paper introduces ORS3D, a novel task that combines language understanding, 3D spatial grounding, and efficient scheduling for embodied AI. It highlights the importance of operations research in optimizing task management, allowing agents to execute multiple actions simultaneously in a 3D environment. The authors present a large dataset, ORS3D-60K, which includes 60,000 complex tasks set in 4,000 real-world scenes to support research in this area. Additionally, they propose GRANT, a multi-modal model that utilizes a scheduling token mechanism to enhance task scheduling and action execution efficiency.'}, 'zh': {'title': 'é«˜æ•ˆè°ƒåº¦ï¼Œæ™ºèƒ½ä½“çš„æ–°æŒ‘æˆ˜', 'desc': 'ORS3Dæ˜¯ä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œç»“åˆäº†è¯­è¨€ç†è§£ã€3Då®šä½å’Œé«˜æ•ˆè°ƒåº¦ã€‚å®ƒè¦æ±‚æ™ºèƒ½ä½“åœ¨æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªå­ä»»åŠ¡ï¼Œä»¥å‡å°‘æ€»å®Œæˆæ—¶é—´ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«60,000ä¸ªå¤åˆä»»åŠ¡çš„å¤§å‹æ•°æ®é›†ORS3D-60Kï¼Œå¹¶æå‡ºäº†GRANTæ¨¡å‹ï¼Œåˆ©ç”¨è°ƒåº¦ä»¤ç‰Œæœºåˆ¶æ¥ä¼˜åŒ–ä»»åŠ¡ç®¡ç†ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒéªŒè¯äº†GRANTåœ¨è¯­è¨€ç†è§£ã€3Då®šä½å’Œè°ƒåº¦æ•ˆç‡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18734', 'title': "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion", 'url': 'https://huggingface.co/papers/2511.18734', 'abstract': 'Yo\'City is an agentic framework that uses off-the-shelf large models to generate user-customized, infinitely expandable 3D city scenes with spatial coherence and high quality across multiple evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo\'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo\'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo\'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo\'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.', 'score': 6, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '43c282d91845e1d7', 'authors': ['Keyang Lu', 'Sifan Zhou', 'Hongbin Xu', 'Gang Xu', 'Zhifei Yang', 'Yikai Wang', 'Zhen Xiao', 'Jieyi Long', 'Ming Li'], 'affiliations': ['Beihang University', 'Beijing Normal University', 'ByteDance Seed', 'Guangming Laboratory', 'Peking University', 'Southeast University', 'Theta Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18734.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#diffusion', '#benchmark', '#graphs', '#multimodal', '#agents', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²', 'desc': "Yo'City â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ĞµÑ€Ñ…Ñƒ Ğ²Ğ½Ğ¸Ğ·, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ (Ğ³Ğ¾Ñ€Ğ¾Ğ´-Ñ€Ğ°Ğ¹Ğ¾Ğ½-ÑĞµÑ‚ĞºĞ°), Ğ³Ğ´Ğµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ, Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ€Ğ°Ğ¹Ğ¾Ğ½. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ†Ğ¸ĞºĞ» Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-ÑĞ¸Ğ½Ñ‚ĞµĞ·-Ñ€ĞµÑ„Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸."}, 'en': {'title': "Endless Custom 3D Cities with Yo'City!", 'desc': "Yo'City is an innovative framework that utilizes large pre-trained models to create customizable and endlessly expandable 3D city environments. It employs a hierarchical structure for city planning, allowing for both global layout and detailed local designs. The framework incorporates a unique synthesis loop for generating 3D scenes, ensuring high quality and spatial coherence. Additionally, it features an interactive expansion mechanism that optimizes city growth based on user input and semantic relationships, outperforming existing methods in various evaluation metrics."}, 'zh': {'title': 'æ— é™æ‰©å±•çš„ä¸ªæ€§åŒ–3DåŸå¸‚ç”Ÿæˆ', 'desc': "Yo'Cityæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç°æˆçš„å¤§å‹æ¨¡å‹ç”Ÿæˆç”¨æˆ·å®šåˆ¶çš„ã€æ— é™æ‰©å±•çš„3DåŸå¸‚åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªä¸Šè€Œä¸‹çš„è§„åˆ’ç­–ç•¥ï¼Œæ„å»ºäº†ä¸€ä¸ªåˆ†å±‚çš„â€œåŸå¸‚-åŒºåŸŸ-ç½‘æ ¼â€ç»“æ„ï¼Œä»¥ç¡®ä¿ç©ºé—´ä¸€è‡´æ€§å’Œé«˜è´¨é‡ã€‚é€šè¿‡â€œç”Ÿäº§-ç²¾ç‚¼-è¯„ä¼°â€çš„å¾ªç¯ï¼ŒYo'Cityå®ç°äº†ç½‘æ ¼çº§åˆ«çš„3Dç”Ÿæˆï¼Œå¹¶å¼•å…¥äº†ç”¨æˆ·äº¤äº’çš„æ‰©å±•æœºåˆ¶ï¼Œæ¨¡æ‹ŸåŸå¸‚çš„æŒç»­æ¼”å˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒYo'Cityåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"}}}, {'id': 'https://huggingface.co/papers/2511.20562', 'title': 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding', 'url': 'https://huggingface.co/papers/2511.20562', 'abstract': 'PhysChoreo generates physically realistic and controllable videos from a single image using part-aware physical property reconstruction and temporally instructed simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '7d510a167d3700ed', 'authors': ['Haoze Zhang', 'Tianyu Huang', 'Zichen Wan', 'Xiaowei Jin', 'Hongzhi Zhang', 'Hui Li', 'Wangmeng Zuo'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20562.jpg', 'data': {'categories': ['#video', '#diffusion', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ„Ğ¾Ñ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ', 'desc': 'PhysChoreo â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Generate Realistic Videos from a Single Image with PhysChoreo!', 'desc': 'PhysChoreo is a new framework that creates realistic videos from just one image by understanding the physical properties of objects in that image. It first analyzes the image to identify and reconstruct the physical characteristics of each part, ensuring accurate representation. Then, it uses a simulation that allows for temporal control, meaning users can instruct how the video should change over time while maintaining physical realism. This approach leads to videos that not only look good but also behave in a believable way, surpassing existing methods in quality and control.'}, 'zh': {'title': 'ä»å•å›¾åƒç”Ÿæˆå¯æ§çš„ç‰©ç†çœŸå®è§†é¢‘', 'desc': 'PhysChoreo æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆå…·æœ‰ç‰©ç†çœŸå®æ„Ÿå’Œå¯æ§æ€§çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡éƒ¨ä»¶æ„ŸçŸ¥çš„ç‰©ç†å±æ€§é‡å»ºï¼Œä¼°è®¡å›¾åƒä¸­æ‰€æœ‰ç‰©ä½“çš„é™æ€åˆå§‹ç‰©ç†å±æ€§ã€‚ç„¶åï¼Œé€šè¿‡æ—¶é—´æŒ‡å¯¼å’Œç‰©ç†å¯ç¼–è¾‘çš„ä»¿çœŸï¼Œåˆæˆå…·æœ‰ä¸°å¯ŒåŠ¨æ€è¡Œä¸ºå’Œç‰©ç†çœŸå®æ„Ÿçš„é«˜è´¨é‡è§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhysChoreo åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸°å¯Œçš„è¡Œä¸ºå’Œç‰©ç†çœŸå®æ„Ÿçš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20250', 'title': 'Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation', 'url': 'https://huggingface.co/papers/2511.20250', 'abstract': 'A two-stage pipeline with a front-end perception task and a back-end 2D-to-3D uplifting task is proposed for accurate 3D motion analysis of a table tennis ball using monocular video.  \t\t\t\t\tAI-generated summary \t\t\t\t Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'f936536ad7c43476', 'authors': ['Daniel Kienzle', 'Katja Ludwig', 'Julian Lorenz', "Shin'ichi Satoh", 'Rainer Lienhart'], 'affiliations': ['National Institute of Informatics', 'University of Augsburg', 'University of Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20250.jpg', 'data': {'categories': ['#video', '#cv', '#dataset', '#synthetic', '#3d'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞÑ‚ 2D Ğº 3D: Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ½Ğ½Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¼ÑÑ‡Ğ° Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ½Ğ½Ğ¸ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¼ÑÑ‡Ğ° Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ¤Ñ€Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° 2D Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞµ Ğ¸Ğ· Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° TTHQ, Ğ° Ğ±ÑĞºĞµĞ½Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ° Ğ¼ÑÑ‡Ğ° Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº ÑÑ‚Ğ¾Ğ»Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ¾Ğ½ĞµÑ†-Ğ²-ĞºĞ¾Ğ½ĞµÑ† Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming 2D Video into 3D Table Tennis Insights', 'desc': 'This paper presents a two-stage pipeline designed for accurately analyzing the 3D motion of a table tennis ball using monocular video. The first stage focuses on perception tasks, utilizing a newly created dataset to train models on 2D data, while the second stage involves uplifting 2D detections to 3D trajectories using synthetic data. The approach addresses challenges such as noise and inaccuracies in real-world video by re-engineering the uplifting model to handle common artifacts like missing detections. Ultimately, this method combines ball and table detection to create a robust application for analyzing table tennis trajectories and spins.'}, 'zh': {'title': 'ç²¾å‡†ä¹’ä¹“çƒä¸‰ç»´è¿åŠ¨åˆ†æçš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„ç®¡é“ï¼Œç”¨äºé€šè¿‡å•ç›®è§†é¢‘å‡†ç¡®åˆ†æä¹’ä¹“çƒçš„ä¸‰ç»´è¿åŠ¨ã€‚è¯¥æ–¹æ³•å°†é—®é¢˜åˆ†ä¸ºå‰ç«¯æ„ŸçŸ¥ä»»åŠ¡å’Œåç«¯2Dåˆ°3Dæå‡ä»»åŠ¡ï¼Œä»¥åº”å¯¹ç°å®ä¸–ç•Œä¸­çƒå’Œæ¡Œå­çš„æ£€æµ‹å™ªå£°å’Œä¸å®Œç¾æ€§ã€‚å‰ç«¯ç»„ä»¶åˆ©ç”¨æ–°åˆ›å»ºçš„TTHQæ•°æ®é›†è¿›è¡Œä¸°å¯Œçš„2Dç›‘ç£è®­ç»ƒï¼Œè€Œåç«¯æå‡ç½‘ç»œåˆ™ä¸“æ³¨äºç‰©ç†æ­£ç¡®çš„åˆæˆæ•°æ®è®­ç»ƒã€‚é€šè¿‡é›†æˆçƒæ£€æµ‹å™¨å’Œæ¡Œå­å…³é”®ç‚¹æ£€æµ‹å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ¦‚å¿µéªŒè¯çš„æå‡æ–¹æ³•è½¬å˜ä¸ºä¸€ä¸ªå®ç”¨ã€ç¨³å¥ä¸”é«˜æ•ˆçš„ç«¯åˆ°ç«¯åº”ç”¨ï¼Œèƒ½å¤Ÿè¿›è¡Œä¹’ä¹“çƒè½¨è¿¹å’Œæ—‹è½¬åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19111', 'title': 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection', 'url': 'https://huggingface.co/papers/2511.19111', 'abstract': 'DiffSeg30k, a dataset of 30k diffusion-edited images, supports fine-grained detection of AI-generated content through semantic segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '2cd544e67cecf15f', 'authors': ['Hai Ci', 'Ziheng Peng', 'Pei Yang', 'Yingxin Xuan', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'South China University of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19111.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#benchmark', '#security', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DiffSeg30k â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 30 Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ²Ğ¾ÑĞµĞ¼ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿ĞµÑ€ĞµĞ½ĞµÑĞµĞ½Ğ° Ğ¸Ğ· ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒ-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºÑ€Ğ¾ÑÑ-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Fine-Grained Detection of AI Edits with DiffSeg30k', 'desc': 'DiffSeg30k is a new dataset containing 30,000 images that have been edited using diffusion techniques, aimed at improving the detection of AI-generated content through semantic segmentation. Unlike previous benchmarks that only classify entire images, this dataset allows for pixel-level annotations, enabling the identification of specific areas that have been altered. It includes a variety of real-world images and utilizes multiple state-of-the-art diffusion models to create realistic editing scenarios. The findings suggest that while segmentation models excel at classifying whole images, they also show promise in accurately localizing edits, thus advancing the field of AI-generated content detection.'}, 'zh': {'title': 'DiffSeg30kï¼šç»†ç²’åº¦æ£€æµ‹AIç”Ÿæˆå†…å®¹çš„æ–°å·¥å…·', 'desc': 'DiffSeg30kæ˜¯ä¸€ä¸ªåŒ…å«3ä¸‡å¼ ç»è¿‡æ‰©æ•£ç¼–è¾‘çš„å›¾åƒçš„æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡è¯­ä¹‰åˆ†å‰²æ”¯æŒå¯¹AIç”Ÿæˆå†…å®¹çš„ç»†ç²’åº¦æ£€æµ‹ã€‚æ‰©æ•£ç¼–è¾‘ä½¿å¾—å±€éƒ¨å›¾åƒåŒºåŸŸçš„ä¿®æ”¹æ›´åŠ çœŸå®ï¼Œå¢åŠ äº†AIç”Ÿæˆå†…å®¹çš„æ£€æµ‹éš¾åº¦ã€‚è¯¥æ•°æ®é›†æä¾›åƒç´ çº§æ³¨é‡Šï¼Œå…è®¸ç ”ç©¶è€…åŒæ—¶å®šä½ç¼–è¾‘åŒºåŸŸå’Œè¯†åˆ«ç¼–è¾‘æ¨¡å‹ã€‚é€šè¿‡å¯¹ä¸‰ç§åŸºçº¿åˆ†å‰²æ–¹æ³•çš„åŸºå‡†æµ‹è¯•ï¼ŒDiffSeg30kå±•ç¤ºäº†åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒå¤±çœŸæ–¹é¢çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20647', 'title': 'Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization', 'url': 'https://huggingface.co/papers/2511.20647', 'abstract': 'A framework combining Determinantal Point Processes and Group Relative Policy Optimization enhances diversity in text-to-video generation without compromising quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '0af8124d75e8298c', 'authors': ['Tahira Kazimi', 'Connor Dunlop', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20647.jpg', 'data': {'categories': ['#video', '#optimization', '#dataset', '#diffusion', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ DPP-GRPO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ (DPP) Ğ´Ğ»Ñ ÑˆÑ‚Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğº Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº plug-and-play ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ, Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ ÑÑ†ĞµĞ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… WAN Ğ¸ CogVideoX Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ VBench Ğ¸ VideoScore Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñƒ.'}, 'en': {'title': 'Enhancing Diversity in Text-to-Video Generation with DPP-GRPO', 'desc': 'This paper presents a new framework called DPP-GRPO that enhances diversity in text-to-video generation while maintaining high quality. It addresses the issue of low diversity in outputs from existing models by treating the problem as a set-level policy optimization task. By integrating Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO), the framework explicitly encourages diverse video outputs by penalizing redundant samples and providing group feedback. The authors demonstrate the effectiveness of their approach on various models and benchmarks, and they also provide a new dataset to facilitate further research in this area.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„å¤šæ ·æ€§ä¸è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚å½“å‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤šä¸ªè§†é¢‘æ—¶ï¼Œå¸¸å¸¸è¾“å‡ºä½å¤šæ ·æ€§çš„ç»“æœã€‚æˆ‘ä»¬å°†è¿™ä¸€æŒ‘æˆ˜è§†ä¸ºä¸€ä¸ªé›†åˆçº§ç­–ç•¥ä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿè¦†ç›–ç»™å®šæç¤ºçš„å¤šç§å¯èƒ½ç»“æœçš„ç­–ç•¥ã€‚é€šè¿‡å¼•å…¥DPP-GRPOæ¡†æ¶ï¼Œæˆ‘ä»¬æ˜ç¡®äº†å¤šæ ·æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²æç¤ºä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡çš„æƒ…å†µä¸‹ï¼Œé¼“åŠ±åœ¨è§†è§‰å¤–è§‚ã€æ‘„åƒæœºè¿åŠ¨å’Œåœºæ™¯ç»“æ„ä¸Šçš„å¤šæ ·ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17943', 'title': 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System', 'url': 'https://huggingface.co/papers/2511.17943', 'abstract': 'SciEducator, an iterative self-evolving multi-agent system, enhances scientific video understanding and education by integrating professional knowledge and step-wise reasoning, outperforming existing models on a new benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '83cb791539f112c1', 'authors': ['Zhiyu Xu', 'Weilong Yan', 'Yufei Shi', 'Xin Meng', 'Tao He', 'Huiping Zhuang', 'Ming Li', 'Hehe Fan'], 'affiliations': ['Guangming Laboratory', 'Jinan University', 'Nanyang Technological University', 'National University of Singapore', 'Peking University', 'South China University of Technology', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17943.jpg', 'data': {'categories': ['#video', '#dataset', '#reasoning', '#science', '#benchmark', '#open_source', '#multimodal', '#agents'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'SciEducator â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„Ğ¸Ñ Ñ†Ğ¸ĞºĞ»Ğ° Ğ”ĞµĞ¼Ğ¸Ğ½Ğ³Ğ° (Plan-Do-Study-Act) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ. SciEducator Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹, Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¼ĞºĞ½ÑƒÑ‚Ñ‹Ğµ MLLMs (Gemini, GPT-4o) Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ SciVBench Ğ¸Ğ· 500 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Scientific Video Education with SciEducator', 'desc': 'SciEducator is a novel multi-agent system designed to enhance the understanding and education of scientific videos by integrating expert knowledge and structured reasoning. It utilizes an iterative self-evolving mechanism based on the Deming Cycle, allowing it to continuously improve its reasoning and feedback processes. This system generates multimodal educational content, including text, visuals, and audio, tailored to specific scientific topics. In evaluations against existing models, SciEducator demonstrates superior performance on the newly created SciVBench benchmark, setting a new standard in scientific video comprehension.'}, 'zh': {'title': 'SciEducatorï¼šç§‘å­¦è§†é¢‘ç†è§£çš„æ–°èŒƒå¼', 'desc': 'SciEducatoræ˜¯ä¸€ä¸ªè¿­ä»£è‡ªæˆ‘è¿›åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡ç§‘å­¦è§†é¢‘çš„ç†è§£å’Œæ•™è‚²ã€‚å®ƒé€šè¿‡æ•´åˆä¸“ä¸šçŸ¥è¯†å’Œé€æ­¥æ¨ç†ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åœ¨ç§‘å­¦è§†é¢‘ç†è§£ä¸­çš„ä¸è¶³ã€‚è¯¥ç³»ç»ŸåŸºäºç»å…¸çš„å¾·æ˜å¾ªç¯ï¼Œé‡‡ç”¨è‡ªæˆ‘è¿›åŒ–çš„æ¨ç†å’Œåé¦ˆæœºåˆ¶ï¼Œèƒ½å¤Ÿè§£é‡Šå¤æ‚çš„ç§‘å­¦æ´»åŠ¨ã€‚SciEducatorè¿˜å¯ä»¥ç”Ÿæˆå¤šæ¨¡æ€çš„æ•™è‚²å†…å®¹ï¼Œæ”¯æŒç§‘å­¦è¿‡ç¨‹çš„å­¦ä¹ å’Œç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15906', 'title': 'Unified all-atom molecule generation with neural fields', 'url': 'https://huggingface.co/papers/2511.15906', 'abstract': 'FuncBind, a framework using neural fields and score-based generative models from computer vision, generates diverse atomic structures across modalities, achieving competitive performance in structure-conditioned molecular design.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'beedb7b629e404c5', 'authors': ['Matthieu Kirchmeyer', 'Pedro O. Pinheiro', 'Emma Willett', 'Karolis Martinkus', 'Joseph Kleinhenz', 'Emily K. Makowski', 'Andrew M. Watkins', 'Vladimir Gligorijevic', 'Richard Bonneau', 'Saeed Saremi'], 'affiliations': ['Antibody Engineering, Genentech', 'Prescient Design, Genentech'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15906.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#science', '#benchmark', '#healthcare', '#open_source', '#multimodal'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'FuncBind â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» ĞºĞ°Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… â€” Ğ¾Ñ‚ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ±ĞµĞ»ĞºĞ¾Ğ² Ğ¸ Ğ°Ğ½Ñ‚Ğ¸Ñ‚ĞµĞ» â€” Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ². FuncBind Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¼Ğ°ĞºÑ€Ğ¾Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ‚ĞµĞ»ÑŒ Ğ°Ğ½Ñ‚Ğ¸Ñ‚ĞµĞ», Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ±ĞµĞ»ĞºĞ°.'}, 'en': {'title': 'FuncBind: Unifying Molecular Design Across Modalities', 'desc': 'FuncBind is a novel framework that leverages neural fields and score-based generative models to create diverse molecular structures across different types of atomic systems. By representing molecules as continuous atomic densities, it allows for a flexible and unified approach to molecular design, overcoming limitations of modality-specific generative models. The framework demonstrates strong performance in generating various molecular types, including small molecules and macrocyclic peptides, conditioned on specific target structures. Additionally, FuncBind has successfully produced new antibody binders through the redesign of specific molecular loops, contributing to advancements in structure-based drug design.'}, 'zh': {'title': 'FuncBindï¼šè·¨æ¨¡æ€çš„åˆ†å­è®¾è®¡æ–°æ¡†æ¶', 'desc': 'FuncBindæ˜¯ä¸€ä¸ªåŸºäºè®¡ç®—æœºè§†è§‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç¥ç»åœºå’ŒåŸºäºè¯„åˆ†çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šç§åŸå­ç»“æ„ï¼Œé€‚ç”¨äºç»“æ„æ¡ä»¶çš„åˆ†å­è®¾è®¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†åˆ†å­è¡¨ç¤ºä¸ºè¿ç»­çš„åŸå­å¯†åº¦ï¼Œå…‹æœäº†ä¼ ç»Ÿç”Ÿæˆæ¨¡å‹åœ¨ç‰¹å®šæ¨¡æ€ä¸Šçš„å±€é™æ€§ã€‚FuncBindå¯ä»¥å¤„ç†ä¸åŒå¤§å°çš„åˆ†å­å’Œå¯å˜çš„åŸå­/æ®‹åŸºæ•°é‡ï¼ŒåŒ…æ‹¬éå…¸å‹æ°¨åŸºé…¸ï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚å®ƒåœ¨ç”Ÿæˆå°åˆ†å­ã€å®ç¯è‚½å’ŒæŠ—ä½“äº’è¡¥å†³å®šåŒºç¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡é‡æ–°è®¾è®¡ç‰¹å®šç»“æ„ç”Ÿæˆæ–°çš„æŠ—ä½“ç»“åˆç‰©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20643', 'title': 'Concept-Aware Batch Sampling Improves Language-Image Pretraining', 'url': 'https://huggingface.co/papers/2511.20643', 'abstract': 'Concept-Aware Batch Sampling (CABS) improves vision-language model performance by flexibly curating training data based on specific concept distributions.  \t\t\t\t\tAI-generated summary \t\t\t\t What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'd7e6295b44753e51', 'authors': ['Adhiraj Ghosh', 'Vishaal Udandarao', 'Thao Nguyen', 'Matteo Farina', 'Mehdi Cherti', 'Jenia Jitsev', 'Sewoong Oh', 'Elisa Ricci', 'Ludwig Schmidt', 'Matthias Bethge'], 'affiliations': ['LAION', 'Stanford University', 'Tubingen AI Center, University of Tubingen', 'University of Cambridge', 'University of Trento', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20643.jpg', 'data': {'categories': ['#cv', '#optimization', '#dataset', '#open_source', '#multimodal', '#training', '#data'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Concept-Aware Batch Sampling (CABS) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, CABS Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ñ‚Ñ‡Ğ¸ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ DataConcept â€” Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 128 Ğ¼Ğ»Ğ½ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ², Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 28 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CABS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CLIP/SigLIP Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Dynamic Data Curation for Enhanced Model Performance', 'desc': 'This paper introduces Concept-Aware Batch Sampling (CABS), a method that enhances the performance of vision-language models by dynamically curating training data based on specific concept distributions. Unlike traditional offline methods that create static datasets, CABS allows for online, task-adaptive data selection, which reduces biases associated with concept-agnostic filtering. The authors present DataConcept, a large dataset of image-text pairs with detailed concept annotations, which serves as the foundation for CABS. By offering two sampling strategiesâ€”Diversity Maximization and Frequency Maximizationâ€”CABS enables the creation of training batches that either cover a wide range of concepts or focus on frequently occurring objects, leading to improved model performance across various benchmarks.'}, 'zh': {'title': 'çµæ´»çš„æ¦‚å¿µæ„ŸçŸ¥æ‰¹é‡é‡‡æ ·æå‡æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¦‚å¿µæ„ŸçŸ¥æ‰¹é‡é‡‡æ ·ï¼ˆCABSï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡çµæ´»åœ°æ ¹æ®ç‰¹å®šæ¦‚å¿µåˆ†å¸ƒæ¥ä¼˜åŒ–è§†è§‰-è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿ã€æ¦‚å¿µæ— å…³çš„æ•°æ®ç­›é€‰æ–¹æ³•ä¸åŒï¼ŒCABSèƒ½å¤Ÿåœ¨çº¿é€‚åº”ä»»åŠ¡éœ€æ±‚ï¼ŒåŠ¨æ€æ„å»ºè®­ç»ƒæ‰¹æ¬¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«1.28äº¿å¯¹å›¾åƒ-æ–‡æœ¬çš„DataConceptæ•°æ®é›†ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†CABSæ¡†æ¶ï¼Œæä¾›äº†å¤šæ ·æ€§æœ€å¤§åŒ–å’Œé¢‘ç‡æœ€å¤§åŒ–ä¸¤ç§å˜ä½“ã€‚é€šè¿‡åœ¨28ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†CABSæ–¹æ³•æ˜¾è‘—æå‡äº†CLIP/SigLIPæ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18394', 'title': "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking", 'url': 'https://huggingface.co/papers/2511.18394', 'abstract': 'Forecasting performance of Large Language Models varies significantly across different domains and question types, influenced by context and external knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': 'fcc9c73d81cec52e', 'authors': ['Chinmay Karkar', 'Paras Chopra'], 'affiliations': ['Lossfunk'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18394.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸ ĞºĞ°Ğº Ğ¼Ñ‹ ÑĞ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹, Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞµĞ´ÑˆĞ¸Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµÑÑŒĞ¼Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ° Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°.'}, 'en': {'title': 'Forecasting with LLMs: Context Matters!', 'desc': "This paper examines how well Large Language Models (LLMs) can predict events in various fields like social, political, and economic domains. It highlights that the accuracy of these predictions is influenced by the structure of the domain and the way questions are framed. The study analyzes the impact of context, question types, and external knowledge on the models' forecasting performance, especially for events that occurred after the model's training cutoff. The findings reveal that the effectiveness of LLMs in forecasting is inconsistent and heavily reliant on the specifics of the inquiry."}, 'zh': {'title': 'é¢„æµ‹èƒ½åŠ›å—ä¸Šä¸‹æ–‡å’Œæé—®æ–¹å¼å½±å“', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒé¢†åŸŸå’Œé—®é¢˜ç±»å‹ä¸Šçš„é¢„æµ‹æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå—åˆ°ä¸Šä¸‹æ–‡å’Œå¤–éƒ¨çŸ¥è¯†çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsåœ¨ç¤¾ä¼šã€æ”¿æ²»å’Œç»æµäº‹ä»¶çš„é¢„æµ‹èƒ½åŠ›ä¸Šè¡¨ç°å‡ºä¸€å®šçš„èƒ½åŠ›ï¼Œä½†å…¶å‡†ç¡®æ€§ä¸é¢†åŸŸç»“æ„å’Œæç¤ºæ¡†æ¶å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œé—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯é‚£äº›å‘ç”Ÿåœ¨æ¨¡å‹æˆªæ­¢æ—¥æœŸä¹‹åçš„äº‹ä»¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œé¢„æµ‹èƒ½åŠ›é«˜åº¦å¯å˜ï¼Œå–å†³äºæˆ‘ä»¬æé—®çš„å†…å®¹å’Œæ–¹å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20639', 'title': 'Latent Collaboration in Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2511.20639', 'abstract': "LatentMAS enables efficient, lossless collaboration among LLM agents in latent space, improving performance and reducing computational costs compared to text-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", 'score': 113, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '4d21683d87445edd', 'authors': ['Jiaru Zou', 'Xiyuan Yang', 'Ruizhong Qiu', 'Gaotang Li', 'Katherine Tieu', 'Pan Lu', 'Ke Shen', 'Hanghang Tong', 'Yejin Choi', 'Jingrui He', 'James Zou', 'Mengdi Wang', 'Ling Yang'], 'affiliations': ['Princeton University', 'Stanford University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20639.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#open_source', '#inference', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ° Ğ½Ğµ ÑĞ»Ğ¾Ğ²', 'desc': 'LatentMAS â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ LLM Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¸ Ğ´ĞµĞ»Ğ¸Ñ‚ÑÑ Ğ¸Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµÑÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ñ‡ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LatentMAS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ° 70-80% ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Collaborative Intelligence in Latent Space', 'desc': "LatentMAS is a novel framework that allows large language model (LLM) agents to collaborate directly in latent space, bypassing the need for text-based communication. This method enhances performance by enabling lossless information exchange through a shared latent working memory, which retains each agent's internal representations. The framework is designed to be training-free, resulting in significant reductions in computational costs and faster inference times. Empirical results show that LatentMAS outperforms traditional text-based multi-agent systems in various reasoning tasks, achieving higher accuracy and efficiency."}, 'zh': {'title': 'æ½œåœ¨ç©ºé—´ä¸­çš„é«˜æ•ˆåä½œ', 'desc': 'LatentMASæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ½œåœ¨ç©ºé—´ä¸­é«˜æ•ˆã€æ— æŸåœ°åä½œã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„æ–¹æ³•ç›¸æ¯”ï¼ŒLatentMASé€šè¿‡ç›´æ¥åœ¨è¿ç»­çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåä½œï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ¯ä¸ªæ™ºèƒ½ä½“é€šè¿‡æœ€åä¸€å±‚çš„éšè—åµŒå…¥ç”Ÿæˆè‡ªå›å½’çš„æ½œåœ¨æ€ç»´ï¼Œå¹¶é€šè¿‡å…±äº«çš„æ½œåœ¨å·¥ä½œè®°å¿†è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œç¡®ä¿ä¿¡æ¯çš„æ— æŸä¼ é€’ã€‚å®éªŒè¯æ˜ï¼ŒLatentMASåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æé«˜äº†14.6%ï¼Œè¾“å‡ºä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†70.8%-83.7%ï¼Œæ¨ç†é€Ÿåº¦æå‡äº†4åˆ°4.3å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.15552', 'title': 'Multimodal Evaluation of Russian-language Architectures', 'url': 'https://huggingface.co/papers/2511.15552', 'abstract': 'Mera Multi is an open multimodal evaluation framework for Russian-spoken architectures, addressing the lack of such benchmarks with 18 newly constructed tasks and a methodology to prevent benchmark leakage.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.', 'score': 78, 'issue_id': 1, 'pub_date': '2025-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '4ceba7a439e57d51', 'authors': ['Artem Chervyakov', 'Ulyana Isaeva', 'Anton Emelyanov', 'Artem Safin', 'Maria Tikhonova', 'Alexander Kharitonov', 'Yulia Lyakh', 'Petr Surovtsev', 'Denis Shevelev', 'Vildan Saburov', 'Vasily Konovalov', 'Elisei Rykov', 'Ivan Sviridov', 'Amina Miftakhova', 'Ilseyar Alimova', 'Alexander Panchenko', 'Alexander Kapitanov', 'Alena Fenogenova'], 'affiliations': ['MERA Team'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.15552.jpg', 'data': {'categories': ['#video', '#audio', '#dataset', '#benchmark', '#multilingual', '#low_resource', '#open_source', '#multimodal', '#survey', '#leakage'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‚ Ğ¿Ğ¾-Ñ€ÑƒÑÑĞºĞ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Mera Multi â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 18 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ»Ğ°Ğ²ÑĞ½ÑĞºĞ¸Ñ… Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ….'}, 'en': {'title': 'Mera Multi: Pioneering Multimodal Evaluation for Russian Language Models', 'desc': 'Mera Multi is a new evaluation framework designed for multimodal large language models (MLLMs) specifically for the Russian language. It introduces 18 unique tasks that assess various modalities such as text, image, audio, and video, addressing the absence of benchmarks in this area. The framework includes a universal taxonomy of multimodal abilities and ensures cultural relevance by creating datasets from scratch. Additionally, it implements strategies to prevent benchmark leakage, making it a robust tool for evaluating both general-purpose and specialized models.'}, 'zh': {'title': 'Mera Multiï¼šä¿„è¯­å¤šæ¨¡æ€è¯„ä¼°çš„æ–°åŸºå‡†', 'desc': 'Mera Multiæ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶ï¼Œä¸“ä¸ºä¿„è¯­æ¶æ„è®¾è®¡ï¼Œå¡«è¡¥äº†å½“å‰ç¼ºä¹ç›¸å…³åŸºå‡†çš„ç©ºç™½ã€‚è¯¥æ¡†æ¶åŒ…å«18ä¸ªæ–°æ„å»ºçš„ä»»åŠ¡ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œæ—¨åœ¨è¯„ä¼°é€šç”¨æ¨¡å‹å’Œç‰¹å®šæ¨¡æ€æ¶æ„çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜²æ­¢åŸºå‡†æ³„æ¼çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ°´å°å’Œç§æœ‰æ•°æ®é›†çš„è®¸å¯ã€‚è™½ç„¶ç›®å‰é‡ç‚¹æ˜¯ä¿„è¯­ï¼Œä½†è¯¥åŸºå‡†æä¾›äº†ä¸€ç§å¯å¤åˆ¶çš„æ–¹æ³•è®ºï¼Œé€‚ç”¨äºæ„å»ºå¤šæ¨¡æ€åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–¯æ‹‰å¤«è¯­è¨€å®¶æ—ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20714', 'title': 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation', 'url': 'https://huggingface.co/papers/2511.20714', 'abstract': 'Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.   Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.', 'score': 45, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '61c18079bdec76a4', 'authors': ['Inferix Team', 'Tianyu Feng', 'Yizeng Han', 'Jiahao He', 'Yuanyu He', 'Xi Lin', 'Teng Liu', 'Hanfeng Lu', 'Jiasheng Tang', 'Wei Wang', 'Zhiyuan Wang', 'Jichao Wu', 'Mingyang Yang', 'Yinghao Yu', 'Zeyu Zhang', 'Bohan Zhuang'], 'affiliations': ['Alibaba DAMO Academy'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20714.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#benchmark', '#open_source', '#robotics', '#inference', '#agents', '#games'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸĞ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'Inferix â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ğ¾Ğ»ÑƒĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ KV Cache, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ¼Ğ¼ĞµÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Inferix Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° world models Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ AI.'}, 'en': {'title': 'Revolutionizing Video Generation with Inferix', 'desc': 'Inferix is an advanced inference engine that focuses on creating immersive worlds through semi-autoregressive decoding, which combines diffusion and autoregressive techniques for generating high-quality videos in real-time. This approach allows for the generation of coherent and stable video sequences by applying diffusion in blocks while considering previous outputs, thus enhancing the overall quality of the video. The engine is designed to support interactive video streaming, enabling users to engage with the generated content dynamically, which is crucial for applications in gaming and AI. Additionally, Inferix integrates a new benchmarking tool, LV-Bench, to facilitate precise evaluation of video generation performance, promoting further research and development in world modeling.'}, 'zh': {'title': 'æ²‰æµ¸å¼ä¸–ç•Œåˆæˆçš„æœªæ¥å¼•æ“', 'desc': 'Inferix æ˜¯ä¸€ä¸ªæ–°ä¸€ä»£æ¨ç†å¼•æ“ï¼Œä¸“ä¸ºæ²‰æµ¸å¼ä¸–ç•Œåˆæˆè€Œè®¾è®¡ï¼Œé‡‡ç”¨åŠè‡ªå›å½’è§£ç æ–¹æ³•ï¼Œç»“åˆæ‰©æ•£å’Œè‡ªå›å½’æŠ€æœ¯ï¼Œå®ç°é«˜è´¨é‡ã€å®æ—¶çš„è§†é¢‘ç”Ÿæˆå’Œäº¤äº’ã€‚å®ƒçš„ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé•¿æ—¶é—´ã€ç‰©ç†çœŸå®ä¸”äº’åŠ¨æ€§å¼ºçš„é«˜è´¨é‡è§†é¢‘ï¼Œé€‚ç”¨äºä»£ç†äººå·¥æ™ºèƒ½ã€å…·èº«äººå·¥æ™ºèƒ½å’Œæ¸¸æˆç­‰é¢†åŸŸã€‚é€šè¿‡åŠè‡ªå›å½’ï¼ˆå—æ‰©æ•£ï¼‰è§£ç èŒƒå¼ï¼ŒInferix ç”Ÿæˆè§†é¢‘æ ‡è®°æ—¶åœ¨æ¯ä¸ªå—å†…åº”ç”¨æ‰©æ•£ï¼ŒåŒæ—¶ä¾èµ–äºä¹‹å‰çš„å—ï¼Œä»è€Œå®ç°æ›´è¿è´¯å’Œç¨³å®šçš„è§†é¢‘åºåˆ—ã€‚è¯¥ç³»ç»Ÿè¿˜é€šè¿‡å¼•å…¥ LLM é£æ ¼çš„ KV ç¼“å­˜ç®¡ç†ï¼Œå…‹æœäº†æ ‡å‡†è§†é¢‘æ‰©æ•£çš„å±€é™æ€§ï¼Œæ”¯æŒé«˜æ•ˆã€å¯å˜é•¿åº¦å’Œé«˜è´¨é‡çš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21579', 'title': 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy', 'url': 'https://huggingface.co/papers/2511.21579', 'abstract': 'Harmony addresses audio-visual synchronization in generative AI by introducing a Cross-Task Synergy training paradigm, Global-Local Decoupled Interaction Module, and Synchronization-Enhanced CFG to improve alignment and fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.', 'score': 23, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': 'e2ca6c1ac1806851', 'authors': ['Teng Hu', 'Zhentao Yu', 'Guozhen Zhang', 'Zihan Su', 'Zhengguang Zhou', 'Youliang Zhang', 'Yuan Zhou', 'Qinglin Lu', 'Ran Yi'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Hunyuan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21579.jpg', 'data': {'categories': ['#video', '#audio', '#architecture', '#diffusion', '#open_source', '#multimodal', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‘ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Harmony â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ diffusion Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°: Ğ´Ñ€ĞµĞ¹Ñ„ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Classifier-Free Guidance. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Cross-Task Synergy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ´Ñ€ĞµĞ¹Ñ„Ğ°, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Global-Local Decoupled Interaction Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Synchronization-Enhanced CFG Ğ´Ğ»Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Harmony Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Achieving Perfect Harmony in Audio-Visual Synchronization', 'desc': 'The paper presents Harmony, a framework designed to improve audio-visual synchronization in generative AI. It introduces a Cross-Task Synergy training paradigm to address issues like Correspondence Drift, which disrupts stable learning of alignment. Additionally, it features a Global-Local Decoupled Interaction Module that enhances temporal alignment and a Synchronization-Enhanced Classifier-Free Guidance (SyncCFG) to boost alignment signals during inference. The results show that Harmony significantly outperforms existing methods in generating high-fidelity audio-visual content with precise synchronization.'}, 'zh': {'title': 'Harmonyï¼šéŸ³è§†é¢‘åŒæ­¥çš„æ–°çªç ´', 'desc': 'Harmony æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆ AI ä¸­éŸ³è§†é¢‘åŒæ­¥çš„é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥è·¨ä»»åŠ¡ååŒè®­ç»ƒèŒƒå¼ã€å…¨å±€-å±€éƒ¨è§£è€¦äº¤äº’æ¨¡å—å’ŒåŒæ­¥å¢å¼ºçš„æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆSyncCFGï¼‰æ¥æé«˜éŸ³è§†é¢‘çš„å¯¹é½å’Œä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆåœ°ç¼“è§£äº†éŸ³è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯¹åº”æ¼‚ç§»é—®é¢˜ï¼Œå¹¶ä¼˜åŒ–äº†æ—¶é—´æ ·å¼çš„å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHarmony åœ¨ç”Ÿæˆè´¨é‡å’ŒéŸ³è§†é¢‘åŒæ­¥ç²¾åº¦ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20478', 'title': 'NVIDIA Nemotron Parse 1.1', 'url': 'https://huggingface.co/papers/2511.20478', 'abstract': 'Nemotron-Parse-1.1 is a lightweight OCR and document parsing model with improved capabilities in general OCR, markdown formatting, structured table parsing, and text extraction from images, using an encoder-decoder architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.', 'score': 20, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'ca79276545807f1f', 'authors': ['Kateryna Chumachenko', 'Amala Sanjay Deshmukh', 'Jarno Seppanen', 'Ilia Karmanov', 'Chia-Chih Chen', 'Lukas Voegtle', 'Philipp Fischer', 'Marek Wawrzos', 'Saeid Motiian', 'Roman Ageev', 'Kedi Wu', 'Alexandre Milesi', 'Maryam Moosaei', 'Krzysztof Pawelec', 'Padmavathy Subramanian', 'Mehrzad Samadi', 'Xin Yu', 'Celina Dear', 'Sarah Stoddard', 'Jenna Diamond', 'Jesse Oliver', 'Leanna Chraghchian', 'Patrick Skelly', 'Tom Balough', 'Yao Xu', 'Jane Polak Scowcroft', 'Daniel Korzekwa', 'Darragh Hanley', 'Sandip Bhaskar', 'Timo Roman', 'Karan Sapra', 'Andrew Tao', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20478.jpg', 'data': {'categories': ['#cv', '#dataset', '#architecture', '#open_source', '#multimodal', '#small_models', '#inference'], 'emoji': 'ğŸ“„', 'ru': {'title': 'Ğ›Ñ‘Ğ³ĞºĞ¸Ğ¹ OCR Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Nemotron-Parse-1.1 â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ 885 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ OCR, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ markdown, Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ¼ NIM Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Document Parsing with Nemotron-Parse-1.1', 'desc': 'Nemotron-Parse-1.1 is a lightweight model designed for optical character recognition (OCR) and document parsing, enhancing the capabilities of its predecessor. It utilizes an encoder-decoder architecture with 885 million parameters, including a compact language decoder, to improve text extraction from images, markdown formatting, and structured table parsing. The model can handle longer output sequences, making it effective for visually dense documents, and it accurately extracts text segments along with their semantic classes. Released publicly, it includes model weights and an optimized container, along with a variant that offers faster processing with minimal quality loss.'}, 'zh': {'title': 'è½»é‡çº§OCRä¸æ–‡æ¡£è§£æçš„æœªæ¥', 'desc': 'Nemotron-Parse-1.1 æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œæ–‡æ¡£è§£ææ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºçš„åŠŸèƒ½ã€‚å®ƒåœ¨ä¸€èˆ¬OCRã€Markdownæ ¼å¼ã€ç»“æ„åŒ–è¡¨æ ¼è§£æå’Œä»å›¾åƒä¸­æå–æ–‡æœ¬æ–¹é¢éƒ½æœ‰æ‰€æå‡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ‹¥æœ‰885Mä¸ªå‚æ•°ï¼Œå¹¶ä¸”æ”¯æŒæ›´é•¿çš„è¾“å‡ºåºåˆ—ï¼Œé€‚ç”¨äºè§†è§‰å¯†é›†å‹æ–‡æ¡£ã€‚æˆ‘ä»¬å°†æ¨¡å‹æƒé‡å…¬å¼€å‘å¸ƒï¼Œå¹¶æä¾›ä¼˜åŒ–çš„NIMå®¹å™¨å’Œéƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19413', 'title': 'UniGame: Turning a Unified Multimodal Model Into Its Own Adversary', 'url': 'https://huggingface.co/papers/2511.19413', 'abstract': 'UniGame, a self-adversarial post-training framework, improves consistency, understanding, generation, and robustness in unified multimodal models by introducing a lightweight perturber at the shared token interface.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame', 'score': 20, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '80667208115b59fb', 'authors': ['Zhaolong Su', 'Wang Lu', 'Hao Chen', 'Sharon Li', 'Jindong Wang'], 'affiliations': ['Carnegie Mellon University', 'University of Wisconsin Madison', 'William & Mary'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19413.jpg', 'data': {'categories': ['#optimization', '#architecture', '#security', '#open_source', '#multimodal', '#training'], 'emoji': 'âš”ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'UniGame â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ² Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞºĞ°Ñ‚ÑŒ Ğ¸ Ğ¾ÑĞ¿Ğ°Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (+4.6%), Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ (+3.6%), ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ adversarial ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼ (+4.8% Ğ¸ +6.2%). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµĞ½, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 1% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UniGame: Enhancing Multimodal Models through Self-Adversarial Training', 'desc': 'The paper introduces UniGame, a self-adversarial framework designed to enhance the performance of Unified Multimodal Models (UMMs). It addresses the inherent inconsistencies in UMMs, where understanding and generation processes conflict due to differing representation needs. By implementing a lightweight perturber at the shared token interface, UniGame allows the generation component to challenge the understanding component, effectively making the model its own adversary. The results show significant improvements in consistency, understanding, generation, and robustness against adversarial attacks, making UniGame a valuable addition to multimodal model training.'}, 'zh': {'title': 'è‡ªå¯¹æŠ—æ¡†æ¶æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ä¸€è‡´æ€§ä¸é²æ£’æ€§', 'desc': 'UniGameæ˜¯ä¸€ç§è‡ªå¯¹æŠ—çš„åè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„ä¸€è‡´æ€§ã€ç†è§£èƒ½åŠ›ã€ç”Ÿæˆèƒ½åŠ›å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å…±äº«æ ‡è®°æ¥å£å¼•å…¥è½»é‡çº§æ‰°åŠ¨å™¨ï¼Œè§£å†³äº†ç†è§£ä¸ç”Ÿæˆä¹‹é—´çš„åŸºæœ¬ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniGameåœ¨ä¸€è‡´æ€§æ–¹é¢æé«˜äº†4.6%ï¼Œåœ¨ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ–¹æ³•å¯¹æ¨¡å‹æ¶æ„æ²¡æœ‰ä¾èµ–ï¼Œå¢åŠ çš„å‚æ•°ä¸åˆ°1%ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•äº’è¡¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21692', 'title': "Revisiting Generalization Across Difficulty Levels: It's Not So Easy", 'url': 'https://huggingface.co/papers/2511.21692', 'abstract': "LLMs do not consistently generalize across different task difficulties, indicating the need for a broad range of difficulty levels in both training and evaluation datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.", 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '004255e7cddfed90', 'authors': ['Yeganeh Kordi', 'Nihal V. Nayak', 'Max Zuo', 'Ilana Nguyen', 'Stephen H. Bach'], 'affiliations': ['Brown University', 'Harvard University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21692.jpg', 'data': {'categories': ['#training', '#dataset', '#data', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ â€” Ğ·Ğ°Ğ»Ğ¾Ğ³ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ (IRT) Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ñ‚Ñ‹ÑÑÑ‡ LLM Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑˆĞµÑÑ‚Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹. Ğ’Ñ‹ÑÑĞ½Ğ¸Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Diversity in Difficulty: Key to LLM Generalization', 'desc': 'This paper examines how well large language models (LLMs) perform on tasks of varying difficulty levels. It highlights that LLMs do not consistently generalize their learning when faced with different task difficulties, which suggests that training and evaluation datasets should include a wide range of difficulties. The authors use Item Response Theory (IRT) to objectively rank example difficulties based on the performance of multiple LLMs, rather than relying on human assessments. The findings indicate that training on only easy or hard data does not lead to reliable improvements across all difficulty levels, emphasizing the need for diverse training data.'}, 'zh': {'title': 'å¤šæ ·åŒ–éš¾åº¦æ˜¯LLMsè®­ç»ƒä¸è¯„ä¼°çš„å…³é”®', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒä»»åŠ¡éš¾åº¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™æ˜¯æœ‰æ•ˆæ•°æ®ç­–åˆ’å’Œè¯„ä¼°çš„å…³é”®é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿè¯„ä¼°ä¸åŒæ¨¡å‹ã€æ•°æ®é›†å’Œç¤ºä¾‹éš¾åº¦çš„ç»†åˆ†ç»„ï¼Œæ¥åˆ†æLLMsçš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè·¨éš¾åº¦çš„æ³›åŒ–é€šå¸¸æœ‰é™ï¼Œè®­ç»ƒåœ¨ç®€å•æˆ–å›°éš¾çš„æ•°æ®ä¸Šæ— æ³•åœ¨æ•´ä¸ªéš¾åº¦èŒƒå›´å†…å®ç°ä¸€è‡´çš„æ”¹è¿›ã€‚ç»“æœå¼ºè°ƒäº†åœ¨LLMsçš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä¸­åŒ…å«å¤šæ ·åŒ–éš¾åº¦çš„é‡è¦æ€§ï¼Œå¿½è§†éš¾åº¦å¯èƒ½ä¼šå¸¦æ¥é£é™©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21395', 'title': 'Monet: Reasoning in Latent Visual Space Beyond Images and Language', 'url': 'https://huggingface.co/papers/2511.21395', 'abstract': 'Monet, a training framework, enables MLLMs to reason in latent visual space using continuous embeddings, addressing challenges like computational cost and supervision, and outperforms on visual reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t "Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '464473ef8a18992a', 'authors': ['Qixun Wang', 'Yang Shi', 'Yifei Wang', 'Yuanxing Zhang', 'Pengfei Wan', 'Kun Gai', 'Xianghua Ying', 'Yisen Wang'], 'affiliations': ['Amazon AGI SF Lab', 'Kling Team', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21395.jpg', 'data': {'categories': ['#optimization', '#dataset', '#reasoning', '#benchmark', '#rlhf', '#open_source', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'Monet â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ VLPO (Visual-latent Policy Optimization) â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Monet-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering MLLMs to Think Visually with Monet', 'desc': 'Monet is a new training framework designed for multimodal large language models (MLLMs) to enhance their ability to reason in visual contexts. It generates continuous embeddings that serve as intermediate visual thoughts, allowing models to think more like humans when interpreting images. The framework addresses significant challenges such as high computational costs and the need for better supervision in training. By introducing a three-stage fine-tuning process and a novel reinforcement learning method called VLPO, Monet achieves superior performance on visual reasoning tasks and demonstrates strong generalization capabilities.'}, 'zh': {'title': 'åœ¨æ½œåœ¨è§†è§‰ç©ºé—´ä¸­æ¨ç†çš„é©å‘½æ€§æ¡†æ¶', 'desc': 'Monetæ˜¯ä¸€ä¸ªè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ½œåœ¨è§†è§‰ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ã€‚å®ƒé€šè¿‡ç”Ÿæˆè¿ç»­åµŒå…¥ä½œä¸ºä¸­é—´è§†è§‰æ€ç»´ï¼Œè§£å†³äº†è®¡ç®—æˆæœ¬å’Œç›‘ç£ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µè’¸é¦çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æµç¨‹ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è§†è§‰æ½œåœ¨ç­–ç•¥ä¼˜åŒ–ï¼ˆVLPOï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†æ½œåœ¨åµŒå…¥çº³å…¥ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19797', 'title': 'Terminal Velocity Matching', 'url': 'https://huggingface.co/papers/2511.19797', 'abstract': 'Terminal Velocity Matching (TVM) generalizes flow matching for high-fidelity generative modeling, achieving state-of-the-art performance on ImageNet with minimal computational steps.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.', 'score': 11, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': 'a473e95b6854acb0', 'authors': ['Linqi Zhou', 'Mathias Parger', 'Ayaan Haque', 'Jiaming Song'], 'affiliations': ['Luma AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19797.jpg', 'data': {'categories': ['#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ: Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ ÑˆĞ°Ğ³Ğ¾Ğ²', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Terminal Velocity Matching (TVM) â€” Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° flow matching, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ›Ğ¸Ğ¿ÑˆĞ¸Ñ†ĞµĞ²Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Diffusion Transformers Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. ĞĞ° ImageNet TVM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°: 3.29 FID Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¸ 1.99 FID Ğ·Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ 256x256.'}, 'en': {'title': 'Revolutionizing Generative Modeling with Terminal Velocity Matching', 'desc': "Terminal Velocity Matching (TVM) is a new method that improves generative modeling by generalizing flow matching techniques. It focuses on the transition between diffusion timesteps and optimizes the model's behavior at the end of the process instead of the beginning. The paper shows that TVM can effectively reduce the distance between real and generated data distributions, even when using models that are not Lipschitz continuous. By introducing efficient architectural changes and a specialized attention kernel, TVM achieves impressive performance on ImageNet with minimal computational steps."}, 'zh': {'title': 'ç»ˆç«¯é€Ÿåº¦åŒ¹é…ï¼šé«˜æ•ˆç”Ÿæˆå»ºæ¨¡çš„æ–°æ–¹æ³•', 'desc': 'ç»ˆç«¯é€Ÿåº¦åŒ¹é…ï¼ˆTVMï¼‰æ˜¯ä¸€ç§æµåŒ¹é…çš„æ¨å¹¿æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦çš„ç”Ÿæˆå»ºæ¨¡ã€‚TVMé€šè¿‡å»ºæ¨¡ä»»æ„ä¸¤ä¸ªæ‰©æ•£æ—¶é—´æ­¥ä¹‹é—´çš„è¿‡æ¸¡ï¼Œå¹¶åœ¨ç»ˆç«¯æ—¶é—´å¯¹å…¶è¡Œä¸ºè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»è€Œæé«˜ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“æ¨¡å‹æ˜¯Lipschitzè¿ç»­æ—¶ï¼ŒTVMä¸ºæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„2-Wassersteinè·ç¦»æä¾›äº†ä¸Šç•Œã€‚é€šè¿‡å¼•å…¥æœ€å°çš„æ¶æ„å˜åŒ–ï¼ŒTVMåœ¨Diffusion Transformersä¸Šå®ç°äº†ç¨³å®šçš„å•é˜¶æ®µè®­ç»ƒï¼Œå¹¶åœ¨ImageNetä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20426', 'title': 'Block Cascading: Training Free Acceleration of Block-Causal Video Models', 'url': 'https://huggingface.co/papers/2511.20426', 'abstract': 'Block Cascading parallelizes video block generation, achieving significant speed improvements without compromising quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '64ccba2bb3605ffc', 'authors': ['Hmrishav Bandyopadhyay', 'Nikhil Pinnaparaju', 'Rahim Entezari', 'Jim Scott', 'Yi-Zhe Song', 'Varun Jampani'], 'affiliations': ['SketchX, University of Surrey', 'Stability AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20426.jpg', 'data': {'categories': ['#video', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Block Cascading, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´ĞµĞ½ÑƒĞ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ°ÑĞºĞ°Ğ´, Ğ³Ğ´Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞ½ÑƒĞ°Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¿ÑÑ‚Ğ¸ GPU Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 2 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ.'}, 'en': {'title': 'Speed Meets Quality: Block Cascading Revolutionizes Video Generation', 'desc': 'The paper introduces Block Cascading, a novel approach to video block generation that enhances speed without sacrificing quality. Traditional block-causal methods face a trade-off between responsiveness and quality, with smaller models being faster but less detailed. Block Cascading allows for parallel generation of video blocks by utilizing partially denoised information from previous blocks, enabling multiple blocks to be processed simultaneously. This method achieves significant speed improvements, doubling the frames per second (FPS) for various model sizes while maintaining high generation quality.'}, 'zh': {'title': 'åŒºå—çº§è”ï¼šæå‡è§†é¢‘ç”Ÿæˆé€Ÿåº¦çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åŒºå—çº§è”æŠ€æœ¯é€šè¿‡å¹¶è¡ŒåŒ–è§†é¢‘å—ç”Ÿæˆï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆé€Ÿåº¦è€Œä¸å½±å“è´¨é‡ã€‚ä¼ ç»Ÿçš„åŒºå—å› æœè§†é¢‘ç”Ÿæˆåœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æƒè¡¡ï¼Œè€ŒåŒºå—çº§è”æœ‰æ•ˆç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œæœªæ¥çš„è§†é¢‘å—ç”Ÿæˆä¸éœ€è¦å®Œå…¨å»å™ªçš„å½“å‰å—ï¼Œå¯ä»¥åˆ©ç”¨éƒ¨åˆ†å»å™ªçš„ä¸Šä¸‹æ–‡è¿›è¡Œç”Ÿæˆã€‚é€šè¿‡ä½¿ç”¨5ä¸ªGPUè¿›è¡Œæ—¶é—´å¹¶è¡Œå¤„ç†ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸Šå®ç°äº†çº¦2å€çš„åŠ é€Ÿï¼Œç¡®ä¿äº†ç”Ÿæˆè´¨é‡çš„ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21688', 'title': 'G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning', 'url': 'https://huggingface.co/papers/2511.21688', 'abstract': 'G$^2$VLM integrates 3D geometry learning with vision-language models to enhance spatial understanding and reasoning, outperforming existing models in these tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': 'c34b3193640c26a7', 'authors': ['Wenbo Hu', 'Jingli Lin', 'Yilin Long', 'Yunlong Ran', 'Lihan Jiang', 'Yifan Wang', 'Chenming Zhu', 'Runsen Xu', 'Tai Wang', 'Jiangmiao Pang'], 'affiliations': ['CUHK', 'FDU', 'HKU', 'SJTU', 'Shanghai AI Lab', 'UCLA', 'USTC', 'ZJU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21688.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#transfer_learning', '#multimodal', '#3d'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GÂ²VLM â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. GÂ²VLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ 3D Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· in-context learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Bridging 3D Geometry and Vision-Language for Better Spatial Intelligence', 'desc': "G$^2$VLM is a new model that combines 3D geometry learning with vision-language models to improve how machines understand and reason about space. Traditional vision-language models struggle with spatial tasks because they don't effectively reconstruct 3D environments from 2D images. G$^2$VLM addresses this by integrating 3D visual features, allowing it to predict 3D attributes and enhance reasoning through in-context learning. The model shows strong performance in both spatial understanding and reasoning tasks, making it a valuable tool for future applications like 3D scene editing."}, 'zh': {'title': 'G$^2$VLMï¼šæå‡ç©ºé—´æ™ºèƒ½çš„æ–°åŸºå‡†', 'desc': 'G$^2$VLMæ˜¯ä¸€ç§ç»“åˆäº†ä¸‰ç»´å‡ ä½•å­¦ä¹ å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡ç©ºé—´ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡ä»äºŒç»´å›¾åƒé‡å»ºä¸‰ç»´ç©ºé—´ï¼Œå¼¥è¡¥äº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ™ºèƒ½æ–¹é¢çš„ä¸è¶³ã€‚G$^2$VLMåˆ©ç”¨å­¦ä¹ åˆ°çš„ä¸‰ç»´è§†è§‰å‡ ä½•ç‰¹å¾ï¼Œç›´æ¥é¢„æµ‹ä¸‰ç»´å±æ€§ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œäº¤é”™æ¨ç†æ¥å¢å¼ºç©ºé—´æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG$^2$VLMåœ¨ç©ºé—´ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„å¼ºåŸºå‡†ï¼Œæ¨åŠ¨æœªæ¥çš„åº”ç”¨å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17889', 'title': 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots', 'url': 'https://huggingface.co/papers/2511.17889', 'abstract': 'A unified vision-language-action framework, MobileVLA-R1, enhances reasoning and control for quadruped robots through supervised chain-of-thought alignment and GRPO reinforcement learning, achieving superior performance in complex environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '95577d9d2690e2fc', 'authors': ['Ting Huang', 'Dongjian Li', 'Rui Yang', 'Zeyu Zhang', 'Zida Yang', 'Hao Tang'], 'affiliations': ['Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17889.jpg', 'data': {'categories': ['#dataset', '#rl', '#reasoning', '#open_source', '#multimodal', '#robotics', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚: Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ² Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸', 'desc': 'MobileVLA-R1 â€” ÑÑ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…Ğ½Ğ¾Ğ³Ğ¸Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ğ¼Ñ‹ÑĞ»Ğ¸, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· GRPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 5% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Quadruped Robots with Unified Vision-Language-Action Framework', 'desc': "The paper presents MobileVLA-R1, a new framework that combines vision, language, and action to improve how quadruped robots understand and execute tasks. It addresses the challenge of linking high-level language instructions with low-level motor actions, which often leads to poor performance in real-world scenarios. By using a large dataset for structured reasoning and a two-stage training approach that integrates supervised learning with reinforcement learning, the framework enhances the robots' reasoning and control capabilities. The results show that MobileVLA-R1 outperforms existing methods, achieving better stability and effectiveness in complex environments."}, 'zh': {'title': 'ç»Ÿä¸€è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶æå‡å››è¶³æœºå™¨äººæ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¡†æ¶MobileVLA-R1ï¼Œæ—¨åœ¨æé«˜å››è¶³æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†å’Œæ§åˆ¶èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºå¤šç²’åº¦çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®é›†ï¼Œæä¾›äº†ç»“æ„åŒ–çš„æ¨ç†ç›‘ç£ï¼Œä»è€Œå®ç°äº†æ˜ç¡®çš„æ¨ç†å’Œè¿ç»­æ§åˆ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£çš„CoTå¯¹é½å’ŒGRPOå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºäº†æ¨ç†çš„ä¸€è‡´æ€§å’Œæ§åˆ¶çš„ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMobileVLA-R1åœ¨è§†è§‰å¯¼èˆªå’Œè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”åœ¨çœŸå®ç¯å¢ƒä¸­éªŒè¯äº†å…¶å¼ºå¤§çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20633', 'title': 'Reinforcing Action Policies by Prophesying', 'url': 'https://huggingface.co/papers/2511.20633', 'abstract': 'ProphRL enhances Vision-Language-Action policies through a learned world model and reinforcement learning tailored to flow-based action heads, improving data efficiency and optimization stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'a4aef34965e3c12e', 'authors': ['Jiahui Zhang', 'Ze Huang', 'Chun Gu', 'Zipei Ma', 'Li Zhang'], 'affiliations': ['Logos Robotics', 'School of Data Science, Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20633.jpg', 'data': {'categories': ['#optimization', '#rl', '#transfer_learning', '#multimodal', '#robotics', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'ProphRL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Vision-Language-Action Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Prophet Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ reinforcement learning Ğ´Ğ»Ñ flow-based action Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº. Prophet â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ FA-GRPO Ğ¸ FlowScale â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: 5-17% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ 24-30% Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ….'}, 'en': {'title': 'Enhancing VLA Policies with ProphRL for Better Efficiency and Stability', 'desc': 'ProphRL is a method that improves Vision-Language-Action (VLA) policies by using a learned world model and reinforcement learning (RL) focused on flow-based action heads. Traditional VLA training often relies on imitation, which can lead to overfitting and poor performance in new situations. ProphRL enhances data efficiency and stability in optimization by introducing a unified action-to-video robot actuation system called Prophet, which learns from diverse robot data. The method also incorporates Flow-action-GRPO and FlowScale to refine action policies, resulting in significant performance improvements in various benchmarks and real-world applications.'}, 'zh': {'title': 'ProphRLï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„é«˜æ•ˆè·¯å¾„', 'desc': 'ProphRL æ˜¯ä¸€ç§é€šè¿‡å­¦ä¹ çš„ä¸–ç•Œæ¨¡å‹å’Œé’ˆå¯¹æµå¼åŠ¨ä½œå¤´çš„å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥çš„æ–¹æ³•ã€‚å®ƒè§£å†³äº†ä¼ ç»Ÿ VLA ç­–ç•¥åœ¨è®­ç»ƒä¸­å®¹æ˜“è¿‡æ‹Ÿåˆå’Œåœ¨ç¯å¢ƒå˜åŒ–æ—¶è¡¨ç°ä¸ç¨³å®šçš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ Prophet ä½œä¸ºç»Ÿä¸€çš„åŠ¨ä½œåˆ°è§†é¢‘çš„æœºå™¨äººæ‰§è¡Œæ¨¡å‹ï¼ŒProphRL æé«˜äº†æ•°æ®æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒProphRL åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸ŠæˆåŠŸç‡æé«˜äº† 5-17%ï¼Œåœ¨çœŸå®æœºå™¨äººä¸Šæé«˜äº† 24-30%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20814', 'title': 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning', 'url': 'https://huggingface.co/papers/2511.20814', 'abstract': 'Sphinx, a synthetic environment for visual perception and reasoning, evaluates large vision-language models and demonstrates that reinforcement learning with verifiable rewards improves model accuracy on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '2abc59c8b690c673', 'authors': ['Md Tanvirul Alam', 'Saksham Aggarwal', 'Justin Yang Chae', 'Nidhi Rastogi'], 'affiliations': ['Rochester Institute of Technology', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20814.jpg', 'data': {'categories': ['#cv', '#dataset', '#rl', '#benchmark', '#multimodal'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ…', 'desc': 'Sphinx â€” ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ñ‹, Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ¸, Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ¸ĞºĞ¾Ğ½ĞºĞ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ñ‹), ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 25 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RLVR) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Sphinx: Enhancing Visual Reasoning with Reinforcement Learning', 'desc': 'The paper introduces Sphinx, a synthetic environment designed for testing visual perception and reasoning in machine learning models. It generates various puzzles that are paired with accurate solutions, allowing for effective evaluation and the creation of large datasets. The study assesses the performance of large vision-language models (LVLMs), revealing that even advanced models like GPT-5 perform significantly below human levels. By applying reinforcement learning with verifiable rewards, the authors show notable improvements in model accuracy across multiple tasks, suggesting a new direction for enhancing multimodal reasoning capabilities.'}, 'zh': {'title': 'Sphinxï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'Sphinxæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰æ„ŸçŸ¥å’Œæ¨ç†çš„åˆæˆç¯å¢ƒï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡ç¨‹åºç”Ÿæˆæ‹¼å›¾ï¼Œç»“åˆå¯éªŒè¯çš„çœŸå®è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒç²¾ç¡®è¯„ä¼°å’Œå¤§è§„æ¨¡æ•°æ®é›†æ„å»ºã€‚è¯¥åŸºå‡†æ¶µç›–25ç§ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬å¯¹ç§°æ£€æµ‹ã€å‡ ä½•å˜æ¢ã€ç©ºé—´æ¨ç†ç­‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20410', 'title': 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs', 'url': 'https://huggingface.co/papers/2511.20410', 'abstract': "TBCM, a self-contained trajectory-based distillation method, enhances diffusion model efficiency by eliminating external data dependency and improving knowledge transfer, achieving high-quality generation with reduced computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.", 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'ddb09bd29feebd0d', 'authors': ['Bao Tang', 'Shuai Zhang', 'Yueting Zhu', 'Jijun Xiang', 'Xin Yang', 'Li Yu', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Huazhong University of Science and Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20410.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#transfer_learning', '#open_source'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'TBCM â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² VAE-ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ TBCM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 40% Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°.'}, 'en': {'title': 'Efficient Diffusion with TBCM: No Data, No Problem!', 'desc': "The Trajectory-Backward Consistency Model (TBCM) is a novel method that enhances the efficiency of diffusion models by removing the need for external training data. It achieves this by utilizing latent representations derived from the teacher model's generation trajectory, which simplifies the distillation process. TBCM not only improves knowledge transfer but also significantly reduces computational resource requirements, making it suitable for resource-constrained environments. Empirical results show that TBCM outperforms existing methods in terms of generation quality and efficiency, achieving impressive scores while cutting down training time and GPU memory usage."}, 'zh': {'title': 'æå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'TBCMæ˜¯ä¸€ç§è‡ªåŒ…å«çš„åŸºäºè½¨è¿¹çš„è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚å®ƒé€šè¿‡ç›´æ¥ä»æ•™å¸ˆæ¨¡å‹çš„ç”Ÿæˆè½¨è¿¹ä¸­æå–æ½œåœ¨è¡¨ç¤ºï¼Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒTBCMæ˜¾è‘—æé«˜äº†ç”Ÿæˆæ•ˆç‡å’Œç®€åŒ–äº†è¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆè´¨é‡ä¸Šæ²¡æœ‰å¦¥åã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTBCMåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ä»èƒ½å®ç°é«˜è´¨é‡ç”Ÿæˆï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šé¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18452', 'title': 'NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering', 'url': 'https://huggingface.co/papers/2511.18452', 'abstract': 'Neighborhood Attention Filtering (NAF) upsamples features from Vision Foundation Models without retraining, achieving state-of-the-art performance across tasks with high efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '0e27982056a19020', 'authors': ['Loick Chambon', 'Paul Couairon', 'Eloi Zablocki', 'Alexandre Boulch', 'Nicolas Thome', 'Matthieu Cord'], 'affiliations': ['Institut Universitaire de France (IUF)', 'Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France', 'Valeo.ai, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18452.jpg', 'data': {'categories': ['#cv', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Neighborhood Attention Filtering (NAF) Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Vision Foundation Models, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. NAF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²ĞµÑĞ¾Ğ², Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¼ÑÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº Ğ»ÑĞ±Ğ¾Ğ¹ VFM, Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ¿ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. NAF Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Upsample Smartly: NAF Transforms Vision Models Without Retraining!', 'desc': 'Neighborhood Attention Filtering (NAF) is a novel method that enhances features from Vision Foundation Models (VFMs) without the need for retraining. It addresses the challenge of upsampling spatially downsampled representations, which is crucial for pixel-level tasks. NAF utilizes Cross-Scale Neighborhood Attention and Rotary Position Embeddings to learn adaptive weights based on the high-resolution input image, allowing it to operate in a zero-shot manner. This approach not only outperforms VFM-specific upsamplers but also maintains high efficiency, making it suitable for various applications including image restoration.'}, 'zh': {'title': 'é‚»åŸŸæ³¨æ„åŠ›è¿‡æ»¤ï¼šé«˜æ•ˆçš„ç‰¹å¾ä¸Šé‡‡æ ·æ–°æ–¹æ³•', 'desc': 'é‚»åŸŸæ³¨æ„åŠ›è¿‡æ»¤ï¼ˆNAFï¼‰æ˜¯ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„ç‰¹å¾ä¸Šé‡‡æ ·æ–¹æ³•ï¼Œèƒ½å¤Ÿä»è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–ç‰¹å¾ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç°æœ‰çš„ä¸Šé‡‡æ ·æ–¹æ³•é¢ä¸´ç»å…¸æ»¤æ³¢å™¨é€Ÿåº¦å¿«ä½†å½¢å¼å›ºå®šä¸ç°ä»£ä¸Šé‡‡æ ·å™¨å‡†ç¡®æ€§é«˜ä½†éœ€é’ˆå¯¹æ¯ä¸ªè§†è§‰åŸºç¡€æ¨¡å‹é‡æ–°è®­ç»ƒä¹‹é—´çš„æƒè¡¡ã€‚NAFé€šè¿‡è·¨å°ºåº¦é‚»åŸŸæ³¨æ„åŠ›å’Œæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å­¦ä¹ è‡ªé€‚åº”çš„ç©ºé—´å’Œå†…å®¹æƒé‡ï¼Œä»…ä¾èµ–é«˜åˆ†è¾¨ç‡è¾“å…¥å›¾åƒè¿›è¡ŒæŒ‡å¯¼ã€‚NAFåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä»ä»»ä½•è§†è§‰åŸºç¡€æ¨¡å‹ä¸­ä¸Šé‡‡æ ·ç‰¹å¾ï¼Œæˆä¸ºé¦–ä¸ªè¶…è¶Šç‰¹å®šè§†è§‰åŸºç¡€æ¨¡å‹ä¸Šé‡‡æ ·å™¨çš„æ¶æ„ï¼Œå±•ç°å‡ºå…¶é«˜æ•ˆæ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19504', 'title': 'Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma', 'url': 'https://huggingface.co/papers/2511.19504', 'abstract': 'The Alignment Trilemma in RLHF shows that achieving representativeness, tractability, and robustness is computationally infeasible, leading to trade-offs in current implementations.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '1b364b9b30873e7d', 'authors': ['Subramanyam Sahoo', 'Aman Chadha', 'Vinija Jain', 'Divya Chaudhary'], 'affiliations': ['AWS Generative AI Innovation Center, Amazon Web Services', 'Berkeley AI Safety Initiative (BASIS), University of California, Berkeley', 'Meta AI', 'Northeastern University, Seattle, WA, USA', 'Stanford University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19504.jpg', 'data': {'categories': ['#alignment', '#ethics', '#security', '#rlhf', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ RLHF Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğ¼, Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ RLHF (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°) Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½ÑĞµÑ‚ÑÑ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµĞ¼Ñ Ğ½ĞµÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸: Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿ Ğ»ÑĞ´ĞµĞ¹, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑÑƒĞ¿ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ RLHF, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹, Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ¸ÑƒĞ¼Ğ²Ğ¸Ñ€Ğ°Ñ‚Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM.'}, 'en': {'title': 'Navigating the Alignment Trilemma in RLHF', 'desc': 'The paper discusses the Alignment Trilemma in Reinforcement Learning from Human Feedback (RLHF), which highlights the challenges of achieving representativeness, tractability, and robustness in AI systems. It reveals that improving one aspect often leads to compromises in others, such as fairness and safety. The authors demonstrate that to achieve both representativeness and robustness for diverse populations, an impractical amount of computational resources is required. They also provide insights into current RLHF practices that tend to favor representativeness at the cost of robustness, leading to issues like bias amplification and preference collapse.'}, 'zh': {'title': 'å¯¹é½ä¸‰éš¾ï¼šRLHFä¸­çš„æƒè¡¡æŒ‘æˆ˜', 'desc': 'åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªç§°ä¸ºå¯¹é½ä¸‰éš¾çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜è¡¨æ˜ï¼Œæƒ³è¦åŒæ—¶å®ç°ä»£è¡¨æ€§ã€å¯å¤„ç†æ€§å’Œé²æ£’æ€§æ˜¯è®¡ç®—ä¸Šä¸å¯è¡Œçš„ï¼Œå› æ­¤åœ¨å½“å‰çš„å®ç°ä¸­éœ€è¦åšå‡ºæƒè¡¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„RLHFç³»ç»Ÿå¾€å¾€ç‰ºç‰²äº†ä»£è¡¨æ€§ï¼Œåªä»åŒè´¨çš„æ ‡æ³¨è€…æ± ä¸­æ”¶é›†å°‘é‡æ ·æœ¬ï¼Œè€ŒçœŸæ­£çš„å…¨çƒä»£è¡¨æ€§éœ€è¦æ›´å¤šçš„æ ·æœ¬ã€‚é€šè¿‡å¤æ‚æ€§ç†è®ºåˆ†æï¼Œæœ¬æ–‡ä¸ºç†è§£RLHFä¸­çš„åè§å’Œå…¶ä»–é—®é¢˜æä¾›äº†ç»Ÿä¸€çš„è§£é‡Šï¼Œå¹¶æå‡ºäº†åº”å¯¹è¿™äº›åŸºæœ¬æƒè¡¡çš„å…·ä½“æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.18005', 'title': 'RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale', 'url': 'https://huggingface.co/papers/2511.18005', 'abstract': 'RAISECity generates high-quality, city-scale 3D worlds with real-world alignment, using an agentic framework with multimodal tools, iterative refinement, and advanced representations.  \t\t\t\t\tAI-generated summary \t\t\t\t City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '24e0ea39f6d0a614', 'authors': ['Shengyuan Wang', 'Zhiheng Zheng', 'Yu Shang', 'Lixuan He', 'Yangcheng Yu', 'Fan Hangyu', 'Jie Feng', 'Qingmin Liao', 'Yong Li'], 'affiliations': ['College of AI, Tsinghua University, Beijing, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China', 'Shenzhen International Graduate School, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18005.jpg', 'data': {'categories': ['#robotics', '#agents', '#3d', '#multimodal'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²', 'desc': 'RAISECity â€” ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'RAISECity: Crafting Realistic 3D Cities with Intelligent Synthesis', 'desc': 'RAISECity is a novel framework designed to generate high-quality, large-scale 3D city environments that align closely with real-world data. It utilizes an agentic approach, integrating various multimodal tools to gather real-world knowledge and create detailed 3D representations. The framework emphasizes iterative refinement and dynamic data processing to reduce errors and improve the quality of the generated scenes. Experimental results demonstrate that RAISECity significantly outperforms existing methods in terms of visual fidelity, shape accuracy, and overall perceptual quality.'}, 'zh': {'title': 'RAISECityï¼šåŸå¸‚è§„æ¨¡3Dä¸–ç•Œçš„æ™ºèƒ½åˆæˆå¼•æ“', 'desc': 'RAISECity æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜è´¨é‡åŸå¸‚è§„æ¨¡ 3D ä¸–ç•Œçš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿä¸ç°å®ä¸–ç•Œå¯¹é½ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ™ºèƒ½åˆæˆå¼•æ“ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å·¥å…·å’Œè¿­ä»£ä¼˜åŒ–æ¥æ„å»ºå¤æ‚çš„ 3D åœºæ™¯ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŠ¨æ€æ•°æ®å¤„ç†å’Œè‡ªæˆ‘åæ€ï¼Œå‡å°‘äº†ç´¯ç§¯è¯¯å·®ï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAISECity åœ¨ç°å®å¯¹é½ã€å½¢çŠ¶ç²¾åº¦å’Œçº¹ç†ä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œé€‚ç”¨äºæ²‰æµ¸å¼åª’ä½“å’Œæ™ºèƒ½ä½“æ¨¡å‹ç­‰åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21208', 'title': 'I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation', 'url': 'https://huggingface.co/papers/2511.21208', 'abstract': 'A novel framework using RaPP and uncertainty quantification improves RUL prediction accuracy and interpretability in multi-sensor systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '9f617ec07eea1183', 'authors': ['Lucas Thil', 'Jesse Read', 'Rim Kaddah', 'Guillaume Doquet'], 'affiliations': ['IRT SystemX, France', 'LIX Ecole Polytechnique, France', 'Safran Tech'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21208.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”§', 'ru': {'title': 'RaPP Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° ÑÑ€Ğ¾ĞºĞ° ÑĞ»ÑƒĞ¶Ğ±Ñ‹', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ (HI) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ RaPP (Reconstruction along Projected Pathways). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RaPP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ¾ĞºĞ° ÑĞ»ÑƒĞ¶Ğ±Ñ‹ (RUL). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Monte Carlo dropout Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ I-GLIDE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ².'}, 'en': {'title': 'Enhancing RUL Prediction with RaPP and Uncertainty Quantification', 'desc': 'This paper presents a new framework that enhances the prediction of remaining useful life (RUL) in systems with multiple sensors by using a method called Reconstruction along Projected Pathways (RaPP). It improves the accuracy of health indicators (HIs) and incorporates uncertainty quantification to better understand the reliability of these indicators. The authors introduce a novel approach called I-GLIDE, which groups sensors to focus on specific degradation mechanisms, making the diagnostics more interpretable. Overall, this work significantly advances RUL prediction by providing clearer insights into system failures and improving the robustness of predictions.'}, 'zh': {'title': 'æå‡RULé¢„æµ‹çš„å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨é‡å»ºæ²¿æŠ•å½±è·¯å¾„ï¼ˆRaPPï¼‰å’Œä¸ç¡®å®šæ€§é‡åŒ–æ¥æé«˜å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿä¸­å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰é¢„æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚é¦–å…ˆï¼Œæœ¬æ–‡é¦–æ¬¡å°†RaPPä½œä¸ºå¥åº·æŒ‡æ ‡ï¼ˆHIï¼‰ç”¨äºRULé¢„æµ‹ï¼Œç»“æœæ˜¾ç¤ºå…¶ä¼˜äºä¼ ç»Ÿçš„é‡å»ºè¯¯å·®åº¦é‡ã€‚å…¶æ¬¡ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›ä¸¢å¼ƒæ³•å’Œæ¦‚ç‡æ½œåœ¨ç©ºé—´å¯¹RaPPè¡ç”Ÿçš„HIè¿›è¡Œéšæœºå’Œè®¤çŸ¥ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæ˜¾è‘—æé«˜äº†RULé¢„æµ‹çš„é²æ£’æ€§ã€‚æœ€åï¼Œæå‡ºäº†æŒ‡æ ‡ç»„çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿéš”ç¦»ä¼ æ„Ÿå™¨å­é›†ä»¥å»ºæ¨¡ç‰¹å®šç³»ç»Ÿçš„é€€åŒ–ï¼Œè¿›è€Œå®ç°å¯è§£é‡Šçš„æœºåˆ¶ç‰¹å®šè¯Šæ–­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.17918', 'title': 'Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization', 'url': 'https://huggingface.co/papers/2511.17918', 'abstract': "Frequency-Adaptive Sharpness Regularization (FASR) enhances 3D Gaussian Splatting's generalization to novel viewpoints by adaptively adjusting regularization based on local image frequency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.", 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': 'c07f57707375b437', 'authors': ['Youngsik Yun', 'Dongjun Gu', 'Youngjung Uh'], 'affiliations': ['UNIST', 'Yonsei University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.17918.jpg', 'data': {'categories': ['#optimization', '#training', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ³Ğ¾ 3D ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Frequency-Adaptive Sharpness Regularization (FASR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ 3D Gaussian Splatting Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Sharpness-Aware Minimization Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»Ñƒ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ´Ğ¸ÑƒÑ ÑĞ¾ÑĞµĞ´ÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing 3D View Synthesis with Frequency-Adaptive Regularization', 'desc': 'Frequency-Adaptive Sharpness Regularization (FASR) improves the generalization of 3D Gaussian Splatting (3DGS) to new viewpoints by adjusting regularization based on local image frequency. The paper identifies that 3DGS struggles with overfitting in few-shot scenarios, which limits its ability to synthesize novel views. FASR reformulates the training objective of 3DGS, allowing it to better balance sharpness and detail preservation during optimization. By dynamically setting regularization weights according to local image characteristics, FASR effectively reduces artifacts and enhances detail reconstruction in generated views.'}, 'zh': {'title': 'è‡ªé€‚åº”æ­£åˆ™åŒ–ï¼Œæå‡3Dè§†è§’æ³›åŒ–èƒ½åŠ›', 'desc': 'é¢‘ç‡è‡ªé€‚åº”é”åº¦æ­£åˆ™åŒ–ï¼ˆFASRï¼‰é€šè¿‡æ ¹æ®å±€éƒ¨å›¾åƒé¢‘ç‡è‡ªé€‚åº”è°ƒæ•´æ­£åˆ™åŒ–ï¼Œå¢å¼ºäº†3Dé«˜æ–¯ç‚¹äº‘åœ¨æ–°è§†è§’ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡3Dé«˜æ–¯ç‚¹äº‘åœ¨å¤§å¤šæ•°é…ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­ï¼Œå®ƒåœ¨æ–°è§†è§’çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆç¨€ç–è§‚æµ‹ã€‚FASRé‡æ–°æ„å»ºäº†3DGSçš„è®­ç»ƒç›®æ ‡ï¼ŒæŒ‡å¯¼å…¶æ”¶æ•›åˆ°æ›´å¥½çš„æ³›åŒ–è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åæ˜ å›¾åƒçš„å±€éƒ¨é¢‘ç‡æ¥è®¾ç½®æ­£åˆ™åŒ–æƒé‡ï¼ŒFASRæœ‰æ•ˆé˜²æ­¢äº†æ–°è§†è§’ä¸­çš„æµ®åŠ¨ä¼ªå½±ï¼Œå¹¶é‡å»ºäº†ç»†èŠ‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21541', 'title': 'Video Generation Models Are Good Latent Reward Models', 'url': 'https://huggingface.co/papers/2511.21541', 'abstract': 'PRFL optimizes video generation preferences in latent space, improving alignment with human preferences while reducing memory consumption and training time.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.', 'score': 45, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '57d09bbf4f0c2aa7', 'authors': ['Xiaoyue Mi', 'Wenqing Yu', 'Jiesong Lian', 'Shibo Jie', 'Ruizhe Zhong', 'Zijun Liu', 'Guozhen Zhang', 'Zixiang Zhou', 'Zhiyong Xu', 'Yuan Zhou', 'Qinglin Lu', 'Fan Tang'], 'affiliations': ['Huazhong University of Science and Technology', 'Nanjing University', 'Peking University', 'Shanghai Jiao Tong University', 'Tencent Hunyuan', 'Tsinghua University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21541.jpg', 'data': {'categories': ['#video', '#alignment', '#optimization', '#rlhf', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ PRFL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµĞ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VAE, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing Video Generation in Latent Space for Human Preferences', 'desc': 'This paper introduces Process Reward Feedback Learning (PRFL), a new method for optimizing video generation that focuses on latent space rather than pixel space. By leveraging pre-trained video generation models, PRFL allows for efficient preference optimization that aligns better with human expectations while minimizing memory usage and training duration. The approach addresses the limitations of existing video reward models, which often rely on costly pixel-space inputs and late-stage optimization. Through extensive experiments, PRFL demonstrates improved performance in generating videos that are both visually appealing and structurally coherent, while also being more resource-efficient.'}, 'zh': {'title': 'æ½œåœ¨ç©ºé—´ä¸­çš„è§†é¢‘ç”Ÿæˆä¼˜åŒ–', 'desc': 'PRFLï¼ˆè¿‡ç¨‹å¥–åŠ±åé¦ˆå­¦ä¹ ï¼‰æ˜¯ä¸€ç§ä¼˜åŒ–è§†é¢‘ç”Ÿæˆåå¥½çš„æ–°æ–¹æ³•ï¼Œä¸“æ³¨äºæ½œåœ¨ç©ºé—´ä¸­çš„ä¼˜åŒ–ã€‚å®ƒé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåå¥½ä¼˜åŒ–ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­å¯¹åƒç´ ç©ºé—´çš„ä¾èµ–ï¼Œä»è€Œå‡å°‘äº†å†…å­˜æ¶ˆè€—å’Œè®­ç»ƒæ—¶é—´ã€‚ä¸ç°æœ‰çš„å›¾åƒç”Ÿæˆå¯¹é½æ–¹æ³•ç›¸æ¯”ï¼ŒPRFLèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¿åŠ¨åŠ¨æ€å’Œç»“æ„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRFLåœ¨ä¸äººç±»åå¥½çš„å¯¹é½æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„RGB ReFLæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21691', 'title': 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls', 'url': 'https://huggingface.co/papers/2511.21691', 'abstract': 'Canvas-to-Image is a unified framework that encodes diverse control signals into a composite canvas image for high-fidelity multimodal image generation, outperforming existing methods in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.', 'score': 32, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '83ac3d1b65e8541d', 'authors': ['Yusuf Dalva', 'Guocheng Gordon Qian', 'Maya Goldenberg', 'Tsai-Shien Chen', 'Kfir Aberman', 'Sergey Tulyakov', 'Pinar Yanardag', 'Kuan-Chieh Jackson Wang'], 'affiliations': ['Snap Inc.', 'UC Merced', 'Virginia Tech'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21691.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#benchmark', '#multimodal', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ…Ğ¾Ğ»ÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Canvas-to-Image â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñæ¶æ§‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-ĞºĞ°Ğ½Ğ²Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ°Ñ ĞºĞ°Ğ½Ğ²Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Unified Control for High-Fidelity Image Generation', 'desc': 'Canvas-to-Image is a novel framework designed to enhance multimodal image generation by integrating various control signals into a single canvas image. This approach allows users to specify multiple parameters, such as text prompts and spatial arrangements, which the model interprets for accurate image creation. By employing a Multi-Task Canvas Training strategy, the framework optimizes the diffusion model to effectively handle diverse controls in a unified manner. The results demonstrate that Canvas-to-Image surpasses existing methods in maintaining identity and adhering to user-defined controls across complex image generation tasks.'}, 'zh': {'title': 'ç»Ÿä¸€ç”»å¸ƒï¼Œç²¾å‡†ç”Ÿæˆå›¾åƒ', 'desc': 'Canvas-to-Image æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥å°†å¤šç§æ§åˆ¶ä¿¡å·ç¼–ç ä¸ºä¸€ä¸ªå¤åˆç”»å¸ƒå›¾åƒï¼Œä»è€Œå®ç°é«˜ä¿çœŸåº¦çš„å¤šæ¨¡æ€å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°ä»£æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æç¤ºã€ä¸»é¢˜å‚è€ƒã€ç©ºé—´æ’åˆ—ã€å§¿åŠ¿çº¦æŸå’Œå¸ƒå±€æ³¨é‡Šç­‰å¤šç§æ§åˆ¶æ—¶çš„å›°éš¾ã€‚é€šè¿‡å°†ä¸åŒçš„æ§åˆ¶ä¿¡å·æ•´åˆåˆ°ä¸€ä¸ªç”»å¸ƒç•Œé¢ä¸­ï¼Œç”¨æˆ·å¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆå…¶æ„å›¾çš„å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCanvas-to-Image åœ¨èº«ä»½ä¿ç•™å’Œæ§åˆ¶éµå¾ªæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.20937', 'title': 'ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction', 'url': 'https://huggingface.co/papers/2511.20937', 'abstract': 'ENACT is a benchmark that evaluates embodied cognition in vision-language models through world modeling from egocentric interaction in a VQA format, revealing performance gaps and anthropocentric biases.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': '8b84892aae7d3479', 'authors': ['Qineng Wang', 'Wenlong Huang', 'Yu Zhou', 'Hang Yin', 'Tianwei Bao', 'Jianwen Lyu', 'Weiyu Liu', 'Ruohan Zhang', 'Jiajun Wu', 'Li Fei-Fei', 'Manling Li'], 'affiliations': ['Northwestern University', 'Stanford University', 'UCLA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.20937.jpg', 'data': {'categories': ['#cv', '#robotics', '#benchmark', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ’Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'ENACT â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼ (Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°) Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼ (Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ½Ğ° 8972 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ VLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¾Ğ¹ Ğ¸ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ»Ğ¸ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°.'}, 'en': {'title': 'Evaluating Embodied Cognition in Vision-Language Models with ENACT', 'desc': 'ENACT is a benchmark designed to assess how well vision-language models (VLMs) understand embodied cognition, which is the idea that intelligence comes from interacting with the world. It evaluates models through a visual question answering (VQA) format that simulates egocentric interactions, using tasks that require world modeling. The benchmark includes two main tasks: forward world modeling, where models reorder observations based on actions, and inverse world modeling, where they reorder actions based on observations. Results show that while VLMs perform better on inverse tasks, they still lag behind human performance, especially as the complexity of interactions increases, revealing biases in their understanding of human-like actions.'}, 'zh': {'title': 'è¯„ä¼°å…·èº«è®¤çŸ¥çš„ENACTåŸºå‡†æµ‹è¯•', 'desc': 'ENACTæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„å…·èº«è®¤çŸ¥èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡ä»è‡ªæˆ‘ä¸­å¿ƒçš„äº¤äº’ä¸­è¿›è¡Œä¸–ç•Œå»ºæ¨¡ï¼Œé‡‡ç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ¼å¼ã€‚å…·èº«è®¤çŸ¥ç†è®ºè®¤ä¸ºï¼Œæ™ºèƒ½æºäºæ„ŸçŸ¥è¿åŠ¨çš„äº’åŠ¨ï¼Œè€Œéè¢«åŠ¨è§‚å¯Ÿã€‚ENACTé€šè¿‡éƒ¨åˆ†å¯è§‚å¯Ÿçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰æ¥æ¡†å®šè¯„ä¼°ï¼ŒåŒ…å«å‰å‘å’Œåå‘ä¸–ç•Œå»ºæ¨¡çš„ä¸¤ä¸ªäº’è¡¥ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå‰æ²¿çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä¸äººç±»ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ï¼Œå¹¶ä¸”è¿™ç§å·®è·éšç€äº¤äº’èŒƒå›´çš„æ‰©å¤§è€ŒåŠ å¤§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21678', 'title': 'Agentic Learner with Grow-and-Refine Multimodal Semantic Memory', 'url': 'https://huggingface.co/papers/2511.21678', 'abstract': 'ViLoMem, a dual-stream memory framework, enhances MLLMs by preserving multimodal semantic knowledge, reducing errors, and improving accuracy across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': 'baf3402956602ca0', 'authors': ['Weihao Bo', 'Shan Zhang', 'Yanpeng Sun', 'Jingjing Wu', 'Qunyi Xie', 'Xiao Tan', 'Kunbin Chen', 'Wei He', 'Xiaofan Li', 'Na Zhao', 'Jingdong Wang', 'Zechao Li'], 'affiliations': ['Adelaide AIML', 'Baidu Inc', 'Nanjing University of Science and Technology', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21678.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#hallucinations', '#multimodal', '#interpretability', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ViLoMem â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ, Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ»ÑÑ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ViLoMem ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing MLLMs with Dual-Stream Memory for Better Learning', 'desc': 'ViLoMem is a new memory framework designed for multimodal large language models (MLLMs) that helps them retain and utilize knowledge from different types of information, like text and images. Unlike traditional memory systems that only remember past actions, ViLoMem uses a dual-stream approach to separately track visual distractions and logical reasoning errors. This allows MLLMs to learn from both their successes and mistakes, improving their performance on various tasks. By continuously updating its memory, ViLoMem enhances accuracy and reduces repetitive errors, making it more aligned with how humans think and learn.'}, 'zh': {'title': 'åŒæµè®°å¿†ï¼Œæå‡å¤šæ¨¡æ€å­¦ä¹ ï¼', 'desc': 'ViLoMemæ˜¯ä¸€ç§åŒæµè®°å¿†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡ä¿ç•™å¤šæ¨¡æ€è¯­ä¹‰çŸ¥è¯†ï¼Œå‡å°‘é”™è¯¯å¹¶æé«˜åŸºå‡†æµ‹è¯•çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè½¨è¿¹çš„è®°å¿†ä¸åŒï¼ŒViLoMemèƒ½å¤Ÿåˆ†åˆ«ç¼–ç è§†è§‰å¹²æ‰°æ¨¡å¼å’Œé€»è¾‘æ¨ç†é”™è¯¯ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹ä»æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒä¸­å­¦ä¹ ã€‚è¯¥ç³»ç»Ÿé€šè¿‡é€æ­¥ç§¯ç´¯å’Œæ›´æ–°å¤šæ¨¡æ€è¯­ä¹‰çŸ¥è¯†ï¼Œé¿å…äº†ç¾éš¾æ€§é—å¿˜ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21662', 'title': 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following', 'url': 'https://huggingface.co/papers/2511.21662', 'abstract': 'Multi-Crit evaluates multimodal models on following diverse criteria with metrics for pluralistic adherence, criterion-switching flexibility, and recognizing preference conflicts, revealing gaps in model capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': 'f985ae4df9bd5fae', 'authors': ['Tianyi Xiong', 'Yi Ge', 'Ming Li', 'Zuolong Zhang', 'Pranav Kulkarni', 'Kaishen Wang', 'Qi He', 'Zeying Zhu', 'Chenxi Liu', 'Ruibo Chen', 'Tong Zheng', 'Yanshuo Chen', 'Xiyao Wang', 'Renrui Zhang', 'Wenhu Chen', 'Heng Huang'], 'affiliations': ['University of Maryland, College Park', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21662.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° LMM Ğ½Ğµ ÑĞ»Ñ‹ÑˆĞ¸Ñ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑƒĞ´ÑŒÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Multi-Crit, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ğ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ ĞµÑ‰Ñ‘ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ»ÑÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ, Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Multi-Crit: Elevating Multimodal Model Evaluation', 'desc': 'The paper introduces Multi-Crit, a benchmark designed to evaluate large multimodal models (LMMs) based on their ability to adhere to diverse evaluation criteria. It highlights the challenges these models face in maintaining consistent performance across pluralistic criteria, especially in open-ended tasks. The study develops three new metrics to assess how well models can switch between criteria and recognize conflicts in preferences. Through comprehensive analysis, it reveals significant gaps in both proprietary and open-source models, paving the way for improved multimodal AI evaluation systems.'}, 'zh': {'title': 'å¤šå…ƒåŒ–è¯„ä¼°æ ‡å‡†çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'Multi-Critæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„æ–°åŸºå‡†ï¼Œæ—¨åœ¨æµ‹è¯•å®ƒä»¬åœ¨éµå¾ªå¤šå…ƒåŒ–è¯„ä¼°æ ‡å‡†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œæ”¶é›†äº†å…·æœ‰å¤šæ ‡å‡†äººç±»æ³¨é‡Šçš„æŒ‘æˆ˜æ€§å“åº”å¯¹ã€‚Multi-Critå¼•å…¥äº†ä¸‰ç§æ–°æŒ‡æ ‡ï¼Œç³»ç»Ÿè¯„ä¼°æ¨¡å‹åœ¨å¤šå…ƒéµå¾ªã€æ ‡å‡†åˆ‡æ¢çµæ´»æ€§å’Œè¯†åˆ«åå¥½å†²çªæ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸€è‡´æ€§å’Œçµæ´»æ€§æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾å¼è¯„ä¼°ä»»åŠ¡ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.21087', 'title': 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing', 'url': 'https://huggingface.co/papers/2511.21087', 'abstract': 'MIRA, a multimodal reasoning agent, enhances diffusion-based image editing by iteratively interpreting complex instructions, improving both semantic consistency and perceptual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.', 'score': 10, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 26', 'zh': '11æœˆ26æ—¥'}, 'hash': 'ee850d64daa54860', 'authors': ['Ziyun Zeng', 'Hang Hua', 'Jiebo Luo'], 'affiliations': ['MIT-IBM Watson AI Lab', 'University of Rochester'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21087.jpg', 'data': {'categories': ['#cv', '#dataset', '#reasoning', '#diffusion', '#synthetic', '#open_source', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MIRA â€” Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑÑÑ‹Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° Ñ†Ğ¸ĞºĞ»Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MIRA-Editing Ğ¸Ğ· 150K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° SFT Ğ¸ GRPO, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'MIRA: Revolutionizing Image Editing with Multimodal Reasoning', 'desc': 'MIRA is a multimodal reasoning agent designed to enhance diffusion-based image editing by interpreting complex user instructions more effectively. It operates through an iterative loop of perception, reasoning, and action, allowing it to simulate a more interactive editing process similar to human interactions. By predicting atomic edit instructions step by step and utilizing visual feedback, MIRA addresses common issues in semantic drift and contextual inaccuracies found in traditional models. The training on a large multimodal dataset and integration with existing image editing models results in improved semantic consistency and perceptual quality in the edited images.'}, 'zh': {'title': 'MIRAï¼šæå‡å›¾åƒç¼–è¾‘çš„æ™ºèƒ½æ¨ç†ä»£ç†', 'desc': 'MIRAæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£è§£é‡Šå¤æ‚æŒ‡ä»¤æ¥å¢å¼ºåŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘ã€‚å®ƒè§£å†³äº†ä¼ ç»Ÿç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç”¨æˆ·æŒ‡ä»¤æ—¶çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œæ„ŸçŸ¥è´¨é‡é—®é¢˜ã€‚MIRAé€šè¿‡ä¸€ä¸ªè¿­ä»£çš„æ„ŸçŸ¥-æ¨ç†-è¡ŒåŠ¨å¾ªç¯ï¼Œé€æ­¥é¢„æµ‹åŸå­ç¼–è¾‘æŒ‡ä»¤ï¼Œå¹¶åˆ©ç”¨è§†è§‰åé¦ˆè¿›è¡Œå†³ç­–ã€‚ä¸å¼€æºå›¾åƒç¼–è¾‘æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶ï¼ŒMIRAæ˜¾è‘—æé«˜äº†ç¼–è¾‘æ•ˆæœï¼Œè¾¾åˆ°äº†ä¸ä¸€äº›ä¸“æœ‰ç³»ç»Ÿç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2511.19757', 'title': 'What does it mean to understand language?', 'url': 'https://huggingface.co/papers/2511.19757', 'abstract': "Language understanding involves transferring information from the core language system to other brain regions for constructing mental models, using world knowledge, and autobiographical memories.  \t\t\t\t\tAI-generated summary \t\t\t\t Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.", 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 24', 'zh': '11æœˆ24æ—¥'}, 'hash': '758068f258e39453', 'authors': ['Colton Casto', 'Anna Ivanova', 'Evelina Fedorenko', 'Nancy Kanwisher'], 'affiliations': ['Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology', 'Kempner Institute for the Study of Natural & Artificial Intelligence, Harvard University', 'School of Psychology, Georgia Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19757.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹Ğº Ğ¶Ğ¸Ğ²Ñ‘Ñ‚ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ·Ğ³Ñƒ, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ, ĞºĞ°Ğº Ğ¼Ğ¾Ğ·Ğ³ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ·Ñ‹Ğº, Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ñ‹ÑĞ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑÑ‚Ñƒ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'Unlocking Language: Bridging Core Systems and Mental Models', 'desc': "This paper discusses how understanding language goes beyond just grasping its basic meaning. It suggests that the brain's core language system has limitations, requiring information to be shared with other brain areas. These areas help create mental models, utilize world knowledge, and recall personal memories. The authors review evidence supporting this idea and propose that advancements in cognitive neuroscience can help test this hypothesis, enhancing our understanding of language processing."}, 'zh': {'title': 'ç†è§£è¯­è¨€çš„æ·±å±‚æ¬¡æœºåˆ¶', 'desc': 'è¯­è¨€ç†è§£ä¸ä»…ä»…æ˜¯æå–è¯­è¨€è¾“å…¥çš„è¡¨é¢æ„ä¹‰ï¼Œè¿˜æ¶‰åŠæ„å»ºä¸°å¯Œçš„å¿ƒç†æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºï¼Œç”±äºå¤§è„‘æ ¸å¿ƒè¯­è¨€ç³»ç»Ÿçš„å¤„ç†èƒ½åŠ›æœ‰é™ï¼Œæ·±å…¥ç†è§£è¯­è¨€éœ€è¦å°†ä¿¡æ¯ä»è¯­è¨€ç³»ç»Ÿè½¬ç§»åˆ°å…¶ä»–å¤§è„‘åŒºåŸŸã€‚è¿™äº›åŒºåŸŸè´Ÿè´£è®¡ç®—æ„ŸçŸ¥å’Œè¿åŠ¨è¡¨å¾ï¼Œæ„å»ºå¿ƒç†æ¨¡å‹ï¼Œä»¥åŠå­˜å‚¨æˆ‘ä»¬çš„ä¸–ç•ŒçŸ¥è¯†å’Œè‡ªä¼ è®°å¿†ã€‚æˆ‘ä»¬å›é¡¾äº†ç°æœ‰çš„è¯æ®ï¼Œå¹¶è®¤ä¸ºè®¤çŸ¥ç¥ç»ç§‘å­¦çš„æœ€æ–°è¿›å±•ä¸ºç›´æ¥æµ‹è¯•è¿™ä¸€å‡è®¾æä¾›äº†æ¦‚å¿µåŸºç¡€å’Œæ–¹æ³•ï¼Œä»è€Œå¼€å¯äº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œä»¥æ­ç¤ºç†è§£è¯­è¨€åœ¨è®¤çŸ¥å’Œç¥ç»å±‚é¢çš„æ„ä¹‰ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (36)', '#agents (82)', '#agi (2)', '#alignment (23)', '#architecture (82)', '#audio (12)', '#benchmark (201)', '#cv (80)', '#data (24)', '#dataset (147)', '#diffusion (49)', '#ethics (4)', '#games (10)', '#graphs (5)', '#hallucinations (18)', '#healthcare (19)', '#inference (37)', '#interpretability (30)', '#leakage (7)', '#long_context (22)', '#low_resource (9)', '#machine_translation (3)', '#math (11)', '#multilingual (15)', '#multimodal (186)', '#open_source (153)', '#optimization (111)', '#plp (11)', '#rag (14)', '#reasoning (117)', '#rl (68)', '#rlhf (27)', '#robotics (35)', '#science (35)', '#security (15)', '#small_models (27)', '#story_generation (1)', '#survey (6)', '#synthetic (51)', '#training (170)', '#transfer_learning (20)', '#video (74)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-11-12 06:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-11-12 06:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-11-12 06:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    