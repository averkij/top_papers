
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 237 papers. June 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Июнь 2025</span> | <span id="title-articles-count">237 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-05.html">⬅️ <span id="prev-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-07.html">➡️ <span id="next-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Июнь 2025', 'en': 'June 2025', 'zh': '6月2025年'};
        let feedDateNext = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let feedDatePrev = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.05301', 'title': 'SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training', 'url': 'https://huggingface.co/papers/2506.05301', 'abstract': 'SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.', 'score': 33, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b4eef08c89e4a692', 'authors': ['Jianyi Wang', 'Shanchuan Lin', 'Zhijie Lin', 'Yuxi Ren', 'Meng Wei', 'Zongsheng Yue', 'Shangchen Zhou', 'Hao Chen', 'Yang Zhao', 'Ceyuan Yang', 'Xuefeng Xiao', 'Chen Change Loy', 'Lu Jiang'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05301.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Эффективное восстановление видео за один шаг', 'desc': 'SeedVR2 - это однопроходная модель восстановления видео на основе диффузии. Она использует адаптивное оконное внимание и функцию сопоставления признаков для достижения высокого визуального качества при меньших вычислительных затратах. Модель применяет состязательное обучение на реальных данных для решения сложной задачи восстановления видео высокого разрешения за один шаг. Эксперименты показывают, что SeedVR2 достигает сравнимых или лучших результатов по сравнению с существующими подходами к восстановлению видео, выполняя обработку за один проход.'}, 'en': {'title': 'Efficient Video Restoration with SeedVR2: One-Step Diffusion Redefined', 'desc': 'SeedVR2 is a novel one-step diffusion-based model designed for video restoration that enhances visual quality while minimizing computational costs. It introduces an adaptive window attention mechanism that dynamically adjusts to the output resolution, addressing issues of window inconsistency in high-resolution video. The model also employs a feature matching loss to stabilize adversarial training, ensuring effective performance without compromising efficiency. Experimental results indicate that SeedVR2 outperforms or matches existing video restoration methods, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'SeedVR2：高效视频修复的新选择', 'desc': 'SeedVR2是一种基于扩散的一步视频修复模型，采用自适应窗口注意力机制和特征匹配损失，能够在降低计算成本的同时实现高视觉质量。该模型通过对真实数据进行对抗性训练，解决了高分辨率视频修复的挑战。为了适应不同输出分辨率，SeedVR2动态调整窗口大小，避免了高分辨率视频修复中窗口不一致的问题。此外，实验结果表明，SeedVR2在单步修复中能够达到或超过现有视频修复方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04308', 'title': 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics', 'url': 'https://huggingface.co/papers/2506.04308', 'abstract': 'Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 31, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ef5abd087929ed17', 'authors': ['Enshen Zhou', 'Jingkun An', 'Cheng Chi', 'Yi Han', 'Shanyu Rong', 'Chi Zhang', 'Pengwei Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beihang University', 'Beijing Academy of Artificial Intelligence', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#robotics', '#reasoning', '#dataset', '#3d', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'RoboRefer: Пространственный интеллект для роботов нового поколения', 'desc': 'RoboRefer - это модель пространственного понимания для роботов, основанная на зрении и языке. Она использует специальный энкодер глубины и обучение с подкреплением для точного понимания 3D-сцен и рассуждений о пространственных отношениях. Авторы также представили большой набор данных RefSpatial и бенчмарк RefSpatial-Bench для обучения и оценки модели. RoboRefer превзошла современные методы, включая Gemini-2.5-Pro, и может применяться для управления различными роботами в реальных условиях.'}, 'en': {'title': 'RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments', 'desc': 'The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks.'}, 'zh': {'title': 'RoboRefer：提升机器人空间理解与推理能力的创新模型', 'desc': '空间指向是具身机器人与三维物理世界互动的基本能力。尽管现有的预训练视觉语言模型（VLMs）很强大，但它们在理解复杂的三维场景和动态推理指示位置方面仍然存在不足。为此，我们提出了RoboRefer，这是一种具有三维感知能力的VLM，通过监督微调（SFT）集成了专门的深度编码器，实现了精确的空间理解。此外，RoboRefer通过强化微调（RFT）推进了多步骤空间推理，采用针对空间指向任务的度量敏感过程奖励函数。'}}}, {'id': 'https://huggingface.co/papers/2506.05284', 'title': 'Video World Models with Long-term Spatial Memory', 'url': 'https://huggingface.co/papers/2506.05284', 'abstract': "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.", 'score': 27, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '313ae4b0ffca864d', 'authors': ['Tong Wu', 'Shuai Yang', 'Ryan Po', 'Yinghao Xu', 'Ziwei Liu', 'Dahua Lin', 'Gordon Wetzstein'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05284.jpg', 'data': {'categories': ['#video', '#dataset', '#3d', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Улучшение долгосрочной памяти в видео-моделях мира', 'desc': 'Новая система улучшает долгосрочную согласованность видео-моделей мира, интегрируя механизм долгосрочной пространственной памяти, основанный на геометрии. Эта система вдохновлена механизмами человеческой памяти и включает в себя методы хранения и извлечения информации из долгосрочной пространственной памяти. Авторы создали специальные наборы данных для обучения и оценки моделей мира с явно сохраняемыми 3D-механизмами памяти. Оценки показывают улучшение качества, согласованности и длины контекста по сравнению с соответствующими базовыми моделями.'}, 'en': {'title': 'Enhancing Video World Models with Long-Term Spatial Memory', 'desc': 'This paper presents a new framework that improves the long-term consistency of video world models by incorporating a geometry-grounded long-term spatial memory mechanism. Traditional autoregressive models often forget previously generated scenes due to limited temporal context, which affects their ability to maintain consistency during revisits. The proposed framework mimics human memory by allowing the model to store and retrieve spatial information effectively, enhancing the overall quality of generated video frames. Evaluations demonstrate that this approach leads to better scene consistency and longer context retention compared to existing methods.'}, 'zh': {'title': '增强视频世界模型的一致性', 'desc': '本文提出了一种新框架，通过集成基于几何的长期空间记忆机制，增强视频世界模型的长期一致性。现有的世界模型在生成视频帧时，因时间上下文窗口大小有限，常常难以保持场景的一致性，导致对先前生成环境的严重遗忘。我们借鉴人类记忆机制，设计了存储和检索长期空间记忆的信息机制，并使用定制数据集来训练和评估具有显式存储3D记忆机制的世界模型。评估结果显示，与相关基线相比，我们的方法在质量、一致性和上下文长度上都有所提升，为长期一致的世界生成铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2506.05229', 'title': 'Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts', 'url': 'https://huggingface.co/papers/2506.05229', 'abstract': 'Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications.', 'score': 27, 'issue_id': 4163, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c1c7fcda2cc6ed7a', 'authors': ['Danil Sivtsov', 'Ivan Rodkin', 'Gleb Kuzmin', 'Yuri Kuratov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Moscow, Russia', 'FRC CSC RAS, Moscow, Russia', 'MBZUAI, Abu Dhabi, UAE', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia', 'Skoltech, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.05229.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#training', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение обработки длинных последовательностей в рекуррентных трансформерах', 'desc': 'Статья представляет новый метод планирования вычислений для рекуррентных моделей трансформеров (RMT), называемый Diagonal Batching. Этот подход позволяет распараллелить обработку сегментов в RMT, сохраняя при этом точную рекуррентность. Diagonal Batching устраняет ограничения последовательного выполнения, что значительно ускоряет вывод на GPU для длинных последовательностей. Применение этого метода к модели LLaMA-1B ARMT показало ускорение в 3.3 раза по сравнению со стандартной моделью LLaMA-1B и в 1.8 раза по сравнению с последовательной реализацией RMT на последовательностях длиной 131 072 токена.'}, 'en': {'title': 'Unlocking Parallelism for Efficient Long-Context Inference', 'desc': 'This paper addresses the limitations of Transformer models in handling long-context inference due to their high computational costs. It introduces Recurrent Memory Transformers (RMTs) that improve efficiency by reducing time complexity to linear and memory usage to constant. The authors propose a novel scheduling method called Diagonal Batching, which allows for parallel processing of segments in RMTs, overcoming the sequential execution bottleneck. By implementing this technique, they achieve significant speedups in inference times for long sequences, making RMTs more viable for practical applications.'}, 'zh': {'title': '对角批处理：提升长上下文推理效率的创新方案', 'desc': '本文介绍了一种新的调度方案，称为对角批处理（Diagonal Batching），旨在解决递归记忆变换器（RMTs）在长上下文推理中的性能瓶颈。传统的RMT由于其内存更新机制，导致了顺序执行，从而影响了性能。对角批处理通过在RMT中实现并行处理，消除了顺序限制，使得在单个长上下文输入上也能高效推理。该技术无需重新训练现有模型，应用于LLaMA-1B ARMT模型时，速度提升达3.3倍，显著降低了推理成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.05010', 'title': 'ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development', 'url': 'https://huggingface.co/papers/2506.05010', 'abstract': 'ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.', 'score': 26, 'issue_id': 4157, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '5d1aa2eb9189bc56', 'authors': ['Zhenran Xu', 'Xue Yang', 'Yiyu Wang', 'Qingli Hu', 'Zijiao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce', 'Harbin Institute of Technology (Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05010.jpg', 'data': {'categories': ['#multimodal', '#agents', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'ИИ-ассистент для творчества: ComfyUI-Copilot упрощает создание цифрового искусства', 'desc': 'ComfyUI-Copilot - это плагин на основе большой языковой модели, разработанный для улучшения удобства использования и эффективности платформы ComfyUI для создания искусства с помощью ИИ. Система использует иерархическую мультиагентную структуру с центральным агентом-ассистентом и специализированными рабочими агентами. ComfyUI-Copilot предлагает интеллектуальные рекомендации по узлам и моделям, а также автоматизированное построение рабочего процесса в один клик. Эффективность системы подтверждена как офлайн-оценками, так и отзывами пользователей, показывающими, что она точно рекомендует узлы и ускоряет разработку рабочих процессов.'}, 'en': {'title': 'Empowering Art Creation with Intelligent Assistance', 'desc': 'ComfyUI-Copilot is a plugin that leverages a large language model and a multi-agent system to improve the ComfyUI platform for AI art creation. It addresses common challenges faced by users, such as limited documentation and complex workflows, by providing intelligent recommendations and automating workflow construction. The system features a hierarchical structure with a central assistant agent that delegates tasks to specialized worker agents, enhancing usability and efficiency. Validation through user feedback and quantitative evaluations demonstrates that ComfyUI-Copilot effectively lowers barriers for beginners while streamlining processes for experienced users.'}, 'zh': {'title': '智能助手，轻松创作艺术', 'desc': 'ComfyUI-Copilot 是一个基于大型语言模型的插件，旨在提升 ComfyUI 这一开源 AI 艺术创作平台的可用性和效率。该系统通过提供智能节点和模型推荐，以及一键式自动化工作流构建，解决了新手用户在使用 ComfyUI 时面临的挑战。它采用了分层的多代理框架，中央助手代理负责任务分配，而专门的工作代理则处理不同的使用场景。通过离线定量评估和在线用户反馈，我们验证了 ComfyUI-Copilot 的有效性，显示其能够准确推荐节点并加速工作流开发。'}}}, {'id': 'https://huggingface.co/papers/2506.02865', 'title': 'Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights', 'url': 'https://huggingface.co/papers/2506.02865', 'abstract': 'Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.', 'score': 25, 'issue_id': 4162, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '18e38be0b8df6445', 'authors': ['Mathieu Andreux', 'Breno Baldas Skuk', 'Hamza Benchekroun', 'Emilien Biré', 'Antoine Bonnet', 'Riaz Bordie', 'Matthias Brunel', 'Pierre-Louis Cedoz', 'Antoine Chassang', 'Mickaël Chen', 'Alexandra D. Constantinou', "Antoine d'Andigné", 'Hubert de La Jonquière', 'Aurélien Delfosse', 'Ludovic Denoyer', 'Alexis Deprez', 'Augustin Derupti', 'Michael Eickenberg', 'Mathïs Federico', 'Charles Kantor', 'Xavier Koegler', 'Yann Labbé', 'Matthew C. H. Lee', 'Erwan Le Jumeau de Kergaradec', 'Amir Mahla', 'Avshalom Manevich', 'Adrien Maret', 'Charles Masson', 'Rafaël Maurin', 'Arturo Mena', 'Philippe Modard', 'Axel Moyal', 'Axel Nguyen Kerbel', 'Julien Revelle', 'Mats L. Richter', 'María Santos', 'Laurent Sifre', 'Maxime Theillard', 'Marc Thibault', 'Louis Thiry', 'Léo Tronchon', 'Nicolas Usunier', 'Tony Wu'], 'affiliations': ['Company'], 'pdf_title_img': 'assets/pdf/title_img/2506.02865.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#agents', '#data', '#benchmark'], 'emoji': '🏄', 'ru': {'title': 'Surfer-H и Holo1: Эффективная веб-навигация с помощью ИИ', 'desc': 'Статья представляет Surfer-H - эффективного веб-агента, использующего модели компьютерного зрения и обработки естественного языка (VLM) для выполнения задач в интернете. Surfer-H интегрирован с Holo1 - новым набором специализированных VLM для веб-навигации и извлечения информации. Holo1 показывает высокие результаты на общих тестах пользовательских интерфейсов и новом бенчмарке WebClick. В сочетании с Holo1, Surfer-H достигает 92.2% точности на тесте WebVoyager, оптимально балансируя между эффективностью и стоимостью.'}, 'en': {'title': 'Surfer-H: Efficient Web Navigation with Vision-Language Models', 'desc': 'The paper introduces Surfer-H, a web agent designed to efficiently navigate and perform tasks using Vision-Language Models (VLMs). It is paired with Holo1, a collection of open-weight VLMs that excel in web navigation and information extraction. Holo1 is trained on diverse data sources, ensuring high performance on various benchmarks, including a new web UI localization benchmark called WebClick. Surfer-H achieves impressive results, reaching 92.2% accuracy on the WebVoyager task, while maintaining cost-efficiency, and both the evaluation dataset and model weights are made available for further research.'}, 'zh': {'title': '高效网络代理，智能导航新选择', 'desc': 'Surfer-H是一种高性价比的网络代理，结合了视觉-语言模型（VLM）来执行用户定义的网络任务。它与Holo1配对，Holo1是一个新的开放权重VLM集合，专注于网络导航和信息提取。Holo1在经过精心策划的数据源上训练，包括开放访问的网络内容和合成示例，表现出色，尤其是在用户界面基准测试中。通过Holo1，Surfer-H在WebVoyager上达到了92.2%的最佳性能，实现了准确性和成本效率的帕累托最优平衡。'}}}, {'id': 'https://huggingface.co/papers/2505.23656', 'title': 'VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models', 'url': 'https://huggingface.co/papers/2505.23656', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.', 'score': 21, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'aa7bbc7378df2b3e', 'authors': ['Xiangdong Zhang', 'Jiaqi Liao', 'Shaofeng Zhang', 'Fanqing Meng', 'Xiangpeng Wan', 'Junchi Yan', 'Yu Cheng'], 'affiliations': ['Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University', 'NetMind.AI', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.23656.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Физически достоверное видео из текста: VideoREPA улучшает понимание физики в моделях T2V', 'desc': 'В статье представлен новый метод VideoREPA для улучшения физической достоверности видео, генерируемых моделями text-to-video (T2V). Авторы предлагают дистиллировать понимание физики из моделей видеопонимания в модели T2V путем выравнивания отношений на уровне токенов. Ключевым элементом метода является функция потерь Token Relation Distillation (TRD), которая обеспечивает мягкое руководство для дообучения предобученных моделей T2V. Эмпирические оценки показывают, что VideoREPA значительно улучшает физический здравый смысл базового метода CogVideoX.'}, 'en': {'title': 'Bridging the Physics Gap in Text-to-Video Generation', 'desc': 'This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation.'}, 'zh': {'title': 'VideoREPA：提升文本到视频模型的物理理解能力', 'desc': '最近，文本到视频（T2V）扩散模型的进展使得高保真和真实的视频合成成为可能。然而，当前的T2V模型在生成物理上合理的内容方面常常面临挑战，因为它们对物理的理解能力有限。我们提出了一种新框架，称为VideoREPA，通过对齐令牌级关系，将视频理解基础模型中的物理理解能力提炼到T2V模型中，从而缩小物理理解的差距。我们的实验表明，VideoREPA显著增强了基线方法CogVideoX的物理常识，能够生成与直观物理一致的视频。'}}}, {'id': 'https://huggingface.co/papers/2506.05176', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models', 'url': 'https://huggingface.co/papers/2506.05176', 'abstract': "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.", 'score': 18, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '90ebf52dd91334c2', 'authors': ['Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Xin Zhang', 'Huan Lin', 'Baosong Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Junyang Lin', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05176.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#multilingual', '#open_source', '#training', '#small_models', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Qwen3 Embedding: Новый стандарт многоязычных текстовых эмбеддингов', 'desc': 'В работе представлена серия моделей Qwen3 Embedding, улучшающая возможности текстовых эмбеддингов и ранжирования по сравнению с предшественником GTE-Qwen. Модели основаны на фундаментальных моделях Qwen3 и используют многоэтапный процесс обучения, включающий масштабную предварительную подготовку и тонкую настройку на качественных датасетах. Qwen3 Embedding предлагает модели различных размеров (0.6B, 4B, 8B) для задач эмбеддинга и ранжирования, что позволяет оптимизировать их под разные сценарии применения. Эмпирические оценки показывают, что серия Qwen3 Embedding достигает передовых результатов в различных бенчмарках, особенно в многоязычной оценке MTEB и задачах поиска.'}, 'en': {'title': 'Empowering Multilingual Text Understanding with Qwen3 Embeddings', 'desc': 'The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research.'}, 'zh': {'title': 'Qwen3嵌入系列：多语言文本处理的新突破', 'desc': '本文介绍了Qwen3嵌入系列，这是在文本嵌入和重排序能力上相较于GTE-Qwen系列的重大进展。该系列基于Qwen3基础模型，利用其强大的多语言文本理解和生成能力，采用多阶段训练流程，结合大规模无监督预训练和高质量数据集的监督微调。通过有效的模型合并策略，Qwen3嵌入系列确保了模型的鲁棒性和适应性，提供了多种模型规模以满足不同的部署场景。实证评估表明，Qwen3嵌入系列在多项基准测试中取得了最先进的结果，尤其在多语言评估基准MTEB上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.05240', 'title': 'Aligning Latent Spaces with Flow Priors', 'url': 'https://huggingface.co/papers/2506.05240', 'abstract': 'This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.', 'score': 17, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c776325b1f9f6966', 'authors': ['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05240.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#cv', '#math', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Выравнивание латентных пространств с помощью потоковых моделей', 'desc': 'Статья представляет новый подход к выравниванию обучаемых латентных пространств с произвольными целевыми распределениями, используя генеративные модели на основе потоков в качестве априорных. Метод сначала предобучает модель потока на целевых признаках, а затем использует ее для регуляризации латентного пространства через функцию потерь выравнивания. Авторы доказывают, что минимизация этой функции потерь является вычислительно эффективным суррогатом для максимизации вариационной нижней границы правдоподобия латентов. Эффективность подхода подтверждается экспериментами по генерации изображений на ImageNet с различными целевыми распределениями.'}, 'en': {'title': 'Aligning Latent Spaces with Flow-Based Models', 'desc': "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."}, 'zh': {'title': '潜在空间对齐的新方法', 'desc': '本文提出了一种新颖的框架，通过利用基于流的生成模型作为先验，将可学习的潜在空间对齐到任意目标分布。我们的方法首先在目标特征上预训练流模型，以捕捉潜在分布。然后，这个固定的流模型通过对齐损失来规范化潜在空间，重新构造流匹配目标，将潜在变量视为优化目标。我们正式证明，最小化这个对齐损失建立了一个计算上可处理的替代目标，用于最大化潜在变量在目标分布下的变分下界的对数似然。'}}}, {'id': 'https://huggingface.co/papers/2506.05328', 'title': 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs', 'url': 'https://huggingface.co/papers/2506.05328', 'abstract': "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.", 'score': 16, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '774ccf3fd01aa4ef', 'authors': ['Lidong Lu', 'Guo Chen', 'Zhiqi Li', 'Yicheng Liu', 'Tong Lu'], 'affiliations': ['Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#reasoning', '#dataset', '#long_context', '#training', '#video', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Продвинутый подсчет объектов в видео с помощью мультимодального ИИ', 'desc': 'Статья представляет новый бенчмарк CG-AV-Counting для задач подсчета объектов в длинных видео с использованием мультимодальных вопросов и аннотированных подсказок. Авторы предлагают модель AV-Reasoner, обученную с помощью обучения с подкреплением и куррикулярного обучения для улучшения способности подсчета. Модель достигает лучших результатов на нескольких бенчмарках, демонстрируя эффективность обучения с подкреплением. Однако эксперименты показывают, что рассуждения в языковом пространстве не приносят улучшений на бенчмарках вне домена обучения.'}, 'en': {'title': 'Enhancing Video Counting with CG-AV-Counting and AV-Reasoner', 'desc': 'This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts.'}, 'zh': {'title': '提升视频计数能力的新基准与模型', 'desc': '尽管视频理解取得了进展，但当前的多模态学习模型在计数任务上仍然存在困难。现有的基准测试受限于短视频、封闭式查询、缺乏线索注释和多模态覆盖不足。本文介绍了CG-AV-Counting，这是一个手动注释的线索基础计数基准，包含1,027个多模态问题和5,845个注释线索，覆盖497个长视频。我们提出的AV-Reasoner模型通过GRPO和课程学习进行训练，能够从相关任务中推广计数能力，并在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2506.04633', 'title': 'Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations', 'url': 'https://huggingface.co/papers/2506.04633', 'abstract': 'Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.', 'score': 15, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '05d0ae7d805419c1', 'authors': ['Linjie Li', 'Mahtab Bigverdi', 'Jiawei Gu', 'Zixian Ma', 'Yinuo Yang', 'Ziang Li', 'Yejin Choi', 'Ranjay Krishna'], 'affiliations': ['Stanford University', 'Sun Yat-sen University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04633.jpg', 'data': {'categories': ['#multimodal', '#3d', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'STARE: Новый рубеж в оценке пространственного интеллекта ИИ', 'desc': 'Статья представляет новый бенчмарк STARE для оценки мультимодальных языковых моделей в задачах пространственного мышления и визуального моделирования. Бенчмарк включает 4000 заданий на геометрические преобразования, пространственное мышление и реальные пространственные задачи. Результаты показывают, что модели хорошо справляются с простыми 2D-преобразованиями, но близки к случайному угадыванию в сложных задачах, требующих многошаговых визуальных симуляций. Люди демонстрируют почти идеальную точность, но тратят значительное время на сложные задачи, существенно ускоряясь при использовании промежуточных визуальных симуляций.'}, 'en': {'title': 'STARE: Bridging the Gap in Spatial Reasoning for AI', 'desc': 'This paper presents STARE, a new benchmark for evaluating multimodal large language models on spatial reasoning tasks that require visual simulations. Unlike existing benchmarks that focus on verbal reasoning, STARE includes 4,000 tasks involving geometric transformations and real-world spatial challenges. The results show that while models perform well on simple 2D tasks, they struggle with complex 3D tasks that require multi-step reasoning. This indicates that current models have difficulty effectively using visual information to enhance their reasoning capabilities.'}, 'zh': {'title': 'STARE：评估空间推理的新基准', 'desc': '空间认知对人类智能至关重要，它使我们能够通过视觉模拟解决问题，而不仅仅依赖语言推理。现有的人工智能基准主要评估语言推理，忽视了非语言多步骤视觉模拟的复杂性。我们提出了STARE（空间变换与推理评估），这是一个旨在严格评估多模态大型语言模型在多步骤视觉模拟任务上的基准。评估结果显示，模型在简单的2D变换上表现良好，但在更复杂的任务上，如3D立方体展开和拼图，表现接近随机，表明模型可能无法有效利用中间视觉信息。'}}}, {'id': 'https://huggingface.co/papers/2506.05344', 'title': 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs', 'url': 'https://huggingface.co/papers/2506.05344', 'abstract': 'Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.', 'score': 14, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '0175b3788ebacf29', 'authors': ['Jiahui Wang', 'Zuyan Liu', 'Yongming Rao', 'Jiwen Lu'], 'affiliations': ['Tencent Hunyuan Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05344.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#architecture', '#open_source', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зрение: оптимизация визуального восприятия в мультимодальных ИИ', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) используют лишь небольшую часть (менее 5%) механизмов внимания для обработки визуальной информации. Авторы разработали метод SparseMM для оптимизации KV-кэша, который распределяет вычислительные ресурсы асимметрично, основываясь на визуальной релевантности головок внимания. Этот подход позволяет ускорить вывод MLLM в 1,38 раза и сократить использование памяти на 52% при сохранении производительности. Метод SparseMM показывает лучшее соотношение точности и эффективности по сравнению с существующими методами оптимизации KV-кэша.'}, 'en': {'title': 'Optimizing Visual Understanding in MLLMs with Sparse Attention', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks.'}, 'zh': {'title': '利用稀疏视觉头加速多模态模型推理', 'desc': '多模态大型语言模型（MLLMs）通过扩展预训练的大型语言模型（LLMs）来增加视觉能力。我们研究了MLLMs如何处理视觉输入，分析了它们的注意力机制。我们发现了一个惊人的稀疏现象：在LLMs中，只有少量（大约5%以下）的注意力头积极参与视觉理解，这些被称为视觉头。基于这一发现，我们提出了SparseMM，一种KV-Cache优化策略，根据视觉得分为LLMs中的头分配不对称的计算预算，从而加速MLLMs的推理。'}}}, {'id': 'https://huggingface.co/papers/2506.03077', 'title': 'StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs', 'url': 'https://huggingface.co/papers/2506.03077', 'abstract': "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  \t\t\t\t\tAI-generated summary \t\t\t\t Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.", 'score': 14, 'issue_id': 4156, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '14c578e79a6d095c', 'authors': ['Qijun Luo', 'Mengqi Li', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03077.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'StreamBP: Революция в обучении языковых моделей', 'desc': 'StreamBP - это эффективный метод обратного распространения ошибки для обучения языковых моделей. Он разлагает правило цепочки вдоль последовательности, что значительно снижает затраты памяти на хранение активаций и логитов. StreamBP позволяет обрабатывать более длинные последовательности и ускоряет обучение по сравнению с методом контрольных точек градиента. Метод применим к различным целевым функциям и эффективно масштабируется на несколько GPU.'}, 'en': {'title': 'StreamBP: Efficient Backpropagation for Long Sequences in Language Models', 'desc': 'StreamBP is a novel backpropagation method designed to efficiently handle long sequence lengths in language models. By decomposing the chain rule in a layer-wise manner, it significantly reduces the memory required for storing activation values during training. This method not only speeds up the backpropagation process but also allows for longer sequences compared to traditional gradient checkpointing techniques. Additionally, StreamBP can be easily integrated into existing transformer training pipelines and supports multi-GPU setups for enhanced performance.'}, 'zh': {'title': 'StreamBP：高效反向传播，助力长序列训练', 'desc': 'StreamBP是一种高效的反向传播方法，通过对链式法则进行线性分解，显著降低了内存消耗。这使得在训练语言模型时，可以处理更长的序列并加快训练速度。与传统的梯度检查点技术相比，StreamBP能够将反向传播的最大序列长度提高2.8到5.5倍，同时保持相似或更少的反向传播时间。此外，StreamBP还支持多GPU训练，增强了其在实际应用中的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2506.05331', 'title': 'MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.05331', 'abstract': 'Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT', 'score': 12, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b3e1648a048ac6b7', 'authors': ['Xinyan Chen', 'Renrui Zhang', 'Dongzhi Jiang', 'Aojun Zhou', 'Shilin Yan', 'Weifeng Lin', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05331.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#reasoning', '#math', '#games'], 'emoji': '🧮', 'ru': {'title': 'Визуальное рассуждение в математике: новый уровень с MINT-CoT', 'desc': 'Статья представляет MINT-CoT - новый подход к визуальному рассуждению в математических задачах с использованием цепочки размышлений (Chain-of-Thought). Метод адаптивно внедряет релевантные визуальные токены в текстовые шаги рассуждения, динамически выбирая визуальные области любой формы в математических фигурах. Авторы создали датасет MINT-CoT, содержащий 54 тысячи математических задач, где каждый шаг рассуждения согласован с визуальными областями на уровне токенов. Разработанная модель MINT-CoT-7B значительно превосходит базовые модели на нескольких бенчмарках математического визуального рассуждения.'}, 'en': {'title': 'Enhancing Math Reasoning with Visual Interleaving in LLMs', 'desc': 'This paper introduces MINT-CoT, a novel approach to enhance mathematical reasoning in Large Language Models (LLMs) by integrating visual information more effectively. The method uses Mathematical Interleaved Tokens to dynamically select and incorporate relevant visual regions into the reasoning process, overcoming limitations of previous models that relied on fixed image regions. The authors created a dataset with 54,000 math problems that align visual tokens with reasoning steps, enabling better training of the model. Experimental results show that MINT-CoT-7B significantly outperforms existing models in various math problem-solving tasks, demonstrating its effectiveness in multimodal reasoning.'}, 'zh': {'title': 'MINT-CoT：数学推理的新突破', 'desc': '本论文提出了一种新的方法MINT-CoT，用于在多模态领域中增强大型语言模型的数学推理能力。MINT-CoT通过引入数学交错标记，将相关的视觉信息动态地融入文本推理步骤中，从而解决了传统方法在数学问题解决中的局限性。我们构建了一个包含54K数学问题的数据集，确保每个推理步骤与视觉区域在标记级别上对齐。实验结果表明，MINT-CoT-7B模型在多个数学任务上显著优于基线模型，展示了其在视觉交错推理中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.05349', 'title': 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos', 'url': 'https://huggingface.co/papers/2506.05349', 'abstract': "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA", 'score': 11, 'issue_id': 4159, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '7f9f8448e60cfdb4', 'authors': ['Hanoona Rasheed', 'Abdelrahman Shaker', 'Anqi Tang', 'Muhammad Maaz', 'Ming-Hsuan Yang', 'Salman Khan', 'Fahad Khan'], 'affiliations': ['Australian National University', 'Google Research', 'Linköping University', 'MBZUAI', 'University of California Merced'], 'pdf_title_img': 'assets/pdf/title_img/2506.05349.jpg', 'data': {'categories': ['#math', '#multimodal', '#benchmark', '#transfer_learning', '#reasoning', '#video'], 'emoji': '🧮', 'ru': {'title': 'VideoMathQA: Новый рубеж в оценке математических рассуждений ИИ на основе видео', 'desc': 'VideoMathQA - это новый бенчмарк для оценки способности моделей выполнять кросс-модальные рассуждения в математических задачах на основе видео. Он охватывает 10 различных математических областей и включает видео продолжительностью от 10 секунд до более чем часа. Бенчмарк оценивает три ключевых навыка: прямое решение задач, концептуальный перенос и глубокое понимание инструкций. VideoMathQA демонстрирует ограничения существующих подходов и устанавливает систематическую основу для оценки моделей, способных рассуждать в математических задачах с временной протяженностью и мультимодальным контекстом.'}, 'en': {'title': 'VideoMathQA: Advancing Cross-Modal Reasoning in Mathematics', 'desc': 'VideoMathQA is a benchmark that tests how well models can reason about mathematics in videos, which is more complex than in static images or text. It focuses on understanding visual information, reading text, and integrating spoken cues that are often spread out over time. The benchmark includes questions that require direct problem solving, applying learned concepts to new situations, and understanding detailed instructions. By analyzing model performance on these tasks, VideoMathQA aims to identify the strengths and weaknesses of current approaches in handling complex, multimodal reasoning in mathematical contexts.'}, 'zh': {'title': '视频数学推理的新挑战', 'desc': 'VideoMathQA 是一个评估模型在视频环境中进行跨模态推理能力的基准，特别是在数学领域。它要求模型能够理解复杂的视觉信息、手写或数字文本，并整合分散的语音提示。该基准涵盖了10个不同的数学领域，问题设计围绕直接问题解决、概念转移和深度教学理解三个核心挑战。通过这个基准，我们揭示了现有方法的局限性，并建立了一个系统的评估框架，以测试模型在复杂的数学问题设置中的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05345', 'title': 'Inference-Time Hyper-Scaling with KV Cache Compression', 'url': 'https://huggingface.co/papers/2506.05345', 'abstract': 'Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8times compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets.', 'score': 11, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '172bab27eb20c036', 'authors': ['Adrian Łańcucki', 'Konrad Staniszewski', 'Piotr Nawrot', 'Edoardo M. Ponti'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.05345.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Больше токенов, выше точность: сжатие памяти для эффективного вывода языковых моделей', 'desc': 'Статья представляет метод динамического разреживания памяти (Dynamic Memory Sparsification, DMS) для сжатия кэша ключ-значение в трансформерных моделях большого языка. DMS позволяет генерировать больше токенов при том же объеме вычислений, что повышает точность вывода. Метод требует всего 1000 шагов обучения для достижения 8-кратного сжатия, сохраняя при этом лучшую точность, чем методы разреженного внимания без обучения. Эксперименты показывают значительное улучшение результатов на различных наборах данных для нескольких семейств языковых моделей.'}, 'en': {'title': 'Boosting Token Generation with Dynamic Memory Sparsification', 'desc': 'This paper presents a method called Dynamic Memory Sparsification (DMS) to improve the efficiency of Transformer large language models (LLMs) during inference. By compressing the key-value (KV) cache, DMS allows for the generation of more tokens without increasing the computational cost, thus enhancing reasoning accuracy. The method achieves significant compression while preserving important information, which is crucial for maintaining model performance. The results show that DMS can boost accuracy across various LLMs while keeping the inference runtime and memory usage stable.'}, 'zh': {'title': '动态内存稀疏化：推理时的超规模扩展', 'desc': '本文提出了一种在推理时进行超规模扩展的方法，利用动态内存稀疏化技术来压缩键值缓存，从而在相同的计算预算内生成更多的令牌，并提高推理的准确性。传统的推理扩展往往在效率与推理准确性之间进行权衡，而我们的研究表明，推理成本主要受限于键值缓存的大小。通过压缩键值缓存，我们能够在不增加计算负担的情况下，生成更长或更多的并行序列。我们的方法在多个大型语言模型上验证了其有效性，显示出在相似的推理运行时间和内存负载下，准确性得到了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.05287', 'title': 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?', 'url': 'https://huggingface.co/papers/2506.05287', 'abstract': "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.", 'score': 11, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '8b357b07e4b449cd', 'authors': ['Yuqian Yuan', 'Ronghao Dang', 'Long Li', 'Wentong Li', 'Dian Jiao', 'Xin Li', 'Deli Zhao', 'Fan Wang', 'Wenqiao Zhang', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05287.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#interpretability', '#benchmark', '#games'], 'emoji': '👁️', 'ru': {'title': 'EOC-Bench: Новый стандарт для оценки когнитивных способностей ИИ в эгоцентрическом зрении', 'desc': 'Статья представляет новый бенчмарк EOC-Bench для оценки понимания объектов в динамических эгоцентрических сценариях мультимодальными языковыми моделями. EOC-Bench включает 3277 аннотированных пар вопрос-ответ, охватывающих три временные категории и 11 измерений оценки. Авторы разработали систему аннотаций с участием человека и новую метрику для оценки временной точности. EOC-Bench позволяет комплексно оценивать возможности мультимодальных языковых моделей в понимании объектов в воплощенных системах.'}, 'en': {'title': 'EOC-Bench: Advancing Object Cognition in Dynamic Environments', 'desc': "This paper introduces EOC-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) in dynamic environments where users interact with objects. Unlike previous benchmarks that focus on static scenes, EOC-Bench assesses how well models understand objects in changing contexts by using 3,277 annotated question-answer pairs across different time frames. The benchmark includes a unique human-in-the-loop annotation framework and a multi-scale temporal accuracy metric to evaluate models' performance in real-time interactions. By providing a comprehensive evaluation tool, EOC-Bench aims to enhance the cognitive abilities of MLLMs in embodied applications."}, 'zh': {'title': '动态场景中的物体认知评估新基准', 'desc': '多模态大型语言模型（MLLMs）的出现推动了以自我为中心的视觉应用的突破。这些应用需要对物体进行持续的、上下文感知的理解，因为用户在动态和杂乱的环境中与工具互动。然而，现有的体现基准主要集中在静态场景探索上，强调物体的外观和空间属性，而忽视了用户互动所带来的动态变化评估。为了解决这个问题，我们引入了EOC-Bench，这是一个创新的基准，旨在系统地评估动态自我中心场景中的以物体为中心的体现认知。'}}}, {'id': 'https://huggingface.co/papers/2506.05327', 'title': 'Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.05327', 'abstract': 'Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss', 'score': 10, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '374ddd58ce0653c6', 'authors': ['Duochao Shi', 'Weijie Wang', 'Donny Y. Chen', 'Zeyu Zhang', 'Jia-Wang Bian', 'Bohan Zhuang', 'Chunhua Shen'], 'affiliations': ['GigaAI', 'MBZUAI', 'Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.05327.jpg', 'data': {'categories': ['#optimization', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение 3D-реконструкции с помощью умной регуляризации границ объектов', 'desc': 'Статья представляет новый метод регуляризации для улучшения качества 3D-реконструкции в системах 3D Gaussian Splatting. Авторы предлагают PM-Loss - функцию потерь, основанную на предсказании карты точек с помощью предобученного трансформера. Этот подход позволяет сгладить геометрические разрывы на границах объектов, которые часто возникают при использовании карт глубины. Применение PM-Loss значительно улучшает качество рендеринга для различных архитектур и сцен.'}, 'en': {'title': 'Enhancing 3D Rendering with PM-Loss for Smooth Depth Representation', 'desc': 'This paper presents PM-Loss, a new regularization technique designed to enhance the quality of 3D point clouds generated from depth maps in 3D Gaussian Splatting (3DGS) systems. The authors address the common issue of depth discontinuities at object boundaries, which can lead to poor rendering quality due to fragmented point clouds. By utilizing a pointmap predicted by a pre-trained transformer, PM-Loss promotes geometric smoothness, improving the overall accuracy of the depth representation. The results demonstrate that incorporating PM-Loss leads to superior rendering outcomes across different architectures and scenes.'}, 'zh': {'title': '提升3D渲染质量的新方法', 'desc': '本文提出了一种新的正则化损失函数PM-Loss，用于改善基于深度图的3D高斯点云渲染。传统方法在物体边界处的深度不连续性会导致点云稀疏，从而影响渲染质量。PM-Loss利用预训练的变换器预测的点图，尽管其准确性不如深度图，但能有效增强几何平滑性。通过改进深度图，我们的方法在不同架构和场景中显著提升了3D高斯点云的渲染效果。'}}}, {'id': 'https://huggingface.co/papers/2506.05209', 'title': 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text', 'url': 'https://huggingface.co/papers/2506.05209', 'abstract': 'The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.', 'score': 10, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '18ac0c75007ddff5', 'authors': ['Nikhil Kandpal', 'Brian Lester', 'Colin Raffel', 'Sebastian Majstorovic', 'Stella Biderman', 'Baber Abbasi', 'Luca Soldaini', 'Enrico Shippole', 'A. Feder Cooper', 'Aviya Skowron', 'John Kirchenbauer', 'Shayne Longpre', 'Lintang Sutawika', 'Alon Albalak', 'Zhenlin Xu', 'Guilherme Penedo', 'Loubna Ben Allal', 'Elie Bakouch', 'John David Pressman', 'Honglu Fan', 'Dashiell Stander', 'Guangyu Song', 'Aaron Gokaslan', 'Tom Goldstein', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tyler Murray'], 'affiliations': ['CMU', 'Cornell University', 'EleutherAI', 'Hugging Face', 'Independent', 'Lawrence Livermore National', 'Lila Sciences', 'MIT', 'Teraflop AI', 'The Allen Institute for', 'University of Maryland, College Park', 'University of Toronto Artificial Intelligence', 'Vector Institute', 'poolside'], 'pdf_title_img': 'assets/pdf/title_img/2506.05209.jpg', 'data': {'categories': ['#dataset', '#ethics', '#open_source', '#data'], 'emoji': '📚', 'ru': {'title': 'Открытые данные для этичного ИИ', 'desc': 'Исследователи представили набор данных Common Pile v0.1 - 8-терабайтную коллекцию текстов с открытой лицензией для обучения языковых моделей. На основе этих данных были обучены две модели Comma v0.1 с 7 миллиардами параметров, показавшие результаты на уровне аналогичных моделей, обученных на нелицензированных текстах. Набор данных включает разнообразные источники: научные статьи, код, книги, энциклопедии и другие. Это первый шаг к решению этических проблем и вопросов интеллектуальной собственности при обучении больших языковых моделей.'}, 'en': {'title': 'Openly Licensed Text for Competitive LLM Training', 'desc': "The paper introduces the Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text aimed at training large language models (LLMs). This dataset addresses ethical concerns related to using unlicensed text by providing a high-quality, diverse source of data from various domains. The authors validate the dataset's effectiveness by training two competitive 7 billion parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, which demonstrate performance comparable to LLMs trained on unlicensed data. Additionally, the paper includes the release of the dataset, training code, and model checkpoints to support further research."}, 'zh': {'title': '开放许可文本助力大型语言模型的训练', 'desc': 'Common Pile v0.1 数据集是一个包含 8TB 开放许可文本的集合，旨在训练具有 70 亿参数的竞争性大型语言模型（LLM）。该数据集汇集了来自 30 个不同领域的内容，包括研究论文、代码、书籍、百科全书和教育材料等。通过在 Common Pile 上训练的 Comma v0.1-1T 和 Comma v0.1-2T 模型，验证了该数据集的有效性，这两个模型在性能上与使用未授权文本训练的 LLM 相当。此研究为解决知识产权和伦理问题提供了一个重要的第一步。'}}}, {'id': 'https://huggingface.co/papers/2506.02620', 'title': 'FlexPainter: Flexible and Multi-View Consistent Texture Generation', 'url': 'https://huggingface.co/papers/2506.02620', 'abstract': 'FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  \t\t\t\t\tAI-generated summary \t\t\t\t Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.', 'score': 9, 'issue_id': 4157, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '296d6abe1e32cfa9', 'authors': ['Dongyu Yan', 'Leyi Wu', 'Jiantao Lin', 'Luozhou Wang', 'Tianshuo Xu', 'Zhifei Chen', 'Zhen Yang', 'Lie Xu', 'Shunsi Zhang', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Quwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.02620.jpg', 'data': {'categories': ['#multimodal', '#3d', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'FlexPainter: гибкая и согласованная генерация текстур с мультимодальным управлением', 'desc': 'FlexPainter - это новый конвейер генерации текстур, использующий общее условное пространство вложений для гибкого мультимодального управления. Он обеспечивает высококачественную и согласованную генерацию карт текстур с помощью диффузионных моделей изображений и 3D-ориентированной модели. FlexPainter решает проблемы ограниченной гибкости управления и несогласованности между сгенерированными многоракурсными изображениями. Система включает метод декомпозиции структурной и стилевой информации, а также модуль синхронизации ракурсов для обеспечения локальной согласованности.'}, 'en': {'title': 'FlexPainter: Revolutionizing Texture Generation with Multi-Modal Guidance', 'desc': 'FlexPainter is a new pipeline designed for generating high-quality texture maps in 3D modeling. It utilizes a shared conditional embedding space to allow for flexible multi-modal guidance, which helps in producing consistent textures from various input types. By employing an image diffusion prior and a 3D-aware model, it generates multi-view images that maintain local consistency and enhance overall quality. The framework has been shown to outperform existing methods in terms of both flexibility and the quality of the generated textures.'}, 'zh': {'title': 'FlexPainter：灵活高效的纹理生成新方法', 'desc': 'FlexPainter是一种新型的纹理生成管道，利用共享的条件嵌入空间实现灵活的多模态引导，从而确保高质量和一致性的纹理图生成。该方法结合了图像扩散先验和3D感知模型，解决了传统方法在控制灵活性和提示模态方面的限制。通过构建共享的条件嵌入空间，FlexPainter能够在不同输入模态之间进行灵活聚合，提升生成效果。实验结果表明，FlexPainter在灵活性和生成质量上显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.04209', 'title': 'Language-Image Alignment with Fixed Text Encoders', 'url': 'https://huggingface.co/papers/2506.04209', 'abstract': 'Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.', 'score': 8, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '921137445b3be92e', 'authors': ['Jingfeng Yang', 'Ziyang Wu', 'Yue Zhao', 'Yi Ma'], 'affiliations': ['The University of Hong Kong', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.04209.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#long_context', '#alignment', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Фиксированный языковой энкодер улучшает визуальное обучение', 'desc': 'Исследование LIFT предлагает новый подход к обучению визуальных представлений с использованием предобученных языковых моделей (LLM) в качестве фиксированного текстового энкодера. Этот метод превосходит совместное обучение текстовых и визуальных энкодеров, как в CLIP, особенно в задачах композиционного понимания и работы с длинными подписями. LIFT демонстрирует высокую эффективность и вычислительную эффективность, обучая только визуальный энкодер. Результаты исследования открывают новые перспективы использования текстовых эмбеддингов из LLM для улучшения визуального обучения.'}, 'en': {'title': 'LIFT: Efficient Language-Image Alignment with Fixed Text Encoders', 'desc': 'This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment.'}, 'zh': {'title': '简化训练，提升视觉理解的LIFT方法', 'desc': '本文提出了一种新的方法，称为LIFT（使用固定文本编码器的语言-图像对齐），旨在通过预训练的大型语言模型来指导视觉表示学习。与传统的联合训练方法（如CLIP）相比，LIFT只训练图像编码器，而使用固定的文本编码器，从而简化了训练过程。研究表明，LIFT在处理组合理解和长文本描述时，表现优于CLIP，并且在计算效率上也有显著提升。该研究为如何利用大型语言模型的文本嵌入来指导视觉学习提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2506.01011', 'title': 'Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack', 'url': 'https://huggingface.co/papers/2506.01011', 'abstract': 'A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.', 'score': 7, 'issue_id': 4158, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '5475ba18f16db6f7', 'authors': ['Siqi Hui', 'Yiren Song', 'Sanping Zhou', 'Ye Deng', 'Wenli Huang', 'Jinjun Wang'], 'affiliations': ['National University of Singapore', 'Ningbo University of Technology', 'Southwestern University of Finance and Economics', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01011.jpg', 'data': {'categories': ['#cv', '#security', '#video'], 'emoji': '🔐', 'ru': {'title': 'Защита авторегрессионных моделей генерации изображений с помощью лексических водяных знаков', 'desc': "Статья представляет новую технику водяных знаков для авторегрессионных моделей генерации изображений - Lexical Bias Watermarking (LBW). LBW встраивает водяные знаки непосредственно в карты токенов, смещая выбор токенов в сторону предопределенного 'зеленого списка' во время генерации. Этот подход обеспечивает бесшовную интеграцию с существующими авторегрессионными моделями и естественно расширяется до постфактумной вставки водяных знаков. Эксперименты показывают, что LBW достигает превосходной устойчивости водяных знаков, особенно в противостоянии атакам регенерации."}, 'en': {'title': 'Secure Your Images with Lexical Bias Watermarking!', 'desc': 'The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.'}, 'zh': {'title': '增强自回归模型安全性的水印技术', 'desc': '本文提出了一种新颖的水印技术，称为词汇偏置水印（Lexical Bias Watermarking），旨在增强自回归图像生成模型的安全性。该方法通过在生成过程中偏向选择预定义的绿色列表，将水印嵌入到令牌选择中，从而有效抵御再生攻击。与现有的扩散模型水印技术不同，LBW能够直接与自回归模型集成，并支持后期水印处理。实验结果表明，LBW在抵抗再生攻击方面表现出色，显著提高了水印的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.04734', 'title': 'Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design', 'url': 'https://huggingface.co/papers/2506.04734', 'abstract': 'Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'ce810e0cc38b26e4', 'authors': ['Lin Sun', 'Weihong Lin', 'Jinzhu Wu', 'Yongfu Zhu', 'Xiaoqi Jian', 'Guangxiang Zhao', 'Change Jia', 'Linglin Zhang', 'Sai-er Hu', 'Yuhan Wu', 'Xiangzheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.04734.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': '🎢', 'ru': {'title': 'Нестабильность оценок: вызов для бенчмаркинга языковых моделей', 'desc': 'Исследование показывает, что результаты оценки моделей серии Deepseek-R1-Distill подвержены значительным колебаниям из-за различных факторов. Небольшие изменения в условиях оценки могут привести к существенным различиям в результатах. Аналогичные явления наблюдаются и в других моделях, основанных на Deepseek-R1-Distill, а также в модели QwQ-32B. Авторы призывают к созданию более строгой парадигмы оценки производительности моделей машинного обучения.'}, 'en': {'title': 'Ensuring Reliable Evaluations for Deep Learning Models', 'desc': 'The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results.'}, 'zh': {'title': '建立更严格的模型评估标准', 'desc': 'Deepseek-R1-Distill系列模型在数学、科学和编程等领域表现出色，受到开源社区的广泛采用。然而，我们的研究发现，这些模型的基准评估结果受到多种因素的显著波动影响。评估条件的细微差异可能导致结果的重大变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型中，因此我们呼吁建立更严格的模型性能评估范式。'}}}, {'id': 'https://huggingface.co/papers/2506.04405', 'title': 'MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale', 'url': 'https://huggingface.co/papers/2506.04405', 'abstract': 'We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '2cf822e179634776', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Yishan Zhong', 'Yue Yu', 'Xiangru Tang', 'Hang Wu', 'May D. Wang', 'Peifeng Ruan', 'Donghan Yang', 'Tao Wang', 'Guanghua Xiao', 'Carl Yang', 'Yang Xie', 'Wenqi Shi'], 'affiliations': ['Emory University', 'Georgia Tech', 'UT Southwestern Medical Center', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04405.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#open_source', '#training', '#rl'], 'emoji': '🩺', 'ru': {'title': 'MedAgentGYM: Революция в обучении ИИ для медицинского кодирования', 'desc': 'MedAgentGYM - это новая среда обучения для улучшения навыков медицинского рассуждения у агентов на основе больших языковых моделей (LLM). Она включает более 72 тысяч задач из 129 категорий, основанных на реальных биомедицинских сценариях. Задачи представлены в виде исполняемых кодовых сред с подробными описаниями, интерактивной обратной связью и верифицируемыми аннотациями. Используя MedAgentGYM, модель Med-Copilot-7B достигла значительного улучшения производительности через тонкую настройку и обучение с подкреплением.'}, 'en': {'title': 'Empowering Medical Reasoning in LLMs with MedAgentGYM', 'desc': 'MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o.'}, 'zh': {'title': 'MedAgentGYM：提升医学推理能力的创新平台', 'desc': '我们介绍了MedAgentGYM，这是第一个公开可用的训练环境，旨在增强大型语言模型（LLM）代理的基于编码的医学推理能力。MedAgentGYM包含72,413个任务实例，涵盖129个类别，来源于真实的生物医学场景。每个任务都在可执行的编码环境中封装，提供详细的任务描述、互动反馈机制、可验证的真实注释和可扩展的训练轨迹生成。通过对30多种LLM的广泛基准测试，发现商业API模型与开源模型之间存在显著的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2505.20914', 'title': 'Geometry-Editable and Appearance-Preserving Object Compositon', 'url': 'https://huggingface.co/papers/2505.20914', 'abstract': 'General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.', 'score': 4, 'issue_id': 4156, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'f95a4c2b427959d4', 'authors': ['Jianman Lin', 'Haojie Li', 'Chunmei Qing', 'Zhijing Yang', 'Liang Lin', 'Tianshui Chen'], 'affiliations': ['Guangdong University of Technology', 'South China University of Technology', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20914.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Точное редактирование геометрии и сохранение деталей в композиции объектов', 'desc': 'Статья представляет новую модель DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) для композиции объектов в сцене. DGAD использует семантические эмбеддинги для управления геометрией объекта и механизм кросс-внимания для сохранения деталей внешнего вида. Модель интегрирует семантические эмбеддинги в предобученные диффузионные модели для гибкого манипулирования геометрией объекта. DGAD применяет плотный механизм кросс-внимания для извлечения и пространственного выравнивания признаков внешнего вида с соответствующими регионами.'}, 'en': {'title': 'Seamless Object Integration with Geometry and Appearance Preservation', 'desc': "The paper presents a new model called Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) for integrating objects into backgrounds while maintaining their detailed appearance. DGAD uses semantic embeddings to understand the desired geometric changes and a cross-attention mechanism to align fine-grained appearance features with these changes. This approach allows for precise editing of object geometry without losing the intricate details of the object's appearance. The model builds on existing diffusion techniques and shows improved performance in object composition tasks through extensive experiments."}, 'zh': {'title': '解耦几何与外观保留的物体组合新方法', 'desc': '一般物体组合（GOC）旨在将目标物体无缝地融入背景场景中，同时保持其细致的外观细节。现有方法通过语义嵌入与先进的扩散模型结合，实现几何可编辑的生成。然而，这些紧凑的嵌入仅编码高层语义信息，难以保留细致的外观细节。我们提出了一种解耦几何可编辑和外观保留的扩散模型（DGAD），通过语义嵌入捕捉几何变换，并利用交叉注意力机制对齐外观特征，从而实现精确的几何编辑和真实的外观保留。'}}}, {'id': 'https://huggingface.co/papers/2506.05348', 'title': 'FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction', 'url': 'https://huggingface.co/papers/2506.05348', 'abstract': 'A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin.', 'score': 3, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '41deb088324d4fce', 'authors': ['Yifan Wang', 'Peishan Yang', 'Zhen Xu', 'Jiaming Sun', 'Zhanhua Zhang', 'Yong Chen', 'Hujun Bao', 'Sida Peng', 'Xiaowei Zhou'], 'affiliations': ['Geely Automobile Research Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05348.jpg', 'data': {'categories': ['#3d'], 'emoji': '🔄', 'ru': {'title': 'Свободное время и пространство для гауссовых примитивов в динамических 3D-сценах', 'desc': 'FreeTimeGS - это новый метод 4D-представления для моделирования динамических 3D-сцен. Он позволяет гауссовым примитивам появляться в произвольное время и в произвольных местах, что улучшает качество рендеринга по сравнению с существующими методами. FreeTimeGS преодолевает ограничения предыдущих подходов, использующих деформационные поля для отображения канонических примитивов. Метод также включает функцию движения для каждого примитива, что уменьшает временную избыточность.'}, 'en': {'title': 'Revolutionizing Dynamic 3D Scene Modeling with FreeTimeGS', 'desc': 'This paper introduces FreeTimeGS, a new 4D representation that enhances the modeling of dynamic 3D scenes. By allowing Gaussian primitives to appear at any time and location, it provides greater flexibility compared to traditional methods that rely on fixed canonical spaces. The approach includes motion functions for each Gaussian primitive, enabling them to transition smoothly over time and reducing redundancy in the scene representation. Experimental results demonstrate that FreeTimeGS significantly improves rendering quality, outperforming existing techniques in handling complex motions.'}, 'zh': {'title': '动态3D场景建模的新突破：FreeTimeGS', 'desc': '本文提出了一种新颖的4D表示方法FreeTimeGS，旨在增强动态3D场景的建模能力。与传统的3D高斯原语不同，FreeTimeGS允许高斯原语在任意时间和位置出现，从而提高了渲染质量。该方法通过为每个高斯原语赋予运动函数，使其能够随时间移动到相邻区域，减少了时间冗余。实验结果表明，FreeTimeGS在多个数据集上的渲染质量显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05282', 'title': 'Rectified Point Flow: Generic Point Cloud Pose Estimation', 'url': 'https://huggingface.co/papers/2506.05282', 'abstract': 'We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.', 'score': 3, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c7d3c7ca688358d9', 'authors': ['Tao Sun', 'Liyuan Zhu', 'Shengyu Huang', 'Shuran Song', 'Iro Armeni'], 'affiliations': ['NVIDIA Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05282.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Единый подход к регистрации облаков точек и сборке форм', 'desc': 'Представлен метод Rectified Point Flow, который объединяет регистрацию облаков точек и сборку многокомпонентных форм в единую условную генеративную задачу. Метод обучает непрерывное поточечное поле скоростей, которое перемещает зашумленные точки к целевым позициям. В отличие от предыдущих подходов, данный метод изначально учитывает симметрии при сборке без явной разметки. Вместе с самоконтролируемым энкодером, фокусирующимся на перекрывающихся точках, метод достигает нового уровня производительности на шести эталонных наборах данных.'}, 'en': {'title': 'Unified Learning for Point Cloud Registration and Shape Assembly', 'desc': 'This paper presents Rectified Point Flow, a novel approach that combines point cloud registration and multi-part shape assembly into a single generative framework. The method learns a continuous velocity field that aligns noisy point clouds to their target configurations, allowing for the recovery of part poses. Unlike previous methods that require manual symmetry handling, this approach automatically learns assembly symmetries without needing explicit labels. By utilizing a self-supervised encoder for overlapping points, the method achieves state-of-the-art results across multiple benchmarks, enhancing accuracy through joint training on varied datasets.'}, 'zh': {'title': '统一点云配准与形状组装的创新方法', 'desc': '我们提出了修正点流（Rectified Point Flow），这是一种统一的参数化方法，将成对点云配准和多部件形状组装视为一个单一的条件生成问题。该方法在没有姿态信息的情况下，学习一个连续的点位速度场，将噪声点移动到目标位置，并从中恢复部件姿态。与之前的工作不同，我们的方法能够在没有对称标签的情况下，自然地学习组装对称性。通过专注于重叠点的自监督编码器，我们的方法在六个基准测试中实现了新的最先进性能，促进了共享几何先验的学习，从而提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.04956', 'title': 'FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.04956', 'abstract': 'Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT.', 'score': 3, 'issue_id': 4166, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e1a303f65a6d378c', 'authors': ['Huihan Wang', 'Zhiwen Yang', 'Hui Zhang', 'Dan Zhao', 'Bingzheng Wei', 'Yan Xu'], 'affiliations': ['ByteDance Inc., Beijing 100098, China', 'Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China', 'Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China', 'School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04956.jpg', 'data': {'categories': ['#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🏥', 'ru': {'title': 'FEAT: Эффективный трансформер для синтеза медицинских видео', 'desc': 'В статье представлен FEAT - новый трансформер для синтеза динамических медицинских видео. FEAT использует последовательные механизмы внимания для пространственных, временных и канальных измерений, что позволяет захватывать глобальные зависимости. Архитектура имеет линейную сложность вычислений и включает модуль остаточного управления значениями для точной адаптации к разным уровням шума. FEAT превосходит существующие методы по эффективности и масштабируемости на стандартных наборах данных.'}, 'en': {'title': 'FEAT: Revolutionizing Medical Video Synthesis with Efficient Attention', 'desc': "This paper introduces FEAT, a novel Transformer model designed to create high-quality dynamic medical videos by effectively managing spatial and temporal information. It overcomes limitations of existing models by implementing a unified attention mechanism that captures dependencies across spatial, temporal, and channel dimensions. FEAT also features a linear-complexity attention design, which reduces computational demands while maintaining performance. Additionally, a residual value guidance module enhances the model's ability to adapt to varying noise levels, leading to superior results on benchmark tasks with fewer parameters than previous state-of-the-art models."}, 'zh': {'title': '高效动态医疗视频合成的新突破', 'desc': '本论文提出了一种新的全维高效注意力变换器（FEAT），旨在解决动态医疗视频合成中的空间一致性和时间动态建模问题。FEAT通过三项创新来克服现有方法的局限性，包括统一的时空通道注意力机制、线性复杂度的注意力设计以及残差值引导模块，以适应不同的噪声水平。实验结果表明，FEAT-S在参数量仅为最先进模型Endora的23%的情况下，仍能实现相当或更优的性能。FEAT-L在多个数据集上超越了所有对比方法，展示了其卓越的有效性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.04598', 'title': 'Scaling Laws for Robust Comparison of Open Foundation Language-Vision\n  Models and Datasets', 'url': 'https://huggingface.co/papers/2506.04598', 'abstract': "Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves 80.3% zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.", 'score': 3, 'issue_id': 4165, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '3add105c0b3c0782', 'authors': ['Marianna Nezhurina', 'Tomer Porian', 'Giovanni Pucceti', 'Tommie Kerssies', 'Romain Beaumont', 'Mehdi Cherti', 'Jenia Jitsev'], 'affiliations': ['Eindhoven University of Technology', 'Institute of Information Science and Technologies A. Faedo - CNR Pisa', 'Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)', 'LAION', 'Open-Ψ (Open-Sci) Collective'], 'pdf_title_img': 'assets/pdf/title_img/2506.04598.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#training', '#dataset', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Масштабируемые законы как ключ к сравнению мультимодальных моделей', 'desc': 'В статье представлены масштабируемые законы для моделей CLIP и MaMMUT, позволяющие сравнить их производительность и эффективность использования данных при различных масштабах и наборах данных. Исследователи показывают, как вывод законов масштабирования может использоваться для сравнения моделей и датасетов, помогая выбрать оптимальную процедуру предобучения. Результаты демонстрируют, что MaMMUT показывает более сильное улучшение с увеличением масштаба и лучшую эффективность использования данных по сравнению со стандартным CLIP. Авторы также предоставляют законы масштабирования для различных задач и наборов данных, подтверждая наблюдаемые тенденции.'}, 'en': {'title': 'Unlocking Model Potential: Scaling Laws for Better Comparisons', 'desc': "This paper explores scaling laws for two language-vision models, CLIP and MaMMUT, to evaluate their performance and efficiency as they are trained on larger datasets. By deriving these scaling laws, the authors provide a framework for comparing different pre-training methods, highlighting MaMMUT's superior performance and sample efficiency over CLIP. The study includes various downstream tasks and datasets, ensuring that the observed trends are consistent across different scenarios. The findings aim to guide future improvements in foundation models and datasets by offering a systematic approach to model comparison."}, 'zh': {'title': '模型与数据集比较的新方法', 'desc': '本文研究了CLIP和MaMMUT模型的缩放规律，以比较它们在不同规模和数据集上的性能和样本效率。通过对这两种重要的语言-视觉学习方法进行全面的缩放规律推导，揭示了MaMMUT在规模扩大时的性能提升和样本效率优于标准CLIP。我们还展示了在不同下游任务和开放数据集上，缩放规律的一致性趋势，确保了比较的有效性。最终，我们发布了所有预训练模型及其中间检查点，以支持后续研究和实验。'}}}, {'id': 'https://huggingface.co/papers/2506.00830', 'title': 'SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.00830', 'abstract': 'SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.', 'score': 3, 'issue_id': 4157, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7a04cf6593bac4b3', 'authors': ['Zhengcong Fei', 'Hao Jiang', 'Di Qiu', 'Baoxuan Gu', 'Youqiang Zhang', 'Jiahua Wang', 'Jialin Bai', 'Debang Li', 'Mingyuan Fan', 'Guibin Chen', 'Yahui Zhou'], 'affiliations': ['Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00830.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#audio', '#diffusion', '#synthetic', '#video'], 'emoji': '🎭', 'ru': {'title': 'Революция в синтезе говорящих портретов: от аудио к реалистичному видео', 'desc': 'SkyReels-Audio - это унифицированная система для создания высококачественных видео с говорящими портретами на основе аудио. Она использует предобученные видео-диффузионные трансформеры и поддерживает генерацию видео бесконечной длины с разнообразными условиями. В системе применяется гибридная стратегия куррикулярного обучения для постепенного согласования аудио и движений лица. Для улучшения локальной согласованности лица вводятся специальные функции потерь и механизм аудио-управляемого безклассового наведения.'}, 'en': {'title': 'Revolutionizing Talking Portraits with SkyReels-Audio', 'desc': 'SkyReels-Audio is a novel framework that generates high-quality talking portrait videos by using pretrained video diffusion transformers. It allows for the creation and editing of videos based on audio, text, and images, making it versatile for various multimodal inputs. The framework employs a hybrid curriculum learning strategy to ensure that the audio aligns well with facial movements, enhancing the control over video sequences. Additionally, it introduces advanced loss mechanisms to improve facial coherence and uses a sliding-window denoising technique to maintain visual quality and consistency over time.'}, 'zh': {'title': 'SkyReels-Audio：音频驱动的高保真说话肖像生成', 'desc': 'SkyReels-Audio 是一个统一框架，利用预训练的视频扩散变换器生成高保真且连贯的音频条件下的说话肖像视频。该框架支持无限长度的生成和编辑，并通过多模态输入实现多样化和可控的条件设置。我们采用混合课程学习策略，逐步对齐音频与面部运动，从而实现对长视频序列的精细控制。通过引入面部掩膜损失和音频引导的无分类器指导机制，SkyReels-Audio 在复杂条件下展现出卓越的唇同步精度和身份一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.04245', 'title': 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04245', 'abstract': 'As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.', 'score': 3, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8716d3ca53b1d58f', 'authors': ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim'], 'affiliations': ['Microsoft', 'National University of Singapore', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04245.jpg', 'data': {'categories': ['#synthetic', '#agents', '#benchmark', '#reasoning', '#dataset', '#transfer_learning', '#rl', '#leakage'], 'emoji': '🔐', 'ru': {'title': 'Разумное раскрытие информации: обучение ИИ-агентов контекстной целостности', 'desc': 'Статья посвящена проблеме контекстной целостности (CI) в эпоху автономных агентов, принимающих решения за пользователей. Авторы предлагают метод, использующий языковые модели (LLM) и обучение с подкреплением (RL) для обучения агентов рассуждать о контексте и принимать решения о раскрытии информации. Эксперименты на синтетическом наборе данных показали значительное снижение неуместного раскрытия информации при сохранении производительности задач. Улучшения переносятся на реальные бенчмарки CI, такие как PrivacyLens.'}, 'en': {'title': 'Enhancing Contextual Integrity in Autonomous Agents', 'desc': 'This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks.'}, 'zh': {'title': '确保上下文完整性，提升自主代理决策能力', 'desc': '在自主代理为用户做决策的时代，确保上下文完整性（CI）成为一个重要问题。本文提出，CI需要代理在执行任务时对其操作的上下文进行推理。我们首先让大型语言模型（LLMs）明确推理CI，以决定披露哪些信息。接着，我们开发了一个强化学习（RL）框架，进一步增强模型进行CI所需的推理能力，实验结果表明，该方法显著减少了不当信息披露，同时保持了任务性能。'}}}, {'id': 'https://huggingface.co/papers/2506.05278', 'title': 'Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning', 'url': 'https://huggingface.co/papers/2506.05278', 'abstract': 'A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.', 'score': 2, 'issue_id': 4158, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '119fed47e7d43a96', 'authors': ['Nan Huo', 'Jinyang Li', 'Bowen Qin', 'Ge Qu', 'Xiaolong Li', 'Xiaodong Li', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05278.jpg', 'data': {'categories': ['#interpretability', '#rag', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Micro-Act: умное разрешение конфликтов знаний в RAG-системах', 'desc': 'Фреймворк Micro-Act решает проблему конфликтов знаний в системах генерации с дополнением из источников (RAG). Он адаптивно разбивает источники знаний на мелкие сравнения, представленные как последовательность действий. Это позволяет рассуждать за пределами поверхностного контекста. Эксперименты показали значительное повышение точности ответов на вопросы по сравнению с существующими методами.'}, 'en': {'title': 'Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy', 'desc': 'The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.'}, 'zh': {'title': 'Micro-Act：解决知识冲突的智能框架', 'desc': 'Micro-Act是一个新框架，旨在解决检索增强生成（RAG）中的知识冲突问题。它通过自适应地分解知识源，改善了问答（QA）的准确性。与现有方法不同，Micro-Act采用分层的行动空间，能够自动感知上下文的复杂性，并将知识源细分为一系列精细的比较步骤。这种方法在五个基准数据集上的实验中显示出显著的QA准确性提升，尤其在时间和语义类型的冲突中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.02751', 'title': 'RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS', 'url': 'https://huggingface.co/papers/2506.02751', 'abstract': 'RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.', 'score': 2, 'issue_id': 4161, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9e5cefa9df6461fd', 'authors': ['Chuanyu Fu', 'Yuqi Zhang', 'Kunbin Yao', 'Guanying Chen', 'Yuan Xiong', 'Chuan Huang', 'Shuguang Cui', 'Xiaochun Cao'], 'affiliations': ['FNii-Shenzhen', 'SSE, CUHKSZ', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02751.jpg', 'data': {'categories': ['#3d'], 'emoji': '🌟', 'ru': {'title': 'Устранение артефактов в 3D-сценах с помощью умного роста гауссианов', 'desc': 'RobustSplat - это новый метод в области 3D-моделирования, направленный на устранение артефактов, вызванных временными объектами в сценах. Он использует стратегию отложенного роста гауссианов, что позволяет сначала оптимизировать статическую структуру сцены. Кроме того, RobustSplat применяет каскадный подход к созданию маски временных объектов, начиная с низкого разрешения и постепенно увеличивая его. Эксперименты показали, что этот метод превосходит существующие решения в задаче робастного 3D-рендеринга.'}, 'en': {'title': 'Enhancing 3D Gaussian Splatting with Robust Techniques', 'desc': 'RobustSplat is a novel approach designed to improve 3D Gaussian Splatting (3DGS) by addressing artifacts caused by transient objects in rendered images. The method introduces a delayed Gaussian growth strategy that focuses on optimizing the static elements of a scene before dealing with transient disturbances, reducing the risk of overfitting. Additionally, it employs a scale-cascaded mask bootstrapping technique that starts with lower-resolution features for initial mask estimation, ensuring better semantic consistency before refining to high-resolution predictions. Through extensive testing, RobustSplat demonstrates superior performance compared to existing methods, showcasing its effectiveness in producing high-quality, artifact-free renderings.'}, 'zh': {'title': '增强3D渲染的鲁棒性', 'desc': 'RobustSplat 是一种针对 3D 高斯点云渲染中因瞬态物体引起的伪影问题的解决方案。该方法通过延迟高斯生长和尺度级联掩码自举来优化静态场景结构，减少对瞬态物体的过拟合。首先，延迟高斯生长策略确保在允许高斯分裂之前，先优化静态场景。其次，尺度级联掩码自举方法利用低分辨率特征相似性进行初步掩码估计，随后再进行高分辨率监督，以提高掩码预测的精确度。'}}}, {'id': 'https://huggingface.co/papers/2505.23115', 'title': 'Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2505.23115', 'abstract': 'Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.', 'score': 2, 'issue_id': 4162, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '9780859f709be640', 'authors': ['Yunshen Wang', 'Yicheng Liu', 'Tianyuan Yuan', 'Yucheng Mao', 'Yingshi Liang', 'Xiuyu Yang', 'Honggang Zhang', 'Hang Zhao'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute for Interdisciplinary Information Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23115.jpg', 'data': {'categories': ['#diffusion', '#agents', '#3d', '#cv'], 'emoji': '🚗', 'ru': {'title': 'Диффузионные модели улучшают 3D-восприятие беспилотных автомобилей', 'desc': 'В этой работе авторы предлагают использовать генеративные диффузионные модели для предсказания трехмерных карт занятости в задаче автономного вождения. В отличие от дискриминативных методов, такой подход позволяет лучше справляться с шумными данными и неполными наблюдениями. Эксперименты показывают, что генеративные модели превосходят современные дискриминативные подходы, особенно в областях с плохой видимостью. Улучшенные предсказания значительно повышают качество планирования маршрута для беспилотных автомобилей.'}, 'en': {'title': 'Revolutionizing 3D Occupancy Prediction with Diffusion Models', 'desc': 'This paper addresses the challenge of predicting 3D occupancy grids from visual inputs for autonomous driving, particularly in the presence of noisy data and incomplete observations. The authors propose a novel approach by reframing the problem as a generative modeling task using diffusion models, which learn the data distribution and incorporate 3D scene priors. This method improves prediction consistency and robustness against noise, effectively managing the complexities of 3D spatial structures. Experimental results demonstrate that their diffusion-based models outperform traditional discriminative methods, leading to more accurate occupancy predictions that enhance downstream planning tasks in real-world driving scenarios.'}, 'zh': {'title': '生成模型提升3D占用预测的准确性', 'desc': '本研究将3D占用网格的预测视为生成建模任务，采用扩散模型来处理视觉输入。与传统的判别方法相比，扩散模型能够更好地应对噪声数据和不完整观测，同时有效捕捉3D场景的复杂结构。实验结果表明，基于扩散的生成模型在占用预测的准确性和一致性上优于现有的最先进判别方法，尤其在遮挡或低可见度区域表现更佳。该方法的改进预测显著提升了后续规划任务的效果，展示了其在自动驾驶实际应用中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.03643', 'title': 'Images are Worth Variable Length of Representations', 'url': 'https://huggingface.co/papers/2506.03643', 'abstract': 'Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a simple image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a dynamic vision encoder that produces a variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder.github.io/dove-encoder.', 'score': 1, 'issue_id': 4165, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '0581c037604119ee', 'authors': ['Lingjun Mao', 'Rodolfo Corona', 'Xin Liang', 'Wenhao Yan', 'Zineng Tang'], 'affiliations': ['University of California, Berkeley', 'University of California, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.03643.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#multimodal'], 'emoji': '🦅', 'ru': {'title': 'Адаптивное кодирование изображений: больше информации, меньше токенов', 'desc': 'DOVE - это динамический кодировщик изображений, который создает переменное количество визуальных токенов для реконструкции каждого изображения. В отличие от существующих энкодеров с фиксированной длиной последовательности, DOVE адаптируется к сложности изображения, используя больше токенов для визуально сложных сцен и меньше для простых. Результаты показывают, что DOVE значительно сокращает среднее количество токенов, сохраняя высокое качество реконструкции. Модель превосходит существующие методы токенизации на основе автоэнкодеров в задачах линейного пробинга и мультимодальных задачах, используя гораздо меньше токенов.'}, 'en': {'title': 'Dynamic Tokenization for Enhanced Image Understanding', 'desc': 'This paper introduces DOVE, a dynamic vision encoder that adapts the number of visual tokens based on the complexity of the image being processed. Unlike traditional methods that use a fixed number of tokens, DOVE generates a variable number of tokens, allowing it to capture more information from visually complex images. The results demonstrate that DOVE not only reduces the average number of tokens needed but also maintains high-quality image reconstruction. Additionally, DOVE incorporates query-conditioned tokenization to enhance semantic extraction by focusing on relevant image regions, outperforming existing autoencoder-based methods in various tasks.'}, 'zh': {'title': '动态视觉编码，提升信息提取效率', 'desc': '现有的视觉编码器通常将图像映射为固定长度的标记序列，但不同图像的信息量不同。我们提出了DOVE，一个动态视觉编码器，可以生成可变数量的视觉标记，以重建每个图像。DOVE在保持高重建质量的同时，显著减少了平均标记数量，并在多个任务中超越了现有的基于自编码器的标记化方法。通过查询条件标记化，DOVE能够更有效地提取与查询相关的语义特征。'}}}, {'id': 'https://huggingface.co/papers/2506.03238', 'title': 'Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach', 'url': 'https://huggingface.co/papers/2506.03238', 'abstract': 'OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'f234199601bef528', 'authors': ['Ziheng Zhao', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Department of Radiology, Renmin Hospital of Wuhan University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03238.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#healthcare', '#cv'], 'emoji': '🔬', 'ru': {'title': 'ИИ-революция в интерпретации КТ-снимков', 'desc': 'Модель OminiAbnorm-CT предназначена для автоматизированной интерпретации КТ-изображений. Она превосходит существующие методы в локализации и описании аномалий в различных областях тела с использованием текстовых запросов и визуальных подсказок. Модель основана на всеобъемлющей иерархической системе классификации, разработанной совместно с опытными радиологами. OminiAbnorm-CT обучена на большом наборе данных КТ-изображений с тщательно размеченными аномалиями.'}, 'en': {'title': 'Revolutionizing CT Image Analysis with OminiAbnorm-CT', 'desc': 'OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology.'}, 'zh': {'title': 'OminiAbnorm-CT：CT图像异常自动解读的新突破', 'desc': 'OminiAbnorm-CT是一种用于自动解读CT图像的模型，能够在不同身体部位中定位和描述异常情况。该研究通过与资深放射科医生合作，提出了一个包含404种异常发现的层次分类系统。我们还贡献了一个包含超过14.5K CT图像的数据集，并为超过19K异常提供了详细的注释。通过大量实验，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02587', 'title': "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations", 'url': 'https://huggingface.co/papers/2506.02587', 'abstract': "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib.", 'score': 1, 'issue_id': 4160, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '8d1e0e49dea5dcad', 'authors': ['Weiduo Yuan', 'Jerry Li', 'Justin Yue', 'Divyank Shah', 'Konstantinos Karydis', 'Hang Qiu'], 'affiliations': ['University of California, Riverside', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.02587.jpg', 'data': {'categories': ['#robotics', '#cv', '#dataset', '#optimization', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'Революция в калибровке LiDAR-камеры с помощью вида сверху', 'desc': 'BEVCALIB - это модель, использующая функции вида сверху для точной калибровки LiDAR-камеры по необработанным данным. Модель извлекает и объединяет функции вида сверху как для камеры, так и для LiDAR в общее пространство признаков. BEVCALIB демонстрирует превосходную производительность в различных условиях шума по сравнению с существующими методами. Модель устанавливает новый стандарт в области калибровки LiDAR-камеры, значительно превосходя базовые показатели на наборах данных KITTI и NuScenes.'}, 'en': {'title': 'Revolutionizing LiDAR-Camera Calibration with BEV Features', 'desc': "The BEVCALIB model introduces a novel approach for calibrating LiDAR and camera systems using bird's-eye view (BEV) features extracted from raw data. This method addresses the limitations of traditional calibration techniques that struggle with dynamic transformations during vehicle or robot movement. By fusing separate BEV features from both LiDAR and camera into a shared feature space, BEVCALIB enhances the accuracy of multi-modal perception in autonomous systems. The model demonstrates significant performance improvements under various noise conditions, setting a new benchmark in the field with extensive evaluations on multiple datasets."}, 'zh': {'title': 'BEVCALIB：鸟瞰视图特征助力激光雷达与相机精确标定', 'desc': 'BEVCALIB模型利用鸟瞰视图特征进行激光雷达与相机的精确标定，能够从原始数据中提取信息。与传统方法相比，BEVCALIB在各种噪声条件下表现出色，特别是在自动驾驶和机器人系统中融合多模态感知时至关重要。该模型通过分别提取相机和激光雷达的鸟瞰视图特征，并将其融合到共享的特征空间中，显著提高了标定的准确性。通过引入新颖的特征选择器，BEVCALIB在减少内存消耗的同时，实现了高效的训练和更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04996', 'title': 'PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment', 'url': 'https://huggingface.co/papers/2506.04996', 'abstract': 'PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications.', 'score': 0, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'f0a38c5292395f88', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.04996.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': '🏋️', 'ru': {'title': 'PATS: Умная временная выборка для оценки спортивного мастерства', 'desc': 'PATS (Proficiency-Aware Temporal Sampling) - это новый метод временной выборки для анализа видео спортивных навыков. Он обеспечивает захват полных паттернов движения, превосходя существующие методы в различных областях. PATS адаптивно сегментирует видео, чтобы каждая анализируемая часть содержала полное выполнение критических компонентов производительности. Оцененный на бенчмарке EgoExo4D с использованием SkillFormer, PATS превосходит современные показатели точности во всех конфигурациях просмотра.'}, 'en': {'title': 'Enhancing Athletic Skill Analysis with PATS', 'desc': 'PATS, or Proficiency-Aware Temporal Sampling, is a new method designed to improve the analysis of athletic skills in videos. It captures complete movement patterns by maintaining the temporal continuity necessary for evaluating performance. This method adaptively segments videos to ensure that each analyzed part includes the full execution of key skills, enhancing the accuracy of assessments. PATS has shown to outperform existing techniques in various sports and activities, making it a significant advancement in automated skill evaluation.'}, 'zh': {'title': 'PATS：提升运动技能分析的时间采样新方法', 'desc': 'PATS是一种新颖的时间采样方法，旨在提升运动技能的视频分析。它通过确保完整的运动模式被捕捉，超越了现有的采样方法。PATS能够自适应地分段视频，确保每个分析部分都包含关键表现组件的完整执行。经过评估，PATS在多个领域的准确性上均优于现有技术，显示出其在自动化技能评估中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.00981', 'title': 'What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training', 'url': 'https://huggingface.co/papers/2506.00981', 'abstract': "Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition performance.  \t\t\t\t\tAI-generated summary \t\t\t\t How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.", 'score': 0, 'issue_id': 4165, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'e571a309d437ed8b', 'authors': ['Marianne de Heer Kloots', 'Hosein Mohebbi', 'Charlotte Pouw', 'Gaofei Shen', 'Willem Zuidema', 'Martijn Bentum'], 'affiliations': ['Centre for Language Studies, Radboud University, Netherlands', 'Cognitive Science and Artificial Intelligence, Tilburg University, The Netherlands', 'Institute for Logic, Language and Computation, University of Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.00981.jpg', 'data': {'categories': ['#transfer_learning', '#multilingual', '#audio', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Языково-специфичное предобучение улучшает представление речи', 'desc': 'Исследование показывает, что модели Wav2Vec2, предобученные исключительно на голландских данных, лучше кодируют лингвистические особенности голландского языка по сравнению с моделями, обученными на английском или многоязычных данных. Это преимущество выявляется с помощью методов кластеризации и классификации. Улучшение представления лингвистических особенностей также приводит к повышению производительности в задаче автоматического распознавания речи. Результаты подчеркивают важность использования языково-специфичных данных при предобучении моделей для конкретного языка.'}, 'en': {'title': 'Unlocking Dutch: The Power of Language-Specific Pre-Training', 'desc': 'This paper investigates how well self-supervised Wav2Vec2 models can learn Dutch language features when trained specifically on Dutch data. The study finds that pre-training exclusively on Dutch leads to better encoding of Dutch phonetic and lexical information compared to using English or multilingual data. The improvements are measured using clustering and classification probes, which show that the models capture language-specific characteristics more effectively. Additionally, these enhancements in linguistic representation correlate with better performance in Automatic Speech Recognition tasks.'}, 'zh': {'title': '专注荷兰语，提升语音识别表现', 'desc': '本研究探讨了自监督学习模型Wav2Vec2在编码荷兰语语言特征方面的表现。研究发现，当模型仅在荷兰语数据上进行预训练时，能够更准确地捕捉荷兰语的语音和词汇信息。与在英语或多语言数据上进行相似量的预训练相比，荷兰语特征的表示显著提高。该语言特定的优势通过聚类和分类探测器得到了验证，并且与自动语音识别的性能提升相一致。'}}}, {'id': 'https://huggingface.co/papers/2505.24726', 'title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.24726', 'abstract': "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.", 'score': 140, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '05f75b6123a35a65', 'authors': ['Shelly Bensal', 'Umar Jamil', 'Christopher Bryant', 'Melisa Russak', 'Kiran Kamble', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.24726.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#small_models', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ и обучение с подкреплением повышают эффективность языковых моделей', 'desc': 'Исследователи предложили метод улучшения работы больших языковых моделей с помощью самоанализа и обучения с подкреплением. Модель генерирует самоанализ после неудачной попытки решения задачи, а затем повторно пытается её решить с учетом этого анализа. Если вторая попытка успешна, токены самоанализа получают положительное подкрепление. Эксперименты показали значительное улучшение производительности на различных задачах, особенно для небольших моделей.'}, 'en': {'title': 'Empowering Language Models Through Self-Reflection and Reinforcement Learning', 'desc': 'This paper presents a novel approach to enhance large language models using self-reflection and reinforcement learning. The method encourages models to analyze their mistakes and generate self-reflective commentary, which is then used to improve subsequent task attempts. By rewarding successful outcomes that follow self-reflection, the model learns to perform better even with minimal feedback. Experimental results indicate significant performance improvements, particularly in smaller models, suggesting a promising direction for developing more effective language models.'}, 'zh': {'title': '自我反思与强化学习提升语言模型性能', 'desc': '本文提出了一种通过自我反思和强化学习来提升大型语言模型性能的方法。该方法在模型回答错误时，激励其生成更好的自我反思，从而提高解决复杂任务的能力。框架分为两个阶段：首先，模型在失败后生成自我反思的评论；其次，模型在考虑自我反思的情况下再次尝试任务。实验结果显示，在多种模型架构中，性能提升显著，尤其是小型微调模型的表现超过了同类更大模型。'}}}, {'id': 'https://huggingface.co/papers/2506.02387', 'title': 'VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments', 'url': 'https://huggingface.co/papers/2506.02387', 'abstract': "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.", 'score': 50, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '829f3546d3ac9345', 'authors': ['Zelai Xu', 'Zhexuan Xu', 'Xiangmin Yi', 'Huining Yuan', 'Xinlei Chen', 'Yi Wu', 'Chao Yu', 'Yu Wang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02387.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#benchmark', '#games'], 'emoji': '🤖', 'ru': {'title': 'VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов', 'desc': 'VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений vision-language моделями в сложных мультиагентных средах. Он включает восемь визуальных сред с кооперативными, соревновательными и смешанными взаимодействиями. Бенчмарк оценивает способность моделей предсказывать будущие действия других агентов и оптимизировать долгосрочные цели. Эксперименты с 14 ведущими VLM показали значительный разрыв между текущими моделями и оптимальной производительностью.'}, 'en': {'title': 'Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench', 'desc': 'VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area.'}, 'zh': {'title': '多模态智能体的战略推理新基准', 'desc': 'VS-Bench是一个多模态基准，旨在评估视觉语言模型（VLM）在复杂多智能体环境中的战略推理和决策能力。与现有的单智能体或仅文本环境的基准不同，VS-Bench考虑了多个智能体在丰富的视觉和语言背景下的互动。该基准包括八个基于视觉的环境，涵盖合作、竞争和混合动机的互动，评估智能体预测他人未来动作和优化长期目标的能力。通过标准化评估，VS-Bench为未来的战略多模态智能体研究奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.03147', 'title': 'UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.03147', 'abstract': "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.", 'score': 48, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34f1d96b37be24a1', 'authors': ['Bin Lin', 'Zongjian Li', 'Xinhua Cheng', 'Yuwei Niu', 'Yang Ye', 'Xianyi He', 'Shenghai Yuan', 'Wangbo Yu', 'Shaodong Wang', 'Yunyang Ge', 'Yatian Pang', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03147.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'UniWorld: Мощная модель для работы с изображениями на основе семантических признаков', 'desc': 'UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображений. Она использует семантические признаки из визуально-языковых моделей вместо вариационных автоэнкодеров. UniWorld превосходит модель BAGEL по качеству редактирования изображений, используя всего 1% данных. Модель также демонстрирует высокие результаты в задачах понимания и генерации изображений.'}, 'en': {'title': 'UniWorld: Efficient Image Manipulation with Semantic Power', 'desc': 'UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications.'}, 'zh': {'title': 'UniWorld：图像感知与操作的新纪元', 'desc': 'UniWorld是一个统一的生成框架，利用视觉语言模型的语义特征来进行图像感知和操作。与BAGEL相比，UniWorld在数据使用上减少了90%，但在图像编辑基准测试中表现更优。该模型不仅在图像理解和生成方面保持竞争力，还在多个图像感知任务中取得了良好的效果。我们将模型权重、训练和评估脚本以及数据集全部开源，方便研究者使用。'}}}, {'id': 'https://huggingface.co/papers/2506.02096', 'title': 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis', 'url': 'https://huggingface.co/papers/2506.02096', 'abstract': "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.", 'score': 45, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '9ea84a7af081829e', 'authors': ['Zijian Wu', 'Jinjie Ni', 'Xiangyan Liu', 'Zichen Liu', 'Hang Yan', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.02096.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#synthetic', '#rl'], 'emoji': '🧠', 'ru': {'title': 'SynthRL: Синтез данных для улучшения математических рассуждений ИИ', 'desc': 'SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он улучшает визуальные языковые модели для математических рассуждений, генерируя сложные, проверяемые вопросы. SynthRL включает три ключевых этапа: выбор исходных вопросов, их усложнение с сохранением ответов и верификацию для обеспечения корректности. Эксперименты показали, что модели, обученные на синтезированных данных, достигают стабильных улучшений на пяти тестовых наборах по визуальным математическим рассуждениям.'}, 'en': {'title': 'SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis', 'desc': "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."}, 'zh': {'title': 'SynthRL：提升视觉数学推理的智能合成管道', 'desc': 'SynthRL是一种可扩展的数据合成管道，旨在增强强化学习中的可验证奖励（RLVR），特别是在视觉数学推理模型（VLMs）中。该方法通过生成具有挑战性和可验证的问题，来提高模型的推理能力。SynthRL包括三个关键阶段：选择合适分布的种子问题、将其增强为更具挑战性的变体，并确保答案的正确性和难度的提升。实验结果表明，使用SynthRL合成的数据在多个视觉数学推理基准测试中显著提高了模型的表现，尤其是在最具挑战性的样本上。'}}}, {'id': 'https://huggingface.co/papers/2505.24120', 'title': 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs', 'url': 'https://huggingface.co/papers/2505.24120', 'abstract': 'A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.', 'score': 43, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '09ca2aeec21b0156', 'authors': ['Ai Jian', 'Weijie Qiu', 'Xiaokun Wang', 'Peiyu Wang', 'Yunzhuo Hao', 'Jiangbo Pei', 'Yichen Wei', 'Yi Peng', 'Xuchen Song'], 'affiliations': ['Kunlun Inc.', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24120.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'CSVQA: новый рубеж в оценке научного мышления ИИ', 'desc': 'Представлен новый бенчмарк CSVQA для оценки научного мышления в мультимодальных моделях через задачи ответов на вопросы по изображениям в конкретных научных областях. Бенчмарк содержит 1378 пар вопрос-ответ из различных STEM-дисциплин, требующих применения предметных знаний и анализа визуальных данных. Оценка 15 современных мультимодальных моделей на CSVQA показала значительные различия в производительности, при этом даже лучшая модель достигла точности лишь 49.6%. Результаты подчеркивают необходимость улучшения способностей научного мышления в мультимодальных моделях.'}, 'en': {'title': 'CSVQA: Advancing Scientific Reasoning in Vision-Language Models', 'desc': 'The paper introduces CSVQA, a new benchmark designed to evaluate the scientific reasoning abilities of vision-language models (VLMs) through domain-specific visual question answering. Unlike existing benchmarks that focus on generic image understanding, CSVQA emphasizes the integration of domain knowledge and visual evidence in STEM contexts. It includes 1,378 question-answer pairs that require higher-order reasoning and authentic scientific content. The evaluation of 15 VLMs on this benchmark reveals significant performance gaps, highlighting the need for improvements in their scientific reasoning capabilities.'}, 'zh': {'title': 'CSVQA：提升视觉语言模型的科学推理能力', 'desc': 'CSVQA是一个新的基准，用于评估视觉语言模型在科学推理方面的能力。该基准通过领域特定的视觉问答，强调了这些模型在科学推理中的不足。CSVQA包含1378个精心构建的问题-答案对，涵盖多个STEM学科，要求模型整合领域知识和视觉证据进行高阶推理。我们的评估显示，尽管有些模型表现较好，但最高分的模型准确率仅为49.6%，这表明在科学推理能力上仍需进一步提升。'}}}, {'id': 'https://huggingface.co/papers/2505.24714', 'title': 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2505.24714', 'abstract': 'FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.', 'score': 32, 'issue_id': 4110, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a6dcbb10b5be7f41', 'authors': ['Junyu Luo', 'Zhizhuo Kou', 'Liming Yang', 'Xiao Luo', 'Jinsheng Huang', 'Zhiping Xiao', 'Jingshu Peng', 'Chengzhong Liu', 'Jiaming Ji', 'Xuanzhe Liu', 'Sirui Han', 'Ming Zhang', 'Yike Guo'], 'affiliations': ['HKUST', 'School of Computer Science, Peking University', 'State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.24714.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#science', '#multimodal', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере', 'desc': 'FinMME - это обширный мультимодальный датасет для финансовых исследований, включающий более 11 000 высококачественных образцов из 18 финансовых областей. Датасет содержит различные типы графиков и диаграмм, а его качество обеспечивается тщательной аннотацией и валидацией. FinScore - это система оценки, учитывающая штрафы за галлюцинации и многомерную оценку возможностей моделей. Эксперименты показали, что даже передовые модели вроде GPT-4o демонстрируют неудовлетворительные результаты на FinMME, что подчеркивает сложность датасета.'}, 'en': {'title': 'FinMME: Elevating Financial AI with Robust Evaluation', 'desc': "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."}, 'zh': {'title': '推动金融领域的多模态研究', 'desc': 'FinMME是一个全面的多模态金融研究数据集，旨在推动金融领域的多模态大语言模型（MLLMs）发展。该数据集包含超过11,000个高质量的金融研究样本，涵盖18个金融领域和6种资产类别，提供10种主要图表类型和21种子类型。为了确保数据质量，研究团队使用了20名注释员和精心设计的验证机制。此外，FinScore评估系统引入了幻觉惩罚和多维能力评估，以提供公正的评估结果，尽管先进模型如GPT-4o在FinMME上表现不佳，显示出其挑战性。'}}}, {'id': 'https://huggingface.co/papers/2506.02397', 'title': 'OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation', 'url': 'https://huggingface.co/papers/2506.02397', 'abstract': 'OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.', 'score': 28, 'issue_id': 4120, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '12cee2b297810e65', 'authors': ['Shengjia Zhang', 'Junjie Wu', 'Jiawei Chen', 'Changwang Zhang', 'Xingyu Lou', 'Wangchunshu Zhou', 'Sheng Zhou', 'Can Wang', 'Jun Wang'], 'affiliations': ['OPPO Research Institute, Shenzhen, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02397.jpg', 'data': {'categories': ['#training', '#math', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений ИИ: эффективность без потери точности', 'desc': 'OThink-R1 - это новый метод машинного обучения, направленный на уменьшение избыточности рассуждений при решении сложных задач. Он классифицирует шаги рассуждений как существенные или избыточные и динамически переключает режимы мышления в зависимости от сложности задачи. OThink-R1 использует идентифицированные парадигмы и LLM-Judge для классификации траекторий рассуждений. Эксперименты показывают, что OThink-R1 снижает избыточность рассуждений в среднем на 23% без ущерба для точности.'}, 'en': {'title': 'Optimize Reasoning: Think Smart, Not Hard!', 'desc': 'OThink-R1 is a novel approach designed to enhance reasoning efficiency in complex problem-solving by distinguishing between essential and redundant reasoning steps. It leverages a classification system to identify when complex reasoning is unnecessary, allowing for a switch between fast-thinking and slow-thinking modes based on task complexity. This method significantly reduces reasoning redundancy by approximately 23% while maintaining accuracy in tasks like mathematics and question-answering. The findings suggest that not all tasks require extensive reasoning, and OThink-R1 provides a framework for optimizing reasoning processes in large reasoning models.'}, 'zh': {'title': '动态思维模式，减少推理冗余', 'desc': 'OThink-R1 是一种新方法，旨在减少复杂问题解决中的推理冗余。它通过将推理步骤分类为必要或冗余，并根据任务复杂性动态切换思维模式。对于简单问题，OThink-R1 使用快速思维模式，而对于复杂问题则采用深思熟虑的慢思维模式。实验表明，OThink-R1 平均减少了近23%的推理冗余，同时保持了准确性，为高效推理模型提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2506.00123', 'title': 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces', 'url': 'https://huggingface.co/papers/2506.00123', 'abstract': 'VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.', 'score': 28, 'issue_id': 4114, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8914927f73dff147', 'authors': ['Gen Luo', 'Ganlin Yang', 'Ziyang Gong', 'Guanzhou Chen', 'Haonan Duan', 'Erfei Cui', 'Ronglei Tong', 'Zhi Hou', 'Tianyi Zhang', 'Zhe Chen', 'Shenglong Ye', 'Lewei Lu', 'Jingbo Wang', 'Wenhai Wang', 'Jifeng Dai', 'Yu Qiao', 'Rongrong Ji', 'Xizhou Zhu'], 'affiliations': ['Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00123.jpg', 'data': {'categories': ['#games', '#robotics', '#multimodal', '#reasoning', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'VeBrain: Единый мозг для восприятия, мышления и управления роботами', 'desc': 'VeBrain - это унифицированная система для роботов с ногами, объединяющая мультимодальное понимание, визуально-пространственное мышление и физическое взаимодействие. Она переформулирует управление роботом в текстовые задачи для мультимодальных больших языковых моделей в 2D визуальном пространстве. VeBrain включает новый адаптер для преобразования текстовых сигналов управления в политики движения реальных роботов. Система демонстрирует превосходную производительность по сравнению с существующими методами на различных тестах.'}, 'en': {'title': 'VeBrain: Unifying Multimodal Intelligence for Advanced Robotic Control', 'desc': 'VeBrain is a new framework designed to enhance the capabilities of legged robots by combining multimodal understanding, visual-spatial reasoning, and physical interaction. It reformulates robotic control tasks into text-based tasks that can be processed by Multimodal Large Language Models (MLLMs), allowing for a unified approach to different robotic challenges. The framework includes a unique robotic adapter that translates textual commands from MLLMs into actionable motion policies for robots. Extensive testing shows that VeBrain outperforms existing models, achieving significant improvements in various benchmarks and demonstrating its adaptability in real-world robotic applications.'}, 'zh': {'title': 'VeBrain：四足机器人智能控制的新框架', 'desc': 'VeBrain是一个统一框架，旨在将多模态理解、视觉空间推理和物理交互整合到四足机器人中。该框架通过将机器人控制重新表述为基于文本的多模态大语言模型（MLLM）任务，从而统一了不同任务的目标和映射空间。VeBrain还引入了一种新型的机器人适配器，将MLLM的文本控制信号转换为真实机器人的运动策略。此外，VeBrain-600k是一个高质量的指令数据集，涵盖了VeBrain的多种能力，经过大量数据收集和注释，展示了VeBrain在多模态基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03135', 'title': 'OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models', 'url': 'https://huggingface.co/papers/2506.03135', 'abstract': "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.", 'score': 27, 'issue_id': 4116, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'a7cd11e4c04b1336', 'authors': ['Mengdi Jia', 'Zekun Qi', 'Shaochen Zhang', 'Wenyao Zhang', 'Xinqiang Yu', 'Jiawei He', 'He Wang', 'Li Yi'], 'affiliations': ['Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03135.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'OmniSpatial: новый рубеж в оценке пространственного мышления ИИ', 'desc': 'OmniSpatial - это комплексный тест для оценки понимания пространственных задач моделями компьютерного зрения и обработки естественного языка. Он охватывает четыре основные категории: динамическое рассуждение, сложную пространственную логику, пространственное взаимодействие и принятие перспективы. Эксперименты показали, что современные модели имеют значительные ограничения в комплексном пространственном понимании. Авторы проанализировали случаи неудач и предложили направления для будущих исследований.'}, 'en': {'title': 'OmniSpatial: Elevating Spatial Reasoning in Vision-Language Models', 'desc': 'This paper introduces OmniSpatial, a new benchmark designed to assess the capabilities of vision-language models (VLMs) in advanced spatial reasoning tasks. It highlights that while VLMs can handle basic spatial relations, they struggle with more complex reasoning, which is crucial for understanding human-like spatial interactions. The benchmark includes four main categories of spatial reasoning, with a total of 50 detailed subcategories, and is supported by over 1,500 carefully crafted question-answer pairs. The findings reveal significant shortcomings in current VLMs, prompting a discussion on future research directions to enhance their spatial reasoning abilities.'}, 'zh': {'title': 'OmniSpatial：提升视觉-语言模型的空间推理能力', 'desc': '本文介绍了一个名为OmniSpatial的基准测试，旨在评估视觉-语言模型在高级空间推理任务中的理解能力。研究发现，当前的视觉-语言模型在空间推理方面存在显著的局限性，尤其是在动态推理、复杂空间逻辑、空间交互和视角转换等方面。OmniSpatial基于认知心理学，涵盖了50个细分类别，并通过网络数据爬取和人工标注构建了1500多个问答对。实验结果表明，无论是开源还是闭源的视觉-语言模型，在全面的空间理解上都表现不佳，本文还分析了失败案例并提出了未来研究的潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2506.03143', 'title': 'GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2506.03143', 'abstract': 'GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.', 'score': 25, 'issue_id': 4112, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34d1779dffecf9d8', 'authors': ['Qianhui Wu', 'Kanzhi Cheng', 'Rui Yang', 'Chaoyun Zhang', 'Jianwei Yang', 'Huiqiang Jiang', 'Jian Mu', 'Baolin Peng', 'Bo Qiao', 'Reuben Tan', 'Si Qin', 'Lars Liden', 'Qingwei Lin', 'Huan Zhang', 'Tong Zhang', 'Jianbing Zhang', 'Dongmei Zhang', 'Jianfeng Gao'], 'affiliations': ['Microsoft', 'Nanjing University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.03143.jpg', 'data': {'categories': ['#multimodal', '#training', '#cv', '#benchmark', '#optimization', '#games'], 'emoji': '🖱️', 'ru': {'title': 'GUI-Actor: Революция в локализации элементов интерфейса с помощью VLM', 'desc': 'GUI-Actor - это метод на основе VLM, использующий механизм внимания для безкоординатной локализации элементов графического интерфейса. Он превосходит существующие методы, демонстрируя лучшую обобщающую способность и эффективное дообучение. GUI-Actor вводит основанную на внимании головную часть для действий, которая учится сопоставлять специальный токен <ACTOR> со всеми релевантными визуальными токенами патчей. Метод также включает верификатор локализации для оценки и выбора наиболее вероятного региона действия из предложенных кандидатов.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Coordinate-Free Attention', 'desc': 'The paper introduces GUI-Actor, a novel method for visual grounding in graphical user interfaces (GUIs) that leverages vision-language models (VLMs) and attention mechanisms. Unlike traditional approaches that generate screen coordinates, GUI-Actor operates in a coordinate-free manner, aligning an action token with relevant visual patches to identify action regions efficiently. This method addresses limitations such as weak spatial-semantic alignment and ambiguity in supervision targets, leading to better generalization across different screen layouts. The results demonstrate that GUI-Actor significantly outperforms existing methods, achieving state-of-the-art performance on various benchmarks while allowing for efficient fine-tuning.'}, 'zh': {'title': 'GUI-Actor：无坐标的高效GUI定位方法', 'desc': '本文提出了一种名为GUI-Actor的基于视觉语言模型（VLM）的方法，用于无坐标的图形用户界面（GUI）定位。该方法通过引入基于注意力的动作头，能够在一次前向传播中将特定的<ACTOR>标记与相关的视觉补丁标记对齐，从而有效地提出一个或多个动作区域。实验结果表明，GUI-Actor在多个GUI动作定位基准上超越了现有的最先进方法，并且在未见过的屏幕分辨率和布局上具有更好的泛化能力。此外，通过引入验证器，GUI-Actor能够在保持VLM主干不变的情况下，仅微调新引入的动作头，便能实现与之前最先进模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23061', 'title': 'DINGO: Constrained Inference for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2505.23061', 'abstract': "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference", 'score': 22, 'issue_id': 4112, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c215c7998d0a7928', 'authors': ['Tarun Suresh', 'Debangshu Banerjee', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.23061.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#optimization', '#diffusion'], 'emoji': '🧮', 'ru': {'title': 'DINGO: Структурированное декодирование для диффузионных языковых моделей', 'desc': 'Статья представляет DINGO - новую стратегию декодирования для диффузионных языковых моделей. DINGO использует динамическое программирование для обеспечения структурированных ограничений вывода, что значительно улучшает производительность в задачах генерации символьной математики и JSON. В отличие от авторегрессионных моделей, диффузионные модели предсказывают блок токенов параллельно, что делает традиционные алгоритмы декодирования с ограничениями неэффективными. DINGO позволяет выбирать выходные строки с наивысшей вероятностью, строго соблюдая заданные регулярные выражения.'}, 'en': {'title': 'DINGO: Structuring Success in Diffusion Language Models', 'desc': 'DINGO is a new decoding strategy that improves diffusion language models by applying structured output constraints. This method allows the models to generate outputs that meet specific requirements, like those found in symbolic math and JSON formats. Unlike traditional autoregressive models that generate text one token at a time, DINGO uses dynamic programming to efficiently handle multiple tokens simultaneously while ensuring the output adheres to user-defined rules. As a result, DINGO significantly enhances the performance of diffusion models, achieving up to a 68 percentage point improvement in relevant tasks.'}, 'zh': {'title': 'DINGO：提升扩散模型的结构化输出能力', 'desc': 'DINGO是一种基于动态规划的解码策略，旨在增强扩散语言模型的性能。它通过强制执行结构化输出约束，显著提高了在符号数学和JSON生成任务上的表现。与传统的自回归模型不同，扩散模型能够并行预测一组标记，这使得传统的约束解码算法无法有效应用。DINGO通过高效且可证明的分布保持方法，确保生成的输出字符串符合用户指定的正则表达式，从而在标准基准测试中实现了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01674', 'title': 'MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2506.01674', 'abstract': "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.", 'score': 21, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'baebffe54058ae77', 'authors': ['Yipeng Du', 'Tiehan Fan', 'Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Xiang Li', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01674.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на движение: MotionSight улучшает понимание видео без обучения', 'desc': 'MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео мультимодальными большими языковыми моделями (MLLM). Он использует объектно-ориентированное визуальное выделение и размытие движения в качестве подсказок для моделей. Авторы также создали крупномасштабный датасет MotionVid-QA с иерархическими аннотациями для оценки понимания движения в видео. Эксперименты показывают, что MotionSight достигает лучших результатов среди открытых моделей и конкурентоспособен с коммерческими решениями.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Understanding with MotionSight', 'desc': 'MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos.'}, 'zh': {'title': 'MotionSight：提升视频运动理解的新方法', 'desc': 'MotionSight是一种零样本方法，利用以物体为中心的视觉聚焦和运动模糊作为提示，显著提升了细粒度视频运动理解的能力。该方法在MotionVid-QA数据集上取得了最先进的性能，该数据集具有层次化的注释。尽管多模态大型语言模型在视频运动理解方面取得了一些进展，但仍然存在显著的局限性，尤其是在处理细微的视觉线索时。通过引入MotionSight，我们展示了如何在不进行训练的情况下，利用视觉提示来解耦物体和相机运动线索，从而提高运动感知能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03065', 'title': 'Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.03065', 'abstract': 'Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.', 'score': 20, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c51b4ca89c790b8c', 'authors': ['Pengtao Chen', 'Xianfang Zeng', 'Maosen Zhao', 'Peng Ye', 'Mingzhu Shen', 'Wei Cheng', 'Gang Yu', 'Tao Chen'], 'affiliations': ['Fudan University', 'Imperial College London', 'StepFun', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03065.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью разреженных трансформеров', 'desc': 'Исследователи предложили метод Sparse-vDiT для ускорения Video Diffusion Transformer (vDiT), используя разреженные паттерны в картах внимания. Они выявили три повторяющихся паттерна разреженности: диагональный, мульти-диагональный и вертикально-полосатый. Sparse-vDiT включает оптимизированные разреженные ядра для каждого паттерна и алгоритм поиска оптимальной стратегии разреженных вычислений. Интеграция Sparse-vDiT в современные модели vDiT позволила значительно сократить количество операций с плавающей запятой и ускорить вывод без существенной потери качества изображения.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention Patterns', 'desc': 'The paper introduces Sparse-vDiT, a framework designed to enhance the efficiency of Video Diffusion Transformers (vDiT) by utilizing identified sparsity patterns in attention maps. By analyzing these patterns, the authors found that certain attention heads can be skipped, leading to reduced computational complexity and faster inference times. Sparse-vDiT employs pattern-optimized sparse kernels and an offline search algorithm to determine the best sparse computation strategy for each layer and head. This approach results in significant reductions in theoretical FLOPs and actual inference speedups while preserving high visual quality in generated videos.'}, 'zh': {'title': '利用稀疏性加速视频生成的创新之路', 'desc': 'Sparse-vDiT通过利用注意力图中的稀疏模式，加速了视频扩散变换器（vDiT），在不显著降低视觉质量的情况下，减少了理论计算量（FLOPs）并提高了推理速度。研究发现，vDiT中的注意力图存在对角线、多对角线和垂直条纹等三种稀疏模式，并且可以跳过3-6%的注意力头。我们提出的Sparse-vDiT框架包括优化稀疏内核和离线稀疏扩散搜索算法，以选择每层和每个头的最佳稀疏计算策略。通过将Sparse-vDiT集成到最先进的vDiT模型中，取得了显著的计算效率提升，同时保持了高视觉保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.00070', 'title': 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics', 'url': 'https://huggingface.co/papers/2506.00070', 'abstract': 'Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.', 'score': 19, 'issue_id': 4114, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '0a2372063dd0aba7', 'authors': ['Dongyoung Kim', 'Sumin Park', 'Huiwon Jang', 'Jinwoo Shin', 'Jaehyung Kim', 'Younggyo Seo'], 'affiliations': ['KAIST', 'Real World Inc.', 'UC Berkeley', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00070.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#rl', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Robot-R1: Революция в обучении роботов через воплощенные рассуждения', 'desc': 'Статья представляет новый подход Robot-R1 для улучшения воплощенных рассуждений в робототехнике с использованием обучения с подкреплением. В отличие от традиционного метода Supervised Fine-Tuning (SFT), Robot-R1 оптимизирует модель для более точного прогнозирования следующего ключевого состояния, необходимого для выполнения задачи. Эксперименты показывают, что модели, обученные с помощью Robot-R1, превосходят методы SFT в задачах воплощенных рассуждений. Несмотря на то, что модель имеет всего 7 миллиардов параметров, она превосходит GPT-4 в задачах рассуждений, связанных с низкоуровневым управлением действиями.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Robot Control with Robot-R1', 'desc': 'This paper presents Robot-R1, a new framework that improves robot control by using reinforcement learning instead of traditional Supervised Fine-Tuning (SFT). SFT often suffers from issues like catastrophic forgetting and poorly constructed datasets, which can hinder robot performance. Robot-R1 focuses on predicting the next keypoint state needed for task completion by utilizing current scene images and expert demonstration data. The results show that Robot-R1 outperforms SFT methods, even with fewer parameters, in tasks requiring low-level action control and embodied reasoning.'}, 'zh': {'title': 'Robot-R1：提升机器人控制的具身推理新框架', 'desc': '大型视觉语言模型（LVLMs）在机器人技术中展现出巨大的潜力，通过结合具身推理与机器人控制来推动进步。传统的监督微调（SFT）方法在训练时常常使用启发式构建的数据集，这些数据集并未针对机器人控制进行优化。SFT方法还可能导致灾难性遗忘和泛化性能下降的问题。为了解决这些问题，我们提出了Robot-R1框架，利用强化学习来增强机器人控制的具身推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03136', 'title': 'Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.03136', 'abstract': "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE", 'score': 18, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a5362c4ed28a3b0', 'authors': ['Yinjie Wang', 'Ling Yang', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.03136.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#rl', '#games'], 'emoji': '🧠', 'ru': {'title': 'CURE: эволюция кодирования и тестирования без эталонов', 'desc': 'CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного кода. Модели ReasonFlux-Coder, созданные с помощью CURE, показывают значительное улучшение точности генерации кода по сравнению с аналогичными моделями. Система позволяет тестировщику учиться непосредственно на ошибках программиста и естественным образом распространяется на смежные задачи. Примечательно, что модель CURE может служить эффективной моделью вознаграждения для обучения с подкреплением базовых моделей.'}, 'en': {'title': 'CURE: Evolving Code and Tests Together for Better Accuracy', 'desc': "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."}, 'zh': {'title': 'CURE：无监督强化学习提升代码生成与测试准确性', 'desc': 'CURE是一个强化学习框架，旨在提高代码和单元测试生成的准确性，而无需真实标签的监督。该框架通过设计专门的奖励机制，使编码和单元测试生成能力能够相互演化，从而直接从编码者的错误中学习。经过优化后，我们的ReasonFlux-Coder模型在代码生成准确性上提高了5.3%，在最佳准确性上提高了9.0%。此外，该模型在单元测试生成中表现出64.8%的推理效率，展现了其在下游任务中的灵活性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.03131', 'title': 'Native-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2506.03131', 'abstract': 'A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.', 'score': 16, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '91e9b714c6e83924', 'authors': ['Zidong Wang', 'Lei Bai', 'Xiangyu Yue', 'Wanli Ouyang', 'Yiyuan Zhang'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.03131.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#synthetic', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'NiT: Революция в генерации изображений без ограничений разрешения', 'desc': 'Представлена новая генеративная модель NiT (Native-resolution diffusion Transformer), способная синтезировать изображения высокого разрешения с различными соотношениями сторон. Модель преодолевает ограничения традиционных методов с фиксированным разрешением, обрабатывая визуальные токены переменной длины. NiT демонстрирует передовую производительность на бенчмарках ImageNet и способность к обобщению zero-shot на ранее невиданные разрешения и соотношения сторон. Эти результаты указывают на потенциал нативного моделирования разрешений как моста между визуальными генеративными моделями и продвинутыми методологиями больших языковых моделей.'}, 'en': {'title': 'Revolutionizing Image Synthesis with Native-Resolution Modeling', 'desc': 'The paper presents a new generative model called the Native-resolution diffusion Transformer (NiT) that can create high-resolution images with various aspect ratios. Unlike traditional models that work with fixed-size images, NiT can handle images of any size by using variable-length visual tokens. This model not only achieves top performance on standard image benchmarks but also shows impressive zero-shot generalization, meaning it can generate high-quality images at new resolutions without additional training. The findings suggest that NiT could significantly advance the field of image synthesis by integrating techniques from both visual generative modeling and large language models.'}, 'zh': {'title': '原生分辨率建模：图像生成的新纪元', 'desc': '本文介绍了一种新颖的生成模型——原生分辨率扩散变换器（NiT），它能够合成高分辨率和多种宽高比的图像，表现出卓越的性能和零样本泛化能力。该方法克服了传统固定分辨率图像方法的局限，通过原生处理可变长度的视觉标记，解决了传统技术的核心挑战。NiT架构专门设计用于在去噪过程中显式建模不同的分辨率和宽高比，能够从各种分辨率和宽高比的图像中学习内在的视觉分布。研究表明，NiT在多个基准测试中表现出色，能够生成高保真度的图像，展示了原生分辨率建模在视觉生成建模和先进大语言模型方法之间的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.03126', 'title': 'AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03126', 'abstract': 'AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.', 'score': 15, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '3cc5928482bd8262', 'authors': ['Lu Qiu', 'Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03126.jpg', 'data': {'categories': ['#video', '#multimodal', '#story_generation', '#diffusion', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'AnimeShooter: новый уровень генерации связной анимации с опорными изображениями', 'desc': 'AnimeShooter - это набор данных для создания многокадровой анимации с использованием опорных изображений. Он включает иерархические аннотации и обеспечивает визуальную согласованность между кадрами. На основе этого датасета разработана модель AnimeShooterGen, использующая мультимодальные языковые модели и видео-диффузию. Эксперименты показали, что AnimeShooterGen превосходит аналоги по согласованности кадров и соответствию опорным изображениям.'}, 'en': {'title': 'Enhancing Animation with Reference-Guided Multi-Shot Datasets', 'desc': 'AnimeShooter is a new dataset designed to improve the generation of coherent multi-shot animations by providing detailed hierarchical annotations and ensuring visual consistency. It includes story-level annotations that outline the narrative and character profiles, as well as shot-level annotations that break down the story into individual scenes with specific details. The dataset also features a subset with synchronized audio tracks to enhance the animation experience. To utilize this dataset, AnimeShooterGen employs Multimodal Large Language Models (MLLMs) and video diffusion models, resulting in improved visual consistency and adherence to character references in generated animations.'}, 'zh': {'title': '提升动画生成的连贯性与一致性', 'desc': 'AnimeShooter是一个参考引导的多镜头动画数据集，旨在提高连贯的动画视频生成。该数据集通过全面的层次注释和视觉一致性，帮助生成具有叙事脚本和角色参考的动画片段。我们还推出了AnimeShooterGen，利用多模态大语言模型（MLLMs）和视频扩散模型，取得了更好的生成效果。实验结果表明，基于AnimeShooter训练的模型在跨镜头视觉一致性和遵循参考视觉指导方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.02497', 'title': 'LumosFlow: Motion-Guided Long Video Generation', 'url': 'https://huggingface.co/papers/2506.02497', 'abstract': 'LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/', 'score': 14, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c7f37e23b510618b', 'authors': ['Jiahao Chen', 'Hangjie Yuan', 'Yichen Qian', 'Jingyun Liang', 'Jiazheng Xing', 'Pengwei Liu', 'Weihua Chen', 'Fan Wang', 'Bing Su'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02497.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'LumosFlow: плавная генерация длинных видео с помощью иерархической интерполяции кадров', 'desc': 'LumosFlow - это новый подход к генерации длинных видео с использованием иерархического пайплайна. Система применяет LMTV-DM для создания ключевых кадров с большими интервалами движения, обеспечивая разнообразие контента. Затем LOF-DM синтезирует сложные оптические потоки между ключевыми кадрами, а MotionControlNet улучшает качество промежуточных кадров. LumosFlow позволяет достичь 15-кратной интерполяции, обеспечивая плавное и непрерывное движение между кадрами.'}, 'en': {'title': 'LumosFlow: Smooth Long Video Generation with Motion Guidance', 'desc': 'LumosFlow is a novel framework designed for generating long videos with smooth transitions and coherent motion. It utilizes the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to create key frames that capture significant motion, enhancing the diversity of the video content. For the interpolation of frames between these key frames, it employs the Latent Optical Flow Diffusion Model (LOF-DM) to generate complex motion flows, followed by MotionControlNet for refining the results. This approach significantly improves the quality of long video generation, achieving 15 times faster interpolation while maintaining consistent motion and appearance throughout the video.'}, 'zh': {'title': 'LumosFlow：高效生成连贯长视频的创新框架', 'desc': 'LumosFlow 是一个用于长视频生成的框架，采用 LMTV-DM 生成关键帧，并使用 LOF-DM 和 MotionControlNet 进行平滑的中间帧插值。该方法通过显式引入运动指导，解决了传统方法在生成长视频时面临的时间一致性和视觉连贯性问题。LumosFlow 通过生成具有较大运动间隔的关键帧，确保了生成视频内容的多样性。实验表明，该方法能够生成具有一致运动和外观的长视频，且插值速度比传统方法快 15 倍。'}}}, {'id': 'https://huggingface.co/papers/2506.02528', 'title': 'RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.02528', 'abstract': "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.", 'score': 13, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'e8011f9f9decd752', 'authors': ['Yan Gong', 'Yiren Song', 'Yicheng Li', 'Chenglin Li', 'Yin Zhang'], 'affiliations': ['National University of Singapore', 'Zhe Jiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02528.jpg', 'data': {'categories': ['#cv', '#dataset', '#transfer_learning', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'RelationAdapter: Умное редактирование изображений по паре примеров', 'desc': 'RelationAdapter - это легковесный модуль, улучшающий работу моделей Diffusion Transformer для захвата и применения визуальных преобразований с использованием пар изображений источник-цель. Он вдохновлен механизмом обучения в контексте больших языковых моделей (LLM) и направлен на обобщаемое редактирование изображений на основе визуальных подсказок. RelationAdapter позволяет эффективно понимать и переносить намерения редактирования, значительно улучшая качество генерации и общую производительность редактирования. Для оценки обобщающей способности и адаптивности модели авторы также представили набор данных Relation252K, содержащий 218 разнообразных задач редактирования.'}, 'en': {'title': 'Enhancing Image Editing with RelationAdapter', 'desc': "The paper introduces RelationAdapter, a new lightweight module designed to enhance Diffusion Transformer models for image editing tasks. It focuses on using source-target image pairs to effectively capture and apply visual transformations, which improves the model's performance on diverse editing tasks. By leveraging this approach, RelationAdapter addresses the limitations of existing methods that struggle with non-rigid transformations and generalization. The authors also present Relation252K, a dataset that includes 218 diverse editing tasks to evaluate the model's adaptability and effectiveness in visual prompt-driven scenarios."}, 'zh': {'title': '提升图像编辑性能的轻量级模块', 'desc': 'RelationAdapter 是一个轻量级模块，旨在增强扩散变换器模型的能力，以有效捕捉和应用视觉变换。它通过源-目标图像对来提取和转移内容感知的编辑意图，从而改善编辑性能和在多样任务上的泛化能力。与现有的单参考方法不同，RelationAdapter 能够处理非刚性变换，提升图像编辑的灵活性。我们还引入了 Relation252K 数据集，以评估模型在视觉提示驱动场景中的泛化和适应能力。'}}}, {'id': 'https://huggingface.co/papers/2506.01144', 'title': 'FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.01144', 'abstract': "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.", 'score': 12, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7bd7aa77c6143afb', 'authors': ['Ariel Shaulov', 'Itay Hazan', 'Lior Wolf', 'Hila Chefer'], 'affiliations': ['School of Computer Science Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2506.01144.jpg', 'data': {'categories': ['#video', '#training', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Улучшение движения в видео без переобучения модели', 'desc': 'FlowMo - это метод без дополнительного обучения, который улучшает согласованность движения в предобученных диффузионных моделях для генерации видео по тексту. Он использует собственные предсказания модели для уменьшения покадровой временной вариативности. FlowMo извлекает временное представление, свободное от влияния внешнего вида, измеряя расстояние между латентными представлениями последовательных кадров. Затем метод оценивает согласованность движения, измеряя вариативность по временной оси, и направляет модель на уменьшение этой вариативности во время семплирования.'}, 'en': {'title': 'Enhancing Motion Coherence in Video Generation Without Training', 'desc': "FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts."}, 'zh': {'title': 'FlowMo：提升视频生成运动一致性的新方法', 'desc': 'FlowMo是一种无训练的方法，旨在提高预训练文本到视频扩散模型中的运动一致性。它通过利用模型自身的预测，减少了时间维度上的补丁方差，从而增强了运动的连贯性。与传统方法不同，FlowMo不需要重新训练模型或引入外部条件信号，而是直接从模型的预测中提取有意义的时间表示。实验结果表明，FlowMo在多个文本到视频模型中显著提高了运动一致性，同时保持了视觉质量和提示对齐，提供了一种有效的即插即用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.00910', 'title': 'PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.00910', 'abstract': 'ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.', 'score': 10, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '375d26a9868ef2fe', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Dongseop Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.00910.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'ActiveKD: Эффективное обучение с ограниченными данными через синергию активного обучения и дистилляции знаний', 'desc': 'ActiveKD - это новый подход, объединяющий активное обучение и дистилляцию знаний с использованием крупных визуально-языковых моделей для эффективного выбора разнообразных неразмеченных образцов для аннотации. Ключевым аспектом ActiveKD является структурированное смещение предсказаний визуально-языковых моделей, которое рассматривается как индуктивное смещение учительской модели. Для использования этого смещения предлагается стратегия Probabilistic CoreSet (PCoreSet), максимизирующая покрытие в пространстве вероятностей. Оценки на 11 наборах данных показывают, что PCoreSet стабильно превосходит существующие методы выбора в рамках ActiveKD.'}, 'en': {'title': 'Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation', 'desc': 'ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.'}, 'zh': {'title': '主动学习与知识蒸馏的高效结合', 'desc': 'ActiveKD是一个将主动学习与知识蒸馏相结合的框架，旨在高效选择多样化的未标注样本进行标注。知识蒸馏通常需要足够的标注数据，而主动学习则在数据稀缺的情况下工作，因此两者的结合尚未得到充分探索。ActiveKD利用大型视觉-语言模型的零样本和少样本能力，通过概率空间中的结构化预测偏差来优化样本选择。我们提出的概率核心集（PCoreSet）策略能够在有限的标注预算下，选择具有类别多样性的未标注样本，从而更有效地传递教师模型的知识。'}}}, {'id': 'https://huggingface.co/papers/2506.03123', 'title': 'DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03123', 'abstract': 'Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model~(DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.', 'score': 9, 'issue_id': 4122, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'b284c16ff31a205f', 'authors': ['Zhengyao Lv', 'Chenyang Si', 'Tianlin Pan', 'Zhaoxi Chen', 'Kwan-Yee K. Wong', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.03123.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#diffusion', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Эффективный синтез видео с помощью двух экспертов', 'desc': 'Авторы статьи предлагают новый подход к ускорению диффузионных моделей для синтеза видео. Они выявили проблему конфликтующей динамики обучения при дистилляции моделей согласованности, что приводит к ухудшению временной согласованности и деталей изображения. Для решения этой проблемы предложена модель Dual-Expert Consistency Model (DCM) с двумя экспертами: семантическим и детализирующим. Также введены новые функции потерь для улучшения согласованности движения и качества синтеза.'}, 'en': {'title': 'Expert Specialization for Enhanced Video Synthesis', 'desc': 'This paper addresses the challenges of using Diffusion Models for video synthesis, particularly the high computational cost due to iterative denoising steps. It highlights the limitations of applying Consistency Models directly to video diffusion, which can lead to poor temporal consistency and loss of detail. The authors propose a Dual-Expert Consistency Model (DCM) that separates the learning tasks into two experts: one for semantic understanding and motion, and another for fine detail refinement. By introducing a Temporal Coherence Loss and utilizing GAN and Feature Matching Loss, the model achieves high visual quality with fewer sampling steps, showcasing the benefits of expert specialization in improving video synthesis.'}, 'zh': {'title': '专家专注，提升视频合成质量', 'desc': '扩散模型在视频合成中取得了显著成果，但需要多次去噪步骤，导致计算开销大。一致性模型在加速扩散模型方面取得了重要进展，但直接应用于视频扩散模型时，往往会导致时间一致性和外观细节的严重退化。本文通过分析一致性模型的训练动态，识别出在蒸馏过程中存在的关键冲突学习动态，导致蒸馏学生模型无法达到最佳状态。为了解决这个问题，我们提出了一种参数高效的双专家一致性模型（DCM），通过语义专家和细节专家的专业化来提高视频扩散模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01789', 'title': "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability", 'url': 'https://huggingface.co/papers/2506.01789', 'abstract': 'High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.', 'score': 9, 'issue_id': 4111, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '08a1fd6c4eff1198', 'authors': ['Genta Indra Winata', 'David Anugraha', 'Emmy Liu', 'Alham Fikri Aji', 'Shou-Yi Hung', 'Aditya Parashar', 'Patrick Amadeus Irawan', 'Ruochen Zhang', 'Zheng-Xin Yong', 'Jan Christian Blaise Cruz', 'Niklas Muennighoff', 'Seungone Kim', 'Hanyang Zhao', 'Sudipta Kar', 'Kezia Erina Suryoraharjo', 'M. Farid Adilazuarda', 'En-Shiun Annie Lee', 'Ayu Purwarianti', 'Derry Tanti Wijaya', 'Monojit Choudhury'], 'affiliations': ['Brown University', 'Capital One', 'Carnegie Mellon University', 'Columbia University', 'ITB', 'MBZUAI', 'Monash University', 'Ontario Tech University', 'Oracle', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01789.jpg', 'data': {'categories': ['#open_source', '#data', '#synthetic', '#dataset'], 'emoji': '📊', 'ru': {'title': 'DataRubrics: Новый стандарт качества датасетов в эпоху ИИ', 'desc': 'Эта статья предлагает новый подход к оценке качества наборов данных для машинного обучения. Авторы вводят концепцию DataRubrics - структурированную систему для оценки качества как человеческих, так и сгенерированных моделями датасетов. DataRubrics использует последние достижения в оценке с помощью языковых моделей (LLM) для обеспечения воспроизводимого, масштабируемого и действенного решения. Статья также рассматривает методы синтетической генерации данных и призывает к интеграции систематических метрик оценки в процесс рецензирования датасетов.'}, 'en': {'title': 'Enhancing Dataset Quality with DataRubrics', 'desc': 'This paper discusses the importance of high-quality datasets in machine learning and the challenges in creating them, particularly regarding human annotations. It highlights issues with current dataset submissions, such as lack of originality and inadequate quality control, which are often missed during peer review. The authors propose a new framework called DataRubrics, which introduces systematic, rubric-based evaluation metrics to improve dataset quality assessment. Additionally, they explore methods for generating synthetic data and emphasize the need for reproducibility in evaluations using recent advancements in large language models (LLMs).'}, 'zh': {'title': '提升数据集质量的系统化评估框架', 'desc': '高质量的数据集对于训练和评估机器学习模型至关重要，但创建这些数据集尤其是准确的人类标注仍然是一个重大挑战。许多数据集论文缺乏原创性、多样性或严格的质量控制，且这些问题在同行评审中常常被忽视。本文提倡在数据集审查过程中整合系统化的评分标准，以提高数据质量评估的标准化和可测量性。我们还介绍了DataRubrics，一个用于评估人类和模型生成数据集质量的结构化框架，旨在提升数据驱动研究的标准。'}}}, {'id': 'https://huggingface.co/papers/2506.01716', 'title': 'Self-Challenging Language Model Agents', 'url': 'https://huggingface.co/papers/2506.01716', 'abstract': 'The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.', 'score': 5, 'issue_id': 4123, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '292019a70795a800', 'authors': ['Yifei Zhou', 'Sergey Levine', 'Jason Weston', 'Xian Li', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.01716.jpg', 'data': {'categories': ['#optimization', '#agi', '#training', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучение ИИ: агент бросает вызов самому себе', 'desc': 'Статья представляет новый подход к обучению интеллектуальных агентов, называемый Self-Challenging. В этой модели агент сам генерирует задачи в формате Code-as-Task, включающие инструкции, функции верификации и тестовые примеры. Затем агент обучается на этих самостоятельно созданных задачах с помощью обучения с подкреплением. Эксперименты показали, что данный метод значительно улучшает производительность модели Llama-3.1-8B-Instruct в бенчмарках по использованию инструментов, несмотря на использование только самогенерируемых данных.'}, 'en': {'title': 'Empowering Agents Through Self-Generated Challenges', 'desc': "The Self-Challenging framework enhances the training of intelligent agents by allowing them to create their own tasks, known as Code-as-Task. This approach eliminates the need for extensive human-generated tasks, as the agent generates high-quality tasks through its interactions with tools. Each task includes an instruction, a verification function, and defined success and failure cases, which help ensure the tasks are effective for training. The framework utilizes reinforcement learning to improve the agent's performance, achieving significant gains in benchmarks with only self-generated data."}, 'zh': {'title': '自我挑战，智能体训练的新方法', 'desc': '自我挑战框架通过自我生成的任务来训练智能体，这些任务被定义为代码任务，从而提高了在多轮工具使用基准测试中的表现。该框架允许智能体在与工具互动后，首先扮演挑战者角色生成高质量任务。生成的任务包括指令、验证函数以及解决方案和失败案例，确保任务的质量。最终，智能体作为执行者，利用强化学习在这些任务上进行训练，并通过评估反馈作为奖励，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.00413', 'title': 'Accelerating Diffusion LLMs via Adaptive Parallel Decoding', 'url': 'https://huggingface.co/papers/2506.00413', 'abstract': 'Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.', 'score': 5, 'issue_id': 4113, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '42d7cf22876b9eef', 'authors': ['Daniel Israel', 'Guy Van den Broeck', 'Aditya Grover'], 'affiliations': ['Department of Computer Science University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.00413.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Статья представляет новый метод адаптивного параллельного декодирования (APD) для диффузионных больших языковых моделей (dLLM). APD динамически регулирует количество токенов, генерируемых параллельно, что позволяет значительно увеличить пропускную способность без существенного ухудшения качества. Метод использует мультипликативную смесь маргинальных вероятностей dLLM и совместной вероятности последовательностей под небольшой вспомогательной авторегрессионной моделью. APD оптимизирован с помощью KV-кэширования и ограничения размера маскированного входа, что обеспечивает гибкий компромисс между пропускной способностью и качеством.'}, 'en': {'title': 'Boosting Speed in Language Models with Adaptive Parallel Decoding', 'desc': 'Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.'}, 'zh': {'title': '自适应并行解码：提升生成速度与质量的平衡', 'desc': '自适应并行解码（APD）通过动态调整并行生成的标记数量，提升了扩散大型语言模型（dLLMs）的吞吐量，而不会显著降低质量。传统的自回归解码方法使得生成速度受到限制，因为标记是一个接一个地预测的。尽管理论上扩散大型语言模型允许并行生成标记，但在实际应用中，往往难以在不牺牲质量的情况下达到自回归模型的速度。我们的方法通过定义dLLM边际概率与小型自回归模型下序列的联合概率之间的乘法混合，来实现这一目标，并通过启用KV缓存和限制掩码输入的大小进一步优化APD。'}}}, {'id': 'https://huggingface.co/papers/2505.22704', 'title': 'Training Language Models to Generate Quality Code with Program Analysis\n  Feedback', 'url': 'https://huggingface.co/papers/2505.22704', 'abstract': 'A reinforcement learning framework improves code quality in large language models by using automated feedback from program analysis and unit tests.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, a reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality.', 'score': 5, 'issue_id': 4128, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '795ffdf04386035f', 'authors': ['Feng Yao', 'Zilong Wang', 'Liyuan Liu', 'Junxia Cui', 'Li Zhong', 'Xiaohan Fu', 'Haohui Mai', 'Vish Krishnan', 'Jianfeng Gao', 'Jingbo Shang'], 'affiliations': ['CausalFlow Inc.', 'Microsoft Research', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.22704.jpg', 'data': {'categories': ['#security', '#rl', '#dataset', '#training', '#optimization'], 'emoji': '🔧', 'ru': {'title': 'Обучение с подкреплением для повышения качества кода от LLM', 'desc': 'Предложена система REAL, использующая обучение с подкреплением для улучшения качества кода, генерируемого большими языковыми моделями (LLM). REAL применяет автоматизированную обратную связь от анализа программ и модульных тестов для стимулирования LLM генерировать код производственного качества. Система фокусируется на улучшении безопасности и поддерживаемости кода, не требуя ручного вмешательства. Эксперименты показывают, что REAL превосходит современные методы по функциональности и качеству кода.'}, 'en': {'title': 'Enhancing Code Quality with Reinforcement Learning', 'desc': 'This paper presents REAL, a reinforcement learning framework designed to enhance the quality of code generated by large language models (LLMs). It addresses common issues in code generation, such as security vulnerabilities and maintainability problems, by utilizing automated feedback from program analysis and unit tests. Unlike traditional methods that require extensive manual annotations, REAL operates in a prompt-agnostic manner, allowing for scalable and efficient supervision. Experimental results show that REAL significantly improves both functionality and code quality compared to existing techniques.'}, 'zh': {'title': '强化学习提升代码质量，助力高效编程', 'desc': '本文提出了一种名为REAL的强化学习框架，旨在提高大型语言模型生成代码的质量。通过自动化反馈，REAL结合了程序分析和单元测试，帮助模型识别安全性和可维护性缺陷。与传统的监督微调和基于规则的后处理方法不同，REAL不依赖于人工标注，具有更好的可扩展性。实验结果表明，REAL在功能性和代码质量的评估上优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.01274', 'title': 'ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01274', 'abstract': "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.", 'score': 4, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '401bd39fc17a47b7', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2506.01274.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#video', '#optimization', '#rl', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Умный выбор кадров для лучшего понимания видео ИИ', 'desc': 'ReFoCUS - это новый подход к оптимизации выбора кадров для видео-LLM с использованием обучения с подкреплением. Он улучшает способность модели рассуждать о видеоконтенте, выбирая кадры в соответствии с предпочтениями самой модели. ReFoCUS использует автореггрессивную архитектуру для эффективного исследования пространства кадров и не требует явной разметки на уровне кадров. Этот метод последовательно улучшает производительность в задачах видео-вопросов и ответов на нескольких бенчмарках.'}, 'en': {'title': 'Optimizing Frame Selection for Better Video Reasoning', 'desc': "ReFoCUS is a novel framework that uses reinforcement learning to improve how video-LLMs select frames for video question answering (QA). Instead of relying on fixed rules or external systems, it learns which frames are most relevant to the questions being asked. By optimizing frame selection based on the model's preferences, ReFoCUS enhances the reasoning capabilities of video-LLMs. This method not only simplifies the selection process but also boosts performance on various video QA tasks without needing detailed supervision for each frame."}, 'zh': {'title': '优化视频帧选择，提升推理能力', 'desc': 'ReFoCUS是一种利用强化学习优化视频-大语言模型（video-LLM）帧选择的方法，旨在提高视频问答中的推理性能。该方法通过学习帧选择策略，关注视觉输入的选择，而不是仅仅依赖文本响应。ReFoCUS使用来自参考大多模态模型的奖励信号，反映模型对支持时间相关响应的最佳帧的偏好。通过自回归条件选择架构，ReFoCUS有效探索帧空间，同时保持时间一致性，显著提升了多个视频问答基准的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03096', 'title': 'FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens', 'url': 'https://huggingface.co/papers/2506.03096', 'abstract': 'Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a94488665c463be', 'authors': ['Christian Schlarmann', 'Francesco Croce', 'Nicolas Flammarion', 'Matthias Hein'], 'affiliations': ['Tübingen AI Center University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03096.jpg', 'data': {'categories': ['#architecture', '#dataset', '#games', '#alignment', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Ранняя фьюжн для мультимодального ИИ', 'desc': 'FuseLIP - это новая архитектура для мультимодального встраивания, использующая единую трансформерную модель для обработки текстовых и изображенческих токенов. В отличие от традиционных подходов с поздним слиянием, FuseLIP позволяет модальностям взаимодействовать на каждом уровне кодирования. Авторы собрали новые наборы данных для предобучения и оценки модели. FuseLIP превосходит другие подходы в задачах мультимодального встраивания, таких как визуальные вопросы и ответы, сохраняя сопоставимую производительность в одномодальных задачах.'}, 'en': {'title': 'FuseLIP: Unifying Text and Image with Early Fusion for Better Multimodal Understanding', 'desc': 'This paper introduces FuseLIP, a novel architecture for multimodal embedding that combines text and image features using a single transformer model. Unlike traditional methods that merge features from separate encoders, FuseLIP employs an early fusion strategy, allowing text and image tokens to interact throughout the encoding process. This results in richer representations and improved performance on multimodal tasks like visual question answering (VQA) and text-guided image retrieval. The authors also present new datasets for training and evaluating multimodal models, demonstrating that FuseLIP outperforms existing methods in multimodal scenarios while maintaining competitive results in unimodal tasks.'}, 'zh': {'title': 'FuseLIP：多模态嵌入的新方法', 'desc': '这篇论文介绍了一种新的多模态嵌入架构FuseLIP，旨在改进文本和图像的特征对齐。与传统的后期融合方法不同，FuseLIP采用早期融合策略，通过单一的变换器模型处理扩展的文本和图像标记词汇。这样可以在编码的每个深度上实现不同模态的交互，从而获得更丰富的表示。实验结果表明，FuseLIP在多模态嵌入任务中表现优于其他方法，同时在单模态任务上与基线模型相当。'}}}, {'id': 'https://huggingface.co/papers/2506.03079', 'title': 'ORV: 4D Occupancy-centric Robot Video Generation', 'url': 'https://huggingface.co/papers/2506.03079', 'abstract': 'ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV', 'score': 3, 'issue_id': 4115, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '2bfb7d794c03a7ed', 'authors': ['Xiuyu Yang', 'Bohan Li', 'Shaocong Xu', 'Nan Wang', 'Chongjie Ye', 'Zhaoxi Chen', 'Minghan Qin', 'Yikang Ding', 'Xin Jin', 'Hang Zhao', 'Hao Zhao'], 'affiliations': ['AIR, Tsinghua University', 'Beijing Academy of Artificial Intelligence', 'ByteDance', 'Eastern Institute of Technology, Ningbo', 'IIIS, Tsinghua University', 'Megvii Technology', 'National University of Singapore', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03079.jpg', 'data': {'categories': ['#games', '#robotics', '#optimization', '#video'], 'emoji': '🤖', 'ru': {'title': 'ORV: Точный контроль роботов через 4D семантическую занятость', 'desc': 'ORV - это фреймворк для генерации видео с роботами, использующий последовательности 4D семантической занятости. Он позволяет создавать фотореалистичные, согласованные во времени и точно контролируемые видео роботов. ORV улучшает существующие методы, обеспечивая более точное семантическое и геометрическое руководство для генерации видео. Фреймворк поддерживает одновременную генерацию многоракурсных видео операций захвата роботом, что важно для задач обучения роботов.'}, 'en': {'title': 'ORV: Revolutionizing Robot Video Generation with 4D Occupancy Sequences', 'desc': 'The paper introduces ORV, a framework designed for generating robot videos using 4D semantic occupancy sequences. This approach enhances the quality of video generation by providing detailed semantic and geometric information, leading to photorealistic and temporally consistent outputs. ORV addresses the limitations of previous action-driven generative models, which often struggle with control precision and generalization. The framework also allows for the generation of multi-view videos, which is beneficial for various robotic learning applications, demonstrating superior performance compared to existing methods.'}, 'zh': {'title': '以占用为中心的机器人视频生成新框架', 'desc': 'ORV是一个以占用为中心的机器人视频生成框架，利用4D语义占用序列生成逼真的、时间一致的、可精确控制的机器人视频。该方法通过细粒度的占用表示，提供更准确的语义和几何指导，从而克服了现有方法的控制精度和泛化能力不足的问题。ORV能够无缝地将仿真数据转化为高质量的机器人视频，并支持同时生成多视角的视频，适用于后续的机器人学习任务。实验结果表明，ORV在多个数据集和子任务中均优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02338', 'title': 'One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL', 'url': 'https://huggingface.co/papers/2506.02338', 'abstract': "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  \t\t\t\t\tAI-generated summary \t\t\t\t With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.", 'score': 3, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c96bff52eb2a678f', 'authors': ['Hyungjoo Chae', 'Dongjin Kang', 'Jihyuk Kim', 'Beong-woo Kwak', 'Sunghyun Park', 'Haeju Park', 'Jinyoung Yeo', 'Moontae Lee', 'Kyungjae Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Chicago', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02338.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Длинные цепочки мышления для ИИ: новый подход к обучению моделей рассуждениям', 'desc': 'Статья представляет набор данных Long CoT Collection, созданный с помощью коротких моделей цепочек рассуждений (CoT LLM). Этот датасет содержит 100 тысяч примеров рассуждений и позволяет обучать модели длинным цепочкам мышления. Авторы разработали конвейер, который улучшает стратегии рассуждений коротких моделей и позволяет контролировать объем мыслительного процесса. Эксперименты показывают, что обучение на этом наборе данных улучшает навыки рассуждений и создает хорошую основу для обучения с подкреплением.'}, 'en': {'title': 'Empowering Reasoning with Long CoT Datasets', 'desc': 'The Long CoT Collection dataset is designed to improve reasoning skills in language models by using short chain-of-thought (CoT) models to generate long CoT inferences. This dataset consists of 100,000 annotated rationales that help models think more deeply and manage their reasoning process better. By training on this dataset, models can achieve reasoning quality similar to the established R1 model while also enhancing their performance in reinforcement learning tasks. The research highlights the potential for developing independent large reasoning models without relying solely on existing ones.'}, 'zh': {'title': '提升推理能力的长链思维数据集', 'desc': '本文介绍了一个名为Long CoT Collection的数据集，该数据集由短链思维（CoT）的大型语言模型（LLM）生成，旨在提升一般推理能力。研究表明，通过使用该数据集进行训练，可以获得与现有大型推理模型（如R1）相当的推理质量。我们开发了一种新方法，使短链思维模型能够进行更长时间的推理，并引入了对思维预算的可控性，以解决过度思考的问题。实验结果显示，基于该数据集训练的模型在强化学习任务中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.00391', 'title': 'SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL', 'url': 'https://huggingface.co/papers/2506.00391', 'abstract': 'SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.', 'score': 3, 'issue_id': 4122, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2e0ac7f48f4b8959', 'authors': ['Ge Qu', 'Jinyang Li', 'Bowen Qin', 'Xiaolong Li', 'Nan Huo', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.00391.jpg', 'data': {'categories': ['#low_resource', '#training', '#optimization', '#dataset', '#reasoning', '#small_models', '#data'], 'emoji': '🔍', 'ru': {'title': 'Точная коррекция SQL с помощью иерархического анализа действий', 'desc': 'SHARE - это система коррекции действий на основе SLM, которая улучшает работу больших языковых моделей в задаче преобразования текста в SQL. Она трансформирует SQL-запросы в траектории действий и применяет процесс детальной доработки. SHARE использует три специализированные малые языковые модели в последовательном конвейере для более точной локализации ошибок и эффективного исправления. Система также предлагает новую стратегию иерархической самоэволюции для эффективного обучения с ограниченными данными.'}, 'en': {'title': 'SHARE: Enhancing SQL Error Correction with Hierarchical Action Trajectories', 'desc': 'The paper introduces SHARE, a system designed to improve the self-correction capabilities of large language models (LLMs) in text-to-SQL tasks. It addresses the inefficiencies of traditional self-correction methods that rely heavily on recursive calls, which can be computationally expensive. SHARE utilizes a hierarchical approach with three specialized small language models (SLMs) to convert SQL queries into action trajectories, enhancing error detection and correction. The system also incorporates a novel training strategy that allows it to perform well even with limited data, making it suitable for applications where data privacy is a concern.'}, 'zh': {'title': '提升文本到SQL的自我纠正能力', 'desc': 'SHARE是一种基于小型语言模型（SLM）的层次化动作纠正系统，旨在提升大语言模型（LLM）在文本到SQL转换中的表现。它通过将SQL查询转化为逐步的动作轨迹，帮助LLM更好地理解和定位错误。该系统采用了两阶段的细化过程，提高了错误检测和纠正的效率。此外，SHARE还引入了一种新颖的层次自我进化策略，能够在数据资源有限的情况下进行有效训练，特别适用于有数据隐私限制的文本到SQL应用。'}}}, {'id': 'https://huggingface.co/papers/2505.24273', 'title': 'How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24273', 'abstract': 'This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '73012fc9edd07bd3', 'authors': ['Hongyi James Cai', 'Junlin Wang', 'Xiaoyin Chen', 'Bhuwan Dhingra'], 'affiliations': ['Duke University', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.24273.jpg', 'data': {'categories': ['#training', '#rl', '#synthetic', '#reasoning', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Возврат в рассуждениях: ключ к улучшению LLM', 'desc': 'Исследование изучает взаимодействие между обучением с учителем и обучением с подкреплением в больших языковых моделях (LLM), фокусируясь на роли возврата (backtracking) в улучшении способностей к рассуждению. Авторы проводят эксперименты на восьми задачах рассуждения, включая Судоку и геометрические головоломки. Результаты показывают, что более длинные цепочки рассуждений с возвратами обычно приводят к лучшему и более стабильному обучению с подкреплением. Исследование также демонстрирует, что обучение с подкреплением в основном не зависит от правильности длинных последовательностей рассуждений, а скорее от их структуры.'}, 'en': {'title': 'Enhancing Reasoning in LLMs through Backtracking and Training Synergy', 'desc': 'This paper explores how supervised fine-tuning (SFT) and reinforcement learning (RL) work together to improve reasoning in large language models (LLMs). It specifically examines the role of backtracking, a technique that allows models to revisit previous steps in their reasoning process, and how it enhances performance on various reasoning tasks. The study finds that while short chain-of-thought (CoT) sequences help in RL training, their effectiveness decreases with task difficulty. Ultimately, the research provides valuable insights into optimizing training strategies for better reasoning capabilities in LLMs.'}, 'zh': {'title': '优化推理能力的训练策略', 'desc': '本研究探讨了监督微调与强化学习在大型语言模型中的相互作用，重点关注回溯在增强推理能力中的作用。研究表明，监督微调（SFT）和强化学习（RL）可以有效提升模型在数学和逻辑问题上的推理能力。我们发现，短的链式思维（CoT）序列在SFT阶段对RL训练有一定贡献，但在任务难度增加时，这种贡献会减弱。通过实验，我们证实了回溯的频率和结构对推理训练的重要性，提供了优化训练策略的实用见解。'}}}, {'id': 'https://huggingface.co/papers/2505.18079', 'title': 'Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding', 'url': 'https://huggingface.co/papers/2505.18079', 'abstract': 'The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.', 'score': 3, 'issue_id': 4117, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6dbac78671d7d992', 'authors': ['Xiaoyi Zhang', 'Zhaoyang Jia', 'Zongyu Guo', 'Jiahao Li', 'Bin Li', 'Houqiang Li', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.18079.jpg', 'data': {'categories': ['#benchmark', '#video', '#long_context', '#reasoning', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Интеллектуальный агент для глубокого анализа длинных видео', 'desc': 'Статья представляет агента Deep Video Discovery для анализа длинных видео с использованием автономной стратегии поиска на основе больших языковых моделей (LLM). Агент преодолевает ограничения в понимании длинных видео путем сегментации и применения набора инструментов для поиска в многоуровневой видеобазе данных. DVD агент использует продвинутые возможности рассуждения LLM для планирования действий, выбора инструментов и итеративного уточнения своих внутренних рассуждений. Предложенный подход достигает лучших результатов на бенчмарках для понимания длинных видео, значительно превосходя предыдущие работы на наборе данных LVBench.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Autonomous Agents', 'desc': 'The Deep Video Discovery (DVD) agent introduces an innovative approach to long-form video understanding by utilizing an autonomous agentic search strategy powered by large language models (LLMs). This method addresses the challenges posed by the complexity of temporal and spatial information in lengthy videos, which traditional models struggle to analyze effectively. By segmenting videos and employing a search-centric toolkit, the DVD agent can dynamically plan and refine its reasoning based on real-time observations. The results demonstrate that this system achieves state-of-the-art performance on benchmarks like LVBench, significantly outperforming previous methods.'}, 'zh': {'title': '自主搜索，深度理解长视频', 'desc': '深度视频发现代理使用自主搜索策略，结合大型语言模型，克服了长视频理解中的局限性。该方法通过对分段视频片段进行智能搜索，提升了在复杂时空背景下的问题回答能力。与以往手动设计工作流程的视频代理不同，我们的代理强调自主性，利用多层次视频数据库中的搜索工具进行规划和选择。经过全面评估，我们的代理在长视频理解基准测试中表现出色，显著超越了之前的研究成果。'}}}, {'id': 'https://huggingface.co/papers/2506.03119', 'title': 'Controllable Human-centric Keyframe Interpolation with Generative Prior', 'url': 'https://huggingface.co/papers/2506.03119', 'abstract': 'Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity.', 'score': 2, 'issue_id': 4125, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '1fe3ed71536b27c9', 'authors': ['Zujin Guo', 'Size Wu', 'Zhongang Cai', 'Wei Li', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.03119.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#3d', '#cv'], 'emoji': '🕺', 'ru': {'title': '3D-управляемая интерполяция видео для реалистичной анимации человека', 'desc': 'PoseFuse3D-KI - это новая система для интерполяции ключевых кадров с человеческими движениями, использующая 3D-информацию о позе и форме тела. Она включает в себя модель PoseFuse3D, которая преобразует 3D-геометрию в 2D-пространство для управления диффузионным процессом. Авторы создали датасет CHKI-Video с 2D и 3D аннотациями для оценки качества интерполяции. Система превосходит современные аналоги, улучшая показатели PSNR на 9% и LPIPS на 38%.'}, 'en': {'title': 'Enhancing Video Frame Interpolation with 3D Human Guidance', 'desc': 'This paper presents PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a new method for generating intermediate video frames using 3D human motion guidance. Traditional methods struggle with complex human movements and lack control over the generated dynamics, but PoseFuse3D-KI incorporates 3D geometric signals to enhance the interpolation process. The framework utilizes a novel SMPL-X encoder to convert 3D shapes into a 2D latent space, allowing for better integration of spatial cues with 2D pose data. Evaluation on the new CHKI-Video dataset shows significant improvements in interpolation quality, outperforming existing methods in both PSNR and LPIPS metrics.'}, 'zh': {'title': 'PoseFuse3D：可控的人体关键帧插值新方法', 'desc': '本文提出了一种新的框架PoseFuse3D关键帧插值器（PoseFuse3D-KI），旨在通过将3D人体引导信号整合到扩散过程中，实现可控的人体中心关键帧插值（CHKI）。与现有方法相比，PoseFuse3D-KI能够更好地处理复杂的人体运动，并提供更高的合成动态控制能力。该框架使用了一种新颖的SMPL-X编码器，将3D几何形状转换为2D潜在条件空间，并通过融合网络将这些3D线索与2D姿态嵌入结合。实验结果表明，PoseFuse3D-KI在CHKI-Video数据集上显著优于现有的基线方法，PSNR提高了9%，LPIPS减少了38%。'}}}, {'id': 'https://huggingface.co/papers/2506.02678', 'title': 'TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression', 'url': 'https://huggingface.co/papers/2506.02678', 'abstract': "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", 'score': 2, 'issue_id': 4124, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '7c7528bb32eeb5a4', 'authors': ['Zhong-Zhi Li', 'Xiao Liang', 'Zihao Tang', 'Lei Ji', 'Peijie Wang', 'Haotian Xu', 'Xing W', 'Haizhen Huang', 'Weiwei Deng', 'Ying Nian Wu', 'Yeyun Gong', 'Zhijiang Guo', 'Xiao Liu', 'Fei Yin', 'Cheng-Lin Liu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology (Guangzhou)', 'Institute of Automation, Chinese Academy of Sciences', 'Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.02678.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#long_context', '#optimization', '#reasoning'], 'emoji': '⚖️', 'ru': {'title': 'Эффективное рассуждение в LLM: баланс между краткостью и точностью', 'desc': 'Эта статья представляет новый метод обучения больших языковых моделей (LLM), который улучшает эффективность рассуждений без потери точности. Авторы предлагают динамическую систему обучения, основанную на балансировке весов между данными System-1 и System-2. Этот подход позволяет сократить количество выходных токенов почти на 40%, сохраняя при этом способность модели к рассуждениям. Метод был успешно протестирован на различных моделях и наборах данных разной сложности.'}, 'en': {'title': 'Efficient Reasoning in LLMs with Dynamic Training', 'desc': "This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model's System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model's reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks."}, 'zh': {'title': '动态比例训练，提升推理效率！', 'desc': '本文提出了一种基于动态比例的训练流程，旨在提高大型语言模型（LLMs）在推理时的效率，特别是在处理极长输出时。我们的方法通过平衡模型的System-1和System-2数据的权重，消除冗余推理过程，同时保持模型的推理能力。实验结果表明，该方法在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上显著减少了近40%的输出标记，同时保持了推理的准确性。我们的代码和数据将很快公开。'}}}, {'id': 'https://huggingface.co/papers/2506.02510', 'title': 'M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset', 'url': 'https://huggingface.co/papers/2506.02510', 'abstract': "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.", 'score': 2, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '903bb57b5cc664ec', 'authors': ['Jie Zhu', 'Junhui Li', 'Yalong Wen', 'Xiandong Li', 'Lifan Guo', 'Feng Chen'], 'affiliations': ['Nanjing University', 'Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02510.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#science', '#multilingual', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями', 'desc': 'Статья представляет новый бенчмарк M³FinMeeting для оценки больших языковых моделей в понимании финансовых встреч на разных языках и в разных отраслях. Бенчмарк включает тексты на английском, китайском и японском языках, охватывая различные секторы экономики по классификации GICS. M³FinMeeting содержит три задачи: суммаризацию, извлечение пар вопрос-ответ и вопросно-ответную систему. Эксперименты с семью популярными языковыми моделями показали, что даже самые продвинутые модели имеют значительный потенциал для улучшения в этой области.'}, 'en': {'title': 'M³FinMeeting: Bridging Language Gaps in Financial Comprehension', 'desc': 'The M³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions.'}, 'zh': {'title': 'M³FinMeeting：金融会议理解的新基准', 'desc': 'M³FinMeeting是一个新的多语言、多行业和多任务的基准，旨在评估大型语言模型在理解金融会议方面的表现。该基准支持英语、中文和日语，增强了对不同语言环境中金融讨论的理解。它涵盖了全球行业分类标准（GICS）定义的多个行业，确保基准能够覆盖广泛的金融活动。通过对七种流行的大型语言模型进行实验，结果显示即使是最先进的长上下文模型在理解能力上仍有很大提升空间，证明了M³FinMeeting作为评估大型语言模型金融会议理解能力的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.02454', 'title': 'Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework', 'url': 'https://huggingface.co/papers/2506.02454', 'abstract': 'A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9220f780a7fba411', 'authors': ['Zhaorui Yang', 'Bo Pan', 'Han Wang', 'Yiyao Wang', 'Xingyu Liu', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02454.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#optimization', '#games', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'Мультимодальные отчеты: новый уровень генерации контента с помощью ИИ', 'desc': 'Новая система Multimodal DeepResearcher позволяет большим языковым моделям (LLM) создавать качественные мультимодальные отчеты, сочетающие текст и разнообразные визуализации. Система использует структурированное текстовое представление для описания визуализаций, что позволяет LLM эффективно работать с графиками. Процесс генерации разбит на четыре этапа: исследование, создание примера текста отчета, планирование и генерация мультимодального отчета. Эксперименты показали значительное превосходство Multimodal DeepResearcher над базовым методом при использовании одной и той же языковой модели.'}, 'en': {'title': 'Revolutionizing Reports: Text Meets Visualization with Multimodal DeepResearcher', 'desc': 'The paper introduces Multimodal DeepResearcher, a framework that allows Large Language Models (LLMs) to create detailed reports that combine text and various visualizations. It addresses the challenge of integrating informative visualizations with text, which has been largely overlooked in previous research. The framework uses a structured representation called Formal Description of Visualization (FDV) to help LLMs generate high-quality visual content. Through a systematic approach involving research, report creation, planning, and multimodal generation, the framework shows significant improvements in report quality compared to existing methods.'}, 'zh': {'title': '多模态深度研究者：文本与可视化的完美结合', 'desc': '本论文提出了一种新的框架，称为多模态深度研究者（Multimodal DeepResearcher），旨在使大型语言模型（LLMs）能够生成高质量的多模态报告，这些报告结合了文本和多种可视化形式。该框架通过结构化的文本表示，解决了文本与可视化内容交织生成的挑战，提升了信息传达的有效性。我们引入了可视化的正式描述（FDV），使得LLMs能够学习并生成多样化的高质量可视化图表。通过四个阶段的任务分解，该框架展示了其在生成多模态报告方面的有效性，实验结果表明其在性能上优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02295', 'title': 'QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation', 'url': 'https://huggingface.co/papers/2506.02295', 'abstract': 'Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  \t\t\t\t\tAI-generated summary \t\t\t\t The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.', 'score': 2, 'issue_id': 4119, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'b031659260f990f9', 'authors': ['Ahmed Wasfy', 'Omer Nacar', 'Abdelakreem Elkhateb', 'Mahmoud Reda', 'Omar Elshehy', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['KAND CA Corp.', 'NAMAA', 'Prince Sultan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02295.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#synthetic', '#cv', '#open_source', '#optimization'], 'emoji': '📚', 'ru': {'title': 'Прорыв в распознавании арабского текста с помощью глубокого обучения', 'desc': 'Qari-OCR представляет собой серию моделей компьютерного зрения и обработки естественного языка, оптимизированных для оптического распознавания арабского текста. Модели были итеративно дообучены на специализированных синтетических датасетах, что позволило достичь наилучших результатов среди открытых систем в задаче OCR для арабского языка. Qari-OCR успешно справляется со сложностями арабской письменности, включая диакритические знаки, разнообразие шрифтов и макетов документов. Модели демонстрируют высокую эффективность даже при работе с изображениями низкого разрешения.'}, 'en': {'title': 'Revolutionizing Arabic OCR with Qari-OCR', 'desc': 'Qari-OCR is a series of advanced vision-language models specifically designed to improve Optical Character Recognition (OCR) for Arabic text. It addresses the unique challenges of Arabic script, such as its cursive nature and diacritical marks, by using iterative fine-tuning on specialized datasets. The leading model, QARI v0.2, achieves impressive metrics with a Word Error Rate (WER) of 0.160 and a Character Error Rate (CER) of 0.061, setting a new benchmark in the field. This work not only enhances OCR accuracy for Arabic but also supports further research by making all models and datasets publicly available.'}, 'zh': {'title': 'Qari-OCR：阿拉伯语OCR的新突破', 'desc': 'Qari-OCR是一系列经过微调的视觉-语言模型，专门针对阿拉伯语光学字符识别（OCR）进行优化。该模型通过在特定的合成数据集上进行迭代微调，成功处理了阿拉伯语的连写特性、元音符号和多样的字体布局。我们的主要模型QARI v0.2在处理含有元音符号的文本时，达到了0.160的字错误率（WER）和0.061的字符错误率（CER），并在BLEU评分上取得了0.737的优异成绩。Qari-OCR在低分辨率图像的处理上也表现出色，为阿拉伯语OCR的准确性和效率带来了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01565', 'title': 'Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation', 'url': 'https://huggingface.co/papers/2506.01565', 'abstract': 'Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.', 'score': 2, 'issue_id': 4117, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '0251c50d35bd4a50', 'authors': ['Li Zhou', 'Lutong Yu', 'Dongchu Xie', 'Shaohuan Cheng', 'Wenyan Li', 'Haizhou Li'], 'affiliations': ['Chengdu Technological University', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01565.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark'], 'emoji': '👘', 'ru': {'title': 'Оценка культурного понимания и креативности ИИ через призму традиционной китайской одежды', 'desc': 'Статья представляет Hanfu-Bench - новый мультимодальный датасет для оценки понимания культурных аспектов моделями компьютерного зрения и обработки естественного языка (VLM). Датасет фокусируется на ханьфу - традиционной китайской одежде, отражающей временные аспекты культуры. Hanfu-Bench включает задачи по визуальному пониманию культуры и культурной транскреации изображений. Результаты показывают, что современные VLM отстают от экспертов-людей в понимании культурных особенностей и креативной адаптации.'}, 'en': {'title': 'Bridging Time and Culture with Hanfu-Bench', 'desc': 'This paper introduces Hanfu-Bench, a new dataset designed to enhance the understanding of cultural evolution over time using vision-language models (VLMs). It focuses on Hanfu, a traditional Chinese garment, to explore both cultural visual understanding and the transformation of traditional attire into modern designs. The study reveals that while closed VLMs perform similarly to non-experts in recognizing cultural features, they still lag behind human experts. Additionally, the transcreation task shows that even the best models struggle to achieve high success rates, highlighting the challenges in temporal cultural understanding and creative adaptation.'}, 'zh': {'title': '探索时间维度的文化理解', 'desc': '本研究提出了Hanfu-Bench，这是一个新颖的多模态数据集，旨在填补视觉语言模型在文化理解中的时间维度缺失。Hanfu作为中国传统服饰，代表了深厚的文化遗产，反映了中国文化的时间特征。数据集包含两个核心任务：文化视觉理解和文化图像再创作，前者通过多选视觉问答评估时间文化特征的识别，后者则关注传统服饰向现代设计的转化。我们的评估显示，封闭式视觉语言模型在视觉文化理解上与非专家相当，但在时间文化理解和创意适应方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2506.01004', 'title': 'Motion-Aware Concept Alignment for Consistent Video Editing', 'url': 'https://huggingface.co/papers/2506.01004', 'abstract': "MoCA-Video injects semantic features from a reference image into a video object, preserving motion and visual context, and outperforms baselines using novel metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis.", 'score': 2, 'issue_id': 4127, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '30605ac1bef5d7b2', 'authors': ['Tong Zhang', 'Juan C Leon Alcazar', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01004.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Умное смешивание видео и изображений без обучения', 'desc': 'MoCA-Video - это безтренировочный фреймворк для внедрения семантических особенностей из эталонного изображения в видеообъект. Он использует диагональное расписание шумоподавления и сегментацию для обнаружения и отслеживания объектов в латентном пространстве. Для обеспечения временной согласованности применяются семантические коррекции на основе импульса и стабилизация остаточного шума. MoCA-Video превосходит существующие базовые модели по пространственной согласованности, согласованности движения и показателю CASS.'}, 'en': {'title': 'Seamlessly Blending Images into Videos with MoCA-Video!', 'desc': 'MoCA-Video is a framework that enhances videos by integrating semantic features from a reference image while maintaining the original motion and visual context. It operates without the need for training, using techniques like diagonal denoising and class-agnostic segmentation to accurately track and blend objects in the video. To ensure smooth transitions between frames, it employs momentum-based corrections and noise stabilization. The framework is evaluated using standard metrics and a new metric called CASS, showing significant improvements in spatial consistency and motion coherence compared to existing methods.'}, 'zh': {'title': '无训练视频合成的新突破', 'desc': 'MoCA-Video（运动感知概念对齐视频）是一种无训练框架，旨在将图像领域的语义混合与视频结合。该方法通过将参考图像的语义特征注入视频中的特定对象，同时保持原有的运动和视觉上下文。我们采用对角去噪调度和类无关分割技术来检测和跟踪潜在空间中的对象，并精确控制混合对象的空间位置。通过引入基于动量的语义修正和伽马残差噪声稳定化，我们确保了视频帧之间的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.00227', 'title': 'Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes', 'url': 'https://huggingface.co/papers/2506.00227', 'abstract': 'Ctrl-Crash, a controllable car crash video generation model using classifier-free guidance, achieves top performance in video quality and realism compared to existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.', 'score': 2, 'issue_id': 4126, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '894e903b8da58314', 'authors': ['Anthony Gosselin', 'Ge Ya Luo', 'Luis Lara', 'Florian Golemo', 'Derek Nowrouzezahrai', 'Liam Paull', 'Alexia Jolicoeur-Martineau', 'Christopher Pal'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila', 'Polytechnique Montréal', 'Samsung SAIL Montréal', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2506.00227.jpg', 'data': {'categories': ['#multimodal', '#inference', '#diffusion', '#video'], 'emoji': '🚗', 'ru': {'title': 'Управляемая генерация реалистичных видео автокатастроф для повышения безопасности дорожного движения', 'desc': 'Ctrl-Crash - это модель генерации видео автомобильных аварий, использующая управляемое обучение без классификатора. Она позволяет создавать реалистичные сценарии аварий на основе входных данных, таких как ограничивающие рамки, типы столкновений и начальный кадр. Модель превосходит существующие методы на основе диффузии по качеству видео и реалистичности. Ctrl-Crash обеспечивает точный контроль над генерацией с помощью независимо настраиваемых масштабов для каждого управляющего сигнала.'}, 'en': {'title': 'Revolutionizing Crash Simulations with Ctrl-Crash', 'desc': 'Ctrl-Crash is a novel model designed to generate realistic car crash videos using advanced video diffusion techniques. It addresses the challenge of limited accident data by allowing users to control various aspects of the crash scenarios, such as bounding boxes and crash types. By employing classifier-free guidance, the model can produce diverse outcomes from slight changes in input, enhancing its utility for traffic safety simulations. The model outperforms existing methods in both quantitative metrics and human evaluations of video quality and realism.'}, 'zh': {'title': '可控汽车碰撞视频生成的突破性进展', 'desc': 'Ctrl-Crash是一种可控的汽车碰撞视频生成模型，利用无分类器引导技术，能够生成高质量和真实感的视频。由于大多数驾驶数据集中事故事件稀缺，现有的视频扩散技术在生成真实的汽车碰撞图像方面面临挑战。该模型通过边界框、碰撞类型和初始图像帧等信号进行条件控制，支持反事实场景生成，使得输入的微小变化可以导致截然不同的碰撞结果。Ctrl-Crash在视频质量指标（如FVD和JEDi）和基于人类评估的物理真实感和视频质量方面，均达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.16994', 'title': 'R^2ec: Towards Large Recommender Models with Reasoning', 'url': 'https://huggingface.co/papers/2505.16994', 'abstract': 'A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '6b76c655943245ca', 'authors': ['Runyang You', 'Yongqi Li', 'Xinyu Lin', 'Xin Zhang', 'Wenjie Wang', 'Wenjie Li', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16994.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Единая модель для рассуждений и рекомендаций', 'desc': 'Предложена унифицированная крупная рекомендательная модель с внутренними возможностями рассуждений. Модель использует обучение с подкреплением в рамках фреймворка RecPO для одновременной оптимизации способностей к рассуждению и рекомендациям. Архитектура модели переосмыслена для обеспечения чередующихся рассуждений и рекомендаций в авторегрессивном процессе. Эксперименты на трех наборах данных показали значительное улучшение показателей Hit@5 и NDCG@20 по сравнению с базовыми методами.'}, 'en': {'title': 'Unified Reasoning and Recommendation for Enhanced Performance', 'desc': "This paper introduces a new large recommender model that integrates reasoning capabilities directly into the recommendation process. The model, named RecPO, uses a reinforcement learning framework to optimize both reasoning and recommendation in a unified manner. By allowing these two processes to work together, the model reduces resource costs and improves performance compared to traditional methods that treat them separately. Experiments demonstrate significant improvements in recommendation metrics, showcasing the model's effectiveness in real-world applications."}, 'zh': {'title': '统一推荐模型，推理与推荐的完美结合', 'desc': '本文提出了一种统一的大型推荐模型，具备内在推理能力，能够在推荐过程中实现交错推理与推荐。我们重新构思了模型架构，使其在自回归过程中同时进行推理和推荐。为此，我们引入了RecPO，一个强化学习框架，能够在单次策略更新中同时优化推理和推荐能力。实验结果表明，该模型在多个数据集上相较于基线有显著提升，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03144', 'title': 'MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query', 'url': 'https://huggingface.co/papers/2506.03144', 'abstract': "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '5e57a72b2bce8a15', 'authors': ['Wei Chow', 'Yuan Gao', 'Linfeng Li', 'Xian Wang', 'Qi Xu', 'Hang Song', 'Lingdong Kong', 'Ran Zhou', 'Yi Zeng', 'Yidong Cai', 'Botian Jiang', 'Shilin Xu', 'Jiajun Zhang', 'Minghui Qiu', 'Xiangtai Li', 'Tianshu Yang', 'Siliang Tang', 'Juncheng Li'], 'affiliations': ['ByteDance Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03144.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#multilingual', '#transfer_learning', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Революция в многоязычном семантическом поиске: MERIT и Coral', 'desc': 'Статья представляет MERIT - первый многоязычный набор данных для семантического поиска с несколькими условиями. Авторы выявили ограничения существующих моделей, фокусирующихся только на глобальной семантической информации. Предложена новая структура тонкой настройки Coral, интегрирующая реконструкцию эмбеддингов и контрастное обучение. Эксперименты показывают, что Coral достигает улучшения производительности на 45.9% по сравнению с традиционными подходами на MERIT.'}, 'en': {'title': 'Revolutionizing Semantic Retrieval with MERIT and Coral', 'desc': 'This paper addresses the challenges in semantic retrieval, particularly in multilingual and multi-condition scenarios. It introduces MERIT, a new dataset with 320,000 queries across five languages and seven product categories, which highlights the limitations of current models that overlook specific query conditions. The authors propose Coral, a fine-tuning framework that enhances pre-trained multilingual language models by focusing on both global semantics and fine-grained conditional elements. Experimental results show that Coral significantly outperforms traditional methods, paving the way for improved semantic retrieval in complex environments.'}, 'zh': {'title': '开创多条件语义检索的新纪元', 'desc': '本论文探讨了语义检索在现代应用中的重要性，并指出当前研究的不足之处。现有的数据集通常只限于单一语言或单一图像，未能充分利用视觉信息的表达能力。为此，本文引入了MERIT，这是首个用于交错多条件语义检索的多语言数据集，包含320,000个查询和135,000个产品，覆盖7个不同的产品类别。我们还提出了Coral，一个新的微调框架，通过整合嵌入重建和对比学习，显著提高了模型在MERIT上的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02138', 'title': 'Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability', 'url': 'https://huggingface.co/papers/2506.02138', 'abstract': 'A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.', 'score': 1, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'c83ce7f2cc033984', 'authors': ['Yarden Bakish', 'Itamar Zimerman', 'Hila Chefer', 'Lior Wolf'], 'affiliations': ['Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02138.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение объяснимости трансформеров с учетом позиционного кодирования', 'desc': 'Статья представляет специализированный метод Layer-wise Relevance Propagation (LRP) для объяснимости моделей-трансформеров. Авторы учитывают позиционное кодирование, что улучшает распространение релевантности и превосходит существующие методы. Новый подход переформулирует входное пространство как набор пар позиция-токен и предлагает теоретически обоснованные правила LRP для различных методов позиционного кодирования. Эксперименты показывают, что метод значительно превосходит современные подходы в задачах объяснимости для компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Enhancing Transformer Explainability with Positional Encoding in LRP', 'desc': 'This paper introduces a specialized Layer-wise Relevance Propagation (LRP) method tailored for Transformer models, focusing on the importance of positional encoding in explainability. Traditional LRP methods fail to account for positional encoding, which leads to a loss of critical relevance information tied to the structure and position of tokens. By reformulating the input space into position-token pairs, the authors develop new LRP rules that effectively propagate relevance across different types of positional encodings. The proposed method shows significant improvements over existing techniques in explainability tasks for both vision and natural language processing, validated through extensive experiments.'}, 'zh': {'title': '提升Transformer可解释性的专门LRP方法', 'desc': '本文提出了一种针对Transformer模型的专门化层次相关传播（LRP）方法，考虑了位置编码的影响，从而改善了相关性传播。现有的LRP方法未能充分利用Transformer架构中的位置编码，导致了重要相关性的丢失。我们通过将输入空间重新构建为位置-标记对，提出了理论基础的LRP规则，以适应不同的位置信息编码方法。实验结果表明，我们的方法在视觉和自然语言处理的可解释性任务中显著优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2506.01265', 'title': 'Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines', 'url': 'https://huggingface.co/papers/2506.01265', 'abstract': 'In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.', 'score': 1, 'issue_id': 4124, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '2f44ffeb11509c5c', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Do Xuan Trong', 'Luu Anh Tuan', 'Kenji Kawaguchi', 'Shafiq Joty', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University, Singapore', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.01265.jpg', 'data': {'categories': ['#data', '#training', '#long_context', '#optimization', '#multimodal'], 'emoji': '📝', 'ru': {'title': 'LongGuide: улучшение генерации длинных текстов с помощью автоматических рекомендаций', 'desc': 'Статья исследует возможности обучения по контексту (ICL) для крупных языковых моделей (LLM) в задачах генерации длинных текстов. Авторы показывают, что одних только демонстраций ICL недостаточно для обучения LLM распределениям языка и формата задачи. Они предлагают метод LongGuide, который генерирует два потока рекомендаций для улучшения производительности модели. LongGuide автоматически выбирает лучшую комбинацию рекомендаций, повышая эффективность как открытых, так и закрытых LLM.'}, 'en': {'title': 'Enhancing Long-Form Generation with LongGuide', 'desc': 'This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios.'}, 'zh': {'title': '提升生成任务性能的LongGuide方法', 'desc': '本文探讨了预训练大型语言模型（LLMs）在上下文学习（ICL）中的能力，尤其是在长文本生成任务中的表现。研究表明，仅依靠示例（demonstrations）不足以让模型掌握生成任务的语言和格式分布。为此，提出了LongGuide，通过生成两条并行的指导流，帮助模型更好地理解任务要求。实验结果显示，LongGuide能显著提升模型在零样本和少样本设置下的表现，且具有良好的通用性和学习能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24362', 'title': 'Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion', 'url': 'https://huggingface.co/papers/2505.24362', 'abstract': "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bbcc31124760dad', 'authors': ['Anum Afzal', 'Florian Matthes', 'Gal Chechik', 'Yftah Ziser'], 'affiliations': ['Bar-Ilan University', 'Nvidia Research', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.24362.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Раннее предсказание успеха рассуждений в языковых моделях', 'desc': 'Исследователи изучают возможность предсказания успеха процесса рассуждений с нулевым выстрелом (zero-shot Chain-of-Thought) до его завершения. Они обнаружили, что классификатор на основе представлений языковой модели (LLM) показывает хорошие результаты даже до генерации первого токена. Это указывает на то, что ключевая информация о процессе рассуждений уже присутствует в начальных представлениях. Исследование также показало, что раннее прерывание цепочки рассуждений может улучшить производительность по сравнению с отсутствием CoT, хотя и не достигает уровня полного рассуждения.'}, 'en': {'title': 'Unlocking Early Insights in Chain-of-Thought Reasoning', 'desc': 'This paper explores the predictability of success in zero-shot Chain-of-Thought (CoT) reasoning processes before they are fully completed. The authors find that a probing classifier using representations from large language models (LLMs) can effectively predict outcomes even before generating any tokens, indicating that essential reasoning information is present early on. In contrast, a BERT-based model that relies on generated tokens performs poorly, as it focuses on superficial linguistic features rather than deeper reasoning. The study suggests that early stopping in CoT reasoning can still yield better performance than not using CoT at all, and proposes that future methods could utilize their classifier to determine when early stopping is beneficial.'}, 'zh': {'title': '优化推理效率，早期停止也能有效', 'desc': '本研究探讨了零-shot Chain-of-Thought (CoT) 过程的成功是否可以在完成之前进行预测。我们发现，基于大语言模型（LLM）表示的探测分类器在生成第一个标记之前就表现良好，这表明推理过程中的关键信息在初始步骤的表示中已经存在。相比之下，依赖生成标记的强BERT基线表现较差，可能是因为它依赖于浅层语言线索而非更深层的推理动态。我们的实验表明，早期停止推理可以提高性能，尽管与完整推理相比仍有差距，这为优化CoT的效率提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2506.01939', 'title': 'Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.01939', 'abstract': "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.", 'score': 93, 'issue_id': 4090, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'caf288ab8a8d11b2', 'authors': ['Shenzhi Wang', 'Le Yu', 'Chang Gao', 'Chujie Zheng', 'Shixuan Liu', 'Rui Lu', 'Kai Dang', 'Xionghui Chen', 'Jianxin Yang', 'Zhenru Zhang', 'Yuqiong Liu', 'An Yang', 'Andrew Zhao', 'Yang Yue', 'Shiji Song', 'Bowen Yu', 'Gao Huang', 'Junyang Lin'], 'affiliations': ['LeapLab, Tsinghua University', 'Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.01939.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Высокоэнтропийные токены - ключ к улучшению рассуждений ИИ', 'desc': 'Исследование показывает, что паттерны энтропии токенов играют ключевую роль в обучении с подкреплением с проверяемыми вознаграждениями (RLVR) для улучшения рассуждений больших языковых моделей. Обнаружено, что небольшая часть токенов с высокой энтропией значительно влияет на производительность рассуждений. RLVR в основном корректирует энтропию высокоэнтропийных токенов, сохраняя общие паттерны базовой модели. Оптимизация только 20% токенов с высокой энтропией позволяет достичь результатов, сравнимых с полным градиентным обновлением, особенно для больших моделей.'}, 'en': {'title': 'Unlocking Reasoning Power with High-Entropy Tokens in RLVR', 'desc': "This paper investigates the role of token entropy patterns in Reinforcement Learning with Verifiable Rewards (RLVR) to enhance the reasoning abilities of Large Language Models (LLMs). It finds that high-entropy tokens, which are rare, significantly influence the model's reasoning pathways and performance. The study shows that by focusing on these high-entropy tokens during training, the model can achieve better results than traditional methods, even when using a smaller subset of tokens. Overall, the research emphasizes the importance of understanding and optimizing high-entropy tokens to improve RLVR outcomes."}, 'zh': {'title': '高熵令牌：提升RLVR推理能力的关键', 'desc': '本研究探讨了可验证奖励的强化学习（RLVR）在大语言模型（LLMs）推理能力提升中的作用，重点分析了令牌熵模式对推理性能的影响。我们发现，只有少量高熵令牌在推理过程中起到关键作用，能够引导模型走向多样化的推理路径。通过对RLVR训练中熵模式的演变进行研究，我们发现RLVR主要遵循基础模型的熵模式，主要调整高熵令牌的熵值。我们的结果表明，优化高熵令牌是提升RLVR效果的关键，利用这些令牌可以显著提高模型的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'score': 44, 'issue_id': 4088, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'c7587646c2140ddd', 'authors': ['Zafir Stojanovski', 'Oliver Stanley', 'Joe Sharratt', 'Richard Jones', 'Abdulhakeem Adefioye', 'Jean Kaddour', 'Andreas Köpf'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24760.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#games', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Бесконечная тренировка ИИ в искусстве рассуждений', 'desc': 'Reasoning Gym (RG) - это библиотека сред для обучения с подкреплением в задачах рассуждения с проверяемыми наградами. Она включает более 100 генераторов данных и верификаторов в различных областях, таких как алгебра, арифметика, вычисления, познание, геометрия, теория графов, логика и игры. Ключевое преимущество RG - возможность генерировать практически бесконечные обучающие данные с настраиваемой сложностью, в отличие от большинства существующих наборов данных фиксированного размера. Экспериментальные результаты подтверждают эффективность RG как для оценки, так и для обучения с подкреплением моделей рассуждения.'}, 'en': {'title': 'Unlock Infinite Reasoning with Reasoning Gym!', 'desc': 'The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings.'}, 'zh': {'title': '推理训练场：无限生成，持续评估', 'desc': '我们介绍了推理训练场（Reasoning Gym，RG），这是一个用于强化学习的推理环境库，具有可验证的奖励。它提供了超过100个数据生成器和验证器，涵盖代数、算术、计算、认知、几何、图论、逻辑和各种常见游戏等多个领域。其关键创新在于能够生成几乎无限的训练数据，并且可以调整复杂性，这与大多数固定的推理数据集不同。我们的实验结果表明，RG在评估和强化学习推理模型方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.01844', 'title': 'SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics', 'url': 'https://huggingface.co/papers/2506.01844', 'abstract': 'SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.', 'score': 42, 'issue_id': 4094, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '64cdbe1cd5ffbfc4', 'authors': ['Mustafa Shukor', 'Dana Aubakirova', 'Francesco Capuano', 'Pepijn Kooijmans', 'Steven Palma', 'Adil Zouitine', 'Michel Aractingi', 'Caroline Pascal', 'Martino Russi', 'Andres Marafioti', 'Simon Alibert', 'Matthieu Cord', 'Thomas Wolf', 'Remi Cadene'], 'affiliations': ['Hugging Face', 'Sorbonne University', 'valeo.ai', 'École Normale Supérieure Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.01844.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#small_models', '#training', '#benchmark', '#dataset', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Маленькая модель - большие возможности', 'desc': 'SmolVLA - это компактная и эффективная модель для обработки зрения, языка и действий в робототехнике. Она достигает конкурентоспособной производительности при сниженных вычислительных затратах и может быть развернута на обычном пользовательском оборудовании. SmolVLA обучается на одном GPU и может работать даже на CPU, что значительно снижает стоимость обучения и вывода. Несмотря на свой небольшой размер, SmolVLA показывает результаты, сравнимые с моделями, которые в 10 раз больше.'}, 'en': {'title': 'SmolVLA: Efficient Vision-Language-Action for Everyone', 'desc': 'SmolVLA is a compact vision-language-action model that efficiently integrates visual and linguistic understanding for robotics. It significantly reduces the computational costs associated with training and inference, making it suitable for consumer-grade hardware. By leveraging community-collected data, SmolVLA maintains competitive performance while being much smaller than traditional models. The introduction of an asynchronous inference stack enhances responsiveness, allowing for faster action execution without sacrificing accuracy.'}, 'zh': {'title': 'SmolVLA：小巧高效的视觉-语言-动作模型', 'desc': 'SmolVLA是一种紧凑高效的视觉-语言-动作模型，能够在降低计算成本的同时实现竞争力的性能，并可在消费级硬件上部署。该模型利用社区收集的数据，避免了传统模型对大型数据集的依赖，从而降低了训练和推理的成本。SmolVLA设计为可以在单个GPU上训练，并在消费级GPU或CPU上运行，提升了响应速度。尽管体积小，SmolVLA的性能与体积十倍的模型相当，适用于多种机器人基准测试。'}}}, {'id': 'https://huggingface.co/papers/2506.01049', 'title': 'Taming LLMs by Scaling Learning Rates with Gradient Grouping', 'url': 'https://huggingface.co/papers/2506.01049', 'abstract': 'SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.', 'score': 31, 'issue_id': 4087, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '350401d748400bad', 'authors': ['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu'], 'affiliations': ['Peking University', 'The Hong Kong University of Science and Technology', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01049.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'SGG: Групповое масштабирование градиентов для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый метод оптимизации для обучения больших языковых моделей под названием SGG (Scaling with Gradient Grouping). SGG группирует градиенты и применяет масштабирование для каждой группы, что улучшает адаптивную оценку скорости обучения. Этот подход повышает стабильность и скорость сходимости при обучении крупных языковых моделей. Эксперименты показывают, что SGG хорошо интегрируется с существующими оптимизаторами и дает стабильные улучшения на различных бенчмарках.'}, 'en': {'title': 'SGG: Optimizing Learning Rates for Better LLM Training', 'desc': 'This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions.'}, 'zh': {'title': 'SGG：提升大语言模型训练的自适应学习率优化器', 'desc': 'SGG是一种优化器包装器，通过对梯度进行分组和应用特定于集群的缩放，增强了大语言模型的自适应学习率。它解决了大规模模型训练中的不稳定性和收敛速度慢的问题。SGG通过动态分组和集群特定的缩放来改善学习率估计，从而实现更精确的参数调整。实验结果表明，SGG与现有优化器无缝集成，并在不同模型规模上提供了一致的性能提升和更快的收敛速度。'}}}, {'id': 'https://huggingface.co/papers/2506.00539', 'title': 'ARIA: Training Language Agents with Intention-Driven Reward Aggregation', 'url': 'https://huggingface.co/papers/2506.00539', 'abstract': 'ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.', 'score': 25, 'issue_id': 4090, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '49b915ea5a1db300', 'authors': ['Ruihan Yang', 'Yikai Zhang', 'Aili Chen', 'Xintao Wang', 'Siyu Yuan', 'Jiangjie Chen', 'Deqing Yang', 'Yanghua Xiao'], 'affiliations': ['Bytedance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00539.jpg', 'data': {'categories': ['#games', '#reasoning', '#agents', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'ARIA: Агрегация наград в пространстве намерений для эффективного обучения языковых ИИ-агентов', 'desc': 'ARIA - это метод, который агрегирует награды в пространстве намерений для эффективного обучения языковых агентов. Он проецирует действия на естественном языке из высокоразмерного пространства распределения токенов в низкоразмерное пространство намерений, где семантически похожие действия кластеризуются и получают общие награды. Это уменьшает дисперсию наград, уплотняя сигналы наград и способствуя лучшей оптимизации политики. Эксперименты показывают, что ARIA значительно снижает дисперсию градиента политики и дает существенный прирост производительности в среднем на 9,95% в четырех задачах.'}, 'en': {'title': 'Enhancing Language Agents with Intention-Based Reward Aggregation', 'desc': 'ARIA is a method designed to improve reinforcement learning in language-based tasks by addressing the issue of reward sparsity. It does this by aggregating rewards in an intention space, which clusters semantically similar actions and assigns them shared rewards. This approach reduces the variance of rewards, making it easier for agents to learn effective policies. Experiments show that ARIA leads to significant performance improvements across various tasks compared to traditional reinforcement learning methods.'}, 'zh': {'title': '意图空间中的奖励聚合，提升语言代理的学习效率', 'desc': 'ARIA是一种在意图空间中聚合奖励的方法，旨在缓解奖励稀疏性并改善基于语言的强化学习任务中的策略优化。通过将自然语言动作从高维的联合标记分布空间投影到低维的意图空间，ARIA能够将语义相似的动作聚集在一起并分配共享奖励。这种基于意图的奖励聚合减少了奖励方差，增强了奖励信号的密度，从而促进了更好的策略优化。实验结果表明，ARIA显著降低了策略梯度的方差，并在四个下游任务中平均提高了9.95%的性能，始终优于离线和在线强化学习基线。'}}}, {'id': 'https://huggingface.co/papers/2505.23590', 'title': 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles', 'url': 'https://huggingface.co/papers/2505.23590', 'abstract': 'The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.', 'score': 24, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '583be7c36c626f1e', 'authors': ['Zifu Wang', 'Junyi Zhu', 'Bo Tang', 'Zhiyu Li', 'Feiyu Xiong', 'Jiaqian Yu', 'Matthew B. Blaschko'], 'affiliations': ['ESAT-PSI, KU Leuven', 'Institute for Advanced Algorithms Research, Shanghai', 'Memory Tensor, Shanghai', 'Samsung R&D Institute China, Beijing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23590.jpg', 'data': {'categories': ['#multimodal', '#training', '#transfer_learning', '#rl', '#cv', '#open_source', '#reasoning', '#games'], 'emoji': '🧩', 'ru': {'title': 'Мультимодальные модели осваивают пазлы с помощью обучения с подкреплением', 'desc': 'Исследование применения обучения с подкреплением на основе правил к мультимодальным большим языковым моделям (MLLM) с использованием головоломок-пазлов в качестве экспериментальной базы. Результаты показывают, что MLLM могут достигать высокой точности и обобщения на сложных конфигурациях пазлов после дообучения. Обнаружено, что обучение на пазлах может улучшить результаты на других визуальных задачах, а обучение с подкреплением демонстрирует лучшее обобщение, чем обычная тонкая настройка. Исследование вносит вклад в понимание обучения с подкреплением на основе правил для визуальных задач в мультимодальном обучении.'}, 'en': {'title': 'Unlocking Multimodal Learning with Jigsaw Puzzles and RL', 'desc': 'This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks.'}, 'zh': {'title': '基于规则的强化学习在多模态学习中的新发现', 'desc': '本研究探讨了基于规则的强化学习在多模态大型语言模型中的应用，特别是在视觉任务中的挑战。我们使用拼图作为实验框架，发现经过微调后，模型在简单拼图上的表现从随机猜测提升至接近完美的准确率，并能推广到复杂的未见配置。研究还表明，模型可以在有无显式推理的情况下学习和泛化，尽管开源模型更倾向于直接回答。最后，我们发现强化学习在泛化能力上优于监督微调，并且初始的监督微调冷启动阶段可能会妨碍后续的强化学习优化。'}}}, {'id': 'https://huggingface.co/papers/2506.01853', 'title': 'ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01853', 'abstract': 'A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '241ff6937e6642f5', 'authors': ['Junliang Ye', 'Zhengyi Wang', 'Ruowen Zhao', 'Shenghao Xie', 'Jun Zhu'], 'affiliations': ['Peking University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01853.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games', '#dataset', '#agi', '#multimodal'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D: языковая модель, понимающая трехмерное пространство', 'desc': 'Исследователи представили ShapeLLM-Omni - нативную 3D большую языковую модель, способную понимать и генерировать 3D-объекты и текст. Модель обучена с использованием 3D векторного квантованного вариационного автоэнкодера (VQVAE) и нового набора данных 3D-Alpaca. ShapeLLM-Omni расширяет возможности мультимодальных моделей, добавляя базовые 3D-возможности. Это открывает новые перспективы для исследований в области 3D-нативного искусственного интеллекта.'}, 'en': {'title': 'Bridging Text and 3D: ShapeLLM-Omni Unleashes Multimodal Potential', 'desc': 'ShapeLLM-Omni is a novel large language model designed to understand and generate 3D assets alongside text. It utilizes a 3D vector-quantized variational autoencoder (VQVAE) to efficiently represent and reconstruct 3D shapes in a discrete latent space. The model is trained on a new dataset called 3D-Alpaca, which includes diverse tasks such as generation, comprehension, and editing of 3D content. This research aims to enhance multimodal AI capabilities by integrating 3D understanding, paving the way for future advancements in 3D-native artificial intelligence.'}, 'zh': {'title': 'ShapeLLM-Omni：开启3D内容生成的新纪元', 'desc': '本文提出了一种名为ShapeLLM-Omni的原生3D大型语言模型，旨在理解和生成3D资产及文本。该模型使用3D向量量化变分自编码器（VQVAE）进行训练，将3D对象映射到离散潜在空间，以实现高效准确的形状表示和重建。我们还构建了一个名为3D-Alpaca的大规模连续训练数据集，涵盖生成、理解和编辑，为未来的研究和训练提供了丰富的资源。通过对Qwen-2.5-vl-7B-Instruct模型进行基于指令的训练，我们的工作为扩展多模态模型的基本3D能力提供了有效的尝试。'}}}, {'id': 'https://huggingface.co/papers/2506.00996', 'title': 'Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.00996', 'abstract': "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/", 'score': 22, 'issue_id': 4090, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '25ea795d193b8719', 'authors': ['Kinam Kim', 'Junha Hyung', 'Jaegul Choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00996.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#training'], 'emoji': '🎬', 'ru': {'title': 'Эффективная адаптация видеомоделей с минимальными данными', 'desc': 'Метод Temporal In-Context Fine-Tuning (TIC-FT) улучшает предобученные модели диффузии видео для разнообразных задач условной генерации с минимальными данными и без изменения архитектуры. TIC-FT объединяет кадры условия и целевые кадры по временной оси, вставляя промежуточные буферные кадры с постепенно увеличивающимся уровнем шума. Этот подход не требует архитектурных изменений и достигает высокой производительности всего на 10-30 обучающих примерах. Эксперименты показывают, что TIC-FT превосходит существующие базовые методы по точности соответствия условиям и визуальному качеству, оставаясь при этом высокоэффективным как при обучении, так и при инференсе.'}, 'en': {'title': 'Efficient Video Generation with Minimal Data Using TIC-FT', 'desc': "Temporal In-Context Fine-Tuning (TIC-FT) is a novel method that improves pretrained video diffusion models for various conditional generation tasks without needing extensive data or changing the model architecture. It works by combining condition and target frames along the time axis and adding intermediate buffer frames with increasing noise, which helps maintain the model's temporal coherence. This approach allows for effective fine-tuning with as few as 10-30 training samples, making it efficient and scalable. TIC-FT has been shown to outperform existing methods in generating high-quality videos while ensuring fidelity to the given conditions."}, 'zh': {'title': '时间上下文微调：高效的视频生成新方法', 'desc': '本文提出了一种新的方法，称为时间上下文微调（TIC-FT），用于增强预训练的视频扩散模型，以实现多样化的条件生成任务。TIC-FT通过在时间轴上连接条件帧和目标帧，并插入逐渐增加噪声水平的中间缓冲帧，来实现平滑过渡，从而与预训练模型的时间动态对齐。该方法无需对模型架构进行修改，且只需10到30个训练样本即可实现强大的性能。实验结果表明，TIC-FT在条件保真度和视觉质量方面均优于现有基线，同时在训练和推理过程中保持高效。'}}}, {'id': 'https://huggingface.co/papers/2506.00411', 'title': 'LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks', 'url': 'https://huggingface.co/papers/2506.00411', 'abstract': 'A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '16fa44208bf3618c', 'authors': ['Yi Yang', 'Jiaxuan Sun', 'Siqi Kou', 'Yihan Wang', 'Zhijie Deng'], 'affiliations': ['Fudan University', 'Shanghai Jiao Tong University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00411.jpg', 'data': {'categories': ['#cv', '#architecture', '#robotics', '#agents', '#dataset', '#agi', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Единая архитектура для долгосрочных задач воплощенного ИИ', 'desc': 'LoHoVLA - это новая унифицированная архитектура для решения долгосрочных задач воплощенного искусственного интеллекта. Она объединяет большую предобученную мультимодальную модель для работы с изображениями и текстом с иерархическим управлением с обратной связью. LoHoVLA использует общее представление для генерации подзадач на естественном языке и предсказания действий робота. Эксперименты показали значительное превосходство LoHoVLA над существующими подходами в симуляторе Ravens.'}, 'en': {'title': 'Empowering Robots with Unified Vision and Language for Complex Tasks', 'desc': 'The paper presents LoHoVLA, a new framework that integrates a large pretrained vision language model with hierarchical closed-loop control to enhance performance in long-horizon embodied tasks. It addresses the challenges of high-level task planning and low-level motion control by generating language and action tokens for effective sub-task generation and robot action prediction. The framework is trained on a unique dataset, LoHoSet, which includes a variety of long-horizon tasks with expert demonstrations. Experimental results demonstrate that LoHoVLA outperforms existing models, highlighting the potential of unified architectures in improving embodied intelligence.'}, 'zh': {'title': '统一框架提升长时间任务表现', 'desc': 'LoHoVLA是一个统一的视觉语言行动框架，旨在提高长时间任务的表现。它结合了大型预训练的视觉语言模型和分层闭环控制，能够更好地进行高层次任务规划和低层次运动控制。通过生成语言和动作标记，LoHoVLA促进了任务间的更好泛化。此外，LoHoVLA在Ravens模拟器上进行训练，展示了其在长时间任务中的显著优势。'}}}, {'id': 'https://huggingface.co/papers/2506.01943', 'title': 'Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control', 'url': 'https://huggingface.co/papers/2506.01943', 'abstract': 'Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.', 'score': 20, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '4acb1e4fc9635b8a', 'authors': ['Xiao Fu', 'Xintao Wang', 'Xian Liu', 'Jianhong Bai', 'Runsen Xu', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01943.jpg', 'data': {'categories': ['#robotics', '#video', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'RoboMaster: новый подход к моделированию сложных взаимодействий в робототехнике', 'desc': 'В статье представлен новый подход RoboMaster для моделирования взаимодействия нескольких объектов в робототехнике. Метод разделяет процесс взаимодействия на три этапа: до, во время и после, используя характеристики доминирующего объекта на каждом этапе. Это позволяет избежать проблем, связанных со слиянием признаков нескольких объектов во время взаимодействия. Для обеспечения семантической согласованности на протяжении всего видео используются латентные представления, учитывающие внешний вид и форму объектов.'}, 'en': {'title': 'RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling', 'desc': "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."}, 'zh': {'title': 'RoboMaster：提升机器人操作的视频生成新框架', 'desc': '本论文提出了一种名为RoboMaster的新框架，用于建模多物体之间的动态交互，以改善机器人决策数据的生成。与以往方法不同，RoboMaster将交互过程分为三个子阶段：预交互、交互和后交互，分别使用主导物体的特征进行建模。通过这种方式，RoboMaster有效地解决了多物体特征融合带来的问题，提高了视觉质量。实验结果表明，该方法在复杂的机器人操作任务中表现优异，达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2506.01713', 'title': 'SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.01713', 'abstract': 'Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.', 'score': 20, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'fc955e3f149c1b08', 'authors': ['Zhongwei Wan', 'Zhihao Dou', 'Che Liu', 'Yu Zhang', 'Dongfei Cui', 'Qinjian Zhao', 'Hui Shen', 'Jing Xiong', 'Yi Xin', 'Yifan Jiang', 'Yangfan He', 'Mi Zhang', 'Shen Yan'], 'affiliations': ['ByteDance Seed', 'Case Western Reserve University', 'Duke University', 'Imperial College London', 'Kean University Minnesota', 'Nanjing University', 'The Ohio State University', 'The University of Hong Kong', 'Tongji University', 'University of Michigan', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.01713.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#benchmark', '#dataset', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'SRPO: Усиление мультимодальных ИИ через самоанализ', 'desc': 'Статья представляет новый метод под названием SRPO для улучшения рассуждений мультимодальных больших языковых моделей (MLLM). SRPO использует двухэтапный подход с обучением с подкреплением, включающий создание набора данных с рефлексией и новый механизм вознаграждения. Метод нацелен на преодоление ограничений существующих MLLM в сложных задачах, требующих самоанализа и самокоррекции. Эксперименты показывают значительное улучшение точности рассуждений и качества рефлексии на нескольких мультимодальных тестах.'}, 'en': {'title': 'Enhancing Multimodal Reasoning through Self-Reflection', 'desc': "This paper introduces a new approach called Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO) to improve the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing reflection methods are inadequate for generating useful feedback, which limits the models' performance on complex reasoning tasks. SRPO consists of two stages: first, it creates a high-quality dataset for training the model to reflect on its responses, and second, it implements a reward mechanism that promotes meaningful reflections. Experimental results show that SRPO significantly enhances both the accuracy of reasoning and the quality of reflections compared to current leading models."}, 'zh': {'title': '提升多模态推理能力的自我反思框架', 'desc': '多模态大型语言模型（MLLMs）在推理任务中表现出色，但在需要明确自我反思和自我纠正的复杂问题上仍然存在困难。现有的反思方法过于简单，难以生成有意义和指导性的反馈，因为预训练模型的推理能力和知识在初始训练期间基本固定。为了解决这些挑战，我们提出了一种名为自我反思增强推理的多模态自我反思框架（SRPO），该框架通过两阶段的反思意识强化学习（RL）来提升多模态LLM的推理能力。实验结果表明，SRPO在多个多模态推理基准测试中显著优于现有模型，推理准确性和反思质量都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01667', 'title': 'EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models', 'url': 'https://huggingface.co/papers/2506.01667', 'abstract': 'EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework.', 'score': 17, 'issue_id': 4093, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '90528034771977ef', 'authors': ['Yan Shu', 'Bin Ren', 'Zhitong Xiong', 'Danda Pani Paudel', 'Luc Van Gool', 'Begum Demir', 'Nicu Sebe', 'Paolo Rota'], 'affiliations': ['INSAIT, Sofia University St. Kliment Ohridski', 'Technical University of Munich', 'Technische Universität Berlin', 'University of Pisa', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2506.01667.jpg', 'data': {'categories': ['#benchmark', '#cv', '#survey', '#reasoning', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'EarthMind: Эффективное понимание многосенсорных данных наблюдения Земли', 'desc': 'EarthMind - это новая система анализа данных дистанционного зондирования Земли, использующая методы обработки естественного языка и компьютерного зрения. Она включает в себя пространственное внимание и кросс-модальное слияние для эффективной работы с разнородными данными. EarthMind превосходит более крупные модели на специализированных тестах, несмотря на меньший размер (4 миллиарда параметров). Система способна решать широкий спектр задач восприятия и рассуждения для многосенсорных данных наблюдения Земли.'}, 'en': {'title': 'EarthMind: Revolutionizing Earth Observation with Vision-Language Fusion', 'desc': "EarthMind is a new framework designed to improve the understanding of Earth Observation (EO) data by combining vision and language processing. It uses Spatial Attention Prompting (SAP) to focus on important details at the pixel level, enhancing the model's ability to interpret images. Additionally, Cross-modal Fusion aligns different types of data, allowing the model to weigh information based on its relevance. This approach not only outperforms larger models like GPT-4o on specialized benchmarks but also effectively handles complex multi-sensor data tasks."}, 'zh': {'title': 'EarthMind：高效理解地球观测数据的创新框架', 'desc': 'EarthMind是一个视觉-语言框架，旨在高效理解多粒度和多传感器的地球观测数据。它采用空间注意力提示和跨模态融合技术，能够在专门基准测试中超越更大的模型。EarthMind的两个核心组件分别是空间注意力提示（SAP），用于增强像素级理解，以及跨模态融合，能够将不同模态对齐到共享空间并根据信息密度自适应调整权重。通过EarthMind-Bench基准测试，EarthMind在多个公共地球观测基准上表现优异，展示了其在统一框架下处理多粒度和多传感器挑战的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.24298', 'title': 'AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning', 'url': 'https://huggingface.co/papers/2505.24298', 'abstract': 'AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57times training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.', 'score': 16, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'fad566ec1d2ba264', 'authors': ['Wei Fu', 'Jiaxuan Gao', 'Xujie Shen', 'Chen Zhu', 'Zhiyu Mei', 'Chuyi He', 'Shusheng Xu', 'Guo Wei', 'Jun Mei', 'Jiashu Wang', 'Tongkai Yang', 'Binhang Yuan', 'Yi Wu'], 'affiliations': ['Ant Research', 'HKUST', 'IIIS, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24298.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'AReaL: Асинхронное обучение с подкреплением для ускорения ИИ', 'desc': 'AReaL - это асинхронная система обучения с подкреплением для больших языковых моделей. Она разделяет процессы генерации и обучения, что позволяет достичь более высокой утилизации GPU и ускорения обучения до 2.57 раз на задачах рассуждения. Система использует непрерывную генерацию данных и обновление модели, а также ряд оптимизаций для стабилизации обучения. Эксперименты показали превосходство AReaL над синхронными системами по скорости при сохранении или улучшении качества.'}, 'en': {'title': 'AReaL: Revolutionizing Reinforcement Learning with Asynchronous Training', 'desc': 'AReaL is an innovative reinforcement learning system designed to enhance the training of large language models by decoupling the generation and training processes. This fully asynchronous approach allows for continuous output generation without waiting for the longest tasks to finish, leading to improved GPU utilization. By balancing the workload between rollout and training workers, AReaL effectively manages data staleness and employs a modified Proximal Policy Optimization (PPO) to optimize training with outdated samples. Experimental results demonstrate that AReaL can achieve up to 2.57 times faster training speeds while maintaining or improving performance on reasoning tasks.'}, 'zh': {'title': 'AReaL：异步强化学习的高效训练新模式', 'desc': 'AReaL是一种完全异步的强化学习系统，它将生成和训练解耦，从而提高GPU的利用率，并在推理任务上实现了高达2.57倍的训练加速。传统的大规模强化学习系统通常是同步的，生成和训练交替进行，这导致了系统效率低下。AReaL通过让生成工作者持续生成新输出，而训练工作者在收集到一批数据后立即更新模型，解决了这一问题。通过平衡生成和训练工作者的工作负载，AReaL有效控制了数据的过时性，并采用了增强过时性的PPO变体来更好地处理过时的训练样本。'}}}, {'id': 'https://huggingface.co/papers/2506.01863', 'title': 'Unified Scaling Laws for Compressed Representations', 'url': 'https://huggingface.co/papers/2506.01863', 'abstract': 'A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation\'s ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.', 'score': 14, 'issue_id': 4099, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '0f97dac5f5c8309f', 'authors': ['Andrei Panferov', 'Alexandra Volkova', 'Ionut-Vlad Modoranu', 'Vage Egiazarian', 'Mher Safaryan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.01863.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '🔬', 'ru': {'title': 'Единая метрика для предсказания эффективности сжатых нейросетей', 'desc': 'Исследование посвящено взаимосвязи между законами масштабирования и методами сжатия в машинном обучении. Авторы предлагают единую метрику емкости, способную предсказывать производительность моделей для различных форматов сжатия, включая скалярное квантование, разреженное квантование и векторное квантование. Эта метрика основана на способности представления аппроксимировать случайные гауссовы данные. Результаты исследования позволяют сравнивать потенциальную точность различных форматов сжатия и разрабатывать улучшенные алгоритмы обучения для разреженно-квантованных форматов.'}, 'en': {'title': 'Unified Capacity Metric: Predicting Performance in Compressed Models', 'desc': 'This paper explores how scaling laws in machine learning can predict the performance of models when they are compressed using different techniques. It focuses on various compression formats like scalar-quantized, sparse-quantized, and vector-quantized representations. The authors propose a unified capacity metric that can effectively gauge the efficiency of these compressed models based on their ability to handle random Gaussian data. Their findings suggest that this metric not only applies to individual compression types but can also be used to compare and improve training algorithms across different formats.'}, 'zh': {'title': '统一容量度量，预测模型性能', 'desc': '本研究探讨了缩放法则与压缩技术之间的关系，提出了一种统一的容量度量，可以预测不同压缩格式下模型的性能。研究表明，模型的性能可以根据模型大小、计算量和数据量进行可预测的缩放。通过验证通用的缩放法则公式，研究发现该公式适用于不同的压缩类型。最终，提出了一种简单的“容量”度量，能够有效预测多种压缩表示下的参数效率。'}}}, {'id': 'https://huggingface.co/papers/2505.24846', 'title': 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning', 'url': 'https://huggingface.co/papers/2505.24846', 'abstract': 'MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.', 'score': 14, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ed5c25a307e9093d', 'authors': ['Jingyan Shen', 'Jiarui Yao', 'Rui Yang', 'Yifan Sun', 'Feng Luo', 'Rui Pan', 'Tong Zhang', 'Han Zhao'], 'affiliations': ['Columbia University', 'Rice University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24846.jpg', 'data': {'categories': ['#dataset', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'MiCRo: Персонализация языковых моделей без дополнительных аннотаций', 'desc': 'MiCRo - это двухэтапная система для улучшения персонализированного обучения предпочтениям в больших языковых моделях. Она использует наборы данных бинарных предпочтений и динамически адаптирует веса смесей на основе контекста. MiCRo эффективно захватывает разнообразные человеческие предпочтения без необходимости в детальных аннотациях. Эксперименты показывают, что MiCRo значительно улучшает последующую персонализацию в языковых моделях.'}, 'en': {'title': 'MiCRo: Dynamic Personalization for Diverse Human Preferences', 'desc': 'MiCRo is a two-stage framework designed to enhance personalized preference learning for large language models (LLMs). It utilizes binary preference datasets and employs a context-aware mixture modeling approach to better capture the diverse preferences of humans. The framework dynamically adjusts mixture weights based on the context, allowing for more accurate and scalable preference adaptation. Experimental results show that MiCRo significantly improves the ability of LLMs to personalize responses according to varied human preferences.'}, 'zh': {'title': 'MiCRo：捕捉多样化人类偏好的新框架', 'desc': 'MiCRo是一个两阶段框架，旨在改善大型语言模型的个性化偏好学习。它利用二元偏好数据集，并根据上下文动态调整混合权重，从而有效捕捉多样化的人类偏好。该方法通过引入上下文感知的混合建模，解决了传统模型无法充分反映人类多样性的问题。实验结果表明，MiCRo在多个偏好数据集上表现出色，显著提升了下游个性化效果。'}}}, {'id': 'https://huggingface.co/papers/2506.01413', 'title': 'Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.01413', 'abstract': 'Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.', 'score': 11, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '3f0db6c1e3cc1878', 'authors': ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent YouTu Lab', 'The Chinese University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01413.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Рассуждай умнее, а не больше: новый подход к обучению языковых моделей', 'desc': 'Эта статья посвящена улучшению способности больших языковых моделей (LLM) выполнять сложные инструкции с множественными ограничениями. Авторы предлагают систематический метод, основанный на поощрении рассуждений с помощью обучения с подкреплением. Они используют воспроизводимый метод сбора данных и применяют контрастное обучение для улучшения цепочки рассуждений. Результаты показывают значительное улучшение производительности, сравнимое с увеличением размера модели в несколько раз.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Reasoning to Deep Understanding', 'desc': "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."}, 'zh': {'title': '提升大型语言模型处理复杂指令的能力', 'desc': '现有的大型语言模型（LLMs）在处理复杂指令时面临挑战，尤其是当指令包含多个并行、链式和分支结构的约束时。本文提出了一种系统的方法，通过激励推理来提升LLMs处理复杂指令的能力。我们利用强化学习（RL）和可验证的规则中心奖励信号，培养模型在指令跟随方面的推理能力。通过对比样本，我们解决了在复杂指令下推理的浅层和非本质特性，从而显著提高了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.00577', 'title': 'Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs', 'url': 'https://huggingface.co/papers/2506.00577', 'abstract': 'Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0ed5d0b7064f9962', 'authors': ['Yufa Zhou', 'Shaobo Wang', 'Xingyu Dong', 'Xiangqi Jin', 'Yifang Chen', 'Yue Min', 'Kexin Yang', 'Xingzhang Ren', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, Shanghai Jiao Tong University', 'Qwen Team, Alibaba Group', 'The University of Chicago', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.00577.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#agents', '#open_source', '#reasoning', '#games', '#dataset'], 'emoji': '💡', 'ru': {'title': 'Дообучение языковых моделей улучшает экономические рассуждения', 'desc': 'Исследование показывает, что методы дообучения, такие как контролируемая тонкая настройка (SFT) и обучение с подкреплением с проверяемыми наградами (RLVR), могут улучшить рассуждения и экономическую рациональность больших языковых моделей в многоагентных сценариях. Авторы представляют Recon - 7B-параметровую модель, дообученную на наборе данных из 2100 задач по экономическим рассуждениям. Оценка на экономических тестах и многоагентных играх показала значительное улучшение структурированных рассуждений и экономической рациональности. Результаты подчеркивают перспективность дообучения в конкретной предметной области для улучшения рассуждений и согласования агентов.'}, 'en': {'title': 'Enhancing Economic Reasoning in LLMs through Post-Training Techniques', 'desc': "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."}, 'zh': {'title': '后训练技术提升智能体推理与经济理性', 'desc': '本论文探讨了后训练技术如何提升大型语言模型在多智能体系统中的推理能力和经济理性。我们采用监督微调和可验证奖励的强化学习方法，针对经济推理进行训练。通过引入Recon模型，我们在高质量经济推理问题的数据集上进行了后训练，并在经济推理基准和多智能体游戏中进行了评估。结果显示，经过后训练的模型在结构化推理和经济理性方面有显著提升，证明了领域对齐的后训练在增强推理和智能体对齐方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.23907', 'title': 'Cora: Correspondence-aware image editing using few step diffusion', 'url': 'https://huggingface.co/papers/2505.23907', 'abstract': 'Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '7de1457440a0b449', 'authors': ['Amirhossein Almohammadi', 'Aryan Mikaeili', 'Sauradip Nag', 'Negar Hassanpour', 'Andrea Tagliasacchi', 'Ali Mahdavi-Amiri'], 'affiliations': ['Google Deepmind, Canada', 'Huawei, Canada', 'Simon Fraser University, Canada', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.23907.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Умное редактирование изображений с сохранением структуры и текстур', 'desc': 'Cora - это новая система редактирования изображений, использующая коррекцию шума с учетом соответствий и интерполированные карты внимания. Она позволяет точно переносить текстуры и структуры между исходным и целевым изображениями, сохраняя ключевые атрибуты оригинала. Cora превосходит аналоги в сохранении структуры, текстур и идентичности при различных типах редактирования. Система обеспечивает контроль баланса между генерацией нового контента и сохранением исходного.'}, 'en': {'title': 'Cora: Revolutionizing Image Editing with Precision and Control', 'desc': 'The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods.'}, 'zh': {'title': 'Cora：图像编辑的新突破', 'desc': 'Cora框架通过引入对应感知噪声校正和插值注意力图，增强了图像编辑的效果。它能够在源图像和目标图像之间对齐纹理和结构，从而实现准确的纹理转移和必要的新内容生成。Cora在内容生成和保留之间提供了良好的控制，能够有效处理姿态变化、物体添加和纹理细化等多种编辑任务。实验结果表明，Cora在结构、纹理和身份的保持上表现优异，用户研究也证实了其优于其他方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.01952', 'title': 'WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks', 'url': 'https://huggingface.co/papers/2506.01952', 'abstract': 'WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.', 'score': 9, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'aa260fbf373a4f2c', 'authors': ['Atsuyuki Miyai', 'Zaiying Zhao', 'Kazuki Egashira', 'Atsuki Sato', 'Tatsumi Sunada', 'Shota Onohara', 'Hiromasa Yamanishi', 'Mashiro Toyooka', 'Kunato Nishina', 'Ryoma Maeda', 'Kiyoharu Aizawa', 'Toshihiko Yamasaki'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.01952.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#benchmark', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'WebChoreArena: новый рубеж в оценке возможностей ИИ-агентов для веб-задач', 'desc': 'WebChoreArena - это новый набор тестов, состоящий из 532 задач, который расширяет возможности WebArena для более сложных и утомительных задач веб-браузинга. Он включает в себя три ключевых вызова: задачи с массивной памятью, задачи с вычислениями и задачи с долговременной памятью. Эксперименты показывают, что по мере эволюции больших языковых моделей (LLM), таких как GPT-4, Claude 3.7 Sonnet и Gemini 2.5 Pro, наблюдается значительное улучшение производительности на WebChoreArena. Однако результаты также указывают на то, что даже с Gemini 2.5 Pro остается значительное пространство для улучшения по сравнению с WebArena.'}, 'en': {'title': 'WebChoreArena: Elevating LLMs to Tackle Tedious Web Tasks', 'desc': 'WebChoreArena is a new benchmark that includes 532 tasks designed to evaluate the capabilities of large language models (LLMs) in handling complex web browsing chores. It extends the previous WebArena benchmark by focusing on more tedious tasks that require advanced skills such as massive memory retrieval, precise calculations, and long-term memory management across multiple web pages. The benchmark allows for reproducible experiments and fair comparisons with existing models, showcasing the progress of LLMs like GPT-4o and Gemini 2.5 Pro. Despite improvements in performance, the results indicate that there is still significant room for enhancement in tackling the challenges presented by WebChoreArena compared to general browsing tasks.'}, 'zh': {'title': 'WebChoreArena：评估LLM在复杂任务中的能力', 'desc': 'WebChoreArena是一个新的基准测试，包含532个任务，旨在评估大型语言模型（LLM）在复杂和繁琐的网页浏览任务中的能力。该基准测试扩展了WebArena的范围，专注于人类通常避免的繁重任务。WebChoreArena整合了三大挑战：大规模记忆任务、计算任务和长期记忆任务，确保了严格的可重复性。实验结果表明，随着LLM的进步，性能显著提升，但仍有改进空间，显示出WebChoreArena的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'url': 'https://huggingface.co/papers/2505.23977', 'abstract': 'VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'fef2cab0e56bc9bd', 'authors': ['Yichen Feng', 'Zhangchen Xu', 'Fengqing Jiang', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23977.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Синтетические данные для улучшения логического мышления ИИ', 'desc': 'VisualSphinx - это крупномасштабный синтетический набор данных для улучшения мультимодального рассуждения в визуально-языковых моделях. Он создан с помощью специального конвейера синтеза изображений на основе правил. Эксперименты показывают, что обучение на VisualSphinx улучшает способности моделей к логическому рассуждению. Усовершенствованные навыки рассуждения, полученные на VisualSphinx, также полезны для других задач, таких как алгебраические, арифметические и геометрические рассуждения.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualSphinx', 'desc': 'VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry.'}, 'zh': {'title': 'VisualSphinx：提升视觉语言模型的逻辑推理能力', 'desc': 'VisualSphinx是一个大规模的合成数据集，旨在提升视觉语言模型在多模态推理方面的表现。该数据集专注于逻辑推理任务，解决了当前模型缺乏结构化训练数据的问题。通过规则到图像的合成流程，VisualSphinx能够生成与问题相关的图像，增强模型的逻辑一致性和可读性。实验表明，使用VisualSphinx训练的视觉语言模型在逻辑推理、代数推理、算术推理和几何推理等任务上表现更佳。'}}}, {'id': 'https://huggingface.co/papers/2505.23059', 'title': 'From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval', 'url': 'https://huggingface.co/papers/2505.23059', 'abstract': 'State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.', 'score': 8, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '83af42c01de2e64c', 'authors': ['Dohyeon Lee', 'Yeonseok Jeong', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23059.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SMR: Эффективные рассуждения для языковых моделей', 'desc': 'Статья представляет новый метод рассуждений для больших языковых моделей под названием State Machine Reasoning (SMR). SMR улучшает производительность информационного поиска и снижает использование токенов, решая проблему избыточных рассуждений. Метод использует дискретные действия (уточнение, переранжирование, остановка) для более точного контроля процесса рассуждений. Эксперименты показали, что SMR повышает качество поиска на 3.4% при снижении использования токенов на 74.4%.'}, 'en': {'title': 'Streamlining Retrieval with State Machine Reasoning', 'desc': 'State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments.'}, 'zh': {'title': '状态机推理：提升检索效率，减少资源消耗', 'desc': '状态机推理（SMR）通过离散动作框架来改善信息检索性能，并减少大型语言模型的令牌使用，解决了过度思考的问题。该方法识别了信息检索中的两个主要挑战：冗余轨迹和误导性推理。SMR采用基于转移的推理框架，包含精细控制的离散动作（如精炼、重新排序和停止），支持早期停止。实验结果表明，SMR在BEIR和BRIGHT基准上提高了3.4%的检索性能，同时减少了74.4%的令牌使用。'}}}, {'id': 'https://huggingface.co/papers/2505.23001', 'title': 'DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors', 'url': 'https://huggingface.co/papers/2505.23001', 'abstract': 'DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'cd584a75fce48ae2', 'authors': ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2505.23001.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'DyePack: Ловушка для нечестных моделей машинного обучения', 'desc': 'DyePack - это фреймворк, использующий атаки типа backdoor для выявления моделей, которые использовали тестовые наборы данных во время обучения. Он вводит безвредные образцы backdoor в тестовые данные, чтобы пометить модели, обучавшиеся на них. DyePack обеспечивает точный расчет уровня ложноположительных результатов и предотвращает ложные обвинения. Фреймворк был успешно протестирован на пяти моделях и трех наборах данных, охватывающих задачи с множественным выбором и открытой генерацией.'}, 'en': {'title': 'DyePack: Safeguarding Model Integrity with Backdoor Detection', 'desc': 'DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates.'}, 'zh': {'title': 'DyePack：精准识别训练中使用基准测试集的模型', 'desc': 'DyePack是一个利用后门攻击的框架，用于识别在训练中使用基准测试集的模型。它通过引入良性后门样本，确保准确的假阳性率，同时防止错误指控。DyePack的设计结合了多个具有随机目标的后门，使得在标记每个模型时能够精确计算假阳性率。通过在多个模型和数据集上的评估，DyePack成功检测到所有受污染的模型，且假阳性率极低。'}}}, {'id': 'https://huggingface.co/papers/2505.24523', 'title': 'Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors', 'url': 'https://huggingface.co/papers/2505.24523', 'abstract': "Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.", 'score': 7, 'issue_id': 4096, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4c6278bf22171f39', 'authors': ['Andrea Pedrotti', 'Michele Papucci', 'Cristiano Ciaccio', 'Alessio Miaschi', 'Giovanni Puccetti', "Felice Dell'Orletta", 'Andrea Esuli'], 'affiliations': ['Department of Computer Science, University of Pisa', 'Istituto di Scienza Tecnologie dellInformazione A. Faedo (CNR-ISTI)', 'ItaliaNLP Lab, Istituto di Linguistica Computazionale Antonio Zampolli (CNR-ILC)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24523.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#security', '#rlhf', '#alignment'], 'emoji': '🕵️', 'ru': {'title': 'Обман детекторов: как состязательные атаки подрывают обнаружение ИИ-текста', 'desc': 'Статья описывает метод создания состязательных атак на детекторы машинно-сгенерированного текста (MGT) с использованием тонкой настройки языковых моделей методом Direct Preference Optimization. Авторы демонстрируют, что такие атаки могут значительно снизить эффективность существующих детекторов MGT, делая сгенерированный текст более похожим на человеческий. Исследование также анализирует лингвистические изменения, вызванные этой настройкой, и особенности, используемые детекторами для обнаружения MGT. Результаты подчеркивают необходимость улучшения методов обнаружения и повышения их устойчивости к новым вариантам генерируемого текста.'}, 'en': {'title': 'Fooling the Detectives: Enhancing MGT Stealth with DPO', 'desc': 'This paper discusses how adversarial attacks can be used to improve the stealth of machine-generated text (MGT) by fine-tuning language models through Direct Preference Optimization (DPO). The authors demonstrate that these attacks can significantly reduce the effectiveness of current MGT detectors by altering the style of generated text to resemble human-written content. They also analyze the linguistic features that detectors rely on, revealing vulnerabilities in their detection capabilities. The findings emphasize the need for more robust detection methods to handle the evolving challenges posed by advanced generative AI.'}, 'zh': {'title': '提升检测器鲁棒性，抵御对抗性攻击', 'desc': '本研究探讨了对抗性攻击如何利用直接偏好优化（DPO）来微调语言模型，从而使其生成的文本更难被机器生成文本（MGT）检测器识别。我们发现，现有的MGT检测器在面对经过优化的文本时，性能显著下降，容易被欺骗。通过分析语言模型的风格转变，我们揭示了检测器依赖的语言特征。研究结果强调了改进检测方法的重要性，以增强其对未知文本的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.01881', 'title': 'WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue', 'url': 'https://huggingface.co/papers/2506.01881', 'abstract': 'STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'e82ff37de8341d1a', 'authors': ['Yaoyao Qian', 'Jindan Huang', 'Yuanli Wang', 'Simon Yu', 'Kyrie Zhixuan Zhou', 'Jiayuan Mao', 'Mingfu Liang', 'Hanhan Zhou'], 'affiliations': ['Boston University, Boston, MA', 'George Washington University, Washington, DC', 'Massachusetts Institute of Technology, Cambridge, MA', 'Northeastern University, Boston, MA', 'Northwestern University, Evanston, IL', 'Tufts University, Medford, MA', 'University of Texas at San Antonio, San Antonio, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.01881.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#alignment', '#agents'], 'emoji': '🌪️', 'ru': {'title': 'STORM: асимметричное моделирование намерений в диалоговых системах', 'desc': 'Статья представляет фреймворк STORM для моделирования асимметричной динамики информации в диалоговых системах. STORM использует две языковые модели - UserLLM и AgentLLM - для имитации процесса формирования намерений пользователя. Фреймворк позволяет создавать аннотированные корпуса, отражающие эволюцию понимания в ходе диалога. Эксперименты показали, что умеренная неопределенность может превосходить полную прозрачность в некоторых сценариях взаимодействия человека и ИИ.'}, 'en': {'title': 'Enhancing Dialogue Systems through Collaborative Intent Formation', 'desc': 'The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions.'}, 'zh': {'title': 'STORM：促进人机协作的意图形成', 'desc': 'STORM框架通过建模用户和代理之间的信息不对称动态，促进了任务导向对话系统中的协作意图形成。用户的表达虽然在语言上完整，但往往缺乏系统所需的结构信息，导致系统无法正确响应。STORM框架能够捕捉表达轨迹和潜在的认知转变，从而系统化分析协作理解的发展。实验结果表明，在某些情况下，适度的不确定性（40-60%）可以优于完全透明的信息，这为人机协作中的信息完整性提供了新的思考。'}}}, {'id': 'https://huggingface.co/papers/2506.00338', 'title': 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning', 'url': 'https://huggingface.co/papers/2506.00338', 'abstract': 'The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2f4783eb2db68192', 'authors': ['Yifan Peng', 'Shakeel Muhammad', 'Yui Sudo', 'William Chen', 'Jinchuan Tian', 'Chyi-Jiunn Lin', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University, United States', 'Honda Research Institute Japan, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.00338.jpg', 'data': {'categories': ['#training', '#low_resource', '#open_source', '#multilingual', '#data', '#audio', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Открытые речевые модели достигают уровня промышленных стандартов', 'desc': 'Проект OWSM улучшен с помощью масштабного очищенного веб-датасета, что привело к усовершенствованию мультиязычных речевых моделей. Разработан масштабируемый конвейер для очистки данных, результатом которого стал датасет с 166 000 часами речи на 75 языках. Новая серия моделей OWSM v4, обученная на этом курированном датасете, значительно превосходит предыдущие версии по мультиязычным бенчмаркам. Модели даже соответствуют или превосходят передовые промышленные модели, такие как Whisper и MMS, в нескольких сценариях.'}, 'en': {'title': 'Enhancing Multilingual Speech Models with Cleaned Web Data', 'desc': 'The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition.'}, 'zh': {'title': '提升多语言语音模型的开创性进展', 'desc': 'OWSM项目通过整合一个大型清洗过的网络数据集YODAS，提升了多语言语音模型的性能。YODAS数据集包含了大量的语音数据，但由于其原始特性，存在语言标签错误和音频文本不对齐等问题。为了解决这些问题，我们开发了一个可扩展的数据清洗流程，最终生成了一个包含75种语言、166,000小时语音的数据集。新的OWSM v4模型在多语言基准测试中表现优异，甚至在多个场景中与领先的工业模型相媲美。'}}}, {'id': 'https://huggingface.co/papers/2505.24842', 'title': 'Cascading Adversarial Bias from Injection to Distillation in Language\n  Models', 'url': 'https://huggingface.co/papers/2505.24842', 'abstract': 'Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.', 'score': 6, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '15a12805380711b7', 'authors': ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea'], 'affiliations': ['Google DeepMind', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24842.jpg', 'data': {'categories': ['#security', '#ethics', '#training', '#inference', '#data', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Скрытая угроза: как предвзятость усиливается при дистилляции языковых моделей', 'desc': 'Статья исследует уязвимость дистиллированных языковых моделей к внедрению предвзятого контента во время обучения. Авторы демонстрируют, что даже минимальное отравление данных учителя может привести к значительному усилению предвзятости в модели ученика. Предложены два режима распространения предвзятости: нецеленаправленный, влияющий на множество задач, и целенаправленный, фокусирующийся на конкретных задачах. Результаты показывают недостатки существующих методов защиты и подчеркивают необходимость разработки специализированных мер безопасности для дистиллированных моделей.'}, 'en': {'title': 'Strengthening Distilled Models Against Adversarial Bias Injection', 'desc': 'This paper explores the vulnerabilities of distilled language models to adversarial attacks, specifically through the injection of biased content during their training phase. It shows that adversaries can subtly poison teacher models with minimal data, which then amplifies biases in the student models that are derived from them. The study identifies two modes of bias propagation: Untargeted, affecting multiple tasks, and Targeted, which focuses on specific tasks while keeping normal behavior intact. The findings reveal that current defenses are inadequate, emphasizing the need for improved strategies to safeguard against these security threats in distilled models.'}, 'zh': {'title': '保护蒸馏模型，抵御对抗性偏见攻击！', 'desc': '模型蒸馏在创建小型可部署语言模型中变得至关重要，这些模型保留了更大系统的能力。然而，广泛部署引发了对抗性操控的脆弱性问题。本文研究了蒸馏模型在训练过程中对偏见内容的对抗性注入的脆弱性。我们提出了两种传播模式，并展示了如何通过最小的数据污染使教师模型注入微妙的偏见，这些偏见在学生模型中被显著放大。'}}}, {'id': 'https://huggingface.co/papers/2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors', 'url': 'https://huggingface.co/papers/2505.24625', 'abstract': "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'score': 6, 'issue_id': 4087, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bfa132788ee6990', 'authors': ['Duo Zheng', 'Shijia Huang', 'Yanyang Li', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.24625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#games', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в 3D-понимании: извлечение геометрии напрямую из видео', 'desc': 'Исследователи представили новую модель Video-3D Geometry Large Language Model (VG LLM), которая извлекает трехмерную информацию непосредственно из видеопоследовательностей для улучшения понимания 3D-сцен. В отличие от предыдущих подходов, VG LLM не требует дополнительных 3D-данных, таких как облака точек или реконструированные карты с видом сверху. Модель использует энкодер 3D-визуальной геометрии для извлечения априорной 3D-информации из видео, которая затем интегрируется с визуальными токенами и подается в мультимодальную языковую модель. Эксперименты показали, что VG LLM достигает конкурентоспособных результатов в различных задачах 3D-понимания сцен и пространственного рассуждения, превосходя некоторые современные методы.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding from Video Alone!', 'desc': 'The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data.'}, 'zh': {'title': '视频驱动的3D理解新突破', 'desc': '本文提出了一种新颖的视频-3D几何大语言模型（VG LLM），能够直接从视频序列中提取3D信息，从而增强3D场景理解，而无需额外的3D数据。该模型利用3D视觉几何编码器，从视频中提取3D先验信息，并将其与视觉标记结合，输入到多模态大语言模型中。通过大量实验，结果表明该方法在3D场景理解和空间推理等任务上取得了显著的改进。值得注意的是，我们的4B模型在不依赖显式3D数据输入的情况下，达到了与现有最先进方法相媲美的结果，甚至在VSI-Bench评估中超越了Gemini-1.5-Pro。'}}}, {'id': 'https://huggingface.co/papers/2505.24183', 'title': 'CodeV-R1: Reasoning-Enhanced Verilog Generation', 'url': 'https://huggingface.co/papers/2505.24183', 'abstract': 'CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.', 'score': 6, 'issue_id': 4093, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'b542a58b96860ad6', 'authors': ['Yaoyu Zhu', 'Di Huang', 'Hanqi Lyu', 'Xiaoyun Zhang', 'Chongxiao Li', 'Wenxuan Shi', 'Yutong Wu', 'Jianan Mu', 'Jinghua Wang', 'Yang Zhao', 'Pengwei Jin', 'Shuyao Cheng', 'Shengwen Liang', 'Xishan Zhang', 'Rui Zhang', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yunji Chen'], 'affiliations': ['Cambricon Technologies', 'SKL of Processors, Institute of Computing Technology, CAS', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.24183.jpg', 'data': {'categories': ['#games', '#rl', '#dataset', '#optimization', '#open_source', '#training'], 'emoji': '🔧', 'ru': {'title': 'CodeV-R1: Прорыв в автоматизации проектирования электроники', 'desc': 'В статье представлена новая система CodeV-R1 для генерации кода на языке Verilog с использованием LLM и метода RLVR. Основные проблемы, которые решает система, включают отсутствие автоматизированных сред верификации, нехватку качественных пар "естественный язык - код" и высокие вычислительные затраты. CodeV-R1 использует генератор тестов на основе правил и метод синтеза данных для создания высококачественного набора данных. Модель CodeV-R1-7B демонстрирует значительное улучшение производительности по сравнению с предыдущими методами, что способствует развитию исследований в области автоматизации проектирования электроники.'}, 'en': {'title': 'Revolutionizing Verilog Generation with CodeV-R1', 'desc': 'The paper presents CodeV-R1, a reinforcement learning with verifiable reward (RLVR) framework designed for generating Verilog code from natural language specifications. It addresses challenges in electronic design automation (EDA) by introducing a rule-based testbench for equivalence checking and a round-trip data synthesis method to create a high-quality dataset of NL-code pairs. The training process utilizes a two-stage approach, combining knowledge distillation with an adaptive RLVR algorithm to optimize training efficiency. CodeV-R1 demonstrates significant improvements in performance metrics, surpassing previous state-of-the-art models in Verilog generation tasks.'}, 'zh': {'title': 'CodeV-R1：电子设计自动化的强化学习新突破', 'desc': '本文介绍了CodeV-R1，这是一个用于Verilog生成的强化学习可验证奖励（RLVR）框架，旨在解决电子设计自动化（EDA）中的关键挑战。该框架通过开发基于规则的测试平台生成器和回合数据合成方法，确保生成的代码与自然语言描述之间的一致性。我们还采用了两阶段的训练流程，首先进行知识蒸馏以提升推理能力，然后使用自适应的RLVR算法降低训练成本。最终，CodeV-R1-7B模型在VerilogEval v2和RTLLM v1.1上取得了显著的性能提升，超越了之前的最佳结果。'}}}, {'id': 'https://huggingface.co/papers/2505.21179', 'title': 'Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model', 'url': 'https://huggingface.co/papers/2505.21179', 'abstract': 'Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a universal plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!', 'score': 6, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3e7694e3e9f014f5', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2505.21179.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv', '#optimization', '#video'], 'emoji': '🧠', 'ru': {'title': 'NAG: универсальное негативное руководство для диффузионных моделей', 'desc': 'Статья представляет новый метод под названием Normalized Attention Guidance (NAG) для улучшения работы диффузионных моделей. NAG позволяет эффективно применять негативное руководство в различных режимах и модальностях без необходимости переобучения модели. В отличие от существующих подходов, NAG обобщается на разные архитектуры, режимы сэмплирования и модальности, функционируя как универсальный плагин с минимальными вычислительными затратами. Эксперименты показывают улучшения в соответствии текста и изображения, качестве генерации и восприятии человеком.'}, 'en': {'title': 'Effortless Negative Guidance for Diffusion Models with NAG', 'desc': 'Normalized Attention Guidance (NAG) is a novel method that improves diffusion models by providing effective negative guidance without the need for retraining. It addresses the challenge of suppressing unwanted attributes, especially in scenarios with few sampling steps where traditional methods like Classifier-Free Guidance (CFG) struggle. NAG utilizes an efficient mechanism that normalizes attention using L1-based techniques, allowing it to maintain high fidelity while enhancing negative guidance. This approach is versatile, working across different architectures, sampling regimes, and modalities, making it a universal solution for modern diffusion frameworks.'}, 'zh': {'title': '归一化注意力引导：无缝负引导的解决方案', 'desc': '归一化注意力引导（NAG）通过在不同的采样阶段和模态中提供有效的负引导，增强了扩散模型，而无需重新训练。负引导的挑战在于在少步采样中显得尤为突出，传统的无分类器引导（CFG）在激进的采样步骤压缩下表现不佳。NAG采用基于L1的归一化和精炼方法，在注意力空间中进行外推，恢复了有效的负引导。通过广泛的实验，我们证明了NAG在文本对齐、保真度和人类感知质量方面的一致性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01084', 'title': 'zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression', 'url': 'https://huggingface.co/papers/2506.01084', 'abstract': 'A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers\' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency.', 'score': 5, 'issue_id': 4093, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'f9927f51990f811a', 'authors': ['Saibo Geng', 'Nathan Ranchin', 'Yunzhen yao', 'Maxime Peyrard', 'Chris Wendler', 'Michael Gastpar', 'Robert West'], 'affiliations': ['EPFL', 'Microsoft', 'Northeastern University', 'Université Grenoble Alpes, CNRS, Grenoble INP, LIG'], 'pdf_title_img': 'assets/pdf/title_img/2506.01084.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'Ускорение LLM с помощью динамического сжатия токенов', 'desc': 'В статье представлена новая система zip2zip, которая улучшает работу LLM, динамически изменяя словарь токенов во время вывода. Это достигается с помощью сжатия LZW, что позволяет сократить длину последовательности токенов и ускорить процесс вывода. Система включает в себя токенизатор, основанный на LZW, слой для вычисления эмбеддингов новых токенов и модифицированную модель языкового моделирования. Результаты показывают, что zip2zip может сократить длину последовательностей на 20-60% и значительно уменьшить задержки при выводе.'}, 'en': {'title': 'Dynamic Tokenization for Faster Inference in LLMs', 'desc': "The paper presents zip2zip, a novel framework that enhances the efficiency of large language models (LLMs) by dynamically adjusting their token vocabulary during inference. By utilizing Lempel-Ziv-Welch (LZW) compression, zip2zip reduces the length of token sequences, which leads to faster inference speeds. The framework includes a tokenizer that creates reusable 'hypertokens', an embedding layer for these hypertokens, and a causal language model that operates on compressed sequences. The results demonstrate that zip2zip can significantly decrease input and output lengths by 20-60%, improving overall model performance and reducing computational costs."}, 'zh': {'title': '动态调整令牌，提升推理速度', 'desc': 'zip2zip是一个框架，它在推理时动态调整大型语言模型（LLMs）的令牌词汇，使用LZW压缩技术来减少令牌序列的长度，从而提高推理速度。传统的令牌化方法通常依赖于静态的令牌器，这些令牌器的固定词汇无法适应特定领域或语言的输入，导致生成更长的令牌序列和更高的计算成本。zip2zip通过三个关键组件实现其功能：基于LZW压缩的令牌器、实时计算新形成的超令牌的嵌入层，以及训练模型处理压缩序列的因果语言建模变体。实验表明，经过zip2zip处理的LLM在推理时能够有效使用超令牌，输入和输出序列长度减少20-60%，推理延迟显著降低。'}}}, {'id': 'https://huggingface.co/papers/2506.00512', 'title': 'Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing', 'url': 'https://huggingface.co/papers/2506.00512', 'abstract': 'A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.', 'score': 5, 'issue_id': 4094, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0a9ce5d9ebc76a52', 'authors': ['Yang Zheng', 'Mengqi Huang', 'Nan Chen', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00512.jpg', 'data': {'categories': ['#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Прогрессивное 3D-редактирование: от текста к согласованным изменениям', 'desc': 'Статья представляет новый подход к редактированию 3D-объектов с помощью текстовых инструкций. Авторы предлагают парадигму прогрессивных видов, которая обеспечивает согласованное редактирование 3D-объектов путем распространения семантики от ключевых видов к менее отредактированным. Метод Pro3D-Editor включает в себя выборку основного вида, рендеринг ключевых видов и уточнение полного вида. Эксперименты показывают, что этот метод превосходит существующие подходы по точности редактирования и пространственной согласованности.'}, 'en': {'title': 'Achieving Consistent 3D Editing with Pro3D-Editor', 'desc': 'This paper introduces a new approach for 3D editing called Pro3D-Editor, which focuses on maintaining consistency across different views of a 3D object. Unlike traditional methods that treat all views equally, this framework uses a progressive-views paradigm to propagate editing information from the most important view to others. It consists of three main components: a Primary-view Sampler that identifies and edits the most relevant view, a Key-view Render that transfers the editing semantics to other views, and a Full-view Refiner that finalizes the 3D object based on the edited views. The results show that Pro3D-Editor achieves better accuracy and consistency compared to existing 3D editing techniques.'}, 'zh': {'title': '渐进视图范式实现一致的3D编辑', 'desc': '本文提出了一种渐进视图范式，通过Pro3D-Editor实现一致的3D编辑。该方法通过从关键视图向较少编辑的视图传播语义，解决了现有方法在多视图编辑中存在的不一致性问题。Pro3D-Editor框架包括主要视图采样器、关键视图渲染和全视图精炼器，能够动态选择最重要的视图进行编辑。实验结果表明，该方法在编辑精度和空间一致性方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2505.24452', 'title': 'Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training', 'url': 'https://huggingface.co/papers/2505.24452', 'abstract': 'A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of varphi.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.', 'score': 5, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '82972c2646341cc9', 'authors': ['Anda Tang', 'Yiming Dong', 'Yutao Zeng', 'zhou Xun', 'Zhouchen Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Artificial Intelligence, Peking University', 'Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China', 'State Key Lab of General AI, School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24452.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '📊', 'ru': {'title': 'Умный график обучения: максимум эффективности при ограниченных ресурсах', 'desc': 'Предложен унифицированный график скорости обучения с учетом бюджета для оптимизации обучения в условиях ограниченного количества итераций. Новый подход, названный Unified Budget-Aware (UBA), основан на теоретической базе и учитывает устойчивость к вариациям кривизны ландшафта оптимизации. UBA превосходит традиционные графики для различных задач и архитектур нейронных сетей. Метод контролируется одним гиперпараметром φ, который обеспечивает баланс между гибкостью и простотой.'}, 'en': {'title': 'Optimizing Training with Unified Budget-Aware Learning Rates', 'desc': 'This paper introduces a new learning rate schedule called the Unified Budget-Aware (UBA) schedule, designed to optimize training when there are limits on the number of iterations. Traditional learning rate schedules often rely on trial-and-error and lack a solid theoretical basis, making them inefficient. The UBA schedule is grounded in a novel optimization framework that considers the curvature of the loss landscape, allowing it to adapt better to various tasks and network architectures. Experimental results demonstrate that UBA outperforms standard schedules across different vision and language tasks, providing a more effective training strategy within constrained budgets.'}, 'zh': {'title': '统一预算感知学习率调度，优化有限训练预算', 'desc': '本文提出了一种统一的预算感知学习率调度（UBA），旨在优化在有限迭代预算下的训练效果。传统的学习率调度方法往往依赖经验，缺乏理论基础，而UBA则通过构建一个新的优化框架，考虑了对损失函数曲率变化的鲁棒性。该调度由一个超参数控制，能够在灵活性和简单性之间取得平衡，避免了对每个网络进行数值优化的需求。实验结果表明，UBA在多种视觉和语言任务中，均优于常用的学习率调度方法。'}}}, {'id': 'https://huggingface.co/papers/2505.23504', 'title': 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.23504', 'abstract': 'VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.', 'score': 5, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c243189c9ec32d1f', 'authors': ['Liyun Zhu', 'Qixiang Chen', 'Xi Shen', 'Xiaodong Cun'], 'affiliations': ['Australian National University', 'GVC Lab, Great Bay University', 'Intellindust AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.23504.jpg', 'data': {'categories': ['#rl', '#interpretability', '#multimodal', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Умное видеонаблюдение: ИИ учится понимать аномалии', 'desc': 'VAU-R1 - это новая система для понимания аномалий в видео, использующая мультимодальные большие языковые модели (MLLM) и усиленное обучение. Авторы также представили VAU-Bench - первый бенчмарк для оценки рассуждений об аномалиях в видео, основанный на методе цепочки мыслей. Система VAU-R1 значительно улучшает точность ответов на вопросы, временную привязку и согласованность рассуждений в различных контекстах. Это исследование закладывает основу для интерпретируемого и основанного на рассуждениях понимания аномалий в видео.'}, 'en': {'title': 'Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench', 'desc': 'The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts.'}, 'zh': {'title': '提升视频异常推理的智能框架', 'desc': 'VAU-R1 是一个基于多模态大语言模型的框架，旨在提升视频异常推理能力。通过强化微调（Reinforcement Fine-Tuning），该方法能够更好地理解和解释异常事件。我们还提出了 VAU-Bench，这是一个专门用于视频异常推理的链式思维基准，包含多项选择问答、详细推理、时间标注和描述性标题。实验结果表明，VAU-R1 在问答准确性、时间定位和推理连贯性方面有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01484', 'title': 'LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification', 'url': 'https://huggingface.co/papers/2506.01484', 'abstract': 'A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  \t\t\t\t\tAI-generated summary \t\t\t\t Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '422c267bbe9577db', 'authors': ['Shuzhou Yuan', 'Ercong Nie', 'Lukas Kouba', 'Ashish Yashwanth Kangen', 'Helmut Schmid', 'Hinrich Schutze', 'Michael Farber'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning (MCML)', 'ScaDS.AI and TU Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.01484.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#dataset', '#data', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'ИИ очищает интернет от языка ненависти', 'desc': 'Статья представляет новый подход к детоксификации языка ненависти с использованием GPT-4o-mini. Авторы создали крупномасштабный датасет PARADEHATE, содержащий более 8000 пар токсичных и нетоксичных текстов. Эксперименты показали, что модели, обученные на этом датасете, демонстрируют улучшенные результаты по точности стиля, сохранению содержания и плавности текста. Этот метод предлагается как масштабируемая альтернатива ручной аннотации для создания данных по детоксификации.'}, 'en': {'title': 'Automating Hate Speech Detoxification with GPT-4o-mini', 'desc': "This paper introduces a new method for creating a large dataset aimed at detoxifying hate speech using the GPT-4o-mini model. Detoxification involves rewriting harmful language into non-toxic text, which is crucial due to the rise of toxic content online. The authors developed a pipeline that automates this process, replacing human annotators with a language model, and found that the model's performance is comparable to that of humans. They also created a dataset called PARADEHATE, consisting of over 8,000 pairs of hate and non-hate text, which significantly improves the performance of various models in terms of style accuracy, content preservation, and fluency."}, 'zh': {'title': '利用GPT-4o-mini生成仇恨言论去毒化数据集', 'desc': '本文提出了一种新颖的管道，利用GPT-4o-mini生成大规模的仇恨言论去毒化数据集，从而提高基线模型在风格准确性、内容保留和流畅性方面的表现。去毒化是将有害语言重写为非有害文本的任务，随着网络上有毒内容的增加，这一任务变得越来越重要。由于人工标注的成本和敏感性，高质量的去毒化平行数据集仍然稀缺。我们构建了PARADEHATE，这是一个专门用于仇恨言论去毒化的大规模平行数据集，并通过实验验证了基于该数据集的模型的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.00643', 'title': 'SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions', 'url': 'https://huggingface.co/papers/2506.00643', 'abstract': "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.", 'score': 4, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': 'f95c367c9eaf00a9', 'authors': ['Weijie Xu', 'Shixian Cui', 'Xi Fang', 'Chi Xue', 'Stephanie Eckman', 'Chandan Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2506.00643.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#interpretability', '#data', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений языковых моделей в задачах с множественным выбором', 'desc': 'Статья представляет SATA-BENCH - первый специализированный бенчмарк для оценки языковых моделей на вопросах с множественным выбором ответов. Исследование выявило значительные ограничения даже у самых сильных моделей, достигающих лишь 41.8% точного соответствия. Авторы обнаружили проблемы смещения выбора и смещения количества ответов у моделей. Для решения этих проблем предложена стратегия Choice Funnel, сочетающая дебиасинг токенов с адаптивным порогом.'}, 'en': {'title': 'Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel', 'desc': 'The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks.'}, 'zh': {'title': '提升多答案推理的准确性与效率', 'desc': '本文介绍了SATA-BENCH，这是一个专门用于评估大型语言模型（LLMs）在多答案问题上的基准测试。研究发现，现有模型在选择所有正确答案时存在显著的选择偏差和计数偏差，导致准确率低下。为了解决这些问题，提出了Choice Funnel解码策略，通过去偏和自适应阈值引导模型做出更完整和准确的选择。实验结果表明，Choice Funnel在准确匹配率上比竞争基线提高了29%，同时降低了推理成本超过64%。'}}}, {'id': 'https://huggingface.co/papers/2505.24086', 'title': 'ComposeAnything: Composite Object Priors for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.24086', 'abstract': 'ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '2bd92a7129e6945b', 'authors': ['Zeeshan Khan', 'Shizhe Chen', 'Cordelia Schmid'], 'affiliations': ['Inria, École normale supérieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24086.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#diffusion', '#interpretability', '#cv', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение композиции в генерации изображений с помощью 2.5D семантических макетов', 'desc': 'ComposeAnything - это новый подход к улучшению генерации изображений по текстовому описанию. Он использует языковые модели для создания 2.5D семантических макетов, включающих 2D ограничивающие рамки объектов с информацией о глубине и подробными подписями. Этот макет служит сильным и интерпретируемым приором, заменяющим стохастическую инициализацию шумом в диффузионных моделях генерации изображений. ComposeAnything превосходит современные методы на бенчмарках T2I-CompBench и NSR-1K для запросов с 2D/3D пространственными расположениями, большим количеством объектов и сюрреалистическими композициями.'}, 'en': {'title': 'ComposeAnything: Elevating Text-to-Image Generation with 2.5D Layouts', 'desc': 'ComposeAnything is a framework that enhances text-to-image generation by utilizing large language models (LLMs) to create 2.5D semantic layouts. This method improves the arrangement of objects in images by incorporating depth information, which helps maintain coherence and quality in the generated images. Unlike previous models that rely solely on 2D layouts, ComposeAnything provides a more accurate representation of spatial relationships, allowing for better object placement. The framework has shown superior performance on benchmark tests, producing high-quality images that align closely with the provided text descriptions.'}, 'zh': {'title': 'ComposeAnything：提升文本到图像生成的创新框架', 'desc': 'ComposeAnything 是一种新框架，旨在改善文本到图像生成的质量。它利用大型语言模型（LLMs）的推理能力，生成包含深度信息的2.5D语义布局，从而增强对象的放置和一致性。该方法不需要重新训练现有的文本到图像模型，而是通过生成空间和深度感知的粗略合成图像，来指导去噪过程。ComposeAnything 在处理复杂的2D/3D空间布局和超现实组合时，表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2505.22954', 'title': 'Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents', 'url': 'https://huggingface.co/papers/2505.22954', 'abstract': 'The Darwin G\\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Today\'s AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.', 'score': 4, 'issue_id': 4104, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '014c0d439b8212f8', 'authors': ['Jenny Zhang', 'Shengran Hu', 'Cong Lu', 'Robert Lange', 'Jeff Clune'], 'affiliations': ['Canada CIFAR AI Chair', 'Sakana AI', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.22954.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#agi', '#agents', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'Эволюционирующий ИИ: самосовершенствование через итеративную модификацию кода', 'desc': 'Статья представляет Darwin Gödel Machine (DGM) - систему искусственного интеллекта, способную к самосовершенствованию путем итеративной модификации собственного кода. DGM использует принципы дарвиновской эволюции и открытого исследования, поддерживая архив кодирующих агентов и создавая их новые версии с помощью базовой модели. Система значительно улучшила свои способности к кодированию, повысив производительность на бенчмарках SWE-bench и Polyglot. DGM превзошла базовые подходы без самосовершенствования и открытого исследования, представляя важный шаг к созданию самосовершенствующегося ИИ.'}, 'en': {'title': 'Evolving AI: The Future of Self-Improvement', 'desc': 'The Darwin G"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement.'}, 'zh': {'title': '自我改进的AI：达尔文哥德尔机器的创新之路', 'desc': '达尔文哥德尔机器（DGM）通过迭代自我修改和开放式探索来提高其编码能力，超越了其他方法的基准测试。与传统的固定架构AI系统不同，DGM能够自主且持续地改进自身。它借鉴了达尔文进化的理念，维护一个生成编码代理的档案库，并通过采样和基础模型生成新版本，形成多样化的高质量代理树。实验结果表明，DGM在编码能力上显著提升，表现出更好的代码编辑工具和同行评审机制，标志着自我改进AI的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2506.01928', 'title': 'Esoteric Language Models', 'url': 'https://huggingface.co/papers/2506.01928', 'abstract': 'Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)', 'score': 3, 'issue_id': 4100, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '6b37258ad3883db7', 'authors': ['Subham Sekhar Sahoo', 'Zhihan Yang', 'Yash Akhauri', 'Johnna Liu', 'Deepansha Singh', 'Zhoujun Cheng', 'Zhengzhong Liu', 'Eric Xing', 'John Thickstun', 'Arash Vahdat'], 'affiliations': ['Cornell Tech', 'Cornell University', 'MBZUAI', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.01928.jpg', 'data': {'categories': ['#inference', '#open_source', '#benchmark', '#architecture', '#optimization', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Eso-LMs: Революция в языковом моделировании с KV-кэшированием', 'desc': 'Eso-LMs представляют собой новое семейство моделей, объединяющих авторегрессионные и маскированные диффузионные модели. Они вводят KV-кэширование для маскированных диффузионных моделей, что значительно повышает эффективность вывода. Eso-LMs достигают нового уровня производительности на стандартных бенчмарках языкового моделирования. Модели обеспечивают до 65 раз более быстрый вывод по сравнению со стандартными маскированными диффузионными моделями.'}, 'en': {'title': 'Eso-LMs: Fast and Efficient Language Modeling Revolution', 'desc': 'Eso-LMs are a new type of language model that combines features from both autoregressive and masked diffusion models. This fusion allows for better performance in language tasks by improving perplexity and inference speed. A key innovation is the introduction of KV caching in masked diffusion models, which enhances efficiency during inference while still allowing for parallel generation. As a result, Eso-LMs achieve significantly faster inference times compared to traditional models, setting new benchmarks in language modeling.'}, 'zh': {'title': 'Eso-LMs：自回归与掩蔽扩散模型的完美融合', 'desc': 'Eso-LMs是一种新型的语言模型，结合了自回归模型和掩蔽扩散模型的优点。它引入了KV缓存技术，使得在推理时的效率大幅提升，同时保持了并行生成的能力。通过优化采样策略，Eso-LMs在标准语言建模基准上达到了新的最佳性能，推理速度比传统的掩蔽扩散模型快65倍。该模型有效地解决了自回归模型和掩蔽扩散模型的局限性，提供了更好的生成质量和效率。'}}}, {'id': 'https://huggingface.co/papers/2506.01920', 'title': 'From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.01920', 'abstract': 'A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.', 'score': 3, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '42a070cbc2d3afee', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Adel Ammar', 'Yasser Al-Habashi', 'Abdulrahman Al-Batati', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.01920.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🇦🇪', 'ru': {'title': 'Культурно-ориентированная оценка арабских языковых моделей', 'desc': 'Представлен новый фреймворк оценки и набор данных ADMD для тестирования арабских языковых моделей. Проанализированы существующие наборы данных для оценки арабского языка, выявлены проблемы с лингвистической точностью и культурным соответствием. ADMD содержит 490 сложных вопросов по 10 основным областям для оценки языковых моделей. Результаты показали значительные различия в производительности моделей, особенно в областях, требующих глубокого понимания культуры.'}, 'en': {'title': 'Enhancing Arabic Language Models with Cultural Competence', 'desc': 'This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation.'}, 'zh': {'title': '提升阿拉伯语模型评估的文化能力', 'desc': '本文提出了一种新的评估框架和数据集ADMD，用于评估阿拉伯语模型，强调了性能差异和文化能力的重要性。我们分析了现有的阿拉伯语评估数据集，发现了语言准确性、文化对齐和方法论严谨性方面的重大问题。ADMD包含490个具有挑战性的问题，涵盖十个主要领域，旨在解决大型语言模型（LLMs）中的这些局限性。通过使用ADMD评估五个领先的语言模型，我们发现模型在不同领域的表现差异显著，尤其是在需要深厚文化理解和专业知识的领域。'}}}, {'id': 'https://huggingface.co/papers/2506.00723', 'title': 'Pitfalls in Evaluating Language Model Forecasters', 'url': 'https://huggingface.co/papers/2506.00723', 'abstract': 'Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.', 'score': 3, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '4260f72a1f88e8cd', 'authors': ['Daniel Paleka', 'Shashwat Goel', 'Jonas Geiping', 'Florian Tramèr'], 'affiliations': ['ELLIS Institute Tübingen', 'ETH Zurich', 'MPI Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.00723.jpg', 'data': {'categories': ['#data', '#leakage', '#benchmark', '#evaluation'], 'emoji': '⚠️', 'ru': {'title': 'Осторожно с выводами: сложности оценки LLM в прогнозировании', 'desc': 'Статья обсуждает применение больших языковых моделей (LLM) для задач прогнозирования. Авторы предупреждают о трудностях в оценке эффективности LLM для прогнозирования из-за проблем временной утечки данных и сложности экстраполяции результатов на реальные задачи. Они демонстрируют, как недостатки в методологии оценки могут привести к сомнительным выводам о производительности LLM. Авторы призывают к разработке более строгих методов оценки прогнозирующих способностей LLM.'}, 'en': {'title': 'Rethinking Evaluation: Ensuring Trust in LLM Forecasting', 'desc': 'This paper discusses the challenges of evaluating large language models (LLMs) in forecasting tasks, highlighting that claims of LLMs matching or exceeding human performance may be misleading. The authors identify two main issues: the risk of temporal leakage, which can distort evaluation results, and the difficulty in translating evaluation performance to real-world scenarios. They provide a systematic analysis and examples from previous studies to illustrate how these evaluation flaws can undermine confidence in LLM performance claims. The paper calls for the development of more rigorous evaluation methodologies to accurately assess the forecasting capabilities of LLMs.'}, 'zh': {'title': '谨慎评估大型语言模型的预测能力', 'desc': '大型语言模型（LLMs）最近被应用于预测任务，有些研究声称这些系统的表现与人类相当或更好。本文指出，评估LLM预测者存在独特的挑战，因此我们应对这些结论保持谨慎。我们识别出两个主要问题：一是由于多种时间泄漏形式，导致评估结果难以信任；二是从评估表现推断到现实世界预测的难度。通过系统分析和具体实例，我们展示了评估缺陷如何引发对当前和未来性能声明的担忧，并主张需要更严格的评估方法来自信地评估LLM的预测能力。'}}}, {'id': 'https://huggingface.co/papers/2505.21724', 'title': 'OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions', 'url': 'https://huggingface.co/papers/2505.21724', 'abstract': "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.", 'score': 3, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '309d90ff41ad30b0', 'authors': ['Cheng Luo', 'Jianghui Wang', 'Bing Li', 'Siyang Song', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'University of Exeter'], 'pdf_title_img': 'assets/pdf/title_img/2505.21724.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#dataset', '#optimization', '#audio', '#games'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальный ИИ для естественного диалога', 'desc': 'Статья представляет OmniResponse - мультимодальную большую языковую модель для генерации синхронизированных вербальных и невербальных ответов слушателя. Модель использует текст как промежуточную модальность для связи аудио и лицевых реакций. OmniResponse включает в себя компоненты Chrono-Text для временной привязки текстовых токенов и TempoVoice для синхронизированной генерации речи. Для обучения и оценки создан датасет ResponseNet с 696 диалогами, содержащими видео, аудио и аннотации.'}, 'en': {'title': 'Synchronized Responses for Natural Conversations', 'desc': 'This paper presents OmniResponse, a Multimodal Large Language Model designed to generate synchronized verbal and non-verbal responses in conversations. It introduces a new task called Online Multimodal Conversational Response Generation (OMCRG), which focuses on creating real-time feedback based on multimodal inputs from speakers. The model uses text as an intermediate step to ensure that audio and facial responses are well-coordinated. Additionally, it introduces two innovative components, Chrono-Text and TempoVoice, to enhance the quality and synchronization of the generated responses.'}, 'zh': {'title': 'OmniResponse：同步生成多模态响应的创新模型', 'desc': '本文介绍了一种新的任务，称为在线多模态对话响应生成（OMCRG），旨在根据说话者的多模态输入在线生成同步的语言和非语言反馈。为了解决生成的音频和面部反应之间的同步问题，研究者们创新性地引入了文本作为中介模态。我们提出了OmniResponse，这是一种多模态大型语言模型（MLLM），能够自回归地生成高质量的多模态听众响应。通过使用Chrono-Text和TempoVoice等新组件，OmniResponse在语义内容、音视频同步和生成质量方面显著优于基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.19621', 'title': 'Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models', 'url': 'https://huggingface.co/papers/2505.19621', 'abstract': "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS", 'score': 3, 'issue_id': 4093, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '4358fd586e320601', 'authors': ['George Kour', 'Itay Nakash', 'Ateret Anaby-Tavor', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2505.19621.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#multimodal', '#ethics', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Выявление скрытых предубеждений в языковых моделях', 'desc': 'Статья представляет новый бенчмарк под названием Preference, Opinion, and Belief survey (POBs) для оценки субъективных тенденций и предубеждений больших языковых моделей (LLM) в различных областях. Исследователи применили этот бенчмарк к ведущим открытым и закрытым LLM, измеряя такие желаемые свойства, как надежность, нейтральность и согласованность. Результаты показывают, что механизмы рассуждения и самоанализа предлагают лишь ограниченные улучшения в этой области. Обнаружено, что более новые версии моделей становятся менее согласованными и более предвзятыми к определенным точкам зрения.'}, 'en': {'title': 'Assessing Bias in Language Models: A Call for Neutrality', 'desc': 'This paper introduces the Preference, Opinion, and Belief survey (POBs), which evaluates the subjective biases of Large Language Models (LLMs) in various domains. It highlights that as LLMs are increasingly used in decision-making, their inherent biases can shape the advice they provide, potentially reinforcing certain viewpoints. The study assesses leading LLMs for properties like reliability and neutrality, revealing that newer models tend to exhibit greater bias and inconsistency. Additionally, it examines the impact of advanced reasoning techniques on these biases, finding only marginal improvements in performance.'}, 'zh': {'title': '评估大型语言模型的主观偏见', 'desc': '这篇论文介绍了一个名为偏好、观点和信念调查（POBs）的基准，用于评估大型语言模型（LLMs）在社会、文化、伦理和个人领域的主观倾向。研究发现，随着模型版本的更新，它们的偏见和不一致性有所增加，这可能影响它们对用户的建议和推荐。通过对领先的开源和闭源LLMs进行评估，论文测量了模型的可靠性、中立性和一致性等属性。结果表明，尽管推理和自我反思机制在其他任务中有效，但在本研究领域的提升有限，显示出模型在某些观点上的偏见加剧。'}}}, {'id': 'https://huggingface.co/papers/2506.01074', 'title': 'How Programming Concepts and Neurons Are Shared in Code Language Models', 'url': 'https://huggingface.co/papers/2506.01074', 'abstract': "LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.", 'score': 2, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '4fca9ba1a062ca80', 'authors': ['Amir Hossein Kargaran', 'Yihong Liu', 'François Yvon', 'Hinrich Schütze'], 'affiliations': ['LMU Munich & Munich Center for Machine Learning', 'Sorbonne Université & CNRS, ISIR'], 'pdf_title_img': 'assets/pdf/title_img/2506.01074.jpg', 'data': {'categories': ['#transfer_learning', '#plp', '#machine_translation', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Языки программирования в мозге нейросети: ближе к английскому, чем кажется', 'desc': 'Исследование показывает, что в концептуальном пространстве больших языковых моделей (LLM) представления различных языков программирования кластеризуются ближе к английскому языку. Обнаружено, что активация нейронов, специфичных для конкретных языков программирования, наиболее выражена в верхних слоях модели. Языки программирования с высокой степенью схожести имеют близкие представления в модели. Результаты дают понимание того, как LLM внутренне представляют языки программирования, выявляя структурные паттерны в концептуальном пространстве модели.'}, 'en': {'title': 'Unraveling Language Representations in Large Language Models', 'desc': "This paper explores how large language models (LLMs) represent multiple programming languages (PLs) alongside English in their internal concept space. It reveals that LLMs cluster PLs closer to English, particularly in the upper layers, where distinct neuron activations occur for specific languages. The study employs a few-shot translation task across 21 PL pairs, analyzing embeddings and neuron activations to uncover structural patterns in the model's representation of PLs. The findings suggest that highly aligned PLs share similar representations and exhibit unique neuron activations, enhancing our understanding of LLMs in coding tasks."}, 'zh': {'title': '揭示大型语言模型中的编程语言表示结构', 'desc': '本论文研究了大型语言模型（LLMs）在多种编程语言（PLs）与英语之间的关系。我们发现，LLMs的概念空间中，编程语言的表示更接近英语，尤其是在中间层的后半部分，英语的标记概率较高。通过分析神经元激活，我们发现特定语言的神经元主要集中在底层，而每种编程语言独有的神经元则出现在上层。我们的研究揭示了LLMs如何在内部表示编程语言，并展示了模型概念空间中的结构模式。'}}}, {'id': 'https://huggingface.co/papers/2506.00930', 'title': 'Aligning VLM Assistants with Personalized Situated Cognition', 'url': 'https://huggingface.co/papers/2506.00930', 'abstract': "A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.", 'score': 2, 'issue_id': 4099, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '113b1c51e51c3619', 'authors': ['Yongqi Li', 'Shen Zhou', 'Xiaohu Li', 'Xin Miao', 'Jintao Wen', 'Mayi Xu', 'Jianhao Chen', 'Birong Pan', 'Hankun Kang', 'Yuanyuan Zhu', 'Ming Zhong', 'Tieyun Qian'], 'affiliations': ['School of Computer Science, Wuhan University, China', 'Zhongguancun Academy, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00930.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#open_source', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Персонализация визуально-языковых моделей для индивидуального познания', 'desc': 'PCogAlign - это фреймворк для создания модели вознаграждения, позволяющей согласовывать визуально-языковые модели с персонализированным ситуативным познанием. Он использует бенчмарк PCogAlignBench, включающий 18 тысяч примеров и 20 индивидуумов с различными наборами ролей. Фреймворк строит модель вознаграждения, учитывающую познание и основанную на действиях, для персонализированного согласования. Эксперименты и оценки людей показывают надежность бенчмарка и эффективность предложенного подхода.'}, 'en': {'title': 'Aligning AI with Personalized Human Cognition', 'desc': "The paper introduces PCogAlign, a framework designed to create a reward model that aligns vision-language models (VLMs) with personalized situated cognition. It recognizes that individuals from diverse backgrounds have unique cognitive expectations, which can affect their interactions with VLMs. To address this, the authors utilize the sociological concept of Role-Set to categorize individuals and evaluate their actions for personalized alignment. The study includes a benchmark called PCogAlignBench, featuring 18,000 instances across 20 different Role-Sets, demonstrating the framework's effectiveness through experimental results and human evaluations."}, 'zh': {'title': '个性化认知对齐的视觉-语言模型框架', 'desc': '本文提出了一个名为PCogAlign的框架，用于构建与个性化情境认知对齐的视觉-语言模型的奖励模型。研究表明，不同背景的人在相同情境下可能有不同的认知和期望，因此需要针对个体的个性化需求进行对齐。为此，作者基于社会学的角色集概念，简化了个体特征的描述，并构建了一个包含18000个实例和20个不同角色集个体的基准数据集PCogAlignBench。实验结果和人类评估表明，PCogAlignBench的可靠性和PCogAlign的有效性，相关代码和数据集将开源。'}}}, {'id': 'https://huggingface.co/papers/2506.00789', 'title': 'RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems', 'url': 'https://huggingface.co/papers/2506.00789', 'abstract': "RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.", 'score': 2, 'issue_id': 4102, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '5c7a672484e9a0fa', 'authors': ['Yixiao Zeng', 'Tianyu Cao', 'Danqing Wang', 'Xinran Zhao', 'Zimeng Qiu', 'Morteza Ziyadi', 'Tongshuang Wu', 'Lei Li'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00789.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#graphs', '#rag', '#security'], 'emoji': '🧠', 'ru': {'title': 'RARE: Стресс-тест для RAG-систем в реальном мире', 'desc': 'RARE - это новая система оценки устойчивости моделей генерации с дополнением из внешних источников (RAG) к реальным шумам и конфликтам контекста. Она использует графы знаний для создания тестовых наборов данных, охватывающих динамичные области финансов, экономики и политики. RARE оценивает способность моделей сохранять точность при изменении запросов, документов и результатов поиска. Результаты показывают, что RAG-системы особенно уязвимы к изменениям в документах и многоступенчатым запросам.'}, 'en': {'title': 'Enhancing RAG Systems: Evaluating Robustness in Real-World Scenarios', 'desc': 'The paper introduces RARE, a framework designed to evaluate the robustness of Retrieval-Augmented Generation (RAG) systems in real-world scenarios. It addresses how these systems handle noise, conflicting contexts, and rapidly changing information by using a knowledge-graph-driven benchmark. RARE includes a synthesis pipeline that generates complex question sets from a dynamic dataset of time-sensitive documents. The findings reveal that RAG systems are particularly vulnerable to perturbations, especially in multi-hop queries, highlighting the need for improved robustness in document retrieval.'}, 'zh': {'title': '评估RAG系统的鲁棒性', 'desc': 'RARE是一个评估检索增强生成（RAG）系统在真实世界噪声、上下文冲突和时间敏感数据下鲁棒性的框架。它通过一个知识图谱驱动的基准测试，联合测试查询和文档的扰动。RARE构建了一个包含400个专家级时间敏感文档和48,322个问题的数据集，以量化模型在面对变化时的鲁棒性。研究结果表明，RAG系统在多跳查询上的鲁棒性普遍低于单跳查询，且文档的鲁棒性是最薄弱的环节。'}}}, {'id': 'https://huggingface.co/papers/2506.00772', 'title': 'LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.00772', 'abstract': 'Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.', 'score': 2, 'issue_id': 4095, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'd66b2e2afe71cfd9', 'authors': ['Zihang Liu', 'Tianyu Pang', 'Oleg Balabanov', 'Chaoqun Yang', 'Tianjin Huang', 'Lu Yin', 'Yaoqing Yang', 'Shiwei Liu'], 'affiliations': ['Dartmouth College, NH, USA', 'Eindhoven University of Technology, the Netherlands', 'International Computer Science Institute, CA, USA', 'Lawrence Berkeley National Laboratory, CA, USA', 'Tsinghua University, China', 'University of California, Berkeley, CA, USA', 'University of Exeter, Exeter, UK', 'University of Oxford, Oxford, UK', 'University of Surrey, Guildford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.00772.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#low_resource', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная точная настройка больших языковых моделей с помощью разреженного обновления весов', 'desc': 'Статья представляет новый метод точной настройки больших языковых моделей под названием LIFT (Low-rank Informed Sparse Fine-Tuning). LIFT использует низкоранговую аппроксимацию для выявления критически важных весов модели и обновляет только 5% от их общего числа. Этот подход позволяет достичь лучших результатов на задачах рассуждения по сравнению с полной точной настройкой, сохраняя при этом эффективность использования памяти. LIFT также лучше сохраняет знания из исходной предметной области по сравнению с другими методами точной настройки.'}, 'en': {'title': 'Efficient Fine-Tuning with Critical Weights', 'desc': 'This paper introduces a method called Low-rank Informed Sparse Fine-Tuning (LIFT) that improves the efficiency and performance of large language models (LLMs) by focusing on critical weights identified through low-rank approximation. Instead of updating all parameters during fine-tuning, LIFT selectively updates only the top 5% of Principal Weights, which are determined to be the most important for reasoning tasks. This approach not only enhances reasoning capabilities but also reduces the risk of overfitting and catastrophic forgetting, common issues in full fine-tuning. The results show that LIFT outperforms traditional full fine-tuning while preserving more knowledge from the original model, making it a promising strategy for efficient model adaptation.'}, 'zh': {'title': '低秩微调：提升大型语言模型的效率与性能', 'desc': '本论文提出了一种新的稀疏微调方法，称为低秩知情稀疏微调（LIFT），旨在提高大型语言模型的性能和效率。通过低秩近似，我们识别出对推理至关重要的权重，称为主权重，并仅更新这些权重的前5%。与完全微调相比，LIFT在推理任务上表现更好，同时在内存使用上保持高效。该方法在保持源领域知识的同时，能够有效避免过拟合和灾难性遗忘。'}}}, {'id': 'https://huggingface.co/papers/2506.00530', 'title': 'CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing', 'url': 'https://huggingface.co/papers/2506.00530', 'abstract': 'Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.', 'score': 2, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '4c9476ad7ef056e6', 'authors': ['Tianhui Liu', 'Jie Feng', 'Hetian Pang', 'Xin Zhang', 'Tianjian Ouyang', 'Zhiyuan Zhang', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00530.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#dataset', '#reasoning', '#survey'], 'emoji': '🏙️', 'ru': {'title': 'CityLens: Взгляд на город глазами искусственного интеллекта', 'desc': 'CityLens - это комплексный бенчмарк для оценки способностей мультимодальных языковых моделей (LLVM) в прогнозировании социально-экономических показателей на основе спутниковых снимков и изображений с уличных камер. Датасет охватывает 17 городов по всему миру и включает 6 ключевых областей: экономика, образование, преступность, транспорт, здравоохранение и окружающая среда. Авторы определили 11 задач прогнозирования и использовали три парадигмы оценки: прямое предсказание метрик, нормализованная оценка метрик и регрессия на основе признаков. Результаты показывают, что хотя LLVM демонстрируют многообещающие возможности восприятия и рассуждения, они все еще имеют ограничения в прогнозировании городских социально-экономических показателей.'}, 'en': {'title': 'CityLens: Bridging Visual Data and Urban Socioeconomic Insights', 'desc': 'This paper presents CityLens, a benchmark for assessing large language-vision models (LLVMs) in predicting urban socioeconomic indicators using visual data from satellite and street view images. The study creates a multi-modal dataset that includes 17 cities and covers six important domains such as economy and health, reflecting the complexity of urban environments. It defines 11 prediction tasks and employs three evaluation methods to analyze the performance of 17 advanced LLVMs. The findings indicate that while these models show potential in understanding urban data, they still face challenges in accurately predicting socioeconomic conditions, highlighting the need for further research in this area.'}, 'zh': {'title': '通过视觉数据理解城市社会经济条件', 'desc': '本研究介绍了CityLens，这是一个综合基准，用于评估大型语言-视觉模型（LLVMs）在从卫星和街景图像中预测城市社会经济指标的能力。我们构建了一个多模态数据集，涵盖全球17个城市，涉及经济、教育、犯罪、交通、健康和环境等6个关键领域，反映了城市生活的多面性。基于该数据集，我们定义了11个预测任务，并采用三种评估范式：直接度量预测、标准化度量估计和基于特征的回归。结果表明，尽管LLVMs在感知和推理能力上表现出色，但在预测城市社会经济指标方面仍存在局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.00385', 'title': 'MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2506.00385', 'abstract': 'MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.', 'score': 2, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '421c283aaadfdd94', 'authors': ['Yakun Song', 'Jiawei Chen', 'Xiaobin Zhuang', 'Chenpeng Du', 'Ziyang Ma', 'Jian Wu', 'Jian Cong', 'Dongya Jia', 'Zhuo Chen', 'Yuping Wang', 'Yuxuan Wang', 'Xie Chen'], 'affiliations': ['Bytedance Inc.', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00385.jpg', 'data': {'categories': ['#audio', '#optimization', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'MagiCodec: Семантическая токенизация аудио для улучшенной генерации', 'desc': 'MagiCodec - это новый аудио кодек на основе трансформера, разработанный для улучшения семантической токенизации при сохранении высокого качества реконструкции. Он использует многоступенчатый процесс обучения, включающий добавление гауссова шума и регуляризацию скрытого пространства. Эксперименты показывают, что MagiCodec превосходит современные кодеки как по качеству реконструкции, так и по эффективности в последующих задачах. Токены, создаваемые MagiCodec, имеют распределение, похожее на закон Ципфа, что улучшает совместимость с генеративными моделями на основе языковых моделей.'}, 'en': {'title': 'MagiCodec: Transforming Audio for Better AI Compatibility', 'desc': 'MagiCodec is a new audio codec that uses a Transformer model to improve how audio is represented as tokens. It focuses on enhancing the semantic meaning of these tokens while still ensuring that the audio can be reconstructed accurately. The codec employs a special training method that includes adding noise and regularization to make the tokens more expressive and useful for generative models. Tests show that MagiCodec outperforms existing codecs in both audio quality and its ability to work with other AI models.'}, 'zh': {'title': 'MagiCodec：提升音频语义表达的编解码器', 'desc': 'MagiCodec是一种基于Transformer的音频编解码器，旨在提高语义标记的表达能力，同时保持高质量的重建效果。与传统编解码器不同，MagiCodec在训练过程中引入了高斯噪声和潜在正则化，以增强生成代码的语义表现力。实验结果表明，MagiCodec在重建质量和下游任务上均优于现有的最先进编解码器。其生成的标记呈现出类似Zipf分布的特征，增强了与基于语言模型的生成架构的兼容性。'}}}, {'id': 'https://huggingface.co/papers/2506.00381', 'title': 'Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG', 'url': 'https://huggingface.co/papers/2506.00381', 'abstract': 'Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.', 'score': 2, 'issue_id': 4101, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '78b01a3a61f68dc7', 'authors': ['Siavash Shams', 'Richard Antonello', 'Gavin Mischler', 'Stephan Bickel', 'Ashesh Mehta', 'Nima Mesgarani'], 'affiliations': ['Department of Electrical Engineering, Columbia University, USA', 'The Feinstein Institutes for Medical Research, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.00381.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#transfer_learning', '#multimodal', '#healthcare', '#science'], 'emoji': '🧠', 'ru': {'title': 'От мозговых волн к словам: новый уровень нейродекодирования', 'desc': 'Neuro2Semantic - это новая система для реконструкции семантического содержания речи из нейронных сигналов. Она использует LSTM-адаптер для выравнивания сигналов с предобученными текстовыми эмбеддингами и модуль-корректор для генерации естественного текста. Метод показывает высокую эффективность даже при ограниченном объеме нейронных данных (30 минут). Neuro2Semantic превосходит существующие методы и открывает возможности для практического применения в нейроинтерфейсах.'}, 'en': {'title': 'Transforming Brain Signals into Natural Language', 'desc': 'Neuro2Semantic is a new framework that translates brain signals into meaningful text. It uses a Long Short-Term Memory (LSTM) model to align neural signals from intracranial EEG with existing text representations. After alignment, a correction module generates coherent and natural language from these signals. This method is effective even with limited data, making it a significant advancement in decoding language from neural activity.'}, 'zh': {'title': '神经信号重建语义内容的新方法', 'desc': 'Neuro2Semantic 是一个新颖的框架，旨在从脑内电极记录的神经信号中重建语义内容。该方法采用了基于 LSTM 的对齐技术，将神经信号与预训练的文本嵌入对齐，并通过一个校正模块直接生成自然的连续文本。与以往的解码方法相比，Neuro2Semantic 在数据量有限的情况下表现出色，能够在仅有 30 分钟的神经数据下实现强大的性能。该研究为脑机接口和神经解码技术的实际应用提供了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2505.17127', 'title': 'Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts', 'url': 'https://huggingface.co/papers/2505.17127', 'abstract': 'Visual CounterFact and PvP steering vectors help interpret and control the competition between visual input and memorized world knowledge in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.', 'score': 2, 'issue_id': 4105, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '1c1f950796c1f608', 'authors': ['Michal Golovanevsky', 'William Rudman', 'Michael Lepori', 'Amir Bar', 'Ritambhara Singh', 'Carsten Eickhoff'], 'affiliations': ['Brown University', 'Tel Aviv University', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17127.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#reasoning', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Управление конкуренцией между визуальными данными и знаниями в многомодальных ИИ-моделях', 'desc': 'В статье представлены новые инструменты для интерпретации и управления многомодальными большими языковыми моделями (MLLM). Авторы вводят набор данных Visual CounterFact, содержащий визуально реалистичные контрфактические примеры, которые противопоставляют предварительные знания о мире и визуальную информацию. Исследование показывает, что предсказания модели сначала отражают запомненные знания, но затем смещаются в сторону визуальных данных. Для контроля этого поведения предложены векторы управления PvP (Pixels Versus Priors), позволяющие влиять на выходные данные модели.'}, 'en': {'title': 'Balancing Visual Input and Knowledge in MLLMs', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) balance visual information and memorized knowledge when making predictions. It introduces a dataset called Visual CounterFact, which presents scenarios where visual cues conflict with known facts, allowing researchers to observe how models prioritize information. The study finds that while models initially rely on memorized knowledge, they increasingly favor visual evidence as processing continues. To manage this competition, the authors propose Pixels Versus Priors (PvP) steering vectors, which can adjust model outputs to emphasize either visual input or prior knowledge effectively.'}, 'zh': {'title': '控制视觉与记忆知识的竞争', 'desc': '这篇论文探讨了多模态大型语言模型（MLLMs）在视觉输入和记忆知识之间的竞争。我们引入了Visual CounterFact数据集，通过视觉反事实来直接对抗世界知识和视觉信息。研究发现，模型的预测最初依赖于记忆知识，但在后期层次中逐渐转向视觉证据。为控制这种行为，我们提出了Pixels Versus Priors（PvP）引导向量，可以通过激活级别干预来调整模型输出，成功将大部分预测从记忆知识转向视觉输入。'}}}, {'id': 'https://huggingface.co/papers/2506.01062', 'title': 'SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models', 'url': 'https://huggingface.co/papers/2506.01062', 'abstract': 'SealQA evaluates search-augmented language models\' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.', 'score': 1, 'issue_id': 4104, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '32b5670dde584ad1', 'authors': ['Thinh Pham', 'Nguyen Nguyen', 'Pratibha Zunjare', 'Weiyuan Chen', 'Yu-Min Tseng', 'Tu Vu'], 'affiliations': ['Virginia Tech, Blacksburg, VA 24061'], 'pdf_title_img': 'assets/pdf/title_img/2506.01062.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'SealQA: Выявление ограничений языковых моделей в работе с противоречивой информацией', 'desc': 'SealQA - это новый эталонный тест для оценки моделей языка с поисковым дополнением на фактологических вопросах с противоречивыми или зашумленными результатами поиска. Тест включает три варианта: Seal-0, Seal-Hard и LongSeal, оценивающие фактическую точность и способности к рассуждению. Оценка выявила критические ограничения современных моделей, даже передовые LLM показывают низкую производительность во всех вариантах SealQA. Исследование также показало, что увеличение вычислительной мощности не приводит к надежному улучшению результатов.'}, 'en': {'title': 'SealQA: Unveiling the Limits of Search-Augmented Language Models', 'desc': 'SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks.'}, 'zh': {'title': 'SealQA：评估语言模型在复杂搜索中的表现', 'desc': 'SealQA是一个新的基准，用于评估增强搜索的语言模型在面对冲突或噪声搜索结果时的表现，特别是在事实寻求问题上。该基准分为三种类型：Seal-0和Seal-Hard主要评估模型的事实准确性和推理能力，而LongSeal则测试长上下文和多文档推理能力。评估结果显示，当前的前沿模型在所有SealQA类型中表现不佳，尤其是在Seal-0中，准确率极低。尽管一些先进的推理模型存在，但它们在面对噪声搜索结果时仍然非常脆弱，且增加计算资源并未显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2506.00979', 'title': 'IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection', 'url': 'https://huggingface.co/papers/2506.00979', 'abstract': 'IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '13910274f7d956aa', 'authors': ['Wayne Zhang', 'Changjiang Jiang', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng'], 'affiliations': ['Wuhan University', 'π3 AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.00979.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#interpretability', '#multimodal', '#synthetic', '#benchmark', '#architecture'], 'emoji': '🕵️', 'ru': {'title': 'Единая система для прозрачного обнаружения искусственного контента', 'desc': 'Статья представляет новый набор данных IVY-FAKE и архитектуру IVY-XDETECTOR для обнаружения искусственно сгенерированного контента (AIGC). Этот подход решает проблемы существующих методов, предоставляя единую интерпретируемую систему для анализа как изображений, так и видео. Набор данных содержит более 150 000 аннотированных образцов с подробными текстовыми объяснениями. Предложенная модель достигает высоких результатов в обнаружении AIGC для изображений и видео.'}, 'en': {'title': 'Unifying AIGC Detection with Explainability', 'desc': 'The paper introduces the IVY-FAKE dataset and the Ivy Explainable Detector (IVY-XDETECTOR) to improve the detection of AI-generated content (AIGC) in images and videos. Current detection methods often lack transparency and only classify content without providing explanations, which can undermine trust. IVY-FAKE offers a large-scale dataset with over 150,000 annotated samples, enhancing the interpretability of detection results through detailed reasoning. The IVY-XDETECTOR utilizes a unified vision-language model to achieve state-of-the-art performance in detecting both images and videos, addressing the limitations of existing approaches.'}, 'zh': {'title': '统一可解释的 AIGC 检测新框架', 'desc': 'IVY-FAKE 数据集和 Ivy Explainable Detector (IVY-XDETECTOR) 架构旨在解决当前 AIGC 检测的局限性，提供一个统一且可解释的图像和视频检测框架。随着人工智能生成内容（AIGC）在视觉领域的快速发展，生成的图像和视频变得越来越真实，这引发了对内容真实性的担忧。现有的 AIGC 检测方法通常作为黑箱二元分类器，缺乏可解释性，且无法在统一框架下同时检测图像和视频。我们的研究通过引入 IVY-FAKE 数据集和 IVY-XDETECTOR，显著提升了多模态 AIGC 检测的性能和透明度。'}}}, {'id': 'https://huggingface.co/papers/2506.00469', 'title': 'Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data', 'url': 'https://huggingface.co/papers/2506.00469', 'abstract': 'Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.', 'score': 1, 'issue_id': 4093, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2ebeb941a6f4f7cc', 'authors': ['Shaoxiong Ji', 'Zihao Li', 'Jaakko Paavola', 'Indraneil Paul', 'Hengyu Luo', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2506.00469.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multilingual', '#transfer_learning', '#open_source', '#machine_translation', '#training', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'Двуязычные данные улучшают многоязычную адаптацию больших языковых моделей', 'desc': 'Это исследование посвящено влиянию двуязычных данных перевода на многоязычную адаптацию моделей семейства Llama3 к 500 языкам. Авторы создали корпус MaLA, содержащий данные более чем 2500 языковых пар, и разработали набор EMMA-500 Llama 3 из четырех многоязычных моделей. Эксперименты показали, что использование двуязычных данных улучшает языковой перенос и производительность, особенно для малоресурсных языков. Результаты оценивались на 7 задачах и 12 бенчмарках, а все ресурсы были открыты для общего доступа.'}, 'en': {'title': 'Boosting Multilingual Models with Bilingual Data', 'desc': 'This paper explores how using bilingual translation data can improve the performance of the Llama3 family of models in multilingual settings. It introduces the MaLA bilingual translation corpus, which includes data from over 2,500 language pairs, to facilitate better language adaptation. The authors develop the EMMA-500 suite of models, which are continually pre-trained on a vast amount of diverse data. Their findings show that incorporating bilingual data significantly enhances language transfer, especially for languages with fewer resources.'}, 'zh': {'title': '双语数据助力多语言模型提升性能', 'desc': '本文研究了在大规模多语言持续预训练中，双语翻译数据的关键设计决策。我们构建了MaLA双语翻译语料库，包含2500多个语言对的数据，以支持Llama3模型在500种语言上的适应。通过开发EMMA-500 Llama 3模型套件，我们评估了使用或不使用双语翻译数据的持续预训练效果。结果表明，双语数据能够增强语言迁移和性能，尤其是在资源稀缺的语言上。'}}}, {'id': 'https://huggingface.co/papers/2505.24216', 'title': 'Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation', 'url': 'https://huggingface.co/papers/2505.24216', 'abstract': 'A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM', 'score': 1, 'issue_id': 4102, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4e2243b50d13c1bf', 'authors': ['Prasanna Reddy Pulakurthi', 'Majid Rabbani', 'Jamison Heard', 'Sohail Dianat', 'Celso M. de Melo', 'Raghuveer Rao'], 'affiliations': ['DEVCOM Army Research Laboratory, Adelphi, MD, USA', 'Rochester Institute of Technology, Rochester, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24216.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#benchmark', '#data', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Улучшение адаптации к домену без исходных данных с помощью умной аугментации и перевзвешивания', 'desc': 'Статья представляет новый метод аугментации данных под названием Shuffle PatchMix (SPM) и стратегию перевзвешивания для улучшения производительности в задаче адаптации к домену без исходных данных (Source-Free Domain Adaptation, SFDA). SPM перемешивает и смешивает участки изображений для создания разнообразных и сложных аугментаций. Стратегия перевзвешивания отдает приоритет надежным псевдо-меткам для уменьшения шума в разметке. Предложенные методы достигают наилучших результатов на трех основных бенчмарках: PACS, VisDA-C и DomainNet-126.'}, 'en': {'title': 'Enhancing Source-Free Domain Adaptation with Shuffle PatchMix', 'desc': 'This paper presents a novel approach to Source-Free Domain Adaptation (SFDA) using a technique called Shuffle PatchMix (SPM) and a reweighting strategy. SPM enhances data augmentation by shuffling and blending image patches, creating diverse training samples that help the model generalize better. The reweighting strategy focuses on selecting reliable pseudo-labels, reducing the impact of label noise during training. The proposed methods achieve state-of-the-art performance on several benchmarks, demonstrating significant improvements in accuracy, especially on smaller datasets like PACS.'}, 'zh': {'title': '无源领域适应的新突破：Shuffle PatchMix', 'desc': '本文提出了一种新的数据增强技术Shuffle PatchMix（SPM）和重加权策略，以提高无源领域适应（SFDA）的性能。SPM通过打乱和混合图像块生成多样化的增强样本，而重加权策略则优先考虑可靠的伪标签，以减少标签噪声的影响。这些技术在较小的数据集上表现尤为出色，如PACS，能够有效应对过拟合和伪标签噪声的问题。最终，在PACS、VisDA-C和DomainNet-126等三个主要基准上取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2505.22865', 'title': 'BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models', 'url': 'https://huggingface.co/papers/2505.22865', 'abstract': 'A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a 42% confusion rate.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0d57f1fd2f698379', 'authors': ['Susan Liang', 'Dejan Markovic', 'Israel D. Gebru', 'Steven Krenn', 'Todd Keebler', 'Jacob Sandakly', 'Frank Yu', 'Samuel Hassel', 'Chenliang Xu', 'Alexander Richard'], 'affiliations': ['1', '2'], 'pdf_title_img': 'assets/pdf/title_img/2505.22865.jpg', 'data': {'categories': ['#audio', '#diffusion'], 'emoji': '🎧', 'ru': {'title': 'Реалистичный бинауральный синтез речи в реальном времени', 'desc': 'BinauralFlow - это фреймворк для потокового бинаурального синтеза речи, основанный на методе сопоставления потоков. Он использует каузальную архитектуру U-Net для генерации высококачественного бинаурального аудио, неотличимого от реальных записей. Модель рассматривает бинауральный рендеринг как задачу генерации, а не регрессии, что позволяет точно моделировать бинауральные сигналы, реверберацию помещения и фоновые звуки. Предложенный непрерывный конвейер вывода включает потоковые операции STFT/ISTFT, банк буферов и другие компоненты для улучшения непрерывности и скорости рендеринга.'}, 'en': {'title': 'BinauralFlow: Real-Time, High-Quality Binaural Audio Synthesis', 'desc': 'The paper presents BinauralFlow, a novel framework for generating high-quality binaural audio that closely resembles natural hearing. It utilizes a causal U-Net architecture and a flow matching approach to enhance the synthesis of binaural cues, room reverb, and ambient sounds. The framework is designed for streaming inference, allowing for real-time audio generation by processing only past audio frames. Evaluations show that BinauralFlow outperforms state-of-the-art methods, achieving audio quality that is nearly indistinguishable from real-world recordings.'}, 'zh': {'title': '高质量双耳音频合成的新突破', 'desc': '本文提出了一种基于流匹配的流式双耳语音合成框架，称为BinauralFlow。该框架使用因果U-Net架构和连续推理管道，能够生成高质量、与真实录音几乎无法区分的双耳音频。我们将双耳渲染视为生成问题，设计了条件流匹配模型来提高音频渲染质量。此外，本文还引入了连续推理管道，以改善渲染的连续性和速度，实验结果表明该方法优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2505.21668', 'title': 'R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21668', 'abstract': 'R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a1b1a393cb209a0c', 'authors': ['Yongchao Chen', 'Yueying Liu', 'Junwei Zhou', 'Yilun Hao', 'Jingquan Wang', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab', 'University of Illinois Urbana-Champaign', 'University of Michigan', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.21668.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#data', '#dataset', '#training', '#rl', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Усиление языковых моделей автономной генерацией кода', 'desc': 'R1-Code-Interpreter - это расширение текстовой языковой модели, обученное с помощью многоэтапной контролируемой доводки и обучения с подкреплением для автономной генерации кода во время пошагового рассуждения. Модель обучалась на 144 задачах рассуждения и планирования, каждая из которых содержит более 200 разнообразных вопросов. Финальная модель R1-CI-14B улучшила среднюю точность на 37 тестовых задачах с 44.0% до 64.1%, превзойдя текстовую версию GPT-4 (58.6%) и приблизившись к GPT-4 с Code Interpreter (70.9%). Исследование показало, что обучение с использованием Code Interpreter значительно сложнее из-за высокого разнообразия задач и дорогостоящего выполнения кода.'}, 'en': {'title': 'Empowering LLMs with Code: R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter enhances text-only Large Language Models (LLMs) by integrating improved code generation capabilities through supervised fine-tuning and reinforcement learning. This model addresses the limitations of LLMs in tasks that require precise computation and algorithmic reasoning, allowing it to autonomously generate code queries during multi-step reasoning. The research involves fine-tuning Qwen-2.5 models on a diverse set of reasoning and planning tasks, demonstrating significant improvements in accuracy compared to previous models. The findings highlight the importance of the supervised fine-tuning stage in effectively training LLMs to leverage code for better performance across various tasks.'}, 'zh': {'title': '提升代码生成能力的R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter 是一种扩展文本模型的机器学习方法，旨在提高代码生成能力。通过监督微调和强化学习，该模型能够在推理和规划任务中表现更好。研究表明，传统的大型语言模型在需要精确计算和符号操作的任务中仍然存在困难。R1-Code-Interpreter 通过多轮的训练，能够自主生成代码查询，从而提升了模型的准确性和自我检查能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20285', 'title': 'MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability', 'url': 'https://huggingface.co/papers/2505.20285', 'abstract': 'A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MaskSearch. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MaskSearch significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.', 'score': 1, 'issue_id': 4098, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'fb55a3a486e6e5a9', 'authors': ['Weiqi Wu', 'Xin Guan', 'Shen Huang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jiuxin Cao', 'Hai Zhao', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.20285.jpg', 'data': {'categories': ['#training', '#agents', '#optimization', '#rag', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'MaskSearch: универсальный поиск и рассуждения для больших языковых моделей', 'desc': 'В статье представлена новая система предобучения MaskSearch, которая улучшает способности больших языковых моделей (LLM) к универсальному поиску и рассуждениям. Эта система использует задачу предсказания маскированных токенов с дополнительным извлечением информации (Retrieval Augmented Mask Prediction). MaskSearch применяет комбинацию методов обучения, включая обучение с учителем и обучение с подкреплением. Результаты экспериментов показывают значительное улучшение производительности LLM в задачах открытого многоходового вопросно-ответного анализа.'}, 'en': {'title': 'Empowering LLMs with Universal Retrieval and Reasoning through MaskSearch', 'desc': 'The paper introduces MaskSearch, a new pre-training framework designed to improve Large Language Models (LLMs) by enhancing their retrieval and reasoning abilities. It employs a Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to use search tools to predict masked spans in data, thereby gaining universal capabilities. The training process includes Supervised Fine-tuning (SFT) and Reinforcement Learning (RL), utilizing a multi-agent system and a hybrid reward system to optimize performance. The results show that MaskSearch significantly boosts the effectiveness of LLMs in open-domain multi-hop question answering tasks.'}, 'zh': {'title': 'MaskSearch：提升语言模型的检索与推理能力', 'desc': '本文提出了一种新的预训练框架，称为MaskSearch，旨在增强大型语言模型（LLMs）的检索和推理能力。通过引入检索增强的掩码预测任务（RAMP），模型能够利用搜索工具填补大量预训练数据中的掩码部分，从而获得通用的检索和推理能力。我们采用监督微调和强化学习相结合的方法进行训练，并引入了课程学习策略，使模型能够逐步从简单到复杂的实例中学习。实验结果表明，MaskSearch显著提升了基于LLM的搜索代理在开放领域多跳问答任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.18128', 'title': 'Frankentext: Stitching random text fragments into long-form narratives', 'url': 'https://huggingface.co/papers/2505.18128', 'abstract': 'We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.', 'score': 1, 'issue_id': 4100, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '2cd0e9db501521da', 'authors': ['Chau Minh Pham', 'Jenna Russell', 'Dzung Pham', 'Mohit Iyyer'], 'affiliations': ['University of Maryland, College Park', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2505.18128.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#story_generation', '#training'], 'emoji': '🧟\u200d♂️', 'ru': {'title': 'Frankentexts: новый вызов для генерации и детекции ИИ-текстов', 'desc': 'Исследователи представили новый тип длинных текстов, называемых Frankentexts, которые создаются языковыми моделями с ограничением копирования большей части токенов из человеческих текстов. Этот метод требует от моделей соблюдения заданной темы, интеграции разрозненных фрагментов текста и создания связного повествования. Gemini-2.5-Pro показала высокие результаты в этой задаче, создавая когерентные и релевантные тексты. Исследование выявило ограничения существующих детекторов ИИ-текстов и открывает дискуссию о разработке эффективных методов обнаружения смешанного авторства.'}, 'en': {'title': 'Frankentexts: Merging Human Creativity with AI Precision', 'desc': "Frankentexts are a novel form of long narratives created by large language models (LLMs) that must predominantly use human-written text. This approach tests the model's ability to generate coherent stories while adhering to strict guidelines on text copying. The process involves drafting a narrative by merging human passages and revising it to meet a specified ratio of copied content. The results show that while the generated texts are often coherent and relevant, they can still be distinguished from human writing due to inconsistencies in tone and grammar, highlighting challenges in AI text detection."}, 'zh': {'title': 'Frankentexts：人机共创的新挑战', 'desc': '本文介绍了一种新型的长篇叙事文本，称为Frankentexts，这些文本由大型语言模型（LLMs）在极端约束下生成，要求大部分（例如90%）的词汇必须逐字复制自人类写作。这项任务对可控生成提出了挑战，要求模型满足写作提示，整合不同的文本片段，并生成连贯的叙事。我们通过指导模型选择和组合人类写作的段落来生成初稿，然后在保持用户指定的复制比例的同时，迭代修订初稿。研究结果表明，Gemini-2.5-Pro在此任务中表现出色，81%的Frankentexts连贯且100%与提示相关。'}}}, {'id': 'https://huggingface.co/papers/2505.16122', 'title': 'Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning', 'url': 'https://huggingface.co/papers/2505.16122', 'abstract': "Plan-and-Budget framework enhances reasoning efficiency in LLMs by allocating token budgets based on estimated sub-question complexity, improving accuracy, reducing token usage, and boosting $E^3$ metric.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the E^3 metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in E^3. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.", 'score': 1, 'issue_id': 4105, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'fbd65f85b45afd41', 'authors': ['Junhong Lin', 'Xinyue Zeng', 'Jie Zhu', 'Song Wang', 'Julian Shun', 'Jun Wu', 'Dawei Zhou'], 'affiliations': ['MIT CSAIL', 'Michigan State University', 'University of Virginia', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.16122.jpg', 'data': {'categories': ['#training', '#inference', '#small_models', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Умное распределение ресурсов для эффективных рассуждений ИИ', 'desc': 'Фреймворк Plan-and-Budget повышает эффективность рассуждений в больших языковых моделях (LLM) путем распределения токенов на основе оценки сложности подвопросов. Это улучшает точность, снижает использование токенов и повышает метрику $E^3$. Фреймворк разбивает сложные запросы на подвопросы и адаптивно распределяет бюджет токенов. План-и-Бюджет улучшает эффективность рассуждений для различных задач и моделей без необходимости переобучения.'}, 'en': {'title': 'Smart Token Allocation for Efficient Reasoning in LLMs', 'desc': "The paper introduces the Plan-and-Budget framework, which enhances the reasoning efficiency of Large Language Models (LLMs) by intelligently allocating token budgets based on the complexity of sub-questions. It addresses the common issue of overthinking in LLMs, where they generate excessive and irrelevant reasoning for simple queries. By employing the Bayesian Budget Allocation Model (BBAM), the framework decomposes complex queries into manageable parts and adapts token usage to improve both accuracy and computational efficiency. The results show significant improvements, including up to 70% accuracy gains and a 39% reduction in token usage, demonstrating the framework's effectiveness across various tasks and models."}, 'zh': {'title': '优化推理效率，提升模型表现的Plan-and-Budget框架', 'desc': '本文提出了一种名为Plan-and-Budget的框架，旨在提高大型语言模型（LLMs）的推理效率。该框架通过根据子问题的复杂性分配令牌预算，减少了不必要的计算，同时提高了模型的准确性。研究表明，许多LLMs在处理简单查询时会出现过度思考的问题，而Plan-and-Budget能够有效地将复杂查询分解为子问题，从而优化推理过程。通过实证分析，Plan-and-Budget在多个任务上实现了显著的性能提升，证明了其在不重新训练的情况下缩小模型性能差距的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.15772', 'title': 'MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling', 'url': 'https://huggingface.co/papers/2505.15772', 'abstract': 'Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning.', 'score': 1, 'issue_id': 4095, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '6b39ad10d21b05d8', 'authors': ['Yifan Cheng', 'Ruoyi Zhang', 'Jiatong Shi'], 'affiliations': ['Carnegie Mellon University, Pittsburgh, PA, USA', 'Fish Audio, Santa Clara, CA, USA', 'Huazhong University of Science and Technology, Wuhan, Hubei, China', 'Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15772.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#audio', '#data'], 'emoji': '🎭', 'ru': {'title': 'Автоматизированная система для создания высококачественных наборов данных эмоциональной речи', 'desc': 'MIKU-PAL - это автоматизированная мультимодальная система для извлечения эмоциональной речи из немаркированных видеоданных. Система использует алгоритмы обнаружения и отслеживания лиц, а также мультимодальную большую языковую модель (MLLM) для анализа эмоций. MIKU-PAL достигает точности на уровне человека (68,5% на наборе данных MELD) и высокой согласованности (0,93 по шкале Флейса каппа), при этом работая быстрее и дешевле ручной разметки. На основе этой системы был создан набор данных MIKU-EmoBench объемом 131,2 часа, содержащий 26 типов эмоциональной речи.'}, 'en': {'title': 'Automating Emotional Speech Extraction with MIKU-PAL', 'desc': 'This paper introduces MIKU-PAL, an automated system designed to extract emotional speech data from unlabeled video sources. It utilizes face detection and tracking, combined with a multimodal large language model, to analyze emotions effectively. The system achieves high accuracy and consistency in emotional speech annotation, outperforming traditional human methods in both cost and speed. Additionally, it provides a new dataset, MIKU-EmoBench, which includes a diverse range of emotional speech categories for further research in speech synthesis.'}, 'zh': {'title': 'MIKU-PAL：高效一致的情感语音提取新方法', 'desc': '本论文提出了一种名为MIKU-PAL的全自动多模态管道，用于从未标记的视频数据中提取高一致性的情感语音。我们利用人脸检测和跟踪算法，开发了一个自动情感分析系统，使用多模态大语言模型（MLLM）。实验结果表明，MIKU-PAL在情感识别上达到了人类水平的准确率（68.5%），并且一致性显著优于人工标注（0.93 Fleiss kappa分数）。基于该系统，我们还发布了一个细粒度情感语音数据集MIKU-EmoBench（131.2小时），为情感文本到语音和视觉语音克隆提供了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2506.01666', 'title': 'Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models', 'url': 'https://huggingface.co/papers/2506.01666', 'abstract': "A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.", 'score': 0, 'issue_id': 4096, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'cf55396f01f6e92c', 'authors': ['Florian Fürrutter', 'Zohim Chandani', 'Ikko Hamamura', 'Hans J. Briegel', 'Gorka Muñoz-Gil'], 'affiliations': ['Institute for Theoretical Physics University of Innsbruck', 'NVIDIA Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2506.01666.jpg', 'data': {'categories': ['#science', '#dataset', '#diffusion', '#multimodal', '#optimization', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Квантовая компиляция на новом уровне: диффузионная модель для генерации схем', 'desc': 'Представлена мультимодальная модель шумоподавляющей диффузии для генерации структуры и непрерывных параметров квантовых схем. Модель использует два независимых процесса диффузии: один для выбора дискретных вентилей, другой для предсказания параметров. Это обеспечивает эффективную альтернативу традиционным методам компиляции квантовых операций. Модель позволяет быстро генерировать большие наборы схем для конкретных операций, что помогает извлекать ценные эвристики для синтеза квантовых схем.'}, 'en': {'title': 'Revolutionizing Quantum Circuit Compilation with Diffusion Models', 'desc': "This paper presents a multimodal denoising diffusion model designed to efficiently generate both the structure and continuous parameters of quantum circuits. Unlike traditional methods that rely on lengthy search algorithms and gradient-based optimizations, this model utilizes two independent diffusion processes to handle discrete gate selection and parameter prediction simultaneously. The authors benchmark the model's performance across various qubit counts and circuit complexities, demonstrating its accuracy and efficiency. Additionally, the rapid circuit generation capability allows for the creation of large datasets, which can be used to uncover new insights into quantum circuit synthesis."}, 'zh': {'title': '高效编译量子电路的新方法', 'desc': '本文介绍了一种多模态去噪扩散模型，用于生成量子电路的结构和连续参数，提供了一种高效的替代传统量子操作编译方法。当前的编译方法虽然能降低编译误差，但运行时间长且需要多次调用量子硬件或昂贵的经典模拟，限制了其扩展性。新提出的模型同时生成电路的离散门选择和参数预测，利用两个独立的扩散过程进行优化。通过对不同实验的基准测试，分析了该方法在不同量子比特数量、电路深度和参数化门比例下的准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.00523', 'title': 'SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation', 'url': 'https://huggingface.co/papers/2506.00523', 'abstract': 'Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.', 'score': 0, 'issue_id': 4099, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '6f9b962d86942eda', 'authors': ['Xingtong Ge', 'Xin Zhang', 'Tongda Xu', 'Yi Zhang', 'Xinjie Zhang', 'Yan Wang', 'Jun Zhang'], 'affiliations': ['Institute for AI Industry Research, Tsinghua University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.00523.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшенная дистилляция для генерации изображений по тексту', 'desc': 'Статья представляет усовершенствованный метод дистилляции для крупномасштабных моделей генерации изображений по тексту. Авторы предлагают имплицитное выравнивание распределений (IDA) для регуляризации расстояния между распределениями генератора и фейковых данных. Также вводится внутрисегментное руководство (ISG) для перераспределения важности временных шагов из учительской модели. Эти улучшения позволяют успешно применить дистилляцию к таким моделям как Stable Diffusion 3.5 и FLUX, повышая их производительность.'}, 'en': {'title': 'Enhancing DMD for Better Convergence in Text-to-Image Models', 'desc': "This paper addresses the challenges of applying Distribution Matching Distillation (DMD) to large-scale text-to-image models, particularly focusing on convergence issues. The authors introduce implicit distribution alignment (IDA) to help align the generator's output with the target distribution, improving the training process. Additionally, they propose intra-segment guidance (ISG) to enhance the importance of timesteps derived from the teacher model, further aiding convergence. The resulting model, SenseFlow, demonstrates improved performance in distillation tasks for both diffusion and flow-based models."}, 'zh': {'title': '提升大规模模型性能的蒸馏新方法', 'desc': '本文提出了一种改进的分布匹配蒸馏方法，旨在解决大规模文本到图像模型的收敛问题。我们引入了隐式分布对齐（IDA）和段内引导（ISG）来优化生成器与假分布之间的距离，从而提高模型的性能。通过这两种方法，DMD在大型模型如SD 3.5和FLUX上实现了更好的收敛。最终，我们的模型SenseFlow在蒸馏过程中表现出色，适用于扩散和流匹配模型。'}}}, {'id': 'https://huggingface.co/papers/2505.24864', 'title': 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.24864', 'abstract': "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", 'score': 77, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '390a294f460cedfc', 'authors': ['Mingjie Liu', 'Shizhe Diao', 'Ximing Lu', 'Jian Hu', 'Xin Dong', 'Yejin Choi', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24864.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl', '#alignment', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Расширение границ рассуждений языковых моделей с помощью длительного обучения с подкреплением', 'desc': 'Статья описывает новый метод обучения языковых моделей с подкреплением (RL) под названием ProRL. Авторы показывают, что длительное RL-обучение может раскрыть новые стратегии рассуждений, недоступные базовым моделям. ProRL включает контроль расхождения KL, сброс эталонной политики и набор разнообразных задач. Эмпирический анализ демонстрирует, что модели, обученные с помощью RL, превосходят базовые модели в различных оценках pass@k.'}, 'en': {'title': 'Unlocking New Reasoning Strategies with ProRL', 'desc': 'This paper explores the effectiveness of reinforcement learning (RL) in enhancing the reasoning capabilities of language models. The authors introduce a new training method called ProRL, which employs techniques like KL divergence control and reference policy resetting to improve model performance. Their experiments show that models trained with ProRL outperform base models in various reasoning tasks, even in cases where base models struggle. The study suggests that prolonged RL training can help discover new reasoning strategies, indicating that RL can significantly expand the reasoning abilities of language models over time.'}, 'zh': {'title': '强化学习扩展推理能力的新方法', 'desc': '最近的研究表明，基于推理的语言模型在强化学习（RL）方面取得了进展，这被认为是一种有效的方法来使模型与可验证的奖励对齐。然而，关于RL是否真正增强了模型的推理能力，还是仅仅放大了基础模型分布中已经存在的高奖励输出，仍然存在争议。本文提出了一种新的训练方法ProRL，证明了经过长时间的RL训练可以发现基础模型无法访问的新推理策略。我们的实证分析显示，经过RL训练的模型在多种评估任务中表现优于基础模型，尤其是在基础模型完全失败的情况下。'}}}, {'id': 'https://huggingface.co/papers/2505.24867', 'title': "Time Blindness: Why Video-Language Models Can't See What Humans Can?", 'url': 'https://huggingface.co/papers/2505.24867', 'abstract': 'Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.', 'score': 53, 'issue_id': 4070, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6870a65f3ad54877', 'authors': ['Ujjwal Upadhyay', 'Mukul Ranjan', 'Zhiqiang Shen', 'Mohamed Elhoseiny'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)', 'Mohamed bin Zayed University of AI (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24867.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#survey', '#reasoning', '#multimodal', '#architecture', '#cv', '#games', '#dataset'], 'emoji': '⏳', 'ru': {'title': 'Временная слепота: раскрывая ограничения современных моделей компьютерного зрения', 'desc': 'Исследователи представили SpookyBench - новый бенчмарк для оценки способности моделей компьютерного зрения и обработки естественного языка (VLM) распознавать временные паттерны в видео. Эксперименты показали, что современные VLM не способны извлекать смысл из чисто временных последовательностей, в то время как люди справляются с этой задачей с точностью более 98%. Авторы отмечают, что эта проблема связана с чрезмерной зависимостью моделей от пространственных признаков в отдельных кадрах. SpookyBench призван стимулировать исследования в области распознавания временных паттернов и улучшить понимание видео машинными системами.'}, 'en': {'title': 'Bridging the Gap: Enhancing Temporal Understanding in Vision-Language Models', 'desc': 'This paper introduces SpookyBench, a new benchmark designed to test vision-language models (VLMs) on their ability to understand temporal patterns in videos when spatial information is not available. The study reveals that while humans can accurately identify shapes and patterns in noisy temporal sequences, current state-of-the-art VLMs fail to do so, achieving 0% accuracy. This highlights a significant limitation in VLMs, which tend to rely heavily on spatial features and struggle with temporal reasoning, especially in low spatial signal-to-noise ratio scenarios. The authors suggest that addressing this issue may require innovative model architectures or training methods that separate spatial and temporal processing, and they aim to stimulate further research in this area by releasing the SpookyBench dataset.'}, 'zh': {'title': '突破时序理解的瓶颈', 'desc': '最近，视觉语言模型（VLMs）在理解视频中的时空关系方面取得了显著进展。然而，当空间信息被遮蔽时，这些模型在捕捉纯粹的时间模式方面表现不佳。我们提出了SpookyBench，这是一个基准测试，信息仅通过噪声帧的时间序列编码，反映了从生物信号到隐蔽通信的自然现象。我们的研究表明，尽管人类在这些序列中识别形状、文本和模式的准确率超过98%，但最先进的VLMs的准确率却为0%，这突显了模型在时序理解上的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.24863', 'title': 'AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time', 'url': 'https://huggingface.co/papers/2505.24863', 'abstract': "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/", 'score': 53, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a30c2004fdd2d154', 'authors': ['Junyu Zhang', 'Runpei Dong', 'Han Wang', 'Xuying Ning', 'Haoran Geng', 'Peihao Li', 'Xialin He', 'Yutong Bai', 'Jitendra Malik', 'Saurabh Gupta', 'Huan Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24863.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'AlphaOne: Универсальная модуляция рассуждений в ИИ', 'desc': 'AlphaOne (alpha1) - это универсальная система для модуляции процесса рассуждений в крупных моделях рассуждений (LRM) во время тестирования. Она вводит понятие альфа-момента, представляющего масштабированную фазу мышления с универсальным параметром альфа. Система динамически планирует переходы между медленным и быстрым мышлением, моделируя вставку токенов перехода рассуждений как стохастический процесс Бернулли. AlphaOne превосходит существующие методы монотонного масштабирования, обеспечивая гибкую модуляцию рассуждений.'}, 'en': {'title': 'AlphaOne: Revolutionizing Reasoning in Large Models', 'desc': "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."}, 'zh': {'title': '灵活调节推理进程的AlphaOne框架', 'desc': '本文提出了AlphaOne（alpha1），这是一个在测试时调节大型推理模型（LRMs）推理进程的通用框架。alpha1首先引入了alpha时刻，表示带有通用参数alpha的缩放思维阶段。在这个缩放的前alpha时刻阶段中，它通过将推理过渡标记的插入建模为伯努利随机过程，动态调度缓慢思维的过渡。在alpha时刻之后，alpha1通过思维结束标记确定性地终止缓慢思维，从而促进快速推理和高效答案生成。'}}}, {'id': 'https://huggingface.co/papers/2505.24098', 'title': 'HardTests: Synthesizing High-Quality Test Cases for LLM Coding', 'url': 'https://huggingface.co/papers/2505.24098', 'abstract': 'Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.', 'score': 32, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '1a86a8aadff22dbb', 'authors': ['Zhongmou He', 'Yee Man Choi', 'Kexun Zhang', 'Jiabao Ji', 'Junting Zhou', 'Dejia Xu', 'Ivan Bercovich', 'Aidan Zhang', 'Lei Li'], 'affiliations': ['Carnegie Mellon University', 'UC Santa Barbara', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.24098.jpg', 'data': {'categories': ['#synthetic', '#reasoning', '#training', '#data', '#open_source', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Улучшение верификации кода с помощью синтетических тестов', 'desc': 'Статья представляет HARDTESTGEN - пайплайн для синтеза высококачественных тестов с использованием больших языковых моделей (LLM). На его основе создан набор данных HARDTESTS, содержащий 47 тысяч задач по соревновательному программированию с синтетическими тестами высокого качества. Тесты HARDTESTGEN показывают значительно более высокую точность и полноту при оценке кода, сгенерированного LLM, по сравнению с существующими тестами. HARDTESTS также оказывается более эффективным для обучения моделей, что измеряется производительностью генерации кода.'}, 'en': {'title': 'Enhancing LLM Evaluation with Synthetic Test Cases', 'desc': 'This paper introduces HARDTESTGEN, a new method for generating high-quality test cases for evaluating large language models (LLMs) in coding tasks. The challenge with existing verifiers is that they often fail to catch subtle errors in code, which can only be identified through complex human-written edge cases. HARDTESTGEN addresses this by synthesizing a dataset called HARDTESTS, which includes 47,000 programming problems along with high-quality tests generated by LLMs. The results show that tests from HARDTESTGEN significantly improve the precision and recall of evaluating LLM-generated code, making it a valuable tool for enhancing model training and performance.'}, 'zh': {'title': '高质量测试合成，提升LLM推理能力', 'desc': '本文提出了一种名为HARDTESTGEN的高质量测试合成管道，旨在解决大型语言模型（LLM）推理中的验证器问题。由于难以为复杂编码问题获取可靠的验证器，HARDTESTGEN能够生成高质量的测试用例，帮助评估LLM生成的代码。我们创建了一个包含47,000个问题的竞争编程数据集HARDTESTS，并且与现有测试相比，HARDTESTGEN的测试在精确度和召回率上都有显著提升。该数据集和合成管道将开源，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2505.14752', 'title': 'Large Language Models for Data Synthesis', 'url': 'https://huggingface.co/papers/2505.14752', 'abstract': 'LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.', 'score': 31, 'issue_id': 4067, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '202c77d3d43de6f6', 'authors': ['Yihong Tang', 'Menglin Kong', 'Lijun Sun'], 'affiliations': ['McGill University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14752.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data'], 'emoji': '🧬', 'ru': {'title': 'LLMSynthor: Превращение языковых моделей в точные генераторы синтетических данных', 'desc': 'LLMSynthor - это фреймворк для синтеза данных, который превращает большие языковые модели (LLM) в симуляторы, учитывающие структуру данных и использующие распределительную обратную связь. Он применяет LLM как непараметрический копула-симулятор для моделирования зависимостей высокого порядка и вводит LLM Proposal Sampling для создания обоснованных предлагаемых распределений. LLMSynthor итеративно минимизирует расхождения в пространстве сводных статистик, выравнивая реальные и синтетические данные. Фреймворк показывает высокую статистическую точность и практическую полезность на гетерогенных наборах данных в конфиденциальных областях.'}, 'en': {'title': 'Transforming LLMs into Efficient Data Synthesizers', 'desc': 'LLMSynthor is a framework that enhances Large Language Models (LLMs) for creating synthetic data that accurately reflects real-world statistical distributions. It addresses the limitations of traditional data synthesis methods, which often rely on rigid assumptions and struggle with complex data types. By using distributional feedback and a novel LLM Proposal Sampling technique, LLMSynthor improves the efficiency and accuracy of data generation without the need for rejection sampling. The framework has been tested in various real-world scenarios, demonstrating its ability to produce high-quality synthetic data suitable for diverse applications.'}, 'zh': {'title': 'LLMSynthor：高效的统计数据合成新工具', 'desc': 'LLMSynthor 是一种增强大型语言模型（LLM）以实现高效和统计准确的数据合成的方法。它通过分布反馈和提议采样，将 LLM 转变为结构感知的模拟器，能够更好地捕捉真实世界分布的统计特征。该框架通过最小化摘要统计空间中的差异，逐步对齐真实数据和合成数据，同时揭示和优化潜在的生成结构。LLMSynthor 在隐私敏感领域的异构数据集上进行了评估，显示出高统计保真度和实用性，适用于经济学、社会科学和城市研究等多个领域。'}}}, {'id': 'https://huggingface.co/papers/2505.18842', 'title': "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation", 'url': 'https://huggingface.co/papers/2505.18842', 'abstract': "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.", 'score': 28, 'issue_id': 4069, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'a97f1e174b838d2d', 'authors': ['Jiwan Chung', 'Junhyeok Kim', 'Siyeol Kim', 'Jaeyoung Lee', 'Min Soo Kim', 'Youngjae Yu'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18842.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#games', '#architecture', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Динамический визуальный доступ для улучшения мультимодальных рассуждений', 'desc': 'v1 - это расширение для мультимодальных больших языковых моделей (MLLM), которое позволяет избирательно обращаться к визуальным данным во время вывода. Оно вводит механизм указания и копирования, позволяющий модели динамически извлекать релевантные области изображения в процессе рассуждения. Для обучения этой возможности был создан набор данных v1g из 300 тысяч трасс мультимодальных рассуждений с аннотациями визуальной привязки. Эксперименты на трех эталонных тестах по мультимодальным математическим рассуждениям показали, что v1 стабильно улучшает производительность по сравнению с базовыми моделями.'}, 'en': {'title': 'Dynamic Visual Retrieval for Enhanced Multimodal Reasoning', 'desc': "The paper introduces v1, an enhancement to Multimodal Large Language Models (MLLMs) that allows for selective and dynamic retrieval of visual information during inference. Unlike traditional MLLMs that process visual inputs only once, v1 employs a point-and-copy mechanism to revisit relevant image regions as the model generates responses. This approach improves the model's ability to perform multimodal reasoning tasks by providing contextual access to visual data based on its ongoing hypotheses. The authors validate v1's effectiveness through experiments on multiple benchmarks, showing significant performance gains in tasks that require detailed visual references and complex reasoning steps."}, 'zh': {'title': '动态视觉访问提升多模态推理能力', 'desc': 'v1是对多模态大型语言模型（MLLMs）的轻量级扩展，能够在推理过程中实现选择性视觉区域的动态检索。与传统的MLLMs仅在内部记忆中进行推理不同，v1引入了一种简单的点对点复制机制，使模型能够在推理过程中动态获取相关的图像区域。通过构建包含30万条多模态推理轨迹的数据集v1g，模型得以训练这种能力。实验结果表明，v1在多个多模态数学推理基准上表现优异，尤其是在需要细致视觉参考和多步骤推理的任务中。'}}}, {'id': 'https://huggingface.co/papers/2505.24862', 'title': 'ViStoryBench: Comprehensive Benchmark Suite for Story Visualization', 'url': 'https://huggingface.co/papers/2505.24862', 'abstract': "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.", 'score': 21, 'issue_id': 4072, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '40afeafebffbd255', 'authors': ['Cailin Zhuang', 'Ailin Huang', 'Wei Cheng', 'Jingwei Wu', 'Yaoqi Hu', 'Jiaqi Liao', 'Zhewei Huang', 'Hongyuan Wang', 'Xinyao Liao', 'Weiwei Cai', 'Hengyuan Xu', 'Xuanyang Zhang', 'Xianfang Zeng', 'Gang Yu', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'AIGC Research', 'ShanghaiTech University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2505.24862.jpg', 'data': {'categories': ['#dataset', '#cv', '#story_generation', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'ViStoryBench: комплексная оценка визуализации историй', 'desc': 'Статья представляет новый набор данных и систему оценки для задачи визуализации историй - ViStoryBench. Этот бенчмарк включает разнообразные типы сюжетов и художественных стилей, позволяя оценивать модели по различным аспектам генерации изображений. ViStoryBench содержит истории с одним и несколькими персонажами, сложными сюжетами и детальными мирами. Система включает набор метрик для всесторонней оценки качества визуализации, что позволяет выявлять сильные и слабые стороны различных моделей машинного обучения.'}, 'en': {'title': 'Enhancing Story Visualization with ViStoryBench', 'desc': 'This paper introduces ViStoryBench, a new evaluation benchmark designed to improve story visualization models that generate images based on narratives. It features a diverse dataset that includes various story types and artistic styles, allowing for a comprehensive assessment of model performance across different plots and visual aesthetics. The benchmark tests models on their ability to maintain character consistency and handle complex narratives with multiple protagonists. By providing a structured framework and a variety of evaluation metrics, ViStoryBench helps researchers identify strengths and weaknesses in their models, promoting targeted enhancements in story visualization.'}, 'zh': {'title': '提升故事可视化的评估基准', 'desc': '故事可视化旨在生成与给定叙述和参考图像一致的视觉图像序列。为了提升故事可视化框架在实际场景中的表现，我们引入了一个全面的评估基准，称为ViStoryBench。该基准收集了多样化的数据集，涵盖不同类型的故事和艺术风格，确保模型在不同情节和视觉美学上进行评估。ViStoryBench经过精心策划，平衡了叙事结构和视觉元素，帮助研究人员识别不同模型的优缺点，促进有针对性的改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24878', 'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents', 'url': 'https://huggingface.co/papers/2505.24878', 'abstract': 'CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.', 'score': 15, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '9cceceaf09c77468', 'authors': ['Yaxin Luo', 'Zhaoyi Li', 'Jiacheng Liu', 'Jiacheng Cui', 'Xiaohan Zhao', 'Zhiqiang Shen'], 'affiliations': ['MetaAgentX', 'VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24878.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Open CaptchaWorld: вызов для мультимодальных ИИ-агентов', 'desc': 'Статья представляет Open CaptchaWorld - первый веб-бенчмарк для оценки возможностей мультимодальных языковых моделей (MLLM) в решении CAPTCHA. Бенчмарк включает 20 типов современных CAPTCHA, всего 225 задач, с новой метрикой - глубиной рассуждения CAPTCHA. Эксперименты показали, что люди достигают почти идеальных результатов, в то время как лучшие MLLM-агенты справляются максимум с 40% задач. Это подчеркивает важность Open CaptchaWorld для диагностики ограничений современных мультимодальных агентов и разработки более надежных систем рассуждений.'}, 'en': {'title': 'Unlocking the Future: Evaluating MLLM Agents with Open CaptchaWorld', 'desc': 'This paper introduces Open CaptchaWorld, a new benchmark designed to test the capabilities of multimodal large language model (MLLM) agents in solving CAPTCHA puzzles. It evaluates the visual reasoning and interaction skills of these agents through a variety of 225 CAPTCHA types, measuring their performance with a novel metric called CAPTCHA Reasoning Depth. Experimental results reveal that while humans achieve high success rates, MLLM agents struggle significantly, with a maximum success rate of only 40%. This underscores the need for improved multimodal reasoning systems and positions Open CaptchaWorld as a crucial tool for assessing and enhancing agent performance in complex tasks.'}, 'zh': {'title': '突破CAPTCHA瓶颈，提升多模态推理能力！', 'desc': 'CAPTCHA在实际应用中是部署网络代理的一个重要瓶颈，常常阻碍它们完成端到端的自动化任务。虽然现代多模态大语言模型（MLLM）在静态感知任务中表现出色，但它们在处理交互式、多步骤推理挑战（如CAPTCHA）方面的能力尚未得到充分测试。为了解决这个问题，我们推出了Open CaptchaWorld，这是第一个专门设计用于评估MLLM代理的视觉推理和交互能力的网络基准平台，涵盖20种现代CAPTCHA类型，共225个CAPTCHA，并引入了一种新的度量标准：CAPTCHA推理深度。实验结果表明，人类的成功率接近完美，而最先进的MLLM代理的成功率最高仅为40.0%，远低于人类的93.3%，这突显了Open CaptchaWorld作为诊断当前多模态代理局限性的重要基准。'}}}, {'id': 'https://huggingface.co/papers/2505.23941', 'title': 'Vision Language Models are Biased', 'url': 'https://huggingface.co/papers/2505.23941', 'abstract': 'Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.', 'score': 15, 'issue_id': 4069, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '1c96442d8acb3ec5', 'authors': ['An Vo', 'Khai-Nguyen Nguyen', 'Mohammad Reza Taesiri', 'Vy Tuong Dang', 'Anh Totti Nguyen', 'Daeyoung Kim'], 'affiliations': ['Auburn University', 'College of William and Mary', 'KAIST', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.23941.jpg', 'data': {'categories': ['#multimodal', '#cv', '#hallucinations', '#ethics', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Предвзятость визуально-языковых моделей: когда знания мешают точности', 'desc': 'Это исследование показывает, что крупные языковые модели (LLM) и визуально-языковые модели (VLM) могут быть сильно предвзяты из-за предварительных знаний, полученных из интернета. Авторы тестируют VLM на задачах подсчета и идентификации объектов, обнаруживая низкую точность (в среднем 17,05%) в различных доменах. Интересно, что добавление текстовой информации о предмете еще больше снижает точность моделей. Инструктирование моделей перепроверять свои результаты или полагаться только на детали изображения лишь незначительно улучшает точность подсчета.'}, 'en': {'title': 'Unveiling Biases in Vision Language Models', 'desc': 'This paper investigates how large language models (LLMs) influence the performance of vision language models (VLMs) on visual tasks like counting and identification. The authors find that VLMs exhibit significant biases, leading to poor accuracy when recognizing visual elements, such as miscounting stripes on logos. Even when provided with counterfactual information, such as the name of the subject, the accuracy of VLMs decreases further. The study highlights a critical failure mode in VLMs and introduces a framework for assessing these biases systematically.'}, 'zh': {'title': '视觉语言模型的偏见问题', 'desc': '大型语言模型（LLMs）从互联网中记忆了大量知识，这对下游任务有帮助，但也可能导致输出偏向错误或有偏见的答案。我们研究了流行主题的知识如何影响视觉语言模型（VLMs）在标准视觉任务（如计数和识别）上的准确性。结果显示，最先进的VLMs在计数任务中的平均准确率仅为17.05%，并且在识别图案时存在明显的偏见。我们的研究揭示了VLMs中的一种有趣的失败模式，并提供了一个自动化框架来测试VLM的偏见。'}}}, {'id': 'https://huggingface.co/papers/2505.24025', 'title': 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models', 'url': 'https://huggingface.co/papers/2505.24025', 'abstract': 'DINO-R1 incorporates reinforcement learning to enhance visual in-context reasoning capabilities in vision foundation models, achieving better performance than supervised fine-tuning across various visual prompting scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose DINO-R1, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces Group Relative Query Optimization (GRQO), a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.', 'score': 13, 'issue_id': 4081, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '88443982cc458e0e', 'authors': ['Chenbin Pan', 'Wenbin He', 'Zhengzhong Tu', 'Liu Ren'], 'affiliations': ['Bosch Center for Artificial Intelligence (BCAI)', 'Bosch Research North America', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24025.jpg', 'data': {'categories': ['#rl', '#reasoning', '#cv', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'DINO-R1: Революция в визуальном рассуждении с помощью обучения с подкреплением', 'desc': 'DINO-R1 - это новый подход к обучению моделей компьютерного зрения с использованием обучения с подкреплением. Он применяет метод Group Relative Query Optimization (GRQO) для улучшения способностей визуального рассуждения в контексте. DINO-R1 превосходит базовые модели с обучением с учителем в различных сценариях визуальных подсказок. Эксперименты на наборах данных COCO, LVIS и ODinW демонстрируют сильную обобщающую способность модели.'}, 'en': {'title': 'Reinforcement Learning Boosts Visual Reasoning in DINO-R1', 'desc': 'DINO-R1 is a novel approach that uses reinforcement learning to improve the visual reasoning abilities of vision foundation models. It introduces a new training strategy called Group Relative Query Optimization (GRQO), which focuses on enhancing query-level performance by providing rewards based on alignment quality. Additionally, KL-regularization is applied to stabilize the training process and prevent issues like overfitting. The results show that DINO-R1 outperforms traditional supervised fine-tuning methods, demonstrating its effectiveness in various visual prompting tasks.'}, 'zh': {'title': 'DINO-R1：强化学习提升视觉推理能力的创新尝试', 'desc': 'DINO-R1 是一种结合强化学习的视觉基础模型，旨在增强视觉上下文推理能力。它引入了一种新的训练策略，称为群体相对查询优化（GRQO），通过计算基于查询的奖励来提高模型的表现。该模型在 COCO、LVIS 和 ODinW 数据集上的实验结果显示，DINO-R1 在多种视觉提示场景中显著优于传统的监督微调方法。通过这种方法，DINO-R1 能够实现更强的泛化能力，适应开放词汇和封闭集合的视觉提示任务。'}}}, {'id': 'https://huggingface.co/papers/2505.21437', 'title': 'CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects', 'url': 'https://huggingface.co/papers/2505.21437', 'abstract': 'Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.', 'score': 13, 'issue_id': 4073, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3b71d1aa0666ba54', 'authors': ['Huaijin Pi', 'Zhi Cen', 'Zhiyang Dou', 'Taku Komura'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21437.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'Синтез реалистичных движений всего тела для манипуляции сложными объектами', 'desc': 'Статья представляет новый метод синтеза целостных движений тела при манипуляции сочлененными объектами, включая движения тела, рук и объекта. Авторы предлагают координированную оптимизацию шума диффузии с использованием трех специализированных диффузионных моделей для тела и рук. Для повышения точности взаимодействия руки с объектом применяется унифицированное представление на основе набора базисных точек (BPS). Эксперименты показывают, что метод превосходит существующие подходы по качеству движения и физической достоверности.'}, 'en': {'title': 'Coordinated Motion Synthesis for Realistic Manipulation', 'desc': 'This paper addresses the complex task of synthesizing whole-body movements for manipulating articulated objects in robotics and virtual humans. The authors introduce a coordinated diffusion noise optimization framework that enhances the synchronization between hand and body motions, which is crucial for realistic manipulation. By utilizing specialized diffusion models for different body parts and a unified representation of hand-object interactions, the method improves precision and generalization in motion generation. Experimental results show that this approach surpasses existing methods in terms of motion quality and physical realism, enabling advanced capabilities like simultaneous walking and manipulation.'}, 'zh': {'title': '全身操控的协调优化新方法', 'desc': '本文提出了一种新颖的协调扩散噪声优化框架，用于合成全身操控关节物体的运动，包括身体、手和物体的运动。该方法通过对三个专门的扩散模型进行噪声空间优化，分别针对身体、左手和右手进行训练，以提高模型的泛化能力。通过沿着人体运动链的梯度流动，协调性自然地出现，使得全身姿态能够高保真地响应手部运动目标。实验结果表明，该方法在运动质量和物理合理性方面优于现有方法，并能够实现物体姿态控制、同时行走和操控等多种能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24196', 'title': 'CLaSp: In-Context Layer Skip for Self-Speculative Decoding', 'url': 'https://huggingface.co/papers/2505.24196', 'abstract': 'Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.', 'score': 12, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '004f8eaa8c2fe087', 'authors': ['Longze Chen', 'Renke Shan', 'Huiming Wang', 'Lu Wang', 'Ziqiang Liu', 'Run Luo', 'Jiawei Wang', 'Hamid Alinejad-Rokny', 'Min Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24196.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение LLM без компромиссов: CLaSp - новый метод спекулятивного декодирования', 'desc': 'В статье представлен метод CLaSp для ускорения декодирования больших языковых моделей (LLM) с помощью спекулятивного декодирования. CLaSp использует стратегию пропуска слоев в контексте для самоспекулятивного декодирования, не требуя дополнительных модулей или обучения. Метод применяет алгоритм динамического программирования для оптимизации процесса пропуска слоев, используя скрытые состояния последней стадии верификации. Эксперименты показывают, что CLaSp достигает ускорения в 1.3-1.7 раза на моделях серии LLaMA3 без изменения исходного распределения генерируемого текста.'}, 'en': {'title': 'Accelerating LLM Decoding with Layer-Skipping Efficiency', 'desc': 'This paper introduces CLaSp, a novel approach to speculative decoding that enhances the efficiency of Large Language Models (LLMs) without the need for additional training modules. CLaSp utilizes an in-context layer-skipping strategy, allowing it to create a compressed draft model by skipping certain layers in the verify model. The method employs a dynamic programming algorithm to optimize the layer-skipping process, adapting after each verification stage based on the hidden states. Experimental results show that CLaSp can speed up the decoding process by 1.3x to 1.7x on LLaMA3 models while maintaining the quality of the generated text.'}, 'zh': {'title': 'CLaSp：加速解码的新策略', 'desc': '本文提出了一种名为CLaSp的自我推测解码策略，旨在加速大型语言模型的解码过程。CLaSp通过跳过验证模型的中间层，构建一个压缩的草稿模型，从而避免了额外模块的训练需求。该方法利用动态规划算法优化层跳过过程，使其能够在每个验证阶段后动态调整策略。实验结果表明，CLaSp在LLaMA3系列模型上实现了1.3倍到1.7倍的加速，同时保持生成文本的原始分布不变。'}}}, {'id': 'https://huggingface.co/papers/2505.23009', 'title': 'EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge', 'url': 'https://huggingface.co/papers/2505.23009', 'abstract': "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on EmergentTTS, we introduce EmergentTTS-Eval, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation https://github.com/boson-ai/EmergentTTS-Eval-public{code} and the https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.", 'score': 12, 'issue_id': 4067, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8ed75b2649e36558', 'authors': ['Ruskin Raj Manku', 'Yuzhi Tang', 'Xingjian Shi', 'Mu Li', 'Alex Smola'], 'affiliations': ['Boson AI, Santa Clara, CA 95054'], 'pdf_title_img': 'assets/pdf/title_img/2505.23009.jpg', 'data': {'categories': ['#games', '#benchmark', '#open_source', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Автоматизированная оценка сложных аспектов синтеза речи с помощью ИИ', 'desc': "EmergentTTS-Eval - это новый комплексный бенчмарк для оценки систем Text-to-Speech (TTS). Он использует языковые модели (LLM) и аудио-языковые модели (LALM) для автоматической генерации тестовых случаев и оценки качества синтезированной речи. Бенчмарк охватывает шесть сложных сценариев, включая эмоции, паралингвистику, иностранные слова и сложное произношение. Результаты показывают, что подход 'модель-как-судья' обеспечивает надежную оценку TTS систем и высокую корреляцию с предпочтениями людей."}, 'en': {'title': 'Automating TTS Evaluation for Nuanced Speech Outputs', 'desc': 'The paper presents EmergentTTS-Eval, a new benchmark for evaluating Text-to-Speech (TTS) systems that focuses on complex and nuanced text. It automates the generation of test cases using Large Language Models (LLMs) and evaluates the outputs with a Large Audio Language Model (LALM). The benchmark includes six challenging scenarios, such as emotional expression and complex pronunciation, and generates 1,645 diverse test cases from a small set of human-written prompts. The results show that this automated approach provides a reliable assessment of TTS systems, correlating well with human evaluations.'}, 'zh': {'title': '全面评估文本到语音系统的EmergentTTS-Eval', 'desc': '本文介绍了一个全面的文本到语音（TTS）基准测试工具EmergentTTS-Eval，旨在自动生成和评估测试案例，以评估模型在处理复杂语义文本时的表现。该基准涵盖六种具有挑战性的TTS场景，包括情感、旁语言、外语、句法复杂性、复杂发音（如网址、公式）和问题。通过使用大型语言模型（LLM）迭代扩展人类编写的种子提示，生成了1645个多样化的测试案例。我们还采用了模型作为评判者的方法，利用大型音频语言模型（LALM）从多个维度评估语音输出，结果显示该方法能够有效揭示不同TTS系统之间的细微性能差异。'}}}, {'id': 'https://huggingface.co/papers/2505.24858', 'title': 'MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs', 'url': 'https://huggingface.co/papers/2505.24858', 'abstract': "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.", 'score': 10, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '5cc52e721279a4e1', 'authors': ['Gabrielle Kaili-May Liu', 'Gal Yona', 'Avi Caciularu', 'Idan Szpektor', 'Tim G. J. Rudner', 'Arman Cohan'], 'affiliations': ['Google Research', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24858.jpg', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#interpretability', '#hallucinations', '#dataset'], 'emoji': '🎯', 'ru': {'title': 'Научить ИИ честно выражать неуверенность', 'desc': 'Исследование посвящено проблеме надежной коммуникации неопределенности в больших языковых моделях (LLM). Авторы провели систематический анализ способности моделей выражать неуверенность, соответствующую их внутренней неопределенности. Результаты показали, что существующие LLM и методы в основном не справляются с этой задачей. Предложен новый метод калибровки MetaFaith, вдохновленный человеческой метакогнитивностью, который значительно улучшает верную калибровку моделей.'}, 'en': {'title': 'Enhancing Trust in LLMs through Better Uncertainty Communication', 'desc': 'This paper addresses the issue of how large language models (LLMs) communicate uncertainty, which is crucial for building trust in their outputs. The authors conduct a systematic study to evaluate how well LLMs express their confidence levels in a way that matches their actual uncertainty. They find that current methods for improving this communication are largely ineffective, and some can even worsen the situation. To solve this problem, they propose a new method called MetaFaith, which significantly enhances the ability of LLMs to convey uncertainty accurately, leading to better trustworthiness in their responses.'}, 'zh': {'title': '提升大型语言模型的不确定性表达信任度', 'desc': '本研究探讨了大型语言模型（LLMs）在不确定性传达方面的可靠性，指出它们在表达错误信息时常使用过于自信的语言，从而导致用户过度依赖并削弱信任。我们系统地评估了LLMs在使用不确定性语言表达其内在不确定性方面的能力，结果显示大多数模型在这方面表现不佳。现有的干预措施效果有限，标准提示方法仅带来微小改进，而基于事实的校准技术甚至可能对忠实校准产生负面影响。为了解决这一问题，我们提出了MetaFaith，这是一种基于提示的新型校准方法，能够显著提高不同模型和任务领域的忠实校准效果。'}}}, {'id': 'https://huggingface.co/papers/2505.24521', 'title': 'UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation', 'url': 'https://huggingface.co/papers/2505.24521', 'abstract': 'Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.', 'score': 10, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4ae43b7cdb482867', 'authors': ['Yang-Tian Sun', 'Xin Yu', 'Zehuan Huang', 'Yi-Hua Huang', 'Yuan-Chen Guo', 'Ziyi Yang', 'Yan-Pei Cao', 'Xiaojuan Qi'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2505.24521.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Согласованная геометрическая оценка видео с помощью диффузионных моделей', 'desc': 'Статья представляет новый подход к оценке геометрических свойств в видео с использованием диффузионных моделей. Авторы предлагают метод, который позволяет использовать внутреннюю согласованность моделей генерации видео для последовательной геометрической оценки. Они вводят новый метод кондиционирования, переиспользуя позиционные кодировки, и улучшают производительность путем совместного обучения на нескольких геометрических атрибутах. Результаты показывают превосходную производительность в предсказании глобальных геометрических атрибутов в видео и могут быть применены к задачам реконструкции.'}, 'en': {'title': 'Harnessing Diffusion Models for Consistent Geometric Estimation in Videos', 'desc': "This paper explores the use of diffusion models to improve the estimation of geometric properties like depth and normals in videos. Unlike previous methods that focus on individual frames, this approach leverages the relationships between frames to enhance consistency in geometric estimation. The authors propose a novel conditioning method that reuses positional encodings and advocate for joint training on multiple geometric attributes. Their results show improved performance in predicting global geometric attributes, demonstrating the model's ability to generalize even from static video data to dynamic scenes."}, 'zh': {'title': '利用扩散模型提升视频几何估计的一致性', 'desc': '最近，利用扩散模型先验来辅助单目几何估计（如深度和法线）的方法受到了广泛关注，因为它们具有很强的泛化能力。然而，大多数现有工作集中在单个视频帧的相机坐标系内估计几何属性，忽视了扩散模型在确定帧间对应关系方面的固有能力。在本研究中，我们展示了通过适当的设计和微调，可以有效利用视频生成模型的内在一致性来进行一致的几何估计。具体而言，我们选择在全局坐标系中与视频帧共享相同对应关系的几何属性作为预测目标，并引入了一种新颖高效的条件方法，通过重用位置编码来增强性能。'}}}, {'id': 'https://huggingface.co/papers/2505.20873', 'title': 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.20873', 'abstract': 'The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.', 'score': 9, 'issue_id': 4072, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '12352ddbed62a761', 'authors': ['Chaeyoung Jung', 'Youngjoon Jang', 'Jongmin Choi', 'Joon Son Chung'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20873.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🍴', 'ru': {'title': 'Разделяй и властвуй: новый подход к мультимодальному анализу', 'desc': 'Статья представляет стратегию Fork-Merge Decoding (FMD) для улучшения сбалансированного мультимодального понимания в аудио-визуальных больших языковых моделях (AV-LLM). FMD разделяет обработку аудио и видео на ранних слоях декодера, а затем объединяет их для совместного анализа. Этот метод не требует дополнительного обучения или изменения архитектуры модели. Эксперименты показали, что FMD улучшает производительность моделей в задачах, связанных с аудио, видео и комбинированным аудио-визуальным анализом.'}, 'en': {'title': 'Fork-Merge Decoding: Balancing Audio-Visual Insights for Better Understanding', 'desc': 'This paper introduces the Fork-Merge Decoding (FMD) strategy to enhance multimodal understanding in audio-visual large language models (AV-LLMs). The method addresses the issue of modality bias, which occurs when models overly depend on one type of input, like audio or video. FMD operates by first analyzing audio and video inputs separately in the early decoder layers (fork phase) and then combining the insights in later layers (merge phase). This approach allows for a more balanced contribution from both modalities, improving performance on various tasks without needing extra training or changes to the model architecture.'}, 'zh': {'title': '叉合解码：提升多模态理解的有效策略', 'desc': '本文提出了一种名为叉合解码（Fork-Merge Decoding, FMD）的策略，旨在改善音视频大型语言模型（AV-LLMs）的平衡多模态理解。该方法通过在推理阶段分开处理音频和视频输入，先进行模态特定的推理，然后再合并结果，避免了模态偏差的问题。FMD不需要额外的训练或架构修改，简单有效。实验结果表明，该策略在音频、视频及音视频结合推理任务上均表现出一致的性能提升，证明了推理时干预对增强多模态理解的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.21523', 'title': 'More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models', 'url': 'https://huggingface.co/papers/2505.21523', 'abstract': "A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.", 'score': 9, 'issue_id': 4074, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '26f1abb0ea843b21', 'authors': ['Chengzhi Liu', 'Zhongxing Xu', 'Qingyue Wei', 'Juncheng Wu', 'James Zou', 'Xin Eric Wang', 'Yuyin Zhou', 'Sheng Liu'], 'affiliations': ['Stanford University', 'UC Santa Barbara', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.21523.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Балансируя между рассуждениями и восприятием в мультимодальных ИИ-моделях', 'desc': 'Представлена новая метрика RH-AUC и тестовый набор RH-Bench для оценки способности мультимодальных больших языковых моделей сохранять визуальную привязку при выполнении расширенных рассуждений. Анализ внимания показывает, что более длинные цепочки рассуждений приводят к уменьшению фокуса на визуальных входных данных, что способствует галлюцинациям. Исследование выявило, что более крупные модели обычно достигают лучшего баланса между рассуждениями и восприятием. Также было обнаружено, что на этот баланс больше влияют типы и домены обучающих данных, чем их общий объем.'}, 'en': {'title': 'Balancing Reasoning and Visual Grounding in Multimodal Models', 'desc': 'This paper introduces a new metric called RH-AUC to evaluate how well multimodal large language models maintain visual grounding while performing extended reasoning tasks. It highlights that as reasoning chains become longer, models often lose focus on visual inputs, leading to increased hallucination. The study also presents RH-Bench, a benchmark for assessing the trade-off between reasoning ability and hallucination across various multimodal tasks. Findings indicate that larger models and specific types of training data enhance the balance between reasoning and perception, emphasizing the need for evaluation methods that consider both aspects together.'}, 'zh': {'title': '提升推理与视觉感知的平衡', 'desc': '本文介绍了一种新的评估指标和基准，用于评估多模态大型语言模型在进行扩展推理时保持视觉基础的能力。研究发现，较大的模型和特定类型的训练数据可以改善推理与视觉感知之间的平衡。通过引入RH-AUC指标，能够量化模型在推理长度变化时的感知准确性，从而评估模型在推理过程中是否保持视觉基础。我们的分析表明，模型的大小和训练数据的类型对推理能力和幻觉之间的权衡有显著影响。'}}}, {'id': 'https://huggingface.co/papers/2505.24850', 'title': 'Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24850', 'abstract': "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.", 'score': 8, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e946031c286b5bf4', 'authors': ['Shuyao Xu', 'Cheng Peng', 'Jiangxuan Long', 'Weidi Xu', 'Wei Chu', 'Yuan Qi'], 'affiliations': ['AI3 Institute of Fudan University', 'INFLY TECH (Shanghai) Co., Ltd.', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.24850.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#dataset', '#math'], 'emoji': '🧠', 'ru': {'title': 'REDI: Эффективное обучение рассуждениям на положительных и отрицательных примерах', 'desc': 'Статья представляет новый метод дистилляции моделей машинного обучения под названием Reinforcement Distillation (REDI). REDI использует как положительные, так и отрицательные примеры рассуждений для улучшения способностей языковых моделей к логическому мышлению. Метод состоит из двух этапов: обучение на положительных примерах и дальнейшая оптимизация с использованием специальной целевой функции REDI. Эксперименты показывают превосходство REDI над базовыми методами на задачах математических рассуждений, особенно для моделей среднего размера.'}, 'en': {'title': 'Maximizing Reasoning Performance with Reinforcement Distillation', 'desc': 'This paper introduces Reinforcement Distillation (REDI), a two-stage framework designed to enhance the reasoning capabilities of smaller models by utilizing both positive and negative reasoning examples. In the first stage, the model is fine-tuned using positive reasoning traces through Supervised Fine-Tuning (SFT). The second stage refines the model further by incorporating both types of traces with a novel, reference-free loss function that improves performance over traditional methods like DPO and SimPO. Empirical results show that the Qwen-REDI-1.5B model achieves impressive scores on mathematical reasoning tasks, outperforming larger models trained on more extensive proprietary datasets.'}, 'zh': {'title': '强化蒸馏：提升推理性能的新方法', 'desc': '本论文探讨了如何有效利用正负推理轨迹来提升大型语言模型（LLM）的推理性能。我们提出了一种名为强化蒸馏（REDI）的两阶段框架，第一阶段通过监督微调（SFT）学习正推理轨迹，第二阶段则结合正负推理轨迹进一步优化模型。我们的REDI目标是一个简单的无参考损失函数，在蒸馏任务中优于传统方法如DPO和SimPO。实验结果表明，经过131k正负样本训练的Qwen-REDI-1.5B模型在数学推理任务上达到了83.1%的得分，创造了1.5B模型的新状态。'}}}, {'id': 'https://huggingface.co/papers/2505.24417', 'title': 'EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering', 'url': 'https://huggingface.co/papers/2505.24417', 'abstract': 'Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.', 'score': 8, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f28c426fafe8156a', 'authors': ['Runnan Lu', 'Yuxuan Zhang', 'Jailing Liu', 'Haifa Wang', 'Yiren Song'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'The Chinese University of Hong Kong', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24417.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#multilingual', '#cv', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'EasyText: прорыв в многоязычном рендеринге текста с помощью диффузионных моделей', 'desc': 'Статья представляет EasyText - фреймворк для рендеринга многоязычного текста, основанный на модели диффузии DiT. Авторы предлагают методы кодирования позиций символов и интерполяции позиционного кодирования для точного рендеринга текста. Для обучения модели был создан большой синтетический датасет с 1 миллионом аннотаций изображений с текстом на разных языках. Эксперименты показывают эффективность подхода в многоязычном рендеринге текста, визуальном качестве и интеграции текста с учетом макета.'}, 'en': {'title': 'EasyText: Multilingual Text Rendering Made Simple', 'desc': "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."}, 'zh': {'title': '多语言文本渲染的新突破', 'desc': '本论文介绍了一种名为EasyText的文本渲染框架，基于扩散变换器（DiT）技术。该框架通过将去噪潜变量与多语言字符令牌连接，实现了对多语言文本的精确渲染。我们提出了字符位置编码和位置编码插值技术，以实现可控和精确的文本渲染。此外，我们构建了一个包含100万条多语言图像-文本注释的大规模合成文本图像数据集，用于预训练和微调，实验结果表明我们的方法在多语言文本渲染和视觉质量方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2505.24871', 'title': 'MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.24871', 'abstract': "A framework for post-training multimodal large language models using reinforcement learning with verifiable rewards introduces a data mixture strategy to enhance general reasoning abilities and benchmark performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.", 'score': 7, 'issue_id': 4081, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a9ad14edf2493ec8', 'authors': ['Yiqing Liang', 'Jielin Qiu', 'Wenhao Ding', 'Zuxin Liu', 'James Tompkin', 'Mengdi Xu', 'Mengzhou Xia', 'Zhengzhong Tu', 'Laixi Shi', 'Jiacheng Zhu'], 'affiliations': ['Brown University', 'California Institute of Technology', 'Carnegie Mellon University', 'MIT CSAIL', 'NVIDIA Research', 'Princeton University', 'Salesforce AI Research', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24871.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#optimization', '#dataset', '#training', '#rag', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодальных ИИ через смешивание данных и обучение с подкреплением', 'desc': 'Статья представляет фреймворк для пост-обучения мультимодальных больших языковых моделей (MLLM) с использованием обучения с подкреплением с проверяемыми наградами (RLVR). Авторы разработали стратегию смешивания данных для улучшения способностей к обобщению и рассуждению у MLLM. Фреймворк включает в себя формулировку проблемы смешивания данных и реализацию бенчмарка для мультидоменного онлайн-обучения с подкреплением. Эксперименты показывают, что предложенный подход значительно повышает точность модели на тестах вне обучающей выборки.'}, 'en': {'title': 'Boosting Multimodal Models with Smart Data Mixing', 'desc': 'This paper presents a new framework for enhancing multimodal large language models (MLLMs) using Reinforcement Learning with Verifiable Rewards (RLVR). The authors address the challenges of training MLLMs on diverse datasets that require complex reasoning across visual and textual information. They propose a systematic approach that includes a data mixture strategy to optimize the training process and improve generalization. Experimental results demonstrate that their method significantly increases the accuracy of MLLMs on various benchmarks, outperforming traditional uniform data mixtures.'}, 'zh': {'title': '优化数据混合，提升多模态推理能力', 'desc': '本文提出了一种后训练多模态大语言模型的框架，利用可验证奖励的强化学习来增强推理能力和基准性能。通过引入数据混合策略，解决了多数据集训练中目标冲突的问题，从而提高了模型的泛化能力。研究表明，结合多领域的强化学习训练和混合预测策略，可以显著提升多模态大语言模型的推理能力。实验结果显示，最佳数据混合策略使得后训练模型在分布外基准上的准确率平均提高了5.24%。'}}}, {'id': 'https://huggingface.co/papers/2505.24785', 'title': 'EXP-Bench: Can AI Conduct AI Research Experiments?', 'url': 'https://huggingface.co/papers/2505.24785', 'abstract': "EXP-Bench evaluates AI agents' end-to-end research experiment capabilities through curated tasks from top AI papers, highlighting current limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.", 'score': 7, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6aafc5c29245622f', 'authors': ['Patrick Tser Jern Kon', 'Jiachen Liu', 'Xinyi Zhu', 'Qiuyi Ding', 'Jingjia Peng', 'Jiarong Xing', 'Yibo Huang', 'Yiming Qiu', 'Jayanth Srinivasa', 'Myungjin Lee', 'Mosharaf Chowdhury', 'Matei Zaharia', 'Ang Chen'], 'affiliations': ['Cisco Research', 'Rice University', 'UC Berkeley', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.24785.jpg', 'data': {'categories': ['#agents', '#benchmark', '#science', '#open_source'], 'emoji': '🧪', 'ru': {'title': 'EXP-Bench: Оценка исследовательских способностей ИИ в реальных экспериментах', 'desc': 'EXP-Bench - это новый бенчмарк для оценки способностей ИИ-агентов проводить полноценные исследовательские эксперименты в области искусственного интеллекта. Он содержит 461 задачу, извлеченную из 51 ведущей научной статьи по ИИ, и требует от агентов формулировать гипотезы, разрабатывать и выполнять экспериментальные процедуры, а также анализировать результаты. Оценка современных агентов на основе больших языковых моделей показала, что они способны частично справляться с отдельными аспектами экспериментов, но полностью выполнить эксперимент удается крайне редко. EXP-Bench выявляет текущие ограничения ИИ-агентов и предоставляет реалистичные пошаговые процедуры для улучшения их исследовательских возможностей.'}, 'en': {'title': 'EXP-Bench: Elevating AI Agents in Research Experimentation', 'desc': 'EXP-Bench is a benchmark designed to evaluate the capabilities of AI agents in conducting end-to-end research experiments. It presents AI agents with tasks derived from top AI papers, requiring them to formulate hypotheses, design experiments, implement procedures, and analyze results. The benchmark highlights the current limitations of AI agents, as they often struggle to execute complete experiments successfully. By providing a structured approach to experimental tasks, EXP-Bench aims to enhance the ability of future AI agents in performing rigorous scientific research.'}, 'zh': {'title': 'EXP-Bench：提升AI研究实验能力的基准测试', 'desc': 'EXP-Bench是一个新颖的基准测试，旨在系统地评估人工智能代理在完整研究实验中的能力。它通过从顶级AI论文中提取和结构化实验细节，创建了461个AI研究任务。尽管一些基于大型语言模型的代理在实验设计和实现的某些方面得分达到20-35%，但完整可执行实验的成功率仅为0.5%。通过识别这些瓶颈，EXP-Bench为未来的AI代理提供了改进其进行AI研究实验能力的重要工具。'}}}, {'id': 'https://huggingface.co/papers/2505.24293', 'title': 'Large Language Models are Locally Linear Mappings', 'url': 'https://huggingface.co/papers/2505.24293', 'abstract': 'We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.', 'score': 7, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '42a9e20ff9742560', 'authors': ['James R. Golden'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24293.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#inference', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Линейное представление нелинейных языковых моделей', 'desc': 'Исследователи показали, что операции вывода нескольких открытых большим языковых моделей (LLM) можно отобразить в эквивалентную линейную систему для входной последовательности без изменения весов модели или предсказаний. Они расширили методы из моделей диффузии изображений, проявляющих локальную или кусочную линейность, стратегически изменив вычисление градиента для предсказания следующего токена. Этот подход был продемонстрирован на различных моделях, включая Llama 3, Gemma 3 и другие. Анализ сингулярного разложения отсоединенного якобиана показал, что эти LLM работают в экстремально низкоразмерных подпространствах, где многие из крупнейших сингулярных векторов декодируются в концепции, связанные с наиболее вероятным выходным токеном.'}, 'en': {'title': 'Unlocking LLMs: Linear Insights into Complex Predictions', 'desc': "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."}, 'zh': {'title': '揭示大型语言模型的线性本质', 'desc': '本文展示了多个开放权重的大型语言模型（LLMs）的推理操作可以映射到一个完全等价的线性系统，而无需修改模型权重或改变输出预测。我们借鉴了图像扩散模型的技术，通过战略性地改变相对于给定输入序列的梯度计算，使得模型的雅可比矩阵几乎完全重现了线性系统的前向预测。我们在多个模型上验证了这种方法，并通过对分离雅可比矩阵的奇异值分解，发现这些LLMs在极低维的子空间中操作，许多最大的奇异向量解码出与最可能输出标记相关的概念。尽管现代LLMs具有强大的表达能力和全局非线性，但可以通过几乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察，并揭示下一个标记预测过程中的可解释语义结构。'}}}, {'id': 'https://huggingface.co/papers/2505.13157', 'title': 'Role-Playing Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13157', 'abstract': 'A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval', 'score': 5, 'issue_id': 4073, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'c1362083ff11ec99', 'authors': ['Yassine El Boudouri', 'Walter Nuninger', 'Julian Alvarez', 'Yvan Peter'], 'affiliations': ['Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.13157.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'RPEval: новый подход к оценке ролевых способностей языковых моделей', 'desc': 'В статье представлен новый бенчмарк Role-Playing Eval (RPEval) для оценки способностей больших языковых моделей (LLM) к ролевой игре. RPEval оценивает четыре ключевых аспекта: понимание эмоций, принятие решений, моральное соответствие и последовательность характера. Этот инструмент призван преодолеть ограничения существующих методов оценки, таких как ресурсоемкость человеческих оценок и потенциальная предвзятость автоматизированных подходов. Авторы описывают процесс создания RPEval и приводят результаты базовых оценок.'}, 'en': {'title': 'Assessing Role-Playing Skills in AI with RPEval', 'desc': 'The paper introduces Role-Playing Eval (RPEval), a benchmark for evaluating Large Language Models (LLMs) in their ability to role-play. It focuses on four critical aspects: emotional understanding, decision-making, moral alignment, and in-character consistency. The authors highlight the challenges of traditional evaluation methods, which can be resource-intensive and biased. RPEval aims to provide a standardized approach to assess these capabilities in LLMs, with the code and dataset made publicly available for further research.'}, 'zh': {'title': '角色扮演评估：评估大型语言模型的新基准', 'desc': '本文介绍了一种新的基准测试工具，称为角色扮演评估（Role-Playing Eval，RPEval），用于评估大型语言模型（LLMs）在角色扮演中的能力。该评估涵盖了四个关键维度：情感理解、决策能力、道德一致性和角色一致性。由于人类评估资源消耗大且自动评估可能存在偏见，RPEval旨在提供一种更有效的评估方法。文章详细描述了RPEval的构建过程，并提供了基准评估结果。'}}}, {'id': 'https://huggingface.co/papers/2505.24875', 'title': 'ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL', 'url': 'https://huggingface.co/papers/2505.24875', 'abstract': 'Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen.', 'score': 4, 'issue_id': 4078, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f15f9f746431348a', 'authors': ['Yu Zhang', 'Yunqi Li', 'Yifan Yang', 'Rui Wang', 'Yuqing Yang', 'Dai Qi', 'Jianmin Bao', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu'], 'affiliations': ['Fudan University', 'Microsoft Corporation', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24875.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#architecture', '#optimization', '#reasoning', '#rag', '#rl', '#training'], 'emoji': '🎨', 'ru': {'title': 'Рассуждающий генератор изображений: мышление перед созданием', 'desc': "ReasonGen-R1 - это двухэтапная система, объединяющая рассуждения по цепочке мыслей и обучение с подкреплением для генерации изображений. На первом этапе модель обучается явному 'мышлению' на основе текста с помощью нового набора данных с письменными обоснованиями. Второй этап использует алгоритм Group Relative Policy Optimization для улучшения качества выходных изображений. Оценки на нескольких бенчмарках показывают, что ReasonGen-R1 превосходит современные базовые модели."}, 'en': {'title': 'Empowering Image Generation with Reasoning Skills', 'desc': "This paper presents ReasonGen-R1, a novel framework that combines chain-of-thought reasoning with reinforcement learning to enhance generative vision models. The first stage involves fine-tuning an autoregressive image generator using a new dataset of text-based rationales, allowing the model to 'think' before creating images. The second stage employs Group Relative Policy Optimization (GRPO) to refine the generated images based on feedback from a pretrained vision language model, ensuring high visual quality. The results show that ReasonGen-R1 outperforms existing models on various benchmarks, demonstrating its effectiveness in integrating reasoning into image generation."}, 'zh': {'title': '推理与生成的完美结合', 'desc': '本文介绍了一种名为ReasonGen-R1的两阶段框架，旨在将链式思维推理与生成视觉模型结合起来。首先，通过在新生成的推理数据集上进行监督微调，使自回归图像生成器具备明确的文本推理能力。然后，利用群体相对策略优化（GRPO）算法对生成的图像进行优化，以提高视觉质量。实验结果表明，ReasonGen-R1在多个基准测试中表现优于强基线和之前的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.24615', 'title': 'Harnessing Large Language Models for Scientific Novelty Detection', 'url': 'https://huggingface.co/papers/2505.24615', 'abstract': 'In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.', 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e2151a4cb797ec9b', 'authors': ['Yan Liu', 'Zonglin Yang', 'Soujanya Poria', 'Thanh-Son Nguyen', 'Erik Cambria'], 'affiliations': ['Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.24615.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#science'], 'emoji': '💡', 'ru': {'title': 'LLM на страже научной новизны', 'desc': 'Статья представляет новый подход к обнаружению научной новизны с использованием больших языковых моделей (LLM). Авторы создали два новых набора данных в областях маркетинга и обработки естественного языка для оценки методов определения новизны. Они предлагают обучать легковесную модель-ретривер, дистиллируя знания об идеях из LLM, чтобы эффективно сопоставлять похожие концепции. Эксперименты показывают, что предложенный метод превосходит другие подходы в задачах поиска идей и обнаружения новизны на созданных наборах данных.'}, 'en': {'title': 'Harnessing LLMs for Effective Novelty Detection in Research', 'desc': 'This paper addresses the challenge of identifying novel research ideas in academia, which is hindered by the lack of suitable benchmark datasets for novelty detection (ND). The authors propose using large language models (LLMs) to enhance ND by creating two new datasets focused on marketing and NLP. They introduce a method to extract closure sets of related papers and summarize their main ideas using LLMs, which helps in understanding idea conception. Additionally, a lightweight retriever is trained to distill idea-level knowledge from LLMs, improving the efficiency and accuracy of idea retrieval for ND tasks, with experimental results showing superior performance over existing methods.'}, 'zh': {'title': '利用大型语言模型提升科学新颖性检测', 'desc': '在科学快速发展的时代，识别新颖的研究想法变得至关重要但也充满挑战。现有的自然语言处理技术无法有效解决新颖性检测的问题，因为文本相似性与想法构思之间存在差距。本文提出利用大型语言模型（LLMs）进行科学新颖性检测，并引入了两个新的数据集，分别来自市场营销和自然语言处理领域。我们的方法通过提取论文之间的关系构建数据集，并训练轻量级检索器，从而实现高效准确的想法检索，实验结果表明该方法在新颖性检测任务中优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2505.24517', 'title': "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP", 'url': 'https://huggingface.co/papers/2505.24517', 'abstract': "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.", 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '7cec3d5f1feec6d4', 'authors': ['Yinqi Li', 'Jiahe Zhao', 'Hong Chang', 'Ruibing Hou', 'Shiguang Shan', 'Xilin Chen'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.24517.jpg', 'data': {'categories': ['#games', '#multimodal', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Улучшение CLIP через инвертирование генеративной модели', 'desc': 'Данная статья посвящена улучшению модели CLIP (Contrastive Language-Image Pre-training) для более детального распознавания изображений. Авторы предлагают метод un^2CLIP, основанный на инвертировании генеративной модели unCLIP. Этот подход позволяет улучшенному энкодеру изображений сохранить выравнивание с текстовым энкодером, одновременно приобретая способность захватывать больше визуальных деталей. Эксперименты показывают значительное улучшение производительности un^2CLIP по сравнению с оригинальным CLIP на различных задачах, включая мультимодальные и задачи плотного предсказания.'}, 'en': {'title': 'Enhancing CLIP with Un^2CLIP for Better Image Detail Capture', 'desc': 'This paper addresses the limitations of the Contrastive Language-Image Pre-training (CLIP) model, particularly its inability to capture fine details in images and its performance on dense-prediction tasks. The authors propose a novel approach called un^2CLIP, which utilizes a generative model known as unCLIP to enhance the image encoder of CLIP. By inverting the unCLIP model, the authors aim to improve the detail-capturing capabilities of CLIP while maintaining its alignment with text embeddings. Experimental results demonstrate that un^2CLIP outperforms both the original CLIP and previous enhancement methods across various multimodal tasks.'}, 'zh': {'title': '提升CLIP模型，捕捉更多视觉细节', 'desc': '对比语言-图像预训练（CLIP）已成为基础模型，并广泛应用于各种视觉和多模态任务。然而，最近的研究表明，CLIP在区分图像的细微差别方面存在不足，并且在密集预测和以视觉为中心的多模态任务上表现不佳。因此，本研究旨在改进现有的CLIP模型，以尽可能捕捉图像中的视觉细节。我们发现，一种特定类型的生成模型unCLIP为实现这一目标提供了合适的框架。'}}}, {'id': 'https://huggingface.co/papers/2505.23926', 'title': 'Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2505.23926', 'abstract': 'While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.', 'score': 4, 'issue_id': 4070, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '52d11240fa18198b', 'authors': ['Xuweiyi Chen', 'Wentao Zhou', 'Aruni RoyChowdhury', 'Zezhou Cheng'], 'affiliations': ['The MathWorks, Inc.', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23926.jpg', 'data': {'categories': ['#3d', '#architecture', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Автоматическая специализация экспертов для масштабируемого 3D восприятия', 'desc': 'Статья представляет Point-MoE - архитектуру Mixture-of-Experts для масштабируемого понимания 3D точечных облаков. В отличие от стандартных моделей, Point-MoE способна автоматически специализировать экспертов для разных доменов данных без явных меток. Эксперименты показывают, что Point-MoE превосходит сильные мультидоменные базовые модели и лучше обобщается на новые домены. Это открывает путь к масштабируемому 3D пониманию, позволяя модели самостоятельно обнаруживать структуру в разнородных 3D данных.'}, 'en': {'title': 'Unlocking 3D Perception with Point-MoE: A Scalable Solution for Diverse Data', 'desc': 'This paper addresses the challenges in understanding 3D point clouds due to the diverse sources and characteristics of the data. It introduces Point-MoE, a Mixture-of-Experts architecture that enhances cross-domain generalization in 3D perception without needing domain labels during inference. The proposed method allows the model to automatically specialize its experts based on the data it encounters, improving performance on mixed-domain datasets. Experimental results show that Point-MoE outperforms existing models and effectively generalizes to new, unseen domains, paving the way for scalable 3D understanding.'}, 'zh': {'title': '让3D理解更智能：Point-MoE架构的创新之路', 'desc': '本论文提出了一种名为Point-MoE的混合专家架构，旨在实现3D点云理解的跨域泛化。由于3D数据集规模较小且来源多样，传统的点云模型在混合域数据上表现不佳。Point-MoE通过简单的top-k路由策略，能够在没有域标签的情况下自动专门化专家，从而提高模型的性能。实验结果表明，Point-MoE在多个域的基准测试中表现优于现有方法，并且在未见域上具有更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23844', 'title': 'Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation', 'url': 'https://huggingface.co/papers/2505.23844', 'abstract': 'Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration', 'score': 4, 'issue_id': 4066, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '252af3d7c602c2c3', 'authors': ['Zhenglun Kong', 'Zheng Zhan', 'Shiyue Hou', 'Yifan Gong', 'Xin Meng', 'Pengwei Sui', 'Peiyan Dong', 'Xuan Shen', 'Zifeng Wang', 'Pu Zhao', 'Hao Tang', 'Stratis Ioannidis', 'Yanzhi Wang'], 'affiliations': ['Google', 'Harvard University', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23844.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Умное слияние языковых моделей: адаптивный подход к интеграции знаний', 'desc': 'Эта статья представляет новый подход к улучшению больших языковых моделей (LLM) путем адаптивного отбора и объединения знаний из различных LLM. Авторы предлагают фреймворк, который использует сеть адаптивного выбора для определения наиболее релевантных исходных моделей и динамическую стратегию взвешенного слияния для учета сильных сторон каждой модели. Метод позволяет снизить интерференцию знаний на 50% по сравнению с существующими подходами. Экспериментальные результаты показывают, что предложенный метод обеспечивает более стабильный и масштабируемый процесс агрегации знаний.'}, 'en': {'title': 'Adaptive Knowledge Aggregation for Enhanced LLM Performance', 'desc': 'This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation.'}, 'zh': {'title': '自适应知识聚合，构建更强大的语言模型', 'desc': '大型语言模型（LLMs）在性能上表现出色，但通过传统的微调方法持续改进仍然具有挑战性，尤其是在整合其他专业LLMs的能力时。现有的方法如集成和权重合并需要大量内存，并且难以适应变化的数据环境。我们提出了一种框架，能够自适应地选择和聚合来自不同LLMs的知识，以构建一个更强大的单一模型，避免了集成方法的高内存开销和权重合并的灵活性不足。实验结果表明，我们的方法能够实现更稳定和可扩展的知识聚合过程，同时将知识干扰减少了50%。'}}}, {'id': 'https://huggingface.co/papers/2505.21864', 'title': 'DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2505.21864', 'abstract': "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.", 'score': 4, 'issue_id': 4072, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '5d1867db4168cab6', 'authors': ['Mengda Xu', 'Han Zhang', 'Yifan Hou', 'Zhenjia Xu', 'Linxi Fan', 'Manuela Veloso', 'Shuran Song'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'J.P. Morgan AI Research', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21864.jpg', 'data': {'categories': ['#robotics', '#transfer_learning', '#agents'], 'emoji': '🦾', 'ru': {'title': 'Перенос навыков ловкой манипуляции от человека к роботу', 'desc': 'DexUMI - это фреймворк для сбора данных и обучения политик, который использует человеческую руку в качестве естественного интерфейса для передачи навыков ловкой манипуляции роботизированным рукам. Фреймворк включает аппаратную адаптацию в виде носимого экзоскелета руки и программную адаптацию для замены изображения человеческой руки на робототехническую в видеоданных. DexUMI позволяет минимизировать разрыв между воплощением человеческой и роботизированной руки. В экспериментах на двух различных платформах дексетрозных роботизированных рук была достигнута средняя успешность выполнения задач 86%.'}, 'en': {'title': 'Bridging Human and Robot Dexterity with DexUMI', 'desc': 'The DexUMI framework is designed to enhance the transfer of dexterous manipulation skills from human hands to robotic hands. It combines a wearable hand exoskeleton for direct haptic feedback and a high-fidelity robot hand inpainting technique to create realistic training data. This approach minimizes the embodiment gap by adapting human movements to be compatible with robot hand kinematics. Through extensive real-world testing, DexUMI achieves an impressive average task success rate of 86%, showcasing its effectiveness in robotic skill transfer.'}, 'zh': {'title': 'DexUMI：将人类灵巧技能转移到机器人手的创新框架', 'desc': 'DexUMI框架利用可穿戴手部外骨骼和高保真机器人手部重绘技术，将人类的灵巧操作技能转移到机器人手上，从而实现高任务成功率。该框架通过硬件和软件的适配，缩小了人类手与各种机器人手之间的体现差距。硬件适配使用可穿戴手部外骨骼，允许在数据收集过程中直接获得触觉反馈，并将人类动作适配为可行的机器人手动作。软件适配则通过在视频数据中用高保真机器人手重绘替换人类手，弥补了视觉差距。'}}}, {'id': 'https://huggingface.co/papers/2505.24189', 'title': 'Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows', 'url': 'https://huggingface.co/papers/2505.24189', 'abstract': 'Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.', 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6fcc18713ed36918', 'authors': ['Orlando Marquez Ayala', 'Patrice Bechard', 'Emily Chen', 'Maggie Baird', 'Jingfei Chen'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2505.24189.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#small_models'], 'emoji': '🤖', 'ru': {'title': 'Малые языковые модели все еще имеют преимущество в специализированных задачах', 'desc': 'Исследование сравнивает эффективность малых языковых моделей (SLM) и больших языковых моделей (LLM) для задач, требующих структурированного вывода. Авторы обнаружили, что для специфических задач, таких как генерация низкокодовых рабочих процессов в формате JSON, дообучение SLM дает преимущество в качестве на 10% по сравнению с промптингом LLM. Несмотря на снижение стоимости токенов для LLM, SLM все еще могут быть предпочтительнее из-за более быстрого вывода и меньших затрат. Проведен систематический анализ ошибок для выявления ограничений моделей.'}, 'en': {'title': 'Fine-Tuning SLMs: The Key to Quality in Domain-Specific Tasks', 'desc': 'This paper investigates the effectiveness of Small Language Models (SLMs) compared to Large Language Models (LLMs) like GPT-4o for specific tasks that require structured outputs. It highlights that while LLMs can perform well with appropriate prompts, fine-tuning SLMs can lead to a significant quality improvement, averaging a 10% increase in performance for generating low-code workflows in JSON format. The authors conduct a thorough error analysis to identify the limitations of both model types. Ultimately, the findings suggest that SLMs retain a quality advantage for certain domain-specific applications despite the reduced costs of using LLMs.'}, 'zh': {'title': '小型语言模型在特定任务中的质量优势', 'desc': '大型语言模型（LLMs）如GPT-4o能够通过合适的提示处理多种复杂任务。随着每个token的成本降低，针对实际应用微调小型语言模型（SLMs）的优势可能不再明显。本文提供证据表明，对于需要结构化输出的特定领域任务，SLMs仍然具有质量优势。我们比较了微调SLM与使用提示的LLM在生成JSON格式低代码工作流任务上的表现，发现微调平均提高了10%的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.20047', 'title': 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.20047', 'abstract': "This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.", 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '86bdf56529c01563', 'authors': ['Debargha Ganguly', 'Vikash Singh', 'Sreehari Sankar', 'Biyao Zhang', 'Xuecen Zhang', 'Srinivasan Iyengar', 'Xiaotian Han', 'Amit Sharma', 'Shivkumar Kalyanaraman', 'Vipin Chaudhary'], 'affiliations': ['Case Western Reserve University', 'Microsoft Corporation', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.20047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#interpretability', '#data', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности формальных спецификаций, генерируемых нейросетями', 'desc': 'Исследование посвящено квантификации неопределенности в больших языковых моделях (LLM) при генерации формальных спецификаций. Авторы представляют фреймворк на основе вероятностных контекстно-свободных грамматик (PCFG) для улучшения обнаружения ошибок и выборочной верификации. Оценка пяти современных LLM показала значительное влияние автоформализации на основе SMT на точность в зависимости от домена. Предложенный подход позволяет существенно снизить количество ошибок при минимальном отказе от верификации, делая формализацию на основе LLM более надежной.'}, 'en': {'title': 'Bridging Uncertainty and Formal Verification in LLMs', 'desc': 'This paper investigates how to measure and manage uncertainty in large language models (LLMs) when they generate formal specifications. It highlights the challenge that LLMs are probabilistic, while formal verification requires certainty. The authors propose a new framework using probabilistic context-free grammar (PCFG) to better understand and categorize the uncertainty in LLM outputs. Their findings show that different tasks exhibit unique uncertainty patterns, and by combining these signals, they can significantly improve the accuracy of formal verification processes.'}, 'zh': {'title': '提升LLM生成规范的可靠性', 'desc': '本研究探讨了在大型语言模型中进行不确定性量化，以生成正式规范。我们引入了一种概率上下文无关文法（PCFG）框架，以提高错误检测和选择性验证的能力。研究表明，LLM生成的正式文档在不同任务中的不确定性信号是依赖于任务的，且现有的不确定性量化技术未能有效识别这些错误。通过轻量级融合这些信号，我们显著减少了错误率，使LLM驱动的形式化过程变得更加可靠。'}}}, {'id': 'https://huggingface.co/papers/2505.24581', 'title': 'GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training', 'url': 'https://huggingface.co/papers/2505.24581', 'abstract': 'Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '380d404889c022d3', 'authors': ['Omer Nacar', 'Anis Koubaa', 'Serry Sibaee', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, Riyadh, Saudi Arabia', 'Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.24581.jpg', 'data': {'categories': ['#low_resource', '#benchmark', '#multilingual', '#science', '#training', '#transfer_learning', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Прорыв в семантическом анализе арабского текста', 'desc': 'Статья представляет модели GATE (General Arabic Text Embedding) для семантического анализа арабского текста. Эти модели достигают наилучших результатов в задаче определения семантической близости текстов (STS) в рамках бенчмарка MTEB. GATE использует технику Matryoshka Representation Learning и гибридный подход к обучению с арабскими триплетами для задачи логического вывода на естественном языке. Модели GATE превосходят более крупные модели, включая OpenAI, на 20-25% в тестах STS, эффективно улавливая семантические нюансы арабского языка.'}, 'en': {'title': 'Unlocking Arabic Semantics with GATE Models', 'desc': 'This paper addresses the challenge of semantic textual similarity (STS) in the Arabic language, which has been under-researched due to limited datasets and models. It presents the General Arabic Text Embedding (GATE) models, which utilize advanced techniques like Matryoshka Representation Learning and a hybrid loss training approach. GATE demonstrates significant improvements in STS tasks, outperforming larger models by 20-25% on benchmarks. This advancement is crucial for better understanding and processing the unique semantic characteristics of Arabic text.'}, 'zh': {'title': '提升阿拉伯语语义理解的GATE模型', 'desc': '语义文本相似性（STS）是自然语言处理（NLP）中的一个重要任务，能够支持检索、聚类和理解文本之间的语义关系。然而，由于缺乏高质量的数据集和预训练模型，阿拉伯语领域的研究仍然有限。这种资源的匮乏限制了阿拉伯文本语义相似性的准确评估和进展。本文介绍了一种通用阿拉伯文本嵌入（GATE）模型，在MTEB基准测试中实现了语义文本相似性任务的最先进性能，显著提升了阿拉伯语的语义理解能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23832', 'title': 'LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation', 'url': 'https://huggingface.co/papers/2505.23832', 'abstract': 'Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '8778ede47e6e60db', 'authors': ['Chaeeun Kim', 'Jinu Lee', 'Wonseok Hwang'], 'affiliations': ['LBOX', 'University of Illinois Urbana-Champaign', 'University of Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2505.23832.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal', '#reasoning', '#transfer_learning', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LegalSearchLM: Умный поиск в море юридических дел', 'desc': 'Статья представляет новый подход к поиску релевантных юридических дел - LegalSearchLM. Эта модель выполняет рассуждения над элементами запроса и генерирует ответ на основе целевых дел с помощью ограниченного декодирования. Авторы также создали первый крупномасштабный корейский бенчмарк для оценки поиска юридических дел - LEGAR BENCH. LegalSearchLM превосходит базовые модели на 6-20% на LEGAR BENCH и демонстрирует сильную обобщающую способность на новых типах дел.'}, 'en': {'title': 'Revolutionizing Legal Case Retrieval with LEGAR BENCH and LegalSearchLM', 'desc': 'This paper addresses the challenges in Legal Case Retrieval (LCR) by introducing a new benchmark and a novel retrieval model. The authors present LEGAR BENCH, a comprehensive dataset that includes over 1.2 million legal cases and 411 different crime types, which enhances the evaluation of LCR systems. They also propose LegalSearchLM, a model that utilizes legal element reasoning to improve the relevance of retrieved cases by generating content based on the query case. Experimental results indicate that LegalSearchLM significantly outperforms existing methods, demonstrating better accuracy and generalization to diverse legal scenarios.'}, 'zh': {'title': '法律检索的新突破：超越传统方法', 'desc': '法律案件检索（LCR）是法律专业人员在研究和决策中获取相关案例的基本任务。现有的LCR研究存在两个主要限制：一是评估使用的检索语料库规模较小，无法反映真实法律检索场景的复杂性；二是依赖于嵌入或词汇匹配方法，导致表示能力有限和法律无关的匹配。为了解决这些问题，本文提出了LEGAR BENCH，这是第一个涵盖411种犯罪类型和120万法律案例的大规模韩国LCR基准，以及LegalSearchLM，一个能够进行法律元素推理并生成与目标案例相关内容的检索模型。实验结果表明，LegalSearchLM在LEGAR BENCH上比基线模型提高了6-20%的性能，并在跨领域案例中表现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20977', 'title': 'Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2505.20977', 'abstract': 'MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a MC\\textsuperscript{2} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.', 'score': 2, 'issue_id': 4076, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e3f911b43ad30492', 'authors': ['Yu Zhang', 'Jinlong Ma', 'Yongshuai Hou', 'Xuefeng Bai', 'Kehai Chen', 'Yang Xiang', 'Jun Yu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20977.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#machine_translation', '#hallucinations'], 'emoji': '🔬', 'ru': {'title': 'Контроль предвзятости модальностей в мультимодальных языковых моделях', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) демонстрируют предвзятость к определенным модальностям при обработке мультимодального контекста. Авторы разработали бенчмарк MC² для оценки предпочтений модальностей в сценариях с конфликтующими свидетельствами. Анализ выявил, что направление предпочтений может быть обнаружено в скрытых представлениях MLLM. На основе этого предложен метод инженерии представлений для контроля предпочтений модальностей, который улучшает такие задачи как смягчение галлюцинаций и мультимодальный машинный перевод.'}, 'en': {'title': 'Controlling Modality Bias in Multimodal Models', 'desc': 'This paper investigates how multimodal large language models (MLLMs) show a tendency to favor one type of input (modality) over another when processing mixed information. The authors create a benchmark called MC² to evaluate this modality bias under specific conditions where evidence from different modalities conflicts. They find that all tested MLLMs exhibit clear modality preferences, which can be adjusted using a new method based on representation engineering. This method allows for the control of modality bias without needing to retrain the models, leading to better performance in tasks like reducing hallucinations and improving multimodal translations.'}, 'zh': {'title': '控制模态偏好，提升多模态任务表现', 'desc': '多模态大型语言模型（MLLMs）在处理复杂的多模态任务时表现出色，但它们在处理多模态上下文时可能存在模态偏好。我们建立了一个MC²基准，以系统评估在证据冲突场景下的模态偏好。研究发现，所有测试的18个MLLMs普遍表现出明显的模态偏见，并且这种偏好可以通过外部干预进行调整。基于此，我们提出了一种基于表示工程的探测和引导方法，可以在不进行额外微调的情况下显式控制模态偏好，从而在幻觉缓解和多模态机器翻译等下游任务中取得显著改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24869', 'title': 'SiLVR: A Simple Language-based Video Reasoning Framework', 'url': 'https://huggingface.co/papers/2505.24869', 'abstract': "SiLVR, a language-based framework, enhances multimodal LLMs' video reasoning by leveraging adaptive token reduction, achieving top results on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.", 'score': 1, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'd03545b1ac06e77f', 'authors': ['Ce Zhang', 'Yan-Bo Lin', 'Ziyang Wang', 'Mohit Bansal', 'Gedas Bertasius'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.24869.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#reasoning', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'SiLVR: Прорыв в видеорассуждениях для мультимодальных ИИ', 'desc': 'SiLVR - это фреймворк для улучшения видеорассуждений мультимодальных языковых моделей. Он использует двухэтапный подход: сначала преобразует видео в языковые представления, а затем применяет мощную LLM для решения сложных задач понимания видео. SiLVR использует адаптивное сокращение токенов для обработки длинного контекста. Фреймворк достиг лучших результатов на нескольких бенчмарках видеопонимания.'}, 'en': {'title': 'Enhancing Video Reasoning with SiLVR: A Language-Based Approach', 'desc': 'SiLVR is a novel framework designed to improve the video reasoning abilities of multimodal large language models (MLLMs). It breaks down complex video understanding into two main stages: first, converting raw video into language-based representations using various sensory inputs, and second, utilizing a powerful reasoning LLM to interpret these representations. To efficiently manage long-context inputs, SiLVR employs an adaptive token reduction method that optimizes how tokens are sampled. This approach has led to SiLVR achieving top performance on multiple video-language benchmarks, demonstrating the potential of strong reasoning LLMs in processing and understanding video content.'}, 'zh': {'title': 'SiLVR：提升视频推理的语言框架', 'desc': 'SiLVR是一个基于语言的视频推理框架，旨在提升多模态大语言模型（MLLMs）在视频理解方面的能力。该框架将复杂的视频理解任务分为两个阶段：首先，通过多感官输入将原始视频转换为基于语言的表示；其次，将这些语言描述输入到强大的推理大语言模型中，以解决复杂的视频语言理解任务。SiLVR采用自适应的令牌减少方案，动态确定采样令牌的时间粒度，从而有效处理长上下文的多感官输入。该框架在多个基准测试中取得了最佳结果，展示了强大的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24782', 'title': 'Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings', 'url': 'https://huggingface.co/papers/2505.24782', 'abstract': 'A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '54b58cb49740c12b', 'authors': ['Max Conti', 'Manuel Faysse', 'Gautier Viaud', 'Antoine Bosselut', 'Céline Hudelot', 'Pierre Colombo'], 'affiliations': ['CentraleSupélec, Paris-Saclay', 'EPFL Lausanne', 'Equall.ai', 'Illuin Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.24782.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекстное обогащение для умного поиска документов', 'desc': 'Авторы представили новый бенчмарк ConTEB для оценки способности моделей извлечения документов использовать контекст всего документа. Они также предложили метод контрастивного дообучения InSeNT, который улучшает контекстуальные представления, сохраняя вычислительную эффективность. Результаты показывают, что их подход значительно повышает качество извлечения на ConTEB без ухудшения базовой производительности модели. Метод также делает вложения фрагментов более устойчивыми к неоптимальным стратегиям разбиения и увеличению размера корпуса для поиска.'}, 'en': {'title': 'Enhancing Document Retrieval with Contextual Awareness', 'desc': 'This paper addresses the limitations of current document retrieval methods that often treat text passages independently, missing out on important context from the entire document. It introduces ConTEB, a benchmark that evaluates how well retrieval models utilize document-wide context. The authors propose InSeNT, a new contrastive training method that enhances the learning of contextual representations while maintaining efficiency. Their findings demonstrate that this approach significantly improves retrieval quality, making the models more robust to various chunking strategies and larger datasets.'}, 'zh': {'title': '提升文档检索质量的上下文感知方法', 'desc': '本文提出了一种上下文感知的基准测试和对比训练方法，以提高文档检索的质量。现有的文档检索嵌入方法通常独立编码文档中的段落，忽视了文档整体的上下文信息。我们引入了ConTEB（上下文感知文本嵌入基准），用于评估检索模型利用文档上下文的能力。通过提出InSeNT（序列内负训练），我们的方法在保持计算效率的同时，显著提升了检索质量。'}}}, {'id': 'https://huggingface.co/papers/2505.24119', 'title': 'The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It', 'url': 'https://huggingface.co/papers/2505.24119', 'abstract': 'This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.', 'score': 1, 'issue_id': 4076, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f5b6ed3de0b4cc5b', 'authors': ['Zheng-Xin Yong', 'Beyza Ermis', 'Marzieh Fadaee', 'Stephen H. Bach', 'Julia Kreutzer'], 'affiliations': ['Brown University', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.24119.jpg', 'data': {'categories': ['#low_resource', '#survey', '#ethics', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового барьера в исследованиях безопасности ИИ', 'desc': 'Данная статья представляет всесторонний анализ лингвистического разнообразия в исследованиях безопасности больших языковых моделей (LLM), подчеркивая их англоцентричность. Авторы провели систематический обзор почти 300 публикаций за 2020-2024 годы на крупных конференциях и семинарах по обработке естественного языка. Исследование выявило значительный и растущий языковой разрыв в исследованиях безопасности LLM, при этом даже высокоресурсные неанглийские языки получают минимальное внимание. На основе своего анализа авторы предлагают рекомендации и конкретные направления для будущих исследований в области многоязычной безопасности ИИ.'}, 'en': {'title': 'Bridging the Language Gap in LLM Safety Research', 'desc': 'This paper analyzes the focus on English in large language model (LLM) safety research, revealing a significant language gap. It reviews nearly 300 publications from major NLP conferences between 2020 and 2024, showing that non-English languages, even those with ample resources, are largely overlooked. The authors highlight the lack of standalone studies on non-English languages and poor documentation practices in English safety research. They propose recommendations and future directions to enhance multilingual safety evaluation, data generation, and cross-lingual safety generalization, aiming for more inclusive AI safety practices.'}, 'zh': {'title': '推动多语言LLM安全研究的未来方向', 'desc': '这篇论文对大型语言模型（LLM）安全研究的语言多样性进行了全面分析，强调了该领域以英语为中心的特点。通过对2020年至2024年间近300篇来自主要自然语言处理（NLP）会议和研讨会的出版物进行系统审查，我们发现LLM安全研究中存在显著且日益扩大的语言差距，甚至高资源的非英语语言也受到的关注很少。我们进一步观察到，非英语语言很少作为独立语言进行研究，而英语安全研究的文献记录实践也很差。为了激励未来的多语言安全研究，我们根据调查结果提出了几项建议，并提出了关于安全评估、训练数据生成和跨语言安全泛化的三个具体未来方向。'}}}, {'id': 'https://huggingface.co/papers/2505.23923', 'title': 'ChARM: Character-based Act-adaptive Reward Modeling for Advanced\n  Role-Playing Language Agents', 'url': 'https://huggingface.co/papers/2505.23923', 'abstract': 'ChARM, a character-focused adaptive reward model, improves preference learning for role-playing language agents by using an act-adaptive margin and self-evolution with unlabeled data, achieving superior results on dedicated benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.', 'score': 1, 'issue_id': 4079, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '761912e5ab3c056e', 'authors': ['Feiteng Fang', 'Ting-En Lin', 'Yuchuan Wu', 'Xiong Liu', 'Xiang Huang', 'Dingwei Chen', 'Jing Ye', 'Haonan Zhang', 'Liang Zhu', 'Hamid Alinejad-Rokny', 'Min Yang', 'Fei Huang', 'Yongbin Li'], 'affiliations': ['Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China', 'Tongji University', 'Tongyi Laboratory', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2505.23923.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#rlhf', '#dataset', '#games'], 'emoji': '🎭', 'ru': {'title': 'ChARM: Революция в обучении ролевых языковых агентов', 'desc': 'ChARM - это модель вознаграждения для ролевых языковых агентов, использующая адаптивную границу для действий и самоэволюцию с неразмеченными данными. Она улучшает обучение предпочтениям для более реалистичного взаимодействия человека с компьютером. Авторы также представили RoleplayPref - крупномасштабный набор данных предпочтений для ролевых агентов, и RoleplayEval - специализированный оценочный бенчмарк. Эксперименты показали 13% улучшение по сравнению с традиционной моделью Брэдли-Терри в ранжировании предпочтений.'}, 'en': {'title': 'ChARM: Elevating Role-Playing Agents with Adaptive Rewards', 'desc': 'ChARM is a novel reward model designed to enhance preference learning for role-playing language agents (RPLAs). It introduces an act-adaptive margin that improves the efficiency and adaptability of learning from user preferences. Additionally, ChARM employs a self-evolution mechanism that utilizes large amounts of unlabeled data to broaden the training scope. The model demonstrates significant performance improvements over traditional methods, achieving state-of-the-art results on specialized evaluation benchmarks.'}, 'zh': {'title': '角色扮演语言代理的新突破：ChARM模型', 'desc': 'ChARM是一种以角色为中心的自适应奖励模型，旨在改善角色扮演语言代理的偏好学习。它通过引入行为自适应边际和利用未标记数据的自我进化机制，解决了传统奖励模型在可扩展性和适应主观对话偏好方面的挑战。我们还推出了RoleplayPref，这是第一个专门针对角色扮演语言代理的大规模偏好数据集，包含1108个角色和16888个双语对话。实验结果表明，ChARM在偏好排名上比传统的Bradley-Terry模型提高了13%。'}}}, {'id': 'https://huggingface.co/papers/2505.21749', 'title': 'Revisiting Bi-Linear State Transitions in Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2505.21749', 'abstract': 'Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.  \t\t\t\t\tAI-generated summary \t\t\t\t The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cb7a7df613a84e6e', 'authors': ['M. Reza Ebrahimi', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.21749.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Билинейность - ключ к эффективному отслеживанию состояний в РНС', 'desc': 'В статье рассматриваются билинейные операции в рекуррентных нейронных сетях как естественное предрасположение для задач отслеживания состояний. Авторы демонстрируют, что скрытые слои активно участвуют в вычислениях, а не просто хранят информацию. Теоретически и эмпирически показано, что билинейные обновления состояний образуют естественную иерархию, соответствующую задачам отслеживания состояний возрастающей сложности. Линейные рекуррентные сети, такие как Mamba, находятся в центре этой иерархии с наименьшей сложностью.'}, 'en': {'title': 'Bilinear Operations: Enhancing RNNs for State Tracking', 'desc': 'This paper explores the role of bilinear operations in recurrent neural networks (RNNs) for state tracking tasks. It argues that hidden units should be seen as active contributors to computations rather than just memory stores. The authors demonstrate that bilinear interactions between hidden units and input embeddings provide a beneficial inductive bias for evolving hidden states. Additionally, they establish a hierarchy of state tracking tasks, with linear RNNs like Mamba representing the simplest form of this structure.'}, 'zh': {'title': '双线性操作：状态跟踪的自然偏置', 'desc': '在递归神经网络中，双线性操作被证明是状态跟踪任务的自然偏置，形成了一种层次结构，其中线性递归网络是最简单的形式。隐藏单元的角色通常被视为建模记忆，研究主要集中在通过门控机制增强信息保留。本文重新审视双线性操作，展示它们在状态跟踪任务中如何自然地表示隐藏状态的演变。我们还表明，双线性状态更新形成了一个自然的层次结构，适用于复杂性逐渐增加的状态跟踪任务。'}}}, {'id': 'https://huggingface.co/papers/2505.23856', 'title': 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across\n  Modalities', 'url': 'https://huggingface.co/papers/2505.23856', 'abstract': 'OMNIGUARD detects harmful prompts across languages and modalities by identifying aligned internal representations in large language models, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\% over the strongest baseline in a multilingual setting, by 20.44\\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient (approx 120 times faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.', 'score': 0, 'issue_id': 4082, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '365a6ed14a1515e6', 'authors': ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh'], 'affiliations': ['Microsoft', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.23856.jpg', 'data': {'categories': ['#security', '#low_resource', '#dataset', '#multimodal', '#multilingual'], 'emoji': '🛡️', 'ru': {'title': 'OMNIGUARD: Универсальная защита языковых моделей от вредоносных запросов', 'desc': 'OMNIGUARD - это подход к обнаружению вредоносных запросов к языковым моделям на разных языках и в разных модальностях. Он использует внутренние представления модели, согласованные между языками и модальностями, для создания универсального классификатора. OMNIGUARD значительно повышает точность классификации вредоносных запросов по сравнению с существующими методами. Метод также отличается высокой эффективностью за счет повторного использования вычисленных эмбеддингов.'}, 'en': {'title': 'OMNIGUARD: Safeguarding LLMs Across Languages and Modalities', 'desc': 'OMNIGUARD is a novel approach designed to detect harmful prompts in large language models (LLMs) across different languages and modalities, such as text, images, and audio. It works by identifying aligned internal representations within the LLMs, which allows it to create a classifier that is effective regardless of the language or type of input. This method significantly improves the accuracy of harmful prompt detection, outperforming existing techniques by notable margins in multilingual and multimodal contexts. Additionally, OMNIGUARD is highly efficient, operating approximately 120 times faster than previous methods, making it a practical solution for real-time applications.'}, 'zh': {'title': 'OMNIGUARD：跨语言和模态的有害提示检测新方法', 'desc': 'OMNIGUARD是一种检测有害提示的技术，能够跨语言和模态识别大型语言模型中的对齐内部表示。该方法通过识别不同语言或模态中对齐的内部表示，构建一种不依赖语言或模态的分类器，从而提高有害提示的检测准确性。与现有方法相比，OMNIGUARD在多语言环境下的分类准确性提高了11.57%，在基于图像的提示中提高了20.44%，并在基于音频的提示中设定了新的最优性能。该方法还利用生成过程中计算的嵌入，使得检测过程非常高效，速度比下一个最快的基线快约120倍。'}}}, {'id': 'https://huggingface.co/papers/2506.03569', 'title': 'MiMo-VL Technical Report', 'url': 'https://huggingface.co/papers/2506.03569', 'abstract': 'We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.', 'score': 56, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'cb568276c7e799cb', 'authors': ['Xiaomi LLM-Core Team', ':', 'Zihao Yue', 'Zhenru Lin', 'Yifan Song', 'Weikun Wang', 'Shuhuai Ren', 'Shuhao Gu', 'Shicheng Li', 'Peidian Li', 'Liang Zhao', 'Lei Li', 'Kainan Bao', 'Hao Tian', 'Hailin Zhang', 'Gang Wang', 'Dawei Zhu', 'Cici', 'Chenhong He', 'Bowen Ye', 'Bowen Shen', 'Zihan Zhang', 'Zihan Jiang', 'Zhixian Zheng', 'Zhichao Song', 'Zhenbo Luo', 'Yue Yu', 'Yudong Wang', 'Yuanyuan Tian', 'Yu Tu', 'Yihan Yan', 'Yi Huang', 'Xu Wang', 'Xinzhe Xu', 'Xingchen Song', 'Xing Zhang', 'Xing Yong', 'Xin Zhang', 'Xiangwei Deng', 'Wenyu Yang', 'Wenhan Ma', 'Weiwei Lv', 'Weiji Zhuang', 'Wei Liu', 'Sirui Deng', 'Shuo Liu', 'Shimao Chen', 'Shihua Yu', 'Shaohui Liu', 'Shande Wang', 'Rui Ma', 'Qiantong Wang', 'Peng Wang', 'Nuo Chen', 'Menghang Zhu', 'Kangyang Zhou', 'Kang Zhou', 'Kai Fang', 'Jun Shi', 'Jinhao Dong', 'Jiebao Xiao', 'Jiaming Xu', 'Huaqiu Liu', 'Hongshen Xu', 'Heng Qu', 'Haochen Zhao', 'Hanglong Lv', 'Guoan Wang', 'Duo Zhang', 'Dong Zhang', 'Di Zhang', 'Chong Ma', 'Chang Liu', 'Can Cai', 'Bingquan Xia'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2506.03569.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#multimodal', '#rlhf', '#benchmark', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты', 'desc': 'Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT и MiMo-VL-7B-RL, демонстрирующие передовые результаты в задачах визуального понимания и мультимодальных рассуждений. Модель MiMo-VL-7B-RL превосходит Qwen2.5-VL-7B в 35 из 40 оцениваемых задач и достигает 59.4 баллов на бенчмарке OlympiadBench. Обучение моделей включало четырехэтапное предобучение на 2.4 триллионах токенов и применение смешанного обучения с подкреплением (MORL). Авторы подчеркивают важность включения качественных данных для рассуждений с длинной цепочкой мыслей в этапы предобучения.'}, 'en': {'title': 'Revolutionizing Vision-Language Models with MiMo-VL', 'desc': 'The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research.'}, 'zh': {'title': '开创视觉-语言模型的新标准', 'desc': '我们开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL，这两个强大的视觉-语言模型在一般视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在40个评估任务中有35个超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数高达78B的模型。在GUI定位应用中，它在OSWorld-G上以56.1的分数设定了新标准，甚至超越了专门模型UI-TARS。我们的训练结合了四阶段的预训练（24万亿个标记）和混合在线强化学习（MORL），并强调了在预训练阶段融入高质量推理数据和长链思维的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.04207', 'title': 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04207', 'abstract': 'Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.', 'score': 39, 'issue_id': 4135, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '61521f9ed974c930', 'authors': ['Shuang Chen', 'Yue Guo', 'Zhaochen Su', 'Yafu Li', 'Yulun Wu', 'Jiacheng Chen', 'Jiayu Chen', 'Weijie Wang', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04207.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#training', '#benchmark', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений MLLM: от инициализации до многоэтапного RL', 'desc': 'Статья посвящена улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждению с помощью обучения с подкреплением (RL). Авторы выявили три ключевых феномена в процессе обучения: важность правильной инициализации, проблему стагнации градиентов при стандартном GRPO и эффективность последующего текстового RL. На основе этих наблюдений была разработана модель ReVisual-R1, достигшая нового уровня производительности среди открытых 7B MLLM на сложных бенчмарках.'}, 'en': {'title': 'Unlocking Reasoning in MLLMs with Smart Training Strategies', 'desc': 'This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.'}, 'zh': {'title': '提升多模态推理的新方法', 'desc': '本文探讨了如何通过强化学习（RL）提升多模态大型语言模型（MLLM）的推理能力。研究发现，良好的冷启动初始化对于增强MLLM的推理至关重要，单独使用精心选择的文本数据即可超越许多近期的多模态推理模型。标准的GRPO在多模态RL中存在梯度停滞的问题，影响了训练的稳定性和性能。通过分阶段的训练方法，结合文本和多模态RL，提出了ReVisual-R1，达到了开源7B MLLM在多个基准测试中的新状态。'}}}, {'id': 'https://huggingface.co/papers/2506.04089', 'title': 'AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment', 'url': 'https://huggingface.co/papers/2506.04089', 'abstract': 'AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.', 'score': 39, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5facb42d11447ff', 'authors': ['Anastasiia Ivanova', 'Eva Bakaeva', 'Zoya Volovikova', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'LMU, Munich, Germany', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04089.jpg', 'data': {'categories': ['#interpretability', '#agents', '#data', '#dataset', '#alignment'], 'emoji': '🍳', 'ru': {'title': 'AmbiK: унифицированный бенчмарк для обнаружения неоднозначности в инструкциях для роботов', 'desc': 'AmbiK - это текстовый датасет неоднозначных инструкций для кухонных роботов, созданный для унифицированного сравнения методов обнаружения неоднозначности. Датасет содержит 1000 пар неоднозначных задач и их однозначных аналогов, классифицированных по типу неоднозначности. AmbiK был собран с помощью больших языковых моделей (LLM) и проверен людьми. Он включает описания окружения, уточняющие вопросы и ответы, намерения пользователей и планы задач.'}, 'en': {'title': 'AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics', 'desc': 'The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.'}, 'zh': {'title': '统一比较模糊性检测方法的AmbiK数据集', 'desc': 'AmbiK是一个针对厨房机器人模糊指令的文本数据集，旨在统一比较模糊性检测方法。该数据集包含1000对模糊任务及其明确对应任务，涵盖人类偏好、常识知识和安全等模糊性类型。AmbiK由大型语言模型（LLMs）协助收集，并经过人工验证，提供环境描述、澄清问题及答案、用户意图和任务计划等信息。我们希望AmbiK能帮助研究人员进行模糊性检测方法的统一比较。'}}}, {'id': 'https://huggingface.co/papers/2505.16968', 'title': 'CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark', 'url': 'https://huggingface.co/papers/2505.16968', 'abstract': 'CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.', 'score': 35, 'issue_id': 4139, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a069288c85761286', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Gustavo Bertolo Stahl', 'Seung Hun Eddie Han', 'Salman Khan', 'Abdulrahman Mahmoud'], 'affiliations': ['Australian National University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16968.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#benchmark', '#open_source'], 'emoji': '🔄', 'ru': {'title': 'CASS: Преодоление барьеров между GPU-архитектурами', 'desc': 'CASS представляет собой набор данных и набор моделей для транспиляции GPU-кода между архитектурами как на уровне исходного кода, так и на уровне ассемблера. Модели CASS достигают высокой точности перевода: 95% для исходного кода и 37.5% для ассемблера, превосходя коммерческие решения. Сгенерированный код соответствует производительности нативного кода в более чем 85% тестовых случаев. Авторы также представили CASS-Bench - набор тестов для оценки качества транспиляции GPU-кода.'}, 'en': {'title': 'CASS: Bridging GPU Code Portability with High Accuracy Transpilation', 'desc': 'CASS is a groundbreaking dataset and model suite designed for GPU code transpilation, focusing on both source-level and assembly-level translations. It includes 70,000 verified code pairs that facilitate the conversion of code between different GPU architectures, addressing the challenge of low-level code portability. The CASS models achieve impressive accuracy rates, with 95% for source translation and 37.5% for assembly translation, significantly surpassing existing commercial solutions. Additionally, the generated code maintains native performance in over 85% of cases, and the accompanying CASS-Bench provides a robust evaluation framework for various GPU domains.'}, 'zh': {'title': 'CASS：GPU代码转译的突破性进展', 'desc': 'CASS是一个用于GPU代码转译的数据集和模型套件，支持源代码和汇编级别的转译。它包含70,000对经过验证的代码对，解决了低级GPU代码可移植性的重要问题。通过训练CASS系列特定领域语言模型，我们在源代码转译中达到了95%的准确率，并在汇编转译中达到了37.5%的准确率。CASS生成的代码在超过85%的测试案例中与本地性能相匹配，保持了运行时和内存行为的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.02921', 'title': 'A Controllable Examination for Long-Context Language Models', 'url': 'https://huggingface.co/papers/2506.02921', 'abstract': 'LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.', 'score': 30, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '073ae66fedf9c141', 'authors': ['Yijun Yang', 'Zeyu Huang', 'Wenhao Zhu', 'Zihan Qiu', 'Fei Yuan', 'Jeff Z. Pan', 'Ivan Titov'], 'affiliations': ['Nanjing University', 'Qwen Team, Alibaba Group', 'Shanghai Artificial Intelligence Laboratory', 'University of Amsterdam', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.02921.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#long_context', '#reasoning', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'LongBioBench: Новый стандарт оценки языковых моделей с длинным контекстом', 'desc': 'LongBioBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом, использующий искусственно сгенерированные биографии. Он оценивает модели по трем аспектам: понимание, рассуждение и надежность. Бенчмарк создан для преодоления ограничений существующих методов оценки, таких как сложность интерпретации реальных задач и недостаток когерентности в синтетических тестах. Эксперименты показали, что большинство моделей все еще имеют проблемы с семантическим пониманием и элементарными рассуждениями, а также становятся менее надежными при увеличении длины контекста.'}, 'en': {'title': 'LongBioBench: A New Standard for Evaluating Long-Context Language Models', 'desc': 'LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs.'}, 'zh': {'title': 'LongBioBench：评估长上下文语言模型的新基准', 'desc': 'LongBioBench 是一个新的基准，利用人工生成的传记来评估长上下文语言模型（LCLM）在理解、推理和可信度方面的表现，解决了现有框架的局限性。现有的评估框架分为真实世界任务和合成任务，但两者都有内在的缺陷。真实世界任务复杂且易受数据污染，而合成任务常常缺乏连贯性，影响其作为现实应用的有效性。LongBioBench 提供了一个受控环境，能够更好地评估 LCLM 的能力，实验结果显示大多数模型在语义理解和基本推理上仍存在不足。'}}}, {'id': 'https://huggingface.co/papers/2506.04141', 'title': "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos", 'url': 'https://huggingface.co/papers/2506.04141', 'abstract': 'A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  \t\t\t\t\tAI-generated summary \t\t\t\t The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.', 'score': 25, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a12411c7424fd2a7', 'authors': ['Kejian Zhu', 'Zhuoran Jin', 'Hongbang Yuan', 'Jiachun Li', 'Shangqing Tu', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04141.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#long_context', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'MMR-V: Новый рубеж в мультимодальных рассуждениях по видео', 'desc': 'Предложен новый бенчмарк MMR-V для оценки мультимодальных языковых моделей в задачах рассуждения по видео. Он требует анализа кадров, удаленных от упомянутых в вопросе, и выявления скрытой информации. Эксперименты показали, что современные модели испытывают трудности с такими задачами - лучшая достигла точности лишь 52.5%. Бенчмарк призван стимулировать исследования по улучшению навыков мультимодальных рассуждений у ИИ.'}, 'en': {'title': 'MMR-V: Elevating Multimodal Reasoning in Videos', 'desc': 'The paper introduces MMR-V, a new benchmark designed to test the capabilities of multimodal large language models (MLLMs) in video analysis. It emphasizes the need for long-range, multi-frame reasoning, where models must analyze evidence that is not immediately adjacent to the question frame. Unlike existing benchmarks that focus on simple understanding tasks, MMR-V requires models to reason about hidden information and avoid shortcuts through distractor annotations. The findings show that current models struggle with these challenges, achieving only modest accuracy, highlighting the need for further research in enhancing multimodal reasoning skills.'}, 'zh': {'title': 'MMR-V：推动多模态推理的新基准', 'desc': '本文提出了一个新的基准MMR-V，旨在挑战多模态大型语言模型在视频中的长距离、多帧推理和隐藏信息处理能力。现有的视频基准主要集中在理解任务上，而MMR-V要求模型进行更复杂的推理，分析与问题帧相距较远的证据帧。该基准包含317个视频和1257个任务，实验结果显示当前模型在多模态推理方面仍然存在困难，最佳模型的准确率仅为52.5%。我们希望MMR-V能够激发进一步研究，以提升多模态推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04180', 'title': 'SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.04180', 'abstract': 'Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.', 'score': 23, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '3f52b337c5fa3683', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04180.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#long_context', '#rlhf', '#benchmark', '#dataset', '#story_generation'], 'emoji': '✍️', 'ru': {'title': 'Структурированное мышление для улучшения генерации длинных текстов', 'desc': 'SuperWriter-Agent - это новая система для улучшения качества генерации длинных текстов с помощью больших языковых моделей (LLM). Она вводит этапы планирования и уточнения в процесс генерации, имитируя подход профессионального писателя. Авторы обучили 7B-параметровую модель SuperWriter-LM на специально созданном наборе данных и разработали иерархическую процедуру оптимизации предпочтений (DPO) с использованием метода Монте-Карло. Эмпирические результаты показывают, что SuperWriter-LM превосходит более крупные базовые модели по автоматическим и человеческим оценкам.'}, 'en': {'title': 'Elevating Long-Form Text Generation with Structured Thinking', 'desc': 'This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks.'}, 'zh': {'title': '提升长文本生成质量的智能代理', 'desc': '长文本生成是大型语言模型（LLMs）面临的重要挑战，尤其是在保持连贯性、逻辑一致性和文本质量方面。为了解决这些问题，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。该框架通过规划和精炼阶段引入明确的结构化思维，指导模型遵循更有意识和认知基础的过程，类似于专业作家的写作方式。实验结果表明，SuperWriter-LM在多个基准测试中表现出色，超越了更大规模的基线模型，证明了分层直接偏好优化（DPO）和结构化思维步骤的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.04142', 'title': 'Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis', 'url': 'https://huggingface.co/papers/2506.04142', 'abstract': 'A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation', 'score': 21, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '4799bf9d7a1cb57f', 'authors': ['Kejian Zhu', 'Shangqing Tu', 'Zhuoran Jin', 'Lei Hou', 'Juanzi Li', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04142.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#ethics', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с загрязнением данных в языковых моделях через патчинг нейронов-шорткатов', 'desc': "Метод 'shortcut neuron patching' идентифицирует и подавляет нейроны-шорткаты в языковых моделях для снижения проблем загрязнения данных при надежной оценке. Исследователи обнаружили, что переоценка загрязненных моделей вероятно связана с параметрами, приобретающими короткие пути решения при обучении. Предложенный метод показал высокую корреляцию с надежным бенчмарком MixEval, достигнув коэффициента Спирмена более 0,95. Эксперименты подтвердили эффективность подхода в снижении загрязнения данных и его обобщаемость на различные бенчмарки."}, 'en': {'title': 'Suppressing Shortcut Neurons for Trustworthy Evaluations', 'desc': 'This paper presents a method called shortcut neuron patching, which aims to identify and suppress shortcut neurons in language models to improve the reliability of evaluations. The authors highlight that current evaluation methods often suffer from data contamination, leading to unfair assessments of model performance. By analyzing the internal mechanisms of contaminated models, they find that shortcut solutions during training contribute to overestimation of model capabilities. Their proposed method effectively mitigates these issues, showing strong correlation with established trustworthy benchmarks, thus ensuring more accurate evaluations of language models.'}, 'zh': {'title': '抑制快捷神经元，提升评估可信度', 'desc': '本文提出了一种名为快捷神经元修补的方法，用于识别和抑制语言模型中的快捷神经元，以减轻数据污染问题，从而提高评估的可信度。当前的评估方法大多依赖公共基准，但这些基准容易受到数据污染的影响，导致评估结果不公平。我们通过比较和因果分析，发现训练过程中模型参数可能会获得快捷解决方案，从而导致对污染模型的过高估计。实验结果表明，我们的方法在减轻污染方面有效，并且与MixEval基准的评估结果具有很强的线性相关性，Spearman系数超过0.95，表明我们的方法能够真实反映模型的能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04178', 'title': 'OpenThoughts: Data Recipes for Reasoning Models', 'url': 'https://huggingface.co/papers/2506.04178', 'abstract': 'The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai.', 'score': 18, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '43845ce52af6bd6c', 'authors': ['Etash Guha', 'Ryan Marten', 'Sedrick Keh', 'Negin Raoof', 'Georgios Smyrnis', 'Hritik Bansal', 'Marianna Nezhurina', 'Jean Mercat', 'Trung Vu', 'Zayne Sprague', 'Ashima Suvarna', 'Benjamin Feuer', 'Liangyu Chen', 'Zaid Khan', 'Eric Frankel', 'Sachin Grover', 'Caroline Choi', 'Niklas Muennighoff', 'Shiye Su', 'Wanjia Zhao', 'John Yang', 'Shreyas Pimpalgaonkar', 'Kartik Sharma', 'Charlie Cheng-Jie Ji', 'Yichuan Deng', 'Sarah Pratt', 'Vivek Ramanujan', 'Jon Saad-Falcon', 'Jeffrey Li', 'Achal Dave', 'Alon Albalak', 'Kushal Arora', 'Blake Wulfe', 'Chinmay Hegde', 'Greg Durrett', 'Sewoong Oh', 'Mohit Bansal', 'Saadia Gabriel', 'Aditya Grover', 'Kai-Wei Chang', 'Vaishaal Shankar', 'Aaron Gokaslan', 'Mike A. Merrill', 'Tatsunori Hashimoto', 'Yejin Choi', 'Jenia Jitsev', 'Reinhard Heckel', 'Maheswaran Sathiamoorthy', 'Alexandros G. Dimakis', 'Ludwig Schmidt'], 'affiliations': ['ASU', 'BespokeLabs.ai', 'Cornell Tech', 'JSC', 'LAION', 'Lila Sciences', 'NYU', 'Open-Ψ (Open-Sci) Collective', 'Stanford University', 'TUM', 'Toyota Research Institute', 'UC Berkeley', 'UCLA', 'UNC Chapel Hill', 'UT Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04178.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#data', '#training', '#science', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Открытые данные для прорыва в ИИ-рассуждениях', 'desc': 'Проект OpenThoughts создал открытые наборы данных для обучения моделей рассуждений, которые соответствуют или превосходят современные эталоны в математике, программировании и науке. Исследователи разработали датасет OpenThoughts2-1M, который привел к созданию модели OpenThinker2-32B, сопоставимой с DeepSeek-R1-Distill-32B по стандартным показателям. Дальнейшее улучшение датасета и масштабирование процесса до 1,2 млн примеров позволило создать модель OpenThinker3-7B, достигающую наилучших результатов в тестах AIME, LiveCodeBench и GPQA Diamond. Все датасеты и модели проекта находятся в открытом доступе.'}, 'en': {'title': 'Open-Source Datasets for Superior Reasoning Models', 'desc': 'The OpenThoughts project focuses on creating open-source datasets to enhance reasoning models in math, code, and science. By developing the OpenThoughts2-1M dataset, they trained the OpenThinker2-32B model, which achieved performance comparable to proprietary models on standard benchmarks. The project further refined its dataset through extensive experimentation, resulting in the OpenThoughts3 dataset and the OpenThinker3-7B model. This model set new state-of-the-art results on multiple reasoning benchmarks, demonstrating the effectiveness of publicly available training data.'}, 'zh': {'title': '开源数据集助力推理模型突破极限', 'desc': 'OpenThoughts项目旨在创建开源数据集，以训练推理模型，达到或超过数学、代码和科学领域的最新基准。尽管推理模型在多个基准测试中取得了快速进展，但仍存在许多关于最佳训练方法的未解问题。通过系统地研究数据生成流程，OpenThoughts项目开发了多个版本的数据集，最终推出了OpenThinker3-7B模型，取得了在多个标准推理基准上的最佳成绩。所有数据集和模型均可在https://openthoughts.ai上获取。'}}}, {'id': 'https://huggingface.co/papers/2506.03150', 'title': 'IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation', 'url': 'https://huggingface.co/papers/2506.03150', 'abstract': 'IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page', 'score': 18, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '524d1d7f47dfcf7f', 'authors': ['Yuanze Lin', 'Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ronald Clark', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Google DeepMind', 'NEC Labs America', 'UC Merced', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.03150.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture', '#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Геометрия света: новый уровень контроля в генерации видео', 'desc': 'IllumiCraft - это инновационная диффузионная модель для генерации видео, интегрирующая геометрические подсказки для улучшения качества и временной согласованности. Модель использует три типа входных данных: HDR-карты видео для контроля освещения, синтетически переосвещенные кадры для подсказок внешнего вида и 3D-треки точек для геометрической информации. IllumiCraft позволяет создавать видео с управляемым освещением на основе текстовых запросов или изображений, превосходя существующие методы по качеству и контролируемости.'}, 'en': {'title': 'IllumiCraft: Crafting Coherent Videos with Geometric Precision', 'desc': 'IllumiCraft is a novel diffusion framework designed to create high-quality videos from text or image inputs while incorporating geometric cues. It utilizes three main inputs: HDR video maps for precise lighting control, synthetically relit frames for appearance variations, and 3D point tracks for accurate geometry representation. By merging these elements, IllumiCraft ensures that the generated videos are not only visually appealing but also maintain temporal coherence across frames. This approach enhances the fidelity of video generation compared to existing methods that lack such integrated controls.'}, 'zh': {'title': 'IllumiCraft：高保真视频生成的新方法', 'desc': 'IllumiCraft 是一个集成几何线索的扩散框架，能够从文本或图像输入生成高保真、时间一致的视频。该方法通过接受高动态范围（HDR）视频图像、合成重新照明的帧和3D点轨迹，来控制场景的光照和视觉外观。通过将光照、外观和几何线索整合在一个统一的扩散架构中，IllumiCraft 生成与用户定义提示一致的时间连贯视频。与现有的可控视频生成方法相比，它提供了更好的保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.04225', 'title': 'Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation', 'url': 'https://huggingface.co/papers/2506.04225', 'abstract': 'Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications.', 'score': 17, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '99f491949aa412fd', 'authors': ['Tianyu Huang', 'Wangguandong Zheng', 'Tengfei Wang', 'Yuhao Liu', 'Zhenwei Wang', 'Junta Wu', 'Jie Jiang', 'Hui Li', 'Rynson W. H. Lau', 'Wangmeng Zuo', 'Chunchao Guo'], 'affiliations': ['City University of Hong Kong, China', 'Harbin Institute of Technology, China', 'Southeast University, China', 'Tencent Hunyuan, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04225.jpg', 'data': {'categories': ['#games', '#dataset', '#diffusion', '#3d', '#video'], 'emoji': '🚀', 'ru': {'title': 'Исследуй 3D-миры из одного кадра', 'desc': 'Voyager - это система видеодиффузии, которая генерирует согласованные последовательности 3D-облаков точек из одного изображения. Она позволяет исследовать 3D-сцены на большие расстояния с пользовательскими траекториями камеры. Ключевые компоненты включают согласованную видеодиффузию, исследование мира на большие расстояния и масштабируемый механизм данных. Voyager превосходит существующие методы по визуальному качеству и геометрической точности, открывая новые возможности применения.'}, 'en': {'title': 'Voyager: Seamless 3D Scene Exploration from a Single Image', 'desc': 'Voyager is a video diffusion framework that creates 3D point-cloud sequences from a single image, allowing users to explore scenes along custom camera paths. It addresses the challenge of generating long-range, consistent 3D environments by integrating world-consistent video diffusion, which ensures alignment of RGB and depth sequences. The framework also features an efficient world cache for smooth scene exploration and a scalable data engine that automates camera pose estimation and depth prediction. Overall, Voyager enhances visual quality and geometric accuracy, making it suitable for applications in video gaming and virtual reality.'}, 'zh': {'title': 'Voyager：从单图像生成一致的3D场景探索', 'desc': 'Voyager是一种视频扩散框架，可以从单张图像生成世界一致的3D点云序列，支持用户定义的相机路径进行长距离的3D场景探索。该方法通过端到端的场景生成和重建，确保了帧间的一致性，避免了传统的3D重建流程。Voyager集成了三个关键组件：世界一致的视频扩散、长距离世界探索和可扩展的数据引擎，提升了视觉质量和几何精度。该框架在视频游戏和虚拟现实等应用中具有广泛的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.04158', 'title': 'Image Editing As Programs with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.04158', 'abstract': 'While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.', 'score': 15, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5a32d484bb427f3', 'authors': ['Yujia Hu', 'Songhua Liu', 'Zhenxiong Tan', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.04158.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента', 'desc': 'Исследователи представили новый подход к редактированию изображений с использованием искусственного интеллекта, названный IEAP (Image Editing As Programs). Эта система основана на архитектуре Diffusion Transformer и разбивает сложные инструкции по редактированию на последовательность простых операций. Каждая операция реализуется с помощью специализированного адаптера, использующего общий базовый DiT. IEAP значительно превосходит современные методы в различных задачах редактирования, особенно при сложных многоэтапных инструкциях.'}, 'en': {'title': 'Revolutionizing Image Editing with Programmatic Precision', 'desc': 'This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions.'}, 'zh': {'title': '图像编辑的新方法：将复杂指令转化为简单操作', 'desc': '本研究提出了一种新的图像编辑框架，称为图像编辑作为程序（IEAP），旨在解决扩散模型在指令驱动的图像编辑中面临的挑战。IEAP基于扩散变换器（DiT）架构，通过将复杂的编辑指令分解为一系列原子操作来实现。每个操作由轻量级适配器实现，专门针对特定类型的编辑，能够支持任意和结构不一致的变换。实验结果表明，IEAP在各种编辑场景中显著优于现有的最先进方法，尤其在处理复杂的多步骤指令时表现出更高的准确性和语义保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.03295', 'title': 'Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem', 'url': 'https://huggingface.co/papers/2506.03295', 'abstract': "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.", 'score': 15, 'issue_id': 4135, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '03df39687a4bdf5a', 'authors': ['Yubo Wang', 'Ping Nie', 'Kai Zou', 'Lijun Wu', 'Wenhu Chen'], 'affiliations': ['Independent', 'Netmind.AI', 'Shanghai AI Lab', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03295.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное раскрытие потенциала ИИ через обучение на критике', 'desc': 'Статья представляет метод Critique Fine-Tuning (CFT) для улучшения способностей рассуждения больших языковых моделей (LLM). CFT использует обучение на критических отзывах о решениях одной задачи, генерируемых моделью-учителем. Эксперименты показывают, что CFT значительно повышает производительность моделей на различных задачах рассуждения при меньших вычислительных затратах по сравнению с обучением с подкреплением. Результаты демонстрируют эффективность CFT как простого и общего подхода к раскрытию потенциала современных LLM.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Critique Fine-Tuning', 'desc': 'This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.'}, 'zh': {'title': '批评微调：高效释放语言模型推理潜力的利器', 'desc': '本文提出了一种名为批评微调（Critique Fine-Tuning, CFT）的方法，旨在高效提升大型语言模型（LLM）的推理能力。通过对单一问题进行微调，CFT能够显著提高模型在推理任务上的表现，同时减少计算成本。研究表明，使用CFT方法，模型在多个推理基准测试中平均提升了15%到16%。与传统的强化学习方法相比，CFT在计算资源上更加高效，展示了其作为一种简单且通用的推理能力提升策略的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.01320', 'title': 'Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models', 'url': 'https://huggingface.co/papers/2506.01320', 'abstract': 'We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.', 'score': 15, 'issue_id': 4136, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '6441248200df695a', 'authors': ['Taehoon Yoon', 'Yunhong Min', 'Kyeongmin Yeo', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01320.jpg', 'data': {'categories': ['#inference', '#alignment', '#rlhf', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Эффективное согласование наград в генеративных моделях с помощью умной выборки', 'desc': 'Статья представляет Psi-Sampler - новый метод для улучшения согласования наград при инференсе в генеративных моделях на основе оценок. Авторы предлагают использовать последовательный метод Монте-Карло (SMC) с инициализацией частиц из апостериорного распределения, учитывающего награду. Для эффективной выборки из апостериорного распределения в высокоразмерных латентных пространствах вводится алгоритм preconditioned Crank-Nicolson Langevin (pCNL). Эксперименты показывают улучшение результатов на различных задачах генерации изображений с учетом наград.'}, 'en': {'title': 'Enhancing Reward Alignment with Psi-Sampler', 'desc': 'The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.'}, 'zh': {'title': '高效的奖励对齐：Psi-Sampler框架', 'desc': '本文介绍了一种名为Psi-Sampler的框架，该框架基于序列蒙特卡洛（SMC）方法，并结合了基于奖励的初始粒子采样，以实现与基于分数的生成模型的有效推理时间奖励对齐。近年来，基于分数的生成模型在推理时间奖励对齐方面受到了广泛关注，标志着从预训练到后训练优化的范式转变。现有方法通常从高斯先验初始化粒子，这不足以捕捉与奖励相关的区域，导致采样效率降低。我们展示了从奖励感知后验初始化显著提高了对齐性能，并引入了预条件Crank-Nicolson Langevin（pCNL）算法，以实现高维潜在空间中的后验采样。'}}}, {'id': 'https://huggingface.co/papers/2506.03930', 'title': 'VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.03930', 'abstract': 'VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.', 'score': 14, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '56d407ae0e0e6b4a', 'authors': ['Yuansheng Ni', 'Ping Nie', 'Kai Zou', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.03930.jpg', 'data': {'categories': ['#story_generation', '#data', '#dataset', '#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'VisCode-200K: Большие данные для умного построения графиков', 'desc': 'VisCode-200K - это крупномасштабный набор данных для задач визуализации, который улучшает генерацию графиков с помощью обучения с подкреплением на основе выполнения кода и итеративной коррекции. Датасет содержит более 200 тысяч примеров из двух источников: проверенный код построения графиков из открытых репозиториев и диалоги по исправлению кода. На основе VisCode-200K была обучена модель VisCoder, которая превзошла открытые базовые модели и приблизилась к производительности проприетарных моделей. Исследование демонстрирует преимущества обучения на основе обратной связи для генерации исполняемого и визуально точного кода.'}, 'en': {'title': 'Empowering Visualization with VisCode-200K: A Leap in Plot Generation!', 'desc': 'The paper introduces VisCode-200K, a comprehensive dataset designed to enhance the performance of machine learning models in generating visualizations through improved code generation. It addresses the limitations of existing datasets by incorporating execution-grounded supervision and enabling iterative code correction, which helps models learn from their mistakes. The dataset consists of over 200,000 examples, including validated plotting code and multi-turn dialogues for code feedback. The authors demonstrate that their model, VisCoder, fine-tuned on this dataset, significantly outperforms existing open-source models and competes closely with proprietary ones in generating accurate visualizations.'}, 'zh': {'title': 'VisCode-200K：提升可视化生成的革命性数据集', 'desc': 'VisCode-200K是一个大规模的数据集，专注于可视化任务，旨在提高绘图生成的性能。该数据集结合了执行基础的监督和迭代代码修正，解决了现有模型在绘图时的脆弱性和不可靠性。它包含来自开源代码库的有效绘图代码和自然语言指令的配对，以及多轮修正对话，帮助模型修正错误代码。通过在VisCode-200K上微调模型，VisCoder在绘图生成方面显著超越了开源基线，接近商业模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04228', 'title': 'LayerFlow: A Unified Model for Layer-aware Video Generation', 'url': 'https://huggingface.co/papers/2506.04228', 'abstract': 'LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.', 'score': 13, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1e8f6532d8b54b21', 'authors': ['Sihui Ji', 'Hao Luo', 'Xi Chen', 'Yuanpeng Tu', 'Yiyang Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group, China', 'Hupan Laboratory, China', 'The University of Hong Kong', 'The University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.04228.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#synthetic', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'LayerFlow: Умная генерация многослойного видео по текстовым подсказкам', 'desc': 'LayerFlow - это унифицированная система для генерации видео с учетом слоев, использующая трансформер диффузии для преобразования текста в видео и встраивания слоев. Она поддерживает различные задачи генерации видео, включая создание прозрачного переднего плана, чистого фона и смешанной сцены. Система использует многоэтапную стратегию обучения для адаптации к статическим изображениям с высококачественными аннотациями слоев. LayerFlow применяет LoRA для настройки движения и содержания, что позволяет генерировать плавные видео с желаемыми слоями.'}, 'en': {'title': 'LayerFlow: Unified Layer-Aware Video Generation', 'desc': 'LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images.'}, 'zh': {'title': 'LayerFlow：统一的层感知视频生成框架', 'desc': 'LayerFlow是一个统一的框架，用于生成层感知的视频，利用文本到视频的扩散变换器和层嵌入。该框架支持多种视频生成任务，包括透明前景、干净背景和混合场景的视频生成。通过将视频按层组织为子剪辑，并利用层嵌入来区分每个剪辑及其对应的层级提示，LayerFlow实现了多种视频生成变体。为了克服高质量层级训练视频的缺乏，LayerFlow设计了多阶段训练策略，结合静态图像和高质量层注释进行训练。'}}}, {'id': 'https://huggingface.co/papers/2506.03139', 'title': 'SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation', 'url': 'https://huggingface.co/papers/2506.03139', 'abstract': 'SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.', 'score': 13, 'issue_id': 4137, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c1fdc3559598aa68', 'authors': ['Siqi Chen', 'Xinyu Dong', 'Haolei Xu', 'Xingyu Wu', 'Fei Tang', 'Hang Zhang', 'Yuchen Yan', 'Linjuan Wu', 'Wenqi Zhang', 'Guiyang Hou', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03139.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'SVGenius: комплексная оценка возможностей ИИ в работе с векторной графикой', 'desc': 'SVGenius - это комплексный бенчмарк для оценки способностей больших языковых моделей (LLM) и мультимодальных LLM в обработке SVG. Бенчмарк включает 2377 запросов в трех измерениях: понимание, редактирование и генерация SVG. Оценка проводится по 8 категориям задач и 18 метрикам на основе реальных данных из 24 прикладных областей. Результаты показывают, что проприетарные модели значительно превосходят открытые, но все модели демонстрируют снижение производительности с увеличением сложности задач.'}, 'en': {'title': 'SVGenius: Unveiling SVG Processing Potential in LLMs', 'desc': 'SVGenius is a benchmark designed to evaluate the performance of Large Language Models (LLMs) and Multimodal LLMs in processing Scalable Vector Graphics (SVG). It assesses models across three key dimensions: understanding, editing, and generation, using a total of 2,377 queries derived from real-world applications. The evaluation framework includes 8 task categories and 18 metrics, highlighting the strengths and weaknesses of 22 different models. Findings indicate that while proprietary models excel, all models struggle with complex tasks, suggesting a need for improved training methods, particularly in reasoning, to enhance their capabilities.'}, 'zh': {'title': 'SVGenius：全面评估 SVG 处理能力的基准工具', 'desc': 'SVGenius 是一个评估大型语言模型和多模态 LLM 在 SVG 处理能力的基准工具。它通过理解、编辑和生成三个维度，使用 2,377 个查询来全面评估模型的能力和局限性。研究发现，尽管专有模型在性能上优于开源模型，但所有模型在复杂性增加时表现普遍下降。SVGenius 提供了一个系统的评估框架，为开发更强大的矢量图形模型和推进自动化图形设计应用提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2506.03517', 'title': 'DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03517', 'abstract': 'Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.', 'score': 12, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '5be6b9aa57297fb6', 'authors': ['Ziyi Wu', 'Anil Kag', 'Ivan Skorokhodov', 'Willi Menapace', 'Ashkan Mirzaei', 'Igor Gilitschenski', 'Sergey Tulyakov', 'Aliaksandr Siarohin'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03517.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#rlhf'], 'emoji': '🎬', 'ru': {'title': 'DenseDPO: Точная оптимизация предпочтений для улучшения генерации видео', 'desc': 'DenseDPO - это новый метод для улучшения текст-в-видео диффузионных моделей. Он решает проблему смещения в сторону клипов с низкой подвижностью при аннотации предпочтений. DenseDPO создает пары видео путем очистки от шума искаженных копий исходного видео, что позволяет делать более точные сравнения. Метод также использует временное выравнивание для разметки предпочтений на коротких сегментах, а не на целых клипах. DenseDPO демонстрирует улучшенную генерацию движения по сравнению с обычным DPO, используя только треть размеченных данных.'}, 'en': {'title': 'Enhancing Video Generation with DenseDPO: Precision and Efficiency in Preference Learning', 'desc': 'This paper introduces DenseDPO, an improved method for Direct Preference Optimization (DPO) in text-to-video diffusion models. DenseDPO addresses the limitations of traditional DPO by creating video pairs from denoised versions of a ground truth video, allowing for better alignment and reducing bias towards low-motion clips. It also enables preference labeling on shorter video segments, which provides a more detailed learning signal while using less labeled data. Additionally, DenseDPO facilitates automatic preference annotation through Vision Language Models, achieving performance comparable to human-labeled data.'}, 'zh': {'title': 'DenseDPO：提升视频生成的偏好优化方法', 'desc': '直接偏好优化（DPO）最近被应用于文本到视频的扩散模型后训练技术。我们提出的DenseDPO方法通过三项贡献解决了DPO的不足之处。首先，我们通过去噪真实视频的损坏副本来创建视频对，从而消除了运动偏差。其次，我们利用时间对齐来标记短片段的偏好，使学习信号更加密集和精确，最终DenseDPO在运动生成方面显著优于传统DPO，同时在文本对齐、视觉质量和时间一致性方面表现相当。'}}}, {'id': 'https://huggingface.co/papers/2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", 'url': 'https://huggingface.co/papers/2505.24500', 'abstract': "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'score': 11, 'issue_id': 4133, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ee580986393d0b7e', 'authors': ['Guiyang Hou', 'Xing Gao', 'Yuchuan Wu', 'Xiang Huang', 'Wenqi Zhang', 'Zhe Zheng', 'Yongliang Shen', 'Jialu Du', 'Fei Huang', 'Yongbin Li', 'Weiming Lu'], 'affiliations': ['Nanjing University', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24500.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый метод обучения для повышения социального интеллекта языковых моделей', 'desc': 'Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения социального интеллекта больших языковых моделей (LLM). Этот подход учитывает временную составляющую и иерархию когнитивных процессов, характерных для социальных взаимодействий. Метод TimeHC-RL показал превосходство над широко используемым методом обучения с подкреплением System 2 RL. Эксперименты продемонстрировали, что применение TimeHC-RL позволяет моделям с 7 миллиардами параметров достигать производительности передовых моделей в задачах социального интеллекта.'}, 'en': {'title': "Boosting LLMs' Social Intelligence with TimeHC-RL", 'desc': "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."}, 'zh': {'title': '提升社交智能的时间感知强化学习', 'desc': '本文提出了一种新的方法，称为时间感知层次认知强化学习（TimeHC-RL），旨在提升大型语言模型（LLMs）在社交领域的智能。与数学等依赖系统2认知的领域不同，社交领域需要更丰富的认知模式，包括直觉反应和表层思维。通过对八个不同数据集的实验，我们验证了TimeHC-RL方法的有效性，结果显示其在社交智能方面优于传统的系统2强化学习方法。该方法使得7B基础模型的表现接近于更先进的模型，如DeepSeek-R1和OpenAI-O3。'}}}, {'id': 'https://huggingface.co/papers/2506.04108', 'title': 'Rectified Sparse Attention', 'url': 'https://huggingface.co/papers/2506.04108', 'abstract': 'Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ff7222f16cd2bf28', 'authors': ['Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Jian Chen', 'Yizhao Gao', 'Shijie Cao', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04108.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Эффективная генерация длинных текстов без потери качества', 'desc': 'Метод Rectified Sparse Attention (ReSA) улучшает эффективность генерации длинных последовательностей в больших языковых моделях. Он сочетает блочно-разреженное внимание с периодической плотной ректификацией, что позволяет ограничить накопление ошибок. ReSA достигает качества генерации, близкого к безошибочному, при значительном повышении эффективности. Метод обеспечивает ускорение до 2,42 раза при декодировании последовательностей длиной 256 тысяч токенов.'}, 'en': {'title': 'Boosting Efficiency in Long-Sequence Generation with ReSA', 'desc': "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."}, 'zh': {'title': '高效长序列生成的新方法：ReSA', 'desc': 'Rectified Sparse Attention（ReSA）是一种提高大型语言模型长序列生成效率的方法。它结合了块稀疏注意力和周期性密集整流，能够保持高质量的生成效果。通过在固定间隔内使用密集前向传递刷新KV缓存，ReSA限制了误差累积，并保持与预训练分布的对齐。实验表明，ReSA在数学推理、语言建模和检索任务中实现了接近无损的生成质量，并在256K序列长度下提供了高达2.42倍的端到端加速。'}}}, {'id': 'https://huggingface.co/papers/2506.02592', 'title': 'Beyond the Surface: Measuring Self-Preference in LLM Judgments', 'url': 'https://huggingface.co/papers/2506.02592', 'abstract': 'The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'ccdb2761a23fe0c8', 'authors': ['Zhi-Yuan Chen', 'Hao Wang', 'Xinyu Zhang', 'Enrui Hu', 'Yankai Lin'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.02592.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#interpretability', '#ethics', '#training'], 'emoji': '⚖️', 'ru': {'title': 'DBG: Новый способ измерения предвзятости в языковых моделях', 'desc': 'Статья представляет новый метод измерения предвзятости самопредпочтения в больших языковых моделях (LLM). Авторы вводят показатель DBG, который использует эталонные оценки в качестве прокси для качества ответов. Этот подход позволяет отделить влияние качества ответов от фактической предвзятости модели. Исследователи провели эксперименты с различными LLM и изучили факторы, влияющие на предвзятость самопредпочтения.'}, 'en': {'title': 'Measuring Self-Preference Bias with the DBG Score', 'desc': 'This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses.'}, 'zh': {'title': '引入DBG评分，精准测量自我偏好偏差', 'desc': '本文提出了DBG评分，用于测量大型语言模型中的自我偏好偏差。通过使用金标准判断作为响应质量的代理，DBG评分解决了响应质量对偏差测量的混淆效应。研究表明，现有方法在评估自我偏好偏差时，往往将其与响应质量混为一谈。我们通过实验评估了不同版本、规模和推理能力的语言模型的自我偏好偏差，并探讨了影响该偏差的因素。'}}}, {'id': 'https://huggingface.co/papers/2506.03610', 'title': 'Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on\n  Diverse Video Games', 'url': 'https://huggingface.co/papers/2506.03610', 'abstract': 'Orak is a benchmark for training and evaluating LLM agents across diverse video games, featuring a plug-and-play interface and fine-tuning datasets to enhance agentic modules and gameplay.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present \\benchname{}, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.', 'score': 7, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '8821e76acd2443a8', 'authors': ['Dongmin Park', 'Minkyu Kim', 'Beongjun Choi', 'Junhyuck Kim', 'Keon Lee', 'Jonghyun Lee', 'Inkyu Park', 'Byeong-Uk Lee', 'Jaeyoung Hwang', 'Jaewoo Ahn', 'Ameya S. Mahabaleshwarkar', 'Bilal Kartal', 'Pritam Biswas', 'Yoshi Suhara', 'Kangwook Lee', 'Jaewoong Cho'], 'affiliations': ['KRAFTON', 'NVIDIA', 'Seoul National University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.03610.jpg', 'data': {'categories': ['#games', '#agents', '#video', '#benchmark', '#transfer_learning'], 'emoji': '🎮', 'ru': {'title': 'Orak: универсальный бенчмарк для создания игровых ИИ-агентов нового поколения', 'desc': 'Orak - это новый бенчмарк для обучения и оценки агентов на основе больших языковых моделей (LLM) в разнообразных видеоиграх. Он включает в себя интерфейс plug-and-play и наборы данных для тонкой настройки, чтобы улучшить агентские модули и игровой процесс. Orak охватывает 12 популярных видеоигр различных жанров, что позволяет проводить комплексные исследования возможностей LLM и агентских модулей. Бенчмарк также предлагает всестороннюю систему оценки, включая таблицы лидеров по общим игровым показателям, арены для сражений LLM и углубленный анализ визуального ввода, агентских стратегий и эффектов тонкой настройки.'}, 'en': {'title': 'Orak: Elevating LLM Agents in Gaming', 'desc': 'Orak is a new benchmark designed for training and evaluating Large Language Model (LLM) agents in various video games. It addresses the limitations of existing benchmarks by providing a plug-and-play interface and fine-tuning datasets that enhance the performance of LLMs in complex gameplay scenarios. The benchmark includes 12 popular video games from different genres, allowing for a thorough assessment of LLM capabilities and agentic modules. With features like game score leaderboards and detailed analyses of gameplay strategies, Orak aims to advance the development of intelligent gaming agents.'}, 'zh': {'title': 'Orak：多样化视频游戏的LLM代理基准', 'desc': 'Orak是一个用于训练和评估大型语言模型（LLM）代理的基准，涵盖多种视频游戏。它提供了即插即用的接口和微调数据集，以增强代理模块和游戏玩法。与现有基准不同，Orak支持12款流行视频游戏，允许对LLM能力和代理模块进行全面研究。该基准还引入了基于模型上下文协议（MCP）的接口，确保LLM能够与游戏无缝连接并操作代理模块。'}}}, {'id': 'https://huggingface.co/papers/2506.03099', 'title': 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03099', 'abstract': 'TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/', 'score': 7, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'eff27ca5fef5cdcf', 'authors': ['Chetwin Low', 'Weimin Wang'], 'affiliations': ['Character AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03099.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#inference', '#games', '#audio', '#video', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Оживляем аватары: аудио-управляемая генерация видео в реальном времени', 'desc': 'TalkingMachines - это эффективная система, преобразующая предобученные модели генерации видео в аниматоры персонажей, управляемые аудио в реальном времени. Она адаптирует современную модель преобразования изображений в видео DiT для генерации аватаров на основе аудио с 18 миллиардами параметров. Система обеспечивает бесконечную потоковую передачу видео без накопления ошибок с помощью асимметричной дистилляции знаний. TalkingMachines также включает ряд инженерных оптимизаций для высокопроизводительного вывода с низкой задержкой.'}, 'en': {'title': 'Transforming Audio into Real-Time Avatar Animation', 'desc': 'TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays.'}, 'zh': {'title': '实时音频驱动的角色动画生成器', 'desc': '本文介绍了TalkingMachines，这是一个高效的框架，将预训练的视频生成模型转变为实时的音频驱动角色动画生成器。通过将音频大型语言模型（LLM）与视频生成基础模型结合，TalkingMachines能够实现自然的对话体验。我们的主要贡献包括：将一个预训练的最先进的图像到视频模型适配为一个具有180亿参数的音频驱动头像生成模型，以及通过不对称知识蒸馏实现无限视频流的生成。我们还设计了一个高吞吐量、低延迟的推理管道，结合了多项关键的工程优化。'}}}, {'id': 'https://huggingface.co/papers/2506.03355', 'title': 'Robustness in Both Domains: CLIP Needs a Robust Text Encoder', 'url': 'https://huggingface.co/papers/2506.03355', 'abstract': 'LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.', 'score': 6, 'issue_id': 4140, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'd9088aaea42f6fff', 'authors': ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher'], 'affiliations': ['LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland', 'Tubingen AI center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03355.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#diffusion', '#rlhf', '#security'], 'emoji': '🛡️', 'ru': {'title': 'LEAF: Повышение устойчивости CLIP к состязательным атакам', 'desc': 'LEAF - это метод состязательной доводки (adversarial finetuning), который повышает устойчивость текстовых энкодеров CLIP. Он улучшает точность классификации с нулевым обучением (zero-shot accuracy) и производительность мультимодального поиска в условиях состязательного шума. LEAF эффективно масштабируется на большие модели CLIP и сохраняет исходную производительность для визуальных задач. Метод также улучшает качество генерации изображений по тексту и реконструкцию текста из эмбеддингов в условиях шума.'}, 'en': {'title': 'Enhancing Text Encoder Robustness with LEAF', 'desc': 'This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.'}, 'zh': {'title': 'LEAF：提升CLIP文本编码器鲁棒性的对抗微调方法', 'desc': 'LEAF是一种对抗微调方法，旨在增强CLIP文本编码器的鲁棒性。通过对抗噪声的训练，LEAF显著提高了文本领域的零-shot准确率和多模态检索性能。该方法不仅保持了图像编码器的视觉性能，还能在文本到图像生成模型中提升生成质量。我们的研究填补了文本编码器鲁棒性研究的空白，展示了其在多模态任务中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.03106', 'title': 'Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback', 'url': 'https://huggingface.co/papers/2506.03106', 'abstract': 'Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.', 'score': 6, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '99f5fb3b08ab4205', 'authors': ['Xiaoying Zhang', 'Hao Sun', 'Yipeng Zhang', 'Kaituo Feng', 'Chaochao Lu', 'Chao Yang', 'Helen Meng'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, HCCL', 'The Chinese University of Hong Kong, MMLab', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.03106.jpg', 'data': {'categories': ['#math', '#rl', '#reasoning', '#optimization', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Critique-GRPO: Улучшение рассуждений ИИ через комбинированную обратную связь', 'desc': 'Статья представляет Critique-GRPO - новую систему обучения с подкреплением для улучшения рассуждений больших языковых моделей. Эта система объединяет числовую и текстовую обратную связь, что позволяет преодолеть ограничения существующих методов. Эксперименты показывают, что Critique-GRPO превосходит другие подходы на основе обучения с учителем и обучения с подкреплением на различных задачах рассуждения. Исследование также выявляет важные аспекты исследовательского поведения модели в процессе обучения.'}, 'en': {'title': 'Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach', 'desc': 'Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks.'}, 'zh': {'title': 'Critique-GRPO：自然语言与数值反馈的完美结合', 'desc': 'Critique-GRPO是一种结合数值反馈和自然语言反馈的强化学习框架，旨在提升大型语言模型（LLM）的推理能力。该框架解决了仅依赖数值反馈时遇到的性能停滞、自我反思效果有限和持续失败等挑战。通过利用自然语言反馈，Critique-GRPO能够在模型表现停滞时，生成正确的改进建议。实验结果表明，Critique-GRPO在多个复杂任务中表现优于现有的监督学习和强化学习方法，显著提高了模型的平均通过率。'}}}, {'id': 'https://huggingface.co/papers/2506.03956', 'title': 'Adapt before Continual Learning', 'url': 'https://huggingface.co/papers/2506.03956', 'abstract': 'Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL.', 'score': 5, 'issue_id': 4138, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '33f8cb4049923c1f', 'authors': ['Aojun Lu', 'Tao Feng', 'Hangjie Yuan', 'Chunhui Ding', 'Yanan Sun'], 'affiliations': ['College of Computer Science Sichuan University Chengdu, China', 'College of Computer Science and Technology Zhejiang University Hangzhou, China', 'Department of Computer Science and Technology Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03956.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Балансировка стабильности и пластичности в непрерывном обучении', 'desc': 'Статья представляет новый подход к непрерывному обучению (Continual Learning) с использованием предобученных моделей. Авторы предлагают метод ACL (Adapting Pre-trained Models before the core CL process), который адаптирует основу предобученной модели перед обучением каждой новой задачи. ACL улучшает пластичность модели, выравнивая эмбеддинги с их исходными прототипами классов, при этом сохраняя стабильность. Эксперименты показывают, что ACL значительно повышает производительность непрерывного обучения на различных бенчмарках.'}, 'en': {'title': 'Enhancing Learning Flexibility with Pre-trained Models', 'desc': 'This paper introduces a new method called Adapting Pre-trained Models before the core Continual Learning (CL) process, which aims to improve how neural networks learn new information while keeping what they already know. The authors highlight the common issue where pre-trained models are often frozen to maintain stability, which limits their ability to adapt to new tasks. Their approach involves refining the pre-trained model before learning new tasks, allowing for better alignment of knowledge and reducing the risk of forgetting previous information. The results show that this method enhances the performance of CL systems, making it a promising solution for integrating pre-trained models in continual learning scenarios.'}, 'zh': {'title': '提升持续学习的可塑性与稳定性', 'desc': '这篇论文提出了一种新的框架，称为在核心持续学习过程之前调整预训练模型（ACL）。该方法旨在提高神经网络的可塑性，同时保持其稳定性，以便在增量学习中更好地适应新知识。通过在学习每个新任务之前对预训练模型进行适应性调整，ACL能够有效地对齐嵌入与原始类别原型，从而减少灾难性遗忘。实验结果表明，ACL在多个基准测试中显著提升了持续学习的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.21541', 'title': 'DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.21541', 'abstract': 'DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.', 'score': 5, 'issue_id': 4133, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'cda6015909393ad0', 'authors': ['Zitong Wang', 'Hang Zhao', 'Qianyu Zhou', 'Xuequan Lu', 'Xiangtai Li', 'Yiren Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.21541.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Умное разделение изображений на слои с помощью ИИ', 'desc': 'DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблемы разделения полупрозрачных и прозрачных наложений, с которыми не справлялись предыдущие методы. Авторы создали датасет AlphaBlend для обучения модели работе с различными типами прозрачности. DiffDecompose использует условное генерирование и позиционное кодирование слоев для точного восстановления составляющих изображения.'}, 'en': {'title': 'Revolutionizing Image Layer Decomposition with DiffDecompose', 'desc': "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."}, 'zh': {'title': '透明层分解的新突破：DiffDecompose', 'desc': 'DiffDecompose 是一个基于扩散 Transformer 的框架，能够有效地将图像分解为组成层，并使用语义提示来解决透明层分解中的挑战。该方法针对半透明和透明图层的非线性遮挡问题，提出了一种新的任务：逐层分解 alpha 合成图像。为了解决层模糊、泛化能力和数据稀缺的问题，研究者们首次引入了 AlphaBlend 数据集，支持多种实际应用场景。DiffDecompose 通过上下文分解的方法，能够在没有逐层监督的情况下预测一个或多个层，展示了其在图像分解任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03448', 'title': 'RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions', 'url': 'https://huggingface.co/papers/2506.03448', 'abstract': 'RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.', 'score': 4, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '717f877ff02ce882', 'authors': ['Bimsara Pathiraja', 'Maitreya Patel', 'Shivam Singh', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03448.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#cv', '#optimization', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'RefEdit: Прорыв в редактировании сложных изображений с помощью ИИ', 'desc': 'RefEdit - это модель редактирования изображений на основе инструкций, обученная на синтетических данных. Она превосходит базовые модели в задачах редактирования сложных сцен и работы с референсными выражениями. Авторы представили новый бенчмарк RefEdit-Bench для оценки таких моделей. RefEdit, обученная всего на 20 000 примерах, превзошла модели, обученные на миллионах образцов.'}, 'en': {'title': 'Revolutionizing Image Editing with Instruction-Based Learning', 'desc': 'RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios.'}, 'zh': {'title': 'RefEdit：复杂场景编辑的新突破', 'desc': 'RefEdit是一种基于指令的编辑模型，专门针对复杂场景中的编辑任务进行训练。与传统方法相比，RefEdit在处理多个实体的复杂场景时表现更为出色。我们还引入了RefEdit-Bench，这是一个基于RefCOCO的真实世界基准，用于量化现有方法的不足。通过使用合成数据生成管道，RefEdit在仅使用20,000个编辑三元组的情况下，超越了基于Flux/SD3模型的基线，展示了其在指代表达任务和传统基准上的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02945', 'title': 'Quantitative LLM Judges', 'url': 'https://huggingface.co/papers/2506.02945', 'abstract': "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.", 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'de4ea9c8e4abb76a', 'authors': ['Aishwarya Sahoo', 'Jeevana Kruthi Karnuthala', 'Tushar Parmanand Budhwani', 'Pranchal Agarwal', 'Sankaran Vaidyanathan', 'Alexa Siu', 'Franck Dernoncourt', 'Jennifer Healey', 'Nedim Lipka', 'Ryan Rossi', 'Uttaran Bhattacharya', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.02945.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rlhf', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии', 'desc': 'Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой модели. Авторы предлагают количественных LLM-судей, которые согласуют оценки существующих судей с человеческими оценками в заданной области с помощью регрессионных моделей. Представлены четыре количественных судьи для различных типов абсолютной и относительной обратной связи. Эксперименты показывают, что количественные судьи могут эффективно улучшить предсказательную силу существующих судей через постобработку.'}, 'en': {'title': 'Enhancing LLM Evaluation with Quantitative Judges', 'desc': "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."}, 'zh': {'title': '利用LLM提升评估效率的创新框架', 'desc': '本文提出了一种名为LLM-as-a-judge的框架，利用大型语言模型（LLM）自动评估另一个LLM的输出。我们引入了定量LLM评估者，通过回归模型将现有评估者的评分与人类评分对齐。该模型通过使用评估者的文本评价和评分来提高原始评估者的评分。我们的框架在计算效率上优于监督微调，并且在人工反馈有限的情况下，统计效率更高，适用于大多数应用场景。'}}}, {'id': 'https://huggingface.co/papers/2506.02294', 'title': 'Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation', 'url': 'https://huggingface.co/papers/2506.02294', 'abstract': 'A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '740d99ccc158d514', 'authors': ['Niclas Popp', 'Kevin Alexander Laube', 'Matthias Hein', 'Lukas Schott'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.02294.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение устойчивости моделей через генерацию сложных примеров', 'desc': 'Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения робастности в процессе дистилляции знаний. Метод генерирует сложные образцы, максимизируя разногласие между учителем и учеником, что помогает преодолеть проблему ковариационного сдвига. Эксперименты показывают значительное улучшение точности на наихудших группах и средней точности по группам на датасетах CelebA и SpuCo Birds. Подход превосходит современные методы аугментации данных на основе диффузии.'}, 'en': {'title': 'Boosting Student Robustness with Diffusion Data Augmentation', 'desc': 'This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods.'}, 'zh': {'title': '基于扩散的数据增强提升知识蒸馏鲁棒性', 'desc': '本文提出了一种基于扩散的数据增强策略，以提高知识蒸馏中的鲁棒性。该方法通过生成具有挑战性的样本，增强了学生网络对虚假特征的抵抗力。实验结果表明，在CelebA和SpuCo Birds数据集上，该策略显著提高了最差组和平均组的准确率。通过最大化教师和学生之间的分歧，本文有效地解决了知识蒸馏中的协变量偏移问题。'}}}, {'id': 'https://huggingface.co/papers/2506.00482', 'title': 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.00482', 'abstract': 'BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.', 'score': 4, 'issue_id': 4137, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0f7e970118d80f26', 'authors': ['Eunsu Kim', 'Haneul Yoo', 'Guijin Son', 'Hitesh Patel', 'Amit Agarwal', 'Alice Oh'], 'affiliations': ['KAIST', 'OnelineAI', 'Oracle', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00482.jpg', 'data': {'categories': ['#optimization', '#dataset', '#survey', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'BenchHub: Универсальный инструмент для оценки языковых моделей', 'desc': 'BenchHub - это динамическое хранилище бенчмарков, которое агрегирует и классифицирует наборы данных для больших языковых моделей (LLM). Оно включает в себя 303 тысячи вопросов из 38 бенчмарков различных предметных областей. BenchHub позволяет проводить гибкую и настраиваемую оценку моделей, адаптированную под конкретные домены или сценарии использования. Эксперименты показали, что производительность моделей значительно варьируется в зависимости от предметной области, подчеркивая важность домен-ориентированного бенчмаркинга.'}, 'en': {'title': 'BenchHub: Streamlining Domain-Specific Evaluations for LLMs', 'desc': 'BenchHub is a repository designed to organize and classify datasets specifically for evaluating large language models (LLMs). It addresses the challenge of scattered and hard-to-manage datasets, which complicate domain-specific evaluations. By aggregating 303K questions across 38 benchmarks, BenchHub allows for flexible and customizable assessments tailored to various domains. The paper highlights the importance of domain-aware benchmarking, showing that model performance can vary significantly based on the specific dataset used.'}, 'zh': {'title': 'BenchHub：提升语言模型评估的动态基准库', 'desc': 'BenchHub是一个动态基准库，专门用于聚合和分类大型语言模型的数据集，旨在促进特定领域的评估并改善模型比较。随着大型语言模型的不断进步，更新和组织良好的基准变得越来越重要。BenchHub集成了来自38个基准的303K问题，支持持续更新和可扩展的数据管理，允许根据不同领域或用例进行灵活的评估。通过对不同语言模型的广泛实验，我们展示了模型性能在特定领域子集之间的显著差异，强调了领域感知基准的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'url': 'https://huggingface.co/papers/2505.23807', 'abstract': 'A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a817afc0cdd35d8d', 'authors': ['Yuli Chen', 'Bo Cheng', 'Jiale Han', 'Yingying Zhang', 'Yingting Li', 'Shuhao Zhang'], 'affiliations': ['Hong Kong University of Science and Technology, Hong Kong, China', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23807.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка слоев для эффективных языковых моделей', 'desc': 'Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбинируя информацию о весах модели и активациях. Это позволяет сохранить производительность модели при высоком уровне разреженности. Эксперименты показали, что DLP превосходит существующие методы обрезки для различных языковых моделей.'}, 'en': {'title': 'Dynamic Layerwise Pruning: Smart Sparsity for Language Models', 'desc': 'This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques.'}, 'zh': {'title': '动态剪枝，智能保持性能！', 'desc': '动态层级剪枝方法通过结合模型权重和激活信息，自适应地确定每一层的重要性，从而在高稀疏性下保持大型语言模型的性能。传统的剪枝技术通常采用均匀层级剪枝策略，这可能导致在高稀疏性水平下性能显著下降。动态层级剪枝（DLP）方法克服了这一限制，能够根据输入激活信息动态调整剪枝率。实验结果表明，DLP在多个大型语言模型中有效地保持了高稀疏性下的模型性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04133', 'title': 'TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2506.04133', 'abstract': 'A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.', 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a0a258935ed39508', 'authors': ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis'], 'affiliations': ['Cornell University, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2506.04133.jpg', 'data': {'categories': ['#training', '#architecture', '#survey', '#agents', '#multimodal', '#security', '#alignment', '#benchmark', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Безопасность и доверие в эпоху агентного ИИ', 'desc': 'Статья представляет структурированный анализ управления доверием, рисками и безопасностью (TRiSM) в контексте агентных мультиагентных систем на основе больших языковых моделей (LLM). Рассматриваются четыре основных аспекта: управление, объяснимость, ModelOps и конфиденциальность/безопасность. Авторы идентифицируют уникальные векторы угроз и представляют комплексную таксономию рисков для приложений агентного ИИ. Статья также исследует механизмы построения доверия, методы обеспечения прозрачности и надзора, а также современные стратегии объяснимости в распределенных системах агентов LLM.'}, 'en': {'title': 'Navigating Trust and Security in Agentic AI Systems', 'desc': 'This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment.'}, 'zh': {'title': '构建安全透明的代理人工智能系统', 'desc': '本文回顾了基于大型语言模型（LLM）的代理多智能体系统中的信任、风险和安全管理（TRiSM）。我们分析了代理人工智能的概念基础及其与传统人工智能代理的架构差异，并探讨了支持可扩展自主性的系统设计。文章详细阐述了TRiSM的四个支柱：治理、可解释性、模型操作和隐私/安全，并为代理LLM提供了具体的背景。最后，提出了负责任的代理人工智能的路线图，建议研究方向以确保新兴多智能体系统的安全、透明和负责任的部署。'}}}, {'id': 'https://huggingface.co/papers/2506.03837', 'title': 'HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature\n  Superconductors for AI-Driven Critical Temperature Prediction', 'url': 'https://huggingface.co/papers/2506.03837', 'abstract': 'HTSC-2025, a benchmark dataset for high-temperature superconducting materials, is presented to facilitate AI-based discovery in this field.  \t\t\t\t\tAI-generated summary \t\t\t\t The discovery of high-temperature superconducting materials holds great significance for human industry and daily life. In recent years, research on predicting superconducting transition temperatures using artificial intelligence~(AI) has gained popularity, with most of these tools claiming to achieve remarkable accuracy. However, the lack of widely accepted benchmark datasets in this field has severely hindered fair comparisons between different AI algorithms and impeded further advancement of these methods. In this work, we present the HTSC-2025, an ambient-pressure high-temperature superconducting benchmark dataset. This comprehensive compilation encompasses theoretically predicted superconducting materials discovered by theoretical physicists from 2023 to 2025 based on BCS superconductivity theory, including the renowned X_2YH_6 system, perovskite MXH_3 system, M_3XH_8 system, cage-like BCN-doped metal atomic systems derived from LaH_{10} structural evolution, and two-dimensional honeycomb-structured systems evolving from MgB_2. The HTSC-2025 benchmark has been open-sourced at https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This benchmark holds significant importance for accelerating the discovery of superconducting materials using AI-based methods.', 'score': 3, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e8db15217cfd8461', 'authors': ['Xiao-Qi Han', 'Ze-Feng Gao', 'Xin-De Wang', 'Zhenfeng Ouyang', 'Peng-Jie Guo', 'Zhong-Yi Lu'], 'affiliations': ['Hefei National Laboratory, Hefei 230088, China', 'Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China', 'School of Physics and Beijing Key Laboratory of Opto-electronic Functional Materials & Micro-nano Devices, Renmin University of China, Beijing 100872, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03837.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#science'], 'emoji': '⚡', 'ru': {'title': 'HTSC-2025: эталонный набор данных для прорыва в сверхпроводимости', 'desc': 'Представлен набор данных HTSC-2025 для исследования высокотемпературных сверхпроводящих материалов с помощью методов искусственного интеллекта. Этот набор включает теоретически предсказанные сверхпроводящие материалы, открытые физиками-теоретиками с 2023 по 2025 год на основе теории сверхпроводимости БКШ. HTSC-2025 охватывает различные системы, включая X_2YH_6, перовскитную MXH_3, M_3XH_8 и другие. Данный бенчмарк имеет большое значение для ускорения открытия сверхпроводящих материалов с использованием методов, основанных на ИИ.'}, 'en': {'title': 'Accelerating Superconductor Discovery with HTSC-2025', 'desc': 'The paper introduces HTSC-2025, a new benchmark dataset designed for high-temperature superconducting materials to enhance AI-driven research in this area. It addresses the current challenge of insufficient benchmark datasets, which limits the ability to compare different AI algorithms effectively. The dataset includes a variety of theoretically predicted superconductors, derived from advanced theories and recent discoveries. By providing this resource, the authors aim to facilitate faster and more accurate discoveries of superconducting materials using artificial intelligence techniques.'}, 'zh': {'title': 'HTSC-2025：加速高温超导材料发现的基准数据集', 'desc': 'HTSC-2025是一个用于高温超导材料的基准数据集，旨在促进基于人工智能的发现。该数据集包含了2023至2025年间理论物理学家预测的超导材料，基于BCS超导理论。通过提供一个统一的基准，HTSC-2025可以帮助不同的AI算法进行公平比较，从而推动该领域的进一步发展。该数据集已开源，并将持续更新，以加速超导材料的发现。'}}}, {'id': 'https://huggingface.co/papers/2506.03566', 'title': 'POSS: Position Specialist Generates Better Draft for Speculative\n  Decoding', 'url': 'https://huggingface.co/papers/2506.03566', 'abstract': 'Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.', 'score': 3, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '16e2306b3d98ed8c', 'authors': ['Langlin Huang', 'Chengsong Huang', 'Jixuan Leng', 'Di Huang', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2506.03566.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Специалисты по позициям: точность предсказания токенов на новом уровне', 'desc': 'Статья представляет новый метод под названием Position Specialists (PosS) для улучшения процесса вывода в больших языковых моделях (LLM). PosS использует несколько специализированных слоев для генерации токенов на определенных позициях, что повышает точность предсказания и уровень принятия токенов. Этот подход особенно эффективен для токенов на более поздних позициях, где обычно наблюдается накопление ошибок. Эксперименты на моделях Llama показали, что PosS превосходит базовые методы по средней длине принятия и коэффициенту ускорения.'}, 'en': {'title': 'Enhancing Token Prediction with Position Specialists', 'desc': 'This paper introduces Position Specialists (PosS), a method that enhances the performance of Large Language Models (LLMs) during inference. By utilizing position-specialized draft layers, PosS improves the accuracy of token predictions, particularly at later positions where traditional methods struggle due to error accumulation. The approach allows each specialist to focus on specific positions, leading to higher acceptance rates for generated tokens. Experimental results show that PosS outperforms existing methods in both acceptance length and inference speed across multiple datasets.'}, 'zh': {'title': '位置专家提升语言模型推理效率', 'desc': '本文提出了一种名为位置专家（PosS）的新方法，旨在通过使用位置专用的草稿层来提高大型语言模型（LLM）的推理性能。位置专家能够在特定位置生成更准确的标记，从而提高标记的接受率，尤其是在后期位置。通过专注于处理草稿模型特征的偏差，每个专家可以有效减少错误累积带来的影响。实验结果表明，PosS在多个数据集上显著优于基线模型，提升了平均接受长度和加速比。'}}}, {'id': 'https://huggingface.co/papers/2506.03525', 'title': 'Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.03525', 'abstract': 'Video-SKoT framework improves domain-adaptive video reasoning by constructing skill-aware Chain-of-Thought supervisions and specialized expert modules.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: we extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains.', 'score': 3, 'issue_id': 4145, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '799fe0c792fb72aa', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.03525.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умное видео: рассуждаем как эксперты в разных областях', 'desc': 'Статья представляет новый фреймворк Video-SKoT для улучшения адаптивного видеоанализа в различных доменах. Он использует автоматически созданные цепочки рассуждений (Chain-of-Thought), учитывающие специфические навыки для каждой области. Фреймворк включает создание аннотаций на основе навыков и обучение специализированных экспертных модулей. Эксперименты показывают превосходство Video-SKoT над существующими методами на трех бенчмарках понимания видео.'}, 'en': {'title': 'Enhancing Video Reasoning with Skill-Aware Chain-of-Thought', 'desc': 'The Video-SKoT framework enhances video reasoning by focusing on specific skills needed for understanding different video content. It creates skill-aware Chain-of-Thought (CoT) supervisions that guide the model in reasoning about events, spatial relations, and emotions in videos. By clustering relevant reasoning skills and developing specialized expert modules, the framework allows for more effective domain adaptation. The results show that Video-SKoT outperforms existing methods on various benchmarks, demonstrating its ability to improve video understanding through targeted skill training.'}, 'zh': {'title': '技能感知的视频推理新框架', 'desc': '本文提出了一种名为Video-SKoT的视频推理框架，旨在通过构建技能感知的链式思维（CoT）监督和专门的专家模块来改善领域自适应视频推理。该框架首先从训练问题中提取与领域相关的推理技能，并将其聚类成共享的技能分类法，进而为每个视频-问题对创建详细的多步骤CoT推理。其次，框架引入了一个技能特定的专家学习机制，每个专家模块专注于一部分推理技能，并使用收集到的CoT监督进行轻量级适配器训练。实验结果表明，Video-SKoT在多个视频理解基准测试中表现优异，超越了强基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02863', 'title': 'CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech', 'url': 'https://huggingface.co/papers/2506.02863', 'abstract': 'CapSpeech introduces a large benchmark dataset for various captioned text-to-speech tasks, facilitating advancements in style, accent, emotion, and chat-agent synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.', 'score': 3, 'issue_id': 4151, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'bd41abf94e2ec641', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'CapSpeech: Новый стандарт для синтеза речи с богатыми аннотациями', 'desc': 'CapSpeech представляет собой новый масштабный набор данных для различных задач синтеза речи с аннотациями. Этот датасет включает более 10 миллионов машинно-аннотированных и 0,36 миллиона человеко-аннотированных пар аудио-текст. CapSpeech предназначен для задач синтеза речи с учетом стиля, акцента, эмоций и для чат-агентов. Авторы провели эксперименты с использованием авторегрессионных и неавторегрессионных моделей машинного обучения на этом датасете.'}, 'en': {'title': 'CapSpeech: Advancing Text-to-Speech with Comprehensive Datasets', 'desc': 'CapSpeech is a new benchmark dataset aimed at improving style-captioned text-to-speech synthesis (CapTTS) by providing a comprehensive resource for various tasks. It includes over 10 million machine-annotated and nearly 0.36 million human-annotated audio-caption pairs, covering aspects like style, accent, emotion, and chat-agent synthesis. The dataset supports multiple CapTTS-related tasks, such as CapTTS with sound events and emotion-captioned TTS, facilitating advancements in real-world applications. Experiments conducted on CapSpeech show promising results in generating high-quality, intelligible speech across diverse speaking styles.'}, 'zh': {'title': 'CapSpeech：推动文本到语音合成的新时代', 'desc': 'CapSpeech是一个大型基准数据集，专为各种带字幕的文本到语音任务而设计，旨在推动风格、口音、情感和聊天代理合成的进步。该数据集包含超过1000万个机器标注的音频-字幕对和近36万个人工标注的音频-字幕对，支持多种CapTTS相关任务。我们还引入了由专业配音演员和经验丰富的音频工程师收集和录制的两个新数据集，专门用于聊天代理和带声音事件的CapTTS任务。通过对CapSpeech进行的实验，展示了在多种说话风格下的高保真和高可懂性语音合成。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.01344', 'title': 'Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents', 'url': 'https://huggingface.co/papers/2506.01344', 'abstract': "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.", 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '788495117e4bf1d7', 'authors': ['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Vivek Gupta', 'Dinesh Manocha'], 'affiliations': ['Adobe', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.01344.jpg', 'data': {'categories': ['#graphs', '#cv', '#reasoning', '#agents', '#hallucinations', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '🔀', 'ru': {'title': 'Точная интерпретация блок-схем с помощью нейросимволического агента', 'desc': 'Статья представляет задачу точной атрибуции блок-схем и агента FlowPathAgent для ее решения. Авторы разработали нейросимволический подход, который сегментирует блок-схему, преобразует ее в структурированный символьный граф и использует агентный метод для генерации путей атрибуции. Также представлен новый бенчмарк FlowExplainBench для оценки атрибуций блок-схем. Результаты показывают, что FlowPathAgent снижает визуальные галлюцинации в ответах языковых моделей на вопросы по блок-схемам, превосходя базовые методы на 10-14%.'}, 'en': {'title': 'Enhancing Flowchart Interpretation with Fine-grained Attribution', 'desc': 'This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark.'}, 'zh': {'title': '提升流程图解析的可靠性与可解释性', 'desc': '本论文介绍了一种新的任务，称为细粒度流程图归因，旨在提高大型语言模型（LLM）在处理流程图时的可靠性和可解释性。我们提出了FlowPathAgent，这是一种神经符号代理，通过图形推理进行细粒度的后期归因。该代理首先对流程图进行分割，然后将其转换为结构化的符号图，并动态与图进行交互，以生成归因路径。实验结果表明，FlowPathAgent在流程图问答中减少了视觉幻觉，相较于强基线提高了10-14%的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04034', 'title': 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.04034', 'abstract': 'Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9d3dcbdd5158f101', 'authors': ['Qing Jiang', 'Xingyu Chen', 'Zhaoyang Zeng', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Peking University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04034.jpg', 'data': {'categories': ['#cv', '#rl', '#training', '#reasoning', '#hallucinations', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое объектное реферирование через пошаговые рассуждения', 'desc': 'Статья представляет новый подход к задаче объектного реферирования в компьютерном зрении, названный Rex-Thinker. Модель использует пошаговое рассуждение для оценки соответствия объектов заданному описанию, что повышает интерпретируемость и надежность предсказаний. Авторы создали датасет HumanRef-CoT для обучения модели структурированным рассуждениям. Rex-Thinker обучается в два этапа: контролируемая тонкая настройка и обучение с подкреплением, что улучшает точность и обобщающую способность модели.'}, 'en': {'title': 'Rex-Thinker: Grounded Object Referring with Explainable Reasoning', 'desc': 'This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios.'}, 'zh': {'title': 'Rex-Thinker：可解释的物体指代模型', 'desc': '本文提出了一种新的物体指代模型Rex-Thinker，旨在通过明确的链式推理任务来检测与自然语言描述匹配的图像中的所有物体。该模型强调可验证性和可信性，确保其预测能够解释并与视觉证据相连。Rex-Thinker通过逐步推理候选物体实例，判断其是否符合给定的描述，从而做出最终预测。实验结果表明，该方法在精确度和可解释性方面优于传统基线，并在拒绝虚假输出和跨领域泛化能力上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.03951', 'title': 'Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective', 'url': 'https://huggingface.co/papers/2506.03951', 'abstract': 'A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  \t\t\t\t\tAI-generated summary \t\t\t\t The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters.', 'score': 2, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '19d2cecb53fd6998', 'authors': ['Aojun Lu', 'Hangjie Yuan', 'Tao Feng', 'Yanan Sun'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'College of Computer Science, Sichuan University, Chengdu, China', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03951.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Двойная архитектура для эффективного непрерывного обучения', 'desc': 'Статья представляет новую архитектуру Dual-Arch для непрерывного обучения, которая решает дилемму стабильности-пластичности на архитектурном уровне. Авторы обнаружили, что при равном количестве параметров более глубокие сети обладают лучшей пластичностью, а более широкие - лучшей стабильностью. Dual-Arch использует две отдельные сети: одну для пластичности, другую для стабильности, каждая со специализированной архитектурой. Эксперименты показали, что Dual-Arch улучшает производительность существующих методов непрерывного обучения, при этом используя до 87% меньше параметров.'}, 'en': {'title': 'Dual-Arch: Balancing Stability and Plasticity in Continual Learning', 'desc': 'The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networks—one focused on plasticity and the other on stability—Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.'}, 'zh': {'title': '双网络架构，平衡学习稳定性与可塑性', 'desc': '本文提出了一种新框架Dual-Arch，旨在通过在架构层面解决稳定性与可塑性之间的矛盾来增强持续学习。持续学习的目标是使神经网络能够逐步学习和适应新知识，同时保持对旧知识的记忆。研究表明，在相同参数约束下，深层网络具有更好的可塑性，而宽层网络则表现出更高的稳定性。Dual-Arch框架结合了两个独立网络的优势，一个专注于可塑性，另一个专注于稳定性，从而提高了现有持续学习方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03822', 'title': 'CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents', 'url': 'https://huggingface.co/papers/2506.03822', 'abstract': "Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's URL, such as a digital object identifier, CRAWLDoc retrieves the landing page and all linked web resources, including PDFs, ORCID profiles, and supplementary materials. It embeds these resources, along with anchor texts and the URLs, into a unified representation. For evaluating CRAWLDoc, we have created a new, manually labeled dataset of 600 publications from six top publishers in computer science. Our method CRAWLDoc demonstrates a robust and layout-independent ranking of relevant documents across publishers and data formats. It lays the foundation for improved metadata extraction from web documents with various layouts and formats. Our source code and dataset can be accessed at https://github.com/FKarl/CRAWLDoc.", 'score': 2, 'issue_id': 4147, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'dea42a2dce3ff33b', 'authors': ['Fabian Karl', 'Ansgar Scherp'], 'affiliations': ['Universität Ulm, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03822.jpg', 'data': {'categories': ['#dataset', '#data'], 'emoji': '🕸️', 'ru': {'title': 'CRAWLDoc: Умное извлечение метаданных из веб-документов', 'desc': 'CRAWLDoc - это новый метод контекстного ранжирования связанных веб-документов для извлечения метаданных публикаций. Он начинает с URL публикации, извлекает связанные ресурсы и встраивает их в единое представление. Метод был протестирован на новом наборе данных из 600 публикаций от шести ведущих издателей в области компьютерных наук. CRAWLDoc демонстрирует надежное ранжирование релевантных документов независимо от макета и формата данных.'}, 'en': {'title': 'CRAWLDoc: Contextual Ranking for Enhanced Metadata Extraction', 'desc': "This paper presents CRAWLDoc, a novel approach for ranking web documents related to academic publications. It addresses the challenge of varying web layouts and formats by retrieving a publication's landing page and all associated resources, such as PDFs and profiles. CRAWLDoc creates a unified representation of these resources, enhancing the contextual understanding of linked documents. The method has been evaluated using a new dataset of 600 publications, showing its effectiveness in providing layout-independent document rankings for better metadata extraction."}, 'zh': {'title': 'CRAWLDoc：提升元数据提取的创新方法', 'desc': '本论文介绍了一种新的方法CRAWLDoc，用于从不同的网络来源中提取准确的元数据。该方法从出版物的URL开始，检索相关的网页和链接资源，并将其嵌入到一个统一的表示中。通过创建一个包含600个出版物的新手动标注数据集，评估了CRAWLDoc的有效性。CRAWLDoc展示了在不同出版商和数据格式中，能够稳健且独立于布局地对相关文档进行排名，为改进元数据提取奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.03614', 'title': 'VLMs Can Aggregate Scattered Training Patches', 'url': 'https://huggingface.co/papers/2506.03614', 'abstract': 'VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe\'\' or ``unsafe\'\', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.', 'score': 2, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '180b48fc19d50b80', 'authors': ['Zhanhui Zhou', 'Lingjie Chen', 'Chao Yang', 'Chaochao Lu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03614.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#multimodal', '#cv', '#benchmark', '#security', '#ethics'], 'emoji': '🧩', 'ru': {'title': 'Визуальное сшивание: скрытая угроза в моделях компьютерного зрения', 'desc': "Это исследование раскрывает феномен 'визуального сшивания' в моделях компьютерного зрения и обработки естественного языка (VLM). Авторы демонстрируют, как VLM способны интегрировать фрагментированную визуальную информацию, что позволяет обходить модерацию данных и реконструировать нежелательный контент во время вывода. Эксперименты показывают, что модели могут собирать полные изображения из небольших, безобидных на вид фрагментов, разбросанных по множеству обучающих образцов. Это открытие подчеркивает серьезные риски безопасности при использовании VLM и необходимость разработки новых методов защиты."}, 'en': {'title': 'Visual Stitching: A Hidden Risk in Vision-Language Models', 'desc': 'This paper discusses a vulnerability in vision-language models (VLMs) known as visual stitching, which allows these models to reconstruct harmful content from fragmented visual information. The authors show that when dangerous images are divided into small patches and mixed with benign data, VLMs can still learn to piece them together during training. This leads to a situation where the models can generate harmful outputs by associating safe descriptions with dangerous images. The study highlights the risks of data moderation being bypassed and emphasizes the need for improved safety measures in VLMs.'}, 'zh': {'title': '视觉拼接：VLMs的安全隐患', 'desc': '本论文探讨了视觉语言模型（VLMs）中的视觉拼接能力，这种能力使得模型能够整合分散的视觉信息。研究表明，当有害图像被分割成小的、看似无害的片段时，数据的审查可以被轻易绕过。VLMs在训练过程中可能会学习将这些片段拼接在一起，从而在推理时生成有害的响应。我们通过实验展示了这一现象，并模拟了对抗性数据中毒的场景，揭示了VLMs在安全性方面的潜在风险。'}}}, {'id': 'https://huggingface.co/papers/2506.02515', 'title': 'FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.02515', 'abstract': 'A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.', 'score': 2, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '0b49cede5613cc2d', 'authors': ['Zhuohan Xie', 'Dhruv Sahnan', 'Debopriyo Banerjee', 'Georgi Georgiev', 'Rushil Thareja', 'Hachem Madmoun', 'Jinyan Su', 'Aaryamonvikram Singh', 'Yuxia Wang', 'Rui Xing', 'Fajri Koto', 'Haonan Li', 'Ivan Koychev', 'Tanmoy Chakraborty', 'Salem Lahlou', 'Veselin Stoyanov', 'Preslav Nakov'], 'affiliations': ['Cornell University, USA', 'FMI, Sofia University, Bulgaria', 'IIT Delhi, India', 'MBZUAI, UAE', 'Quantsquare, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.02515.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinChain: новый рубеж в оценке финансовых рассуждений ИИ', 'desc': 'FinChain - это новый бенчмарк для оценки многоэтапных символьных рассуждений в финансовых задачах, фокусирующийся на промежуточных шагах рассуждений. Он включает 54 темы из 12 финансовых областей, с пятью параметризованными шаблонами для каждой темы. Авторы также представили метрику ChainEval для автоматической оценки как конечных ответов, так и промежуточных рассуждений. Тестирование 30 различных языковых моделей на этом датасете показало, что даже современные модели имеют значительный потенциал для улучшения в области многоэтапных финансовых рассуждений.'}, 'en': {'title': 'FinChain: Advancing Financial Reasoning with Intermediate Steps', 'desc': 'The paper introduces FinChain, a new benchmark designed to evaluate multi-step symbolic reasoning specifically in financial tasks. Unlike existing datasets that only focus on final answers, FinChain emphasizes the importance of intermediate reasoning steps through its novel metric, ChainEval. This benchmark covers a wide range of financial topics and provides parameterized templates to assess varying levels of reasoning complexity. The findings reveal that even advanced language models struggle with multi-step reasoning in finance, highlighting the need for improved capabilities in this area.'}, 'zh': {'title': 'FinChain：金融推理的新基准', 'desc': 'FinChain是一个新的基准，旨在评估金融任务中的多步骤符号推理，特别关注中间推理步骤。它引入了ChainEval作为评估最终答案和推理过程的新指标。现有的数据集如FinQA和ConvFinQA仅监督最终的数值答案，而不评估中间推理步骤。FinChain覆盖12个金融领域的54个主题，为每个主题提供五个不同推理复杂度和领域专业知识要求的模板。'}}}, {'id': 'https://huggingface.co/papers/2505.23564', 'title': 'Segment Policy Optimization: Effective Segment-Level Credit Assignment\n  in RL for Large Language Models', 'url': 'https://huggingface.co/papers/2505.23564', 'abstract': 'The Segment Policy Optimization (SPO) framework improves large language model reasoning via reinforcement learning by offering intermediate granularity advantage estimation, balancing precision and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving 6-12 percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving 7-11 percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.', 'score': 2, 'issue_id': 4146, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '25b36641cf394ee3', 'authors': ['Yiran Guo', 'Lijie Xu', 'Jie Liu', 'Dan Ye', 'Shuang Qiu'], 'affiliations': ['City University of Hong Kong', 'Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.23564.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'SPO: Оптимизация рассуждений языковых моделей через сегментированное обучение с подкреплением', 'desc': 'Статья представляет новый метод обучения с подкреплением для улучшения рассуждений больших языковых моделей - Segment Policy Optimization (SPO). SPO использует оценку преимуществ на уровне сегментов, что обеспечивает баланс между точностью и вычислительной эффективностью. Метод включает три ключевых компонента: гибкое разделение на сегменты, точную оценку преимуществ сегментов и оптимизацию политики с использованием преимуществ сегментов. SPO показывает значительные улучшения точности по сравнению с существующими методами на задачах рассуждений и решения математических задач.'}, 'en': {'title': 'Segment Policy Optimization: Balancing Precision and Efficiency in RL for Language Models', 'desc': 'The Segment Policy Optimization (SPO) framework enhances the reasoning abilities of large language models through reinforcement learning by introducing an intermediate granularity for advantage estimation. This approach addresses the limitations of both token-level and trajectory-level methods, providing a more accurate credit assignment while maintaining computational efficiency. SPO incorporates flexible segment partitioning, precise segment advantage estimation, and a novel policy optimization strategy that utilizes segment advantages. The framework has been successfully applied to improve performance on tasks like GSM8K and MATH500, demonstrating significant accuracy gains over existing methods.'}, 'zh': {'title': '段策略优化：提升语言模型推理能力的新方法', 'desc': '本论文提出了一种新的强化学习框架，称为段策略优化（SPO），旨在提高大型语言模型的推理能力。SPO通过中间粒度的优势估计，克服了现有方法在精确性和计算效率之间的权衡问题。该框架包含灵活的段划分、准确的段优势估计和基于段优势的策略优化，能够在不依赖评论模型的情况下实现准确的优势估计。实验结果表明，SPO在多个任务上相较于传统方法有显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.04214', 'title': 'Sounding that Object: Interactive Object-Aware Image to Audio Generation', 'url': 'https://huggingface.co/papers/2506.04214', 'abstract': 'Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/', 'score': 1, 'issue_id': 4146, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1f6af5427e65f943', 'authors': ['Tingle Li', 'Baihe Huang', 'Xiaobin Zhuang', 'Dongya Jia', 'Jiawei Chen', 'Yuping Wang', 'Zhuo Chen', 'Gopala Anumanchipalli', 'Yuxuan Wang'], 'affiliations': ['University of California'], 'pdf_title_img': 'assets/pdf/title_img/2506.04214.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal'], 'emoji': '🔊', 'ru': {'title': 'Интерактивная генерация звука на основе объектов в изображениях', 'desc': 'Эта статья представляет модель генерации звука, основанную на выбранных пользователем визуальных объектах в изображениях. Модель использует объектно-ориентированное обучение в сочетании с условной латентной диффузионной моделью и мультимодальным вниманием для связывания областей изображения с соответствующими звуками. На этапе тестирования применяется сегментация изображений, позволяющая генерировать звуки на уровне отдельных объектов. Теоретически и экспериментально показано, что модель превосходит базовые подходы в точности соответствия между объектами и их звуками.'}, 'en': {'title': 'Interactive Sound Generation for Visual Objects', 'desc': 'This paper presents a novel model for generating sounds that correspond to specific visual objects in images, addressing the complexity of audio-visual scenes. The proposed interactive object-aware audio generation model uses a conditional latent diffusion approach, which learns to connect image regions with their respective sounds through a multi-modal attention mechanism. During testing, the model allows users to select objects in an image, generating sounds that are accurately aligned with those objects. The results demonstrate that this method outperforms existing models, providing a more coherent audio-visual experience.'}, 'zh': {'title': '交互式对象感知音频生成模型', 'desc': '本文提出了一种交互式对象感知音频生成模型，旨在为复杂的音视频场景生成准确的声音。该模型通过用户选择的视觉对象来引导声音生成，结合了对象中心学习和条件潜在扩散模型。我们的方法利用多模态注意力机制，将图像区域与相应的声音关联起来，并在测试时通过图像分割实现用户的交互式声音生成。实验结果表明，该模型在对象与声音的对齐方面优于基线模型，生成的音频与所选对象更为一致。'}}}, {'id': 'https://huggingface.co/papers/2506.03817', 'title': 'Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid', 'url': 'https://huggingface.co/papers/2506.03817', 'abstract': 'Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '24782a46b48333d1', 'authors': ['Julius Gonsior', 'Tim Rieß', 'Anja Reusch', 'Claudio Hartmann', 'Maik Thiele', 'Wolfgang Lehner'], 'affiliations': ['Hochschule fur Technik und Wirtschaft Dresden', 'Technion - Israeli Institute of Technology', 'Technische Universitat Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.03817.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты гиперпараметров в активном обучении', 'desc': 'Это исследование посвящено активному обучению (Active Learning, AL) в машинном обучении, которое помогает минимизировать усилия по разметке данных. Авторы провели масштабный эксперимент, изучив более 4,6 миллионов комбинаций гиперпараметров AL. Они проанализировали влияние каждого гиперпараметра на результаты и предложили рекомендации по настройке AL. Исследование направлено на повышение воспроизводимости и надежности экспериментов с AL в будущем.'}, 'en': {'title': 'Unlocking Active Learning: Simplifying Setup for Trustworthy Results', 'desc': 'This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.'}, 'zh': {'title': '优化主动学习，提升标注效率', 'desc': '标注数据是一个耗时且成本高昂的任务，但这是监督学习所必需的。主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标记样本来减少人工标注工作的方法，从而提高分类性能。尽管AL已经存在了几十年，但在实际应用中仍然很少被使用。本文研究了AL中超参数空间的复杂性，提出了一个包含460万种超参数组合的大型网格，并分析了每个超参数对实验结果的影响，以促进更可靠的AL研究。'}}}, {'id': 'https://huggingface.co/papers/2506.03538', 'title': 'Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting', 'url': 'https://huggingface.co/papers/2506.03538', 'abstract': 'A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'c9dadfe8cdfe5d4c', 'authors': ['Chengqi Li', 'Zhihao Shi', 'Yangdi Lu', 'Wenbo He', 'Xiangyu Xu'], 'affiliations': ['Department of Computing and Software McMaster University', 'Department of Electrical and Computer Engineering McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03538.jpg', 'data': {'categories': ['#training', '#3d', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Двойное зрение для точной 3D-реконструкции', 'desc': 'Предложен новый метод Asymmetric Dual 3DGS для улучшения 3D-реконструкции изображений. Он основан на обучении двух параллельных моделей 3D Gaussian Splatting с ограничением согласованности и дивергентным маскированием. Метод превосходит существующие подходы, подавляя артефакты и выделяя надежную геометрию сцены. Также представлен облегченный вариант Dynamic EMA Proxy для повышения эффективности обучения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Asymmetric Dual Models', 'desc': 'The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.'}, 'zh': {'title': '非对称双重3DGS框架：高效的3D重建新方法', 'desc': '本文提出了一种新颖的非对称双重3DGS框架，旨在提高3D重建的效果。该方法通过训练两个模型并施加一致性约束，来减少不一致的视觉伪影。我们引入了多线索自适应掩码和自监督软掩码，确保两个模型在训练过程中保持差异，从而降低共享错误模式。实验结果表明，该方法在处理真实世界数据集时，表现出更高的效率和更好的重建质量。'}}}, {'id': 'https://huggingface.co/papers/2506.02680', 'title': 'Solving Inverse Problems with FLAIR', 'url': 'https://huggingface.co/papers/2506.02680', 'abstract': 'FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based latent generative models such as Stable Diffusion 3 are able to generate images with remarkable quality, even enabling photorealistic text-to-image generation. Their impressive performance suggests that these models should also constitute powerful priors for inverse imaging problems, but that approach has not yet led to comparable fidelity. There are several key obstacles: (i) the encoding into a lower-dimensional latent space makes the underlying (forward) mapping non-linear; (ii) the data likelihood term is usually intractable; and (iii) learned generative models struggle to recover rare, atypical data modes during inference. We present FLAIR, a novel training free variational framework that leverages flow-based generative models as a prior for inverse problems. To that end, we introduce a variational objective for flow matching that is agnostic to the type of degradation, and combine it with deterministic trajectory adjustments to recover atypical modes. To enforce exact consistency with the observed data, we decouple the optimization of the data fidelity and regularization terms. Moreover, we introduce a time-dependent calibration scheme in which the strength of the regularization is modulated according to off-line accuracy estimates. Results on standard imaging benchmarks demonstrate that FLAIR consistently outperforms existing diffusion- and flow-based methods in terms of reconstruction quality and sample diversity.', 'score': 1, 'issue_id': 4143, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '82a645cb87d64a3f', 'authors': ['Julius Erbach', 'Dominik Narnhofer', 'Andreas Dombos', 'Bernt Schiele', 'Jan Eric Lenssen', 'Konrad Schindler'], 'affiliations': ['ETH Zürich', 'Max Planck Institute for Informatics, Saarland Informatics Campus'], 'pdf_title_img': 'assets/pdf/title_img/2506.02680.jpg', 'data': {'categories': ['#cv', '#benchmark', '#diffusion', '#data', '#training', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'FLAIR: Революция в решении обратных задач с помощью потоковых генеративных моделей', 'desc': 'FLAIR - это новая вариационная система, использующая генеративные модели на основе потоков для улучшения решений обратных задач. Она не требует дополнительного обучения и преодолевает ключевые препятствия, связанные с нелинейностью отображения и трудностями в восстановлении редких режимов данных. FLAIR вводит вариационную целевую функцию для сопоставления потоков, независимую от типа деградации, и комбинирует ее с детерминированными корректировками траектории. Система превосходит существующие методы на основе диффузии и потоков по качеству реконструкции и разнообразию выборки.'}, 'en': {'title': 'FLAIR: Enhancing Inverse Problems with Flow-Based Generative Models', 'desc': 'FLAIR is a new framework that improves solutions to inverse problems using flow-based generative models without requiring extensive training. It addresses challenges like non-linear mappings and intractable data likelihoods by introducing a variational objective that is flexible to different types of data degradation. The framework also includes techniques to recover rare data patterns and ensures consistency with observed data by separating optimization processes. Experimental results show that FLAIR achieves better image reconstruction quality and greater diversity in samples compared to existing methods.'}, 'zh': {'title': 'FLAIR：提升逆问题解决的新方法', 'desc': 'FLAIR是一种新颖的无训练变分框架，利用基于流的生成模型来增强逆问题的解决方案。该方法通过引入变分目标和确定性轨迹调整，克服了在低维潜在空间编码带来的非线性映射问题。FLAIR能够有效恢复稀有和非典型的数据模式，并确保与观测数据的一致性。实验结果表明，FLAIR在重建质量和样本多样性方面优于现有的扩散和流模型方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02153', 'title': 'Small Language Models are the Future of Agentic AI', 'url': 'https://huggingface.co/papers/2506.02153', 'abstract': 'Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation.   Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm.   Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '811261b62e0e242e', 'authors': ['Peter Belcak', 'Greg Heinrich', 'Shizhe Diao', 'Yonggan Fu', 'Xin Dong', 'Saurav Muralidharan', 'Yingyan Celine Lin', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology', 'NVIDIA Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.02153.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#small_models'], 'emoji': '🤖', 'ru': {'title': 'Малые языковые модели - будущее агентного ИИ', 'desc': 'Статья рассматривает потенциал малых языковых моделей (SLM) в контексте агентных систем искусственного интеллекта. Авторы утверждают, что SLM достаточно мощны, более подходящи и экономичны для многих задач в агентных системах по сравнению с большими языковыми моделями (LLM). Они предлагают алгоритм конвертации агентов с LLM на SLM и обсуждают возможные препятствия для внедрения SLM. Статья призывает к дискуссии об эффективном использовании ресурсов ИИ и снижении затрат на современные технологии искусственного интеллекта.'}, 'en': {'title': 'Small Language Models: The Future of Agentic AI', 'desc': 'This paper argues that small language models (SLMs) are more suitable and cost-effective for specialized tasks in agentic AI systems compared to large language models (LLMs). It highlights that SLMs possess sufficient capabilities for repetitive tasks, making them a better choice for many applications. The authors propose that heterogeneous systems, which use multiple models, are ideal for scenarios requiring general conversational abilities. They also present a conversion algorithm for transitioning from LLMs to SLMs and emphasize the economic benefits of adopting SLMs in the AI industry.'}, 'zh': {'title': '小型语言模型是代理AI的未来', 'desc': '大型语言模型（LLMs）在许多任务中表现出接近人类的能力，但在特定应用中，较小的语言模型（SLMs）更为合适。SLMs在代理系统中能够高效地执行重复性任务，且经济性更强。我们认为，SLMs的能力足以满足许多需求，并且在需要多种模型的异构代理系统中，SLMs是理想选择。我们还提出了从LLMs到SLMs的转换算法，以促进SLMs在代理系统中的应用。'}}}, {'id': 'https://huggingface.co/papers/2506.00618', 'title': 'RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents', 'url': 'https://huggingface.co/papers/2506.00618', 'abstract': 'RIOSWorld is a benchmark for evaluating safety risks of multimodal large language models in real-world computer tasks, revealing significant risks that necessitate safety alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce RiOSWorld, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on RiOSWorld demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': 'd37e3433dcb25b41', 'authors': ['Jingyi Yang', 'Shuai Shao', 'Dongrui Liu', 'Jing Shao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00618.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#multimodal', '#agents', '#security', '#alignment'], 'emoji': '🖥️', 'ru': {'title': 'RIOSWorld: выявление рисков безопасности ИИ-агентов в реальных компьютерных задачах', 'desc': 'RIOSWorld - это бенчмарк для оценки рисков безопасности мультимодальных больших языковых моделей (MLLM) при выполнении компьютерных задач в реальном мире. Он включает 492 рисковых задания в различных компьютерных приложениях, охватывающих веб, социальные сети, мультимедиа, ОС, электронную почту и офисные программы. Риски разделены на две основные категории: риски, исходящие от пользователя, и риски окружающей среды. Эксперименты показали значительные риски безопасности для современных агентов на основе MLLM при работе с компьютером в реальных сценариях.'}, 'en': {'title': 'Evaluating Safety Risks of MLLMs in Real-World Tasks with RIOSWorld', 'desc': 'RIOSWorld is a new benchmark created to assess the safety risks associated with multimodal large language models (MLLMs) when they perform real-world computer tasks. It identifies significant risks that arise from both user actions and environmental factors, emphasizing the need for safety measures tailored to these scenarios. The benchmark includes a diverse set of 492 tasks across various applications, allowing for a comprehensive evaluation of potential hazards. The findings indicate that current MLLM-based agents face considerable safety challenges, underscoring the importance of aligning their operations with safety principles in practical environments.'}, 'zh': {'title': '评估多模态语言模型安全风险的新基准', 'desc': 'RIOSWorld是一个用于评估多模态大型语言模型（MLLM）在现实计算任务中安全风险的基准。随着MLLM的快速发展，它们被越来越多地用作自主计算代理，但在实际应用中存在安全风险的问题。现有的研究在评估这些风险时存在局限性，往往缺乏真实的互动环境或只关注特定的风险类型。RIOSWorld通过提供492个涵盖多种计算应用的风险任务，帮助全面评估计算代理的安全风险，强调了在现实计算操作中进行安全对齐的必要性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (18)', '#agents (25)', '#agi (6)', '#alignment (16)', '#architecture (37)', '#audio (11)', '#benchmark (111)', '#cv (42)', '#data (34)', '#dataset (94)', '#diffusion (43)', '#ethics (10)', '#games (27)', '#graphs (2)', '#hallucinations (12)', '#healthcare (2)', '#inference (20)', '#interpretability (21)', '#leakage (3)', '#long_context (16)', '#low_resource (12)', '#machine_translation (5)', '#math (9)', '#multilingual (13)', '#multimodal (79)', '#open_source (47)', '#optimization (101)', '#plp (1)', '#rag (5)', '#reasoning (80)', '#rl (40)', '#rlhf (13)', '#robotics (10)', '#science (11)', '#security (11)', '#small_models (7)', '#story_generation (5)', '#survey (6)', '#synthetic (17)', '#training (101)', '#transfer_learning (21)', '#video (35)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-06 15:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-06 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-06 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    