
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 33 papers. June 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Июнь 2025</span> | <span id="title-articles-count">33 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-05.html">⬅️ <span id="prev-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-07.html">➡️ <span id="next-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Июнь 2025', 'en': 'June 2025', 'zh': '6月2025年'};
        let feedDateNext = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let feedDatePrev = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.24864', 'title': 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.24864', 'abstract': "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", 'score': 62, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '390a294f460cedfc', 'authors': ['Mingjie Liu', 'Shizhe Diao', 'Ximing Lu', 'Jian Hu', 'Xin Dong', 'Yejin Choi', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24864.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl', '#alignment', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Расширение границ рассуждений языковых моделей с помощью длительного обучения с подкреплением', 'desc': 'Статья описывает новый метод обучения языковых моделей с подкреплением (RL) под названием ProRL. Авторы показывают, что длительное RL-обучение может раскрыть новые стратегии рассуждений, недоступные базовым моделям. ProRL включает контроль расхождения KL, сброс эталонной политики и набор разнообразных задач. Эмпирический анализ демонстрирует, что модели, обученные с помощью RL, превосходят базовые модели в различных оценках pass@k.'}, 'en': {'title': 'Unlocking New Reasoning Strategies with ProRL', 'desc': 'This paper explores the effectiveness of reinforcement learning (RL) in enhancing the reasoning capabilities of language models. The authors introduce a new training method called ProRL, which employs techniques like KL divergence control and reference policy resetting to improve model performance. Their experiments show that models trained with ProRL outperform base models in various reasoning tasks, even in cases where base models struggle. The study suggests that prolonged RL training can help discover new reasoning strategies, indicating that RL can significantly expand the reasoning abilities of language models over time.'}, 'zh': {'title': '强化学习扩展推理能力的新方法', 'desc': '最近的研究表明，基于推理的语言模型在强化学习（RL）方面取得了进展，这被认为是一种有效的方法来使模型与可验证的奖励对齐。然而，关于RL是否真正增强了模型的推理能力，还是仅仅放大了基础模型分布中已经存在的高奖励输出，仍然存在争议。本文提出了一种新的训练方法ProRL，证明了经过长时间的RL训练可以发现基础模型无法访问的新推理策略。我们的实证分析显示，经过RL训练的模型在多种评估任务中表现优于基础模型，尤其是在基础模型完全失败的情况下。'}}}, {'id': 'https://huggingface.co/papers/2505.24863', 'title': 'AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time', 'url': 'https://huggingface.co/papers/2505.24863', 'abstract': "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/", 'score': 37, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a30c2004fdd2d154', 'authors': ['Junyu Zhang', 'Runpei Dong', 'Han Wang', 'Xuying Ning', 'Haoran Geng', 'Peihao Li', 'Xialin He', 'Yutong Bai', 'Jitendra Malik', 'Saurabh Gupta', 'Huan Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24863.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'AlphaOne: Универсальная модуляция рассуждений в ИИ', 'desc': 'AlphaOne (alpha1) - это универсальная система для модуляции процесса рассуждений в крупных моделях рассуждений (LRM) во время тестирования. Она вводит понятие альфа-момента, представляющего масштабированную фазу мышления с универсальным параметром альфа. Система динамически планирует переходы между медленным и быстрым мышлением, моделируя вставку токенов перехода рассуждений как стохастический процесс Бернулли. AlphaOne превосходит существующие методы монотонного масштабирования, обеспечивая гибкую модуляцию рассуждений.'}, 'en': {'title': 'AlphaOne: Revolutionizing Reasoning in Large Models', 'desc': "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."}, 'zh': {'title': '灵活调节推理进程的AlphaOne框架', 'desc': '本文提出了AlphaOne（alpha1），这是一个在测试时调节大型推理模型（LRMs）推理进程的通用框架。alpha1首先引入了alpha时刻，表示带有通用参数alpha的缩放思维阶段。在这个缩放的前alpha时刻阶段中，它通过将推理过渡标记的插入建模为伯努利随机过程，动态调度缓慢思维的过渡。在alpha时刻之后，alpha1通过思维结束标记确定性地终止缓慢思维，从而促进快速推理和高效答案生成。'}}}, {'id': 'https://huggingface.co/papers/2505.24867', 'title': "Time Blindness: Why Video-Language Models Can't See What Humans Can?", 'url': 'https://huggingface.co/papers/2505.24867', 'abstract': 'Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.', 'score': 35, 'issue_id': 4070, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6870a65f3ad54877', 'authors': ['Ujjwal Upadhyay', 'Mukul Ranjan', 'Zhiqiang Shen', 'Mohamed Elhoseiny'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)', 'Mohamed bin Zayed University of AI (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24867.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#survey', '#reasoning', '#multimodal', '#architecture', '#cv', '#games', '#dataset'], 'emoji': '⏳', 'ru': {'title': 'Временная слепота: раскрывая ограничения современных моделей компьютерного зрения', 'desc': 'Исследователи представили SpookyBench - новый бенчмарк для оценки способности моделей компьютерного зрения и обработки естественного языка (VLM) распознавать временные паттерны в видео. Эксперименты показали, что современные VLM не способны извлекать смысл из чисто временных последовательностей, в то время как люди справляются с этой задачей с точностью более 98%. Авторы отмечают, что эта проблема связана с чрезмерной зависимостью моделей от пространственных признаков в отдельных кадрах. SpookyBench призван стимулировать исследования в области распознавания временных паттернов и улучшить понимание видео машинными системами.'}, 'en': {'title': 'Bridging the Gap: Enhancing Temporal Understanding in Vision-Language Models', 'desc': 'This paper introduces SpookyBench, a new benchmark designed to test vision-language models (VLMs) on their ability to understand temporal patterns in videos when spatial information is not available. The study reveals that while humans can accurately identify shapes and patterns in noisy temporal sequences, current state-of-the-art VLMs fail to do so, achieving 0% accuracy. This highlights a significant limitation in VLMs, which tend to rely heavily on spatial features and struggle with temporal reasoning, especially in low spatial signal-to-noise ratio scenarios. The authors suggest that addressing this issue may require innovative model architectures or training methods that separate spatial and temporal processing, and they aim to stimulate further research in this area by releasing the SpookyBench dataset.'}, 'zh': {'title': '突破时序理解的瓶颈', 'desc': '最近，视觉语言模型（VLMs）在理解视频中的时空关系方面取得了显著进展。然而，当空间信息被遮蔽时，这些模型在捕捉纯粹的时间模式方面表现不佳。我们提出了SpookyBench，这是一个基准测试，信息仅通过噪声帧的时间序列编码，反映了从生物信号到隐蔽通信的自然现象。我们的研究表明，尽管人类在这些序列中识别形状、文本和模式的准确率超过98%，但最先进的VLMs的准确率却为0%，这突显了模型在时序理解上的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.18842', 'title': "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation", 'url': 'https://huggingface.co/papers/2505.18842', 'abstract': "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.", 'score': 26, 'issue_id': 4069, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'a97f1e174b838d2d', 'authors': ['Jiwan Chung', 'Junhyeok Kim', 'Siyeol Kim', 'Jaeyoung Lee', 'Min Soo Kim', 'Youngjae Yu'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18842.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#games', '#architecture', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Динамический визуальный доступ для улучшения мультимодальных рассуждений', 'desc': 'v1 - это расширение для мультимодальных больших языковых моделей (MLLM), которое позволяет избирательно обращаться к визуальным данным во время вывода. Оно вводит механизм указания и копирования, позволяющий модели динамически извлекать релевантные области изображения в процессе рассуждения. Для обучения этой возможности был создан набор данных v1g из 300 тысяч трасс мультимодальных рассуждений с аннотациями визуальной привязки. Эксперименты на трех эталонных тестах по мультимодальным математическим рассуждениям показали, что v1 стабильно улучшает производительность по сравнению с базовыми моделями.'}, 'en': {'title': 'Dynamic Visual Retrieval for Enhanced Multimodal Reasoning', 'desc': "The paper introduces v1, an enhancement to Multimodal Large Language Models (MLLMs) that allows for selective and dynamic retrieval of visual information during inference. Unlike traditional MLLMs that process visual inputs only once, v1 employs a point-and-copy mechanism to revisit relevant image regions as the model generates responses. This approach improves the model's ability to perform multimodal reasoning tasks by providing contextual access to visual data based on its ongoing hypotheses. The authors validate v1's effectiveness through experiments on multiple benchmarks, showing significant performance gains in tasks that require detailed visual references and complex reasoning steps."}, 'zh': {'title': '动态视觉访问提升多模态推理能力', 'desc': 'v1是对多模态大型语言模型（MLLMs）的轻量级扩展，能够在推理过程中实现选择性视觉区域的动态检索。与传统的MLLMs仅在内部记忆中进行推理不同，v1引入了一种简单的点对点复制机制，使模型能够在推理过程中动态获取相关的图像区域。通过构建包含30万条多模态推理轨迹的数据集v1g，模型得以训练这种能力。实验结果表明，v1在多个多模态数学推理基准上表现优异，尤其是在需要细致视觉参考和多步骤推理的任务中。'}}}, {'id': 'https://huggingface.co/papers/2505.14752', 'title': 'Large Language Models for Data Synthesis', 'url': 'https://huggingface.co/papers/2505.14752', 'abstract': 'LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.', 'score': 25, 'issue_id': 4067, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '202c77d3d43de6f6', 'authors': ['Yihong Tang', 'Menglin Kong', 'Lijun Sun'], 'affiliations': ['McGill University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14752.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data'], 'emoji': '🧬', 'ru': {'title': 'LLMSynthor: Превращение языковых моделей в точные генераторы синтетических данных', 'desc': 'LLMSynthor - это фреймворк для синтеза данных, который превращает большие языковые модели (LLM) в симуляторы, учитывающие структуру данных и использующие распределительную обратную связь. Он применяет LLM как непараметрический копула-симулятор для моделирования зависимостей высокого порядка и вводит LLM Proposal Sampling для создания обоснованных предлагаемых распределений. LLMSynthor итеративно минимизирует расхождения в пространстве сводных статистик, выравнивая реальные и синтетические данные. Фреймворк показывает высокую статистическую точность и практическую полезность на гетерогенных наборах данных в конфиденциальных областях.'}, 'en': {'title': 'Transforming LLMs into Efficient Data Synthesizers', 'desc': 'LLMSynthor is a framework that enhances Large Language Models (LLMs) for creating synthetic data that accurately reflects real-world statistical distributions. It addresses the limitations of traditional data synthesis methods, which often rely on rigid assumptions and struggle with complex data types. By using distributional feedback and a novel LLM Proposal Sampling technique, LLMSynthor improves the efficiency and accuracy of data generation without the need for rejection sampling. The framework has been tested in various real-world scenarios, demonstrating its ability to produce high-quality synthetic data suitable for diverse applications.'}, 'zh': {'title': 'LLMSynthor：高效的统计数据合成新工具', 'desc': 'LLMSynthor 是一种增强大型语言模型（LLM）以实现高效和统计准确的数据合成的方法。它通过分布反馈和提议采样，将 LLM 转变为结构感知的模拟器，能够更好地捕捉真实世界分布的统计特征。该框架通过最小化摘要统计空间中的差异，逐步对齐真实数据和合成数据，同时揭示和优化潜在的生成结构。LLMSynthor 在隐私敏感领域的异构数据集上进行了评估，显示出高统计保真度和实用性，适用于经济学、社会科学和城市研究等多个领域。'}}}, {'id': 'https://huggingface.co/papers/2505.24098', 'title': 'HardTests: Synthesizing High-Quality Test Cases for LLM Coding', 'url': 'https://huggingface.co/papers/2505.24098', 'abstract': 'Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.', 'score': 22, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '1a86a8aadff22dbb', 'authors': ['Zhongmou He', 'Yee Man Choi', 'Kexun Zhang', 'Jiabao Ji', 'Junting Zhou', 'Dejia Xu', 'Ivan Bercovich', 'Aidan Zhang', 'Lei Li'], 'affiliations': ['Carnegie Mellon University', 'UC Santa Barbara', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.24098.jpg', 'data': {'categories': ['#synthetic', '#reasoning', '#training', '#data', '#open_source', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Улучшение верификации кода с помощью синтетических тестов', 'desc': 'Статья представляет HARDTESTGEN - пайплайн для синтеза высококачественных тестов с использованием больших языковых моделей (LLM). На его основе создан набор данных HARDTESTS, содержащий 47 тысяч задач по соревновательному программированию с синтетическими тестами высокого качества. Тесты HARDTESTGEN показывают значительно более высокую точность и полноту при оценке кода, сгенерированного LLM, по сравнению с существующими тестами. HARDTESTS также оказывается более эффективным для обучения моделей, что измеряется производительностью генерации кода.'}, 'en': {'title': 'Enhancing LLM Evaluation with Synthetic Test Cases', 'desc': 'This paper introduces HARDTESTGEN, a new method for generating high-quality test cases for evaluating large language models (LLMs) in coding tasks. The challenge with existing verifiers is that they often fail to catch subtle errors in code, which can only be identified through complex human-written edge cases. HARDTESTGEN addresses this by synthesizing a dataset called HARDTESTS, which includes 47,000 programming problems along with high-quality tests generated by LLMs. The results show that tests from HARDTESTGEN significantly improve the precision and recall of evaluating LLM-generated code, making it a valuable tool for enhancing model training and performance.'}, 'zh': {'title': '高质量测试合成，提升LLM推理能力', 'desc': '本文提出了一种名为HARDTESTGEN的高质量测试合成管道，旨在解决大型语言模型（LLM）推理中的验证器问题。由于难以为复杂编码问题获取可靠的验证器，HARDTESTGEN能够生成高质量的测试用例，帮助评估LLM生成的代码。我们创建了一个包含47,000个问题的竞争编程数据集HARDTESTS，并且与现有测试相比，HARDTESTGEN的测试在精确度和召回率上都有显著提升。该数据集和合成管道将开源，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2505.24862', 'title': 'ViStoryBench: Comprehensive Benchmark Suite for Story Visualization', 'url': 'https://huggingface.co/papers/2505.24862', 'abstract': "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.", 'score': 20, 'issue_id': 4072, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '40afeafebffbd255', 'authors': ['Cailin Zhuang', 'Ailin Huang', 'Wei Cheng', 'Jingwei Wu', 'Yaoqi Hu', 'Jiaqi Liao', 'Zhewei Huang', 'Hongyuan Wang', 'Xinyao Liao', 'Weiwei Cai', 'Hengyuan Xu', 'Xuanyang Zhang', 'Xianfang Zeng', 'Gang Yu', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'AIGC Research', 'ShanghaiTech University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2505.24862.jpg', 'data': {'categories': ['#dataset', '#cv', '#story_generation', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'ViStoryBench: комплексная оценка визуализации историй', 'desc': 'Статья представляет новый набор данных и систему оценки для задачи визуализации историй - ViStoryBench. Этот бенчмарк включает разнообразные типы сюжетов и художественных стилей, позволяя оценивать модели по различным аспектам генерации изображений. ViStoryBench содержит истории с одним и несколькими персонажами, сложными сюжетами и детальными мирами. Система включает набор метрик для всесторонней оценки качества визуализации, что позволяет выявлять сильные и слабые стороны различных моделей машинного обучения.'}, 'en': {'title': 'Enhancing Story Visualization with ViStoryBench', 'desc': 'This paper introduces ViStoryBench, a new evaluation benchmark designed to improve story visualization models that generate images based on narratives. It features a diverse dataset that includes various story types and artistic styles, allowing for a comprehensive assessment of model performance across different plots and visual aesthetics. The benchmark tests models on their ability to maintain character consistency and handle complex narratives with multiple protagonists. By providing a structured framework and a variety of evaluation metrics, ViStoryBench helps researchers identify strengths and weaknesses in their models, promoting targeted enhancements in story visualization.'}, 'zh': {'title': '提升故事可视化的评估基准', 'desc': '故事可视化旨在生成与给定叙述和参考图像一致的视觉图像序列。为了提升故事可视化框架在实际场景中的表现，我们引入了一个全面的评估基准，称为ViStoryBench。该基准收集了多样化的数据集，涵盖不同类型的故事和艺术风格，确保模型在不同情节和视觉美学上进行评估。ViStoryBench经过精心策划，平衡了叙事结构和视觉元素，帮助研究人员识别不同模型的优缺点，促进有针对性的改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24878', 'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents', 'url': 'https://huggingface.co/papers/2505.24878', 'abstract': 'CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.', 'score': 14, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '9cceceaf09c77468', 'authors': ['Yaxin Luo', 'Zhaoyi Li', 'Jiacheng Liu', 'Jiacheng Cui', 'Xiaohan Zhao', 'Zhiqiang Shen'], 'affiliations': ['MetaAgentX', 'VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24878.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Open CaptchaWorld: вызов для мультимодальных ИИ-агентов', 'desc': 'Статья представляет Open CaptchaWorld - первый веб-бенчмарк для оценки возможностей мультимодальных языковых моделей (MLLM) в решении CAPTCHA. Бенчмарк включает 20 типов современных CAPTCHA, всего 225 задач, с новой метрикой - глубиной рассуждения CAPTCHA. Эксперименты показали, что люди достигают почти идеальных результатов, в то время как лучшие MLLM-агенты справляются максимум с 40% задач. Это подчеркивает важность Open CaptchaWorld для диагностики ограничений современных мультимодальных агентов и разработки более надежных систем рассуждений.'}, 'en': {'title': 'Unlocking the Future: Evaluating MLLM Agents with Open CaptchaWorld', 'desc': 'This paper introduces Open CaptchaWorld, a new benchmark designed to test the capabilities of multimodal large language model (MLLM) agents in solving CAPTCHA puzzles. It evaluates the visual reasoning and interaction skills of these agents through a variety of 225 CAPTCHA types, measuring their performance with a novel metric called CAPTCHA Reasoning Depth. Experimental results reveal that while humans achieve high success rates, MLLM agents struggle significantly, with a maximum success rate of only 40%. This underscores the need for improved multimodal reasoning systems and positions Open CaptchaWorld as a crucial tool for assessing and enhancing agent performance in complex tasks.'}, 'zh': {'title': '突破CAPTCHA瓶颈，提升多模态推理能力！', 'desc': 'CAPTCHA在实际应用中是部署网络代理的一个重要瓶颈，常常阻碍它们完成端到端的自动化任务。虽然现代多模态大语言模型（MLLM）在静态感知任务中表现出色，但它们在处理交互式、多步骤推理挑战（如CAPTCHA）方面的能力尚未得到充分测试。为了解决这个问题，我们推出了Open CaptchaWorld，这是第一个专门设计用于评估MLLM代理的视觉推理和交互能力的网络基准平台，涵盖20种现代CAPTCHA类型，共225个CAPTCHA，并引入了一种新的度量标准：CAPTCHA推理深度。实验结果表明，人类的成功率接近完美，而最先进的MLLM代理的成功率最高仅为40.0%，远低于人类的93.3%，这突显了Open CaptchaWorld作为诊断当前多模态代理局限性的重要基准。'}}}, {'id': 'https://huggingface.co/papers/2505.21437', 'title': 'CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects', 'url': 'https://huggingface.co/papers/2505.21437', 'abstract': 'Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.', 'score': 13, 'issue_id': 4073, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3b71d1aa0666ba54', 'authors': ['Huaijin Pi', 'Zhi Cen', 'Zhiyang Dou', 'Taku Komura'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21437.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'Синтез реалистичных движений всего тела для манипуляции сложными объектами', 'desc': 'Статья представляет новый метод синтеза целостных движений тела при манипуляции сочлененными объектами, включая движения тела, рук и объекта. Авторы предлагают координированную оптимизацию шума диффузии с использованием трех специализированных диффузионных моделей для тела и рук. Для повышения точности взаимодействия руки с объектом применяется унифицированное представление на основе набора базисных точек (BPS). Эксперименты показывают, что метод превосходит существующие подходы по качеству движения и физической достоверности.'}, 'en': {'title': 'Coordinated Motion Synthesis for Realistic Manipulation', 'desc': 'This paper addresses the complex task of synthesizing whole-body movements for manipulating articulated objects in robotics and virtual humans. The authors introduce a coordinated diffusion noise optimization framework that enhances the synchronization between hand and body motions, which is crucial for realistic manipulation. By utilizing specialized diffusion models for different body parts and a unified representation of hand-object interactions, the method improves precision and generalization in motion generation. Experimental results show that this approach surpasses existing methods in terms of motion quality and physical realism, enabling advanced capabilities like simultaneous walking and manipulation.'}, 'zh': {'title': '全身操控的协调优化新方法', 'desc': '本文提出了一种新颖的协调扩散噪声优化框架，用于合成全身操控关节物体的运动，包括身体、手和物体的运动。该方法通过对三个专门的扩散模型进行噪声空间优化，分别针对身体、左手和右手进行训练，以提高模型的泛化能力。通过沿着人体运动链的梯度流动，协调性自然地出现，使得全身姿态能够高保真地响应手部运动目标。实验结果表明，该方法在运动质量和物理合理性方面优于现有方法，并能够实现物体姿态控制、同时行走和操控等多种能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23941', 'title': 'Vision Language Models are Biased', 'url': 'https://huggingface.co/papers/2505.23941', 'abstract': 'Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.', 'score': 12, 'issue_id': 4069, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '1c96442d8acb3ec5', 'authors': ['An Vo', 'Khai-Nguyen Nguyen', 'Mohammad Reza Taesiri', 'Vy Tuong Dang', 'Anh Totti Nguyen', 'Daeyoung Kim'], 'affiliations': ['Auburn University', 'College of William and Mary', 'KAIST', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.23941.jpg', 'data': {'categories': ['#multimodal', '#cv', '#hallucinations', '#ethics', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Предвзятость визуально-языковых моделей: когда знания мешают точности', 'desc': 'Это исследование показывает, что крупные языковые модели (LLM) и визуально-языковые модели (VLM) могут быть сильно предвзяты из-за предварительных знаний, полученных из интернета. Авторы тестируют VLM на задачах подсчета и идентификации объектов, обнаруживая низкую точность (в среднем 17,05%) в различных доменах. Интересно, что добавление текстовой информации о предмете еще больше снижает точность моделей. Инструктирование моделей перепроверять свои результаты или полагаться только на детали изображения лишь незначительно улучшает точность подсчета.'}, 'en': {'title': 'Unveiling Biases in Vision Language Models', 'desc': 'This paper investigates how large language models (LLMs) influence the performance of vision language models (VLMs) on visual tasks like counting and identification. The authors find that VLMs exhibit significant biases, leading to poor accuracy when recognizing visual elements, such as miscounting stripes on logos. Even when provided with counterfactual information, such as the name of the subject, the accuracy of VLMs decreases further. The study highlights a critical failure mode in VLMs and introduces a framework for assessing these biases systematically.'}, 'zh': {'title': '视觉语言模型的偏见问题', 'desc': '大型语言模型（LLMs）从互联网中记忆了大量知识，这对下游任务有帮助，但也可能导致输出偏向错误或有偏见的答案。我们研究了流行主题的知识如何影响视觉语言模型（VLMs）在标准视觉任务（如计数和识别）上的准确性。结果显示，最先进的VLMs在计数任务中的平均准确率仅为17.05%，并且在识别图案时存在明显的偏见。我们的研究揭示了VLMs中的一种有趣的失败模式，并提供了一个自动化框架来测试VLM的偏见。'}}}, {'id': 'https://huggingface.co/papers/2505.24196', 'title': 'CLaSp: In-Context Layer Skip for Self-Speculative Decoding', 'url': 'https://huggingface.co/papers/2505.24196', 'abstract': 'Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.', 'score': 11, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '004f8eaa8c2fe087', 'authors': ['Longze Chen', 'Renke Shan', 'Huiming Wang', 'Lu Wang', 'Ziqiang Liu', 'Run Luo', 'Jiawei Wang', 'Hamid Alinejad-Rokny', 'Min Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24196.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение LLM без компромиссов: CLaSp - новый метод спекулятивного декодирования', 'desc': 'В статье представлен метод CLaSp для ускорения декодирования больших языковых моделей (LLM) с помощью спекулятивного декодирования. CLaSp использует стратегию пропуска слоев в контексте для самоспекулятивного декодирования, не требуя дополнительных модулей или обучения. Метод применяет алгоритм динамического программирования для оптимизации процесса пропуска слоев, используя скрытые состояния последней стадии верификации. Эксперименты показывают, что CLaSp достигает ускорения в 1.3-1.7 раза на моделях серии LLaMA3 без изменения исходного распределения генерируемого текста.'}, 'en': {'title': 'Accelerating LLM Decoding with Layer-Skipping Efficiency', 'desc': 'This paper introduces CLaSp, a novel approach to speculative decoding that enhances the efficiency of Large Language Models (LLMs) without the need for additional training modules. CLaSp utilizes an in-context layer-skipping strategy, allowing it to create a compressed draft model by skipping certain layers in the verify model. The method employs a dynamic programming algorithm to optimize the layer-skipping process, adapting after each verification stage based on the hidden states. Experimental results show that CLaSp can speed up the decoding process by 1.3x to 1.7x on LLaMA3 models while maintaining the quality of the generated text.'}, 'zh': {'title': 'CLaSp：加速解码的新策略', 'desc': '本文提出了一种名为CLaSp的自我推测解码策略，旨在加速大型语言模型的解码过程。CLaSp通过跳过验证模型的中间层，构建一个压缩的草稿模型，从而避免了额外模块的训练需求。该方法利用动态规划算法优化层跳过过程，使其能够在每个验证阶段后动态调整策略。实验结果表明，CLaSp在LLaMA3系列模型上实现了1.3倍到1.7倍的加速，同时保持生成文本的原始分布不变。'}}}, {'id': 'https://huggingface.co/papers/2505.24521', 'title': 'UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation', 'url': 'https://huggingface.co/papers/2505.24521', 'abstract': 'Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.', 'score': 10, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4ae43b7cdb482867', 'authors': ['Yang-Tian Sun', 'Xin Yu', 'Zehuan Huang', 'Yi-Hua Huang', 'Yuan-Chen Guo', 'Ziyi Yang', 'Yan-Pei Cao', 'Xiaojuan Qi'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2505.24521.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Согласованная геометрическая оценка видео с помощью диффузионных моделей', 'desc': 'Статья представляет новый подход к оценке геометрических свойств в видео с использованием диффузионных моделей. Авторы предлагают метод, который позволяет использовать внутреннюю согласованность моделей генерации видео для последовательной геометрической оценки. Они вводят новый метод кондиционирования, переиспользуя позиционные кодировки, и улучшают производительность путем совместного обучения на нескольких геометрических атрибутах. Результаты показывают превосходную производительность в предсказании глобальных геометрических атрибутов в видео и могут быть применены к задачам реконструкции.'}, 'en': {'title': 'Harnessing Diffusion Models for Consistent Geometric Estimation in Videos', 'desc': "This paper explores the use of diffusion models to improve the estimation of geometric properties like depth and normals in videos. Unlike previous methods that focus on individual frames, this approach leverages the relationships between frames to enhance consistency in geometric estimation. The authors propose a novel conditioning method that reuses positional encodings and advocate for joint training on multiple geometric attributes. Their results show improved performance in predicting global geometric attributes, demonstrating the model's ability to generalize even from static video data to dynamic scenes."}, 'zh': {'title': '利用扩散模型提升视频几何估计的一致性', 'desc': '最近，利用扩散模型先验来辅助单目几何估计（如深度和法线）的方法受到了广泛关注，因为它们具有很强的泛化能力。然而，大多数现有工作集中在单个视频帧的相机坐标系内估计几何属性，忽视了扩散模型在确定帧间对应关系方面的固有能力。在本研究中，我们展示了通过适当的设计和微调，可以有效利用视频生成模型的内在一致性来进行一致的几何估计。具体而言，我们选择在全局坐标系中与视频帧共享相同对应关系的几何属性作为预测目标，并引入了一种新颖高效的条件方法，通过重用位置编码来增强性能。'}}}, {'id': 'https://huggingface.co/papers/2505.24858', 'title': 'MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs', 'url': 'https://huggingface.co/papers/2505.24858', 'abstract': "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.", 'score': 9, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '5cc52e721279a4e1', 'authors': ['Gabrielle Kaili-May Liu', 'Gal Yona', 'Avi Caciularu', 'Idan Szpektor', 'Tim G. J. Rudner', 'Arman Cohan'], 'affiliations': ['Google Research', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24858.jpg', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#interpretability', '#hallucinations', '#dataset'], 'emoji': '🎯', 'ru': {'title': 'Научить ИИ честно выражать неуверенность', 'desc': 'Исследование посвящено проблеме надежной коммуникации неопределенности в больших языковых моделях (LLM). Авторы провели систематический анализ способности моделей выражать неуверенность, соответствующую их внутренней неопределенности. Результаты показали, что существующие LLM и методы в основном не справляются с этой задачей. Предложен новый метод калибровки MetaFaith, вдохновленный человеческой метакогнитивностью, который значительно улучшает верную калибровку моделей.'}, 'en': {'title': 'Enhancing Trust in LLMs through Better Uncertainty Communication', 'desc': 'This paper addresses the issue of how large language models (LLMs) communicate uncertainty, which is crucial for building trust in their outputs. The authors conduct a systematic study to evaluate how well LLMs express their confidence levels in a way that matches their actual uncertainty. They find that current methods for improving this communication are largely ineffective, and some can even worsen the situation. To solve this problem, they propose a new method called MetaFaith, which significantly enhances the ability of LLMs to convey uncertainty accurately, leading to better trustworthiness in their responses.'}, 'zh': {'title': '提升大型语言模型的不确定性表达信任度', 'desc': '本研究探讨了大型语言模型（LLMs）在不确定性传达方面的可靠性，指出它们在表达错误信息时常使用过于自信的语言，从而导致用户过度依赖并削弱信任。我们系统地评估了LLMs在使用不确定性语言表达其内在不确定性方面的能力，结果显示大多数模型在这方面表现不佳。现有的干预措施效果有限，标准提示方法仅带来微小改进，而基于事实的校准技术甚至可能对忠实校准产生负面影响。为了解决这一问题，我们提出了MetaFaith，这是一种基于提示的新型校准方法，能够显著提高不同模型和任务领域的忠实校准效果。'}}}, {'id': 'https://huggingface.co/papers/2505.24417', 'title': 'EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering', 'url': 'https://huggingface.co/papers/2505.24417', 'abstract': 'Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.', 'score': 8, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f28c426fafe8156a', 'authors': ['Runnan Lu', 'Yuxuan Zhang', 'Jailing Liu', 'Haifa Wang', 'Yiren Song'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'The Chinese University of Hong Kong', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24417.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#multilingual', '#cv', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'EasyText: прорыв в многоязычном рендеринге текста с помощью диффузионных моделей', 'desc': 'Статья представляет EasyText - фреймворк для рендеринга многоязычного текста, основанный на модели диффузии DiT. Авторы предлагают методы кодирования позиций символов и интерполяции позиционного кодирования для точного рендеринга текста. Для обучения модели был создан большой синтетический датасет с 1 миллионом аннотаций изображений с текстом на разных языках. Эксперименты показывают эффективность подхода в многоязычном рендеринге текста, визуальном качестве и интеграции текста с учетом макета.'}, 'en': {'title': 'EasyText: Multilingual Text Rendering Made Simple', 'desc': "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."}, 'zh': {'title': '多语言文本渲染的新突破', 'desc': '本论文介绍了一种名为EasyText的文本渲染框架，基于扩散变换器（DiT）技术。该框架通过将去噪潜变量与多语言字符令牌连接，实现了对多语言文本的精确渲染。我们提出了字符位置编码和位置编码插值技术，以实现可控和精确的文本渲染。此外，我们构建了一个包含100万条多语言图像-文本注释的大规模合成文本图像数据集，用于预训练和微调，实验结果表明我们的方法在多语言文本渲染和视觉质量方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2505.20873', 'title': 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.20873', 'abstract': 'The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.', 'score': 8, 'issue_id': 4072, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '12352ddbed62a761', 'authors': ['Chaeyoung Jung', 'Youngjoon Jang', 'Jongmin Choi', 'Joon Son Chung'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20873.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🍴', 'ru': {'title': 'Разделяй и властвуй: новый подход к мультимодальному анализу', 'desc': 'Статья представляет стратегию Fork-Merge Decoding (FMD) для улучшения сбалансированного мультимодального понимания в аудио-визуальных больших языковых моделях (AV-LLM). FMD разделяет обработку аудио и видео на ранних слоях декодера, а затем объединяет их для совместного анализа. Этот метод не требует дополнительного обучения или изменения архитектуры модели. Эксперименты показали, что FMD улучшает производительность моделей в задачах, связанных с аудио, видео и комбинированным аудио-визуальным анализом.'}, 'en': {'title': 'Fork-Merge Decoding: Balancing Audio-Visual Insights for Better Understanding', 'desc': 'This paper introduces the Fork-Merge Decoding (FMD) strategy to enhance multimodal understanding in audio-visual large language models (AV-LLMs). The method addresses the issue of modality bias, which occurs when models overly depend on one type of input, like audio or video. FMD operates by first analyzing audio and video inputs separately in the early decoder layers (fork phase) and then combining the insights in later layers (merge phase). This approach allows for a more balanced contribution from both modalities, improving performance on various tasks without needing extra training or changes to the model architecture.'}, 'zh': {'title': '叉合解码：提升多模态理解的有效策略', 'desc': '本文提出了一种名为叉合解码（Fork-Merge Decoding, FMD）的策略，旨在改善音视频大型语言模型（AV-LLMs）的平衡多模态理解。该方法通过在推理阶段分开处理音频和视频输入，先进行模态特定的推理，然后再合并结果，避免了模态偏差的问题。FMD不需要额外的训练或架构修改，简单有效。实验结果表明，该策略在音频、视频及音视频结合推理任务上均表现出一致的性能提升，证明了推理时干预对增强多模态理解的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.24850', 'title': 'Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24850', 'abstract': "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.", 'score': 7, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e946031c286b5bf4', 'authors': ['Shuyao Xu', 'Cheng Peng', 'Jiangxuan Long', 'Weidi Xu', 'Wei Chu', 'Yuan Qi'], 'affiliations': ['AI3 Institute of Fudan University', 'INFLY TECH (Shanghai) Co., Ltd.', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.24850.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#dataset', '#math'], 'emoji': '🧠', 'ru': {'title': 'REDI: Эффективное обучение рассуждениям на положительных и отрицательных примерах', 'desc': 'Статья представляет новый метод дистилляции моделей машинного обучения под названием Reinforcement Distillation (REDI). REDI использует как положительные, так и отрицательные примеры рассуждений для улучшения способностей языковых моделей к логическому мышлению. Метод состоит из двух этапов: обучение на положительных примерах и дальнейшая оптимизация с использованием специальной целевой функции REDI. Эксперименты показывают превосходство REDI над базовыми методами на задачах математических рассуждений, особенно для моделей среднего размера.'}, 'en': {'title': 'Maximizing Reasoning Performance with Reinforcement Distillation', 'desc': 'This paper introduces Reinforcement Distillation (REDI), a two-stage framework designed to enhance the reasoning capabilities of smaller models by utilizing both positive and negative reasoning examples. In the first stage, the model is fine-tuned using positive reasoning traces through Supervised Fine-Tuning (SFT). The second stage refines the model further by incorporating both types of traces with a novel, reference-free loss function that improves performance over traditional methods like DPO and SimPO. Empirical results show that the Qwen-REDI-1.5B model achieves impressive scores on mathematical reasoning tasks, outperforming larger models trained on more extensive proprietary datasets.'}, 'zh': {'title': '强化蒸馏：提升推理性能的新方法', 'desc': '本论文探讨了如何有效利用正负推理轨迹来提升大型语言模型（LLM）的推理性能。我们提出了一种名为强化蒸馏（REDI）的两阶段框架，第一阶段通过监督微调（SFT）学习正推理轨迹，第二阶段则结合正负推理轨迹进一步优化模型。我们的REDI目标是一个简单的无参考损失函数，在蒸馏任务中优于传统方法如DPO和SimPO。实验结果表明，经过131k正负样本训练的Qwen-REDI-1.5B模型在数学推理任务上达到了83.1%的得分，创造了1.5B模型的新状态。'}}}, {'id': 'https://huggingface.co/papers/2505.24293', 'title': 'Large Language Models are Locally Linear Mappings', 'url': 'https://huggingface.co/papers/2505.24293', 'abstract': 'We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.', 'score': 6, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '42a9e20ff9742560', 'authors': ['James R. Golden'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24293.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#inference', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Линейное представление нелинейных языковых моделей', 'desc': 'Исследователи показали, что операции вывода нескольких открытых большим языковых моделей (LLM) можно отобразить в эквивалентную линейную систему для входной последовательности без изменения весов модели или предсказаний. Они расширили методы из моделей диффузии изображений, проявляющих локальную или кусочную линейность, стратегически изменив вычисление градиента для предсказания следующего токена. Этот подход был продемонстрирован на различных моделях, включая Llama 3, Gemma 3 и другие. Анализ сингулярного разложения отсоединенного якобиана показал, что эти LLM работают в экстремально низкоразмерных подпространствах, где многие из крупнейших сингулярных векторов декодируются в концепции, связанные с наиболее вероятным выходным токеном.'}, 'en': {'title': 'Unlocking LLMs: Linear Insights into Complex Predictions', 'desc': "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."}, 'zh': {'title': '揭示大型语言模型的线性本质', 'desc': '本文展示了多个开放权重的大型语言模型（LLMs）的推理操作可以映射到一个完全等价的线性系统，而无需修改模型权重或改变输出预测。我们借鉴了图像扩散模型的技术，通过战略性地改变相对于给定输入序列的梯度计算，使得模型的雅可比矩阵几乎完全重现了线性系统的前向预测。我们在多个模型上验证了这种方法，并通过对分离雅可比矩阵的奇异值分解，发现这些LLMs在极低维的子空间中操作，许多最大的奇异向量解码出与最可能输出标记相关的概念。尽管现代LLMs具有强大的表达能力和全局非线性，但可以通过几乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察，并揭示下一个标记预测过程中的可解释语义结构。'}}}, {'id': 'https://huggingface.co/papers/2505.24615', 'title': 'Harnessing Large Language Models for Scientific Novelty Detection', 'url': 'https://huggingface.co/papers/2505.24615', 'abstract': 'In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.', 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e2151a4cb797ec9b', 'authors': ['Yan Liu', 'Zonglin Yang', 'Soujanya Poria', 'Thanh-Son Nguyen', 'Erik Cambria'], 'affiliations': ['Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.24615.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#science'], 'emoji': '💡', 'ru': {'title': 'LLM на страже научной новизны', 'desc': 'Статья представляет новый подход к обнаружению научной новизны с использованием больших языковых моделей (LLM). Авторы создали два новых набора данных в областях маркетинга и обработки естественного языка для оценки методов определения новизны. Они предлагают обучать легковесную модель-ретривер, дистиллируя знания об идеях из LLM, чтобы эффективно сопоставлять похожие концепции. Эксперименты показывают, что предложенный метод превосходит другие подходы в задачах поиска идей и обнаружения новизны на созданных наборах данных.'}, 'en': {'title': 'Harnessing LLMs for Effective Novelty Detection in Research', 'desc': 'This paper addresses the challenge of identifying novel research ideas in academia, which is hindered by the lack of suitable benchmark datasets for novelty detection (ND). The authors propose using large language models (LLMs) to enhance ND by creating two new datasets focused on marketing and NLP. They introduce a method to extract closure sets of related papers and summarize their main ideas using LLMs, which helps in understanding idea conception. Additionally, a lightweight retriever is trained to distill idea-level knowledge from LLMs, improving the efficiency and accuracy of idea retrieval for ND tasks, with experimental results showing superior performance over existing methods.'}, 'zh': {'title': '利用大型语言模型提升科学新颖性检测', 'desc': '在科学快速发展的时代，识别新颖的研究想法变得至关重要但也充满挑战。现有的自然语言处理技术无法有效解决新颖性检测的问题，因为文本相似性与想法构思之间存在差距。本文提出利用大型语言模型（LLMs）进行科学新颖性检测，并引入了两个新的数据集，分别来自市场营销和自然语言处理领域。我们的方法通过提取论文之间的关系构建数据集，并训练轻量级检索器，从而实现高效准确的想法检索，实验结果表明该方法在新颖性检测任务中优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2505.24517', 'title': "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP", 'url': 'https://huggingface.co/papers/2505.24517', 'abstract': "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.", 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '7cec3d5f1feec6d4', 'authors': ['Yinqi Li', 'Jiahe Zhao', 'Hong Chang', 'Ruibing Hou', 'Shiguang Shan', 'Xilin Chen'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.24517.jpg', 'data': {'categories': ['#games', '#multimodal', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Улучшение CLIP через инвертирование генеративной модели', 'desc': 'Данная статья посвящена улучшению модели CLIP (Contrastive Language-Image Pre-training) для более детального распознавания изображений. Авторы предлагают метод un^2CLIP, основанный на инвертировании генеративной модели unCLIP. Этот подход позволяет улучшенному энкодеру изображений сохранить выравнивание с текстовым энкодером, одновременно приобретая способность захватывать больше визуальных деталей. Эксперименты показывают значительное улучшение производительности un^2CLIP по сравнению с оригинальным CLIP на различных задачах, включая мультимодальные и задачи плотного предсказания.'}, 'en': {'title': 'Enhancing CLIP with Un^2CLIP for Better Image Detail Capture', 'desc': 'This paper addresses the limitations of the Contrastive Language-Image Pre-training (CLIP) model, particularly its inability to capture fine details in images and its performance on dense-prediction tasks. The authors propose a novel approach called un^2CLIP, which utilizes a generative model known as unCLIP to enhance the image encoder of CLIP. By inverting the unCLIP model, the authors aim to improve the detail-capturing capabilities of CLIP while maintaining its alignment with text embeddings. Experimental results demonstrate that un^2CLIP outperforms both the original CLIP and previous enhancement methods across various multimodal tasks.'}, 'zh': {'title': '提升CLIP模型，捕捉更多视觉细节', 'desc': '对比语言-图像预训练（CLIP）已成为基础模型，并广泛应用于各种视觉和多模态任务。然而，最近的研究表明，CLIP在区分图像的细微差别方面存在不足，并且在密集预测和以视觉为中心的多模态任务上表现不佳。因此，本研究旨在改进现有的CLIP模型，以尽可能捕捉图像中的视觉细节。我们发现，一种特定类型的生成模型unCLIP为实现这一目标提供了合适的框架。'}}}, {'id': 'https://huggingface.co/papers/2505.23926', 'title': 'Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2505.23926', 'abstract': 'While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.', 'score': 4, 'issue_id': 4070, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '52d11240fa18198b', 'authors': ['Xuweiyi Chen', 'Wentao Zhou', 'Aruni RoyChowdhury', 'Zezhou Cheng'], 'affiliations': ['The MathWorks, Inc.', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23926.jpg', 'data': {'categories': ['#3d', '#architecture', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Автоматическая специализация экспертов для масштабируемого 3D восприятия', 'desc': 'Статья представляет Point-MoE - архитектуру Mixture-of-Experts для масштабируемого понимания 3D точечных облаков. В отличие от стандартных моделей, Point-MoE способна автоматически специализировать экспертов для разных доменов данных без явных меток. Эксперименты показывают, что Point-MoE превосходит сильные мультидоменные базовые модели и лучше обобщается на новые домены. Это открывает путь к масштабируемому 3D пониманию, позволяя модели самостоятельно обнаруживать структуру в разнородных 3D данных.'}, 'en': {'title': 'Unlocking 3D Perception with Point-MoE: A Scalable Solution for Diverse Data', 'desc': 'This paper addresses the challenges in understanding 3D point clouds due to the diverse sources and characteristics of the data. It introduces Point-MoE, a Mixture-of-Experts architecture that enhances cross-domain generalization in 3D perception without needing domain labels during inference. The proposed method allows the model to automatically specialize its experts based on the data it encounters, improving performance on mixed-domain datasets. Experimental results show that Point-MoE outperforms existing models and effectively generalizes to new, unseen domains, paving the way for scalable 3D understanding.'}, 'zh': {'title': '让3D理解更智能：Point-MoE架构的创新之路', 'desc': '本论文提出了一种名为Point-MoE的混合专家架构，旨在实现3D点云理解的跨域泛化。由于3D数据集规模较小且来源多样，传统的点云模型在混合域数据上表现不佳。Point-MoE通过简单的top-k路由策略，能够在没有域标签的情况下自动专门化专家，从而提高模型的性能。实验结果表明，Point-MoE在多个域的基准测试中表现优于现有方法，并且在未见域上具有更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23009', 'title': 'EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge', 'url': 'https://huggingface.co/papers/2505.23009', 'abstract': "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on EmergentTTS, we introduce EmergentTTS-Eval, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation https://github.com/boson-ai/EmergentTTS-Eval-public{code} and the https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.", 'score': 4, 'issue_id': 4067, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8ed75b2649e36558', 'authors': ['Ruskin Raj Manku', 'Yuzhi Tang', 'Xingjian Shi', 'Mu Li', 'Alex Smola'], 'affiliations': ['Boson AI, Santa Clara, CA 95054'], 'pdf_title_img': 'assets/pdf/title_img/2505.23009.jpg', 'data': {'categories': ['#games', '#benchmark', '#open_source', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Автоматизированная оценка сложных аспектов синтеза речи с помощью ИИ', 'desc': "EmergentTTS-Eval - это новый комплексный бенчмарк для оценки систем Text-to-Speech (TTS). Он использует языковые модели (LLM) и аудио-языковые модели (LALM) для автоматической генерации тестовых случаев и оценки качества синтезированной речи. Бенчмарк охватывает шесть сложных сценариев, включая эмоции, паралингвистику, иностранные слова и сложное произношение. Результаты показывают, что подход 'модель-как-судья' обеспечивает надежную оценку TTS систем и высокую корреляцию с предпочтениями людей."}, 'en': {'title': 'Automating TTS Evaluation for Nuanced Speech Outputs', 'desc': 'The paper presents EmergentTTS-Eval, a new benchmark for evaluating Text-to-Speech (TTS) systems that focuses on complex and nuanced text. It automates the generation of test cases using Large Language Models (LLMs) and evaluates the outputs with a Large Audio Language Model (LALM). The benchmark includes six challenging scenarios, such as emotional expression and complex pronunciation, and generates 1,645 diverse test cases from a small set of human-written prompts. The results show that this automated approach provides a reliable assessment of TTS systems, correlating well with human evaluations.'}, 'zh': {'title': '全面评估文本到语音系统的EmergentTTS-Eval', 'desc': '本文介绍了一个全面的文本到语音（TTS）基准测试工具EmergentTTS-Eval，旨在自动生成和评估测试案例，以评估模型在处理复杂语义文本时的表现。该基准涵盖六种具有挑战性的TTS场景，包括情感、旁语言、外语、句法复杂性、复杂发音（如网址、公式）和问题。通过使用大型语言模型（LLM）迭代扩展人类编写的种子提示，生成了1645个多样化的测试案例。我们还采用了模型作为评判者的方法，利用大型音频语言模型（LALM）从多个维度评估语音输出，结果显示该方法能够有效揭示不同TTS系统之间的细微性能差异。'}}}, {'id': 'https://huggingface.co/papers/2505.21523', 'title': 'More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models', 'url': 'https://huggingface.co/papers/2505.21523', 'abstract': "A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.", 'score': 4, 'issue_id': 4074, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '26f1abb0ea843b21', 'authors': ['Chengzhi Liu', 'Zhongxing Xu', 'Qingyue Wei', 'Juncheng Wu', 'James Zou', 'Xin Eric Wang', 'Yuyin Zhou', 'Sheng Liu'], 'affiliations': ['Stanford University', 'UC Santa Barbara', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.21523.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Балансируя между рассуждениями и восприятием в мультимодальных ИИ-моделях', 'desc': 'Представлена новая метрика RH-AUC и тестовый набор RH-Bench для оценки способности мультимодальных больших языковых моделей сохранять визуальную привязку при выполнении расширенных рассуждений. Анализ внимания показывает, что более длинные цепочки рассуждений приводят к уменьшению фокуса на визуальных входных данных, что способствует галлюцинациям. Исследование выявило, что более крупные модели обычно достигают лучшего баланса между рассуждениями и восприятием. Также было обнаружено, что на этот баланс больше влияют типы и домены обучающих данных, чем их общий объем.'}, 'en': {'title': 'Balancing Reasoning and Visual Grounding in Multimodal Models', 'desc': 'This paper introduces a new metric called RH-AUC to evaluate how well multimodal large language models maintain visual grounding while performing extended reasoning tasks. It highlights that as reasoning chains become longer, models often lose focus on visual inputs, leading to increased hallucination. The study also presents RH-Bench, a benchmark for assessing the trade-off between reasoning ability and hallucination across various multimodal tasks. Findings indicate that larger models and specific types of training data enhance the balance between reasoning and perception, emphasizing the need for evaluation methods that consider both aspects together.'}, 'zh': {'title': '提升推理与视觉感知的平衡', 'desc': '本文介绍了一种新的评估指标和基准，用于评估多模态大型语言模型在进行扩展推理时保持视觉基础的能力。研究发现，较大的模型和特定类型的训练数据可以改善推理与视觉感知之间的平衡。通过引入RH-AUC指标，能够量化模型在推理长度变化时的感知准确性，从而评估模型在推理过程中是否保持视觉基础。我们的分析表明，模型的大小和训练数据的类型对推理能力和幻觉之间的权衡有显著影响。'}}}, {'id': 'https://huggingface.co/papers/2505.13157', 'title': 'Role-Playing Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13157', 'abstract': 'A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval', 'score': 4, 'issue_id': 4073, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'c1362083ff11ec99', 'authors': ['Yassine El Boudouri', 'Walter Nuninger', 'Julian Alvarez', 'Yvan Peter'], 'affiliations': ['Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.13157.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'RPEval: новый подход к оценке ролевых способностей языковых моделей', 'desc': 'В статье представлен новый бенчмарк Role-Playing Eval (RPEval) для оценки способностей больших языковых моделей (LLM) к ролевой игре. RPEval оценивает четыре ключевых аспекта: понимание эмоций, принятие решений, моральное соответствие и последовательность характера. Этот инструмент призван преодолеть ограничения существующих методов оценки, таких как ресурсоемкость человеческих оценок и потенциальная предвзятость автоматизированных подходов. Авторы описывают процесс создания RPEval и приводят результаты базовых оценок.'}, 'en': {'title': 'Assessing Role-Playing Skills in AI with RPEval', 'desc': 'The paper introduces Role-Playing Eval (RPEval), a benchmark for evaluating Large Language Models (LLMs) in their ability to role-play. It focuses on four critical aspects: emotional understanding, decision-making, moral alignment, and in-character consistency. The authors highlight the challenges of traditional evaluation methods, which can be resource-intensive and biased. RPEval aims to provide a standardized approach to assess these capabilities in LLMs, with the code and dataset made publicly available for further research.'}, 'zh': {'title': '角色扮演评估：评估大型语言模型的新基准', 'desc': '本文介绍了一种新的基准测试工具，称为角色扮演评估（Role-Playing Eval，RPEval），用于评估大型语言模型（LLMs）在角色扮演中的能力。该评估涵盖了四个关键维度：情感理解、决策能力、道德一致性和角色一致性。由于人类评估资源消耗大且自动评估可能存在偏见，RPEval旨在提供一种更有效的评估方法。文章详细描述了RPEval的构建过程，并提供了基准评估结果。'}}}, {'id': 'https://huggingface.co/papers/2505.24189', 'title': 'Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows', 'url': 'https://huggingface.co/papers/2505.24189', 'abstract': 'Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.', 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6fcc18713ed36918', 'authors': ['Orlando Marquez Ayala', 'Patrice Bechard', 'Emily Chen', 'Maggie Baird', 'Jingfei Chen'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2505.24189.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#small_models'], 'emoji': '🤖', 'ru': {'title': 'Малые языковые модели все еще имеют преимущество в специализированных задачах', 'desc': 'Исследование сравнивает эффективность малых языковых моделей (SLM) и больших языковых моделей (LLM) для задач, требующих структурированного вывода. Авторы обнаружили, что для специфических задач, таких как генерация низкокодовых рабочих процессов в формате JSON, дообучение SLM дает преимущество в качестве на 10% по сравнению с промптингом LLM. Несмотря на снижение стоимости токенов для LLM, SLM все еще могут быть предпочтительнее из-за более быстрого вывода и меньших затрат. Проведен систематический анализ ошибок для выявления ограничений моделей.'}, 'en': {'title': 'Fine-Tuning SLMs: The Key to Quality in Domain-Specific Tasks', 'desc': 'This paper investigates the effectiveness of Small Language Models (SLMs) compared to Large Language Models (LLMs) like GPT-4o for specific tasks that require structured outputs. It highlights that while LLMs can perform well with appropriate prompts, fine-tuning SLMs can lead to a significant quality improvement, averaging a 10% increase in performance for generating low-code workflows in JSON format. The authors conduct a thorough error analysis to identify the limitations of both model types. Ultimately, the findings suggest that SLMs retain a quality advantage for certain domain-specific applications despite the reduced costs of using LLMs.'}, 'zh': {'title': '小型语言模型在特定任务中的质量优势', 'desc': '大型语言模型（LLMs）如GPT-4o能够通过合适的提示处理多种复杂任务。随着每个token的成本降低，针对实际应用微调小型语言模型（SLMs）的优势可能不再明显。本文提供证据表明，对于需要结构化输出的特定领域任务，SLMs仍然具有质量优势。我们比较了微调SLM与使用提示的LLM在生成JSON格式低代码工作流任务上的表现，发现微调平均提高了10%的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.23844', 'title': 'Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation', 'url': 'https://huggingface.co/papers/2505.23844', 'abstract': 'Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration', 'score': 3, 'issue_id': 4066, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '252af3d7c602c2c3', 'authors': ['Zhenglun Kong', 'Zheng Zhan', 'Shiyue Hou', 'Yifan Gong', 'Xin Meng', 'Pengwei Sui', 'Peiyan Dong', 'Xuan Shen', 'Zifeng Wang', 'Pu Zhao', 'Hao Tang', 'Stratis Ioannidis', 'Yanzhi Wang'], 'affiliations': ['Google', 'Harvard University', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23844.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Умное слияние языковых моделей: адаптивный подход к интеграции знаний', 'desc': 'Эта статья представляет новый подход к улучшению больших языковых моделей (LLM) путем адаптивного отбора и объединения знаний из различных LLM. Авторы предлагают фреймворк, который использует сеть адаптивного выбора для определения наиболее релевантных исходных моделей и динамическую стратегию взвешенного слияния для учета сильных сторон каждой модели. Метод позволяет снизить интерференцию знаний на 50% по сравнению с существующими подходами. Экспериментальные результаты показывают, что предложенный метод обеспечивает более стабильный и масштабируемый процесс агрегации знаний.'}, 'en': {'title': 'Adaptive Knowledge Aggregation for Enhanced LLM Performance', 'desc': 'This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation.'}, 'zh': {'title': '自适应知识聚合，构建更强大的语言模型', 'desc': '大型语言模型（LLMs）在性能上表现出色，但通过传统的微调方法持续改进仍然具有挑战性，尤其是在整合其他专业LLMs的能力时。现有的方法如集成和权重合并需要大量内存，并且难以适应变化的数据环境。我们提出了一种框架，能够自适应地选择和聚合来自不同LLMs的知识，以构建一个更强大的单一模型，避免了集成方法的高内存开销和权重合并的灵活性不足。实验结果表明，我们的方法能够实现更稳定和可扩展的知识聚合过程，同时将知识干扰减少了50%。'}}}, {'id': 'https://huggingface.co/papers/2505.21864', 'title': 'DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2505.21864', 'abstract': "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.", 'score': 3, 'issue_id': 4072, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '5d1867db4168cab6', 'authors': ['Mengda Xu', 'Han Zhang', 'Yifan Hou', 'Zhenjia Xu', 'Linxi Fan', 'Manuela Veloso', 'Shuran Song'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'J.P. Morgan AI Research', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21864.jpg', 'data': {'categories': ['#robotics', '#transfer_learning', '#agents'], 'emoji': '🦾', 'ru': {'title': 'Перенос навыков ловкой манипуляции от человека к роботу', 'desc': 'DexUMI - это фреймворк для сбора данных и обучения политик, который использует человеческую руку в качестве естественного интерфейса для передачи навыков ловкой манипуляции роботизированным рукам. Фреймворк включает аппаратную адаптацию в виде носимого экзоскелета руки и программную адаптацию для замены изображения человеческой руки на робототехническую в видеоданных. DexUMI позволяет минимизировать разрыв между воплощением человеческой и роботизированной руки. В экспериментах на двух различных платформах дексетрозных роботизированных рук была достигнута средняя успешность выполнения задач 86%.'}, 'en': {'title': 'Bridging Human and Robot Dexterity with DexUMI', 'desc': 'The DexUMI framework is designed to enhance the transfer of dexterous manipulation skills from human hands to robotic hands. It combines a wearable hand exoskeleton for direct haptic feedback and a high-fidelity robot hand inpainting technique to create realistic training data. This approach minimizes the embodiment gap by adapting human movements to be compatible with robot hand kinematics. Through extensive real-world testing, DexUMI achieves an impressive average task success rate of 86%, showcasing its effectiveness in robotic skill transfer.'}, 'zh': {'title': 'DexUMI：将人类灵巧技能转移到机器人手的创新框架', 'desc': 'DexUMI框架利用可穿戴手部外骨骼和高保真机器人手部重绘技术，将人类的灵巧操作技能转移到机器人手上，从而实现高任务成功率。该框架通过硬件和软件的适配，缩小了人类手与各种机器人手之间的体现差距。硬件适配使用可穿戴手部外骨骼，允许在数据收集过程中直接获得触觉反馈，并将人类动作适配为可行的机器人手动作。软件适配则通过在视频数据中用高保真机器人手重绘替换人类手，弥补了视觉差距。'}}}, {'id': 'https://huggingface.co/papers/2505.24581', 'title': 'GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training', 'url': 'https://huggingface.co/papers/2505.24581', 'abstract': 'Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '380d404889c022d3', 'authors': ['Omer Nacar', 'Anis Koubaa', 'Serry Sibaee', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, Riyadh, Saudi Arabia', 'Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.24581.jpg', 'data': {'categories': ['#low_resource', '#benchmark', '#multilingual', '#science', '#training', '#transfer_learning', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Прорыв в семантическом анализе арабского текста', 'desc': 'Статья представляет модели GATE (General Arabic Text Embedding) для семантического анализа арабского текста. Эти модели достигают наилучших результатов в задаче определения семантической близости текстов (STS) в рамках бенчмарка MTEB. GATE использует технику Matryoshka Representation Learning и гибридный подход к обучению с арабскими триплетами для задачи логического вывода на естественном языке. Модели GATE превосходят более крупные модели, включая OpenAI, на 20-25% в тестах STS, эффективно улавливая семантические нюансы арабского языка.'}, 'en': {'title': 'Unlocking Arabic Semantics with GATE Models', 'desc': 'This paper addresses the challenge of semantic textual similarity (STS) in the Arabic language, which has been under-researched due to limited datasets and models. It presents the General Arabic Text Embedding (GATE) models, which utilize advanced techniques like Matryoshka Representation Learning and a hybrid loss training approach. GATE demonstrates significant improvements in STS tasks, outperforming larger models by 20-25% on benchmarks. This advancement is crucial for better understanding and processing the unique semantic characteristics of Arabic text.'}, 'zh': {'title': '提升阿拉伯语语义理解的GATE模型', 'desc': '语义文本相似性（STS）是自然语言处理（NLP）中的一个重要任务，能够支持检索、聚类和理解文本之间的语义关系。然而，由于缺乏高质量的数据集和预训练模型，阿拉伯语领域的研究仍然有限。这种资源的匮乏限制了阿拉伯文本语义相似性的准确评估和进展。本文介绍了一种通用阿拉伯文本嵌入（GATE）模型，在MTEB基准测试中实现了语义文本相似性任务的最先进性能，显著提升了阿拉伯语的语义理解能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23832', 'title': 'LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation', 'url': 'https://huggingface.co/papers/2505.23832', 'abstract': 'Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '8778ede47e6e60db', 'authors': ['Chaeeun Kim', 'Jinu Lee', 'Wonseok Hwang'], 'affiliations': ['LBOX', 'University of Illinois Urbana-Champaign', 'University of Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2505.23832.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal', '#reasoning', '#transfer_learning', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LegalSearchLM: Умный поиск в море юридических дел', 'desc': 'Статья представляет новый подход к поиску релевантных юридических дел - LegalSearchLM. Эта модель выполняет рассуждения над элементами запроса и генерирует ответ на основе целевых дел с помощью ограниченного декодирования. Авторы также создали первый крупномасштабный корейский бенчмарк для оценки поиска юридических дел - LEGAR BENCH. LegalSearchLM превосходит базовые модели на 6-20% на LEGAR BENCH и демонстрирует сильную обобщающую способность на новых типах дел.'}, 'en': {'title': 'Revolutionizing Legal Case Retrieval with LEGAR BENCH and LegalSearchLM', 'desc': 'This paper addresses the challenges in Legal Case Retrieval (LCR) by introducing a new benchmark and a novel retrieval model. The authors present LEGAR BENCH, a comprehensive dataset that includes over 1.2 million legal cases and 411 different crime types, which enhances the evaluation of LCR systems. They also propose LegalSearchLM, a model that utilizes legal element reasoning to improve the relevance of retrieved cases by generating content based on the query case. Experimental results indicate that LegalSearchLM significantly outperforms existing methods, demonstrating better accuracy and generalization to diverse legal scenarios.'}, 'zh': {'title': '法律检索的新突破：超越传统方法', 'desc': '法律案件检索（LCR）是法律专业人员在研究和决策中获取相关案例的基本任务。现有的LCR研究存在两个主要限制：一是评估使用的检索语料库规模较小，无法反映真实法律检索场景的复杂性；二是依赖于嵌入或词汇匹配方法，导致表示能力有限和法律无关的匹配。为了解决这些问题，本文提出了LEGAR BENCH，这是第一个涵盖411种犯罪类型和120万法律案例的大规模韩国LCR基准，以及LegalSearchLM，一个能够进行法律元素推理并生成与目标案例相关内容的检索模型。实验结果表明，LegalSearchLM在LEGAR BENCH上比基线模型提高了6-20%的性能，并在跨领域案例中表现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20047', 'title': 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.20047', 'abstract': "This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.", 'score': 2, 'issue_id': 4075, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '86bdf56529c01563', 'authors': ['Debargha Ganguly', 'Vikash Singh', 'Sreehari Sankar', 'Biyao Zhang', 'Xuecen Zhang', 'Srinivasan Iyengar', 'Xiaotian Han', 'Amit Sharma', 'Shivkumar Kalyanaraman', 'Vipin Chaudhary'], 'affiliations': ['Case Western Reserve University', 'Microsoft Corporation', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.20047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#interpretability', '#data', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности формальных спецификаций, генерируемых нейросетями', 'desc': 'Исследование посвящено квантификации неопределенности в больших языковых моделях (LLM) при генерации формальных спецификаций. Авторы представляют фреймворк на основе вероятностных контекстно-свободных грамматик (PCFG) для улучшения обнаружения ошибок и выборочной верификации. Оценка пяти современных LLM показала значительное влияние автоформализации на основе SMT на точность в зависимости от домена. Предложенный подход позволяет существенно снизить количество ошибок при минимальном отказе от верификации, делая формализацию на основе LLM более надежной.'}, 'en': {'title': 'Bridging Uncertainty and Formal Verification in LLMs', 'desc': 'This paper investigates how to measure and manage uncertainty in large language models (LLMs) when they generate formal specifications. It highlights the challenge that LLMs are probabilistic, while formal verification requires certainty. The authors propose a new framework using probabilistic context-free grammar (PCFG) to better understand and categorize the uncertainty in LLM outputs. Their findings show that different tasks exhibit unique uncertainty patterns, and by combining these signals, they can significantly improve the accuracy of formal verification processes.'}, 'zh': {'title': '提升LLM生成规范的可靠性', 'desc': '本研究探讨了在大型语言模型中进行不确定性量化，以生成正式规范。我们引入了一种概率上下文无关文法（PCFG）框架，以提高错误检测和选择性验证的能力。研究表明，LLM生成的正式文档在不同任务中的不确定性信号是依赖于任务的，且现有的不确定性量化技术未能有效识别这些错误。通过轻量级融合这些信号，我们显著减少了错误率，使LLM驱动的形式化过程变得更加可靠。'}}}, {'id': 'https://huggingface.co/papers/2505.24119', 'title': 'The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It', 'url': 'https://huggingface.co/papers/2505.24119', 'abstract': 'This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.', 'score': 1, 'issue_id': 4076, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f5b6ed3de0b4cc5b', 'authors': ['Zheng-Xin Yong', 'Beyza Ermis', 'Marzieh Fadaee', 'Stephen H. Bach', 'Julia Kreutzer'], 'affiliations': ['Brown University', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.24119.jpg', 'data': {'categories': ['#low_resource', '#survey', '#ethics', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового барьера в исследованиях безопасности ИИ', 'desc': 'Данная статья представляет всесторонний анализ лингвистического разнообразия в исследованиях безопасности больших языковых моделей (LLM), подчеркивая их англоцентричность. Авторы провели систематический обзор почти 300 публикаций за 2020-2024 годы на крупных конференциях и семинарах по обработке естественного языка. Исследование выявило значительный и растущий языковой разрыв в исследованиях безопасности LLM, при этом даже высокоресурсные неанглийские языки получают минимальное внимание. На основе своего анализа авторы предлагают рекомендации и конкретные направления для будущих исследований в области многоязычной безопасности ИИ.'}, 'en': {'title': 'Bridging the Language Gap in LLM Safety Research', 'desc': 'This paper analyzes the focus on English in large language model (LLM) safety research, revealing a significant language gap. It reviews nearly 300 publications from major NLP conferences between 2020 and 2024, showing that non-English languages, even those with ample resources, are largely overlooked. The authors highlight the lack of standalone studies on non-English languages and poor documentation practices in English safety research. They propose recommendations and future directions to enhance multilingual safety evaluation, data generation, and cross-lingual safety generalization, aiming for more inclusive AI safety practices.'}, 'zh': {'title': '推动多语言LLM安全研究的未来方向', 'desc': '这篇论文对大型语言模型（LLM）安全研究的语言多样性进行了全面分析，强调了该领域以英语为中心的特点。通过对2020年至2024年间近300篇来自主要自然语言处理（NLP）会议和研讨会的出版物进行系统审查，我们发现LLM安全研究中存在显著且日益扩大的语言差距，甚至高资源的非英语语言也受到的关注很少。我们进一步观察到，非英语语言很少作为独立语言进行研究，而英语安全研究的文献记录实践也很差。为了激励未来的多语言安全研究，我们根据调查结果提出了几项建议，并提出了关于安全评估、训练数据生成和跨语言安全泛化的三个具体未来方向。'}}}, {'id': 'https://huggingface.co/papers/2505.20977', 'title': 'Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2505.20977', 'abstract': 'MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a MC\\textsuperscript{2} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.', 'score': 1, 'issue_id': 4076, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e3f911b43ad30492', 'authors': ['Yu Zhang', 'Jinlong Ma', 'Yongshuai Hou', 'Xuefeng Bai', 'Kehai Chen', 'Yang Xiang', 'Jun Yu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20977.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#machine_translation', '#hallucinations'], 'emoji': '🔬', 'ru': {'title': 'Контроль предвзятости модальностей в мультимодальных языковых моделях', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) демонстрируют предвзятость к определенным модальностям при обработке мультимодального контекста. Авторы разработали бенчмарк MC² для оценки предпочтений модальностей в сценариях с конфликтующими свидетельствами. Анализ выявил, что направление предпочтений может быть обнаружено в скрытых представлениях MLLM. На основе этого предложен метод инженерии представлений для контроля предпочтений модальностей, который улучшает такие задачи как смягчение галлюцинаций и мультимодальный машинный перевод.'}, 'en': {'title': 'Controlling Modality Bias in Multimodal Models', 'desc': 'This paper investigates how multimodal large language models (MLLMs) show a tendency to favor one type of input (modality) over another when processing mixed information. The authors create a benchmark called MC² to evaluate this modality bias under specific conditions where evidence from different modalities conflicts. They find that all tested MLLMs exhibit clear modality preferences, which can be adjusted using a new method based on representation engineering. This method allows for the control of modality bias without needing to retrain the models, leading to better performance in tasks like reducing hallucinations and improving multimodal translations.'}, 'zh': {'title': '控制模态偏好，提升多模态任务表现', 'desc': '多模态大型语言模型（MLLMs）在处理复杂的多模态任务时表现出色，但它们在处理多模态上下文时可能存在模态偏好。我们建立了一个MC²基准，以系统评估在证据冲突场景下的模态偏好。研究发现，所有测试的18个MLLMs普遍表现出明显的模态偏见，并且这种偏好可以通过外部干预进行调整。基于此，我们提出了一种基于表示工程的探测和引导方法，可以在不进行额外微调的情况下显式控制模态偏好，从而在幻觉缓解和多模态机器翻译等下游任务中取得显著改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24782', 'title': 'Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings', 'url': 'https://huggingface.co/papers/2505.24782', 'abstract': 'A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.', 'score': 0, 'issue_id': 4077, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '54b58cb49740c12b', 'authors': ['Max Conti', 'Manuel Faysse', 'Gautier Viaud', 'Antoine Bosselut', 'Céline Hudelot', 'Pierre Colombo'], 'affiliations': ['CentraleSupélec, Paris-Saclay', 'EPFL Lausanne', 'Equall.ai', 'Illuin Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.24782.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекстное обогащение для умного поиска документов', 'desc': 'Авторы представили новый бенчмарк ConTEB для оценки способности моделей извлечения документов использовать контекст всего документа. Они также предложили метод контрастивного дообучения InSeNT, который улучшает контекстуальные представления, сохраняя вычислительную эффективность. Результаты показывают, что их подход значительно повышает качество извлечения на ConTEB без ухудшения базовой производительности модели. Метод также делает вложения фрагментов более устойчивыми к неоптимальным стратегиям разбиения и увеличению размера корпуса для поиска.'}, 'en': {'title': 'Enhancing Document Retrieval with Contextual Awareness', 'desc': 'This paper addresses the limitations of current document retrieval methods that often treat text passages independently, missing out on important context from the entire document. It introduces ConTEB, a benchmark that evaluates how well retrieval models utilize document-wide context. The authors propose InSeNT, a new contrastive training method that enhances the learning of contextual representations while maintaining efficiency. Their findings demonstrate that this approach significantly improves retrieval quality, making the models more robust to various chunking strategies and larger datasets.'}, 'zh': {'title': '提升文档检索质量的上下文感知方法', 'desc': '本文提出了一种上下文感知的基准测试和对比训练方法，以提高文档检索的质量。现有的文档检索嵌入方法通常独立编码文档中的段落，忽视了文档整体的上下文信息。我们引入了ConTEB（上下文感知文本嵌入基准），用于评估检索模型利用文档上下文的能力。通过提出InSeNT（序列内负训练），我们的方法在保持计算效率的同时，显著提升了检索质量。'}}}, {'id': 'https://huggingface.co/papers/2505.21749', 'title': 'Revisiting Bi-Linear State Transitions in Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2505.21749', 'abstract': 'Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.  \t\t\t\t\tAI-generated summary \t\t\t\t The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.', 'score': 0, 'issue_id': 4077, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cb7a7df613a84e6e', 'authors': ['M. Reza Ebrahimi', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.21749.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Билинейность - ключ к эффективному отслеживанию состояний в РНС', 'desc': 'В статье рассматриваются билинейные операции в рекуррентных нейронных сетях как естественное предрасположение для задач отслеживания состояний. Авторы демонстрируют, что скрытые слои активно участвуют в вычислениях, а не просто хранят информацию. Теоретически и эмпирически показано, что билинейные обновления состояний образуют естественную иерархию, соответствующую задачам отслеживания состояний возрастающей сложности. Линейные рекуррентные сети, такие как Mamba, находятся в центре этой иерархии с наименьшей сложностью.'}, 'en': {'title': 'Bilinear Operations: Enhancing RNNs for State Tracking', 'desc': 'This paper explores the role of bilinear operations in recurrent neural networks (RNNs) for state tracking tasks. It argues that hidden units should be seen as active contributors to computations rather than just memory stores. The authors demonstrate that bilinear interactions between hidden units and input embeddings provide a beneficial inductive bias for evolving hidden states. Additionally, they establish a hierarchy of state tracking tasks, with linear RNNs like Mamba representing the simplest form of this structure.'}, 'zh': {'title': '双线性操作：状态跟踪的自然偏置', 'desc': '在递归神经网络中，双线性操作被证明是状态跟踪任务的自然偏置，形成了一种层次结构，其中线性递归网络是最简单的形式。隐藏单元的角色通常被视为建模记忆，研究主要集中在通过门控机制增强信息保留。本文重新审视双线性操作，展示它们在状态跟踪任务中如何自然地表示隐藏状态的演变。我们还表明，双线性状态更新形成了一个自然的层次结构，适用于复杂性逐渐增加的状态跟踪任务。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (3)', '#architecture (8)', '#audio (1)', '#benchmark (16)', '#cv (6)', '#data (4)', '#dataset (12)', '#diffusion (3)', '#ethics (2)', '#games (4)', '#graphs', '#hallucinations (4)', '#healthcare', '#inference (3)', '#interpretability (5)', '#leakage', '#long_context', '#low_resource (2)', '#machine_translation (1)', '#math (2)', '#multilingual (3)', '#multimodal (12)', '#open_source (6)', '#optimization (12)', '#plp', '#rag', '#reasoning (11)', '#rl (2)', '#rlhf', '#robotics (2)', '#science (3)', '#security', '#small_models (1)', '#story_generation (1)', '#survey (2)', '#synthetic (3)', '#training (12)', '#transfer_learning (5)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-02 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-02 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-02 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    