
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 518 papers. June 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Июнь 2025</span> | <span id="title-articles-count">518 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-05.html">⬅️ <span id="prev-date">05.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-07.html">➡️ <span id="next-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Июнь 2025', 'en': 'June 2025', 'zh': '6月2025年'};
        let feedDateNext = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let feedDatePrev = {'ru': '05.2025', 'en': '05/2025', 'zh': '5月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.05010', 'title': 'ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development', 'url': 'https://huggingface.co/papers/2506.05010', 'abstract': 'ComfyUI-Copilot uses a large language model and multi-agent system to enhance the usability and efficiency of the AI-driven art creation platform ComfyUI.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.', 'score': 49, 'issue_id': 4157, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '5d1aa2eb9189bc56', 'authors': ['Zhenran Xu', 'Xue Yang', 'Yiyu Wang', 'Qingli Hu', 'Zijiao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce', 'Harbin Institute of Technology (Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05010.jpg', 'data': {'categories': ['#multimodal', '#agents', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'ИИ-ассистент для творчества: ComfyUI-Copilot упрощает создание цифрового искусства', 'desc': 'ComfyUI-Copilot - это плагин на основе большой языковой модели, разработанный для улучшения удобства использования и эффективности платформы ComfyUI для создания искусства с помощью ИИ. Система использует иерархическую мультиагентную структуру с центральным агентом-ассистентом и специализированными рабочими агентами. ComfyUI-Copilot предлагает интеллектуальные рекомендации по узлам и моделям, а также автоматизированное построение рабочего процесса в один клик. Эффективность системы подтверждена как офлайн-оценками, так и отзывами пользователей, показывающими, что она точно рекомендует узлы и ускоряет разработку рабочих процессов.'}, 'en': {'title': 'Empowering Art Creation with Intelligent Assistance', 'desc': 'ComfyUI-Copilot is a plugin that leverages a large language model and a multi-agent system to improve the ComfyUI platform for AI art creation. It addresses common challenges faced by users, such as limited documentation and complex workflows, by providing intelligent recommendations and automating workflow construction. The system features a hierarchical structure with a central assistant agent that delegates tasks to specialized worker agents, enhancing usability and efficiency. Validation through user feedback and quantitative evaluations demonstrates that ComfyUI-Copilot effectively lowers barriers for beginners while streamlining processes for experienced users.'}, 'zh': {'title': '智能助手，轻松创作艺术', 'desc': 'ComfyUI-Copilot 是一个基于大型语言模型的插件，旨在提升 ComfyUI 这一开源 AI 艺术创作平台的可用性和效率。该系统通过提供智能节点和模型推荐，以及一键式自动化工作流构建，解决了新手用户在使用 ComfyUI 时面临的挑战。它采用了分层的多代理框架，中央助手代理负责任务分配，而专门的工作代理则处理不同的使用场景。通过离线定量评估和在线用户反馈，我们验证了 ComfyUI-Copilot 的有效性，显示其能够准确推荐节点并加速工作流开发。'}}}, {'id': 'https://huggingface.co/papers/2506.05301', 'title': 'SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training', 'url': 'https://huggingface.co/papers/2506.05301', 'abstract': 'SeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.', 'score': 48, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b4eef08c89e4a692', 'authors': ['Jianyi Wang', 'Shanchuan Lin', 'Zhijie Lin', 'Yuxi Ren', 'Meng Wei', 'Zongsheng Yue', 'Shangchen Zhou', 'Hao Chen', 'Yang Zhao', 'Ceyuan Yang', 'Xuefeng Xiao', 'Chen Change Loy', 'Lu Jiang'], 'affiliations': ['ByteDance', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05301.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#training', '#video', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Эффективное восстановление видео за один шаг', 'desc': 'SeedVR2 - это однопроходная модель восстановления видео на основе диффузии. Она использует адаптивное оконное внимание и функцию сопоставления признаков для достижения высокого визуального качества при меньших вычислительных затратах. Модель применяет состязательное обучение на реальных данных для решения сложной задачи восстановления видео высокого разрешения за один шаг. Эксперименты показывают, что SeedVR2 достигает сравнимых или лучших результатов по сравнению с существующими подходами к восстановлению видео, выполняя обработку за один проход.'}, 'en': {'title': 'Efficient Video Restoration with SeedVR2: One-Step Diffusion Redefined', 'desc': 'SeedVR2 is a novel one-step diffusion-based model designed for video restoration that enhances visual quality while minimizing computational costs. It introduces an adaptive window attention mechanism that dynamically adjusts to the output resolution, addressing issues of window inconsistency in high-resolution video. The model also employs a feature matching loss to stabilize adversarial training, ensuring effective performance without compromising efficiency. Experimental results indicate that SeedVR2 outperforms or matches existing video restoration methods, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'SeedVR2：高效视频修复的新选择', 'desc': 'SeedVR2是一种基于扩散的一步视频修复模型，采用自适应窗口注意力机制和特征匹配损失，能够在降低计算成本的同时实现高视觉质量。该模型通过对真实数据进行对抗性训练，解决了高分辨率视频修复的挑战。为了适应不同输出分辨率，SeedVR2动态调整窗口大小，避免了高分辨率视频修复中窗口不一致的问题。此外，实验结果表明，SeedVR2在单步修复中能够达到或超过现有视频修复方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04308', 'title': 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics', 'url': 'https://huggingface.co/papers/2506.04308', 'abstract': 'Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 39, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ef5abd087929ed17', 'authors': ['Enshen Zhou', 'Jingkun An', 'Cheng Chi', 'Yi Han', 'Shanyu Rong', 'Chi Zhang', 'Pengwei Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beihang University', 'Beijing Academy of Artificial Intelligence', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#robotics', '#reasoning', '#dataset', '#3d', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'RoboRefer: Пространственный интеллект для роботов нового поколения', 'desc': 'RoboRefer - это модель пространственного понимания для роботов, основанная на зрении и языке. Она использует специальный энкодер глубины и обучение с подкреплением для точного понимания 3D-сцен и рассуждений о пространственных отношениях. Авторы также представили большой набор данных RefSpatial и бенчмарк RefSpatial-Bench для обучения и оценки модели. RoboRefer превзошла современные методы, включая Gemini-2.5-Pro, и может применяться для управления различными роботами в реальных условиях.'}, 'en': {'title': 'RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments', 'desc': 'The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks.'}, 'zh': {'title': 'RoboRefer：提升机器人空间理解与推理能力的创新模型', 'desc': '空间指向是具身机器人与三维物理世界互动的基本能力。尽管现有的预训练视觉语言模型（VLMs）很强大，但它们在理解复杂的三维场景和动态推理指示位置方面仍然存在不足。为此，我们提出了RoboRefer，这是一种具有三维感知能力的VLM，通过监督微调（SFT）集成了专门的深度编码器，实现了精确的空间理解。此外，RoboRefer通过强化微调（RFT）推进了多步骤空间推理，采用针对空间指向任务的度量敏感过程奖励函数。'}}}, {'id': 'https://huggingface.co/papers/2506.05284', 'title': 'Video World Models with Long-term Spatial Memory', 'url': 'https://huggingface.co/papers/2506.05284', 'abstract': "A new framework enhances video world models' long-term consistency by integrating a geometry-grounded long-term spatial memory mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Emerging world models autoregressively generate video frames in response to actions, such as camera movements and text prompts, among other control signals. Due to limited temporal context window sizes, these models often struggle to maintain scene consistency during revisits, leading to severe forgetting of previously generated environments. Inspired by the mechanisms of human memory, we introduce a novel framework to enhancing long-term consistency of video world models through a geometry-grounded long-term spatial memory. Our framework includes mechanisms to store and retrieve information from the long-term spatial memory and we curate custom datasets to train and evaluate world models with explicitly stored 3D memory mechanisms. Our evaluations show improved quality, consistency, and context length compared to relevant baselines, paving the way towards long-term consistent world generation.", 'score': 37, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '313ae4b0ffca864d', 'authors': ['Tong Wu', 'Shuai Yang', 'Ryan Po', 'Yinghao Xu', 'Ziwei Liu', 'Dahua Lin', 'Gordon Wetzstein'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05284.jpg', 'data': {'categories': ['#video', '#dataset', '#3d', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Улучшение долгосрочной памяти в видео-моделях мира', 'desc': 'Новая система улучшает долгосрочную согласованность видео-моделей мира, интегрируя механизм долгосрочной пространственной памяти, основанный на геометрии. Эта система вдохновлена механизмами человеческой памяти и включает в себя методы хранения и извлечения информации из долгосрочной пространственной памяти. Авторы создали специальные наборы данных для обучения и оценки моделей мира с явно сохраняемыми 3D-механизмами памяти. Оценки показывают улучшение качества, согласованности и длины контекста по сравнению с соответствующими базовыми моделями.'}, 'en': {'title': 'Enhancing Video World Models with Long-Term Spatial Memory', 'desc': 'This paper presents a new framework that improves the long-term consistency of video world models by incorporating a geometry-grounded long-term spatial memory mechanism. Traditional autoregressive models often forget previously generated scenes due to limited temporal context, which affects their ability to maintain consistency during revisits. The proposed framework mimics human memory by allowing the model to store and retrieve spatial information effectively, enhancing the overall quality of generated video frames. Evaluations demonstrate that this approach leads to better scene consistency and longer context retention compared to existing methods.'}, 'zh': {'title': '增强视频世界模型的一致性', 'desc': '本文提出了一种新框架，通过集成基于几何的长期空间记忆机制，增强视频世界模型的长期一致性。现有的世界模型在生成视频帧时，因时间上下文窗口大小有限，常常难以保持场景的一致性，导致对先前生成环境的严重遗忘。我们借鉴人类记忆机制，设计了存储和检索长期空间记忆的信息机制，并使用定制数据集来训练和评估具有显式存储3D记忆机制的世界模型。评估结果显示，与相关基线相比，我们的方法在质量、一致性和上下文长度上都有所提升，为长期一致的世界生成铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2506.05229', 'title': 'Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers\n  for Long Contexts', 'url': 'https://huggingface.co/papers/2506.05229', 'abstract': 'Transformer models struggle with long-context inference due to their quadratic time and linear memory complexity. Recurrent Memory Transformers (RMTs) offer a solution by reducing the asymptotic cost to linear time and constant memory usage. However, their memory update mechanism leads to sequential execution, causing a performance bottleneck.   We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism across segments in RMTs while preserving exact recurrence. This approach eliminates the sequential constraint, enabling efficient GPU inference even for single long-context inputs without complex batching and pipelining techniques. Because the technique is purely a run-time computation reordering, existing RMT models adopt it with no retraining.   Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential RMT implementation on 131,072-token sequences. By removing sequential bottleneck, Diagonal Batching reduces inference cost and latency, thereby strengthening RMTs as a practical solution for real-world, long-context applications.', 'score': 34, 'issue_id': 4163, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c1c7fcda2cc6ed7a', 'authors': ['Danil Sivtsov', 'Ivan Rodkin', 'Gleb Kuzmin', 'Yuri Kuratov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Moscow, Russia', 'FRC CSC RAS, Moscow, Russia', 'MBZUAI, Abu Dhabi, UAE', 'Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia', 'Skoltech, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.05229.jpg', 'data': {'categories': ['#architecture', '#inference', '#optimization', '#training', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Ускорение обработки длинных последовательностей в рекуррентных трансформерах', 'desc': 'Статья представляет новый метод планирования вычислений для рекуррентных моделей трансформеров (RMT), называемый Diagonal Batching. Этот подход позволяет распараллелить обработку сегментов в RMT, сохраняя при этом точную рекуррентность. Diagonal Batching устраняет ограничения последовательного выполнения, что значительно ускоряет вывод на GPU для длинных последовательностей. Применение этого метода к модели LLaMA-1B ARMT показало ускорение в 3.3 раза по сравнению со стандартной моделью LLaMA-1B и в 1.8 раза по сравнению с последовательной реализацией RMT на последовательностях длиной 131 072 токена.'}, 'en': {'title': 'Unlocking Parallelism for Efficient Long-Context Inference', 'desc': 'This paper addresses the limitations of Transformer models in handling long-context inference due to their high computational costs. It introduces Recurrent Memory Transformers (RMTs) that improve efficiency by reducing time complexity to linear and memory usage to constant. The authors propose a novel scheduling method called Diagonal Batching, which allows for parallel processing of segments in RMTs, overcoming the sequential execution bottleneck. By implementing this technique, they achieve significant speedups in inference times for long sequences, making RMTs more viable for practical applications.'}, 'zh': {'title': '对角批处理：提升长上下文推理效率的创新方案', 'desc': '本文介绍了一种新的调度方案，称为对角批处理（Diagonal Batching），旨在解决递归记忆变换器（RMTs）在长上下文推理中的性能瓶颈。传统的RMT由于其内存更新机制，导致了顺序执行，从而影响了性能。对角批处理通过在RMT中实现并行处理，消除了顺序限制，使得在单个长上下文输入上也能高效推理。该技术无需重新训练现有模型，应用于LLaMA-1B ARMT模型时，速度提升达3.3倍，显著降低了推理成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.05176', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models', 'url': 'https://huggingface.co/papers/2506.05176', 'abstract': "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.", 'score': 34, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '90ebf52dd91334c2', 'authors': ['Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Xin Zhang', 'Huan Lin', 'Baosong Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Junyang Lin', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05176.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#multilingual', '#open_source', '#training', '#small_models', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Qwen3 Embedding: Новый стандарт многоязычных текстовых эмбеддингов', 'desc': 'В работе представлена серия моделей Qwen3 Embedding, улучшающая возможности текстовых эмбеддингов и ранжирования по сравнению с предшественником GTE-Qwen. Модели основаны на фундаментальных моделях Qwen3 и используют многоэтапный процесс обучения, включающий масштабную предварительную подготовку и тонкую настройку на качественных датасетах. Qwen3 Embedding предлагает модели различных размеров (0.6B, 4B, 8B) для задач эмбеддинга и ранжирования, что позволяет оптимизировать их под разные сценарии применения. Эмпирические оценки показывают, что серия Qwen3 Embedding достигает передовых результатов в различных бенчмарках, особенно в многоязычной оценке MTEB и задачах поиска.'}, 'en': {'title': 'Empowering Multilingual Text Understanding with Qwen3 Embeddings', 'desc': 'The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research.'}, 'zh': {'title': 'Qwen3嵌入系列：多语言文本处理的新突破', 'desc': '本文介绍了Qwen3嵌入系列，这是在文本嵌入和重排序能力上相较于GTE-Qwen系列的重大进展。该系列基于Qwen3基础模型，利用其强大的多语言文本理解和生成能力，采用多阶段训练流程，结合大规模无监督预训练和高质量数据集的监督微调。通过有效的模型合并策略，Qwen3嵌入系列确保了模型的鲁棒性和适应性，提供了多种模型规模以满足不同的部署场景。实证评估表明，Qwen3嵌入系列在多项基准测试中取得了最先进的结果，尤其在多语言评估基准MTEB上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.05209', 'title': 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text', 'url': 'https://huggingface.co/papers/2506.05209', 'abstract': 'The Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are typically trained on enormous quantities of unlicensed text, a practice that has led to scrutiny due to possible intellectual property infringement and ethical concerns. Training LLMs on openly licensed text presents a first step towards addressing these issues, but prior data collection efforts have yielded datasets too small or low-quality to produce performant LLMs. To address this gap, we collect, curate, and release the Common Pile v0.1, an eight terabyte collection of openly licensed text designed for LLM pretraining. The Common Pile comprises content from 30 sources that span diverse domains including research papers, code, books, encyclopedias, educational materials, audio transcripts, and more. Crucially, we validate our efforts by training two 7 billion parameter LLMs on text from the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion tokens respectively. Both models attain competitive performance to LLMs trained on unlicensed text with similar computational budgets, such as Llama 1 and 2 7B. In addition to releasing the Common Pile v0.1 itself, we also release the code used in its creation as well as the training mixture and checkpoints for the Comma v0.1 models.', 'score': 27, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '18ac0c75007ddff5', 'authors': ['Nikhil Kandpal', 'Brian Lester', 'Colin Raffel', 'Sebastian Majstorovic', 'Stella Biderman', 'Baber Abbasi', 'Luca Soldaini', 'Enrico Shippole', 'A. Feder Cooper', 'Aviya Skowron', 'John Kirchenbauer', 'Shayne Longpre', 'Lintang Sutawika', 'Alon Albalak', 'Zhenlin Xu', 'Guilherme Penedo', 'Loubna Ben Allal', 'Elie Bakouch', 'John David Pressman', 'Honglu Fan', 'Dashiell Stander', 'Guangyu Song', 'Aaron Gokaslan', 'Tom Goldstein', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Tyler Murray'], 'affiliations': ['CMU', 'Cornell University', 'EleutherAI', 'Hugging Face', 'Independent', 'Lawrence Livermore National', 'Lila Sciences', 'MIT', 'Teraflop AI', 'The Allen Institute for', 'University of Maryland, College Park', 'University of Toronto Artificial Intelligence', 'Vector Institute', 'poolside'], 'pdf_title_img': 'assets/pdf/title_img/2506.05209.jpg', 'data': {'categories': ['#dataset', '#ethics', '#open_source', '#data'], 'emoji': '📚', 'ru': {'title': 'Открытые данные для этичного ИИ', 'desc': 'Исследователи представили набор данных Common Pile v0.1 - 8-терабайтную коллекцию текстов с открытой лицензией для обучения языковых моделей. На основе этих данных были обучены две модели Comma v0.1 с 7 миллиардами параметров, показавшие результаты на уровне аналогичных моделей, обученных на нелицензированных текстах. Набор данных включает разнообразные источники: научные статьи, код, книги, энциклопедии и другие. Это первый шаг к решению этических проблем и вопросов интеллектуальной собственности при обучении больших языковых моделей.'}, 'en': {'title': 'Openly Licensed Text for Competitive LLM Training', 'desc': "The paper introduces the Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text aimed at training large language models (LLMs). This dataset addresses ethical concerns related to using unlicensed text by providing a high-quality, diverse source of data from various domains. The authors validate the dataset's effectiveness by training two competitive 7 billion parameter LLMs, Comma v0.1-1T and Comma v0.1-2T, which demonstrate performance comparable to LLMs trained on unlicensed data. Additionally, the paper includes the release of the dataset, training code, and model checkpoints to support further research."}, 'zh': {'title': '开放许可文本助力大型语言模型的训练', 'desc': 'Common Pile v0.1 数据集是一个包含 8TB 开放许可文本的集合，旨在训练具有 70 亿参数的竞争性大型语言模型（LLM）。该数据集汇集了来自 30 个不同领域的内容，包括研究论文、代码、书籍、百科全书和教育材料等。通过在 Common Pile 上训练的 Comma v0.1-1T 和 Comma v0.1-2T 模型，验证了该数据集的有效性，这两个模型在性能上与使用未授权文本训练的 LLM 相当。此研究为解决知识产权和伦理问题提供了一个重要的第一步。'}}}, {'id': 'https://huggingface.co/papers/2506.02865', 'title': 'Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights', 'url': 'https://huggingface.co/papers/2506.02865', 'abstract': 'Surfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated data sources, including open-access web content, synthetic examples, and self-produced agentic data. Holo1 tops generalist User Interface (UI) benchmarks as well as our new web UI localization benchmark, WebClick. When powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on WebVoyager, striking a Pareto-optimal balance between accuracy and cost-efficiency. To accelerate research advancement in agentic systems, we are open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.', 'score': 27, 'issue_id': 4162, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '18e38be0b8df6445', 'authors': ['Mathieu Andreux', 'Breno Baldas Skuk', 'Hamza Benchekroun', 'Emilien Biré', 'Antoine Bonnet', 'Riaz Bordie', 'Matthias Brunel', 'Pierre-Louis Cedoz', 'Antoine Chassang', 'Mickaël Chen', 'Alexandra D. Constantinou', "Antoine d'Andigné", 'Hubert de La Jonquière', 'Aurélien Delfosse', 'Ludovic Denoyer', 'Alexis Deprez', 'Augustin Derupti', 'Michael Eickenberg', 'Mathïs Federico', 'Charles Kantor', 'Xavier Koegler', 'Yann Labbé', 'Matthew C. H. Lee', 'Erwan Le Jumeau de Kergaradec', 'Amir Mahla', 'Avshalom Manevich', 'Adrien Maret', 'Charles Masson', 'Rafaël Maurin', 'Arturo Mena', 'Philippe Modard', 'Axel Moyal', 'Axel Nguyen Kerbel', 'Julien Revelle', 'Mats L. Richter', 'María Santos', 'Laurent Sifre', 'Maxime Theillard', 'Marc Thibault', 'Louis Thiry', 'Léo Tronchon', 'Nicolas Usunier', 'Tony Wu'], 'affiliations': ['Company'], 'pdf_title_img': 'assets/pdf/title_img/2506.02865.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#agents', '#data', '#benchmark'], 'emoji': '🏄', 'ru': {'title': 'Surfer-H и Holo1: Эффективная веб-навигация с помощью ИИ', 'desc': 'Статья представляет Surfer-H - эффективного веб-агента, использующего модели компьютерного зрения и обработки естественного языка (VLM) для выполнения задач в интернете. Surfer-H интегрирован с Holo1 - новым набором специализированных VLM для веб-навигации и извлечения информации. Holo1 показывает высокие результаты на общих тестах пользовательских интерфейсов и новом бенчмарке WebClick. В сочетании с Holo1, Surfer-H достигает 92.2% точности на тесте WebVoyager, оптимально балансируя между эффективностью и стоимостью.'}, 'en': {'title': 'Surfer-H: Efficient Web Navigation with Vision-Language Models', 'desc': 'The paper introduces Surfer-H, a web agent designed to efficiently navigate and perform tasks using Vision-Language Models (VLMs). It is paired with Holo1, a collection of open-weight VLMs that excel in web navigation and information extraction. Holo1 is trained on diverse data sources, ensuring high performance on various benchmarks, including a new web UI localization benchmark called WebClick. Surfer-H achieves impressive results, reaching 92.2% accuracy on the WebVoyager task, while maintaining cost-efficiency, and both the evaluation dataset and model weights are made available for further research.'}, 'zh': {'title': '高效网络代理，智能导航新选择', 'desc': 'Surfer-H是一种高性价比的网络代理，结合了视觉-语言模型（VLM）来执行用户定义的网络任务。它与Holo1配对，Holo1是一个新的开放权重VLM集合，专注于网络导航和信息提取。Holo1在经过精心策划的数据源上训练，包括开放访问的网络内容和合成示例，表现出色，尤其是在用户界面基准测试中。通过Holo1，Surfer-H在WebVoyager上达到了92.2%的最佳性能，实现了准确性和成本效率的帕累托最优平衡。'}}}, {'id': 'https://huggingface.co/papers/2506.05240', 'title': 'Aligning Latent Spaces with Flow Priors', 'url': 'https://huggingface.co/papers/2506.05240', 'abstract': 'This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.', 'score': 25, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c776325b1f9f6966', 'authors': ['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05240.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#cv', '#math', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Выравнивание латентных пространств с помощью потоковых моделей', 'desc': 'Статья представляет новый подход к выравниванию обучаемых латентных пространств с произвольными целевыми распределениями, используя генеративные модели на основе потоков в качестве априорных. Метод сначала предобучает модель потока на целевых признаках, а затем использует ее для регуляризации латентного пространства через функцию потерь выравнивания. Авторы доказывают, что минимизация этой функции потерь является вычислительно эффективным суррогатом для максимизации вариационной нижней границы правдоподобия латентов. Эффективность подхода подтверждается экспериментами по генерации изображений на ImageNet с различными целевыми распределениями.'}, 'en': {'title': 'Aligning Latent Spaces with Flow-Based Models', 'desc': "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."}, 'zh': {'title': '潜在空间对齐的新方法', 'desc': '本文提出了一种新颖的框架，通过利用基于流的生成模型作为先验，将可学习的潜在空间对齐到任意目标分布。我们的方法首先在目标特征上预训练流模型，以捕捉潜在分布。然后，这个固定的流模型通过对齐损失来规范化潜在空间，重新构造流匹配目标，将潜在变量视为优化目标。我们正式证明，最小化这个对齐损失建立了一个计算上可处理的替代目标，用于最大化潜在变量在目标分布下的变分下界的对数似然。'}}}, {'id': 'https://huggingface.co/papers/2505.23656', 'title': 'VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models', 'url': 'https://huggingface.co/papers/2505.23656', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.', 'score': 24, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'aa7bbc7378df2b3e', 'authors': ['Xiangdong Zhang', 'Jiaqi Liao', 'Shaofeng Zhang', 'Fanqing Meng', 'Xiangpeng Wan', 'Junchi Yan', 'Yu Cheng'], 'affiliations': ['Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University', 'NetMind.AI', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.23656.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Физически достоверное видео из текста: VideoREPA улучшает понимание физики в моделях T2V', 'desc': 'В статье представлен новый метод VideoREPA для улучшения физической достоверности видео, генерируемых моделями text-to-video (T2V). Авторы предлагают дистиллировать понимание физики из моделей видеопонимания в модели T2V путем выравнивания отношений на уровне токенов. Ключевым элементом метода является функция потерь Token Relation Distillation (TRD), которая обеспечивает мягкое руководство для дообучения предобученных моделей T2V. Эмпирические оценки показывают, что VideoREPA значительно улучшает физический здравый смысл базового метода CogVideoX.'}, 'en': {'title': 'Bridging the Physics Gap in Text-to-Video Generation', 'desc': 'This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation.'}, 'zh': {'title': 'VideoREPA：提升文本到视频模型的物理理解能力', 'desc': '最近，文本到视频（T2V）扩散模型的进展使得高保真和真实的视频合成成为可能。然而，当前的T2V模型在生成物理上合理的内容方面常常面临挑战，因为它们对物理的理解能力有限。我们提出了一种新框架，称为VideoREPA，通过对齐令牌级关系，将视频理解基础模型中的物理理解能力提炼到T2V模型中，从而缩小物理理解的差距。我们的实验表明，VideoREPA显著增强了基线方法CogVideoX的物理常识，能够生成与直观物理一致的视频。'}}}, {'id': 'https://huggingface.co/papers/2506.05349', 'title': 'VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal\n  Understanding in Videos', 'url': 'https://huggingface.co/papers/2506.05349', 'abstract': "VideoMathQA evaluates models' ability to perform temporally extended cross-modal reasoning across various mathematical domains in video settings, addressing direct problem solving, conceptual transfer, and deep instructional comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA", 'score': 22, 'issue_id': 4159, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '7f9f8448e60cfdb4', 'authors': ['Hanoona Rasheed', 'Abdelrahman Shaker', 'Anqi Tang', 'Muhammad Maaz', 'Ming-Hsuan Yang', 'Salman Khan', 'Fahad Khan'], 'affiliations': ['Australian National University', 'Google Research', 'Linköping University', 'MBZUAI', 'University of California Merced'], 'pdf_title_img': 'assets/pdf/title_img/2506.05349.jpg', 'data': {'categories': ['#math', '#multimodal', '#benchmark', '#transfer_learning', '#reasoning', '#video'], 'emoji': '🧮', 'ru': {'title': 'VideoMathQA: Новый рубеж в оценке математических рассуждений ИИ на основе видео', 'desc': 'VideoMathQA - это новый бенчмарк для оценки способности моделей выполнять кросс-модальные рассуждения в математических задачах на основе видео. Он охватывает 10 различных математических областей и включает видео продолжительностью от 10 секунд до более чем часа. Бенчмарк оценивает три ключевых навыка: прямое решение задач, концептуальный перенос и глубокое понимание инструкций. VideoMathQA демонстрирует ограничения существующих подходов и устанавливает систематическую основу для оценки моделей, способных рассуждать в математических задачах с временной протяженностью и мультимодальным контекстом.'}, 'en': {'title': 'VideoMathQA: Advancing Cross-Modal Reasoning in Mathematics', 'desc': 'VideoMathQA is a benchmark that tests how well models can reason about mathematics in videos, which is more complex than in static images or text. It focuses on understanding visual information, reading text, and integrating spoken cues that are often spread out over time. The benchmark includes questions that require direct problem solving, applying learned concepts to new situations, and understanding detailed instructions. By analyzing model performance on these tasks, VideoMathQA aims to identify the strengths and weaknesses of current approaches in handling complex, multimodal reasoning in mathematical contexts.'}, 'zh': {'title': '视频数学推理的新挑战', 'desc': 'VideoMathQA 是一个评估模型在视频环境中进行跨模态推理能力的基准，特别是在数学领域。它要求模型能够理解复杂的视觉信息、手写或数字文本，并整合分散的语音提示。该基准涵盖了10个不同的数学领域，问题设计围绕直接问题解决、概念转移和深度教学理解三个核心挑战。通过这个基准，我们揭示了现有方法的局限性，并建立了一个系统的评估框架，以测试模型在复杂的数学问题设置中的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05345', 'title': 'Inference-Time Hyper-Scaling with KV Cache Compression', 'url': 'https://huggingface.co/papers/2506.05345', 'abstract': 'Inference-time hyper-scaling with Dynamic Memory Sparsification in Transformer LLMs allows for increased token generation within the same compute budget by compressing the key-value cache, thereby enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8times compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets.', 'score': 21, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '172bab27eb20c036', 'authors': ['Adrian Łańcucki', 'Konrad Staniszewski', 'Piotr Nawrot', 'Edoardo M. Ponti'], 'affiliations': ['NVIDIA', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.05345.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Больше токенов, выше точность: сжатие памяти для эффективного вывода языковых моделей', 'desc': 'Статья представляет метод динамического разреживания памяти (Dynamic Memory Sparsification, DMS) для сжатия кэша ключ-значение в трансформерных моделях большого языка. DMS позволяет генерировать больше токенов при том же объеме вычислений, что повышает точность вывода. Метод требует всего 1000 шагов обучения для достижения 8-кратного сжатия, сохраняя при этом лучшую точность, чем методы разреженного внимания без обучения. Эксперименты показывают значительное улучшение результатов на различных наборах данных для нескольких семейств языковых моделей.'}, 'en': {'title': 'Boosting Token Generation with Dynamic Memory Sparsification', 'desc': 'This paper presents a method called Dynamic Memory Sparsification (DMS) to improve the efficiency of Transformer large language models (LLMs) during inference. By compressing the key-value (KV) cache, DMS allows for the generation of more tokens without increasing the computational cost, thus enhancing reasoning accuracy. The method achieves significant compression while preserving important information, which is crucial for maintaining model performance. The results show that DMS can boost accuracy across various LLMs while keeping the inference runtime and memory usage stable.'}, 'zh': {'title': '动态内存稀疏化：推理时的超规模扩展', 'desc': '本文提出了一种在推理时进行超规模扩展的方法，利用动态内存稀疏化技术来压缩键值缓存，从而在相同的计算预算内生成更多的令牌，并提高推理的准确性。传统的推理扩展往往在效率与推理准确性之间进行权衡，而我们的研究表明，推理成本主要受限于键值缓存的大小。通过压缩键值缓存，我们能够在不增加计算负担的情况下，生成更长或更多的并行序列。我们的方法在多个大型语言模型上验证了其有效性，显示出在相似的推理运行时间和内存负载下，准确性得到了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.05328', 'title': 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs', 'url': 'https://huggingface.co/papers/2506.05328', 'abstract': "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.", 'score': 20, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '774ccf3fd01aa4ef', 'authors': ['Lidong Lu', 'Guo Chen', 'Zhiqi Li', 'Yicheng Liu', 'Tong Lu'], 'affiliations': ['Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#reasoning', '#dataset', '#long_context', '#training', '#video', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Продвинутый подсчет объектов в видео с помощью мультимодального ИИ', 'desc': 'Статья представляет новый бенчмарк CG-AV-Counting для задач подсчета объектов в длинных видео с использованием мультимодальных вопросов и аннотированных подсказок. Авторы предлагают модель AV-Reasoner, обученную с помощью обучения с подкреплением и куррикулярного обучения для улучшения способности подсчета. Модель достигает лучших результатов на нескольких бенчмарках, демонстрируя эффективность обучения с подкреплением. Однако эксперименты показывают, что рассуждения в языковом пространстве не приносят улучшений на бенчмарках вне домена обучения.'}, 'en': {'title': 'Enhancing Video Counting with CG-AV-Counting and AV-Reasoner', 'desc': 'This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts.'}, 'zh': {'title': '提升视频计数能力的新基准与模型', 'desc': '尽管视频理解取得了进展，但当前的多模态学习模型在计数任务上仍然存在困难。现有的基准测试受限于短视频、封闭式查询、缺乏线索注释和多模态覆盖不足。本文介绍了CG-AV-Counting，这是一个手动注释的线索基础计数基准，包含1,027个多模态问题和5,845个注释线索，覆盖497个长视频。我们提出的AV-Reasoner模型通过GRPO和课程学习进行训练，能够从相关任务中推广计数能力，并在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2506.04633', 'title': 'Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual\n  Simulations', 'url': 'https://huggingface.co/papers/2506.04633', 'abstract': 'Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce STARE(Spatial Transformations and Reasoning Evaluation), a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 4K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.', 'score': 18, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '05d0ae7d805419c1', 'authors': ['Linjie Li', 'Mahtab Bigverdi', 'Jiawei Gu', 'Zixian Ma', 'Yinuo Yang', 'Ziang Li', 'Yejin Choi', 'Ranjay Krishna'], 'affiliations': ['Stanford University', 'Sun Yat-sen University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04633.jpg', 'data': {'categories': ['#multimodal', '#3d', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'STARE: Новый рубеж в оценке пространственного интеллекта ИИ', 'desc': 'Статья представляет новый бенчмарк STARE для оценки мультимодальных языковых моделей в задачах пространственного мышления и визуального моделирования. Бенчмарк включает 4000 заданий на геометрические преобразования, пространственное мышление и реальные пространственные задачи. Результаты показывают, что модели хорошо справляются с простыми 2D-преобразованиями, но близки к случайному угадыванию в сложных задачах, требующих многошаговых визуальных симуляций. Люди демонстрируют почти идеальную точность, но тратят значительное время на сложные задачи, существенно ускоряясь при использовании промежуточных визуальных симуляций.'}, 'en': {'title': 'STARE: Bridging the Gap in Spatial Reasoning for AI', 'desc': 'This paper presents STARE, a new benchmark for evaluating multimodal large language models on spatial reasoning tasks that require visual simulations. Unlike existing benchmarks that focus on verbal reasoning, STARE includes 4,000 tasks involving geometric transformations and real-world spatial challenges. The results show that while models perform well on simple 2D tasks, they struggle with complex 3D tasks that require multi-step reasoning. This indicates that current models have difficulty effectively using visual information to enhance their reasoning capabilities.'}, 'zh': {'title': 'STARE：评估空间推理的新基准', 'desc': '空间认知对人类智能至关重要，它使我们能够通过视觉模拟解决问题，而不仅仅依赖语言推理。现有的人工智能基准主要评估语言推理，忽视了非语言多步骤视觉模拟的复杂性。我们提出了STARE（空间变换与推理评估），这是一个旨在严格评估多模态大型语言模型在多步骤视觉模拟任务上的基准。评估结果显示，模型在简单的2D变换上表现良好，但在更复杂的任务上，如3D立方体展开和拼图，表现接近随机，表明模型可能无法有效利用中间视觉信息。'}}}, {'id': 'https://huggingface.co/papers/2506.05344', 'title': 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs', 'url': 'https://huggingface.co/papers/2506.05344', 'abstract': 'Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.', 'score': 15, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '0175b3788ebacf29', 'authors': ['Jiahui Wang', 'Zuyan Liu', 'Yongming Rao', 'Jiwen Lu'], 'affiliations': ['Tencent Hunyuan Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05344.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#architecture', '#open_source', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зрение: оптимизация визуального восприятия в мультимодальных ИИ', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) используют лишь небольшую часть (менее 5%) механизмов внимания для обработки визуальной информации. Авторы разработали метод SparseMM для оптимизации KV-кэша, который распределяет вычислительные ресурсы асимметрично, основываясь на визуальной релевантности головок внимания. Этот подход позволяет ускорить вывод MLLM в 1,38 раза и сократить использование памяти на 52% при сохранении производительности. Метод SparseMM показывает лучшее соотношение точности и эффективности по сравнению с существующими методами оптимизации KV-кэша.'}, 'en': {'title': 'Optimizing Visual Understanding in MLLMs with Sparse Attention', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks.'}, 'zh': {'title': '利用稀疏视觉头加速多模态模型推理', 'desc': '多模态大型语言模型（MLLMs）通过扩展预训练的大型语言模型（LLMs）来增加视觉能力。我们研究了MLLMs如何处理视觉输入，分析了它们的注意力机制。我们发现了一个惊人的稀疏现象：在LLMs中，只有少量（大约5%以下）的注意力头积极参与视觉理解，这些被称为视觉头。基于这一发现，我们提出了SparseMM，一种KV-Cache优化策略，根据视觉得分为LLMs中的头分配不对称的计算预算，从而加速MLLMs的推理。'}}}, {'id': 'https://huggingface.co/papers/2506.04734', 'title': 'Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design', 'url': 'https://huggingface.co/papers/2506.04734', 'abstract': 'Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.', 'score': 15, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'ce810e0cc38b26e4', 'authors': ['Lin Sun', 'Weihong Lin', 'Jinzhu Wu', 'Yongfu Zhu', 'Xiaoqi Jian', 'Guangxiang Zhao', 'Change Jia', 'Linglin Zhang', 'Sai-er Hu', 'Yuhan Wu', 'Xiangzheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.04734.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': '🎢', 'ru': {'title': 'Нестабильность оценок: вызов для бенчмаркинга языковых моделей', 'desc': 'Исследование показывает, что результаты оценки моделей серии Deepseek-R1-Distill подвержены значительным колебаниям из-за различных факторов. Небольшие изменения в условиях оценки могут привести к существенным различиям в результатах. Аналогичные явления наблюдаются и в других моделях, основанных на Deepseek-R1-Distill, а также в модели QwQ-32B. Авторы призывают к созданию более строгой парадигмы оценки производительности моделей машинного обучения.'}, 'en': {'title': 'Ensuring Reliable Evaluations for Deep Learning Models', 'desc': 'The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results.'}, 'zh': {'title': '建立更严格的模型评估标准', 'desc': 'Deepseek-R1-Distill系列模型在数学、科学和编程等领域表现出色，受到开源社区的广泛采用。然而，我们的研究发现，这些模型的基准评估结果受到多种因素的显著波动影响。评估条件的细微差异可能导致结果的重大变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型中，因此我们呼吁建立更严格的模型性能评估范式。'}}}, {'id': 'https://huggingface.co/papers/2506.03077', 'title': 'StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs', 'url': 'https://huggingface.co/papers/2506.03077', 'abstract': "StreamBP, a memory-efficient and exact backpropagation method, decomposes the chain rule to reduce memory costs, enabling longer sequence lengths and faster training speeds for language models compared to gradient checkpointing.  \t\t\t\t\tAI-generated summary \t\t\t\t Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.", 'score': 15, 'issue_id': 4156, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '14c578e79a6d095c', 'authors': ['Qijun Luo', 'Mengqi Li', 'Lei Zhao', 'Xiao Li'], 'affiliations': ['Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03077.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'StreamBP: Революция в обучении языковых моделей', 'desc': 'StreamBP - это эффективный метод обратного распространения ошибки для обучения языковых моделей. Он разлагает правило цепочки вдоль последовательности, что значительно снижает затраты памяти на хранение активаций и логитов. StreamBP позволяет обрабатывать более длинные последовательности и ускоряет обучение по сравнению с методом контрольных точек градиента. Метод применим к различным целевым функциям и эффективно масштабируется на несколько GPU.'}, 'en': {'title': 'StreamBP: Efficient Backpropagation for Long Sequences in Language Models', 'desc': 'StreamBP is a novel backpropagation method designed to efficiently handle long sequence lengths in language models. By decomposing the chain rule in a layer-wise manner, it significantly reduces the memory required for storing activation values during training. This method not only speeds up the backpropagation process but also allows for longer sequences compared to traditional gradient checkpointing techniques. Additionally, StreamBP can be easily integrated into existing transformer training pipelines and supports multi-GPU setups for enhanced performance.'}, 'zh': {'title': 'StreamBP：高效反向传播，助力长序列训练', 'desc': 'StreamBP是一种高效的反向传播方法，通过对链式法则进行线性分解，显著降低了内存消耗。这使得在训练语言模型时，可以处理更长的序列并加快训练速度。与传统的梯度检查点技术相比，StreamBP能够将反向传播的最大序列长度提高2.8到5.5倍，同时保持相似或更少的反向传播时间。此外，StreamBP还支持多GPU训练，增强了其在实际应用中的灵活性。'}}}, {'id': 'https://huggingface.co/papers/2506.05334', 'title': 'Search Arena: Analyzing Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.05334', 'abstract': "Search Arena is a large-scale human-preference dataset that analyzes user interactions with search-augmented language models, revealing insights into citation influence and source credibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.", 'score': 14, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '531f22c16daba821', 'authors': ['Mihran Miroyan', 'Tsung-Han Wu', 'Logan King', 'Tianle Li', 'Jiayi Pan', 'Xinyan Hu', 'Wei-Lin Chiang', 'Anastasios N. Angelopoulos', 'Trevor Darrell', 'Narges Norouzi', 'Joseph E. Gonzalez'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.05334.jpg', 'data': {'categories': ['#alignment', '#data', '#multilingual', '#dataset', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Глубокий анализ взаимодействия пользователей с ИИ-системами, дополненными поиском', 'desc': 'Search Arena - это масштабный набор данных о предпочтениях пользователей при взаимодействии с языковыми моделями, дополненными поиском. Исследование показывает, что количество цитат влияет на восприятие достоверности ответов, даже если цитируемый контент напрямую не поддерживает утверждения. Анализ выявил предпочтения пользователей к определенным источникам информации, причем платформы, основанные на сообществах, оказались более популярными. Набор данных охватывает разнообразные намерения и языки, содержит полные системные трассировки и около 12 000 оценок предпочтений пользователей.'}, 'en': {'title': 'Unveiling User Preferences in Search-Augmented Language Models', 'desc': 'Search Arena is a comprehensive dataset designed to study user interactions with search-augmented language models (LLMs). It includes over 24,000 multi-turn interactions and 12,000 human preference votes, providing insights into how users perceive citation influence and source credibility. The findings indicate that user preferences are swayed by the number of citations, regardless of their relevance, highlighting a disconnect between perceived and actual credibility. Additionally, the dataset reveals that users favor community-driven sources over static encyclopedic ones, and it demonstrates that web search can enhance performance in various contexts, challenging the notion that LLMs should rely solely on their internal knowledge.'}, 'zh': {'title': '揭示搜索增强模型中的用户偏好与可信度', 'desc': 'Search Arena是一个大规模的人类偏好数据集，分析用户与搜索增强语言模型的互动，揭示了引用影响和来源可信度的见解。该数据集包含超过24,000对多轮用户交互，涵盖多种意图和语言，并提供了约12,000个用户偏好投票的完整系统记录。分析结果显示，用户偏好受到引用数量的影响，即使被引用的内容并不直接支持所声称的观点。此外，用户对不同引用来源的偏好存在差异，表明社区驱动的平台通常更受欢迎，而静态的百科全书来源并不总是合适和可靠。'}}}, {'id': 'https://huggingface.co/papers/2506.05287', 'title': 'EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an\n  Egocentric World?', 'url': 'https://huggingface.co/papers/2506.05287', 'abstract': "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems.", 'score': 13, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '8b357b07e4b449cd', 'authors': ['Yuqian Yuan', 'Ronghao Dang', 'Long Li', 'Wentong Li', 'Dian Jiao', 'Xin Li', 'Deli Zhao', 'Fan Wang', 'Wenqiao Zhang', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05287.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#interpretability', '#benchmark', '#games'], 'emoji': '👁️', 'ru': {'title': 'EOC-Bench: Новый стандарт для оценки когнитивных способностей ИИ в эгоцентрическом зрении', 'desc': 'Статья представляет новый бенчмарк EOC-Bench для оценки понимания объектов в динамических эгоцентрических сценариях мультимодальными языковыми моделями. EOC-Bench включает 3277 аннотированных пар вопрос-ответ, охватывающих три временные категории и 11 измерений оценки. Авторы разработали систему аннотаций с участием человека и новую метрику для оценки временной точности. EOC-Bench позволяет комплексно оценивать возможности мультимодальных языковых моделей в понимании объектов в воплощенных системах.'}, 'en': {'title': 'EOC-Bench: Advancing Object Cognition in Dynamic Environments', 'desc': "This paper introduces EOC-Bench, a new benchmark for evaluating multimodal large language models (MLLMs) in dynamic environments where users interact with objects. Unlike previous benchmarks that focus on static scenes, EOC-Bench assesses how well models understand objects in changing contexts by using 3,277 annotated question-answer pairs across different time frames. The benchmark includes a unique human-in-the-loop annotation framework and a multi-scale temporal accuracy metric to evaluate models' performance in real-time interactions. By providing a comprehensive evaluation tool, EOC-Bench aims to enhance the cognitive abilities of MLLMs in embodied applications."}, 'zh': {'title': '动态场景中的物体认知评估新基准', 'desc': '多模态大型语言模型（MLLMs）的出现推动了以自我为中心的视觉应用的突破。这些应用需要对物体进行持续的、上下文感知的理解，因为用户在动态和杂乱的环境中与工具互动。然而，现有的体现基准主要集中在静态场景探索上，强调物体的外观和空间属性，而忽视了用户互动所带来的动态变化评估。为了解决这个问题，我们引入了EOC-Bench，这是一个创新的基准，旨在系统地评估动态自我中心场景中的以物体为中心的体现认知。'}}}, {'id': 'https://huggingface.co/papers/2506.05331', 'title': 'MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical\n  Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.05331', 'abstract': 'Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT', 'score': 12, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b3e1648a048ac6b7', 'authors': ['Xinyan Chen', 'Renrui Zhang', 'Dongzhi Jiang', 'Aojun Zhou', 'Shilin Yan', 'Weifeng Lin', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05331.jpg', 'data': {'categories': ['#training', '#multimodal', '#dataset', '#reasoning', '#math', '#games'], 'emoji': '🧮', 'ru': {'title': 'Визуальное рассуждение в математике: новый уровень с MINT-CoT', 'desc': 'Статья представляет MINT-CoT - новый подход к визуальному рассуждению в математических задачах с использованием цепочки размышлений (Chain-of-Thought). Метод адаптивно внедряет релевантные визуальные токены в текстовые шаги рассуждения, динамически выбирая визуальные области любой формы в математических фигурах. Авторы создали датасет MINT-CoT, содержащий 54 тысячи математических задач, где каждый шаг рассуждения согласован с визуальными областями на уровне токенов. Разработанная модель MINT-CoT-7B значительно превосходит базовые модели на нескольких бенчмарках математического визуального рассуждения.'}, 'en': {'title': 'Enhancing Math Reasoning with Visual Interleaving in LLMs', 'desc': 'This paper introduces MINT-CoT, a novel approach to enhance mathematical reasoning in Large Language Models (LLMs) by integrating visual information more effectively. The method uses Mathematical Interleaved Tokens to dynamically select and incorporate relevant visual regions into the reasoning process, overcoming limitations of previous models that relied on fixed image regions. The authors created a dataset with 54,000 math problems that align visual tokens with reasoning steps, enabling better training of the model. Experimental results show that MINT-CoT-7B significantly outperforms existing models in various math problem-solving tasks, demonstrating its effectiveness in multimodal reasoning.'}, 'zh': {'title': 'MINT-CoT：数学推理的新突破', 'desc': '本论文提出了一种新的方法MINT-CoT，用于在多模态领域中增强大型语言模型的数学推理能力。MINT-CoT通过引入数学交错标记，将相关的视觉信息动态地融入文本推理步骤中，从而解决了传统方法在数学问题解决中的局限性。我们构建了一个包含54K数学问题的数据集，确保每个推理步骤与视觉区域在标记级别上对齐。实验结果表明，MINT-CoT-7B模型在多个数学任务上显著优于基线模型，展示了其在视觉交错推理中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.05327', 'title': 'Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.05327', 'abstract': 'Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss', 'score': 11, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '374ddd58ce0653c6', 'authors': ['Duochao Shi', 'Weijie Wang', 'Donny Y. Chen', 'Zeyu Zhang', 'Jia-Wang Bian', 'Bohan Zhuang', 'Chunhua Shen'], 'affiliations': ['GigaAI', 'MBZUAI', 'Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.05327.jpg', 'data': {'categories': ['#optimization', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение 3D-реконструкции с помощью умной регуляризации границ объектов', 'desc': 'Статья представляет новый метод регуляризации для улучшения качества 3D-реконструкции в системах 3D Gaussian Splatting. Авторы предлагают PM-Loss - функцию потерь, основанную на предсказании карты точек с помощью предобученного трансформера. Этот подход позволяет сгладить геометрические разрывы на границах объектов, которые часто возникают при использовании карт глубины. Применение PM-Loss значительно улучшает качество рендеринга для различных архитектур и сцен.'}, 'en': {'title': 'Enhancing 3D Rendering with PM-Loss for Smooth Depth Representation', 'desc': 'This paper presents PM-Loss, a new regularization technique designed to enhance the quality of 3D point clouds generated from depth maps in 3D Gaussian Splatting (3DGS) systems. The authors address the common issue of depth discontinuities at object boundaries, which can lead to poor rendering quality due to fragmented point clouds. By utilizing a pointmap predicted by a pre-trained transformer, PM-Loss promotes geometric smoothness, improving the overall accuracy of the depth representation. The results demonstrate that incorporating PM-Loss leads to superior rendering outcomes across different architectures and scenes.'}, 'zh': {'title': '提升3D渲染质量的新方法', 'desc': '本文提出了一种新的正则化损失函数PM-Loss，用于改善基于深度图的3D高斯点云渲染。传统方法在物体边界处的深度不连续性会导致点云稀疏，从而影响渲染质量。PM-Loss利用预训练的变换器预测的点图，尽管其准确性不如深度图，但能有效增强几何平滑性。通过改进深度图，我们的方法在不同架构和场景中显著提升了3D高斯点云的渲染效果。'}}}, {'id': 'https://huggingface.co/papers/2506.02620', 'title': 'FlexPainter: Flexible and Multi-View Consistent Texture Generation', 'url': 'https://huggingface.co/papers/2506.02620', 'abstract': 'FlexPainter, a novel texture generation pipeline, uses a shared conditional embedding space to enable flexible multi-modal guidance, ensuring high-quality and consistent texture map generation using image diffusion priors and a 3D-aware model.  \t\t\t\t\tAI-generated summary \t\t\t\t Texture map production is an important part of 3D modeling and determines the rendering quality. Recently, diffusion-based methods have opened a new way for texture generation. However, restricted control flexibility and limited prompt modalities may prevent creators from producing desired results. Furthermore, inconsistencies between generated multi-view images often lead to poor texture generation quality. To address these issues, we introduce FlexPainter, a novel texture generation pipeline that enables flexible multi-modal conditional guidance and achieves highly consistent texture generation. A shared conditional embedding space is constructed to perform flexible aggregation between different input modalities. Utilizing such embedding space, we present an image-based CFG method to decompose structural and style information, achieving reference image-based stylization. Leveraging the 3D knowledge within the image diffusion prior, we first generate multi-view images simultaneously using a grid representation to enhance global understanding. Meanwhile, we propose a view synchronization and adaptive weighting module during diffusion sampling to further ensure local consistency. Finally, a 3D-aware texture completion model combined with a texture enhancement model is used to generate seamless, high-resolution texture maps. Comprehensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods in both flexibility and generation quality.', 'score': 11, 'issue_id': 4157, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '296d6abe1e32cfa9', 'authors': ['Dongyu Yan', 'Leyi Wu', 'Jiantao Lin', 'Luozhou Wang', 'Tianshuo Xu', 'Zhifei Chen', 'Zhen Yang', 'Lie Xu', 'Shunsi Zhang', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Quwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.02620.jpg', 'data': {'categories': ['#multimodal', '#3d', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'FlexPainter: гибкая и согласованная генерация текстур с мультимодальным управлением', 'desc': 'FlexPainter - это новый конвейер генерации текстур, использующий общее условное пространство вложений для гибкого мультимодального управления. Он обеспечивает высококачественную и согласованную генерацию карт текстур с помощью диффузионных моделей изображений и 3D-ориентированной модели. FlexPainter решает проблемы ограниченной гибкости управления и несогласованности между сгенерированными многоракурсными изображениями. Система включает метод декомпозиции структурной и стилевой информации, а также модуль синхронизации ракурсов для обеспечения локальной согласованности.'}, 'en': {'title': 'FlexPainter: Revolutionizing Texture Generation with Multi-Modal Guidance', 'desc': 'FlexPainter is a new pipeline designed for generating high-quality texture maps in 3D modeling. It utilizes a shared conditional embedding space to allow for flexible multi-modal guidance, which helps in producing consistent textures from various input types. By employing an image diffusion prior and a 3D-aware model, it generates multi-view images that maintain local consistency and enhance overall quality. The framework has been shown to outperform existing methods in terms of both flexibility and the quality of the generated textures.'}, 'zh': {'title': 'FlexPainter：灵活高效的纹理生成新方法', 'desc': 'FlexPainter是一种新型的纹理生成管道，利用共享的条件嵌入空间实现灵活的多模态引导，从而确保高质量和一致性的纹理图生成。该方法结合了图像扩散先验和3D感知模型，解决了传统方法在控制灵活性和提示模态方面的限制。通过构建共享的条件嵌入空间，FlexPainter能够在不同输入模态之间进行灵活聚合，提升生成效果。实验结果表明，FlexPainter在灵活性和生成质量上显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.04209', 'title': 'Language-Image Alignment with Fixed Text Encoders', 'url': 'https://huggingface.co/papers/2506.04209', 'abstract': 'Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.', 'score': 10, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '921137445b3be92e', 'authors': ['Jingfeng Yang', 'Ziyang Wu', 'Yue Zhao', 'Yi Ma'], 'affiliations': ['The University of Hong Kong', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.04209.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#long_context', '#alignment', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Фиксированный языковой энкодер улучшает визуальное обучение', 'desc': 'Исследование LIFT предлагает новый подход к обучению визуальных представлений с использованием предобученных языковых моделей (LLM) в качестве фиксированного текстового энкодера. Этот метод превосходит совместное обучение текстовых и визуальных энкодеров, как в CLIP, особенно в задачах композиционного понимания и работы с длинными подписями. LIFT демонстрирует высокую эффективность и вычислительную эффективность, обучая только визуальный энкодер. Результаты исследования открывают новые перспективы использования текстовых эмбеддингов из LLM для улучшения визуального обучения.'}, 'en': {'title': 'LIFT: Efficient Language-Image Alignment with Fixed Text Encoders', 'desc': 'This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment.'}, 'zh': {'title': '简化训练，提升视觉理解的LIFT方法', 'desc': '本文提出了一种新的方法，称为LIFT（使用固定文本编码器的语言-图像对齐），旨在通过预训练的大型语言模型来指导视觉表示学习。与传统的联合训练方法（如CLIP）相比，LIFT只训练图像编码器，而使用固定的文本编码器，从而简化了训练过程。研究表明，LIFT在处理组合理解和长文本描述时，表现优于CLIP，并且在计算效率上也有显著提升。该研究为如何利用大型语言模型的文本嵌入来指导视觉学习提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2506.01011', 'title': 'Autoregressive Images Watermarking through Lexical Biasing: An Approach\n  Resistant to Regeneration Attack', 'url': 'https://huggingface.co/papers/2506.01011', 'abstract': 'A novel watermarking technique, Lexical Bias Watermarking, enhances the security of autoregressive image generation models by embedding watermarks into token selection, demonstrating superior resistance to regeneration attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.', 'score': 8, 'issue_id': 4158, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '5475ba18f16db6f7', 'authors': ['Siqi Hui', 'Yiren Song', 'Sanping Zhou', 'Ye Deng', 'Wenli Huang', 'Jinjun Wang'], 'affiliations': ['National University of Singapore', 'Ningbo University of Technology', 'Southwestern University of Finance and Economics', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01011.jpg', 'data': {'categories': ['#cv', '#security', '#video'], 'emoji': '🔐', 'ru': {'title': 'Защита авторегрессионных моделей генерации изображений с помощью лексических водяных знаков', 'desc': "Статья представляет новую технику водяных знаков для авторегрессионных моделей генерации изображений - Lexical Bias Watermarking (LBW). LBW встраивает водяные знаки непосредственно в карты токенов, смещая выбор токенов в сторону предопределенного 'зеленого списка' во время генерации. Этот подход обеспечивает бесшовную интеграцию с существующими авторегрессионными моделями и естественно расширяется до постфактумной вставки водяных знаков. Эксперименты показывают, что LBW достигает превосходной устойчивости водяных знаков, особенно в противостоянии атакам регенерации."}, 'en': {'title': 'Secure Your Images with Lexical Bias Watermarking!', 'desc': 'The paper introduces Lexical Bias Watermarking (LBW), a new technique aimed at enhancing the security of autoregressive (AR) image generation models. Unlike traditional methods that embed watermarks in diffusion models, LBW integrates watermarks directly into the token selection process during image generation. This method not only improves the robustness of watermarks against regeneration attacks but also allows for easy adaptation to existing AR frameworks. The approach utilizes a randomized selection of green lists for watermarking, ensuring higher security and effective detection through statistical analysis of token distributions.'}, 'zh': {'title': '增强自回归模型安全性的水印技术', 'desc': '本文提出了一种新颖的水印技术，称为词汇偏置水印（Lexical Bias Watermarking），旨在增强自回归图像生成模型的安全性。该方法通过在生成过程中偏向选择预定义的绿色列表，将水印嵌入到令牌选择中，从而有效抵御再生攻击。与现有的扩散模型水印技术不同，LBW能够直接与自回归模型集成，并支持后期水印处理。实验结果表明，LBW在抵抗再生攻击方面表现出色，显著提高了水印的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.05348', 'title': 'FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene\n  Reconstruction', 'url': 'https://huggingface.co/papers/2506.05348', 'abstract': 'A novel 4D representation, FreeTimeGS, enhances the modeling of dynamic 3D scenes by enabling Gaussian primitives to appear at arbitrary times and locations, improving rendering quality compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin.', 'score': 5, 'issue_id': 4162, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '41deb088324d4fce', 'authors': ['Yifan Wang', 'Peishan Yang', 'Zhen Xu', 'Jiaming Sun', 'Zhanhua Zhang', 'Yong Chen', 'Hujun Bao', 'Sida Peng', 'Xiaowei Zhou'], 'affiliations': ['Geely Automobile Research Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05348.jpg', 'data': {'categories': ['#3d'], 'emoji': '🔄', 'ru': {'title': 'Свободное время и пространство для гауссовых примитивов в динамических 3D-сценах', 'desc': 'FreeTimeGS - это новый метод 4D-представления для моделирования динамических 3D-сцен. Он позволяет гауссовым примитивам появляться в произвольное время и в произвольных местах, что улучшает качество рендеринга по сравнению с существующими методами. FreeTimeGS преодолевает ограничения предыдущих подходов, использующих деформационные поля для отображения канонических примитивов. Метод также включает функцию движения для каждого примитива, что уменьшает временную избыточность.'}, 'en': {'title': 'Revolutionizing Dynamic 3D Scene Modeling with FreeTimeGS', 'desc': 'This paper introduces FreeTimeGS, a new 4D representation that enhances the modeling of dynamic 3D scenes. By allowing Gaussian primitives to appear at any time and location, it provides greater flexibility compared to traditional methods that rely on fixed canonical spaces. The approach includes motion functions for each Gaussian primitive, enabling them to transition smoothly over time and reducing redundancy in the scene representation. Experimental results demonstrate that FreeTimeGS significantly improves rendering quality, outperforming existing techniques in handling complex motions.'}, 'zh': {'title': '动态3D场景建模的新突破：FreeTimeGS', 'desc': '本文提出了一种新颖的4D表示方法FreeTimeGS，旨在增强动态3D场景的建模能力。与传统的3D高斯原语不同，FreeTimeGS允许高斯原语在任意时间和位置出现，从而提高了渲染质量。该方法通过为每个高斯原语赋予运动函数，使其能够随时间移动到相邻区域，减少了时间冗余。实验结果表明，FreeTimeGS在多个数据集上的渲染质量显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05333', 'title': 'Kinetics: Rethinking Test-Time Scaling Laws', 'url': 'https://huggingface.co/papers/2506.05333', 'abstract': 'Inference with small models is less efficient due to memory bottlenecks, leading to a new Kinetics Scaling Law emphasizing sparse attention for better test-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.', 'score': 5, 'issue_id': 4172, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '8462c5d73273bc69', 'authors': ['Ranajoy Sadhukhan', 'Zhuoming Chen', 'Haizhong Zheng', 'Yang Zhou', 'Emma Strubell', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05333.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#small_models'], 'emoji': '🔬', 'ru': {'title': 'Разреженное внимание - ключ к эффективному масштабированию моделей', 'desc': 'Статья представляет новый закон масштабирования Kinetics, который учитывает как вычислительные затраты, так и затраты на доступ к памяти при инференсе моделей машинного обучения. Исследование показывает, что эффективность маленьких моделей часто переоценивается из-за узких мест в памяти. Авторы предлагают новую парадигму масштабирования, основанную на разреженном внимании (sparse attention), что позволяет снизить затраты на токен и генерировать более длинные последовательности. Эмпирические результаты демонстрируют преимущество моделей с разреженным вниманием над плотными аналогами в различных режимах затрат.'}, 'en': {'title': 'Unlocking Efficiency: Sparse Attention for Better Model Performance', 'desc': "This paper introduces the Kinetics Scaling Law, which highlights the importance of sparse attention in improving the efficiency of smaller machine learning models during inference. It argues that previous assessments of smaller models' effectiveness have underestimated the impact of memory access bottlenecks that arise during test-time strategies. The authors demonstrate that larger models, particularly those utilizing sparse attention, can significantly enhance performance while managing resource allocation more effectively. Their empirical results show that sparse attention models outperform dense models, leading to substantial gains in accuracy across various cost regimes."}, 'zh': {'title': '稀疏注意力：提升小模型推理效率的关键', 'desc': '本文探讨了小模型在推理时的效率问题，指出由于内存瓶颈，导致其性能被高估。我们提出了一种新的Kinetics Scaling Law，强调稀疏注意力机制在测试时的优势。通过对不同参数规模模型的分析，发现测试时的计算效率在超过一定阈值的模型上更为显著。实验结果表明，稀疏注意力模型在低成本和高成本环境下均优于密集模型，提升了问题解决的准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.00830', 'title': 'SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.00830', 'abstract': 'SkyReels-Audio is a unified framework using pretrained video diffusion transformers for generating high-fidelity and coherent audio-conditioned talking portrait videos, supported by a hybrid curriculum learning strategy and advanced loss mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions.', 'score': 5, 'issue_id': 4157, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7a04cf6593bac4b3', 'authors': ['Zhengcong Fei', 'Hao Jiang', 'Di Qiu', 'Baoxuan Gu', 'Youqiang Zhang', 'Jiahua Wang', 'Jialin Bai', 'Debang Li', 'Mingyuan Fan', 'Guibin Chen', 'Yahui Zhou'], 'affiliations': ['Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00830.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#audio', '#diffusion', '#synthetic', '#video'], 'emoji': '🎭', 'ru': {'title': 'Революция в синтезе говорящих портретов: от аудио к реалистичному видео', 'desc': 'SkyReels-Audio - это унифицированная система для создания высококачественных видео с говорящими портретами на основе аудио. Она использует предобученные видео-диффузионные трансформеры и поддерживает генерацию видео бесконечной длины с разнообразными условиями. В системе применяется гибридная стратегия куррикулярного обучения для постепенного согласования аудио и движений лица. Для улучшения локальной согласованности лица вводятся специальные функции потерь и механизм аудио-управляемого безклассового наведения.'}, 'en': {'title': 'Revolutionizing Talking Portraits with SkyReels-Audio', 'desc': 'SkyReels-Audio is a novel framework that generates high-quality talking portrait videos by using pretrained video diffusion transformers. It allows for the creation and editing of videos based on audio, text, and images, making it versatile for various multimodal inputs. The framework employs a hybrid curriculum learning strategy to ensure that the audio aligns well with facial movements, enhancing the control over video sequences. Additionally, it introduces advanced loss mechanisms to improve facial coherence and uses a sliding-window denoising technique to maintain visual quality and consistency over time.'}, 'zh': {'title': 'SkyReels-Audio：音频驱动的高保真说话肖像生成', 'desc': 'SkyReels-Audio 是一个统一框架，利用预训练的视频扩散变换器生成高保真且连贯的音频条件下的说话肖像视频。该框架支持无限长度的生成和编辑，并通过多模态输入实现多样化和可控的条件设置。我们采用混合课程学习策略，逐步对齐音频与面部运动，从而实现对长视频序列的精细控制。通过引入面部掩膜损失和音频引导的无分类器指导机制，SkyReels-Audio 在复杂条件下展现出卓越的唇同步精度和身份一致性。'}}}, {'id': 'https://huggingface.co/papers/2505.20914', 'title': 'Geometry-Editable and Appearance-Preserving Object Compositon', 'url': 'https://huggingface.co/papers/2505.20914', 'abstract': 'General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework.', 'score': 5, 'issue_id': 4156, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'f95a4c2b427959d4', 'authors': ['Jianman Lin', 'Haojie Li', 'Chunmei Qing', 'Zhijing Yang', 'Liang Lin', 'Tianshui Chen'], 'affiliations': ['Guangdong University of Technology', 'South China University of Technology', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20914.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Точное редактирование геометрии и сохранение деталей в композиции объектов', 'desc': 'Статья представляет новую модель DGAD (Disentangled Geometry-editable and Appearance-preserving Diffusion) для композиции объектов в сцене. DGAD использует семантические эмбеддинги для управления геометрией объекта и механизм кросс-внимания для сохранения деталей внешнего вида. Модель интегрирует семантические эмбеддинги в предобученные диффузионные модели для гибкого манипулирования геометрией объекта. DGAD применяет плотный механизм кросс-внимания для извлечения и пространственного выравнивания признаков внешнего вида с соответствующими регионами.'}, 'en': {'title': 'Seamless Object Integration with Geometry and Appearance Preservation', 'desc': "The paper presents a new model called Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) for integrating objects into backgrounds while maintaining their detailed appearance. DGAD uses semantic embeddings to understand the desired geometric changes and a cross-attention mechanism to align fine-grained appearance features with these changes. This approach allows for precise editing of object geometry without losing the intricate details of the object's appearance. The model builds on existing diffusion techniques and shows improved performance in object composition tasks through extensive experiments."}, 'zh': {'title': '解耦几何与外观保留的物体组合新方法', 'desc': '一般物体组合（GOC）旨在将目标物体无缝地融入背景场景中，同时保持其细致的外观细节。现有方法通过语义嵌入与先进的扩散模型结合，实现几何可编辑的生成。然而，这些紧凑的嵌入仅编码高层语义信息，难以保留细致的外观细节。我们提出了一种解耦几何可编辑和外观保留的扩散模型（DGAD），通过语义嵌入捕捉几何变换，并利用交叉注意力机制对齐外观特征，从而实现精确的几何编辑和真实的外观保留。'}}}, {'id': 'https://huggingface.co/papers/2506.04598', 'title': 'Scaling Laws for Robust Comparison of Open Foundation Language-Vision\n  Models and Datasets', 'url': 'https://huggingface.co/papers/2506.04598', 'abstract': "Scaling laws are derived for CLIP and MaMMUT to compare their performance and sample efficiency across different scales and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t In studies of transferable learning, scaling laws are obtained for various important foundation models to predict their properties and performance at larger scales. We show here how scaling law derivation can also be used for model and dataset comparison, allowing to decide which procedure is to be preferred for pre-training. For the first time, full scaling laws based on dense measurements across a wide span of model and samples seen scales are derived for two important language-vision learning procedures, CLIP and MaMMUT, that use either contrastive only or contrastive and captioning text generative loss. Ensuring sufficient prediction accuracy for held out points, we use derived scaling laws to compare both models, obtaining evidence for MaMMUT's stronger improvement with scale and better sample efficiency than standard CLIP. To strengthen validity of the comparison, we show scaling laws for various downstream tasks, classification, retrieval, and segmentation, and for different open datasets, DataComp, DFN and Re-LAION, observing consistently the same trends. We show that comparison can also be performed when deriving scaling laws with a constant learning rate schedule, reducing compute cost. Accurate derivation of scaling laws provides thus means to perform model and dataset comparison across scale spans, avoiding misleading conclusions based on measurements from single reference scales only, paving the road for systematic comparison and improvement of open foundation models and datasets for their creation. We release all the pre-trained models with their intermediate checkpoints, including openMaMMUT-L/14, which achieves 80.3% zero-shot ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for reproducing experiments in the paper and raw experiments data can be found at https://github.com/LAION-AI/scaling-laws-for-comparison.", 'score': 4, 'issue_id': 4165, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '3add105c0b3c0782', 'authors': ['Marianna Nezhurina', 'Tomer Porian', 'Giovanni Pucceti', 'Tommie Kerssies', 'Romain Beaumont', 'Mehdi Cherti', 'Jenia Jitsev'], 'affiliations': ['Eindhoven University of Technology', 'Institute of Information Science and Technologies A. Faedo - CNR Pisa', 'Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)', 'LAION', 'Open-Ψ (Open-Sci) Collective'], 'pdf_title_img': 'assets/pdf/title_img/2506.04598.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#transfer_learning', '#training', '#dataset', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Масштабируемые законы как ключ к сравнению мультимодальных моделей', 'desc': 'В статье представлены масштабируемые законы для моделей CLIP и MaMMUT, позволяющие сравнить их производительность и эффективность использования данных при различных масштабах и наборах данных. Исследователи показывают, как вывод законов масштабирования может использоваться для сравнения моделей и датасетов, помогая выбрать оптимальную процедуру предобучения. Результаты демонстрируют, что MaMMUT показывает более сильное улучшение с увеличением масштаба и лучшую эффективность использования данных по сравнению со стандартным CLIP. Авторы также предоставляют законы масштабирования для различных задач и наборов данных, подтверждая наблюдаемые тенденции.'}, 'en': {'title': 'Unlocking Model Potential: Scaling Laws for Better Comparisons', 'desc': "This paper explores scaling laws for two language-vision models, CLIP and MaMMUT, to evaluate their performance and efficiency as they are trained on larger datasets. By deriving these scaling laws, the authors provide a framework for comparing different pre-training methods, highlighting MaMMUT's superior performance and sample efficiency over CLIP. The study includes various downstream tasks and datasets, ensuring that the observed trends are consistent across different scenarios. The findings aim to guide future improvements in foundation models and datasets by offering a systematic approach to model comparison."}, 'zh': {'title': '模型与数据集比较的新方法', 'desc': '本文研究了CLIP和MaMMUT模型的缩放规律，以比较它们在不同规模和数据集上的性能和样本效率。通过对这两种重要的语言-视觉学习方法进行全面的缩放规律推导，揭示了MaMMUT在规模扩大时的性能提升和样本效率优于标准CLIP。我们还展示了在不同下游任务和开放数据集上，缩放规律的一致性趋势，确保了比较的有效性。最终，我们发布了所有预训练模型及其中间检查点，以支持后续研究和实验。'}}}, {'id': 'https://huggingface.co/papers/2506.04405', 'title': 'MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale', 'url': 'https://huggingface.co/papers/2506.04405', 'abstract': 'We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '2cf822e179634776', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Yishan Zhong', 'Yue Yu', 'Xiangru Tang', 'Hang Wu', 'May D. Wang', 'Peifeng Ruan', 'Donghan Yang', 'Tao Wang', 'Guanghua Xiao', 'Carl Yang', 'Yang Xie', 'Wenqi Shi'], 'affiliations': ['Emory University', 'Georgia Tech', 'UT Southwestern Medical Center', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04405.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#open_source', '#training', '#rl'], 'emoji': '🩺', 'ru': {'title': 'MedAgentGYM: Революция в обучении ИИ для медицинского кодирования', 'desc': 'MedAgentGYM - это новая среда обучения для улучшения навыков медицинского рассуждения у агентов на основе больших языковых моделей (LLM). Она включает более 72 тысяч задач из 129 категорий, основанных на реальных биомедицинских сценариях. Задачи представлены в виде исполняемых кодовых сред с подробными описаниями, интерактивной обратной связью и верифицируемыми аннотациями. Используя MedAgentGYM, модель Med-Copilot-7B достигла значительного улучшения производительности через тонкую настройку и обучение с подкреплением.'}, 'en': {'title': 'Empowering Medical Reasoning in LLMs with MedAgentGYM', 'desc': 'MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o.'}, 'zh': {'title': 'MedAgentGYM：提升医学推理能力的创新平台', 'desc': '我们介绍了MedAgentGYM，这是第一个公开可用的训练环境，旨在增强大型语言模型（LLM）代理的基于编码的医学推理能力。MedAgentGYM包含72,413个任务实例，涵盖129个类别，来源于真实的生物医学场景。每个任务都在可执行的编码环境中封装，提供详细的任务描述、互动反馈机制、可验证的真实注释和可扩展的训练轨迹生成。通过对30多种LLM的广泛基准测试，发现商业API模型与开源模型之间存在显著的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2506.04245', 'title': 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04245', 'abstract': 'As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8716d3ca53b1d58f', 'authors': ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim'], 'affiliations': ['Microsoft', 'National University of Singapore', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04245.jpg', 'data': {'categories': ['#synthetic', '#agents', '#benchmark', '#reasoning', '#dataset', '#transfer_learning', '#rl', '#leakage'], 'emoji': '🔐', 'ru': {'title': 'Разумное раскрытие информации: обучение ИИ-агентов контекстной целостности', 'desc': 'Статья посвящена проблеме контекстной целостности (CI) в эпоху автономных агентов, принимающих решения за пользователей. Авторы предлагают метод, использующий языковые модели (LLM) и обучение с подкреплением (RL) для обучения агентов рассуждать о контексте и принимать решения о раскрытии информации. Эксперименты на синтетическом наборе данных показали значительное снижение неуместного раскрытия информации при сохранении производительности задач. Улучшения переносятся на реальные бенчмарки CI, такие как PrivacyLens.'}, 'en': {'title': 'Enhancing Contextual Integrity in Autonomous Agents', 'desc': 'This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks.'}, 'zh': {'title': '确保上下文完整性，提升自主代理决策能力', 'desc': '在自主代理为用户做决策的时代，确保上下文完整性（CI）成为一个重要问题。本文提出，CI需要代理在执行任务时对其操作的上下文进行推理。我们首先让大型语言模型（LLMs）明确推理CI，以决定披露哪些信息。接着，我们开发了一个强化学习（RL）框架，进一步增强模型进行CI所需的推理能力，实验结果表明，该方法显著减少了不当信息披露，同时保持了任务性能。'}}}, {'id': 'https://huggingface.co/papers/2506.05282', 'title': 'Rectified Point Flow: Generic Point Cloud Pose Estimation', 'url': 'https://huggingface.co/papers/2506.05282', 'abstract': 'We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.', 'score': 3, 'issue_id': 4156, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c7d3c7ca688358d9', 'authors': ['Tao Sun', 'Liyuan Zhu', 'Shengyu Huang', 'Shuran Song', 'Iro Armeni'], 'affiliations': ['NVIDIA Research', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05282.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark'], 'emoji': '🧩', 'ru': {'title': 'Единый подход к регистрации облаков точек и сборке форм', 'desc': 'Представлен метод Rectified Point Flow, который объединяет регистрацию облаков точек и сборку многокомпонентных форм в единую условную генеративную задачу. Метод обучает непрерывное поточечное поле скоростей, которое перемещает зашумленные точки к целевым позициям. В отличие от предыдущих подходов, данный метод изначально учитывает симметрии при сборке без явной разметки. Вместе с самоконтролируемым энкодером, фокусирующимся на перекрывающихся точках, метод достигает нового уровня производительности на шести эталонных наборах данных.'}, 'en': {'title': 'Unified Learning for Point Cloud Registration and Shape Assembly', 'desc': 'This paper presents Rectified Point Flow, a novel approach that combines point cloud registration and multi-part shape assembly into a single generative framework. The method learns a continuous velocity field that aligns noisy point clouds to their target configurations, allowing for the recovery of part poses. Unlike previous methods that require manual symmetry handling, this approach automatically learns assembly symmetries without needing explicit labels. By utilizing a self-supervised encoder for overlapping points, the method achieves state-of-the-art results across multiple benchmarks, enhancing accuracy through joint training on varied datasets.'}, 'zh': {'title': '统一点云配准与形状组装的创新方法', 'desc': '我们提出了修正点流（Rectified Point Flow），这是一种统一的参数化方法，将成对点云配准和多部件形状组装视为一个单一的条件生成问题。该方法在没有姿态信息的情况下，学习一个连续的点位速度场，将噪声点移动到目标位置，并从中恢复部件姿态。与之前的工作不同，我们的方法能够在没有对称标签的情况下，自然地学习组装对称性。通过专注于重叠点的自监督编码器，我们的方法在六个基准测试中实现了新的最先进性能，促进了共享几何先验的学习，从而提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.05278', 'title': 'Micro-Act: Mitigate Knowledge Conflict in Question Answering via\n  Actionable Self-Reasoning', 'url': 'https://huggingface.co/papers/2506.05278', 'abstract': 'A framework called Micro-Act addresses Knowledge Conflicts in Retrieval-Augmented Generation by adaptively decomposing knowledge sources, leading to improved QA accuracy compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications.', 'score': 3, 'issue_id': 4158, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '119fed47e7d43a96', 'authors': ['Nan Huo', 'Jinyang Li', 'Bowen Qin', 'Ge Qu', 'Xiaolong Li', 'Xiaodong Li', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05278.jpg', 'data': {'categories': ['#interpretability', '#rag', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Micro-Act: умное разрешение конфликтов знаний в RAG-системах', 'desc': 'Фреймворк Micro-Act решает проблему конфликтов знаний в системах генерации с дополнением из источников (RAG). Он адаптивно разбивает источники знаний на мелкие сравнения, представленные как последовательность действий. Это позволяет рассуждать за пределами поверхностного контекста. Эксперименты показали значительное повышение точности ответов на вопросы по сравнению с существующими методами.'}, 'en': {'title': 'Micro-Act: Resolving Knowledge Conflicts for Better QA Accuracy', 'desc': 'The paper introduces Micro-Act, a novel framework designed to tackle Knowledge Conflicts in Retrieval-Augmented Generation (RAG) systems. Knowledge Conflicts occur when external information contradicts the knowledge embedded in large language models, negatively impacting question answering (QA) tasks. Micro-Act improves QA accuracy by adaptively breaking down knowledge sources into manageable comparisons, allowing for deeper reasoning and better conflict resolution. Experimental results demonstrate that Micro-Act outperforms existing methods across various datasets and conflict types, while also maintaining strong performance on non-conflict questions.'}, 'zh': {'title': 'Micro-Act：解决知识冲突的智能框架', 'desc': 'Micro-Act是一个新框架，旨在解决检索增强生成（RAG）中的知识冲突问题。它通过自适应地分解知识源，改善了问答（QA）的准确性。与现有方法不同，Micro-Act采用分层的行动空间，能够自动感知上下文的复杂性，并将知识源细分为一系列精细的比较步骤。这种方法在五个基准数据集上的实验中显示出显著的QA准确性提升，尤其在时间和语义类型的冲突中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.04956', 'title': 'FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.04956', 'abstract': 'Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT.', 'score': 3, 'issue_id': 4166, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e1a303f65a6d378c', 'authors': ['Huihan Wang', 'Zhiwen Yang', 'Hui Zhang', 'Dan Zhao', 'Bingzheng Wei', 'Yan Xu'], 'affiliations': ['ByteDance Inc., Beijing 100098, China', 'Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China', 'Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China', 'School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04956.jpg', 'data': {'categories': ['#video', '#architecture', '#open_source', '#optimization', '#training'], 'emoji': '🏥', 'ru': {'title': 'FEAT: Эффективный трансформер для синтеза медицинских видео', 'desc': 'В статье представлен FEAT - новый трансформер для синтеза динамических медицинских видео. FEAT использует последовательные механизмы внимания для пространственных, временных и канальных измерений, что позволяет захватывать глобальные зависимости. Архитектура имеет линейную сложность вычислений и включает модуль остаточного управления значениями для точной адаптации к разным уровням шума. FEAT превосходит существующие методы по эффективности и масштабируемости на стандартных наборах данных.'}, 'en': {'title': 'FEAT: Revolutionizing Medical Video Synthesis with Efficient Attention', 'desc': "This paper introduces FEAT, a novel Transformer model designed to create high-quality dynamic medical videos by effectively managing spatial and temporal information. It overcomes limitations of existing models by implementing a unified attention mechanism that captures dependencies across spatial, temporal, and channel dimensions. FEAT also features a linear-complexity attention design, which reduces computational demands while maintaining performance. Additionally, a residual value guidance module enhances the model's ability to adapt to varying noise levels, leading to superior results on benchmark tasks with fewer parameters than previous state-of-the-art models."}, 'zh': {'title': '高效动态医疗视频合成的新突破', 'desc': '本论文提出了一种新的全维高效注意力变换器（FEAT），旨在解决动态医疗视频合成中的空间一致性和时间动态建模问题。FEAT通过三项创新来克服现有方法的局限性，包括统一的时空通道注意力机制、线性复杂度的注意力设计以及残差值引导模块，以适应不同的噪声水平。实验结果表明，FEAT-S在参数量仅为最先进模型Endora的23%的情况下，仍能实现相当或更优的性能。FEAT-L在多个数据集上超越了所有对比方法，展示了其卓越的有效性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.03643', 'title': 'Images are Worth Variable Length of Representations', 'url': 'https://huggingface.co/papers/2506.03643', 'abstract': 'Most existing vision encoders map images into a fixed-length sequence of tokens, overlooking the fact that different images contain varying amounts of information. For example, a visually complex image (e.g., a cluttered room) inherently carries more information and thus deserves more tokens than a simple image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a dynamic vision encoder that produces a variable number of visual tokens (i.e., continuous representation vectors) to reconstruct each image. Our results show that DOVE significantly reduces the average number of tokens while maintaining high reconstruction quality. In several linear probing and downstream multimodal tasks, it outperforms existing autoencoder-based tokenization methods when using far fewer tokens, capturing more expressive semantic features compared to fixed-length encoding. We further extend DOVE with query-conditioned tokenization. By guiding the model to focus on query-relevant regions, it achieves more efficient and targeted semantic extraction. Our code and checkpoints are available at https://dove-encoder.github.io/dove-encoder.', 'score': 3, 'issue_id': 4165, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '0581c037604119ee', 'authors': ['Lingjun Mao', 'Rodolfo Corona', 'Xin Liang', 'Wenhao Yan', 'Zineng Tang'], 'affiliations': ['University of California, Berkeley', 'University of California, San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.03643.jpg', 'data': {'categories': ['#cv', '#architecture', '#optimization', '#multimodal'], 'emoji': '🦅', 'ru': {'title': 'Адаптивное кодирование изображений: больше информации, меньше токенов', 'desc': 'DOVE - это динамический кодировщик изображений, который создает переменное количество визуальных токенов для реконструкции каждого изображения. В отличие от существующих энкодеров с фиксированной длиной последовательности, DOVE адаптируется к сложности изображения, используя больше токенов для визуально сложных сцен и меньше для простых. Результаты показывают, что DOVE значительно сокращает среднее количество токенов, сохраняя высокое качество реконструкции. Модель превосходит существующие методы токенизации на основе автоэнкодеров в задачах линейного пробинга и мультимодальных задачах, используя гораздо меньше токенов.'}, 'en': {'title': 'Dynamic Tokenization for Enhanced Image Understanding', 'desc': 'This paper introduces DOVE, a dynamic vision encoder that adapts the number of visual tokens based on the complexity of the image being processed. Unlike traditional methods that use a fixed number of tokens, DOVE generates a variable number of tokens, allowing it to capture more information from visually complex images. The results demonstrate that DOVE not only reduces the average number of tokens needed but also maintains high-quality image reconstruction. Additionally, DOVE incorporates query-conditioned tokenization to enhance semantic extraction by focusing on relevant image regions, outperforming existing autoencoder-based methods in various tasks.'}, 'zh': {'title': '动态视觉编码，提升信息提取效率', 'desc': '现有的视觉编码器通常将图像映射为固定长度的标记序列，但不同图像的信息量不同。我们提出了DOVE，一个动态视觉编码器，可以生成可变数量的视觉标记，以重建每个图像。DOVE在保持高重建质量的同时，显著减少了平均标记数量，并在多个任务中超越了现有的基于自编码器的标记化方法。通过查询条件标记化，DOVE能够更有效地提取与查询相关的语义特征。'}}}, {'id': 'https://huggingface.co/papers/2506.02751', 'title': 'RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS', 'url': 'https://huggingface.co/papers/2506.02751', 'abstract': 'RobustSplat addresses artifacts in 3D Gaussian Splatting caused by transient objects through delayed Gaussian growth and scale-cascaded mask bootstrapping.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.', 'score': 3, 'issue_id': 4161, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9e5cefa9df6461fd', 'authors': ['Chuanyu Fu', 'Yuqi Zhang', 'Kunbin Yao', 'Guanying Chen', 'Yuan Xiong', 'Chuan Huang', 'Shuguang Cui', 'Xiaochun Cao'], 'affiliations': ['FNii-Shenzhen', 'SSE, CUHKSZ', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02751.jpg', 'data': {'categories': ['#3d'], 'emoji': '🌟', 'ru': {'title': 'Устранение артефактов в 3D-сценах с помощью умного роста гауссианов', 'desc': 'RobustSplat - это новый метод в области 3D-моделирования, направленный на устранение артефактов, вызванных временными объектами в сценах. Он использует стратегию отложенного роста гауссианов, что позволяет сначала оптимизировать статическую структуру сцены. Кроме того, RobustSplat применяет каскадный подход к созданию маски временных объектов, начиная с низкого разрешения и постепенно увеличивая его. Эксперименты показали, что этот метод превосходит существующие решения в задаче робастного 3D-рендеринга.'}, 'en': {'title': 'Enhancing 3D Gaussian Splatting with Robust Techniques', 'desc': 'RobustSplat is a novel approach designed to improve 3D Gaussian Splatting (3DGS) by addressing artifacts caused by transient objects in rendered images. The method introduces a delayed Gaussian growth strategy that focuses on optimizing the static elements of a scene before dealing with transient disturbances, reducing the risk of overfitting. Additionally, it employs a scale-cascaded mask bootstrapping technique that starts with lower-resolution features for initial mask estimation, ensuring better semantic consistency before refining to high-resolution predictions. Through extensive testing, RobustSplat demonstrates superior performance compared to existing methods, showcasing its effectiveness in producing high-quality, artifact-free renderings.'}, 'zh': {'title': '增强3D渲染的鲁棒性', 'desc': 'RobustSplat 是一种针对 3D 高斯点云渲染中因瞬态物体引起的伪影问题的解决方案。该方法通过延迟高斯生长和尺度级联掩码自举来优化静态场景结构，减少对瞬态物体的过拟合。首先，延迟高斯生长策略确保在允许高斯分裂之前，先优化静态场景。其次，尺度级联掩码自举方法利用低分辨率特征相似性进行初步掩码估计，随后再进行高分辨率监督，以提高掩码预测的精确度。'}}}, {'id': 'https://huggingface.co/papers/2506.05313', 'title': 'MARBLE: Material Recomposition and Blending in CLIP-Space', 'url': 'https://huggingface.co/papers/2506.05313', 'abstract': 'MARBLE utilizes material embeddings in CLIP-space to control pre-trained text-to-image models for blending and recomposing material properties in images with parametric control over attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/', 'score': 2, 'issue_id': 4169, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e941d4fe8c8db7a2', 'authors': ['Ta-Ying Cheng', 'Prafull Sharma', 'Mark Boss', 'Varun Jampani'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.05313.jpg', 'data': {'categories': ['#cv', '#games', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Умное редактирование материалов в изображениях с помощью ИИ', 'desc': 'MARBLE - это метод для смешивания материалов и изменения их свойств в изображениях с использованием встраиваний в пространстве CLIP. Он позволяет контролировать предобученные модели text-to-image для редактирования материалов объектов на основе примеров. MARBLE находит блок в U-Net, отвечающий за атрибуцию материалов, и использует неглубокую нейронную сеть для предсказания направления изменения атрибутов. Метод обеспечивает параметрический контроль над такими свойствами материалов как шероховатость, металличность, прозрачность и свечение.'}, 'en': {'title': 'Blend and Recompose Materials with MARBLE!', 'desc': 'MARBLE is a novel method that enhances material editing in images by utilizing material embeddings in CLIP-space. It allows for the blending and recomposing of material properties in images through pre-trained text-to-image models. By identifying specific blocks in the denoising UNet that handle material attributes, MARBLE can manipulate fine-grained properties like roughness and transparency. The method also supports multiple edits in one pass, showcasing its efficiency and versatility in applications such as digital painting.'}, 'zh': {'title': 'MARBLE：智能材料编辑的新方法', 'desc': 'MARBLE是一种利用CLIP空间中的材料嵌入来控制预训练文本到图像模型的方法。它可以实现图像中材料属性的混合和重组，并对细粒度材料属性进行参数化控制。通过在去噪UNet中找到与材料归属相关的块，MARBLE改进了基于示例的材料编辑。该方法能够在一次前向传递中进行多次编辑，并适用于绘画。'}}}, {'id': 'https://huggingface.co/papers/2506.05046', 'title': 'FlowDirector: Training-Free Flow Steering for Precise Text-to-Video\n  Editing', 'url': 'https://huggingface.co/papers/2506.05046', 'abstract': 'FlowDirector, an inversion-free video editing framework, uses ODEs for spatiotemporal coherent editing and attention-guided masking for localized control, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-driven video editing aims to modify video content according to natural language instructions. While recent training-free approaches have made progress by leveraging pre-trained diffusion models, they typically rely on inversion-based techniques that map input videos into the latent space, which often leads to temporal inconsistencies and degraded structural fidelity. To address this, we propose FlowDirector, a novel inversion-free video editing framework. Our framework models the editing process as a direct evolution in data space, guiding the video via an Ordinary Differential Equation (ODE) to smoothly transition along its inherent spatiotemporal manifold, thereby preserving temporal coherence and structural details. To achieve localized and controllable edits, we introduce an attention-guided masking mechanism that modulates the ODE velocity field, preserving non-target regions both spatially and temporally. Furthermore, to address incomplete edits and enhance semantic alignment with editing instructions, we present a guidance-enhanced editing strategy inspired by Classifier-Free Guidance, which leverages differential signals between multiple candidate flows to steer the editing trajectory toward stronger semantic alignment without compromising structural consistency. Extensive experiments across benchmarks demonstrate that FlowDirector achieves state-of-the-art performance in instruction adherence, temporal consistency, and background preservation, establishing a new paradigm for efficient and coherent video editing without inversion.', 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'a9c76743a2f7e8d6', 'authors': ['Guangzhao Li', 'Yanming Yang', 'Chenxi Song', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Central South University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05046.jpg', 'data': {'categories': ['#games', '#video', '#multimodal', '#benchmark', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Плавное и точное редактирование видео с помощью ОДУ и масок внимания', 'desc': 'FlowDirector - это новая структура для редактирования видео без инверсии. Она использует обыкновенные дифференциальные уравнения (ODE) для моделирования процесса редактирования как эволюции в пространстве данных, сохраняя временную согласованность и структурные детали. Механизм маскирования на основе внимания позволяет осуществлять локализованное и контролируемое редактирование. Стратегия редактирования с улучшенным наведением повышает семантическое соответствие инструкциям по редактированию.'}, 'en': {'title': 'FlowDirector: Revolutionizing Video Editing with ODEs and Attention', 'desc': "FlowDirector is a new video editing framework that avoids the common inversion-based techniques, which can cause problems like temporal inconsistencies. Instead, it uses Ordinary Differential Equations (ODEs) to guide video edits directly in the data space, ensuring smooth transitions while maintaining the video's structure and timing. The framework also incorporates an attention-guided masking system that allows for precise control over which parts of the video are edited, preserving the areas that should remain unchanged. Additionally, it employs a guidance-enhanced strategy to improve the alignment of edits with user instructions, achieving top performance in video editing tasks."}, 'zh': {'title': '无反演视频编辑的新范式', 'desc': 'FlowDirector是一种无反演的视频编辑框架，利用常微分方程（ODE）进行时空一致性编辑。该框架通过直接在数据空间中建模编辑过程，确保视频在其固有的时空流形上平滑过渡，从而保持时间一致性和结构细节。为了实现局部可控的编辑，FlowDirector引入了基于注意力的掩膜机制，调节ODE速度场，保护非目标区域的时空特性。此外，采用增强指导的编辑策略，进一步提高了与编辑指令的语义对齐，确保了编辑的完整性和结构一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.04559', 'title': 'Perceptual Decoupling for Scalable Multi-modal Reasoning via\n  Reward-Optimized Captioning', 'url': 'https://huggingface.co/papers/2506.04559', 'abstract': "A reasoning-aligned reinforcement learning strategy enhances visual representations in multi-modal large language models by optimizing captions for downstream reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment.", 'score': 2, 'issue_id': 4170, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'cc35ebabca1ba808', 'authors': ['Yunhao Gou', 'Kai Chen', 'Zhili Liu', 'Lanqing Hong', 'Xin Jin', 'Zhenguo Li', 'James T. Kwok', 'Yu Zhang'], 'affiliations': ['Huawei Cloud', 'Huawei Noahs Ark Lab', 'Southern University of Science and Technology', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04559.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#multimodal', '#rl', '#rag', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Улучшение визуального восприятия ИИ через оптимизацию для задач рассуждения', 'desc': 'Статья представляет метод RACRO для улучшения мультимодальных больших языковых моделей. RACRO использует обучение с подкреплением для оптимизации генерации описаний изображений, ориентированных на последующие задачи рассуждения. Этот подход позволяет разделить восприятие и рассуждение, сохраняя при этом информативность визуальных представлений. Эксперименты показывают, что RACRO достигает наилучших результатов на мультимодальных тестах по математике и естественным наукам.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with RACRO', 'desc': "This paper introduces a new reinforcement learning strategy called Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) to improve how visual information is represented in multi-modal large language models (MLLMs). The approach focuses on generating captions from visual inputs that are both accurate and informative, which helps in enhancing reasoning tasks. By using a reward-based optimization method, RACRO aligns the visual extractor's output with the reasoning goals of the model. The results show that RACRO achieves top performance on various benchmarks while allowing for easier integration with advanced reasoning models without the need for expensive retraining."}, 'zh': {'title': '推理驱动的视觉表示优化策略', 'desc': '本文提出了一种名为RACRO的强化学习策略，旨在通过优化图像描述来增强多模态大语言模型的视觉表示。该方法通过将感知与推理解耦，将视觉输入转换为语言表示，并利用强大的文本推理模型进行处理。RACRO通过奖励优化，使得图像描述既忠实于图像，又能支持准确的推理任务，从而提升视觉对齐能力。实验结果表明，RACRO在多模态数学和科学基准测试中表现优异，且具有良好的可扩展性和适应性。'}}}, {'id': 'https://huggingface.co/papers/2506.04462', 'title': 'Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation', 'url': 'https://huggingface.co/papers/2506.04462', 'abstract': 'Watermarking techniques in LLMs can degrade truthfulness, safety, and helpfulness, and alignment resampling is proposed to restore alignment while ensuring watermark detectability.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.', 'score': 2, 'issue_id': 4168, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '35a6c458e61b75e3', 'authors': ['Apurv Verma', 'NhatHai Phan', 'Shubhendu Trivedi'], 'affiliations': ['MIT', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04462.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#rlhf', '#inference'], 'emoji': '🔒', 'ru': {'title': 'Баланс между водяными знаками и выравниванием в LLM', 'desc': 'Статья исследует влияние методов водяных знаков на большие языковые модели (LLM), выявляя их негативное воздействие на правдивость, безопасность и полезность выходных данных. Авторы обнаружили два паттерна деградации: ослабление защиты и чрезмерное усиление защиты. Для решения этой проблемы предлагается метод Alignment Resampling (AR), использующий внешнюю модель вознаграждения для восстановления выравнивания. Экспериментальные результаты показывают, что AR успешно восстанавливает базовое выравнивание при сохранении сильной обнаруживаемости водяных знаков.'}, 'en': {'title': 'Balancing Watermarking and Model Alignment in LLMs', 'desc': 'This paper investigates how watermarking techniques in large language models (LLMs) can negatively affect their truthfulness, safety, and helpfulness. It identifies two main degradation patterns: guard attenuation, where increased helpfulness compromises safety, and guard amplification, where excessive caution limits helpfulness. To address these issues, the authors propose a method called Alignment Resampling (AR), which uses an external reward model to restore alignment during inference. The results show that AR can effectively recover alignment scores while ensuring that the watermarks remain detectable, highlighting the delicate balance between watermark strength and model performance.'}, 'zh': {'title': '水印与模型对齐的平衡', 'desc': '本文探讨了大语言模型（LLMs）中的水印技术对输出质量的影响，特别是对真实性、安全性和有用性的影响。研究分析了两种流行的水印方法——Gumbel和KGW，如何影响这四个对齐属性。实验结果显示，水印引起的代币分布变化导致了两种不同的降级模式：保护减弱和保护增强。为了解决这些问题，提出了一种名为对齐重采样（AR）的方法，通过外部奖励模型在推理时恢复对齐，同时保持水印的可检测性。'}}}, {'id': 'https://huggingface.co/papers/2506.02587', 'title': "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View\n  Representations", 'url': 'https://huggingface.co/papers/2506.02587', 'abstract': "BEVCALIB model uses bird's-eye view features for accurate LiDAR-camera calibration from raw data, demonstrating superior performance under various noise conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate LiDAR-camera calibration is fundamental to fusing multi-modal perception in autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometric information from the BEV feature, we introduce a novel feature selector to filter the most important features in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations on KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a new state of the art. Under various noise conditions, BEVCALIB outperforms the best baseline in the literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation), respectively. In the open-source domain, it improves the best reproducible baseline by one order of magnitude. Our code and demo results are available at https://cisl.ucr.edu/BEVCalib.", 'score': 2, 'issue_id': 4160, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '8d1e0e49dea5dcad', 'authors': ['Weiduo Yuan', 'Jerry Li', 'Justin Yue', 'Divyank Shah', 'Konstantinos Karydis', 'Hang Qiu'], 'affiliations': ['University of California, Riverside', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.02587.jpg', 'data': {'categories': ['#robotics', '#cv', '#dataset', '#optimization', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'Революция в калибровке LiDAR-камеры с помощью вида сверху', 'desc': 'BEVCALIB - это модель, использующая функции вида сверху для точной калибровки LiDAR-камеры по необработанным данным. Модель извлекает и объединяет функции вида сверху как для камеры, так и для LiDAR в общее пространство признаков. BEVCALIB демонстрирует превосходную производительность в различных условиях шума по сравнению с существующими методами. Модель устанавливает новый стандарт в области калибровки LiDAR-камеры, значительно превосходя базовые показатели на наборах данных KITTI и NuScenes.'}, 'en': {'title': 'Revolutionizing LiDAR-Camera Calibration with BEV Features', 'desc': "The BEVCALIB model introduces a novel approach for calibrating LiDAR and camera systems using bird's-eye view (BEV) features extracted from raw data. This method addresses the limitations of traditional calibration techniques that struggle with dynamic transformations during vehicle or robot movement. By fusing separate BEV features from both LiDAR and camera into a shared feature space, BEVCALIB enhances the accuracy of multi-modal perception in autonomous systems. The model demonstrates significant performance improvements under various noise conditions, setting a new benchmark in the field with extensive evaluations on multiple datasets."}, 'zh': {'title': 'BEVCALIB：鸟瞰视图特征助力激光雷达与相机精确标定', 'desc': 'BEVCALIB模型利用鸟瞰视图特征进行激光雷达与相机的精确标定，能够从原始数据中提取信息。与传统方法相比，BEVCALIB在各种噪声条件下表现出色，特别是在自动驾驶和机器人系统中融合多模态感知时至关重要。该模型通过分别提取相机和激光雷达的鸟瞰视图特征，并将其融合到共享的特征空间中，显著提高了标定的准确性。通过引入新颖的特征选择器，BEVCALIB在减少内存消耗的同时，实现了高效的训练和更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23115', 'title': 'Diffusion-Based Generative Models for 3D Occupancy Prediction in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2505.23115', 'abstract': 'Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.', 'score': 2, 'issue_id': 4162, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '9780859f709be640', 'authors': ['Yunshen Wang', 'Yicheng Liu', 'Tianyuan Yuan', 'Yucheng Mao', 'Yingshi Liang', 'Xiuyu Yang', 'Honggang Zhang', 'Hang Zhao'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Institute for Interdisciplinary Information Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23115.jpg', 'data': {'categories': ['#diffusion', '#agents', '#3d', '#cv'], 'emoji': '🚗', 'ru': {'title': 'Диффузионные модели улучшают 3D-восприятие беспилотных автомобилей', 'desc': 'В этой работе авторы предлагают использовать генеративные диффузионные модели для предсказания трехмерных карт занятости в задаче автономного вождения. В отличие от дискриминативных методов, такой подход позволяет лучше справляться с шумными данными и неполными наблюдениями. Эксперименты показывают, что генеративные модели превосходят современные дискриминативные подходы, особенно в областях с плохой видимостью. Улучшенные предсказания значительно повышают качество планирования маршрута для беспилотных автомобилей.'}, 'en': {'title': 'Revolutionizing 3D Occupancy Prediction with Diffusion Models', 'desc': 'This paper addresses the challenge of predicting 3D occupancy grids from visual inputs for autonomous driving, particularly in the presence of noisy data and incomplete observations. The authors propose a novel approach by reframing the problem as a generative modeling task using diffusion models, which learn the data distribution and incorporate 3D scene priors. This method improves prediction consistency and robustness against noise, effectively managing the complexities of 3D spatial structures. Experimental results demonstrate that their diffusion-based models outperform traditional discriminative methods, leading to more accurate occupancy predictions that enhance downstream planning tasks in real-world driving scenarios.'}, 'zh': {'title': '生成模型提升3D占用预测的准确性', 'desc': '本研究将3D占用网格的预测视为生成建模任务，采用扩散模型来处理视觉输入。与传统的判别方法相比，扩散模型能够更好地应对噪声数据和不完整观测，同时有效捕捉3D场景的复杂结构。实验结果表明，基于扩散的生成模型在占用预测的准确性和一致性上优于现有的最先进判别方法，尤其在遮挡或低可见度区域表现更佳。该方法的改进预测显著提升了后续规划任务的效果，展示了其在自动驾驶实际应用中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.04996', 'title': 'PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill\n  Assessment', 'url': 'https://huggingface.co/papers/2506.04996', 'abstract': 'PATS is a novel temporal sampling method that enhances video analysis of athletic skills by ensuring complete movement patterns are captured, outperforming existing methods across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications.', 'score': 1, 'issue_id': 4161, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'f0a38c5292395f88', 'authors': ['Edoardo Bianchi', 'Antonio Liotta'], 'affiliations': ['Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.04996.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': '🏋️', 'ru': {'title': 'PATS: Умная временная выборка для оценки спортивного мастерства', 'desc': 'PATS (Proficiency-Aware Temporal Sampling) - это новый метод временной выборки для анализа видео спортивных навыков. Он обеспечивает захват полных паттернов движения, превосходя существующие методы в различных областях. PATS адаптивно сегментирует видео, чтобы каждая анализируемая часть содержала полное выполнение критических компонентов производительности. Оцененный на бенчмарке EgoExo4D с использованием SkillFormer, PATS превосходит современные показатели точности во всех конфигурациях просмотра.'}, 'en': {'title': 'Enhancing Athletic Skill Analysis with PATS', 'desc': 'PATS, or Proficiency-Aware Temporal Sampling, is a new method designed to improve the analysis of athletic skills in videos. It captures complete movement patterns by maintaining the temporal continuity necessary for evaluating performance. This method adaptively segments videos to ensure that each analyzed part includes the full execution of key skills, enhancing the accuracy of assessments. PATS has shown to outperform existing techniques in various sports and activities, making it a significant advancement in automated skill evaluation.'}, 'zh': {'title': 'PATS：提升运动技能分析的时间采样新方法', 'desc': 'PATS是一种新颖的时间采样方法，旨在提升运动技能的视频分析。它通过确保完整的运动模式被捕捉，超越了现有的采样方法。PATS能够自适应地分段视频，确保每个分析部分都包含关键表现组件的完整执行。经过评估，PATS在多个领域的准确性上均优于现有技术，显示出其在自动化技能评估中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03238', 'title': 'Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach', 'url': 'https://huggingface.co/papers/2506.03238', 'abstract': 'OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'f234199601bef528', 'authors': ['Ziheng Zhao', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Department of Radiology, Renmin Hospital of Wuhan University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03238.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#healthcare', '#cv'], 'emoji': '🔬', 'ru': {'title': 'ИИ-революция в интерпретации КТ-снимков', 'desc': 'Модель OminiAbnorm-CT предназначена для автоматизированной интерпретации КТ-изображений. Она превосходит существующие методы в локализации и описании аномалий в различных областях тела с использованием текстовых запросов и визуальных подсказок. Модель основана на всеобъемлющей иерархической системе классификации, разработанной совместно с опытными радиологами. OminiAbnorm-CT обучена на большом наборе данных КТ-изображений с тщательно размеченными аномалиями.'}, 'en': {'title': 'Revolutionizing CT Image Analysis with OminiAbnorm-CT', 'desc': 'OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology.'}, 'zh': {'title': 'OminiAbnorm-CT：CT图像异常自动解读的新突破', 'desc': 'OminiAbnorm-CT是一种用于自动解读CT图像的模型，能够在不同身体部位中定位和描述异常情况。该研究通过与资深放射科医生合作，提出了一个包含404种异常发现的层次分类系统。我们还贡献了一个包含超过14.5K CT图像的数据集，并为超过19K异常提供了详细的注释。通过大量实验，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02444', 'title': 'SViMo: Synchronized Diffusion for Video and Motion Generation in\n  Hand-object Interaction Scenarios', 'url': 'https://huggingface.co/papers/2506.02444', 'abstract': "A framework combining visual priors and dynamic constraints within a synchronized diffusion process generates HOI video and motion simultaneously, enhancing video-motion consistency and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at https://github.com/Droliven/SViMo\\_project.", 'score': 1, 'issue_id': 4168, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'a1ed02142b4e22ee', 'authors': ['Lingwei Dang', 'Ruizhi Shao', 'Hongwen Zhang', 'Wei Min', 'Yebin Liu', 'Qingyao Wu'], 'affiliations': ['Department of Automation, Tsinghua University', 'School of Artificial Intelligence, Beijing Normal University', 'School of Software Engineering, South China University of Technology', 'Shadow AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.02444.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#3d', '#games', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'Синхронная генерация видео и движения для реалистичного взаимодействия человека с объектами', 'desc': 'Предложена новая система для одновременной генерации видео и движения взаимодействия человека с объектами (HOI). Метод объединяет визуальные приоры и динамические ограничения в синхронизированном процессе диффузии. Используется тримодальная адаптивная модуляция для выравнивания признаков и 3D полное внимание для моделирования зависимостей. Система включает зависимое от зрения 3D диффузионное моделирование взаимодействий с обратной связью, что улучшает согласованность видео и движений.'}, 'en': {'title': 'Synchronized Diffusion for Realistic HOI Generation', 'desc': 'This paper presents a new framework for generating Hand-Object Interaction (HOI) videos and motions simultaneously, improving consistency and generalization. It combines visual priors and dynamic constraints using a synchronized diffusion process, which allows for the creation of realistic interactions without relying on predefined 3D models. The method employs tri-modal adaptive modulation and 3D full-attention to align features and model dependencies effectively. Experimental results show that this approach outperforms existing methods in producing high-quality, physically plausible HOI sequences, even in new scenarios.'}, 'zh': {'title': '同步扩散生成高保真手-物体交互视频', 'desc': '本文提出了一种新颖的框架，通过结合视觉先验和动态约束，在同步扩散过程中同时生成手-物体交互（HOI）视频和运动。这种方法克服了传统3D HOI运动生成依赖预定义模型和捕获数据的局限性，提升了生成的一致性和泛化能力。我们采用三模态自适应调制和3D全注意力机制来对齐特征，并引入视觉感知的3D交互扩散模型，直接从同步扩散输出生成3D交互序列。实验结果表明，该方法在生成高保真、动态合理的HOI序列方面优于现有技术，尤其在未见过的真实场景中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.00981', 'title': 'What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training', 'url': 'https://huggingface.co/papers/2506.00981', 'abstract': "Self-supervised Wav2Vec2 models encode Dutch linguistic features more accurately when pre-trained exclusively on Dutch data, compared to similar amounts of English or multilingual data, as shown by clustering and classification probes, and demonstrated through improved Automatic Speech Recognition performance.  \t\t\t\t\tAI-generated summary \t\t\t\t How language-specific are speech representations learned by self-supervised models? Existing work has shown that a range of linguistic features can be successfully decoded from end-to-end models trained only on speech recordings. However, it's less clear to what extent pre-training on specific languages improves language-specific linguistic information. Here we test the encoding of Dutch phonetic and lexical information in internal representations of self-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the representation of Dutch linguistic features as compared to pre-training on similar amounts of English or larger amounts of multilingual data. This language-specific advantage is well-detected by trained clustering or classification probes, and partially observable using zero-shot metrics. Furthermore, the language-specific benefit on linguistic feature encoding aligns with downstream performance on Automatic Speech Recognition.", 'score': 1, 'issue_id': 4165, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'e571a309d437ed8b', 'authors': ['Marianne de Heer Kloots', 'Hosein Mohebbi', 'Charlotte Pouw', 'Gaofei Shen', 'Willem Zuidema', 'Martijn Bentum'], 'affiliations': ['Centre for Language Studies, Radboud University, Netherlands', 'Cognitive Science and Artificial Intelligence, Tilburg University, The Netherlands', 'Institute for Logic, Language and Computation, University of Amsterdam, The Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2506.00981.jpg', 'data': {'categories': ['#transfer_learning', '#multilingual', '#audio', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Языково-специфичное предобучение улучшает представление речи', 'desc': 'Исследование показывает, что модели Wav2Vec2, предобученные исключительно на голландских данных, лучше кодируют лингвистические особенности голландского языка по сравнению с моделями, обученными на английском или многоязычных данных. Это преимущество выявляется с помощью методов кластеризации и классификации. Улучшение представления лингвистических особенностей также приводит к повышению производительности в задаче автоматического распознавания речи. Результаты подчеркивают важность использования языково-специфичных данных при предобучении моделей для конкретного языка.'}, 'en': {'title': 'Unlocking Dutch: The Power of Language-Specific Pre-Training', 'desc': 'This paper investigates how well self-supervised Wav2Vec2 models can learn Dutch language features when trained specifically on Dutch data. The study finds that pre-training exclusively on Dutch leads to better encoding of Dutch phonetic and lexical information compared to using English or multilingual data. The improvements are measured using clustering and classification probes, which show that the models capture language-specific characteristics more effectively. Additionally, these enhancements in linguistic representation correlate with better performance in Automatic Speech Recognition tasks.'}, 'zh': {'title': '专注荷兰语，提升语音识别表现', 'desc': '本研究探讨了自监督学习模型Wav2Vec2在编码荷兰语语言特征方面的表现。研究发现，当模型仅在荷兰语数据上进行预训练时，能够更准确地捕捉荷兰语的语音和词汇信息。与在英语或多语言数据上进行相似量的预训练相比，荷兰语特征的表示显著提高。该语言特定的优势通过聚类和分类探测器得到了验证，并且与自动语音识别的性能提升相一致。'}}}, {'id': 'https://huggingface.co/papers/2506.03569', 'title': 'MiMo-VL Technical Report', 'url': 'https://huggingface.co/papers/2506.03569', 'abstract': 'We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.', 'score': 56, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'cb568276c7e799cb', 'authors': ['Xiaomi LLM-Core Team', ':', 'Zihao Yue', 'Zhenru Lin', 'Yifan Song', 'Weikun Wang', 'Shuhuai Ren', 'Shuhao Gu', 'Shicheng Li', 'Peidian Li', 'Liang Zhao', 'Lei Li', 'Kainan Bao', 'Hao Tian', 'Hailin Zhang', 'Gang Wang', 'Dawei Zhu', 'Cici', 'Chenhong He', 'Bowen Ye', 'Bowen Shen', 'Zihan Zhang', 'Zihan Jiang', 'Zhixian Zheng', 'Zhichao Song', 'Zhenbo Luo', 'Yue Yu', 'Yudong Wang', 'Yuanyuan Tian', 'Yu Tu', 'Yihan Yan', 'Yi Huang', 'Xu Wang', 'Xinzhe Xu', 'Xingchen Song', 'Xing Zhang', 'Xing Yong', 'Xin Zhang', 'Xiangwei Deng', 'Wenyu Yang', 'Wenhan Ma', 'Weiwei Lv', 'Weiji Zhuang', 'Wei Liu', 'Sirui Deng', 'Shuo Liu', 'Shimao Chen', 'Shihua Yu', 'Shaohui Liu', 'Shande Wang', 'Rui Ma', 'Qiantong Wang', 'Peng Wang', 'Nuo Chen', 'Menghang Zhu', 'Kangyang Zhou', 'Kang Zhou', 'Kai Fang', 'Jun Shi', 'Jinhao Dong', 'Jiebao Xiao', 'Jiaming Xu', 'Huaqiu Liu', 'Hongshen Xu', 'Heng Qu', 'Haochen Zhao', 'Hanglong Lv', 'Guoan Wang', 'Duo Zhang', 'Dong Zhang', 'Di Zhang', 'Chong Ma', 'Chang Liu', 'Can Cai', 'Bingquan Xia'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2506.03569.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#multimodal', '#rlhf', '#benchmark', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты', 'desc': 'Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT и MiMo-VL-7B-RL, демонстрирующие передовые результаты в задачах визуального понимания и мультимодальных рассуждений. Модель MiMo-VL-7B-RL превосходит Qwen2.5-VL-7B в 35 из 40 оцениваемых задач и достигает 59.4 баллов на бенчмарке OlympiadBench. Обучение моделей включало четырехэтапное предобучение на 2.4 триллионах токенов и применение смешанного обучения с подкреплением (MORL). Авторы подчеркивают важность включения качественных данных для рассуждений с длинной цепочкой мыслей в этапы предобучения.'}, 'en': {'title': 'Revolutionizing Vision-Language Models with MiMo-VL', 'desc': 'The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research.'}, 'zh': {'title': '开创视觉-语言模型的新标准', 'desc': '我们开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL，这两个强大的视觉-语言模型在一般视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在40个评估任务中有35个超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数高达78B的模型。在GUI定位应用中，它在OSWorld-G上以56.1的分数设定了新标准，甚至超越了专门模型UI-TARS。我们的训练结合了四阶段的预训练（24万亿个标记）和混合在线强化学习（MORL），并强调了在预训练阶段融入高质量推理数据和长链思维的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.04207', 'title': 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04207', 'abstract': 'Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.', 'score': 39, 'issue_id': 4135, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '61521f9ed974c930', 'authors': ['Shuang Chen', 'Yue Guo', 'Zhaochen Su', 'Yafu Li', 'Yulun Wu', 'Jiacheng Chen', 'Jiayu Chen', 'Weijie Wang', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04207.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#training', '#benchmark', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений MLLM: от инициализации до многоэтапного RL', 'desc': 'Статья посвящена улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждению с помощью обучения с подкреплением (RL). Авторы выявили три ключевых феномена в процессе обучения: важность правильной инициализации, проблему стагнации градиентов при стандартном GRPO и эффективность последующего текстового RL. На основе этих наблюдений была разработана модель ReVisual-R1, достигшая нового уровня производительности среди открытых 7B MLLM на сложных бенчмарках.'}, 'en': {'title': 'Unlocking Reasoning in MLLMs with Smart Training Strategies', 'desc': 'This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.'}, 'zh': {'title': '提升多模态推理的新方法', 'desc': '本文探讨了如何通过强化学习（RL）提升多模态大型语言模型（MLLM）的推理能力。研究发现，良好的冷启动初始化对于增强MLLM的推理至关重要，单独使用精心选择的文本数据即可超越许多近期的多模态推理模型。标准的GRPO在多模态RL中存在梯度停滞的问题，影响了训练的稳定性和性能。通过分阶段的训练方法，结合文本和多模态RL，提出了ReVisual-R1，达到了开源7B MLLM在多个基准测试中的新状态。'}}}, {'id': 'https://huggingface.co/papers/2506.04089', 'title': 'AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment', 'url': 'https://huggingface.co/papers/2506.04089', 'abstract': 'AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.  \t\t\t\t\tAI-generated summary \t\t\t\t As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.', 'score': 39, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5facb42d11447ff', 'authors': ['Anastasiia Ivanova', 'Eva Bakaeva', 'Zoya Volovikova', 'Alexey K. Kovalev', 'Aleksandr I. Panov'], 'affiliations': ['AIRI, Moscow, Russia', 'LMU, Munich, Germany', 'MIPT, Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04089.jpg', 'data': {'categories': ['#interpretability', '#agents', '#data', '#dataset', '#alignment'], 'emoji': '🍳', 'ru': {'title': 'AmbiK: унифицированный бенчмарк для обнаружения неоднозначности в инструкциях для роботов', 'desc': 'AmbiK - это текстовый датасет неоднозначных инструкций для кухонных роботов, созданный для унифицированного сравнения методов обнаружения неоднозначности. Датасет содержит 1000 пар неоднозначных задач и их однозначных аналогов, классифицированных по типу неоднозначности. AmbiK был собран с помощью больших языковых моделей (LLM) и проверен людьми. Он включает описания окружения, уточняющие вопросы и ответы, намерения пользователей и планы задач.'}, 'en': {'title': 'AmbiK: A Unified Benchmark for Ambiguity Detection in Kitchen Robotics', 'desc': 'The paper introduces AmbiK, a new dataset designed to help researchers evaluate how well different methods can detect ambiguous instructions for kitchen robots. AmbiK contains 1000 pairs of ambiguous and clear tasks, categorized by types of ambiguity such as human preferences and safety concerns. This dataset is unique because it allows for standardized testing of various ambiguity detection techniques, which have previously been difficult to compare due to differing datasets. By providing a common benchmark, AmbiK aims to advance the development of more effective language models in handling real-world instructions.'}, 'zh': {'title': '统一比较模糊性检测方法的AmbiK数据集', 'desc': 'AmbiK是一个针对厨房机器人模糊指令的文本数据集，旨在统一比较模糊性检测方法。该数据集包含1000对模糊任务及其明确对应任务，涵盖人类偏好、常识知识和安全等模糊性类型。AmbiK由大型语言模型（LLMs）协助收集，并经过人工验证，提供环境描述、澄清问题及答案、用户意图和任务计划等信息。我们希望AmbiK能帮助研究人员进行模糊性检测方法的统一比较。'}}}, {'id': 'https://huggingface.co/papers/2505.16968', 'title': 'CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark', 'url': 'https://huggingface.co/papers/2505.16968', 'abstract': 'CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.', 'score': 35, 'issue_id': 4139, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a069288c85761286', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Gustavo Bertolo Stahl', 'Seung Hun Eddie Han', 'Salman Khan', 'Abdulrahman Mahmoud'], 'affiliations': ['Australian National University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.16968.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#benchmark', '#open_source'], 'emoji': '🔄', 'ru': {'title': 'CASS: Преодоление барьеров между GPU-архитектурами', 'desc': 'CASS представляет собой набор данных и набор моделей для транспиляции GPU-кода между архитектурами как на уровне исходного кода, так и на уровне ассемблера. Модели CASS достигают высокой точности перевода: 95% для исходного кода и 37.5% для ассемблера, превосходя коммерческие решения. Сгенерированный код соответствует производительности нативного кода в более чем 85% тестовых случаев. Авторы также представили CASS-Bench - набор тестов для оценки качества транспиляции GPU-кода.'}, 'en': {'title': 'CASS: Bridging GPU Code Portability with High Accuracy Transpilation', 'desc': 'CASS is a groundbreaking dataset and model suite designed for GPU code transpilation, focusing on both source-level and assembly-level translations. It includes 70,000 verified code pairs that facilitate the conversion of code between different GPU architectures, addressing the challenge of low-level code portability. The CASS models achieve impressive accuracy rates, with 95% for source translation and 37.5% for assembly translation, significantly surpassing existing commercial solutions. Additionally, the generated code maintains native performance in over 85% of cases, and the accompanying CASS-Bench provides a robust evaluation framework for various GPU domains.'}, 'zh': {'title': 'CASS：GPU代码转译的突破性进展', 'desc': 'CASS是一个用于GPU代码转译的数据集和模型套件，支持源代码和汇编级别的转译。它包含70,000对经过验证的代码对，解决了低级GPU代码可移植性的重要问题。通过训练CASS系列特定领域语言模型，我们在源代码转译中达到了95%的准确率，并在汇编转译中达到了37.5%的准确率。CASS生成的代码在超过85%的测试案例中与本地性能相匹配，保持了运行时和内存行为的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.02921', 'title': 'A Controllable Examination for Long-Context Language Models', 'url': 'https://huggingface.co/papers/2506.02921', 'abstract': 'LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.', 'score': 30, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '073ae66fedf9c141', 'authors': ['Yijun Yang', 'Zeyu Huang', 'Wenhao Zhu', 'Zihan Qiu', 'Fei Yuan', 'Jeff Z. Pan', 'Ivan Titov'], 'affiliations': ['Nanjing University', 'Qwen Team, Alibaba Group', 'Shanghai Artificial Intelligence Laboratory', 'University of Amsterdam', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.02921.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#long_context', '#reasoning', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'LongBioBench: Новый стандарт оценки языковых моделей с длинным контекстом', 'desc': 'LongBioBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом, использующий искусственно сгенерированные биографии. Он оценивает модели по трем аспектам: понимание, рассуждение и надежность. Бенчмарк создан для преодоления ограничений существующих методов оценки, таких как сложность интерпретации реальных задач и недостаток когерентности в синтетических тестах. Эксперименты показали, что большинство моделей все еще имеют проблемы с семантическим пониманием и элементарными рассуждениями, а также становятся менее надежными при увеличении длины контекста.'}, 'en': {'title': 'LongBioBench: A New Standard for Evaluating Long-Context Language Models', 'desc': 'LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs.'}, 'zh': {'title': 'LongBioBench：评估长上下文语言模型的新基准', 'desc': 'LongBioBench 是一个新的基准，利用人工生成的传记来评估长上下文语言模型（LCLM）在理解、推理和可信度方面的表现，解决了现有框架的局限性。现有的评估框架分为真实世界任务和合成任务，但两者都有内在的缺陷。真实世界任务复杂且易受数据污染，而合成任务常常缺乏连贯性，影响其作为现实应用的有效性。LongBioBench 提供了一个受控环境，能够更好地评估 LCLM 的能力，实验结果显示大多数模型在语义理解和基本推理上仍存在不足。'}}}, {'id': 'https://huggingface.co/papers/2506.04141', 'title': "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos", 'url': 'https://huggingface.co/papers/2506.04141', 'abstract': 'A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.  \t\t\t\t\tAI-generated summary \t\t\t\t The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as "question frame") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities.', 'score': 25, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a12411c7424fd2a7', 'authors': ['Kejian Zhu', 'Zhuoran Jin', 'Hongbang Yuan', 'Jiachun Li', 'Shangqing Tu', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04141.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#long_context', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'MMR-V: Новый рубеж в мультимодальных рассуждениях по видео', 'desc': 'Предложен новый бенчмарк MMR-V для оценки мультимодальных языковых моделей в задачах рассуждения по видео. Он требует анализа кадров, удаленных от упомянутых в вопросе, и выявления скрытой информации. Эксперименты показали, что современные модели испытывают трудности с такими задачами - лучшая достигла точности лишь 52.5%. Бенчмарк призван стимулировать исследования по улучшению навыков мультимодальных рассуждений у ИИ.'}, 'en': {'title': 'MMR-V: Elevating Multimodal Reasoning in Videos', 'desc': 'The paper introduces MMR-V, a new benchmark designed to test the capabilities of multimodal large language models (MLLMs) in video analysis. It emphasizes the need for long-range, multi-frame reasoning, where models must analyze evidence that is not immediately adjacent to the question frame. Unlike existing benchmarks that focus on simple understanding tasks, MMR-V requires models to reason about hidden information and avoid shortcuts through distractor annotations. The findings show that current models struggle with these challenges, achieving only modest accuracy, highlighting the need for further research in enhancing multimodal reasoning skills.'}, 'zh': {'title': 'MMR-V：推动多模态推理的新基准', 'desc': '本文提出了一个新的基准MMR-V，旨在挑战多模态大型语言模型在视频中的长距离、多帧推理和隐藏信息处理能力。现有的视频基准主要集中在理解任务上，而MMR-V要求模型进行更复杂的推理，分析与问题帧相距较远的证据帧。该基准包含317个视频和1257个任务，实验结果显示当前模型在多模态推理方面仍然存在困难，最佳模型的准确率仅为52.5%。我们希望MMR-V能够激发进一步研究，以提升多模态推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04180', 'title': 'SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.04180', 'abstract': 'Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.', 'score': 23, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '3f52b337c5fa3683', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04180.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#long_context', '#rlhf', '#benchmark', '#dataset', '#story_generation'], 'emoji': '✍️', 'ru': {'title': 'Структурированное мышление для улучшения генерации длинных текстов', 'desc': 'SuperWriter-Agent - это новая система для улучшения качества генерации длинных текстов с помощью больших языковых моделей (LLM). Она вводит этапы планирования и уточнения в процесс генерации, имитируя подход профессионального писателя. Авторы обучили 7B-параметровую модель SuperWriter-LM на специально созданном наборе данных и разработали иерархическую процедуру оптимизации предпочтений (DPO) с использованием метода Монте-Карло. Эмпирические результаты показывают, что SuperWriter-LM превосходит более крупные базовые модели по автоматическим и человеческим оценкам.'}, 'en': {'title': 'Elevating Long-Form Text Generation with Structured Thinking', 'desc': 'This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks.'}, 'zh': {'title': '提升长文本生成质量的智能代理', 'desc': '长文本生成是大型语言模型（LLMs）面临的重要挑战，尤其是在保持连贯性、逻辑一致性和文本质量方面。为了解决这些问题，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。该框架通过规划和精炼阶段引入明确的结构化思维，指导模型遵循更有意识和认知基础的过程，类似于专业作家的写作方式。实验结果表明，SuperWriter-LM在多个基准测试中表现出色，超越了更大规模的基线模型，证明了分层直接偏好优化（DPO）和结构化思维步骤的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.04142', 'title': 'Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis', 'url': 'https://huggingface.co/papers/2506.04142', 'abstract': 'A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation', 'score': 21, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '4799bf9d7a1cb57f', 'authors': ['Kejian Zhu', 'Shangqing Tu', 'Zhuoran Jin', 'Lei Hou', 'Juanzi Li', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences', 'The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04142.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#ethics', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с загрязнением данных в языковых моделях через патчинг нейронов-шорткатов', 'desc': "Метод 'shortcut neuron patching' идентифицирует и подавляет нейроны-шорткаты в языковых моделях для снижения проблем загрязнения данных при надежной оценке. Исследователи обнаружили, что переоценка загрязненных моделей вероятно связана с параметрами, приобретающими короткие пути решения при обучении. Предложенный метод показал высокую корреляцию с надежным бенчмарком MixEval, достигнув коэффициента Спирмена более 0,95. Эксперименты подтвердили эффективность подхода в снижении загрязнения данных и его обобщаемость на различные бенчмарки."}, 'en': {'title': 'Suppressing Shortcut Neurons for Trustworthy Evaluations', 'desc': 'This paper presents a method called shortcut neuron patching, which aims to identify and suppress shortcut neurons in language models to improve the reliability of evaluations. The authors highlight that current evaluation methods often suffer from data contamination, leading to unfair assessments of model performance. By analyzing the internal mechanisms of contaminated models, they find that shortcut solutions during training contribute to overestimation of model capabilities. Their proposed method effectively mitigates these issues, showing strong correlation with established trustworthy benchmarks, thus ensuring more accurate evaluations of language models.'}, 'zh': {'title': '抑制快捷神经元，提升评估可信度', 'desc': '本文提出了一种名为快捷神经元修补的方法，用于识别和抑制语言模型中的快捷神经元，以减轻数据污染问题，从而提高评估的可信度。当前的评估方法大多依赖公共基准，但这些基准容易受到数据污染的影响，导致评估结果不公平。我们通过比较和因果分析，发现训练过程中模型参数可能会获得快捷解决方案，从而导致对污染模型的过高估计。实验结果表明，我们的方法在减轻污染方面有效，并且与MixEval基准的评估结果具有很强的线性相关性，Spearman系数超过0.95，表明我们的方法能够真实反映模型的能力。'}}}, {'id': 'https://huggingface.co/papers/2506.04178', 'title': 'OpenThoughts: Data Recipes for Reasoning Models', 'url': 'https://huggingface.co/papers/2506.04178', 'abstract': 'The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond. All of our datasets and models are available on https://openthoughts.ai.', 'score': 18, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '43845ce52af6bd6c', 'authors': ['Etash Guha', 'Ryan Marten', 'Sedrick Keh', 'Negin Raoof', 'Georgios Smyrnis', 'Hritik Bansal', 'Marianna Nezhurina', 'Jean Mercat', 'Trung Vu', 'Zayne Sprague', 'Ashima Suvarna', 'Benjamin Feuer', 'Liangyu Chen', 'Zaid Khan', 'Eric Frankel', 'Sachin Grover', 'Caroline Choi', 'Niklas Muennighoff', 'Shiye Su', 'Wanjia Zhao', 'John Yang', 'Shreyas Pimpalgaonkar', 'Kartik Sharma', 'Charlie Cheng-Jie Ji', 'Yichuan Deng', 'Sarah Pratt', 'Vivek Ramanujan', 'Jon Saad-Falcon', 'Jeffrey Li', 'Achal Dave', 'Alon Albalak', 'Kushal Arora', 'Blake Wulfe', 'Chinmay Hegde', 'Greg Durrett', 'Sewoong Oh', 'Mohit Bansal', 'Saadia Gabriel', 'Aditya Grover', 'Kai-Wei Chang', 'Vaishaal Shankar', 'Aaron Gokaslan', 'Mike A. Merrill', 'Tatsunori Hashimoto', 'Yejin Choi', 'Jenia Jitsev', 'Reinhard Heckel', 'Maheswaran Sathiamoorthy', 'Alexandros G. Dimakis', 'Ludwig Schmidt'], 'affiliations': ['ASU', 'BespokeLabs.ai', 'Cornell Tech', 'JSC', 'LAION', 'Lila Sciences', 'NYU', 'Open-Ψ (Open-Sci) Collective', 'Stanford University', 'TUM', 'Toyota Research Institute', 'UC Berkeley', 'UCLA', 'UNC Chapel Hill', 'UT Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.04178.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#data', '#training', '#science', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Открытые данные для прорыва в ИИ-рассуждениях', 'desc': 'Проект OpenThoughts создал открытые наборы данных для обучения моделей рассуждений, которые соответствуют или превосходят современные эталоны в математике, программировании и науке. Исследователи разработали датасет OpenThoughts2-1M, который привел к созданию модели OpenThinker2-32B, сопоставимой с DeepSeek-R1-Distill-32B по стандартным показателям. Дальнейшее улучшение датасета и масштабирование процесса до 1,2 млн примеров позволило создать модель OpenThinker3-7B, достигающую наилучших результатов в тестах AIME, LiveCodeBench и GPQA Diamond. Все датасеты и модели проекта находятся в открытом доступе.'}, 'en': {'title': 'Open-Source Datasets for Superior Reasoning Models', 'desc': 'The OpenThoughts project focuses on creating open-source datasets to enhance reasoning models in math, code, and science. By developing the OpenThoughts2-1M dataset, they trained the OpenThinker2-32B model, which achieved performance comparable to proprietary models on standard benchmarks. The project further refined its dataset through extensive experimentation, resulting in the OpenThoughts3 dataset and the OpenThinker3-7B model. This model set new state-of-the-art results on multiple reasoning benchmarks, demonstrating the effectiveness of publicly available training data.'}, 'zh': {'title': '开源数据集助力推理模型突破极限', 'desc': 'OpenThoughts项目旨在创建开源数据集，以训练推理模型，达到或超过数学、代码和科学领域的最新基准。尽管推理模型在多个基准测试中取得了快速进展，但仍存在许多关于最佳训练方法的未解问题。通过系统地研究数据生成流程，OpenThoughts项目开发了多个版本的数据集，最终推出了OpenThinker3-7B模型，取得了在多个标准推理基准上的最佳成绩。所有数据集和模型均可在https://openthoughts.ai上获取。'}}}, {'id': 'https://huggingface.co/papers/2506.03150', 'title': 'IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation', 'url': 'https://huggingface.co/papers/2506.03150', 'abstract': 'IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page', 'score': 18, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '524d1d7f47dfcf7f', 'authors': ['Yuanze Lin', 'Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ronald Clark', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Google DeepMind', 'NEC Labs America', 'UC Merced', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.03150.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#architecture', '#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Геометрия света: новый уровень контроля в генерации видео', 'desc': 'IllumiCraft - это инновационная диффузионная модель для генерации видео, интегрирующая геометрические подсказки для улучшения качества и временной согласованности. Модель использует три типа входных данных: HDR-карты видео для контроля освещения, синтетически переосвещенные кадры для подсказок внешнего вида и 3D-треки точек для геометрической информации. IllumiCraft позволяет создавать видео с управляемым освещением на основе текстовых запросов или изображений, превосходя существующие методы по качеству и контролируемости.'}, 'en': {'title': 'IllumiCraft: Crafting Coherent Videos with Geometric Precision', 'desc': 'IllumiCraft is a novel diffusion framework designed to create high-quality videos from text or image inputs while incorporating geometric cues. It utilizes three main inputs: HDR video maps for precise lighting control, synthetically relit frames for appearance variations, and 3D point tracks for accurate geometry representation. By merging these elements, IllumiCraft ensures that the generated videos are not only visually appealing but also maintain temporal coherence across frames. This approach enhances the fidelity of video generation compared to existing methods that lack such integrated controls.'}, 'zh': {'title': 'IllumiCraft：高保真视频生成的新方法', 'desc': 'IllumiCraft 是一个集成几何线索的扩散框架，能够从文本或图像输入生成高保真、时间一致的视频。该方法通过接受高动态范围（HDR）视频图像、合成重新照明的帧和3D点轨迹，来控制场景的光照和视觉外观。通过将光照、外观和几何线索整合在一个统一的扩散架构中，IllumiCraft 生成与用户定义提示一致的时间连贯视频。与现有的可控视频生成方法相比，它提供了更好的保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.04225', 'title': 'Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation', 'url': 'https://huggingface.co/papers/2506.04225', 'abstract': 'Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications.', 'score': 17, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '99f491949aa412fd', 'authors': ['Tianyu Huang', 'Wangguandong Zheng', 'Tengfei Wang', 'Yuhao Liu', 'Zhenwei Wang', 'Junta Wu', 'Jie Jiang', 'Hui Li', 'Rynson W. H. Lau', 'Wangmeng Zuo', 'Chunchao Guo'], 'affiliations': ['City University of Hong Kong, China', 'Harbin Institute of Technology, China', 'Southeast University, China', 'Tencent Hunyuan, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04225.jpg', 'data': {'categories': ['#games', '#dataset', '#diffusion', '#3d', '#video'], 'emoji': '🚀', 'ru': {'title': 'Исследуй 3D-миры из одного кадра', 'desc': 'Voyager - это система видеодиффузии, которая генерирует согласованные последовательности 3D-облаков точек из одного изображения. Она позволяет исследовать 3D-сцены на большие расстояния с пользовательскими траекториями камеры. Ключевые компоненты включают согласованную видеодиффузию, исследование мира на большие расстояния и масштабируемый механизм данных. Voyager превосходит существующие методы по визуальному качеству и геометрической точности, открывая новые возможности применения.'}, 'en': {'title': 'Voyager: Seamless 3D Scene Exploration from a Single Image', 'desc': 'Voyager is a video diffusion framework that creates 3D point-cloud sequences from a single image, allowing users to explore scenes along custom camera paths. It addresses the challenge of generating long-range, consistent 3D environments by integrating world-consistent video diffusion, which ensures alignment of RGB and depth sequences. The framework also features an efficient world cache for smooth scene exploration and a scalable data engine that automates camera pose estimation and depth prediction. Overall, Voyager enhances visual quality and geometric accuracy, making it suitable for applications in video gaming and virtual reality.'}, 'zh': {'title': 'Voyager：从单图像生成一致的3D场景探索', 'desc': 'Voyager是一种视频扩散框架，可以从单张图像生成世界一致的3D点云序列，支持用户定义的相机路径进行长距离的3D场景探索。该方法通过端到端的场景生成和重建，确保了帧间的一致性，避免了传统的3D重建流程。Voyager集成了三个关键组件：世界一致的视频扩散、长距离世界探索和可扩展的数据引擎，提升了视觉质量和几何精度。该框架在视频游戏和虚拟现实等应用中具有广泛的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.04158', 'title': 'Image Editing As Programs with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.04158', 'abstract': 'While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.', 'score': 15, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5a32d484bb427f3', 'authors': ['Yujia Hu', 'Songhua Liu', 'Zhenxiong Tan', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.04158.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента', 'desc': 'Исследователи представили новый подход к редактированию изображений с использованием искусственного интеллекта, названный IEAP (Image Editing As Programs). Эта система основана на архитектуре Diffusion Transformer и разбивает сложные инструкции по редактированию на последовательность простых операций. Каждая операция реализуется с помощью специализированного адаптера, использующего общий базовый DiT. IEAP значительно превосходит современные методы в различных задачах редактирования, особенно при сложных многоэтапных инструкциях.'}, 'en': {'title': 'Revolutionizing Image Editing with Programmatic Precision', 'desc': 'This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions.'}, 'zh': {'title': '图像编辑的新方法：将复杂指令转化为简单操作', 'desc': '本研究提出了一种新的图像编辑框架，称为图像编辑作为程序（IEAP），旨在解决扩散模型在指令驱动的图像编辑中面临的挑战。IEAP基于扩散变换器（DiT）架构，通过将复杂的编辑指令分解为一系列原子操作来实现。每个操作由轻量级适配器实现，专门针对特定类型的编辑，能够支持任意和结构不一致的变换。实验结果表明，IEAP在各种编辑场景中显著优于现有的最先进方法，尤其在处理复杂的多步骤指令时表现出更高的准确性和语义保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.03295', 'title': 'Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem', 'url': 'https://huggingface.co/papers/2506.03295', 'abstract': "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.", 'score': 15, 'issue_id': 4135, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '03df39687a4bdf5a', 'authors': ['Yubo Wang', 'Ping Nie', 'Kai Zou', 'Lijun Wu', 'Wenhu Chen'], 'affiliations': ['Independent', 'Netmind.AI', 'Shanghai AI Lab', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03295.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное раскрытие потенциала ИИ через обучение на критике', 'desc': 'Статья представляет метод Critique Fine-Tuning (CFT) для улучшения способностей рассуждения больших языковых моделей (LLM). CFT использует обучение на критических отзывах о решениях одной задачи, генерируемых моделью-учителем. Эксперименты показывают, что CFT значительно повышает производительность моделей на различных задачах рассуждения при меньших вычислительных затратах по сравнению с обучением с подкреплением. Результаты демонстрируют эффективность CFT как простого и общего подхода к раскрытию потенциала современных LLM.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Critique Fine-Tuning', 'desc': 'This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.'}, 'zh': {'title': '批评微调：高效释放语言模型推理潜力的利器', 'desc': '本文提出了一种名为批评微调（Critique Fine-Tuning, CFT）的方法，旨在高效提升大型语言模型（LLM）的推理能力。通过对单一问题进行微调，CFT能够显著提高模型在推理任务上的表现，同时减少计算成本。研究表明，使用CFT方法，模型在多个推理基准测试中平均提升了15%到16%。与传统的强化学习方法相比，CFT在计算资源上更加高效，展示了其作为一种简单且通用的推理能力提升策略的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.01320', 'title': 'Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models', 'url': 'https://huggingface.co/papers/2506.01320', 'abstract': 'We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.', 'score': 15, 'issue_id': 4136, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '6441248200df695a', 'authors': ['Taehoon Yoon', 'Yunhong Min', 'Kyeongmin Yeo', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01320.jpg', 'data': {'categories': ['#inference', '#alignment', '#rlhf', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Эффективное согласование наград в генеративных моделях с помощью умной выборки', 'desc': 'Статья представляет Psi-Sampler - новый метод для улучшения согласования наград при инференсе в генеративных моделях на основе оценок. Авторы предлагают использовать последовательный метод Монте-Карло (SMC) с инициализацией частиц из апостериорного распределения, учитывающего награду. Для эффективной выборки из апостериорного распределения в высокоразмерных латентных пространствах вводится алгоритм preconditioned Crank-Nicolson Langevin (pCNL). Эксперименты показывают улучшение результатов на различных задачах генерации изображений с учетом наград.'}, 'en': {'title': 'Enhancing Reward Alignment with Psi-Sampler', 'desc': 'The paper presents Psi-Sampler, a framework that enhances reward alignment during inference by using Sequential Monte Carlo (SMC) methods. It addresses the limitations of traditional particle initialization from Gaussian priors, which often fail to capture important reward-related areas. By employing a reward-aware posterior for initialization, the framework significantly boosts sampling efficiency and alignment performance. Additionally, the introduction of the preconditioned Crank-Nicolson Langevin (pCNL) algorithm allows for effective sampling in complex, high-dimensional spaces, leading to improved results in various generative tasks.'}, 'zh': {'title': '高效的奖励对齐：Psi-Sampler框架', 'desc': '本文介绍了一种名为Psi-Sampler的框架，该框架基于序列蒙特卡洛（SMC）方法，并结合了基于奖励的初始粒子采样，以实现与基于分数的生成模型的有效推理时间奖励对齐。近年来，基于分数的生成模型在推理时间奖励对齐方面受到了广泛关注，标志着从预训练到后训练优化的范式转变。现有方法通常从高斯先验初始化粒子，这不足以捕捉与奖励相关的区域，导致采样效率降低。我们展示了从奖励感知后验初始化显著提高了对齐性能，并引入了预条件Crank-Nicolson Langevin（pCNL）算法，以实现高维潜在空间中的后验采样。'}}}, {'id': 'https://huggingface.co/papers/2506.03930', 'title': 'VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation', 'url': 'https://huggingface.co/papers/2506.03930', 'abstract': 'VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation.', 'score': 14, 'issue_id': 4139, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '56d407ae0e0e6b4a', 'authors': ['Yuansheng Ni', 'Ping Nie', 'Kai Zou', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.03930.jpg', 'data': {'categories': ['#story_generation', '#data', '#dataset', '#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'VisCode-200K: Большие данные для умного построения графиков', 'desc': 'VisCode-200K - это крупномасштабный набор данных для задач визуализации, который улучшает генерацию графиков с помощью обучения с подкреплением на основе выполнения кода и итеративной коррекции. Датасет содержит более 200 тысяч примеров из двух источников: проверенный код построения графиков из открытых репозиториев и диалоги по исправлению кода. На основе VisCode-200K была обучена модель VisCoder, которая превзошла открытые базовые модели и приблизилась к производительности проприетарных моделей. Исследование демонстрирует преимущества обучения на основе обратной связи для генерации исполняемого и визуально точного кода.'}, 'en': {'title': 'Empowering Visualization with VisCode-200K: A Leap in Plot Generation!', 'desc': 'The paper introduces VisCode-200K, a comprehensive dataset designed to enhance the performance of machine learning models in generating visualizations through improved code generation. It addresses the limitations of existing datasets by incorporating execution-grounded supervision and enabling iterative code correction, which helps models learn from their mistakes. The dataset consists of over 200,000 examples, including validated plotting code and multi-turn dialogues for code feedback. The authors demonstrate that their model, VisCoder, fine-tuned on this dataset, significantly outperforms existing open-source models and competes closely with proprietary ones in generating accurate visualizations.'}, 'zh': {'title': 'VisCode-200K：提升可视化生成的革命性数据集', 'desc': 'VisCode-200K是一个大规模的数据集，专注于可视化任务，旨在提高绘图生成的性能。该数据集结合了执行基础的监督和迭代代码修正，解决了现有模型在绘图时的脆弱性和不可靠性。它包含来自开源代码库的有效绘图代码和自然语言指令的配对，以及多轮修正对话，帮助模型修正错误代码。通过在VisCode-200K上微调模型，VisCoder在绘图生成方面显著超越了开源基线，接近商业模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04228', 'title': 'LayerFlow: A Unified Model for Layer-aware Video Generation', 'url': 'https://huggingface.co/papers/2506.04228', 'abstract': 'LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.', 'score': 13, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1e8f6532d8b54b21', 'authors': ['Sihui Ji', 'Hao Luo', 'Xi Chen', 'Yuanpeng Tu', 'Yiyang Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group, China', 'Hupan Laboratory, China', 'The University of Hong Kong', 'The University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.04228.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#synthetic', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'LayerFlow: Умная генерация многослойного видео по текстовым подсказкам', 'desc': 'LayerFlow - это унифицированная система для генерации видео с учетом слоев, использующая трансформер диффузии для преобразования текста в видео и встраивания слоев. Она поддерживает различные задачи генерации видео, включая создание прозрачного переднего плана, чистого фона и смешанной сцены. Система использует многоэтапную стратегию обучения для адаптации к статическим изображениям с высококачественными аннотациями слоев. LayerFlow применяет LoRA для настройки движения и содержания, что позволяет генерировать плавные видео с желаемыми слоями.'}, 'en': {'title': 'LayerFlow: Unified Layer-Aware Video Generation', 'desc': 'LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images.'}, 'zh': {'title': 'LayerFlow：统一的层感知视频生成框架', 'desc': 'LayerFlow是一个统一的框架，用于生成层感知的视频，利用文本到视频的扩散变换器和层嵌入。该框架支持多种视频生成任务，包括透明前景、干净背景和混合场景的视频生成。通过将视频按层组织为子剪辑，并利用层嵌入来区分每个剪辑及其对应的层级提示，LayerFlow实现了多种视频生成变体。为了克服高质量层级训练视频的缺乏，LayerFlow设计了多阶段训练策略，结合静态图像和高质量层注释进行训练。'}}}, {'id': 'https://huggingface.co/papers/2506.03139', 'title': 'SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation', 'url': 'https://huggingface.co/papers/2506.03139', 'abstract': 'SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at https://zju-real.github.io/SVGenius.', 'score': 13, 'issue_id': 4137, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c1fdc3559598aa68', 'authors': ['Siqi Chen', 'Xinyu Dong', 'Haolei Xu', 'Xingyu Wu', 'Fei Tang', 'Hang Zhang', 'Yuchen Yan', 'Linjuan Wu', 'Wenqi Zhang', 'Guiyang Hou', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03139.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'SVGenius: комплексная оценка возможностей ИИ в работе с векторной графикой', 'desc': 'SVGenius - это комплексный бенчмарк для оценки способностей больших языковых моделей (LLM) и мультимодальных LLM в обработке SVG. Бенчмарк включает 2377 запросов в трех измерениях: понимание, редактирование и генерация SVG. Оценка проводится по 8 категориям задач и 18 метрикам на основе реальных данных из 24 прикладных областей. Результаты показывают, что проприетарные модели значительно превосходят открытые, но все модели демонстрируют снижение производительности с увеличением сложности задач.'}, 'en': {'title': 'SVGenius: Unveiling SVG Processing Potential in LLMs', 'desc': 'SVGenius is a benchmark designed to evaluate the performance of Large Language Models (LLMs) and Multimodal LLMs in processing Scalable Vector Graphics (SVG). It assesses models across three key dimensions: understanding, editing, and generation, using a total of 2,377 queries derived from real-world applications. The evaluation framework includes 8 task categories and 18 metrics, highlighting the strengths and weaknesses of 22 different models. Findings indicate that while proprietary models excel, all models struggle with complex tasks, suggesting a need for improved training methods, particularly in reasoning, to enhance their capabilities.'}, 'zh': {'title': 'SVGenius：全面评估 SVG 处理能力的基准工具', 'desc': 'SVGenius 是一个评估大型语言模型和多模态 LLM 在 SVG 处理能力的基准工具。它通过理解、编辑和生成三个维度，使用 2,377 个查询来全面评估模型的能力和局限性。研究发现，尽管专有模型在性能上优于开源模型，但所有模型在复杂性增加时表现普遍下降。SVGenius 提供了一个系统的评估框架，为开发更强大的矢量图形模型和推进自动化图形设计应用提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2506.03517', 'title': 'DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03517', 'abstract': 'Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels.', 'score': 12, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '5be6b9aa57297fb6', 'authors': ['Ziyi Wu', 'Anil Kag', 'Ivan Skorokhodov', 'Willi Menapace', 'Ashkan Mirzaei', 'Igor Gilitschenski', 'Sergey Tulyakov', 'Aliaksandr Siarohin'], 'affiliations': ['Snap Research', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03517.jpg', 'data': {'categories': ['#video', '#optimization', '#diffusion', '#rlhf'], 'emoji': '🎬', 'ru': {'title': 'DenseDPO: Точная оптимизация предпочтений для улучшения генерации видео', 'desc': 'DenseDPO - это новый метод для улучшения текст-в-видео диффузионных моделей. Он решает проблему смещения в сторону клипов с низкой подвижностью при аннотации предпочтений. DenseDPO создает пары видео путем очистки от шума искаженных копий исходного видео, что позволяет делать более точные сравнения. Метод также использует временное выравнивание для разметки предпочтений на коротких сегментах, а не на целых клипах. DenseDPO демонстрирует улучшенную генерацию движения по сравнению с обычным DPO, используя только треть размеченных данных.'}, 'en': {'title': 'Enhancing Video Generation with DenseDPO: Precision and Efficiency in Preference Learning', 'desc': 'This paper introduces DenseDPO, an improved method for Direct Preference Optimization (DPO) in text-to-video diffusion models. DenseDPO addresses the limitations of traditional DPO by creating video pairs from denoised versions of a ground truth video, allowing for better alignment and reducing bias towards low-motion clips. It also enables preference labeling on shorter video segments, which provides a more detailed learning signal while using less labeled data. Additionally, DenseDPO facilitates automatic preference annotation through Vision Language Models, achieving performance comparable to human-labeled data.'}, 'zh': {'title': 'DenseDPO：提升视频生成的偏好优化方法', 'desc': '直接偏好优化（DPO）最近被应用于文本到视频的扩散模型后训练技术。我们提出的DenseDPO方法通过三项贡献解决了DPO的不足之处。首先，我们通过去噪真实视频的损坏副本来创建视频对，从而消除了运动偏差。其次，我们利用时间对齐来标记短片段的偏好，使学习信号更加密集和精确，最终DenseDPO在运动生成方面显著优于传统DPO，同时在文本对齐、视觉质量和时间一致性方面表现相当。'}}}, {'id': 'https://huggingface.co/papers/2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", 'url': 'https://huggingface.co/papers/2505.24500', 'abstract': "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'score': 11, 'issue_id': 4133, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ee580986393d0b7e', 'authors': ['Guiyang Hou', 'Xing Gao', 'Yuchuan Wu', 'Xiang Huang', 'Wenqi Zhang', 'Zhe Zheng', 'Yongliang Shen', 'Jialu Du', 'Fei Huang', 'Yongbin Li', 'Weiming Lu'], 'affiliations': ['Nanjing University', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24500.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый метод обучения для повышения социального интеллекта языковых моделей', 'desc': 'Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения социального интеллекта больших языковых моделей (LLM). Этот подход учитывает временную составляющую и иерархию когнитивных процессов, характерных для социальных взаимодействий. Метод TimeHC-RL показал превосходство над широко используемым методом обучения с подкреплением System 2 RL. Эксперименты продемонстрировали, что применение TimeHC-RL позволяет моделям с 7 миллиардами параметров достигать производительности передовых моделей в задачах социального интеллекта.'}, 'en': {'title': "Boosting LLMs' Social Intelligence with TimeHC-RL", 'desc': "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."}, 'zh': {'title': '提升社交智能的时间感知强化学习', 'desc': '本文提出了一种新的方法，称为时间感知层次认知强化学习（TimeHC-RL），旨在提升大型语言模型（LLMs）在社交领域的智能。与数学等依赖系统2认知的领域不同，社交领域需要更丰富的认知模式，包括直觉反应和表层思维。通过对八个不同数据集的实验，我们验证了TimeHC-RL方法的有效性，结果显示其在社交智能方面优于传统的系统2强化学习方法。该方法使得7B基础模型的表现接近于更先进的模型，如DeepSeek-R1和OpenAI-O3。'}}}, {'id': 'https://huggingface.co/papers/2506.04108', 'title': 'Rectified Sparse Attention', 'url': 'https://huggingface.co/papers/2506.04108', 'abstract': 'Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ff7222f16cd2bf28', 'authors': ['Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Jian Chen', 'Yizhao Gao', 'Shijie Cao', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04108.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Эффективная генерация длинных текстов без потери качества', 'desc': 'Метод Rectified Sparse Attention (ReSA) улучшает эффективность генерации длинных последовательностей в больших языковых моделях. Он сочетает блочно-разреженное внимание с периодической плотной ректификацией, что позволяет ограничить накопление ошибок. ReSA достигает качества генерации, близкого к безошибочному, при значительном повышении эффективности. Метод обеспечивает ускорение до 2,42 раза при декодировании последовательностей длиной 256 тысяч токенов.'}, 'en': {'title': 'Boosting Efficiency in Long-Sequence Generation with ReSA', 'desc': "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."}, 'zh': {'title': '高效长序列生成的新方法：ReSA', 'desc': 'Rectified Sparse Attention（ReSA）是一种提高大型语言模型长序列生成效率的方法。它结合了块稀疏注意力和周期性密集整流，能够保持高质量的生成效果。通过在固定间隔内使用密集前向传递刷新KV缓存，ReSA限制了误差累积，并保持与预训练分布的对齐。实验表明，ReSA在数学推理、语言建模和检索任务中实现了接近无损的生成质量，并在256K序列长度下提供了高达2.42倍的端到端加速。'}}}, {'id': 'https://huggingface.co/papers/2506.02592', 'title': 'Beyond the Surface: Measuring Self-Preference in LLM Judgments', 'url': 'https://huggingface.co/papers/2506.02592', 'abstract': 'The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.', 'score': 8, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'ccdb2761a23fe0c8', 'authors': ['Zhi-Yuan Chen', 'Hao Wang', 'Xinyu Zhang', 'Enrui Hu', 'Yankai Lin'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.02592.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#interpretability', '#ethics', '#training'], 'emoji': '⚖️', 'ru': {'title': 'DBG: Новый способ измерения предвзятости в языковых моделях', 'desc': 'Статья представляет новый метод измерения предвзятости самопредпочтения в больших языковых моделях (LLM). Авторы вводят показатель DBG, который использует эталонные оценки в качестве прокси для качества ответов. Этот подход позволяет отделить влияние качества ответов от фактической предвзятости модели. Исследователи провели эксперименты с различными LLM и изучили факторы, влияющие на предвзятость самопредпочтения.'}, 'en': {'title': 'Measuring Self-Preference Bias with the DBG Score', 'desc': 'This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses.'}, 'zh': {'title': '引入DBG评分，精准测量自我偏好偏差', 'desc': '本文提出了DBG评分，用于测量大型语言模型中的自我偏好偏差。通过使用金标准判断作为响应质量的代理，DBG评分解决了响应质量对偏差测量的混淆效应。研究表明，现有方法在评估自我偏好偏差时，往往将其与响应质量混为一谈。我们通过实验评估了不同版本、规模和推理能力的语言模型的自我偏好偏差，并探讨了影响该偏差的因素。'}}}, {'id': 'https://huggingface.co/papers/2506.03610', 'title': 'Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on\n  Diverse Video Games', 'url': 'https://huggingface.co/papers/2506.03610', 'abstract': 'Orak is a benchmark for training and evaluating LLM agents across diverse video games, featuring a plug-and-play interface and fine-tuning datasets to enhance agentic modules and gameplay.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are reshaping the game industry, particularly with more intelligent and human-preferable game characters. However, existing game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents. To fill these gaps, we present \\benchname{}, a foundational benchmark designed to train and evaluate LLM agents across diverse real-world video games. Unlike existing benchmarks, Orak includes 12 popular video games spanning all major genres, enabling comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. To support consistent evaluation of LLMs, we introduce a plug-and-play interface based on Model Context Protocol (MCP) that enables LLMs to seamlessly connect with games and manipulate agentic modules. Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay trajectories across diverse game genres. Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects, establishing a foundation towards building generic gaming agents. Code is available at https://github.com/krafton-ai/Orak.', 'score': 7, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '8821e76acd2443a8', 'authors': ['Dongmin Park', 'Minkyu Kim', 'Beongjun Choi', 'Junhyuck Kim', 'Keon Lee', 'Jonghyun Lee', 'Inkyu Park', 'Byeong-Uk Lee', 'Jaeyoung Hwang', 'Jaewoo Ahn', 'Ameya S. Mahabaleshwarkar', 'Bilal Kartal', 'Pritam Biswas', 'Yoshi Suhara', 'Kangwook Lee', 'Jaewoong Cho'], 'affiliations': ['KRAFTON', 'NVIDIA', 'Seoul National University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.03610.jpg', 'data': {'categories': ['#games', '#agents', '#video', '#benchmark', '#transfer_learning'], 'emoji': '🎮', 'ru': {'title': 'Orak: универсальный бенчмарк для создания игровых ИИ-агентов нового поколения', 'desc': 'Orak - это новый бенчмарк для обучения и оценки агентов на основе больших языковых моделей (LLM) в разнообразных видеоиграх. Он включает в себя интерфейс plug-and-play и наборы данных для тонкой настройки, чтобы улучшить агентские модули и игровой процесс. Orak охватывает 12 популярных видеоигр различных жанров, что позволяет проводить комплексные исследования возможностей LLM и агентских модулей. Бенчмарк также предлагает всестороннюю систему оценки, включая таблицы лидеров по общим игровым показателям, арены для сражений LLM и углубленный анализ визуального ввода, агентских стратегий и эффектов тонкой настройки.'}, 'en': {'title': 'Orak: Elevating LLM Agents in Gaming', 'desc': 'Orak is a new benchmark designed for training and evaluating Large Language Model (LLM) agents in various video games. It addresses the limitations of existing benchmarks by providing a plug-and-play interface and fine-tuning datasets that enhance the performance of LLMs in complex gameplay scenarios. The benchmark includes 12 popular video games from different genres, allowing for a thorough assessment of LLM capabilities and agentic modules. With features like game score leaderboards and detailed analyses of gameplay strategies, Orak aims to advance the development of intelligent gaming agents.'}, 'zh': {'title': 'Orak：多样化视频游戏的LLM代理基准', 'desc': 'Orak是一个用于训练和评估大型语言模型（LLM）代理的基准，涵盖多种视频游戏。它提供了即插即用的接口和微调数据集，以增强代理模块和游戏玩法。与现有基准不同，Orak支持12款流行视频游戏，允许对LLM能力和代理模块进行全面研究。该基准还引入了基于模型上下文协议（MCP）的接口，确保LLM能够与游戏无缝连接并操作代理模块。'}}}, {'id': 'https://huggingface.co/papers/2506.03099', 'title': 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03099', 'abstract': 'TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/', 'score': 7, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'eff27ca5fef5cdcf', 'authors': ['Chetwin Low', 'Weimin Wang'], 'affiliations': ['Character AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03099.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#inference', '#games', '#audio', '#video', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Оживляем аватары: аудио-управляемая генерация видео в реальном времени', 'desc': 'TalkingMachines - это эффективная система, преобразующая предобученные модели генерации видео в аниматоры персонажей, управляемые аудио в реальном времени. Она адаптирует современную модель преобразования изображений в видео DiT для генерации аватаров на основе аудио с 18 миллиардами параметров. Система обеспечивает бесконечную потоковую передачу видео без накопления ошибок с помощью асимметричной дистилляции знаний. TalkingMachines также включает ряд инженерных оптимизаций для высокопроизводительного вывода с низкой задержкой.'}, 'en': {'title': 'Transforming Audio into Real-Time Avatar Animation', 'desc': 'TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays.'}, 'zh': {'title': '实时音频驱动的角色动画生成器', 'desc': '本文介绍了TalkingMachines，这是一个高效的框架，将预训练的视频生成模型转变为实时的音频驱动角色动画生成器。通过将音频大型语言模型（LLM）与视频生成基础模型结合，TalkingMachines能够实现自然的对话体验。我们的主要贡献包括：将一个预训练的最先进的图像到视频模型适配为一个具有180亿参数的音频驱动头像生成模型，以及通过不对称知识蒸馏实现无限视频流的生成。我们还设计了一个高吞吐量、低延迟的推理管道，结合了多项关键的工程优化。'}}}, {'id': 'https://huggingface.co/papers/2506.03355', 'title': 'Robustness in Both Domains: CLIP Needs a Robust Text Encoder', 'url': 'https://huggingface.co/papers/2506.03355', 'abstract': 'LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.', 'score': 6, 'issue_id': 4140, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'd9088aaea42f6fff', 'authors': ['Elias Abad Rocamora', 'Christian Schlarmann', 'Naman Deep Singh', 'Yongtao Wu', 'Matthias Hein', 'Volkan Cevher'], 'affiliations': ['LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland', 'Tubingen AI center, University of Tubingen, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03355.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#diffusion', '#rlhf', '#security'], 'emoji': '🛡️', 'ru': {'title': 'LEAF: Повышение устойчивости CLIP к состязательным атакам', 'desc': 'LEAF - это метод состязательной доводки (adversarial finetuning), который повышает устойчивость текстовых энкодеров CLIP. Он улучшает точность классификации с нулевым обучением (zero-shot accuracy) и производительность мультимодального поиска в условиях состязательного шума. LEAF эффективно масштабируется на большие модели CLIP и сохраняет исходную производительность для визуальных задач. Метод также улучшает качество генерации изображений по тексту и реконструкцию текста из эмбеддингов в условиях шума.'}, 'en': {'title': 'Enhancing Text Encoder Robustness with LEAF', 'desc': 'This paper introduces LEAF, a novel adversarial finetuning method designed to enhance the robustness of CLIP text encoders against adversarial attacks. By addressing the vulnerability of text embeddings, LEAF significantly boosts zero-shot accuracy and improves performance in multimodal retrieval tasks, even under adversarial noise. The method not only preserves the strong performance of image encoders but also enhances the quality of text-to-image generation. Overall, LEAF fills a critical gap in the literature by ensuring that text encoders are as robust as their image counterparts, leading to better model performance in various applications.'}, 'zh': {'title': 'LEAF：提升CLIP文本编码器鲁棒性的对抗微调方法', 'desc': 'LEAF是一种对抗微调方法，旨在增强CLIP文本编码器的鲁棒性。通过对抗噪声的训练，LEAF显著提高了文本领域的零-shot准确率和多模态检索性能。该方法不仅保持了图像编码器的视觉性能，还能在文本到图像生成模型中提升生成质量。我们的研究填补了文本编码器鲁棒性研究的空白，展示了其在多模态任务中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.03106', 'title': 'Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback', 'url': 'https://huggingface.co/papers/2506.03106', 'abstract': 'Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.', 'score': 6, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '99f5fb3b08ab4205', 'authors': ['Xiaoying Zhang', 'Hao Sun', 'Yipeng Zhang', 'Kaituo Feng', 'Chaochao Lu', 'Chao Yang', 'Helen Meng'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, HCCL', 'The Chinese University of Hong Kong, MMLab', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.03106.jpg', 'data': {'categories': ['#math', '#rl', '#reasoning', '#optimization', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Critique-GRPO: Улучшение рассуждений ИИ через комбинированную обратную связь', 'desc': 'Статья представляет Critique-GRPO - новую систему обучения с подкреплением для улучшения рассуждений больших языковых моделей. Эта система объединяет числовую и текстовую обратную связь, что позволяет преодолеть ограничения существующих методов. Эксперименты показывают, что Critique-GRPO превосходит другие подходы на основе обучения с учителем и обучения с подкреплением на различных задачах рассуждения. Исследование также выявляет важные аспекты исследовательского поведения модели в процессе обучения.'}, 'en': {'title': 'Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach', 'desc': 'Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks.'}, 'zh': {'title': 'Critique-GRPO：自然语言与数值反馈的完美结合', 'desc': 'Critique-GRPO是一种结合数值反馈和自然语言反馈的强化学习框架，旨在提升大型语言模型（LLM）的推理能力。该框架解决了仅依赖数值反馈时遇到的性能停滞、自我反思效果有限和持续失败等挑战。通过利用自然语言反馈，Critique-GRPO能够在模型表现停滞时，生成正确的改进建议。实验结果表明，Critique-GRPO在多个复杂任务中表现优于现有的监督学习和强化学习方法，显著提高了模型的平均通过率。'}}}, {'id': 'https://huggingface.co/papers/2506.03956', 'title': 'Adapt before Continual Learning', 'url': 'https://huggingface.co/papers/2506.03956', 'abstract': 'Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL.', 'score': 5, 'issue_id': 4138, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '33f8cb4049923c1f', 'authors': ['Aojun Lu', 'Tao Feng', 'Hangjie Yuan', 'Chunhui Ding', 'Yanan Sun'], 'affiliations': ['College of Computer Science Sichuan University Chengdu, China', 'College of Computer Science and Technology Zhejiang University Hangzhou, China', 'Department of Computer Science and Technology Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03956.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Балансировка стабильности и пластичности в непрерывном обучении', 'desc': 'Статья представляет новый подход к непрерывному обучению (Continual Learning) с использованием предобученных моделей. Авторы предлагают метод ACL (Adapting Pre-trained Models before the core CL process), который адаптирует основу предобученной модели перед обучением каждой новой задачи. ACL улучшает пластичность модели, выравнивая эмбеддинги с их исходными прототипами классов, при этом сохраняя стабильность. Эксперименты показывают, что ACL значительно повышает производительность непрерывного обучения на различных бенчмарках.'}, 'en': {'title': 'Enhancing Learning Flexibility with Pre-trained Models', 'desc': 'This paper introduces a new method called Adapting Pre-trained Models before the core Continual Learning (CL) process, which aims to improve how neural networks learn new information while keeping what they already know. The authors highlight the common issue where pre-trained models are often frozen to maintain stability, which limits their ability to adapt to new tasks. Their approach involves refining the pre-trained model before learning new tasks, allowing for better alignment of knowledge and reducing the risk of forgetting previous information. The results show that this method enhances the performance of CL systems, making it a promising solution for integrating pre-trained models in continual learning scenarios.'}, 'zh': {'title': '提升持续学习的可塑性与稳定性', 'desc': '这篇论文提出了一种新的框架，称为在核心持续学习过程之前调整预训练模型（ACL）。该方法旨在提高神经网络的可塑性，同时保持其稳定性，以便在增量学习中更好地适应新知识。通过在学习每个新任务之前对预训练模型进行适应性调整，ACL能够有效地对齐嵌入与原始类别原型，从而减少灾难性遗忘。实验结果表明，ACL在多个基准测试中显著提升了持续学习的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.21541', 'title': 'DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.21541', 'abstract': 'DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.', 'score': 5, 'issue_id': 4133, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'cda6015909393ad0', 'authors': ['Zitong Wang', 'Hang Zhao', 'Qianyu Zhou', 'Xuequan Lu', 'Xiangtai Li', 'Yiren Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.21541.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Умное разделение изображений на слои с помощью ИИ', 'desc': 'DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблемы разделения полупрозрачных и прозрачных наложений, с которыми не справлялись предыдущие методы. Авторы создали датасет AlphaBlend для обучения модели работе с различными типами прозрачности. DiffDecompose использует условное генерирование и позиционное кодирование слоев для точного восстановления составляющих изображения.'}, 'en': {'title': 'Revolutionizing Image Layer Decomposition with DiffDecompose', 'desc': "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."}, 'zh': {'title': '透明层分解的新突破：DiffDecompose', 'desc': 'DiffDecompose 是一个基于扩散 Transformer 的框架，能够有效地将图像分解为组成层，并使用语义提示来解决透明层分解中的挑战。该方法针对半透明和透明图层的非线性遮挡问题，提出了一种新的任务：逐层分解 alpha 合成图像。为了解决层模糊、泛化能力和数据稀缺的问题，研究者们首次引入了 AlphaBlend 数据集，支持多种实际应用场景。DiffDecompose 通过上下文分解的方法，能够在没有逐层监督的情况下预测一个或多个层，展示了其在图像分解任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03448', 'title': 'RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions', 'url': 'https://huggingface.co/papers/2506.03448', 'abstract': 'RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.', 'score': 4, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '717f877ff02ce882', 'authors': ['Bimsara Pathiraja', 'Maitreya Patel', 'Shivam Singh', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03448.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#cv', '#optimization', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'RefEdit: Прорыв в редактировании сложных изображений с помощью ИИ', 'desc': 'RefEdit - это модель редактирования изображений на основе инструкций, обученная на синтетических данных. Она превосходит базовые модели в задачах редактирования сложных сцен и работы с референсными выражениями. Авторы представили новый бенчмарк RefEdit-Bench для оценки таких моделей. RefEdit, обученная всего на 20 000 примерах, превзошла модели, обученные на миллионах образцов.'}, 'en': {'title': 'Revolutionizing Image Editing with Instruction-Based Learning', 'desc': 'RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios.'}, 'zh': {'title': 'RefEdit：复杂场景编辑的新突破', 'desc': 'RefEdit是一种基于指令的编辑模型，专门针对复杂场景中的编辑任务进行训练。与传统方法相比，RefEdit在处理多个实体的复杂场景时表现更为出色。我们还引入了RefEdit-Bench，这是一个基于RefCOCO的真实世界基准，用于量化现有方法的不足。通过使用合成数据生成管道，RefEdit在仅使用20,000个编辑三元组的情况下，超越了基于Flux/SD3模型的基线，展示了其在指代表达任务和传统基准上的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02945', 'title': 'Quantitative LLM Judges', 'url': 'https://huggingface.co/papers/2506.02945', 'abstract': "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.", 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'de4ea9c8e4abb76a', 'authors': ['Aishwarya Sahoo', 'Jeevana Kruthi Karnuthala', 'Tushar Parmanand Budhwani', 'Pranchal Agarwal', 'Sankaran Vaidyanathan', 'Alexa Siu', 'Franck Dernoncourt', 'Jennifer Healey', 'Nedim Lipka', 'Ryan Rossi', 'Uttaran Bhattacharya', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.02945.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rlhf', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии', 'desc': 'Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой модели. Авторы предлагают количественных LLM-судей, которые согласуют оценки существующих судей с человеческими оценками в заданной области с помощью регрессионных моделей. Представлены четыре количественных судьи для различных типов абсолютной и относительной обратной связи. Эксперименты показывают, что количественные судьи могут эффективно улучшить предсказательную силу существующих судей через постобработку.'}, 'en': {'title': 'Enhancing LLM Evaluation with Quantitative Judges', 'desc': "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."}, 'zh': {'title': '利用LLM提升评估效率的创新框架', 'desc': '本文提出了一种名为LLM-as-a-judge的框架，利用大型语言模型（LLM）自动评估另一个LLM的输出。我们引入了定量LLM评估者，通过回归模型将现有评估者的评分与人类评分对齐。该模型通过使用评估者的文本评价和评分来提高原始评估者的评分。我们的框架在计算效率上优于监督微调，并且在人工反馈有限的情况下，统计效率更高，适用于大多数应用场景。'}}}, {'id': 'https://huggingface.co/papers/2506.02294', 'title': 'Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation', 'url': 'https://huggingface.co/papers/2506.02294', 'abstract': 'A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '740d99ccc158d514', 'authors': ['Niclas Popp', 'Kevin Alexander Laube', 'Matthias Hein', 'Lukas Schott'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.02294.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение устойчивости моделей через генерацию сложных примеров', 'desc': 'Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения робастности в процессе дистилляции знаний. Метод генерирует сложные образцы, максимизируя разногласие между учителем и учеником, что помогает преодолеть проблему ковариационного сдвига. Эксперименты показывают значительное улучшение точности на наихудших группах и средней точности по группам на датасетах CelebA и SpuCo Birds. Подход превосходит современные методы аугментации данных на основе диффузии.'}, 'en': {'title': 'Boosting Student Robustness with Diffusion Data Augmentation', 'desc': 'This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods.'}, 'zh': {'title': '基于扩散的数据增强提升知识蒸馏鲁棒性', 'desc': '本文提出了一种基于扩散的数据增强策略，以提高知识蒸馏中的鲁棒性。该方法通过生成具有挑战性的样本，增强了学生网络对虚假特征的抵抗力。实验结果表明，在CelebA和SpuCo Birds数据集上，该策略显著提高了最差组和平均组的准确率。通过最大化教师和学生之间的分歧，本文有效地解决了知识蒸馏中的协变量偏移问题。'}}}, {'id': 'https://huggingface.co/papers/2506.00482', 'title': 'BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.00482', 'abstract': 'BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.', 'score': 4, 'issue_id': 4137, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0f7e970118d80f26', 'authors': ['Eunsu Kim', 'Haneul Yoo', 'Guijin Son', 'Hitesh Patel', 'Amit Agarwal', 'Alice Oh'], 'affiliations': ['KAIST', 'OnelineAI', 'Oracle', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00482.jpg', 'data': {'categories': ['#optimization', '#dataset', '#survey', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'BenchHub: Универсальный инструмент для оценки языковых моделей', 'desc': 'BenchHub - это динамическое хранилище бенчмарков, которое агрегирует и классифицирует наборы данных для больших языковых моделей (LLM). Оно включает в себя 303 тысячи вопросов из 38 бенчмарков различных предметных областей. BenchHub позволяет проводить гибкую и настраиваемую оценку моделей, адаптированную под конкретные домены или сценарии использования. Эксперименты показали, что производительность моделей значительно варьируется в зависимости от предметной области, подчеркивая важность домен-ориентированного бенчмаркинга.'}, 'en': {'title': 'BenchHub: Streamlining Domain-Specific Evaluations for LLMs', 'desc': 'BenchHub is a repository designed to organize and classify datasets specifically for evaluating large language models (LLMs). It addresses the challenge of scattered and hard-to-manage datasets, which complicate domain-specific evaluations. By aggregating 303K questions across 38 benchmarks, BenchHub allows for flexible and customizable assessments tailored to various domains. The paper highlights the importance of domain-aware benchmarking, showing that model performance can vary significantly based on the specific dataset used.'}, 'zh': {'title': 'BenchHub：提升语言模型评估的动态基准库', 'desc': 'BenchHub是一个动态基准库，专门用于聚合和分类大型语言模型的数据集，旨在促进特定领域的评估并改善模型比较。随着大型语言模型的不断进步，更新和组织良好的基准变得越来越重要。BenchHub集成了来自38个基准的303K问题，支持持续更新和可扩展的数据管理，允许根据不同领域或用例进行灵活的评估。通过对不同语言模型的广泛实验，我们展示了模型性能在特定领域子集之间的显著差异，强调了领域感知基准的重要性。'}}}, {'id': 'https://huggingface.co/papers/2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'url': 'https://huggingface.co/papers/2505.23807', 'abstract': 'A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.', 'score': 4, 'issue_id': 4133, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a817afc0cdd35d8d', 'authors': ['Yuli Chen', 'Bo Cheng', 'Jiale Han', 'Yingying Zhang', 'Yingting Li', 'Shuhao Zhang'], 'affiliations': ['Hong Kong University of Science and Technology, Hong Kong, China', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23807.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка слоев для эффективных языковых моделей', 'desc': 'Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбинируя информацию о весах модели и активациях. Это позволяет сохранить производительность модели при высоком уровне разреженности. Эксперименты показали, что DLP превосходит существующие методы обрезки для различных языковых моделей.'}, 'en': {'title': 'Dynamic Layerwise Pruning: Smart Sparsity for Language Models', 'desc': 'This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques.'}, 'zh': {'title': '动态剪枝，智能保持性能！', 'desc': '动态层级剪枝方法通过结合模型权重和激活信息，自适应地确定每一层的重要性，从而在高稀疏性下保持大型语言模型的性能。传统的剪枝技术通常采用均匀层级剪枝策略，这可能导致在高稀疏性水平下性能显著下降。动态层级剪枝（DLP）方法克服了这一限制，能够根据输入激活信息动态调整剪枝率。实验结果表明，DLP在多个大型语言模型中有效地保持了高稀疏性下的模型性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04133', 'title': 'TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2506.04133', 'abstract': 'A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.', 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a0a258935ed39508', 'authors': ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis'], 'affiliations': ['Cornell University, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2506.04133.jpg', 'data': {'categories': ['#training', '#architecture', '#survey', '#agents', '#multimodal', '#security', '#alignment', '#benchmark', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Безопасность и доверие в эпоху агентного ИИ', 'desc': 'Статья представляет структурированный анализ управления доверием, рисками и безопасностью (TRiSM) в контексте агентных мультиагентных систем на основе больших языковых моделей (LLM). Рассматриваются четыре основных аспекта: управление, объяснимость, ModelOps и конфиденциальность/безопасность. Авторы идентифицируют уникальные векторы угроз и представляют комплексную таксономию рисков для приложений агентного ИИ. Статья также исследует механизмы построения доверия, методы обеспечения прозрачности и надзора, а также современные стратегии объяснимости в распределенных системах агентов LLM.'}, 'en': {'title': 'Navigating Trust and Security in Agentic AI Systems', 'desc': 'This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment.'}, 'zh': {'title': '构建安全透明的代理人工智能系统', 'desc': '本文回顾了基于大型语言模型（LLM）的代理多智能体系统中的信任、风险和安全管理（TRiSM）。我们分析了代理人工智能的概念基础及其与传统人工智能代理的架构差异，并探讨了支持可扩展自主性的系统设计。文章详细阐述了TRiSM的四个支柱：治理、可解释性、模型操作和隐私/安全，并为代理LLM提供了具体的背景。最后，提出了负责任的代理人工智能的路线图，建议研究方向以确保新兴多智能体系统的安全、透明和负责任的部署。'}}}, {'id': 'https://huggingface.co/papers/2506.03837', 'title': 'HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature\n  Superconductors for AI-Driven Critical Temperature Prediction', 'url': 'https://huggingface.co/papers/2506.03837', 'abstract': 'HTSC-2025, a benchmark dataset for high-temperature superconducting materials, is presented to facilitate AI-based discovery in this field.  \t\t\t\t\tAI-generated summary \t\t\t\t The discovery of high-temperature superconducting materials holds great significance for human industry and daily life. In recent years, research on predicting superconducting transition temperatures using artificial intelligence~(AI) has gained popularity, with most of these tools claiming to achieve remarkable accuracy. However, the lack of widely accepted benchmark datasets in this field has severely hindered fair comparisons between different AI algorithms and impeded further advancement of these methods. In this work, we present the HTSC-2025, an ambient-pressure high-temperature superconducting benchmark dataset. This comprehensive compilation encompasses theoretically predicted superconducting materials discovered by theoretical physicists from 2023 to 2025 based on BCS superconductivity theory, including the renowned X_2YH_6 system, perovskite MXH_3 system, M_3XH_8 system, cage-like BCN-doped metal atomic systems derived from LaH_{10} structural evolution, and two-dimensional honeycomb-structured systems evolving from MgB_2. The HTSC-2025 benchmark has been open-sourced at https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This benchmark holds significant importance for accelerating the discovery of superconducting materials using AI-based methods.', 'score': 3, 'issue_id': 4144, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e8db15217cfd8461', 'authors': ['Xiao-Qi Han', 'Ze-Feng Gao', 'Xin-De Wang', 'Zhenfeng Ouyang', 'Peng-Jie Guo', 'Zhong-Yi Lu'], 'affiliations': ['Hefei National Laboratory, Hefei 230088, China', 'Key Laboratory of Quantum State Construction and Manipulation (Ministry of Education), Renmin University of China, Beijing 100872, China', 'School of Physics and Beijing Key Laboratory of Opto-electronic Functional Materials & Micro-nano Devices, Renmin University of China, Beijing 100872, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03837.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#science'], 'emoji': '⚡', 'ru': {'title': 'HTSC-2025: эталонный набор данных для прорыва в сверхпроводимости', 'desc': 'Представлен набор данных HTSC-2025 для исследования высокотемпературных сверхпроводящих материалов с помощью методов искусственного интеллекта. Этот набор включает теоретически предсказанные сверхпроводящие материалы, открытые физиками-теоретиками с 2023 по 2025 год на основе теории сверхпроводимости БКШ. HTSC-2025 охватывает различные системы, включая X_2YH_6, перовскитную MXH_3, M_3XH_8 и другие. Данный бенчмарк имеет большое значение для ускорения открытия сверхпроводящих материалов с использованием методов, основанных на ИИ.'}, 'en': {'title': 'Accelerating Superconductor Discovery with HTSC-2025', 'desc': 'The paper introduces HTSC-2025, a new benchmark dataset designed for high-temperature superconducting materials to enhance AI-driven research in this area. It addresses the current challenge of insufficient benchmark datasets, which limits the ability to compare different AI algorithms effectively. The dataset includes a variety of theoretically predicted superconductors, derived from advanced theories and recent discoveries. By providing this resource, the authors aim to facilitate faster and more accurate discoveries of superconducting materials using artificial intelligence techniques.'}, 'zh': {'title': 'HTSC-2025：加速高温超导材料发现的基准数据集', 'desc': 'HTSC-2025是一个用于高温超导材料的基准数据集，旨在促进基于人工智能的发现。该数据集包含了2023至2025年间理论物理学家预测的超导材料，基于BCS超导理论。通过提供一个统一的基准，HTSC-2025可以帮助不同的AI算法进行公平比较，从而推动该领域的进一步发展。该数据集已开源，并将持续更新，以加速超导材料的发现。'}}}, {'id': 'https://huggingface.co/papers/2506.03566', 'title': 'POSS: Position Specialist Generates Better Draft for Speculative\n  Decoding', 'url': 'https://huggingface.co/papers/2506.03566', 'abstract': 'Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.', 'score': 3, 'issue_id': 4149, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '16e2306b3d98ed8c', 'authors': ['Langlin Huang', 'Chengsong Huang', 'Jixuan Leng', 'Di Huang', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2506.03566.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Специалисты по позициям: точность предсказания токенов на новом уровне', 'desc': 'Статья представляет новый метод под названием Position Specialists (PosS) для улучшения процесса вывода в больших языковых моделях (LLM). PosS использует несколько специализированных слоев для генерации токенов на определенных позициях, что повышает точность предсказания и уровень принятия токенов. Этот подход особенно эффективен для токенов на более поздних позициях, где обычно наблюдается накопление ошибок. Эксперименты на моделях Llama показали, что PosS превосходит базовые методы по средней длине принятия и коэффициенту ускорения.'}, 'en': {'title': 'Enhancing Token Prediction with Position Specialists', 'desc': 'This paper introduces Position Specialists (PosS), a method that enhances the performance of Large Language Models (LLMs) during inference. By utilizing position-specialized draft layers, PosS improves the accuracy of token predictions, particularly at later positions where traditional methods struggle due to error accumulation. The approach allows each specialist to focus on specific positions, leading to higher acceptance rates for generated tokens. Experimental results show that PosS outperforms existing methods in both acceptance length and inference speed across multiple datasets.'}, 'zh': {'title': '位置专家提升语言模型推理效率', 'desc': '本文提出了一种名为位置专家（PosS）的新方法，旨在通过使用位置专用的草稿层来提高大型语言模型（LLM）的推理性能。位置专家能够在特定位置生成更准确的标记，从而提高标记的接受率，尤其是在后期位置。通过专注于处理草稿模型特征的偏差，每个专家可以有效减少错误累积带来的影响。实验结果表明，PosS在多个数据集上显著优于基线模型，提升了平均接受长度和加速比。'}}}, {'id': 'https://huggingface.co/papers/2506.03525', 'title': 'Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.03525', 'abstract': 'Video-SKoT framework improves domain-adaptive video reasoning by constructing skill-aware Chain-of-Thought supervisions and specialized expert modules.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Chain-of-Thought (CoT) reasoning have improved complex video understanding, but existing methods often struggle to adapt to domain-specific skills (e.g., event detection, spatial relation understanding, emotion understanding) over various video content. To address this, we propose Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs and leverages skill-aware CoT supervisions for domain-adaptive video reasoning. First, we construct skill-based CoT annotations: we extract domain-relevant reasoning skills from training questions, cluster them into a shared skill taxonomy, and create detailed multi-step CoT rationale tailored to each video-question pair for training. Second, we introduce a skill-specific expert learning framework. Each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision. We demonstrate the effectiveness of the proposed approach on three video understanding benchmarks, where Video-SKoT consistently outperforms strong baselines. We also provide in-depth analyses on comparing different CoT annotation pipelines and learned skills over multiple video domains.', 'score': 3, 'issue_id': 4145, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '799fe0c792fb72aa', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.03525.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умное видео: рассуждаем как эксперты в разных областях', 'desc': 'Статья представляет новый фреймворк Video-SKoT для улучшения адаптивного видеоанализа в различных доменах. Он использует автоматически созданные цепочки рассуждений (Chain-of-Thought), учитывающие специфические навыки для каждой области. Фреймворк включает создание аннотаций на основе навыков и обучение специализированных экспертных модулей. Эксперименты показывают превосходство Video-SKoT над существующими методами на трех бенчмарках понимания видео.'}, 'en': {'title': 'Enhancing Video Reasoning with Skill-Aware Chain-of-Thought', 'desc': 'The Video-SKoT framework enhances video reasoning by focusing on specific skills needed for understanding different video content. It creates skill-aware Chain-of-Thought (CoT) supervisions that guide the model in reasoning about events, spatial relations, and emotions in videos. By clustering relevant reasoning skills and developing specialized expert modules, the framework allows for more effective domain adaptation. The results show that Video-SKoT outperforms existing methods on various benchmarks, demonstrating its ability to improve video understanding through targeted skill training.'}, 'zh': {'title': '技能感知的视频推理新框架', 'desc': '本文提出了一种名为Video-SKoT的视频推理框架，旨在通过构建技能感知的链式思维（CoT）监督和专门的专家模块来改善领域自适应视频推理。该框架首先从训练问题中提取与领域相关的推理技能，并将其聚类成共享的技能分类法，进而为每个视频-问题对创建详细的多步骤CoT推理。其次，框架引入了一个技能特定的专家学习机制，每个专家模块专注于一部分推理技能，并使用收集到的CoT监督进行轻量级适配器训练。实验结果表明，Video-SKoT在多个视频理解基准测试中表现优异，超越了强基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02863', 'title': 'CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech', 'url': 'https://huggingface.co/papers/2506.02863', 'abstract': 'CapSpeech introduces a large benchmark dataset for various captioned text-to-speech tasks, facilitating advancements in style, accent, emotion, and chat-agent synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.', 'score': 3, 'issue_id': 4151, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'bd41abf94e2ec641', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'CapSpeech: Новый стандарт для синтеза речи с богатыми аннотациями', 'desc': 'CapSpeech представляет собой новый масштабный набор данных для различных задач синтеза речи с аннотациями. Этот датасет включает более 10 миллионов машинно-аннотированных и 0,36 миллиона человеко-аннотированных пар аудио-текст. CapSpeech предназначен для задач синтеза речи с учетом стиля, акцента, эмоций и для чат-агентов. Авторы провели эксперименты с использованием авторегрессионных и неавторегрессионных моделей машинного обучения на этом датасете.'}, 'en': {'title': 'CapSpeech: Advancing Text-to-Speech with Comprehensive Datasets', 'desc': 'CapSpeech is a new benchmark dataset aimed at improving style-captioned text-to-speech synthesis (CapTTS) by providing a comprehensive resource for various tasks. It includes over 10 million machine-annotated and nearly 0.36 million human-annotated audio-caption pairs, covering aspects like style, accent, emotion, and chat-agent synthesis. The dataset supports multiple CapTTS-related tasks, such as CapTTS with sound events and emotion-captioned TTS, facilitating advancements in real-world applications. Experiments conducted on CapSpeech show promising results in generating high-quality, intelligible speech across diverse speaking styles.'}, 'zh': {'title': 'CapSpeech：推动文本到语音合成的新时代', 'desc': 'CapSpeech是一个大型基准数据集，专为各种带字幕的文本到语音任务而设计，旨在推动风格、口音、情感和聊天代理合成的进步。该数据集包含超过1000万个机器标注的音频-字幕对和近36万个人工标注的音频-字幕对，支持多种CapTTS相关任务。我们还引入了由专业配音演员和经验丰富的音频工程师收集和录制的两个新数据集，专门用于聊天代理和带声音事件的CapTTS任务。通过对CapSpeech进行的实验，展示了在多种说话风格下的高保真和高可懂性语音合成。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.01344', 'title': 'Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents', 'url': 'https://huggingface.co/papers/2506.01344', 'abstract': "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.", 'score': 3, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '788495117e4bf1d7', 'authors': ['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Vivek Gupta', 'Dinesh Manocha'], 'affiliations': ['Adobe', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.01344.jpg', 'data': {'categories': ['#graphs', '#cv', '#reasoning', '#agents', '#hallucinations', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '🔀', 'ru': {'title': 'Точная интерпретация блок-схем с помощью нейросимволического агента', 'desc': 'Статья представляет задачу точной атрибуции блок-схем и агента FlowPathAgent для ее решения. Авторы разработали нейросимволический подход, который сегментирует блок-схему, преобразует ее в структурированный символьный граф и использует агентный метод для генерации путей атрибуции. Также представлен новый бенчмарк FlowExplainBench для оценки атрибуций блок-схем. Результаты показывают, что FlowPathAgent снижает визуальные галлюцинации в ответах языковых моделей на вопросы по блок-схемам, превосходя базовые методы на 10-14%.'}, 'en': {'title': 'Enhancing Flowchart Interpretation with Fine-grained Attribution', 'desc': 'This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark.'}, 'zh': {'title': '提升流程图解析的可靠性与可解释性', 'desc': '本论文介绍了一种新的任务，称为细粒度流程图归因，旨在提高大型语言模型（LLM）在处理流程图时的可靠性和可解释性。我们提出了FlowPathAgent，这是一种神经符号代理，通过图形推理进行细粒度的后期归因。该代理首先对流程图进行分割，然后将其转换为结构化的符号图，并动态与图进行交互，以生成归因路径。实验结果表明，FlowPathAgent在流程图问答中减少了视觉幻觉，相较于强基线提高了10-14%的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.04034', 'title': 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.04034', 'abstract': 'Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9d3dcbdd5158f101', 'authors': ['Qing Jiang', 'Xingyu Chen', 'Zhaoyang Zeng', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Peking University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04034.jpg', 'data': {'categories': ['#cv', '#rl', '#training', '#reasoning', '#hallucinations', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое объектное реферирование через пошаговые рассуждения', 'desc': 'Статья представляет новый подход к задаче объектного реферирования в компьютерном зрении, названный Rex-Thinker. Модель использует пошаговое рассуждение для оценки соответствия объектов заданному описанию, что повышает интерпретируемость и надежность предсказаний. Авторы создали датасет HumanRef-CoT для обучения модели структурированным рассуждениям. Rex-Thinker обучается в два этапа: контролируемая тонкая настройка и обучение с подкреплением, что улучшает точность и обобщающую способность модели.'}, 'en': {'title': 'Rex-Thinker: Grounded Object Referring with Explainable Reasoning', 'desc': 'This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios.'}, 'zh': {'title': 'Rex-Thinker：可解释的物体指代模型', 'desc': '本文提出了一种新的物体指代模型Rex-Thinker，旨在通过明确的链式推理任务来检测与自然语言描述匹配的图像中的所有物体。该模型强调可验证性和可信性，确保其预测能够解释并与视觉证据相连。Rex-Thinker通过逐步推理候选物体实例，判断其是否符合给定的描述，从而做出最终预测。实验结果表明，该方法在精确度和可解释性方面优于传统基线，并在拒绝虚假输出和跨领域泛化能力上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.03951', 'title': 'Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective', 'url': 'https://huggingface.co/papers/2506.03951', 'abstract': 'A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.  \t\t\t\t\tAI-generated summary \t\t\t\t The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters.', 'score': 2, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '19d2cecb53fd6998', 'authors': ['Aojun Lu', 'Hangjie Yuan', 'Tao Feng', 'Yanan Sun'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'College of Computer Science, Sichuan University, Chengdu, China', 'Department of Computer Science and Technology, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03951.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Двойная архитектура для эффективного непрерывного обучения', 'desc': 'Статья представляет новую архитектуру Dual-Arch для непрерывного обучения, которая решает дилемму стабильности-пластичности на архитектурном уровне. Авторы обнаружили, что при равном количестве параметров более глубокие сети обладают лучшей пластичностью, а более широкие - лучшей стабильностью. Dual-Arch использует две отдельные сети: одну для пластичности, другую для стабильности, каждая со специализированной архитектурой. Эксперименты показали, что Dual-Arch улучшает производительность существующих методов непрерывного обучения, при этом используя до 87% меньше параметров.'}, 'en': {'title': 'Dual-Arch: Balancing Stability and Plasticity in Continual Learning', 'desc': 'The paper presents a new framework called Dual-Arch that improves Continual Learning (CL) by tackling the stability-plasticity dilemma through architectural innovations. It highlights that deeper networks are better at learning new information (plasticity), while wider networks excel at retaining old knowledge (stability). By utilizing two specialized networks—one focused on plasticity and the other on stability—Dual-Arch effectively balances these competing needs. Experimental results show that this approach not only enhances the performance of existing CL methods but also reduces the model size by up to 87%.'}, 'zh': {'title': '双网络架构，平衡学习稳定性与可塑性', 'desc': '本文提出了一种新框架Dual-Arch，旨在通过在架构层面解决稳定性与可塑性之间的矛盾来增强持续学习。持续学习的目标是使神经网络能够逐步学习和适应新知识，同时保持对旧知识的记忆。研究表明，在相同参数约束下，深层网络具有更好的可塑性，而宽层网络则表现出更高的稳定性。Dual-Arch框架结合了两个独立网络的优势，一个专注于可塑性，另一个专注于稳定性，从而提高了现有持续学习方法的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03822', 'title': 'CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents', 'url': 'https://huggingface.co/papers/2506.03822', 'abstract': "Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's URL, such as a digital object identifier, CRAWLDoc retrieves the landing page and all linked web resources, including PDFs, ORCID profiles, and supplementary materials. It embeds these resources, along with anchor texts and the URLs, into a unified representation. For evaluating CRAWLDoc, we have created a new, manually labeled dataset of 600 publications from six top publishers in computer science. Our method CRAWLDoc demonstrates a robust and layout-independent ranking of relevant documents across publishers and data formats. It lays the foundation for improved metadata extraction from web documents with various layouts and formats. Our source code and dataset can be accessed at https://github.com/FKarl/CRAWLDoc.", 'score': 2, 'issue_id': 4147, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'dea42a2dce3ff33b', 'authors': ['Fabian Karl', 'Ansgar Scherp'], 'affiliations': ['Universität Ulm, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2506.03822.jpg', 'data': {'categories': ['#dataset', '#data'], 'emoji': '🕸️', 'ru': {'title': 'CRAWLDoc: Умное извлечение метаданных из веб-документов', 'desc': 'CRAWLDoc - это новый метод контекстного ранжирования связанных веб-документов для извлечения метаданных публикаций. Он начинает с URL публикации, извлекает связанные ресурсы и встраивает их в единое представление. Метод был протестирован на новом наборе данных из 600 публикаций от шести ведущих издателей в области компьютерных наук. CRAWLDoc демонстрирует надежное ранжирование релевантных документов независимо от макета и формата данных.'}, 'en': {'title': 'CRAWLDoc: Contextual Ranking for Enhanced Metadata Extraction', 'desc': "This paper presents CRAWLDoc, a novel approach for ranking web documents related to academic publications. It addresses the challenge of varying web layouts and formats by retrieving a publication's landing page and all associated resources, such as PDFs and profiles. CRAWLDoc creates a unified representation of these resources, enhancing the contextual understanding of linked documents. The method has been evaluated using a new dataset of 600 publications, showing its effectiveness in providing layout-independent document rankings for better metadata extraction."}, 'zh': {'title': 'CRAWLDoc：提升元数据提取的创新方法', 'desc': '本论文介绍了一种新的方法CRAWLDoc，用于从不同的网络来源中提取准确的元数据。该方法从出版物的URL开始，检索相关的网页和链接资源，并将其嵌入到一个统一的表示中。通过创建一个包含600个出版物的新手动标注数据集，评估了CRAWLDoc的有效性。CRAWLDoc展示了在不同出版商和数据格式中，能够稳健且独立于布局地对相关文档进行排名，为改进元数据提取奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.03614', 'title': 'VLMs Can Aggregate Scattered Training Patches', 'url': 'https://huggingface.co/papers/2506.03614', 'abstract': 'VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions "safe," VLMs may later describe, the full image or a text reference to the scene, as "safe." We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe\'\' or ``unsafe\'\', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching.', 'score': 2, 'issue_id': 4137, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '180b48fc19d50b80', 'authors': ['Zhanhui Zhou', 'Lingjie Chen', 'Chao Yang', 'Chaochao Lu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03614.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#multimodal', '#cv', '#benchmark', '#security', '#ethics'], 'emoji': '🧩', 'ru': {'title': 'Визуальное сшивание: скрытая угроза в моделях компьютерного зрения', 'desc': "Это исследование раскрывает феномен 'визуального сшивания' в моделях компьютерного зрения и обработки естественного языка (VLM). Авторы демонстрируют, как VLM способны интегрировать фрагментированную визуальную информацию, что позволяет обходить модерацию данных и реконструировать нежелательный контент во время вывода. Эксперименты показывают, что модели могут собирать полные изображения из небольших, безобидных на вид фрагментов, разбросанных по множеству обучающих образцов. Это открытие подчеркивает серьезные риски безопасности при использовании VLM и необходимость разработки новых методов защиты."}, 'en': {'title': 'Visual Stitching: A Hidden Risk in Vision-Language Models', 'desc': 'This paper discusses a vulnerability in vision-language models (VLMs) known as visual stitching, which allows these models to reconstruct harmful content from fragmented visual information. The authors show that when dangerous images are divided into small patches and mixed with benign data, VLMs can still learn to piece them together during training. This leads to a situation where the models can generate harmful outputs by associating safe descriptions with dangerous images. The study highlights the risks of data moderation being bypassed and emphasizes the need for improved safety measures in VLMs.'}, 'zh': {'title': '视觉拼接：VLMs的安全隐患', 'desc': '本论文探讨了视觉语言模型（VLMs）中的视觉拼接能力，这种能力使得模型能够整合分散的视觉信息。研究表明，当有害图像被分割成小的、看似无害的片段时，数据的审查可以被轻易绕过。VLMs在训练过程中可能会学习将这些片段拼接在一起，从而在推理时生成有害的响应。我们通过实验展示了这一现象，并模拟了对抗性数据中毒的场景，揭示了VLMs在安全性方面的潜在风险。'}}}, {'id': 'https://huggingface.co/papers/2506.02515', 'title': 'FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.02515', 'abstract': 'A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.', 'score': 2, 'issue_id': 4142, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '0b49cede5613cc2d', 'authors': ['Zhuohan Xie', 'Dhruv Sahnan', 'Debopriyo Banerjee', 'Georgi Georgiev', 'Rushil Thareja', 'Hachem Madmoun', 'Jinyan Su', 'Aaryamonvikram Singh', 'Yuxia Wang', 'Rui Xing', 'Fajri Koto', 'Haonan Li', 'Ivan Koychev', 'Tanmoy Chakraborty', 'Salem Lahlou', 'Veselin Stoyanov', 'Preslav Nakov'], 'affiliations': ['Cornell University, USA', 'FMI, Sofia University, Bulgaria', 'IIT Delhi, India', 'MBZUAI, UAE', 'Quantsquare, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.02515.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#dataset', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'FinChain: новый рубеж в оценке финансовых рассуждений ИИ', 'desc': 'FinChain - это новый бенчмарк для оценки многоэтапных символьных рассуждений в финансовых задачах, фокусирующийся на промежуточных шагах рассуждений. Он включает 54 темы из 12 финансовых областей, с пятью параметризованными шаблонами для каждой темы. Авторы также представили метрику ChainEval для автоматической оценки как конечных ответов, так и промежуточных рассуждений. Тестирование 30 различных языковых моделей на этом датасете показало, что даже современные модели имеют значительный потенциал для улучшения в области многоэтапных финансовых рассуждений.'}, 'en': {'title': 'FinChain: Advancing Financial Reasoning with Intermediate Steps', 'desc': 'The paper introduces FinChain, a new benchmark designed to evaluate multi-step symbolic reasoning specifically in financial tasks. Unlike existing datasets that only focus on final answers, FinChain emphasizes the importance of intermediate reasoning steps through its novel metric, ChainEval. This benchmark covers a wide range of financial topics and provides parameterized templates to assess varying levels of reasoning complexity. The findings reveal that even advanced language models struggle with multi-step reasoning in finance, highlighting the need for improved capabilities in this area.'}, 'zh': {'title': 'FinChain：金融推理的新基准', 'desc': 'FinChain是一个新的基准，旨在评估金融任务中的多步骤符号推理，特别关注中间推理步骤。它引入了ChainEval作为评估最终答案和推理过程的新指标。现有的数据集如FinQA和ConvFinQA仅监督最终的数值答案，而不评估中间推理步骤。FinChain覆盖12个金融领域的54个主题，为每个主题提供五个不同推理复杂度和领域专业知识要求的模板。'}}}, {'id': 'https://huggingface.co/papers/2505.23564', 'title': 'Segment Policy Optimization: Effective Segment-Level Credit Assignment\n  in RL for Large Language Models', 'url': 'https://huggingface.co/papers/2505.23564', 'abstract': 'The Segment Policy Optimization (SPO) framework improves large language model reasoning via reinforcement learning by offering intermediate granularity advantage estimation, balancing precision and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: Token-level methods (e.g., PPO) aim to provide the fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving 6-12 percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving 7-11 percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO.', 'score': 2, 'issue_id': 4146, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '25b36641cf394ee3', 'authors': ['Yiran Guo', 'Lijie Xu', 'Jie Liu', 'Dan Ye', 'Shuang Qiu'], 'affiliations': ['City University of Hong Kong', 'Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.23564.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'SPO: Оптимизация рассуждений языковых моделей через сегментированное обучение с подкреплением', 'desc': 'Статья представляет новый метод обучения с подкреплением для улучшения рассуждений больших языковых моделей - Segment Policy Optimization (SPO). SPO использует оценку преимуществ на уровне сегментов, что обеспечивает баланс между точностью и вычислительной эффективностью. Метод включает три ключевых компонента: гибкое разделение на сегменты, точную оценку преимуществ сегментов и оптимизацию политики с использованием преимуществ сегментов. SPO показывает значительные улучшения точности по сравнению с существующими методами на задачах рассуждений и решения математических задач.'}, 'en': {'title': 'Segment Policy Optimization: Balancing Precision and Efficiency in RL for Language Models', 'desc': 'The Segment Policy Optimization (SPO) framework enhances the reasoning abilities of large language models through reinforcement learning by introducing an intermediate granularity for advantage estimation. This approach addresses the limitations of both token-level and trajectory-level methods, providing a more accurate credit assignment while maintaining computational efficiency. SPO incorporates flexible segment partitioning, precise segment advantage estimation, and a novel policy optimization strategy that utilizes segment advantages. The framework has been successfully applied to improve performance on tasks like GSM8K and MATH500, demonstrating significant accuracy gains over existing methods.'}, 'zh': {'title': '段策略优化：提升语言模型推理能力的新方法', 'desc': '本论文提出了一种新的强化学习框架，称为段策略优化（SPO），旨在提高大型语言模型的推理能力。SPO通过中间粒度的优势估计，克服了现有方法在精确性和计算效率之间的权衡问题。该框架包含灵活的段划分、准确的段优势估计和基于段优势的策略优化，能够在不依赖评论模型的情况下实现准确的优势估计。实验结果表明，SPO在多个任务上相较于传统方法有显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.04214', 'title': 'Sounding that Object: Interactive Object-Aware Image to Audio Generation', 'url': 'https://huggingface.co/papers/2506.04214', 'abstract': 'Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/', 'score': 1, 'issue_id': 4146, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1f6af5427e65f943', 'authors': ['Tingle Li', 'Baihe Huang', 'Xiaobin Zhuang', 'Dongya Jia', 'Jiawei Chen', 'Yuping Wang', 'Zhuo Chen', 'Gopala Anumanchipalli', 'Yuxuan Wang'], 'affiliations': ['University of California'], 'pdf_title_img': 'assets/pdf/title_img/2506.04214.jpg', 'data': {'categories': ['#audio', '#diffusion', '#multimodal'], 'emoji': '🔊', 'ru': {'title': 'Интерактивная генерация звука на основе объектов в изображениях', 'desc': 'Эта статья представляет модель генерации звука, основанную на выбранных пользователем визуальных объектах в изображениях. Модель использует объектно-ориентированное обучение в сочетании с условной латентной диффузионной моделью и мультимодальным вниманием для связывания областей изображения с соответствующими звуками. На этапе тестирования применяется сегментация изображений, позволяющая генерировать звуки на уровне отдельных объектов. Теоретически и экспериментально показано, что модель превосходит базовые подходы в точности соответствия между объектами и их звуками.'}, 'en': {'title': 'Interactive Sound Generation for Visual Objects', 'desc': 'This paper presents a novel model for generating sounds that correspond to specific visual objects in images, addressing the complexity of audio-visual scenes. The proposed interactive object-aware audio generation model uses a conditional latent diffusion approach, which learns to connect image regions with their respective sounds through a multi-modal attention mechanism. During testing, the model allows users to select objects in an image, generating sounds that are accurately aligned with those objects. The results demonstrate that this method outperforms existing models, providing a more coherent audio-visual experience.'}, 'zh': {'title': '交互式对象感知音频生成模型', 'desc': '本文提出了一种交互式对象感知音频生成模型，旨在为复杂的音视频场景生成准确的声音。该模型通过用户选择的视觉对象来引导声音生成，结合了对象中心学习和条件潜在扩散模型。我们的方法利用多模态注意力机制，将图像区域与相应的声音关联起来，并在测试时通过图像分割实现用户的交互式声音生成。实验结果表明，该模型在对象与声音的对齐方面优于基线模型，生成的音频与所选对象更为一致。'}}}, {'id': 'https://huggingface.co/papers/2506.03817', 'title': 'Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid', 'url': 'https://huggingface.co/papers/2506.03817', 'abstract': 'Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '24782a46b48333d1', 'authors': ['Julius Gonsior', 'Tim Rieß', 'Anja Reusch', 'Claudio Hartmann', 'Maik Thiele', 'Wolfgang Lehner'], 'affiliations': ['Hochschule fur Technik und Wirtschaft Dresden', 'Technion - Israeli Institute of Technology', 'Technische Universitat Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.03817.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты гиперпараметров в активном обучении', 'desc': 'Это исследование посвящено активному обучению (Active Learning, AL) в машинном обучении, которое помогает минимизировать усилия по разметке данных. Авторы провели масштабный эксперимент, изучив более 4,6 миллионов комбинаций гиперпараметров AL. Они проанализировали влияние каждого гиперпараметра на результаты и предложили рекомендации по настройке AL. Исследование направлено на повышение воспроизводимости и надежности экспериментов с AL в будущем.'}, 'en': {'title': 'Unlocking Active Learning: Simplifying Setup for Trustworthy Results', 'desc': 'This paper addresses the challenges of using Active Learning (AL) in supervised machine learning, particularly the complexities and trust issues that hinder its adoption. The authors compiled a vast hyperparameter grid with over 4.6 million combinations to analyze how different settings affect AL performance. They conducted the largest AL study to date, recording the results and examining the impact of each hyperparameter on the outcomes. The findings provide insights and recommendations for setting up reproducible AL experiments, aiming to enhance the reliability and effectiveness of AL in real-world applications.'}, 'zh': {'title': '优化主动学习，提升标注效率', 'desc': '标注数据是一个耗时且成本高昂的任务，但这是监督学习所必需的。主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标记样本来减少人工标注工作的方法，从而提高分类性能。尽管AL已经存在了几十年，但在实际应用中仍然很少被使用。本文研究了AL中超参数空间的复杂性，提出了一个包含460万种超参数组合的大型网格，并分析了每个超参数对实验结果的影响，以促进更可靠的AL研究。'}}}, {'id': 'https://huggingface.co/papers/2506.03538', 'title': 'Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting', 'url': 'https://huggingface.co/papers/2506.03538', 'abstract': 'A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released.', 'score': 1, 'issue_id': 4140, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'c9dadfe8cdfe5d4c', 'authors': ['Chengqi Li', 'Zhihao Shi', 'Yangdi Lu', 'Wenbo He', 'Xiangyu Xu'], 'affiliations': ['Department of Computing and Software McMaster University', 'Department of Electrical and Computer Engineering McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03538.jpg', 'data': {'categories': ['#training', '#3d', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Двойное зрение для точной 3D-реконструкции', 'desc': 'Предложен новый метод Asymmetric Dual 3DGS для улучшения 3D-реконструкции изображений. Он основан на обучении двух параллельных моделей 3D Gaussian Splatting с ограничением согласованности и дивергентным маскированием. Метод превосходит существующие подходы, подавляя артефакты и выделяя надежную геометрию сцены. Также представлен облегченный вариант Dynamic EMA Proxy для повышения эффективности обучения.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Asymmetric Dual Models', 'desc': 'The paper presents the Asymmetric Dual 3DGS framework, which enhances 3D reconstruction from images taken in varied conditions. It addresses the challenges of inconsistent lighting and distracting elements by training two models simultaneously with a focus on consistency and divergence. The framework uses a unique masking strategy to prevent the models from converging on the same errors, thus improving the quality of the reconstructions. Experimental results show that this approach is more efficient and effective than current methods, leading to better performance in real-world scenarios.'}, 'zh': {'title': '非对称双重3DGS框架：高效的3D重建新方法', 'desc': '本文提出了一种新颖的非对称双重3DGS框架，旨在提高3D重建的效果。该方法通过训练两个模型并施加一致性约束，来减少不一致的视觉伪影。我们引入了多线索自适应掩码和自监督软掩码，确保两个模型在训练过程中保持差异，从而降低共享错误模式。实验结果表明，该方法在处理真实世界数据集时，表现出更高的效率和更好的重建质量。'}}}, {'id': 'https://huggingface.co/papers/2506.02680', 'title': 'Solving Inverse Problems with FLAIR', 'url': 'https://huggingface.co/papers/2506.02680', 'abstract': 'FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based latent generative models such as Stable Diffusion 3 are able to generate images with remarkable quality, even enabling photorealistic text-to-image generation. Their impressive performance suggests that these models should also constitute powerful priors for inverse imaging problems, but that approach has not yet led to comparable fidelity. There are several key obstacles: (i) the encoding into a lower-dimensional latent space makes the underlying (forward) mapping non-linear; (ii) the data likelihood term is usually intractable; and (iii) learned generative models struggle to recover rare, atypical data modes during inference. We present FLAIR, a novel training free variational framework that leverages flow-based generative models as a prior for inverse problems. To that end, we introduce a variational objective for flow matching that is agnostic to the type of degradation, and combine it with deterministic trajectory adjustments to recover atypical modes. To enforce exact consistency with the observed data, we decouple the optimization of the data fidelity and regularization terms. Moreover, we introduce a time-dependent calibration scheme in which the strength of the regularization is modulated according to off-line accuracy estimates. Results on standard imaging benchmarks demonstrate that FLAIR consistently outperforms existing diffusion- and flow-based methods in terms of reconstruction quality and sample diversity.', 'score': 1, 'issue_id': 4143, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '82a645cb87d64a3f', 'authors': ['Julius Erbach', 'Dominik Narnhofer', 'Andreas Dombos', 'Bernt Schiele', 'Jan Eric Lenssen', 'Konrad Schindler'], 'affiliations': ['ETH Zürich', 'Max Planck Institute for Informatics, Saarland Informatics Campus'], 'pdf_title_img': 'assets/pdf/title_img/2506.02680.jpg', 'data': {'categories': ['#cv', '#benchmark', '#diffusion', '#data', '#training', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'FLAIR: Революция в решении обратных задач с помощью потоковых генеративных моделей', 'desc': 'FLAIR - это новая вариационная система, использующая генеративные модели на основе потоков для улучшения решений обратных задач. Она не требует дополнительного обучения и преодолевает ключевые препятствия, связанные с нелинейностью отображения и трудностями в восстановлении редких режимов данных. FLAIR вводит вариационную целевую функцию для сопоставления потоков, независимую от типа деградации, и комбинирует ее с детерминированными корректировками траектории. Система превосходит существующие методы на основе диффузии и потоков по качеству реконструкции и разнообразию выборки.'}, 'en': {'title': 'FLAIR: Enhancing Inverse Problems with Flow-Based Generative Models', 'desc': 'FLAIR is a new framework that improves solutions to inverse problems using flow-based generative models without requiring extensive training. It addresses challenges like non-linear mappings and intractable data likelihoods by introducing a variational objective that is flexible to different types of data degradation. The framework also includes techniques to recover rare data patterns and ensures consistency with observed data by separating optimization processes. Experimental results show that FLAIR achieves better image reconstruction quality and greater diversity in samples compared to existing methods.'}, 'zh': {'title': 'FLAIR：提升逆问题解决的新方法', 'desc': 'FLAIR是一种新颖的无训练变分框架，利用基于流的生成模型来增强逆问题的解决方案。该方法通过引入变分目标和确定性轨迹调整，克服了在低维潜在空间编码带来的非线性映射问题。FLAIR能够有效恢复稀有和非典型的数据模式，并确保与观测数据的一致性。实验结果表明，FLAIR在重建质量和样本多样性方面优于现有的扩散和流模型方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02153', 'title': 'Small Language Models are the Future of Agentic AI', 'url': 'https://huggingface.co/papers/2506.02153', 'abstract': 'Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation.   Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm.   Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '811261b62e0e242e', 'authors': ['Peter Belcak', 'Greg Heinrich', 'Shizhe Diao', 'Yonggan Fu', 'Xin Dong', 'Saurav Muralidharan', 'Yingyan Celine Lin', 'Pavlo Molchanov'], 'affiliations': ['Georgia Institute of Technology', 'NVIDIA Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.02153.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#small_models'], 'emoji': '🤖', 'ru': {'title': 'Малые языковые модели - будущее агентного ИИ', 'desc': 'Статья рассматривает потенциал малых языковых моделей (SLM) в контексте агентных систем искусственного интеллекта. Авторы утверждают, что SLM достаточно мощны, более подходящи и экономичны для многих задач в агентных системах по сравнению с большими языковыми моделями (LLM). Они предлагают алгоритм конвертации агентов с LLM на SLM и обсуждают возможные препятствия для внедрения SLM. Статья призывает к дискуссии об эффективном использовании ресурсов ИИ и снижении затрат на современные технологии искусственного интеллекта.'}, 'en': {'title': 'Small Language Models: The Future of Agentic AI', 'desc': 'This paper argues that small language models (SLMs) are more suitable and cost-effective for specialized tasks in agentic AI systems compared to large language models (LLMs). It highlights that SLMs possess sufficient capabilities for repetitive tasks, making them a better choice for many applications. The authors propose that heterogeneous systems, which use multiple models, are ideal for scenarios requiring general conversational abilities. They also present a conversion algorithm for transitioning from LLMs to SLMs and emphasize the economic benefits of adopting SLMs in the AI industry.'}, 'zh': {'title': '小型语言模型是代理AI的未来', 'desc': '大型语言模型（LLMs）在许多任务中表现出接近人类的能力，但在特定应用中，较小的语言模型（SLMs）更为合适。SLMs在代理系统中能够高效地执行重复性任务，且经济性更强。我们认为，SLMs的能力足以满足许多需求，并且在需要多种模型的异构代理系统中，SLMs是理想选择。我们还提出了从LLMs到SLMs的转换算法，以促进SLMs在代理系统中的应用。'}}}, {'id': 'https://huggingface.co/papers/2506.00618', 'title': 'RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents', 'url': 'https://huggingface.co/papers/2506.00618', 'abstract': 'RIOSWorld is a benchmark for evaluating safety risks of multimodal large language models in real-world computer tasks, revealing significant risks that necessitate safety alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce RiOSWorld, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on RiOSWorld demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.', 'score': 1, 'issue_id': 4148, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': 'd37e3433dcb25b41', 'authors': ['Jingyi Yang', 'Shuai Shao', 'Dongrui Liu', 'Jing Shao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00618.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#multimodal', '#agents', '#security', '#alignment'], 'emoji': '🖥️', 'ru': {'title': 'RIOSWorld: выявление рисков безопасности ИИ-агентов в реальных компьютерных задачах', 'desc': 'RIOSWorld - это бенчмарк для оценки рисков безопасности мультимодальных больших языковых моделей (MLLM) при выполнении компьютерных задач в реальном мире. Он включает 492 рисковых задания в различных компьютерных приложениях, охватывающих веб, социальные сети, мультимедиа, ОС, электронную почту и офисные программы. Риски разделены на две основные категории: риски, исходящие от пользователя, и риски окружающей среды. Эксперименты показали значительные риски безопасности для современных агентов на основе MLLM при работе с компьютером в реальных сценариях.'}, 'en': {'title': 'Evaluating Safety Risks of MLLMs in Real-World Tasks with RIOSWorld', 'desc': 'RIOSWorld is a new benchmark created to assess the safety risks associated with multimodal large language models (MLLMs) when they perform real-world computer tasks. It identifies significant risks that arise from both user actions and environmental factors, emphasizing the need for safety measures tailored to these scenarios. The benchmark includes a diverse set of 492 tasks across various applications, allowing for a comprehensive evaluation of potential hazards. The findings indicate that current MLLM-based agents face considerable safety challenges, underscoring the importance of aligning their operations with safety principles in practical environments.'}, 'zh': {'title': '评估多模态语言模型安全风险的新基准', 'desc': 'RIOSWorld是一个用于评估多模态大型语言模型（MLLM）在现实计算任务中安全风险的基准。随着MLLM的快速发展，它们被越来越多地用作自主计算代理，但在实际应用中存在安全风险的问题。现有的研究在评估这些风险时存在局限性，往往缺乏真实的互动环境或只关注特定的风险类型。RIOSWorld通过提供492个涵盖多种计算应用的风险任务，帮助全面评估计算代理的安全风险，强调了在现实计算操作中进行安全对齐的必要性。'}}}, {'id': 'https://huggingface.co/papers/2506.14965', 'title': 'Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective', 'url': 'https://huggingface.co/papers/2506.14965', 'abstract': 'Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360', 'score': 32, 'issue_id': 4399, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'ecb3c8bbf20f8213', 'authors': ['Zhoujun Cheng', 'Shibo Hao', 'Tianyang Liu', 'Fan Zhou', 'Yutao Xie', 'Feng Yao', 'Yuexin Bian', 'Yonghao Zhuang', 'Nilabjo Dey', 'Yuheng Zha', 'Yi Gu', 'Kun Zhou', 'Yuqi Wang', 'Yuan Li', 'Richard Fan', 'Jianshu She', 'Chengqian Gao', 'Abulhair Saparov', 'Haonan Li', 'Taylor W. Killian', 'Mikhail Yurochkin', 'Zhengzhong Liu', 'Eric P. Xing', 'Zhiting Hu'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI', 'Purdue University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.14965.jpg', 'data': {'categories': ['#training', '#rl', '#open_source', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Guru: Расширение горизонтов рассуждений LLM с помощью обучения с подкреплением', 'desc': 'Представлен корпус Guru для обучения с подкреплением (RL) в различных областях рассуждений для больших языковых моделей (LLM). Исследование показывает, что эффективность RL варьируется в зависимости от предметной области, причем некоторые области требуют специфического обучения. Разработаны модели Guru-7B и Guru-32B, демонстрирующие улучшенную производительность в сложных задачах рассуждения. Результаты указывают на потенциал RL не только для извлечения существующих знаний, но и для приобретения новых навыков в LLM.'}, 'en': {'title': 'Unlocking Diverse Reasoning with Guru: A Reinforcement Learning Revolution', 'desc': 'The paper introduces Guru, a comprehensive reinforcement learning (RL) reasoning corpus designed to enhance the reasoning capabilities of large language models (LLMs) across various domains. It addresses the challenge of limited RL reward signals by providing 92,000 examples in six distinct reasoning areas, ensuring reliable and effective training. The findings reveal that while some domains benefit from cross-domain training, others require specific in-domain training to improve performance, indicating that RL can lead to real skill development. The authors present two models, Guru-7B and Guru-32B, which achieve state-of-the-art results in RL training, outperforming existing models and enhancing their base performance on complex reasoning tasks.'}, 'zh': {'title': 'Guru：提升RL在推理中的应用', 'desc': '本文介绍了Guru，一个多样化的强化学习（RL）推理语料库，强调了特定领域训练的需求，并展示了在复杂任务中对增强型大型语言模型（LLM）性能的提升。Guru包含92,000个可验证的示例，涵盖数学、代码、科学、逻辑、仿真和表格等六个推理领域，确保了RL训练的可靠性和有效性。研究表明，RL在不同领域的表现差异显著，某些领域（如数学和代码）可以通过跨领域训练获得更好的效果，而其他领域（如逻辑和仿真）则需要在特定领域内进行训练。最终，Guru-7B和Guru-32B模型在多个推理任务中表现出色，超越了现有的最佳基线。'}}}, {'id': 'https://huggingface.co/papers/2506.15564', 'title': 'Show-o2: Improved Native Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2506.15564', 'abstract': 'Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.', 'score': 17, 'issue_id': 4415, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'd12a7d2ea11f47a1', 'authors': ['Jinheng Xie', 'Zhenheng Yang', 'Mike Zheng Shou'], 'affiliations': ['ByteDance', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.15564.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#3d', '#cv', '#video', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Show-o2: Единая мультимодальная модель для понимания и генерации контента', 'desc': 'Статья представляет Show-o2 - усовершенствованную мультимодальную модель, использующую авторегрессионное моделирование и сопоставление потоков в 3D причинном вариационном автоэнкодере. Модель создает единые визуальные представления для задач мультимодального понимания и генерации, объединяя текст, изображения и видео. Show-o2 применяет двухэтапный рецепт обучения для эффективного масштабирования на более крупные модели. Результаты демонстрируют универсальность модели в широком спектре мультимодальных задач понимания и генерации.'}, 'en': {'title': 'Unified Multimodal Mastery with Show-o2', 'desc': 'The paper introduces Show-o2, a novel multimodal model that combines autoregressive modeling and flow matching within a 3D causal variational autoencoder framework. This approach allows for the creation of unified visual representations that can effectively handle both image and video data. By employing a dual-path strategy for spatial and temporal fusion, Show-o2 enhances scalability and performance in multimodal tasks. The model is trained using a two-stage recipe, resulting in a versatile system capable of generating and understanding text, images, and videos.'}, 'zh': {'title': 'Show-o2：统一多模态理解与生成的创新模型', 'desc': '这篇论文介绍了一种改进的统一多模态模型，称为Show-o2。它利用自回归建模和流匹配技术，在3D因果变分自编码器的基础上构建统一的视觉表示。通过空间和时间的双路径融合，Show-o2能够有效处理图像和视频的多模态理解与生成任务。论文还设计了一个两阶段的训练方案，以便有效学习并扩展到更大的模型。'}}}, {'id': 'https://huggingface.co/papers/2506.09827', 'title': 'EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection', 'url': 'https://huggingface.co/papers/2506.09827', 'abstract': 'EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.  \t\t\t\t\tAI-generated summary \t\t\t\t The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.', 'score': 14, 'issue_id': 4400, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '38fbc70bf2ef023b', 'authors': ['Christoph Schuhmann', 'Robert Kaczmarczyk', 'Gollam Rabby', 'Felix Friedrich', 'Maurice Kraus', 'Kourosh Nadi', 'Huu Nguyen', 'Kristian Kersting', 'Sören Auer'], 'affiliations': ['Centre for Cognitive Science', 'DFKI', 'Hessian.AI', 'L3S Research Center Leibniz University of Hannover', 'LAION e.V.', 'Ontocord', 'TIBLeibniz Information Centre for Science and Technology', 'TU Darmstadt', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.09827.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#science', '#synthetic', '#data', '#audio'], 'emoji': '🎭', 'ru': {'title': 'Точное распознавание эмоций в речи с помощью синтетических данных', 'desc': 'EmoNet-Voice - это новый ресурс для распознавания эмоций в речи, включающий крупномасштабный датасет для предобучения и набор данных для бенчмаркинга. Он предлагает оценку 40 эмоциональных категорий с различными уровнями интенсивности, используя синтетические аудиофрагменты, сгенерированные с помощью современных технологий синтеза речи. Датасет прошел тщательную валидацию экспертами-психологами и обеспечивает конфиденциальность данных. Авторы также представили модели Empathic Insight Voice, демонстрирующие высокое согласие с оценками экспертов в задаче распознавания эмоций в речи.'}, 'en': {'title': 'EmoNet-Voice: Revolutionizing Speech Emotion Recognition with Synthetic Audio', 'desc': 'EmoNet-Voice is a new resource that enhances speech emotion recognition (SER) by providing extensive pre-training and benchmark datasets. It includes EmoNet-Voice Big, which features over 4,500 hours of synthetic audio across multiple voices and languages, allowing for fine-grained emotion evaluation across 40 distinct categories. The dataset is designed to address privacy concerns and emotional granularity limitations found in existing SER datasets by using AI-generated audio that simulates real emotional expressions. Additionally, the paper introduces Empathic Insight Voice models that demonstrate high agreement with human expert evaluations, revealing insights into the detection of various emotional intensities.'}, 'zh': {'title': 'EmoNet-Voice：细粒度语音情感识别的新标准', 'desc': 'EmoNet-Voice是一个新的语音情感识别资源，提供了大规模的预训练和基准数据集。它包含超过4500小时的语音数据，涵盖11种声音、40种情感和4种语言，能够进行细粒度的情感评估。通过合成的音频片段，EmoNet-Voice模拟了演员表现特定情感的场景，并经过心理学专家的严格验证。该资源为情感识别模型设定了新的标准，尤其在高唤醒情感（如愤怒）与低唤醒状态（如专注）之间的检测上表现出显著差异。'}}}, {'id': 'https://huggingface.co/papers/2506.14837', 'title': 'Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction', 'url': 'https://huggingface.co/papers/2506.14837', 'abstract': 'ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o.', 'score': 8, 'issue_id': 4396, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '3172095671c65e03', 'authors': ['Chengzhi Xu', 'Yuyang Wang', 'Lai Wei', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'MIFA Lab, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14837.jpg', 'data': {'categories': ['#cv', '#interpretability', '#optimization', '#training', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Точная генерация кода графиков с помощью структурированных инструкций и итеративного уточнения', 'desc': 'ChartIR - это метод итеративного уточнения для улучшения производительности мультимодальных больших языковых моделей (MLLM) в задаче генерации кода по изображению графика. Метод разделяет задачи визуального понимания и перевода в код, используя структурированные инструкции для описания и сравнения графиков. ChartIR применяет двухэтапный подход: начальная генерация кода и итеративное уточнение. Эксперименты показали превосходство ChartIR над другими методами на моделях Qwen2-VL и GPT-4.'}, 'en': {'title': 'ChartIR: Refining Code Generation from Charts with Structured Instructions', 'desc': 'ChartIR is a novel approach that enhances the performance of multimodal large language models (MLLMs) in generating code from charts by separating the tasks of visual understanding and code translation. It employs structured instructions to guide the model in accurately interpreting visual elements and translating them into executable code. The method involves two main stages: initial code generation followed by iterative refinement, which allows for progressive improvements in the output. Experimental results demonstrate that ChartIR significantly outperforms existing methods on both open-source and closed-source models.'}, 'zh': {'title': 'ChartIR：提升图表到代码生成的智能方法', 'desc': 'ChartIR是一种通过结构化指令和迭代优化来提升多模态大语言模型（MLLM）在图表到代码生成任务中的表现的方法。该方法将视觉理解和代码翻译任务分开，首先通过描述和差异两种结构化指令来捕捉图表的视觉元素。接着，ChartIR将整体图表生成流程分为初始代码生成和迭代优化两个阶段，从而逐步提升最终输出的质量。实验结果表明，与其他方法相比，ChartIR在开源模型Qwen2-VL和闭源模型GPT-4o上均表现出更优的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.15154', 'title': 'SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning', 'url': 'https://huggingface.co/papers/2506.15154', 'abstract': 'SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.  \t\t\t\t\tAI-generated summary \t\t\t\t Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.', 'score': 6, 'issue_id': 4397, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'f2b380f0491c0add', 'authors': ['Anuradha Chopra', 'Abhinaba Roy', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.15154.jpg', 'data': {'categories': ['#audio', '#games', '#multimodal', '#architecture', '#science', '#dataset', '#training'], 'emoji': '🎵', 'ru': {'title': 'SonicVerse: Понимание музыки через многозадачное обучение', 'desc': 'SonicVerse - это мультизадачная модель для создания описаний музыки, которая объединяет генерацию подписей с определением музыкальных характеристик. Модель использует проекционную архитектуру, преобразующую аудиовход в языковые токены и одновременно определяющую музыкальные особенности через вспомогательные выходы. Для обучения модели был расширен датасет MusicBench с помощью аннотаций музыкальных характеристик, полученных с использованием MIRFLEX. Экспериментальные результаты показывают, что включение музыкальных характеристик улучшает качество и детализацию генерируемых описаний.'}, 'en': {'title': 'Enhancing Music Descriptions with SonicVerse', 'desc': 'SonicVerse is a multi-task music captioning model that improves the quality of music descriptions by integrating audio feature detection. It captures both low-level acoustic details and high-level musical attributes through a projection-based architecture. This model generates detailed captions for short music pieces and can create time-informed descriptions for longer compositions by using a large-language model. By enhancing the training dataset with music features, SonicVerse demonstrates improved caption quality and detail in its outputs.'}, 'zh': {'title': 'SonicVerse：提升音乐字幕质量的多任务模型', 'desc': 'SonicVerse是一种多任务音乐字幕生成模型，结合了音频特征检测以提高字幕质量。该模型通过关键音调检测、声乐检测等辅助任务，捕捉音乐的低级声学细节和高级音乐属性。其关键贡献在于采用基于投影的架构，将音频输入转换为语言标记，同时通过专用辅助头检测音乐特征。实验结果表明，这种特征的结合显著提升了生成字幕的质量和细节。'}}}, {'id': 'https://huggingface.co/papers/2506.15455', 'title': 'RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2506.15455', 'abstract': 'RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.', 'score': 3, 'issue_id': 4407, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '880c387f77f1729e', 'authors': ['Xinnuo Xu', 'Rachel Lawrence', 'Kshitij Dubey', 'Atharva Pandey', 'Risa Ueno', 'Fabian Falck', 'Aditya V. Nori', 'Rahul Sharma', 'Amit Sharma', 'Javier Gonzalez'], 'affiliations': ['Microsoft Research Cambridge, UK', 'Microsoft Research India'], 'pdf_title_img': 'assets/pdf/title_img/2506.15455.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Оценка истинного рассуждения в LLM через генерацию вариаций задач', 'desc': 'Статья представляет фреймворк RE-IMAGINE для оценки способностей больших языковых моделей (LLM) к рассуждению. Фреймворк генерирует вариации задач, которые нельзя решить простым запоминанием, что позволяет отличить истинное рассуждение от статистического воспроизведения. RE-IMAGINE основан на иерархии уровней рассуждения и может применяться в различных областях, включая математику, программирование и логику. Тестирование нескольких семейств LLM на четырех популярных бенчмарках показало снижение производительности при использовании вариаций задач, что указывает на зависимость от статистического воспроизведения.'}, 'en': {'title': 'Evaluating True Reasoning in Language Models with RE-IMAGINE', 'desc': "The paper introduces RE-IMAGINE, a framework designed to assess the reasoning capabilities of Large Language Models (LLMs) by generating problem variations that require more than just memorization. It builds on the ladder of causation, which categorizes reasoning into three levels: associations, interventions, and counterfactuals. By transforming problems into an intermediate symbolic representation, RE-IMAGINE creates numerous unique problems that challenge LLMs' reasoning skills. The results from testing various LLMs on established benchmarks reveal a significant reliance on statistical recall, suggesting that their high accuracy may not reflect true reasoning abilities."}, 'zh': {'title': '推理能力的新评估：RE-IMAGINE框架', 'desc': 'RE-IMAGINE 是一个评估大型语言模型（LLMs）推理能力的框架。它通过生成无法仅靠记忆解决的问题变体，来检验模型是否真正具备推理能力。该框架基于因果推理的层次结构，能够在不同推理领域（如数学、代码和逻辑）中生成问题变体。研究表明，当模型面对这些变体时，性能下降，显示出它们在一定程度上依赖于统计记忆。'}}}, {'id': 'https://huggingface.co/papers/2505.21115', 'title': 'Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA', 'url': 'https://huggingface.co/papers/2505.21115', 'abstract': 'EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior.', 'score': 81, 'issue_id': 4192, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cbbff6b511a277fa', 'authors': ['Sergey Pletenev', 'Maria Marina', 'Nikolay Ivanov', 'Daria Galimzianova', 'Nikita Krayko', 'Mikhail Salnikov', 'Vasily Konovalov', 'Alexander Panchenko', 'Viktor Moskvoretskii'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'MTS AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21115.jpg', 'data': {'categories': ['#benchmark', '#training', '#low_resource', '#dataset', '#multilingual', '#long_context', '#hallucinations'], 'emoji': '🌳', 'ru': {'title': 'EverGreenQA: новый подход к оценке темпоральности в вопросно-ответных системах', 'desc': "EverGreenQA - это новый многоязычный набор данных для задач вопросно-ответных систем, который включает метки 'вечнозеленых' вопросов. Он используется для оценки способности больших языковых моделей (LLM) кодировать временные аспекты информации. Исследователи протестировали 12 современных LLM на этом наборе данных, оценивая их явное и неявное понимание темпоральности вопросов. Также был разработан классификатор EG-E5, достигающий наилучших результатов в определении 'вечнозеленых' вопросов."}, 'en': {'title': 'Understanding Question Timeliness with EverGreenQA', 'desc': 'The paper introduces EverGreenQA, a new multilingual question answering (QA) dataset designed to evaluate how well large language models (LLMs) understand the concept of temporality in questions. It distinguishes between evergreen questions, which have stable answers, and mutable questions, which can change over time. The authors benchmark 12 LLMs using this dataset to see if they can explicitly or implicitly recognize question temporality through verbalized judgments and uncertainty signals. Additionally, they present EG-E5, a lightweight multilingual classifier that achieves state-of-the-art performance and demonstrate its usefulness in various applications, such as enhancing self-knowledge estimation and filtering QA datasets.'}, 'zh': {'title': '揭示问答中的时间性：EverGreenQA数据集', 'desc': 'EverGreenQA是一个多语言问答数据集，专注于时间性编码，特别是问题的持久性。该数据集通过永恒标签来评估大型语言模型（LLMs）在问答任务中的表现。研究表明，问题的时间性（如永恒性或可变性）对LLMs的回答准确性有重要影响。我们还开发了EG-E5，一个轻量级的多语言分类器，在这一任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01111', 'title': 'FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion', 'url': 'https://huggingface.co/papers/2506.01111', 'abstract': 'A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.', 'score': 25, 'issue_id': 4186, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'a649684de588a812', 'authors': ['Shunian Chen', 'Xinyuan Xie', 'Zheshu Chen', 'Liyan Zhao', 'Owen Lee', 'Zhan Su', 'Qilin Sun', 'Benyou Wang'], 'affiliations': ['South China University of Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01111.jpg', 'data': {'categories': ['#dataset', '#open_source', '#audio', '#multimodal', '#optimization', '#data', '#games'], 'emoji': '🎧', 'ru': {'title': 'Революция в аудио-подписях: мультимодальный ИИ для точного описания звуков', 'desc': 'Статья представляет новый двухэтапный конвейер для улучшения качества аудио-подписей, используя специализированные предобученные модели и большую языковую модель (LLM). Этот метод интегрирует разнообразные мультимодальные сигналы и контекстную информацию для создания более детальных и точных описаний аудио. Авторы также представляют FusionAudio - новый крупномасштабный датасет, содержащий 1,2 миллиона подробных аудио-подписей и 6 миллионов пар вопросов-ответов. Исследование демонстрирует улучшенные аудио-модели, разработанные с использованием FusionAudio, включая аудио-энкодер на основе CLAP с улучшенным выравниванием аудио и текста.'}, 'en': {'title': 'Enhancing Audio Captions with Multimodal Insights', 'desc': 'This paper presents a two-stage pipeline that improves the quality of audio captions by using specialized pretrained models alongside a large language model (LLM). The first stage extracts various contextual cues from audio, such as speech and music, as well as visual information from related videos. In the second stage, the LLM synthesizes these multimodal inputs to create detailed and contextually accurate captions. The work introduces a new dataset, FusionAudio, which contains 1.2 million detailed captions and enhances audio models for better audio-text alignment.'}, 'zh': {'title': '提升音频字幕质量的创新方法', 'desc': '本文提出了一种新颖的两阶段管道，利用专门的预训练模型和大型语言模型来提高音频字幕的质量。该方法通过提取多样的上下文线索，如语音、音乐和视觉信息，来增强音频理解。然后，使用大型语言模型综合这些多模态输入，生成详细且具有上下文意识的音频字幕。此研究的关键贡献包括可扩展的细粒度音频字幕生成方法和一个新的大规模数据集FusionAudio，包含120万条详细字幕和600万对问答。'}}}, {'id': 'https://huggingface.co/papers/2506.05523', 'title': 'MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2506.05523', 'abstract': 'MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in vision-language models (VLMs), current benchmarks for multimodal reasoning fall short in three key dimensions. First, they overwhelmingly rely on static images, failing to capture the temporal complexity of real-world environments. Second, they narrowly focus on mathematical problem-solving, neglecting the broader spectrum of reasoning skills -- including abstract, physical, planning, spatial, and temporal capabilities -- required for robust multimodal intelligence. Third, many benchmarks quickly saturate, offering limited headroom for diagnosing failure modes or measuring continued progress. We introduce MORSE-500 (Multimodal Reasoning Stress-test Environment), a video benchmark composed of 500 fully scripted clips with embedded questions spanning six complementary reasoning categories. Each instance is programmatically generated using deterministic Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and curated real footage. This script-driven design allows fine-grained control over visual complexity, distractor density, and temporal dynamics -- enabling difficulty to be scaled systematically as models improve. Unlike static benchmarks that become obsolete once saturated, MORSE-500 is built to evolve: its controllable generation pipeline supports the creation of arbitrarily challenging new instances, making it ideally suited for stress-testing next-generation models. Initial experiments with state-of-the-art systems -- including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest available at the time, alongside strong open-source models -- reveal substantial performance gaps across all categories, with particularly large deficits in abstract and planning tasks. We release the full dataset, generation scripts, and evaluation harness to support transparent, reproducible, and forward-looking multimodal reasoning research.', 'score': 24, 'issue_id': 4188, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '524394ef06ba3ba1', 'authors': ['Zikui Cai', 'Andrew Wang', 'Anirudh Satheesh', 'Ankit Nakhawa', 'Hyunwoo Jae', 'Keenan Powell', 'Minghui Liu', 'Neel Jay', 'Sungbin Oh', 'Xiyao Wang', 'Yongyuan Liang', 'Tom Goldstein', 'Furong Huang'], 'affiliations': ['Capital One', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.05523.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#video', '#open_source', '#dataset', '#reasoning'], 'emoji': '🎬', 'ru': {'title': 'MORSE-500: Новый рубеж в оценке мультимодального ИИ', 'desc': 'MORSE-500 - это новый видеобенчмарк для оценки мультимодального рассуждения моделей искусственного интеллекта. Он включает 500 специально созданных видеоклипов с вопросами по шести категориям рассуждений, включая абстрактное мышление и планирование. Бенчмарк позволяет систематически увеличивать сложность тестов по мере улучшения моделей. Эксперименты показали значительные пробелы в производительности современных систем, особенно в абстрактных задачах и планировании.'}, 'en': {'title': 'MORSE-500: Evolving Benchmark for Multimodal Reasoning', 'desc': 'MORSE-500 is a new video benchmark designed to evaluate multimodal reasoning in AI across six different categories. It addresses limitations in existing benchmarks by incorporating dynamic video clips instead of static images, allowing for a more realistic assessment of reasoning skills. The benchmark includes a variety of reasoning tasks, such as abstract thinking and planning, which are essential for advanced multimodal intelligence. By providing a scalable and evolving dataset, MORSE-500 aims to facilitate ongoing research and development in multimodal reasoning capabilities.'}, 'zh': {'title': 'MORSE-500：多模态推理的新基准', 'desc': 'MORSE-500是一个包含500个脚本化视频片段的基准测试，旨在评估多模态推理能力。该基准涵盖六个互补的推理类别，强调了在抽象和规划任务中的性能差距。与静态图像基准不同，MORSE-500能够捕捉现实环境的时间复杂性，并支持生成具有不同难度的新实例。通过提供完整的数据集和生成脚本，MORSE-500为多模态推理研究提供了透明和可重复的支持。'}}}, {'id': 'https://huggingface.co/papers/2506.05629', 'title': 'Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs', 'url': 'https://huggingface.co/papers/2506.05629', 'abstract': 'A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of large language models in domain-specific tasks necessitates fine-tuning, which is computationally expensive and technically challenging. This paper focuses on parameter-efficient fine-tuning using soft prompting, a promising approach that adapts pre-trained models to downstream tasks by learning a small set of parameters. We propose a novel Input Dependent Soft Prompting technique with a self-Attention Mechanism (ID-SPAM) that generates soft prompts based on the input tokens and attends different tokens with varying importance. Our method is simple and efficient, keeping the number of trainable parameters small. We show the merits of the proposed approach compared to state-of-the-art techniques on various tasks and show the improved zero shot domain transfer capability.', 'score': 23, 'issue_id': 4186, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c88ec16aee1c43d5', 'authors': ['Ananth Muppidi', 'Abhilash Nandy', 'Sambaran Bandyopadhyay'], 'affiliations': ['Adobe Research, India', 'IIIT Hyderabad, India', 'IIT Kharagpur, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.05629.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Эффективная донастройка языковых моделей с помощью динамических промптов', 'desc': 'Статья представляет новый метод параметрически-эффективной донастройки больших языковых моделей. Предложенная техника Input Dependent Soft Prompting with self-Attention Mechanism (ID-SPAM) генерирует мягкие промпты на основе входных токенов. Метод использует механизм самовнимания для присвоения различной важности разным токенам. ID-SPAM показывает улучшенные результаты по сравнению с современными подходами на различных задачах, особенно в zero-shot переносе на новые домены.'}, 'en': {'title': 'Efficient Fine-Tuning with Input-Dependent Soft Prompts', 'desc': 'This paper introduces a new technique called Input Dependent Soft Prompting with a Self-Attention Mechanism (ID-SPAM) to enhance fine-tuning of large language models. It focuses on making the fine-tuning process more efficient by using a small set of parameters that adapt the model to specific tasks. The self-attention mechanism allows the model to weigh the importance of different input tokens when generating soft prompts. The results demonstrate that ID-SPAM outperforms existing methods, particularly in zero-shot domain transfer scenarios.'}, 'zh': {'title': '输入依赖的软提示，提升微调效率', 'desc': '本文提出了一种新的方法，利用输入依赖的软提示和自注意力机制，来提高大语言模型的参数高效微调能力。这种方法通过学习一小组参数，适应预训练模型到下游任务，减少了计算成本。我们的方法生成基于输入标记的软提示，并对不同的重要性标记进行关注，从而实现了高效的微调。实验结果表明，该方法在多个任务上优于现有技术，并提升了零-shot领域迁移能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05446', 'title': 'Sentinel: SOTA model to protect against prompt injections', 'url': 'https://huggingface.co/papers/2506.05446', 'abstract': "Sentinel, a detection model based on ModernBERT-large, effectively identifies prompt injection attacks with high accuracy and outperforms existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.", 'score': 17, 'issue_id': 4199, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '293cbf4b92765fe2', 'authors': ['Dror Ivry', 'Oran Nahum'], 'affiliations': ['Qualiﬁre, Tel Aviv, IL'], 'pdf_title_img': 'assets/pdf/title_img/2506.05446.jpg', 'data': {'categories': ['#architecture', '#data', '#security', '#ethics', '#dataset', '#benchmark', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Sentinel: Передовая защита от атак внедрения промптов для больших языковых моделей', 'desc': 'Статья представляет Sentinel - модель обнаружения атак внедрения промптов, основанную на архитектуре ModernBERT-large. Sentinel демонстрирует высокую точность (0.987) и F1-меру (0.980) на внутреннем тестовом наборе, превосходя существующие базовые модели. Модель обучена на обширном и разнообразном наборе данных, включающем различные типы атак и доброкачественные инструкции. Исследование подробно описывает архитектуру Sentinel, процесс подготовки данных и методологию обучения.'}, 'en': {'title': 'Sentinel: Safeguarding LLMs from Prompt Injection Attacks', 'desc': 'The paper presents Sentinel, a detection model built on the ModernBERT-large architecture, designed to identify prompt injection attacks in large language models (LLMs). Prompt injection attacks can manipulate LLMs into producing unintended outputs, making effective detection crucial. Sentinel is trained on a diverse dataset that includes various attack types and benign instructions, achieving high accuracy and F1-scores in its evaluations. The results show that Sentinel outperforms existing models, demonstrating its potential as a robust solution for enhancing the security of LLMs against such vulnerabilities.'}, 'zh': {'title': 'Sentinel：高效识别提示注入攻击的检测模型', 'desc': '本论文介绍了一种名为Sentinel的检测模型，基于ModernBERT-large架构，能够高效识别提示注入攻击。提示注入攻击是指恶意输入导致大型语言模型偏离预期指令的情况。Sentinel通过在多样化的数据集上进行微调，达到了先进的检测性能，平均准确率为0.987，F1分数为0.980。该模型在公共基准测试中也表现优于现有的强基线，展示了其卓越的检测能力。'}}}, {'id': 'https://huggingface.co/papers/2506.01872', 'title': 'Is Extending Modality The Right Path Towards Omni-Modality?', 'url': 'https://huggingface.co/papers/2506.01872', 'abstract': 'Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal language models (OLMs) aim to integrate and reason over diverse input modalities--such as text, images, video, and audio--while maintaining strong language capabilities. Despite recent advancements, existing models, especially open-source ones, remain far from true omni-modality, struggling to generalize beyond the specific modality pairs they are trained on or to achieve strong performance when processing multi-modal inputs. We study the effect of extending modality, the dominant technique for training multimodal models, where an off-the-shelf language model is fine-tuned on target-domain and language data. Specifically, we investigate three key questions: (1) Does modality extension compromise core language abilities? (2) Can model merging effectively integrate independently fine-tuned modality-specific models to achieve omni-modality? (3) Does omni-modality extension lead to better knowledge sharing and generalization compared to sequential extension? Through extensive experiments, we analyze these trade-offs and provide insights into the feasibility of achieving true omni-modality using current approaches.', 'score': 16, 'issue_id': 4193, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'f876844db3f1bfbd', 'authors': ['Tinghui Zhu', 'Kai Zhang', 'Muhao Chen', 'Yu Su'], 'affiliations': ['The Ohio State University', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.01872.jpg', 'data': {'categories': ['#training', '#agi', '#multimodal', '#open_source', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Путь к истинной омни-модальности: баланс между расширением и обобщением', 'desc': 'Исследование посвящено изучению влияния расширения модальности и слияния моделей на сохранение языковых способностей и обобщение в омни-модальных языковых моделях. Авторы анализируют, как расширение модальности влияет на основные языковые навыки модели. Они также исследуют эффективность слияния моделей для достижения омни-модальности и сравнивают омни-модальное расширение с последовательным. Эксперименты предоставляют важные выводы о возможности достижения истинной омни-модальности с использованием современных подходов.'}, 'en': {'title': 'Unlocking True Omni-Modality in Language Models', 'desc': 'This research explores how extending the types of data (modalities) and merging different models can help omni-modal language models (OLMs) maintain their language skills and improve their ability to generalize across various inputs. OLMs are designed to work with multiple forms of data, like text and images, but often struggle to perform well when faced with new combinations of these inputs. The study examines whether adding new modalities affects the language capabilities of these models, if merging models trained on different modalities can create a more effective omni-modal model, and whether this approach enhances knowledge sharing and generalization. Through experiments, the paper provides valuable insights into the challenges and potential solutions for achieving true omni-modality in language models.'}, 'zh': {'title': '实现真正的全模态能力', 'desc': '本研究探讨了扩展模态和模型合并对全模态语言模型在保持语言能力和泛化能力方面的影响。全模态语言模型旨在整合和推理多种输入模态，如文本、图像、视频和音频。尽管已有进展，现有模型在真正的全模态能力上仍然存在不足，尤其是在处理多模态输入时的泛化能力较弱。我们通过实验分析了扩展模态对核心语言能力的影响，以及模型合并是否能有效整合独立微调的模态特定模型，以实现全模态能力。'}}}, {'id': 'https://huggingface.co/papers/2506.05573', 'title': 'PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.05573', 'abstract': 'PartCrafter generates complex 3D scenes from single images using a unified compositional architecture with a diffusion transformer, enabling part-aware generation and hierarchical attention.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PartCrafter, the first structured 3D generative model that jointly synthesizes multiple semantically meaningful and geometrically distinct 3D meshes from a single RGB image. Unlike existing methods that either produce monolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an image and then reconstructing each segment, PartCrafter adopts a unified, compositional generation architecture that does not rely on pre-segmented inputs. Conditioned on a single image, it simultaneously denoises multiple 3D parts, enabling end-to-end part-aware generation of both individual objects and complex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh diffusion transformer (DiT) trained on whole objects, inheriting the pretrained weights, encoder, and decoder, and introduces two key innovations: (1) A compositional latent space, where each 3D part is represented by a set of disentangled latent tokens; (2) A hierarchical attention mechanism that enables structured information flow both within individual parts and across all parts, ensuring global coherence while preserving part-level detail during generation. To support part-level supervision, we curate a new dataset by mining part-level annotations from large-scale 3D object datasets. Experiments show that PartCrafter outperforms existing approaches in generating decomposable 3D meshes, including parts that are not directly visible in input images, demonstrating the strength of part-aware generative priors for 3D understanding and synthesis. Code and training data will be released.', 'score': 14, 'issue_id': 4191, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'af089266a7086a2d', 'authors': ['Yuchen Lin', 'Chenguo Lin', 'Panwang Pan', 'Honglei Yan', 'Yiqiang Feng', 'Yadong Mu', 'Katerina Fragkiadaki'], 'affiliations': ['ByteDance', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05573.jpg', 'data': {'categories': ['#3d', '#diffusion', '#open_source', '#architecture', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Создание сложных 3D-сцен из одного изображения', 'desc': 'PartCrafter - это новая модель для генерации структурированных 3D-сцен из одиночных изображений. Она использует композиционную архитектуру на основе диффузионного трансформера для одновременной генерации нескольких семантически значимых 3D-объектов. Модель применяет иерархический механизм внимания для обеспечения глобальной согласованности при сохранении детализации на уровне отдельных частей. PartCrafter превосходит существующие подходы в генерации декомпозируемых 3D-моделей, включая части, не видимые напрямую на входном изображении.'}, 'en': {'title': 'Transforming Images into 3D Worlds with PartCrafter!', 'desc': 'PartCrafter is a novel 3D generative model that creates detailed 3D scenes from a single RGB image without needing pre-segmented inputs. It uses a unified architecture that combines part-aware generation with hierarchical attention, allowing it to generate multiple distinct 3D meshes simultaneously. The model leverages a pretrained diffusion transformer to enhance the quality of the generated parts and maintains coherence across the entire scene. By introducing a compositional latent space and a new dataset for part-level supervision, PartCrafter significantly improves the generation of complex 3D structures, even including parts not visible in the original image.'}, 'zh': {'title': 'PartCrafter：从单图像生成复杂3D场景的创新模型', 'desc': 'PartCrafter是一种新型的3D生成模型，可以从单张RGB图像生成多个语义明确且几何上不同的3D网格。与传统方法不同，PartCrafter采用统一的生成架构，无需预先分割输入图像，能够同时去噪多个3D部分。该模型利用预训练的3D网格扩散变换器，并引入了组合潜在空间和层次注意机制，以确保生成的3D场景在全局一致性的同时保留部分细节。实验结果表明，PartCrafter在生成可分解的3D网格方面优于现有方法，展示了其在3D理解和合成中的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.06276', 'title': 'STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis', 'url': 'https://huggingface.co/papers/2506.06276', 'abstract': 'STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.', 'score': 12, 'issue_id': 4189, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '14f98c6826d7c6bb', 'authors': ['Jiatao Gu', 'Tianrong Chen', 'David Berthelot', 'Huangjie Zheng', 'Yuyang Wang', 'Ruixiang Zhang', 'Laurent Dinh', 'Miguel Angel Bautista', 'Josh Susskind', 'Shuangfei Zhai'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06276.jpg', 'data': {'categories': ['#architecture', '#cv', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'Нормализующие потоки покоряют высоты генерации изображений', 'desc': 'STARFlow - это генеративная модель, объединяющая нормализующие потоки и авторегрессивные трансформеры для синтеза изображений высокого разрешения. Модель использует глубоко-мелкую архитектуру и моделирование в латентном пространстве предобученных автоэнкодеров. Ключевые инновации включают новый алгоритм управления для повышения качества сэмплов и обучение методом максимального правдоподобия без дискретизации. STARFlow достигает конкурентоспособных результатов в задачах генерации изображений по классу и тексту, приближаясь к современным диффузионным моделям.'}, 'en': {'title': 'STARFlow: Merging Flows and Transformers for High-Quality Image Generation', 'desc': 'STARFlow is a generative model that merges normalizing flows with autoregressive Transformers to create high-quality images. It introduces the Transformer Autoregressive Flow (TARFlow), which effectively models continuous distributions while maintaining scalability. Key innovations include a deep-shallow architecture for efficient computation, latent space modeling using pretrained autoencoders, and a novel guidance algorithm to enhance sample quality. This model achieves competitive results in both class-conditional and text-conditional image generation, marking a significant advancement in the use of normalizing flows for high-resolution image synthesis.'}, 'zh': {'title': 'STARFlow：高效图像合成的新纪元', 'desc': 'STARFlow是一种结合了归一化流和自回归变换器的生成模型，能够在高分辨率图像合成中实现强大的性能。其核心是变换器自回归流（TARFlow），将归一化流的表达能力与自回归变换器的结构建模能力相结合。通过深浅设计、在预训练自编码器的潜在空间建模以及新颖的引导算法，STARFlow显著提高了可扩展性和样本质量。该模型能够在连续空间中进行精确的最大似然训练，且在类条件和文本条件的图像生成任务中表现出色，接近最先进的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.05984', 'title': 'Audio-Aware Large Language Models as Judges for Speaking Styles', 'url': 'https://huggingface.co/papers/2506.05984', 'abstract': "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-aware large language models (ALLMs) can understand the textual and non-textual information in the audio input. In this paper, we explore using ALLMs as an automatic judge to assess the speaking styles of speeches. We use ALLM judges to evaluate the speeches generated by SLMs on two tasks: voice style instruction following and role-playing. The speaking style we consider includes emotion, volume, speaking pace, word emphasis, pitch control, and non-verbal elements. We use four spoken language models (SLMs) to complete the two tasks and use humans and ALLMs to judge the SLMs' responses. We compare two ALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and show that the agreement between Gemini and human judges is comparable to the agreement between human evaluators. These promising results show that ALLMs can be used as a judge to evaluate SLMs. Our results also reveal that current SLMs, even GPT-4o-audio, still have room for improvement in controlling the speaking style and generating natural dialogues.", 'score': 12, 'issue_id': 4185, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '10dcc4567ff634c1', 'authors': ['Cheng-Han Chiang', 'Xiaofei Wang', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Radu Kopetz', 'Yao Qian', 'Zhendong Wang', 'Zhengyuan Yang', 'Hung-yi Lee', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05984.jpg', 'data': {'categories': ['#multimodal', '#audio', '#interpretability', '#games'], 'emoji': '🎙️', 'ru': {'title': 'АОБЛМ как объективные судьи стиля речи', 'desc': 'Аудио-осведомленные большие языковые модели (АОБЛМ) способны оценивать стили речи в аудиовходах, демонстрируя производительность, сравнимую с оценками людей-судей. В исследовании АОБЛМ использовались для оценки речей, сгенерированных разговорными языковыми моделями (РЯМ) в задачах следования инструкциям по стилю голоса и ролевой игры. Оценивались такие аспекты, как эмоции, громкость, темп речи, выделение слов, контроль высоты тона и невербальные элементы. Результаты показали, что согласованность между оценками Gemini и человеческих судей сопоставима с согласованностью между оценками разных людей.'}, 'en': {'title': 'Evaluating Speech Styles with AI: ALLMs vs. Human Judges', 'desc': 'This paper discusses the capabilities of audio-aware large language models (ALLMs) in evaluating speaking styles from audio inputs. The authors demonstrate that ALLMs can assess synthesized speech similarly to human judges, focusing on aspects like emotion, volume, and pitch. They compare the performance of two ALLMs, GPT-4o-audio and Gemini-2.5-pro, against human evaluations in tasks involving voice style instruction and role-playing. The findings indicate that while ALLMs can effectively judge speaking styles, there is still potential for improvement in the speaking style control of current spoken language models (SLMs).'}, 'zh': {'title': '音频感知模型：评估说话风格的新工具', 'desc': '音频感知的大型语言模型（ALLMs）能够评估音频输入中的说话风格，其表现与人类评审在情感、音量和音调等维度上的评估相当。本文探讨了使用ALLMs作为自动评审者来评估演讲的说话风格。我们使用四个口语语言模型（SLMs）完成两个任务，并通过人类和ALLMs对SLMs的响应进行评估。研究结果表明，ALLMs可以作为评审工具来评估SLMs，但当前的SLMs在控制说话风格和生成自然对话方面仍有改进空间。'}}}, {'id': 'https://huggingface.co/papers/2506.06253', 'title': 'Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision', 'url': 'https://huggingface.co/papers/2506.06253', 'abstract': 'A survey reviews advancements in video understanding from both egocentric and exocentric perspectives, highlighting applications, tasks, joint learning frameworks, and limitations, with the aim of enhancing machine perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.', 'score': 6, 'issue_id': 4192, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '7ddc25331945068e', 'authors': ['Yuping He', 'Yifei Huang', 'Guo Chen', 'Lidong Lu', 'Baoqi Pei', 'Jilan Xu', 'Tong Lu', 'Yoichi Sato'], 'affiliations': ['Fudan University, Shanghai 200433, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China', 'University of Tokyo, Tokyo, Japan', 'Zhejiang University, Zhejiang 310027, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06253.jpg', 'data': {'categories': ['#multimodal', '#video', '#survey', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Синергия эгоцентрического и экзоцентрического зрения для прорыва в понимании видео машинами', 'desc': 'Статья представляет обзор достижений в области понимания видео с эгоцентрической и экзоцентрической точек зрения. Авторы рассматривают практические приложения, ключевые исследовательские задачи и совместные обучающие фреймворки для интеграции обоих подходов. В работе анализируются три основных направления исследований: улучшение экзоцентрического понимания с помощью эгоцентрических данных, улучшение эгоцентрического анализа с использованием экзоцентрических данных и объединенные фреймворки обучения. Обзор также охватывает наборы данных, ограничения текущих работ и перспективные направления будущих исследований в области машинного восприятия видео.'}, 'en': {'title': 'Bridging Perspectives for Enhanced Video Understanding', 'desc': 'This paper surveys the progress in video understanding by examining both egocentric (first-person) and exocentric (third-person) perspectives. It emphasizes the importance of combining these viewpoints to enhance machine perception of dynamic environments. The authors categorize recent advancements into three main research directions: improving exocentric understanding with egocentric data, enhancing egocentric analysis with exocentric data, and developing joint learning frameworks. The survey also discusses practical applications, key research tasks, benchmark datasets, and identifies limitations in current research while suggesting future directions.'}, 'zh': {'title': '融合视角，提升视频理解', 'desc': '这篇论文综述了视频理解领域的最新进展，重点关注自我中心（第一人称）和外部中心（第三人称）视角的结合。通过整合这两种视角，研究者们希望提升机器对动态环境的感知能力。论文还识别了实现这些应用的关键研究任务，并系统地组织了最近的研究进展。最后，作者讨论了当前工作的局限性，并提出了未来的研究方向，以推动视频理解和人工智能的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.06199', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model', 'url': 'https://huggingface.co/papers/2506.06199', 'abstract': 'A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.', 'score': 4, 'issue_id': 4186, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'c1b9d0a9c29bdf3b', 'authors': ['Hongyan Zhi', 'Peihao Chen', 'Siyuan Zhou', 'Yubo Dong', 'Quanxi Wu', 'Lei Han', 'Mingkui Tan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Pazhou Laboratory', 'South China University of Technology', 'Tencent Robotics'], 'pdf_title_img': 'assets/pdf/title_img/2506.06199.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#robotics', '#optimization', '#3d', '#games'], 'emoji': '🤖', 'ru': {'title': 'Универсальная модель 3D-потока для роботизированных манипуляций', 'desc': 'Статья представляет модель 3D-потока для манипуляций роботов, обученную на данных человеческих и роботизированных манипуляций. Модель использует видеодиффузию и GPT-4o для прогнозирования движения объектов в 3D-пространстве. Авторы создали датасет ManiFlow-110k и разработали механизм рендеринга на основе потока для оценки соответствия предсказанных действий поставленной задаче. Эксперименты показали сильную обобщающую способность модели и возможность адаптации к различным роботизированным системам без специфического обучения.'}, 'en': {'title': 'Empowering Robots with 3D Flow for Versatile Manipulation', 'desc': 'This paper presents a novel 3D flow world model that enables robots to learn manipulation tasks by leveraging both human and robot data. The model predicts how objects move in 3D space, which helps robots plan their actions more effectively. By creating a large dataset called ManiFlow-110k and using a video diffusion approach, the researchers teach robots to understand manipulation physics and generate action plans based on language instructions. The results show that this method allows robots to generalize well across various tasks and adapt to different robotic embodiments without needing specific training for each hardware type.'}, 'zh': {'title': '学习3D流动模型，提升机器人操作能力', 'desc': '本论文提出了一种从人类和机器人操作数据中学习的3D流动世界模型，旨在帮助机器人执行多样化的操作任务。通过合成一个名为ManiFlow-110k的大规模3D光流数据集，模型能够预测交互对象在3D空间中的未来运动。利用视频扩散技术和GPT-4o，模型生成的3D光流轨迹可以指导机器人的操作规划。实验结果表明，该模型在不同的机器人操作任务中具有强大的泛化能力和跨实体适应性。'}}}, {'id': 'https://huggingface.co/papers/2506.05433', 'title': 'Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward', 'url': 'https://huggingface.co/papers/2506.05433', 'abstract': 'Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper', 'score': 4, 'issue_id': 4192, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'fe9abfe1e500f3e2', 'authors': ['Zikang Liu', 'Tongtian Yue', 'Yepeng Tang', 'Longteng Guo', 'Junxian Cai', 'Qingbin Liu', 'Xi Chen', 'Jing Liu'], 'affiliations': ['Basic Algorithm Center, Tencent', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Computer Science and Technology, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05433.jpg', 'data': {'categories': ['#long_context', '#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'Префикс Группер: эффективное масштабирование GRPO для длинных контекстов', 'desc': 'Префикс Группер - это эффективный алгоритм обучения для GRPO (Group Relative Policy Optimization), который устраняет избыточные вычисления общих префиксов. Метод реструктурирует самовнимание на две части, позволяя кодировать общий префикс только один раз, сохраняя при этом полную дифференцируемость и совместимость с обучением от начала до конца. Префикс Группер не меняет динамику оптимизации и конечную производительность политики, что подтверждается теоретически и эмпирически. Этот подход значительно снижает вычислительные затраты на обучение, особенно в сценариях с длинными префиксами, и может быть легко интегрирован в существующие архитектуры на основе GRPO.'}, 'en': {'title': 'Efficiently Scaling GRPO with Prefix Grouper', 'desc': 'The paper introduces Prefix Grouper, a novel algorithm designed to enhance the efficiency of Group Relative Policy Optimization (GRPO) by reducing computational overhead associated with encoding shared prefixes. By implementing a Shared-Prefix Forward strategy, it allows the shared prefix to be encoded only once, which significantly improves scalability in long-context scenarios without compromising the training dynamics or policy performance. The method maintains full differentiability and is compatible with end-to-end training, ensuring that the optimization process remains unchanged. Empirical results demonstrate that Prefix Grouper not only achieves consistent performance but also allows for larger group sizes within the same computational budget, making it a valuable addition to GRPO-based architectures.'}, 'zh': {'title': 'Prefix Grouper：提升 GRPO 的可扩展性', 'desc': 'Prefix Grouper 是一种高效的 GRPO 训练算法，通过共享前缀的前向策略，消除了冗余的前缀计算，从而减少了计算开销。该方法将自注意力结构重组为两个部分，使得共享前缀只需编码一次，同时保持完全的可微性和与端到端训练的兼容性。实验结果表明，Prefix Grouper 在长前缀场景中显著降低了训练的计算成本，同时确保优化动态和最终策略性能不变。该方法可以无缝集成到现有的 GRPO 架构中，支持更大的组大小，从而提高 GRPO 在复杂任务和大模型中的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.04255', 'title': 'HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization', 'url': 'https://huggingface.co/papers/2506.04255', 'abstract': 'HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a "CEO" agent dynamically managing specialized "employee" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU\'s capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.', 'score': 4, 'issue_id': 4189, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'd3d7d73af3533148', 'authors': ['Kunal Pai', 'Parth Shah', 'Harshil Patel'], 'affiliations': ['Independent Researcher', 'UC Davis'], 'pdf_title_img': 'assets/pdf/title_img/2506.04255.jpg', 'data': {'categories': ['#architecture', '#agi', '#optimization', '#benchmark', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Гибкая и эффективная мультиагентная система нового поколения', 'desc': 'HASHIRU - это новая система мультиагентного взаимодействия, которая повышает гибкость, эффективность использования ресурсов и адаптивность. Она использует динамическое управление специализированными агентами и гибридный подход к интеллекту, сочетая небольшие локальные языковые модели и внешние API. HASHIRU включает экономическую модель с затратами на найм/увольнение агентов для стабильности команды и эффективного распределения ресурсов. Система продемонстрировала высокую эффективность в различных задачах, превзойдя некоторые существующие модели.'}, 'en': {'title': 'HASHIRU: Dynamic Intelligence for Efficient Multi-Agent Systems', 'desc': "HASHIRU is a new framework for Multi-Agent Systems (MAS) that improves flexibility and resource efficiency by managing specialized agents dynamically. It uses a hybrid intelligence approach, combining smaller local Large Language Models (LLMs) with external APIs to adapt to different tasks. The framework includes a 'CEO' agent that oversees 'employee' agents based on the specific needs and available resources, promoting efficient team management. Evaluations show HASHIRU's strong performance in various tasks, highlighting its ability to autonomously create tools and manage resources effectively."}, 'zh': {'title': 'HASHIRU：灵活高效的多智能体系统新框架', 'desc': 'HASHIRU是一个新颖的多智能体系统（MAS）框架，旨在提高灵活性、资源效率和适应性。它通过动态管理专门的代理（如“CEO”代理和“员工”代理）来满足任务需求和资源限制。该框架采用混合智能，优先使用较小的本地大语言模型（LLM），并在必要时灵活调用外部API和更大的模型。HASHIRU的评估结果显示其在多个任务上表现出色，展示了其在动态控制和资源感知方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2506.05817', 'title': 'CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming', 'url': 'https://huggingface.co/papers/2506.05817', 'abstract': 'An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming, due to its high reasoning difficulty and precise correctness feedback, has become a key task for both training and evaluating the reasoning capabilities of large language models (LLMs). However, while a large amount of public problem data, such as problem statements and solutions, is available, the test cases of these problems are often difficult to obtain. Therefore, test case generation is a necessary task for building large-scale datasets, and the quality of the test cases directly determines the accuracy of the evaluation. In this paper, we introduce an LLM-based agent system that creates high-quality test cases for competitive programming problems. We apply this system to the CodeContests dataset and propose a new version with improved test cases, named CodeContests+. We evaluated the quality of test cases in CodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels to examine the accuracy of these test cases in evaluation. The results indicated that CodeContests+ achieves significantly higher accuracy than CodeContests, particularly with a notably higher True Positive Rate (TPR). Subsequently, our experiments in LLM Reinforcement Learning (RL) further confirmed that improvements in test case quality yield considerable advantages for RL.', 'score': 3, 'issue_id': 4195, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'f0f3a151758192f0', 'authors': ['Zihan Wang', 'Siyao Liu', 'Yang Sun', 'Hongyan Li', 'Kai Shen'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05817.jpg', 'data': {'categories': ['#benchmark', '#rl', '#optimization', '#reasoning', '#dataset', '#data'], 'emoji': '🏆', 'ru': {'title': 'ИИ создает тесты для соревновательного программирования: повышение точности оценки и эффективности обучения', 'desc': 'Представлена система на основе языковой модели (LLM) для генерации высококачественных тестовых примеров для задач соревновательного программирования. Система применяется к набору данных CodeContests, создавая улучшенную версию CodeContests+. Оценка качества тестовых примеров показала значительно более высокую точность CodeContests+, особенно в отношении истинно положительного коэффициента (TPR). Эксперименты с обучением с подкреплением (RL) подтвердили преимущества улучшенных тестовых примеров для RL.'}, 'en': {'title': 'Enhancing Competitive Programming Evaluation with LLM-Generated Test Cases', 'desc': 'This paper presents a system that uses large language models (LLMs) to generate high-quality test cases for competitive programming problems. The generation of these test cases is crucial because they enhance the evaluation accuracy of models and improve reinforcement learning (RL) performance. The authors introduce a new dataset, CodeContests+, which contains improved test cases derived from the original CodeContests dataset. Their evaluation shows that the new test cases significantly increase the True Positive Rate (TPR) and overall accuracy, demonstrating the benefits of high-quality test case generation for model training and assessment.'}, 'zh': {'title': '基于LLM的高质量测试用例生成', 'desc': '本文介绍了一种基于大型语言模型（LLM）的系统，该系统能够为竞争编程问题生成高质量的测试用例。这项技术提高了模型评估的准确性，并增强了强化学习（RL）的性能。由于竞争编程问题的测试用例难以获取，生成测试用例成为构建大规模数据集的必要任务。我们的实验表明，改进后的测试用例在准确性上显著优于原始数据集，尤其在真实正例率（TPR）方面表现突出。'}}}, {'id': 'https://huggingface.co/papers/2506.06091', 'title': 'MIRIAD: Augmenting LLMs with millions of medical query-response pairs', 'url': 'https://huggingface.co/papers/2506.06091', 'abstract': 'MIRIAD, a large-scale, curated medical QA corpus, enhances LLM accuracy and hallucination detection in healthcare applications.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are bound to transform healthcare with advanced decision support and flexible chat assistants. However, LLMs are prone to generate inaccurate medical content. To ground LLMs in high-quality medical knowledge, LLMs have been equipped with external knowledge via RAG, where unstructured medical knowledge is split into small text chunks that can be selectively retrieved and integrated into the LLMs context. Yet, existing RAG pipelines rely on raw, unstructured medical text, which can be noisy, uncurated and difficult for LLMs to effectively leverage. Systematic approaches to organize medical knowledge to best surface it to LLMs are generally lacking. To address these challenges, we introduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs, each rephrased from and grounded in a passage from peer-reviewed medical literature using a semi-automated pipeline combining LLM generation, filtering, grounding, and human annotation. Unlike prior medical corpora, which rely on unstructured text, MIRIAD encapsulates web-scale medical knowledge in an operationalized query-response format, which enables more targeted retrieval. Experiments on challenging medical QA benchmarks show that augmenting LLMs with MIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with the same source corpus and with the same amount of retrieved text. Moreover, MIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to 37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive map of MIRIAD spanning 56 medical disciplines, enabling clinical users to visually explore, search, and refine medical knowledge. MIRIAD promises to unlock a wealth of down-stream applications, including medical information retrievers, enhanced RAG applications, and knowledge-grounded chat interfaces, which ultimately enables more reliable LLM applications in healthcare.', 'score': 2, 'issue_id': 4204, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '29d01b51d792bee2', 'authors': ['Qinyue Zheng', 'Salman Abdullah', 'Sam Rawal', 'Cyril Zakka', 'Sophie Ostmeier', 'Maximilian Purk', 'Eduardo Reis', 'Eric J. Topol', 'Jure Leskovec', 'Michael Moor'], 'affiliations': ['Center for Artificial Intelligence in Medicine and Imaging, Stanford, CA, USA', 'Department of Biosystems Science and Engineering, ETH Zurich, Basel, Switzerland', 'Department of Computer Science, Stanford University, Stanford, CA, USA', 'Department of Internal Medicine, Mayo Clinic, Phoenix, AZ, USA', 'Department of Radiology, Stanford University, Stanford, CA, USA', 'Hasso-Plattner-Institute for Digital Engineering, University of Potsdam, Potsdam, Germany', 'Hugging Face, New York City, NY, USA', 'Scripps Translational Science Institute, San Diego, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06091.jpg', 'data': {'categories': ['#rag', '#science', '#hallucinations', '#healthcare', '#dataset', '#data'], 'emoji': '🩺', 'ru': {'title': 'MIRIAD: Повышение надежности языковых моделей в медицине', 'desc': 'MIRIAD - это масштабный кураторский корпус медицинских вопросов и ответов, разработанный для повышения точности языковых моделей в здравоохранении. Он содержит более 5,8 миллионов пар вопросов-ответов, созданных на основе рецензируемой медицинской литературы. Использование MIRIAD в качестве источника знаний для языковых моделей улучшает точность ответов на медицинские вопросы на 6,7% по сравнению с базовыми методами. Кроме того, MIRIAD повышает способность языковых моделей обнаруживать медицинские галлюцинации на 22,5-37%.'}, 'en': {'title': 'MIRIAD: Elevating Healthcare AI with Curated Knowledge', 'desc': 'MIRIAD is a large, curated medical question-answering corpus designed to improve the accuracy of large language models (LLMs) in healthcare. It consists of over 5.8 million QA pairs derived from peer-reviewed medical literature, ensuring high-quality and reliable information. By using a semi-automated pipeline that combines LLM generation and human annotation, MIRIAD provides structured medical knowledge that enhances the retrieval-augmented generation (RAG) process. Experiments show that integrating MIRIAD with LLMs significantly boosts accuracy and reduces the occurrence of medical hallucinations, making it a valuable resource for healthcare applications.'}, 'zh': {'title': 'MIRIAD：提升医疗LLM准确性与幻觉检测的关键', 'desc': 'MIRIAD是一个大规模的医学问答语料库，旨在提高大型语言模型（LLM）在医疗应用中的准确性和幻觉检测能力。该语料库包含5821948对医学问答，经过半自动化流程从同行评审的医学文献中提炼而来，确保了医学知识的高质量和结构化。与以往依赖于非结构化文本的医学语料库不同，MIRIAD采用了操作化的查询-响应格式，使得LLM能够更有效地检索和利用医学知识。实验结果表明，使用MIRIAD增强LLM的准确性提高了6.7%，并且幻觉检测能力提升了22.5%到37%。'}}}, {'id': 'https://huggingface.co/papers/2506.05673', 'title': "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning\n  Vision Models from DataSeeds' Annotated Imagery", 'url': 'https://huggingface.co/papers/2506.05673', 'abstract': 'The DataSeeds.AI dataset enhances computer vision models by providing high-quality, peer-ranked images with extensive annotations, leading to improved performance over existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of modern Artificial Intelligence (AI) models, particularly diffusion-based models employed in computer vision and image generation tasks, is undergoing a paradigmatic shift in development methodologies. Traditionally dominated by a "Model Centric" approach, in which performance gains were primarily pursued through increasingly complex model architectures and hyperparameter optimization, the field is now recognizing a more nuanced "Data-Centric" approach. This emergent framework foregrounds the quality, structure, and relevance of training data as the principal driver of model performance. To operationalize this paradigm shift, we introduce the DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately 10,610 high-quality human peer-ranked photography images accompanied by extensive multi-tier annotations. The DSD is a foundational computer vision dataset designed to usher in a new standard for commercial image datasets. Representing a small fraction of DataSeed.AI\'s 100 million-plus image catalog, the DSD provides a scalable foundation necessary for robust commercial and multimodal AI development. Through this in-depth exploratory analysis, we document the quantitative improvements generated by the DSD on specific models against known benchmarks and make the code and the trained models used in our evaluation publicly available.', 'score': 2, 'issue_id': 4202, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '7fbe95059e679a3f', 'authors': ['Sajjad Abdoli', 'Freeman Lewin', 'Gediminas Vasiliauskas', 'Fabian Schonholz'], 'affiliations': ['Emet Research', 'FESSEX', 'Perle.ai', 'Zedge'], 'pdf_title_img': 'assets/pdf/title_img/2506.05673.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#cv', '#multimodal', '#diffusion'], 'emoji': '🌱', 'ru': {'title': 'Качественные данные - ключ к прогрессу в компьютерном зрении', 'desc': "Статья представляет набор данных DataSeeds.AI (DSD), состоящий из 10 610 высококачественных фотографий с подробными аннотациями для задач компьютерного зрения. DSD демонстрирует переход от 'модельно-центричного' к 'дата-центричному' подходу в разработке ИИ моделей. Исследование показывает количественные улучшения производительности моделей, обученных на DSD, по сравнению с существующими бенчмарками. Авторы предоставляют публичный доступ к коду и обученным моделям для дальнейшего изучения."}, 'en': {'title': 'Elevating AI with Quality Data: The DataSeeds.AI Revolution', 'desc': "The paper introduces the DataSeeds.AI dataset, which enhances computer vision models by providing high-quality, peer-ranked images with detailed annotations. It emphasizes a shift from a 'Model Centric' approach to a 'Data-Centric' approach in AI development, highlighting the importance of data quality over model complexity. The dataset consists of around 10,610 carefully curated images that serve as a benchmark for improving model performance. The authors demonstrate the effectiveness of the dataset through quantitative analysis and make their evaluation tools publicly accessible."}, 'zh': {'title': '数据驱动，提升模型性能的新时代', 'desc': 'DataSeeds.AI 数据集通过提供高质量、同行评审的图像和详细注释，提升了计算机视觉模型的性能，超越了现有基准。该研究强调了数据中心的方法，认为训练数据的质量和结构是模型性能的主要驱动因素。我们介绍的 DataSeeds.AI 样本数据集包含约 10,610 张高质量的摄影图像，配有多层次的注释，旨在为商业图像数据集设立新标准。通过对特定模型的定量分析，我们展示了 DSD 在已知基准上的性能提升，并公开了评估中使用的代码和训练模型。'}}}, {'id': 'https://huggingface.co/papers/2506.05579', 'title': 'When Models Know More Than They Can Explain: Quantifying Knowledge\n  Transfer in Human-AI Collaboration', 'url': 'https://huggingface.co/papers/2506.05579', 'abstract': "Research investigates human-AI knowledge transfer through a large-scale study, revealing that AI performance does not consistently correlate with human understanding, requiring dedicated optimization for effective communication.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in AI reasoning have driven substantial improvements across diverse tasks. A critical open question is whether these improvements also yields better knowledge transfer: the ability of models to communicate reasoning in ways humans can understand, apply, and learn from. To investigate this, we introduce Knowledge Integration and Transfer Evaluation (KITE), a conceptual and experimental framework for Human-AI knowledge transfer capabilities and conduct the first large-scale human study (N=118) explicitly designed to measure it. In our two-phase setup, humans first ideate with an AI on problem-solving strategies, then independently implement solutions, isolating model explanations' influence on human understanding. Our findings reveal that although model benchmark performance correlates with collaborative outcomes, this relationship is notably inconsistent, featuring significant outliers, indicating that knowledge transfer requires dedicated optimization. Our analysis identifies behavioral and strategic factors mediating successful knowledge transfer. We release our code, dataset, and evaluation framework to support future work on communicatively aligned models.", 'score': 2, 'issue_id': 4199, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '451fe5ddfc2da95f', 'authors': ['Quan Shi', 'Carlos E. Jimenez', 'Shunyu Yao', 'Nick Haber', 'Diyi Yang', 'Karthik Narasimhan'], 'affiliations': ['OpenAI', 'Princeton Language and Intelligence', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05579.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#dataset', '#benchmark', '#reasoning', '#rlhf', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Эффективность ИИ не гарантирует понятность для человека', 'desc': 'Исследование посвящено передаче знаний между искусственным интеллектом и человеком. Проведено масштабное исследование с участием 118 человек, использующее фреймворк KITE для оценки способности ИИ передавать знания. Результаты показали, что эффективность ИИ в тестах не всегда коррелирует с его способностью объяснять концепции людям. Выявлено, что для улучшения передачи знаний требуется специальная оптимизация моделей машинного обучения.'}, 'en': {'title': 'Optimizing AI for Better Human Understanding', 'desc': 'This paper explores how well artificial intelligence (AI) can share knowledge with humans, focusing on the transfer of understanding between the two. The researchers developed a framework called Knowledge Integration and Transfer Evaluation (KITE) to assess how effectively AI can communicate its reasoning to humans. They conducted a large study with 118 participants to see how AI explanations affected human problem-solving. The results showed that while AI performance on benchmarks is related to collaborative success, it does not always guarantee effective knowledge transfer, highlighting the need for specific optimizations in AI communication.'}, 'zh': {'title': '优化人机知识转移，提升沟通效果', 'desc': '本研究探讨了人类与人工智能之间的知识转移，发现人工智能的表现与人类理解并不总是相关，因此需要专门的优化以实现有效的沟通。我们提出了知识整合与转移评估（KITE）框架，进行了一项大规模的人类研究，旨在测量人类与人工智能的知识转移能力。研究结果表明，尽管模型的基准性能与合作结果有一定的相关性，但这种关系并不稳定，存在显著的异常值，表明知识转移需要专门的优化。我们的分析还识别了影响成功知识转移的行为和策略因素。'}}}, {'id': 'https://huggingface.co/papers/2506.04755', 'title': 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.04755', 'abstract': "A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample's potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at https://github.com/Leo-ssl/RAP.", 'score': 2, 'issue_id': 4194, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e4a5cd694d56b2a0', 'authors': ['Shenshen Li', 'Kaiyuan Deng', 'Lei Wang', 'Hao Yang', 'Chong Peng', 'Peng Yan', 'Fumin Shen', 'Heng Tao Shen', 'Xing Xu'], 'affiliations': ['Meituan', 'Salesforce AI Research', 'School of Computer Science and Technology, Tongji University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04755.jpg', 'data': {'categories': ['#multimodal', '#data', '#optimization', '#dataset', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное мультимодальное обучение: меньше данных, больше рассуждений', 'desc': "Статья представляет новую парадигму отбора данных под названием Reasoning Activation Potential (RAP) для улучшения мультимодального рассуждения в больших языковых моделях. RAP идентифицирует ключевые 'когнитивные' образцы данных, которые стимулируют подлинное мультимодальное рассуждение, используя два оценщика: Causal Discrepancy Estimator и Attention Confidence Estimator. Метод также включает модуль Difficulty-aware Replacement для замены тривиальных примеров на более сложные. Эксперименты показывают, что RAP достигает превосходных результатов, используя всего 9.3% обучающих данных, при этом снижая вычислительные затраты на 43%."}, 'en': {'title': 'Unlocking Multi-Modal Reasoning with Less Data: The RAP Approach', 'desc': 'The paper introduces a new method called Reasoning Activation Potential (RAP) to improve multi-modal reasoning in large language models (MLLMs) using smaller, high-value datasets. It challenges the belief that large amounts of training data are necessary for effective reasoning, showing that only a small subset of samples, known as cognitive samples, can trigger meaningful reasoning. RAP employs two estimators: the Causal Discrepancy Estimator (CDE) to filter out less relevant samples and the Attention Confidence Estimator (ACE) to focus on important tokens during reasoning. The results demonstrate that RAP can enhance performance while significantly reducing the amount of data and computational resources needed.'}, 'zh': {'title': '推理激活潜力：高效的多模态推理新方法', 'desc': '本文提出了一种新的数据选择范式，称为推理激活潜力（RAP），旨在通过使用最小的高价值数据集来增强大型语言模型的多模态推理能力。研究表明，真正的多模态推理只需少量关键样本，而大多数样本的贡献微乎其微。RAP通过两个互补的估计器来识别这些关键样本，从而提高模型的性能并降低计算成本。实验结果显示，RAP方法在仅使用9.3%的训练数据的情况下，性能显著优于传统方法，同时计算成本降低超过43%。'}}}, {'id': 'https://huggingface.co/papers/2506.04120', 'title': 'Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data', 'url': 'https://huggingface.co/papers/2506.04120', 'abstract': 'A hybrid real-to-sim framework combining 3D Gaussian Splatting and physics simulation with MuJoCo allows simultaneous high-fidelity object reconstruction and accurate robot pose calibration from raw trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines.', 'score': 2, 'issue_id': 4194, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'd808acd50e3bfd0c', 'authors': ['Ben Moran', 'Mauro Comi', 'Steven Bohez', 'Tom Erez', 'Zhibin Li', 'Leonard Hasenclever'], 'affiliations': ['Google DeepMind', 'University College London', 'University of Bristol'], 'pdf_title_img': 'assets/pdf/title_img/2506.04120.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Гибридная реконструкция сцен для точного моделирования роботов', 'desc': 'Статья представляет новую гибридную систему для создания реалистичных симуляций роботов на основе реальных данных. Она объединяет 3D Gaussian Splatting для фотореалистичного рендеринга с физическим моделированием в MuJoCo. Система позволяет одновременно реконструировать геометрию объектов с высокой точностью и калибровать положение робота без дополнительных аннотаций. Этот подход решает проблемы окклюзий, шума в данных и динамических элементов сцены, делая создание цифровых двойников более точным и практичным.'}, 'en': {'title': 'Bridging Reality and Simulation for Enhanced Robot Learning', 'desc': 'This paper presents a new framework that combines 3D Gaussian Splatting and physics simulation to improve robot learning from real-world data. It addresses challenges like occlusions and noisy camera poses that make it hard to create accurate digital models of objects. The authors introduce a hybrid representation that merges photorealistic rendering with physics-compatible object meshes, allowing for better simulations. Their end-to-end optimization process refines object geometry, appearance, and robot poses from raw data, achieving high-fidelity reconstructions and accurate pose calibration without needing annotations.'}, 'zh': {'title': '实现真实与仿真的完美结合', 'desc': '本论文提出了一种混合的真实到仿真框架，结合了3D高斯点云和MuJoCo物理仿真，能够从原始轨迹中同时实现高保真物体重建和准确的机器人姿态校准。该框架解决了真实机器人数据中的遮挡、噪声相机姿态和动态场景元素等挑战，创建几何上准确且逼真的数字双胞胎。我们提出了一种新的场景表示方法，将3D高斯点云的光照真实渲染与适合物理仿真的物体网格结合在一起。通过端到端的优化流程，我们能够直接从不精确的机器人轨迹中优化所有场景组件，提升了物体重建的精度和机器人姿态的校准效果。'}}}, {'id': 'https://huggingface.co/papers/2506.00649', 'title': 'GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction', 'url': 'https://huggingface.co/papers/2506.00649', 'abstract': 'GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com', 'score': 2, 'issue_id': 4194, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '32639eb393594459', 'authors': ['Neil De La Fuente', 'Oscar Sainz', 'Iker García-Ferrero', 'Eneko Agirre'], 'affiliations': ['HiTZ Basque Center for Language Technology - Ixa NLP Group', 'Technical University of Munich (TUM)', 'University of the Basque Country (UPV/EHU)'], 'pdf_title_img': 'assets/pdf/title_img/2506.00649.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#synthetic', '#training', '#benchmark'], 'emoji': '🏷️', 'ru': {'title': 'Автоматическое создание схем и руководств для улучшения zero-shot NER', 'desc': 'GUIDEX - это новый метод, который автоматически определяет схемы для конкретных доменов, выводит руководства и генерирует синтетически размеченные примеры для задачи распознавания именованных сущностей (NER). Он позволяет улучшить обобщение на новые домены без использования размеченных человеком данных. Дообучение модели Llama 3.1 с помощью GUIDEX устанавливает новый state-of-the-art результат на семи бенчмарках по zero-shot NER. Модели, обученные с GUIDEX, демонстрируют улучшенное понимание сложных, специфичных для доменов схем аннотации.'}, 'en': {'title': 'Revolutionizing Zero-Shot NER with GUIDEX!', 'desc': 'GUIDEX is a new approach that improves zero-shot Named Entity Recognition (NER) by automatically creating schemas and guidelines for different domains. Traditional NER systems need a lot of human effort to design schemas and label data, which can be expensive and time-consuming. GUIDEX uses Large Language Models to generate synthetic labeled data, helping models perform better in new, unseen domains. By fine-tuning Llama 3.1 with GUIDEX, researchers achieved significant improvements in NER performance, setting new records without relying on extensive human-labeled datasets.'}, 'zh': {'title': 'GUIDEX：零-shot NER的新突破', 'desc': 'GUIDEX是一种新方法，旨在增强零-shot命名实体识别（NER）的能力。它通过自动定义领域特定的模式和推断指导方针，减少了对大量人工标注数据的依赖。使用GUIDEX微调Llama 3.1模型，在七个零-shot NER基准测试中创造了新的最佳成绩。与之前的方法相比，使用GUIDEX训练的模型在没有人工标注数据的情况下，F1分数提高了7分，结合使用时也提高了近2分。'}}}, {'id': 'https://huggingface.co/papers/2506.05551', 'title': 'When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding', 'url': 'https://huggingface.co/papers/2506.05551', 'abstract': 'A framework using Transformer layers with strong attention and a coarse-to-fine strategy reduces semantic hallucination in LMMs for visual perception and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding.', 'score': 1, 'issue_id': 4205, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'e73b3a44a1c359fb', 'authors': ['Yan Shu', 'Hangui Lin', 'Yexin Liu', 'Yan Zhang', 'Gangyan Zeng', 'Yan Li', 'Yu Zhou', 'Ser-Nam Lim', 'Harry Yang', 'Nicu Sebe'], 'affiliations': ['HKUST', 'IIE, CAS', 'NJUST', 'NKU', 'UCAS', 'UCF', 'UIR', 'UNITN'], 'pdf_title_img': 'assets/pdf/title_img/2506.05551.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#cv', '#multimodal', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Борьба с семантическими галлюцинациями в мультимодальных моделях', 'desc': 'Статья представляет новый подход к уменьшению семантических галлюцинаций в крупных мультимодальных моделях (LMM) для визуального восприятия и рассуждения. Авторы обнаружили, что слои трансформера с более сильным вниманием к текстовым областям изображения менее склонны к семантическим галлюцинациям. На основе этого наблюдения они разработали безтренировочный фреймворк, включающий стратегию ZoomText для идентификации потенциальных текстовых областей и метод Grounded Layer Correction для адаптивного использования внутренних представлений слоев, менее подверженных галлюцинациям. Для оценки эффективности подхода авторы создали бенчмарк TextHalu-Bench, содержащий более 1730 образцов с вручную составленными парами вопрос-ответ.'}, 'en': {'title': 'Reducing Semantic Hallucination in Visual Models with Attention', 'desc': "This paper presents a new framework to reduce semantic hallucination in Large Multimodal Models (LMMs) that deal with visual perception and reasoning. The authors identify that stronger attention in Transformer layers helps these models focus better on text regions, leading to fewer hallucinations. They introduce two main components: ZoomText, which uses a coarse-to-fine approach to locate text without needing external detectors, and Grounded Layer Correction, which uses reliable internal representations to correct errors in the model's outputs. The effectiveness of their approach is validated through a new benchmark, TextHalu-Bench, which tests the model's ability to handle both semantic and non-semantic text cases."}, 'zh': {'title': '减少语义幻觉的创新框架', 'desc': '本文提出了一种框架，利用Transformer层的强注意力机制和粗到细的策略，减少大型多模态模型（LMMs）在视觉感知和推理中的语义幻觉现象。研究发现，具有更强注意力的Transformer层在处理场景文本时，较少产生语义幻觉。我们提出的框架包括两个关键组件：ZoomText用于识别潜在文本区域，而Grounded Layer Correction则利用不易产生幻觉的内部表示来修正解码输出。通过引入TextHalu-Bench基准，我们的实验表明，该方法有效减轻了语义幻觉，并在场景文本识别和理解的公共基准上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.03828', 'title': 'AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial\n  Asset Operations and Maintenance', 'url': 'https://huggingface.co/papers/2506.03828', 'abstract': 'A unified framework, AssetOpsBench, is introduced to enable end-to-end automation of industrial asset lifecycle management through domain-specific AI agents.  \t\t\t\t\tAI-generated summary \t\t\t\t AI for Industrial Asset Lifecycle Management aims to automate complex operational workflows -- such as condition monitoring, maintenance planning, and intervention scheduling -- to reduce human workload and minimize system downtime. Traditional AI/ML approaches have primarily tackled these problems in isolation, solving narrow tasks within the broader operational pipeline. In contrast, the emergence of AI agents and large language models (LLMs) introduces a next-generation opportunity: enabling end-to-end automation across the entire asset lifecycle. This paper envisions a future where AI agents autonomously manage tasks that previously required distinct expertise and manual coordination. To this end, we introduce AssetOpsBench -- a unified framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications. We outline the key requirements for such holistic systems and provide actionable insights into building agents that integrate perception, reasoning, and control for real-world industrial operations. The software is available at https://github.com/IBM/AssetOpsBench.', 'score': 0, 'issue_id': 4203, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '38f96a4cd72b114d', 'authors': ['Dhaval Patel', 'Shuxin Lin', 'James Rayfield', 'Nianjun Zhou', 'Roman Vaculin', 'Natalia Martinez', "Fearghal O'donncha", 'Jayant Kalagnanam'], 'affiliations': ['IBM Research - Ireland', 'IBM Research - Yorktown'], 'pdf_title_img': 'assets/pdf/title_img/2506.03828.jpg', 'data': {'categories': ['#architecture', '#science', '#agents', '#optimization', '#agi', '#multimodal'], 'emoji': '🏭', 'ru': {'title': 'ИИ-агенты для комплексного управления промышленными активами', 'desc': 'AssetOpsBench - это унифицированная платформа для автоматизации управления жизненным циклом промышленных активов с помощью ИИ-агентов. Она позволяет разрабатывать, оркестрировать и оценивать агентов для приложений Индустрии 4.0. Платформа интегрирует восприятие, рассуждение и управление для реальных промышленных операций. AssetOpsBench нацелена на полную автоматизацию сложных рабочих процессов, таких как мониторинг состояния, планирование обслуживания и составление графиков вмешательств.'}, 'en': {'title': 'Revolutionizing Industrial Asset Management with AI Automation', 'desc': 'The paper presents AssetOpsBench, a comprehensive framework aimed at automating the entire lifecycle management of industrial assets using specialized AI agents. It addresses the limitations of traditional AI/ML methods that typically focus on isolated tasks, by proposing a system that integrates various operational workflows like condition monitoring and maintenance planning. By leveraging advancements in AI agents and large language models, the framework facilitates seamless end-to-end automation, reducing the need for human intervention. The authors provide guidelines for developing these agents, emphasizing the importance of combining perception, reasoning, and control in real-world industrial settings.'}, 'zh': {'title': '实现工业资产管理的全面自动化', 'desc': '本文介绍了一个统一框架AssetOpsBench，旨在通过特定领域的人工智能代理实现工业资产生命周期管理的端到端自动化。该框架能够自动化复杂的操作工作流程，如状态监测、维护计划和干预调度，从而减少人力负担并最小化系统停机时间。与传统的人工智能/机器学习方法不同，AssetOpsBench能够跨整个资产生命周期实现全面的自动化，管理以前需要不同专业知识和手动协调的任务。我们提供了构建集成感知、推理和控制的代理的关键要求和可行性见解，以支持工业4.0应用。'}}}, {'id': 'https://huggingface.co/papers/2505.20698', 'title': 'Sparsified State-Space Models are Efficient Highway Networks', 'url': 'https://huggingface.co/papers/2505.20698', 'abstract': 'Simba, a hierarchical sparsification method for state-space models, enhances efficiency and information flow in natural language tasks by pruning tokens more aggressively in upper layers.  \t\t\t\t\tAI-generated summary \t\t\t\t State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba.', 'score': 0, 'issue_id': 4204, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '348a1f437846d3e7', 'authors': ['Woomin Song', 'Jihoon Tack', 'Sangwoo Mo', 'Seunghyuk Oh', 'Jinwoo Shin'], 'affiliations': ['Korea Advanced Institute of Science & Technology (KAIST)', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2505.20698.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '🦁', 'ru': {'title': 'Simba: Умное прореживание для быстрых и эффективных языковых моделей', 'desc': 'Статья представляет новый метод Simba для иерархической разреженности моделей пространства состояний (SSM) в задачах обработки естественного языка. Simba применяет более агрессивное прореживание токенов в верхних слоях модели, что улучшает эффективность и поток информации. Метод основан на новом критерии прореживания токенов, измеряющем глобальное влияние токенов на конечный результат. Эксперименты показывают, что Simba превосходит базовую модель Mamba при том же количестве FLOPS в различных задачах обработки языка.'}, 'en': {'title': 'Simba: Sparsifying State-Space Models for Efficient NLP', 'desc': 'This paper introduces Simba, a method designed to improve the efficiency of state-space models (SSMs) in natural language processing tasks. By applying hierarchical sparsification, Simba prunes tokens more aggressively in the upper layers of the model, which tend to be redundant. This approach allows for better information flow and reduces computational costs while maintaining performance. The authors demonstrate that Simba outperforms the baseline model, Mamba, in various tasks, highlighting its effectiveness in managing long sequences.'}, 'zh': {'title': 'Simba：提升状态空间模型效率的稀疏化方法', 'desc': 'Simba是一种针对状态空间模型的分层稀疏化方法，旨在提高自然语言任务中的效率和信息流。该方法通过在上层更积极地修剪令牌，减少冗余信息的传递，优化了模型的计算性能。研究表明，上层通常包含更多的全局信息，而下层则关注局部信息，因此Simba在上层进行更多的稀疏化处理。实验结果显示，Simba在多个自然语言任务中优于基线模型Mamba，且在相同的计算预算下实现了更好的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.09513', 'title': 'ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09513', 'abstract': 'ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.', 'score': 79, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '7c6aa342a51b1d59', 'authors': ['Yu Sun', 'Xingyu Qian', 'Weiwen Xu', 'Hao Zhang', 'Chenghao Xiao', 'Long Li', 'Yu Rong', 'Wenbing Huang', 'Qifeng Bai', 'Tingyang Xu'], 'affiliations': ['Alibaba DAMO Academy', 'Beĳing Key Laboratory of Research on Large Models', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation', 'Gaoling School of', 'Hupan Lab', 'Renmin University of China', 'School of Basic Medical Sciences, Lanzhou University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09513.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#healthcare', '#reasoning', '#training'], 'emoji': '🩺', 'ru': {'title': 'ReasonMed: Прорыв в медицинских вопросно-ответных системах на основе ИИ', 'desc': 'ReasonMed - это крупнейший набор данных для медицинских рассуждений, состоящий из 370 тысяч высококачественных примеров. Он был создан с помощью многоагентного процесса верификации и уточнения, включающего специальный Error Refiner для улучшения цепочек рассуждений. Исследование показало, что комбинация подробных рассуждений по методу Chain-of-Thought с краткими сводками ответов является наиболее эффективной стратегией для обучения моделей медицинских рассуждений. На основе этой стратегии была обучена модель ReasonMed-7B, которая превзошла предыдущие модели на 4.17% и даже превзошла LLaMA3.1-70B на 4.60% в задаче PubMedQA.'}, 'en': {'title': 'Enhancing Medical AI with ReasonMed: A New Benchmark in Reasoning', 'desc': 'ReasonMed is a comprehensive medical reasoning dataset designed to improve the performance of medical question answering models. It consists of 370,000 high-quality examples derived from 1.7 million initial reasoning paths created by various large language models (LLMs). The dataset is refined through a multi-agent process that includes an Error Refiner to correct mistakes in reasoning paths. By combining detailed Chain-of-Thought reasoning with concise summaries, ReasonMed-7B achieves superior results, surpassing previous benchmarks for smaller models and even outperforming larger models on specific tasks.'}, 'zh': {'title': 'ReasonMed：提升医学问答模型的新基准', 'desc': 'ReasonMed是一个大型医学推理数据集，旨在提高医学问答模型的准确性。它结合了详细的推理路径和简洁的总结，创造了新的模型性能基准。该数据集包含370,000个高质量示例，经过多代理验证和精炼过程构建而成。通过结合详细的思维链推理和简洁的答案总结，ReasonMed-7B模型在医学问答任务中表现优异，超越了之前的最佳模型。'}}}, {'id': 'https://huggingface.co/papers/2506.10954', 'title': 'SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks', 'url': 'https://huggingface.co/papers/2506.10954', 'abstract': 'An automated pipeline, SWE-Factory, is introduced to facilitate the creation of large-scale datasets for evaluating and training Large Language Models in GitHub issue resolution tasks, offering efficient environment building, standardized grading, and automated validation.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory.', 'score': 43, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'dfb4cf3e253468bd', 'authors': ['Lianghong Guo', 'Yanlin Wang', 'Caihua Li', 'Pengyu Yang', 'Jiachi Chen', 'Wei Tao', 'Yingtian Zou', 'Duyu Tang', 'Zibin Zheng'], 'affiliations': ['Huawei', 'Independent Researcher', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10954.jpg', 'data': {'categories': ['#data', '#science', '#dataset', '#agents', '#benchmark', '#open_source'], 'emoji': '🏭', 'ru': {'title': 'SWE-Factory: автоматизация создания датасетов для LLM в разработке ПО', 'desc': 'SWE-Factory - это автоматизированный конвейер для создания масштабных датасетов для оценки и обучения больших языковых моделей (LLM) в задачах разрешения проблем на GitHub. Он включает в себя SWE-Builder - мультиагентную систему для автоматизированного построения сред оценки, стандартизированный метод оценки на основе кодов выхода и автоматизированную валидацию fail2pass. Эксперименты показали эффективность конвейера в создании валидных экземпляров задач, точность оценки и высокую производительность валидации. Этот подход призван ускорить сбор масштабных, качественных датасетов для обучения и оценки LLM в задачах разработки программного обеспечения.'}, 'en': {'title': 'Automating Dataset Creation for LLMs in GitHub Issue Resolution', 'desc': 'The paper presents SWE-Factory, an automated pipeline designed to streamline the creation of large-scale datasets for training and evaluating Large Language Models (LLMs) in GitHub issue resolution tasks. It addresses the challenges of environment setup, grading, and validation by integrating three automated components: SWE-Builder for environment construction, a standardized exit-code-based grading system, and an automated fail2pass validation process. Experiments demonstrate that SWE-Factory can efficiently generate valid task instances at a low cost while achieving high accuracy in grading and validation. This innovation aims to enhance the quality and speed of dataset collection for LLM training and evaluation.'}, 'zh': {'title': '自动化管道加速GitHub问题解决数据集构建', 'desc': '本文介绍了一种名为SWE-Factory的自动化管道，旨在简化大规模数据集的创建，以评估和训练大型语言模型在GitHub问题解决任务中的表现。传统的数据集构建过程繁琐且耗时，尤其是在环境搭建、结果评分和任务验证阶段。SWE-Factory通过集成三个核心自动化组件来解决这些问题，包括自动化环境构建的多代理系统SWE-Builder、基于退出代码的标准化评分方法，以及自动化的fail2pass验证过程。实验结果表明，该管道能够有效构建有效的任务实例，并在评分和验证方面表现出高准确率和高精度。'}}}, {'id': 'https://huggingface.co/papers/2506.10910', 'title': 'Magistral', 'url': 'https://huggingface.co/papers/2506.10910', 'abstract': "A scalable reinforcement learning pipeline for training reasoning models demonstrates improvements in multimodal understanding, instruction following, and function calling without relying on existing implementations.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.", 'score': 36, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '59d3abdb994da001', 'authors': ['Mistral-AI', ':', 'Abhinav Rastogi', 'Albert Q. Jiang', 'Andy Lo', 'Gabrielle Berrada', 'Guillaume Lample', 'Jason Rute', 'Joep Barmentlo', 'Karmesh Yadav', 'Kartik Khandelwal', 'Khyathi Raghavi Chandu', 'Léonard Blier', 'Lucile Saulnier', 'Matthieu Dinot', 'Maxime Darrin', 'Neha Gupta', 'Roman Soletskyi', 'Sagar Vaze', 'Teven Le Scao', 'Yihan Wang', 'Adam Yang', 'Alexander H. Liu', 'Alexandre Sablayrolles', 'Amélie Héliou', 'Amélie Martin', 'Andy Ehrenberg', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste Rozière', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'Clémence Lanfranchi', 'Darius Dabert', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jean-Hadrien Chabran', 'Jean-Malo Delignon', 'Joachim Studnia', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Kush Jain', 'Lingxiao Zhao', 'Louis Martin', 'Luyu Gao', 'Lélio Renard Lavaud', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Maximilian Augustin', 'Mickaël Seznec', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'Philomène Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Romain Sauvestre', 'Rémi Delacourt', 'Sanchit Gandhi', 'Sandeep Subramanian', 'Shashwat Dalal', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'Timothée Lacroix', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yunhao Tang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.10910.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning', '#open_source', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты для моделей рассуждений', 'desc': 'Исследователи представили Magistral - модель рассуждений, обученную с помощью масштабируемого конвейера обучения с подкреплением (RL). Этот подход не опирается на существующие реализации, а использует только собственные модели и инфраструктуру компании. Эксперименты показали, что RL на текстовых данных сохраняет или улучшает мультимодальное понимание, следование инструкциям и вызов функций. Авторы открыли исходный код Magistral Small и представили Magistral Medium для задач рассуждения.'}, 'en': {'title': 'Revolutionizing Reasoning with Pure Reinforcement Learning', 'desc': 'This paper presents Magistral, a new reasoning model developed using a scalable reinforcement learning (RL) pipeline. The authors emphasize a novel approach that does not depend on previous implementations or RL traces from other models, focusing instead on their own infrastructure. They demonstrate that training with pure RL on text data can preserve or enhance capabilities in multimodal understanding, instruction following, and function calling. Additionally, they introduce Magistral Medium, which is built on Mistral Medium 3, and make Magistral Small available as open-source software.'}, 'zh': {'title': '可扩展的强化学习管道，提升推理模型能力', 'desc': '本文介绍了Magistral，这是Mistral的第一个推理模型，以及我们自己的可扩展强化学习（RL）管道。我们采用自下而上的方法，完全依赖于自己的模型和基础设施，而不是现有的实现和从先前模型中提取的RL轨迹。研究表明，纯文本数据的RL训练能够保持或改善多模态理解、指令跟随和功能调用的能力。我们还发布了Magistral Medium和开源的Magistral Small，进一步支持推理训练。'}}}, {'id': 'https://huggingface.co/papers/2506.10540', 'title': 'AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation', 'url': 'https://huggingface.co/papers/2506.10540', 'abstract': "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.", 'score': 35, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '5c11232a01ad90bb', 'authors': ['Haoyuan Shi', 'Yunxin Li', 'Xinyu Chen', 'Longyue Wang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce, Hangzhou, China', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10540.jpg', 'data': {'categories': ['#optimization', '#video', '#multimodal', '#story_generation', '#agents'], 'emoji': '🎬', 'ru': {'title': 'AniMaker: умная система для создания анимационных историй из текста', 'desc': 'AniMaker - это многоагентная система для создания анимационных видеороликов по текстовому описанию. Она использует метод Монте-Карло (MCTS-Gen) для эффективной генерации видеоклипов и специальную систему оценки AniEval для выбора наиболее подходящих фрагментов. AniMaker состоит из нескольких агентов, включая режиссера, оператора, рецензента и монтажера. Эксперименты показывают, что AniMaker превосходит существующие модели по качеству и эффективности генерации анимационных историй.'}, 'en': {'title': 'AniMaker: Crafting Coherent Stories from Text with AI', 'desc': 'AniMaker is a multi-agent framework designed to generate coherent storytelling videos from text input, addressing challenges in video generation. It utilizes specialized agents for different tasks, such as storyboard creation and video clip generation, ensuring a consistent narrative flow. The framework incorporates MCTS-Gen for efficient clip generation and AniEval for evaluating animation quality, focusing on story coherence and visual continuity. Experiments show that AniMaker outperforms existing models in both quality and efficiency, making AI-generated storytelling animation more viable for production.'}, 'zh': {'title': 'AniMaker：高效生成连贯故事视频的智能框架', 'desc': 'AniMaker是一个多智能体框架，利用MCTS-Gen和AniEval，从文本输入生成连贯的故事视频，超越了现有模型的质量和效率。该框架通过专门的智能体，如导演智能体、摄影智能体、评审智能体和后期制作智能体，来实现高效的多候选片段生成和故事意识片段选择。AniMaker的核心技术包括MCTS-Gen，它是一种高效的蒙特卡洛树搜索策略，能够智能地导航候选空间，生成高潜力的片段，同时优化资源使用；以及AniEval，这是第一个专门为多镜头动画评估设计的框架。实验表明，AniMaker在质量和效率上均显著优于现有方法，推动了AI生成的故事动画更接近生产标准。'}}}, {'id': 'https://huggingface.co/papers/2506.09993', 'title': 'Text-Aware Image Restoration with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.09993', 'abstract': 'The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/', 'score': 35, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '91f1bb97ef062632', 'authors': ['Jaewon Min', 'Jin Hyeon Kim', 'Paul Hyunbin Cho', 'Jaeeun Lee', 'Jihye Park', 'Minkyu Park', 'Sangpil Kim', 'Hyunhee Park', 'Seungryong Kim'], 'affiliations': ['KAIST AI', 'Korea University', 'Samsung Electronics', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09993.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion', '#hallucinations'], 'emoji': '📝', 'ru': {'title': 'Восстановление изображений с сохранением текстовой информации', 'desc': 'Предложенная система восстановления изображений с учетом текста (TAIR) объединяет многозадачную диффузионную модель с модулем распознавания текста для улучшения как восстановления изображения, так и точности текста. Система превосходит существующие методы на основе диффузии, которые часто создают правдоподобные, но неверные текстоподобные паттерны. Авторы представили SA-Text - крупномасштабный набор данных из 100 тысяч высококачественных изображений с разнообразными текстовыми аннотациями. Предложенная модель TeReDiff интегрирует внутренние признаки диффузионных моделей в модуль распознавания текста, что позволяет извлекать богатые текстовые представления для последующих шагов шумоподавления.'}, 'en': {'title': 'Restoring Images with Textual Precision', 'desc': 'The Text-Aware Image Restoration (TAIR) system addresses the challenge of restoring images while maintaining the accuracy of textual information. Traditional diffusion-based methods often produce incorrect text patterns, leading to what is known as text-image hallucination. TAIR introduces a multi-task diffusion framework, TeReDiff, which combines image restoration with a text-spotting module to improve both visual and textual fidelity. By leveraging a large-scale dataset of annotated images, TAIR significantly enhances text recognition accuracy compared to existing methods.'}, 'zh': {'title': '文本感知图像修复：提升图像与文本的双重恢复', 'desc': '本文提出了一种名为文本感知图像修复（TAIR）的系统，旨在同时恢复图像内容和文本的准确性。现有的扩散基础修复方法在自然图像修复方面表现良好，但在处理图像中的文本区域时常常出现错误的文本模式。TAIR系统结合了多任务扩散框架和文本检测模块，通过联合训练提高了文本识别的准确性。实验结果表明，TAIR在图像修复和文本保真度方面均优于现有的修复方法。'}}}, {'id': 'https://huggingface.co/papers/2506.10857', 'title': 'VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos', 'url': 'https://huggingface.co/papers/2506.10857', 'abstract': "VRBench evaluates long video understanding by assessing multi-step reasoning capabilities across temporal and procedural validity using human-labeled question-answering pairs and reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.", 'score': 30, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '7eabd83f69c00df7', 'authors': ['Jiashuo Yu', 'Yue Wu', 'Meng Chu', 'Zhifei Ren', 'Zizheng Huang', 'Pei Chu', 'Ruijie Zhang', 'Yinan He', 'Qirui Li', 'Songze Li', 'Zhenxiang Li', 'Zhongying Tu', 'Conghui He', 'Yu Qiao', 'Yali Wang', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.10857.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#reasoning', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'VRBench: Оценка глубокого понимания видео через многоступенчатые рассуждения', 'desc': 'VRBench - это новый бенчмарк для оценки способностей моделей машинного обучения к многоступенчатым рассуждениям на основе длинных видео. Он включает в себя 1010 длинных видео со средней продолжительностью 1,6 часа, а также 9468 пар вопросов-ответов и 30292 шага рассуждений с временными метками, размеченных людьми. VRBench оценивает модели как на уровне конечных результатов, так и на уровне процесса рассуждений, используя многофазный конвейер оценки. Бенчмарк был протестирован на 12 языковых моделях (LLM) и 16 визуально-языковых моделях (VLM), предоставив ценные выводы для развития области многоступенчатых рассуждений.'}, 'en': {'title': 'VRBench: Advancing Multi-Step Reasoning in Long Video Understanding', 'desc': 'VRBench is a new benchmark designed to evaluate how well large models understand long videos through multi-step reasoning. It includes 1,010 long videos and thousands of human-labeled question-answering pairs, focusing on both temporal reasoning and procedural validity. The framework allows for the generation of coherent reasoning chains that require multiple steps, assessing models not just on final answers but also on the reasoning process. By testing various large language models (LLMs) and vision-language models (VLMs), VRBench aims to provide insights that enhance the understanding of multi-step reasoning in video comprehension.'}, 'zh': {'title': 'VRBench：长视频理解的新基准', 'desc': 'VRBench是一个用于评估长视频理解的基准，专注于多步骤推理能力，特别是时间推理和程序有效性。该基准包含1010个长视频，平均时长为1.6小时，以及9468个人工标注的多步骤问答对和30292个带时间戳的推理步骤。通过多阶段筛选过程，确保视频情节连贯性，并开发了一个人机协作框架，生成需要多个时间基础步骤的连贯推理链。VRBench设计了一个多阶段评估流程，综合评估模型的结果和过程，推动了多步骤推理领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10274', 'title': 'Discrete Audio Tokens: More Than a Survey!', 'url': 'https://huggingface.co/papers/2506.10274', 'abstract': 'A systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains is presented, covering their taxonomy, evaluation metrics, and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.', 'score': 24, 'issue_id': 4282, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '636451c0543dc586', 'authors': ['Pooneh Mousavi', 'Gallil Maimon', 'Adel Moumen', 'Darius Petermann', 'Jiatong Shi', 'Haibin Wu', 'Haici Yang', 'Anastasia Kuznetsova', 'Artem Ploujnikov', 'Ricard Marxer', 'Bhuvana Ramabhadran', 'Benjamin Elizalde', 'Loren Lugosch', 'Jinyu Li', 'Cem Subakan', 'Phil Woodland', 'Minje Kim', 'Hung-yi Lee', 'Shinji Watanabe', 'Yossi Adi', 'Mirco Ravanelli'], 'affiliations': ['Apple', 'Carnegie Mellon University', 'Concordia University', 'Google', 'Indiana University', 'Laval University', 'Microsoft', 'Mila-Quebec AI Institute', 'National Taiwan University', 'The Hebrew University of Jerusalem', 'University of Cambridge', 'University of Illinois at Urbana-Champaign', 'Université de Montréal', 'Université de Toulon'], 'pdf_title_img': 'assets/pdf/title_img/2506.10274.jpg', 'data': {'categories': ['#survey', '#benchmark', '#audio'], 'emoji': '🎵', 'ru': {'title': 'Аудиотокенизация: от речи до музыки', 'desc': 'В статье представлен систематический обзор и сравнительный анализ дискретных аудиотокенизаторов в областях речи, музыки и общего аудио. Авторы предлагают таксономию подходов к токенизации на основе различных критериев, включая архитектуру энкодер-декодер и методы квантования. Проводится оценка токенизаторов по нескольким критериям, таким как качество реконструкции и эффективность в нисходящих задачах. Исследование выявляет ключевые ограничения и открытые проблемы в этой быстро развивающейся области машинного обучения.'}, 'en': {'title': 'Unlocking the Power of Discrete Audio Tokenization', 'desc': 'This paper reviews and benchmarks different methods of discrete audio tokenization used in speech, music, and general audio. Discrete audio tokens are efficient representations that maintain important audio characteristics while allowing for better storage and processing in large language models. The authors categorize various tokenization techniques and evaluate their performance across multiple tasks, highlighting their strengths and weaknesses. The study aims to provide a comprehensive understanding of the current landscape of audio tokenizers and identify future research directions.'}, 'zh': {'title': '离散音频标记器的系统评估与比较', 'desc': '本文系统回顾并基准测试了离散音频标记器在语音、音乐和一般音频领域的表现。离散音频标记是紧凑的表示方式，旨在保留感知质量、语音内容和说话者特征，同时实现高效存储和推理。我们提出了一种基于编码器-解码器、量化技术、训练范式、流式处理和应用领域的标记化方法分类。研究结果揭示了关键的局限性和未来研究的挑战，为音频处理领域的进一步发展提供了指导。'}}}, {'id': 'https://huggingface.co/papers/2506.10978', 'title': 'Fine-Grained Perturbation Guidance via Attention Head Selection', 'url': 'https://huggingface.co/papers/2506.10978', 'abstract': 'The paper proposes HeadHunter, a systematic framework for selecting attention heads in Diffusion Transformer architectures to enable precise control over image generation quality and style, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head\'s attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.', 'score': 20, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'a646cb2c7b825b74', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Minjae Kim', 'Jaewon Min', 'Wooseok Jang', 'Saungwu Lee', 'Sayak Paul', 'Susung Hong', 'Seungryong Kim'], 'affiliations': ['HuggingFace', 'KAIST', 'KAIST AI', 'Korea University', 'Krea AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.10978.jpg', 'data': {'categories': ['#interpretability', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'Точное управление генерацией изображений через выбор голов внимания', 'desc': 'Статья представляет HeadHunter - систему для выбора голов внимания в архитектурах Diffusion Transformer для точного контроля качества и стиля генерации изображений. Авторы обнаружили, что определенные головы внимания отвечают за различные визуальные концепции, такие как структура, стиль и качество текстуры. HeadHunter позволяет итеративно выбирать головы внимания в соответствии с целями пользователя, обеспечивая детальный контроль над генерацией. Метод превосходит существующие подходы как в улучшении общего качества, так и в управлении конкретными стилями.'}, 'en': {'title': 'HeadHunter: Precision Control in Image Generation with Attention Heads', 'desc': 'The paper introduces HeadHunter, a framework designed to select specific attention heads in Diffusion Transformer architectures for better control over image generation quality and style. It addresses the limitations of existing attention perturbation methods by providing a systematic approach to determine where perturbations should be applied, focusing on individual attention heads rather than entire layers. The authors demonstrate that different heads are responsible for distinct visual concepts, allowing for targeted manipulation of attributes like structure and texture. By implementing SoftPAG, they offer a method to fine-tune perturbation strength, leading to improved image quality and style guidance in large-scale text-to-image models.'}, 'zh': {'title': '精准控制图像生成的注意力头选择框架', 'desc': '本文提出了HeadHunter，这是一个系统化框架，用于选择扩散变换器架构中的注意力头，以实现对图像生成质量和风格的精确控制，超越了现有方法。我们研究了注意力扰动的粒度，从层级到单个注意力头，发现特定的头控制着不同的视觉概念，如结构、风格和纹理质量。基于这一发现，我们提出了“HeadHunter”，通过迭代选择与用户目标一致的注意力头，实现对生成质量和视觉属性的细粒度控制。此外，我们引入了SoftPAG，提供了一个连续的调节工具，以调整扰动强度并抑制伪影。'}}}, {'id': 'https://huggingface.co/papers/2506.10952', 'title': 'Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training', 'url': 'https://huggingface.co/papers/2506.10952', 'abstract': 'Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%.', 'score': 20, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f9cd5a16ae7e2cc3', 'authors': ['Mozhi Zhang', 'Howe Tissue', 'Lu Wang', 'Xipeng Qiu'], 'affiliations': ['Ritzz-AI', 'School of Computer Science, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10952.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#optimization', '#data'], 'emoji': '🧩', 'ru': {'title': 'Умное разложение данных для эффективного обучения языковых моделей', 'desc': 'Domain2Vec - это новый подход к декомпозиции датасетов на мета-домены для оптимизации предобучения языковых моделей. Он использует классификатор для представления датасета в виде распределения по словарю мета-доменов. Это позволяет эффективно находить оптимальную смесь данных для предобучения, улучшая производительность на целевых задачах при меньших вычислительных затратах. Эксперименты показали, что Domain2Vec достигает той же валидационной ошибки на Pile-CC, используя лишь 51.5% вычислений по сравнению с обучением на оригинальной смеси данных The Pile.'}, 'en': {'title': 'Optimize Language Models with Domain2Vec!', 'desc': "Domain2Vec is a new method that breaks down datasets into smaller parts called meta-domains to improve the training of language models. It uses a classifier to create a domain vector that represents the dataset's features, allowing for better alignment between training and validation data distributions. This approach helps in selecting the best combination of data for pretraining language models while using less computational power. Experiments show that Domain2Vec can achieve similar performance with significantly reduced computational costs, enhancing efficiency in machine learning tasks."}, 'zh': {'title': 'Domain2Vec：优化语言模型的高效数据分解方法', 'desc': 'Domain2Vec是一种新颖的方法，它将数据集分解为多个元域的线性组合，以优化语言模型的预训练和下游性能，同时降低计算成本。该方法维护一个元域词汇表，并使用分类器将给定数据集分解为对应于该词汇表的域向量。这些域向量能够在不进行训练的情况下，根据分布对齐假设（DA²）识别出最佳的数据混合，从而降低验证损失。此外，Domain2Vec可以无缝集成到之前的工作中，建模域向量与语言模型性能之间的关系，显著提高了效率和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.09344', 'title': 'Ming-Omni: A Unified Multimodal Model for Perception and Generation', 'url': 'https://huggingface.co/papers/2506.09344', 'abstract': 'Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.', 'score': 20, 'issue_id': 4273, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'b9166301d93dd2bb', 'authors': ['Inclusion AI', 'Biao Gong', 'Cheng Zou', 'Chuanyang Zheng', 'Chunluan Zhou', 'Canxiang Yan', 'Chunxiang Jin', 'Chunjie Shen', 'Dandan Zheng', 'Fudong Wang', 'Furong Xu', 'GuangMing Yao', 'Jun Zhou', 'Jingdong Chen', 'Jianxin Sun', 'Jiajia Liu', 'Jianjiang Zhu', 'Jun Peng', 'Kaixiang Ji', 'Kaiyou Song', 'Kaimeng Ren', 'Libin Wang', 'Lixiang Ru', 'Lele Xie', 'Longhua Tan', 'Lyuxin Xue', 'Lan Wang', 'Mochen Bai', 'Ning Gao', 'Pei Chen', 'Qingpei Guo', 'Qinglong Zhang', 'Qiang Xu', 'Rui Liu', 'Ruijie Xiong', 'Sirui Gao', 'Tinghao Liu', 'Taisong Li', 'Weilong Chai', 'Xinyu Xiao', 'Xiaomei Wang', 'Xiaoxue Chen', 'Xiao Lu', 'Xiaoyu Li', 'Xingning Dong', 'Xuzheng Yu', 'Yi Yuan', 'Yuting Gao', 'Yunxiao Sun', 'Yipeng Chen', 'Yifei Wu', 'Yongjie Lyu', 'Ziping Ma', 'Zipeng Feng', 'Zhijiang Fang', 'Zhihao Qiu', 'Ziyuan Huang', 'Zhengyu He'], 'affiliations': ['Ant Group', 'Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09344.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для всех модальностей: восприятие и генерация в одном', 'desc': 'Статья представляет Ming-Omni - унифицированную мультимодальную модель, способную обрабатывать изображения, текст, аудио и видео. Модель использует специализированные энкодеры для извлечения токенов из разных модальностей и архитектуру MoE с модальноспецифичными роутерами для их обработки. Ming-Omni поддерживает генерацию аудио и изображений, а также контекстно-зависимый чат и редактирование изображений. Экспериментальные результаты показывают, что Ming-Omni предлагает мощное решение для унифицированного восприятия и генерации во всех модальностях.'}, 'en': {'title': 'Ming-Omni: One Model, Many Modalities!', 'desc': 'Ming-Omni is a cutting-edge multimodal model designed to handle various types of data, including images, text, audio, and video. It utilizes dedicated encoders to extract information from these different modalities and employs a mixture of experts (MoE) architecture with modality-specific routers for efficient processing. This innovative design allows Ming-Omni to perform a wide range of tasks, such as generating speech and images, engaging in context-aware conversations, and editing images, all within a single framework. By being open-source and matching the capabilities of advanced models like GPT-4o, Ming-Omni aims to foster further research and development in the field of multimodal AI.'}, 'zh': {'title': 'Ming-Omni：统一多模态处理的强大解决方案', 'desc': 'Ming-Omni是一种统一的多模态模型，能够处理图像、文本、音频和视频。它使用专用编码器提取不同模态的特征，并通过新提出的模态特定路由器进行处理。该模型支持语音和图像生成，能够进行上下文感知的对话和多功能的图像编辑。Ming-Omni是首个开源模型，能够在多模态支持上与GPT-4o相媲美，促进了社区的进一步研究和发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10741', 'title': 'PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework', 'url': 'https://huggingface.co/papers/2506.10741', 'abstract': 'PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft', 'score': 19, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '5c43d96d0a604ef8', 'authors': ['SiXiang Chen', 'Jianyu Lai', 'Jialin Gao', 'Tian Ye', 'Haoyu Chen', 'Hengyu Shi', 'Shitong Shao', 'Yunlong Lin', 'Song Fei', 'Zhaohu Xing', 'Yeying Jin', 'Junfeng Luo', 'Xiaoming Wei', 'Lei Zhu'], 'affiliations': ['Meituan', 'National University of Singapore', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10741.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#architecture', '#open_source', '#dataset', '#data', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект создает эстетичные постеры нового уровня', 'desc': 'PosterCraft - это унифицированная система для создания эстетичных постеров с использованием искусственного интеллекта. Она включает в себя оптимизацию рендеринга текста, обучение с подкреплением для улучшения эстетики и совместную обработку визуальной и текстовой информации. Система использует каскадный рабочий процесс, включающий обучение на больших наборах данных и тонкую настройку с учетом регионов изображения. PosterCraft превосходит существующие открытые решения по точности рендеринга, согласованности макета и общей визуальной привлекательности.'}, 'en': {'title': "Elevating Poster Design with PosterCraft's Unified Framework", 'desc': 'PosterCraft is a novel framework designed to enhance the generation of aesthetic posters by integrating advanced techniques in text rendering and layout optimization. It utilizes a modular pipeline that includes region-aware fine-tuning and aesthetic reinforcement learning to improve the visual quality of the generated images. The framework operates on a cascaded workflow, leveraging large-scale datasets for training and optimizing each component for better performance. Experimental results show that PosterCraft surpasses existing open-source models in rendering accuracy and overall aesthetic appeal, making it competitive with state-of-the-art commercial systems.'}, 'zh': {'title': 'PosterCraft：美学海报生成的新突破', 'desc': 'PosterCraft 是一个改进美学海报生成的统一模块化框架。它通过增强的文本渲染、区域感知微调、美学强化学习和联合视觉-语言优化，提升了海报的生成质量。该框架允许模型自由探索视觉上引人注目的组合，克服了传统模块化管道的局限性。经过多项实验评估，PosterCraft 在渲染精度、布局一致性和整体视觉吸引力方面显著优于开源基线，接近最先进的商业系统的质量。'}}}, {'id': 'https://huggingface.co/papers/2506.10357', 'title': 'Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts', 'url': 'https://huggingface.co/papers/2506.10357', 'abstract': "Optimus-3, an agent using knowledge-enhanced data generation, Mixture-of-Experts routing, and multimodal reasoning-augmented reinforcement learning, achieves superior performance across various tasks in Minecraft.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/", 'score': 18, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '145045d8e634c76b', 'authors': ['Zaijing Li', 'Yuquan Xie', 'Rui Shao', 'Gongwei Chen', 'Weili Guan', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10357.jpg', 'data': {'categories': ['#optimization', '#architecture', '#agents', '#rag', '#rl', '#reasoning', '#multimodal', '#games'], 'emoji': '🤖', 'ru': {'title': 'Optimus-3: Универсальный ИИ-агент покоряет Minecraft', 'desc': 'Статья представляет Optimus-3 - интеллектуального агента для игры Minecraft, использующего усовершенствованные методы машинного обучения. Агент применяет генерацию данных с использованием знаний, маршрутизацию на основе смеси экспертов и обучение с подкреплением, дополненное мультимодальными рассуждениями. Optimus-3 демонстрирует превосходную производительность в различных задачах в открытом мире Minecraft. Это достижение преодолевает ключевые проблемы, такие как недостаток специфических данных, интерференция между разнородными задачами и визуальное разнообразие в открытых средах.'}, 'en': {'title': 'Optimus-3: Mastering Minecraft with Advanced AI Techniques', 'desc': 'Optimus-3 is a general-purpose agent designed for the open-world environment of Minecraft, utilizing advanced techniques in machine learning. It incorporates a knowledge-enhanced data generation pipeline to create high-quality training data, addressing the challenge of insufficient domain-specific data. The agent employs a Mixture-of-Experts (MoE) architecture to effectively manage interference among diverse tasks, allowing for better performance. Additionally, it uses Multimodal Reasoning-Augmented Reinforcement Learning to improve its reasoning capabilities, enabling it to handle the visual diversity present in Minecraft.'}, 'zh': {'title': 'Optimus-3：在Minecraft中超越极限的智能体', 'desc': '本文介绍了Optimus-3，一个利用知识增强数据生成、专家混合路由和多模态推理增强强化学习的智能体。该智能体在Minecraft等开放世界环境中表现出色，解决了领域特定数据不足、异构任务干扰和视觉多样性等挑战。我们提出了一种知识增强的数据生成管道，以提供可扩展的高质量训练数据，并引入了任务级路由的专家混合架构来减轻任务间的干扰。此外，我们开发了多模态推理增强的强化学习方法，以提升智能体在视觉多样性方面的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.09967', 'title': 'Resa: Transparent Reasoning Models via SAEs', 'url': 'https://huggingface.co/papers/2506.09967', 'abstract': "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.", 'score': 18, 'issue_id': 4274, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '03a45cae6d11be64', 'authors': ['Shangshang Wang', 'Julian Asilis', 'Ömer Faruk Akgül', 'Enes Burak Bilgin', 'Ollie Liu', 'Deqing Fu', 'Willie Neiswanger'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09967.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#optimization', '#small_models', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение рассуждению с помощью разреженных автоэнкодеров', 'desc': 'Эта статья представляет метод SAE-Tuning для эффективного улучшения способностей языковых моделей к рассуждению. Метод использует разреженные автоэнкодеры (SAE) для извлечения навыков рассуждения из исходной модели и применения их к целевой модели. SAE-Tuning позволяет достичь производительности, сравнимой с RL-обучением, при значительно меньших затратах времени и ресурсов. Исследование также показывает, что извлеченные навыки рассуждения обладают свойствами обобщаемости и модульности.'}, 'en': {'title': 'Efficient Reasoning Enhancement in Language Models with SAE-Tuning', 'desc': 'The paper introduces SAE-Tuning, a method that enhances reasoning capabilities in language models using sparse autoencoders. This approach allows for significant performance improvements without the need for extensive retraining, achieving cost reductions of over 2000 times and time savings of over 450 times. By training a sparse autoencoder to capture reasoning skills from a source model, the method effectively guides the fine-tuning of a target model using verified question-answer data. The results show that the reasoning abilities gained are both generalizable across datasets and modular, allowing for easy integration into different models without retraining.'}, 'zh': {'title': '高效推理：稀疏自编码器调优的力量', 'desc': 'SAE-Tuning是一种高效的稀疏自编码器调优方法，能够在语言模型中引发强大的推理能力，而无需进行大量的重新训练。该方法首先训练一个稀疏自编码器（SAE），以从源模型中捕捉推理能力，然后利用训练好的SAE指导标准的监督微调过程，从而在目标模型中引发这些能力。通过这种方式，SAE-Tuning在保持推理性能的同时，显著降低了训练成本和时间。研究表明，提取的推理能力具有可泛化和模块化的特性，可以在不同的数据集和模型之间灵活应用。'}}}, {'id': 'https://huggingface.co/papers/2506.09250', 'title': 'Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.09250', 'abstract': 'Evaluation artifacts, particularly token limits and impractical instances in benchmarks, lead to misreported failures in Large Reasoning Models on planning puzzles.  \t\t\t\t\tAI-generated summary \t\t\t\t Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors\' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.', 'score': 18, 'issue_id': 4291, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '2157b13738f4dbea', 'authors': ['C. Opus', 'A. Lawsen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09250.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Переосмысление оценки ИИ: за пределами поверхностных ограничений', 'desc': 'Статья рассматривает проблемы в оценке способностей больших моделей рассуждения (LRM) при решении головоломок планирования. Авторы выявляют три ключевые проблемы в предыдущих исследованиях: ограничения по токенам, неправильная классификация ошибок и наличие нерешаемых задач в тестовых наборах. Они демонстрируют, что при устранении этих артефактов модели показывают высокую точность в решении задач, ранее считавшихся непосильными. Исследование подчеркивает важность тщательного планирования экспериментов при оценке возможностей искусственного интеллекта в области рассуждений.'}, 'en': {'title': 'Rethinking Evaluation: Uncovering True AI Reasoning Power', 'desc': "This paper discusses how certain evaluation methods can misrepresent the performance of Large Reasoning Models (LRMs) on planning puzzles. The authors identify that issues like token limits and impractical benchmark instances lead to what they call 'accuracy collapse'. They argue that many reported failures are due to experimental design flaws rather than actual reasoning deficiencies in the models. By adjusting the evaluation approach, they show that LRMs can perform well on previously deemed difficult tasks, emphasizing the need for better testing methods in AI research."}, 'zh': {'title': '评估设计影响推理模型表现的关键', 'desc': '这篇论文探讨了大型推理模型在规划难题上的评估问题，特别是由于评估工具的限制导致的错误报告。研究表明，模型在复杂度超过某个阈值时出现的“准确性崩溃”主要是由于实验设计的缺陷，而非模型推理能力的根本失败。分析指出了三个关键问题，包括实验超出模型输出的令牌限制，以及评估框架未能区分推理失败与实际限制。通过控制这些实验伪影，初步实验显示模型在之前被报告为完全失败的塔汉诺实例上表现出高准确率，强调了在评估AI推理能力时谨慎设计实验的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.10821', 'title': 'VideoDeepResearch: Long Video Understanding With Agentic Tool Using', 'url': 'https://huggingface.co/papers/2506.10821', 'abstract': "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.", 'score': 16, 'issue_id': 4273, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'e1e5003f31573e97', 'authors': ['Huaying Yuan', 'Zheng Liu', 'Junjie Zhou', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Beijing University of Posts and Telecommunications', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.10821.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#long_context', '#video', '#multimodal', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Агентный подход превосходит мультимодальные модели в понимании длинных видео', 'desc': 'VideoDeepResearch - это новая агентная система для понимания длинных видео, основанная на текстовой модели рассуждений и модульном наборе инструментов. Система формирует стратегию решения задач через рассуждения, выборочно обращаясь к важному видеоконтенту. Эксперименты показали значительное улучшение результатов по сравнению с существующими мультимодальными языковыми моделями на нескольких бенчмарках. Исследование демонстрирует потенциал агентных систем в преодолении ключевых проблем понимания длинных видео.'}, 'en': {'title': 'Revolutionizing Long Video Understanding with Text-Only Reasoning', 'desc': 'VideoDeepResearch is a novel framework designed to improve long video understanding (LVU) tasks without relying on extended context windows or advanced visual perception capabilities. It utilizes a text-only large reasoning model (LRM) in conjunction with a modular toolkit that includes multimodal retrievers and visual perceivers. This system formulates problem-solving strategies through reasoning and selectively accesses relevant video content as needed. Experimental results show that VideoDeepResearch significantly outperforms existing multi-modal large language models (MLLMs) on various LVU benchmarks, demonstrating its effectiveness in tackling complex video understanding challenges.'}, 'zh': {'title': '突破长视频理解的全新框架', 'desc': 'VideoDeepResearch是一种新型的长视频理解框架，它仅依赖文本推理模型和模块化工具，而不需要扩展上下文窗口或增强视觉感知能力。该系统通过推理制定问题解决策略，并利用多模态工具选择性地访问和使用视频内容。我们在多个长视频理解基准上进行了广泛实验，结果显示VideoDeepResearch在性能上显著超越了现有的多模态大语言模型基线。该研究表明，代理系统在解决长视频理解问题中具有很大的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.10974', 'title': 'AutoMind: Adaptive Knowledgeable Agent for Automated Data Science', 'url': 'https://huggingface.co/papers/2506.10974', 'abstract': 'AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.', 'score': 14, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '50bcb63544ac7586', 'authors': ['Yixin Ou', 'Yujie Luo', 'Jingsheng Zheng', 'Lanning Wei', 'Shuofei Qiao', 'Jintian Zhang', 'Da Zheng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2506.10974.jpg', 'data': {'categories': ['#science', '#training', '#dataset', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AutoMind: ИИ-ассистент нового поколения для автоматизации науки о данных', 'desc': 'AutoMind - это новая гибкая система на основе больших языковых моделей для автоматизации задач в области науки о данных. Она интегрирует экспертные знания, использует стратегический поиск решений и адаптивную генерацию кода. AutoMind превосходит существующие системы благодаря использованию базы знаний, алгоритма поиска на основе дерева решений и самонастраивающейся стратегии кодирования. Система показала высокую эффективность на тестовых наборах данных для автоматизированной науки о данных.'}, 'en': {'title': 'AutoMind: Revolutionizing Automated Data Science with Expert Knowledge and Adaptability', 'desc': 'AutoMind is a new framework designed to enhance automated data science by integrating expert knowledge and adapting its approach based on the complexity of tasks. It utilizes a curated knowledge base to inform its decisions, allowing it to tackle more complex problems than traditional systems. The framework employs a knowledgeable tree search algorithm to explore various solutions strategically, improving its problem-solving capabilities. Evaluations show that AutoMind outperforms existing methods, making it a significant advancement in the field of automated machine learning.'}, 'zh': {'title': 'AutoMind：自动化数据科学的新突破', 'desc': 'AutoMind是一个灵活且知识丰富的LLM代理框架，旨在通过整合专家知识、战略性解决方案探索和自适应编码来提升自动化数据科学的能力。与现有系统相比，AutoMind在处理复杂和创新任务时表现更为出色，克服了传统框架的局限性。它通过建立一个经过筛选的专家知识库、采用智能的知识树搜索算法以及动态调整编码策略，来适应不同任务的复杂性。评估结果表明，AutoMind在自动化数据科学基准测试中超越了现有的最先进方法，展现出高效和稳健的特性。'}}}, {'id': 'https://huggingface.co/papers/2506.10953', 'title': 'Build the web for agents, not agents for the web', 'url': 'https://huggingface.co/papers/2506.10953', 'abstract': 'A paradigm shift in web agent research is proposed, advocating for the development of Agentic Web Interfaces (AWIs) to optimize interaction for AI agents within web environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.', 'score': 14, 'issue_id': 4274, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '77dd06867c379745', 'authors': ['Xing Han Lù', 'Gaurav Kamath', 'Marius Mosbach', 'Siva Reddy'], 'affiliations': ['Equal Advising', 'McGill University', 'Mila', 'Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.10953.jpg', 'data': {'categories': ['#agents', '#agi', '#optimization', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Агентные Веб-Интерфейсы: революция во взаимодействии ИИ с веб-средой', 'desc': 'Статья предлагает новую парадигму для веб-агентов, вводя концепцию Агентных Веб-Интерфейсов (AWI). AWI оптимизированы для взаимодействия ИИ-агентов с веб-средой, преодолевая ограничения интерфейсов, созданных для людей. Авторы устанавливают шесть принципов дизайна AWI, учитывающих безопасность, эффективность и стандартизацию. Этот подход нацелен на создание более надежных и прозрачных веб-агентов, что потребует совместных усилий ML-сообщества.'}, 'en': {'title': 'Redefining Web Interaction for AI Agents with AWIs', 'desc': 'This paper proposes a new approach to web agent research by introducing Agentic Web Interfaces (AWIs), which are specifically designed for AI agents to interact with web environments. Current web interfaces are not optimized for the capabilities of AI, leading to inefficiencies and challenges in task completion. The authors outline six guiding principles for designing AWIs, focusing on safety, efficiency, and standardization to benefit all stakeholders involved. This shift aims to enhance the performance and reliability of web agents, encouraging collaboration within the machine learning community.'}, 'zh': {'title': '为代理设计优化网络交互界面', 'desc': '本文提出了网络代理研究的范式转变，倡导开发代理网络接口（AWI），以优化人工智能代理在网络环境中的交互。随着大型语言模型（LLM）和多模态模型的进步，开发能够自主导航和完成任务的网络代理引起了广泛关注。当前的方法面临着人类设计的界面与LLM能力之间的根本不匹配问题，导致处理复杂网络输入时的困难。本文提出的AWI概念旨在为代理设计专门的交互界面，以提高安全性、效率和标准化，推动更高效、可靠和透明的网络代理设计。'}}}, {'id': 'https://huggingface.co/papers/2506.10960', 'title': 'ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark', 'url': 'https://huggingface.co/papers/2506.10960', 'abstract': 'A benchmark for Chinese harmful content detection, coupled with a knowledge-augmented baseline, improves the performance of smaller models without extensive resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.', 'score': 10, 'issue_id': 4276, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'db6960a49f9467ee', 'authors': ['Kangwei Liu', 'Siyuan Cheng', 'Bozhong Tian', 'Xiaozhuan Liang', 'Yuyang Yin', 'Meng Han', 'Ningyu Zhang', 'Bryan Hooi', 'Xi Chen', 'Shumin Deng'], 'affiliations': ['National University of Singapore', 'Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10960.jpg', 'data': {'categories': ['#multilingual', '#small_models', '#low_resource', '#ethics', '#dataset', '#benchmark'], 'emoji': '🇨🇳', 'ru': {'title': 'Улучшение обнаружения вредоносного контента на китайском языке с помощью знаний', 'desc': 'Представлен новый бенчмарк для обнаружения вредоносного контента на китайском языке, включающий шесть категорий и построенный на реальных данных. Процесс аннотации позволил создать базу знаний с экспертными правилами для помощи языковым моделям. Предложен метод дополнения знаниями, объединяющий аннотированные правила и неявные знания больших языковых моделей. Это позволяет небольшим моделям достигать производительности, сравнимой с современными крупными языковыми моделями, без использования значительных ресурсов.'}, 'en': {'title': 'Empowering Small Models for Chinese Harm Detection', 'desc': 'This paper introduces a new benchmark for detecting harmful content in Chinese, addressing the lack of resources in this area compared to English. It features a dataset that is professionally annotated and covers six categories of harmful content, using real-world examples. The authors also develop a knowledge-augmented baseline that combines expert knowledge with insights from large language models, allowing smaller models to perform effectively without needing extensive resources. This approach enhances the accuracy of harmful content detection in Chinese, making it more accessible for various applications.'}, 'zh': {'title': '提升中文有害内容检测的基准与方法', 'desc': '本论文提出了一个针对中文有害内容检测的基准数据集，涵盖六个代表性类别，并完全基于真实世界数据进行专业标注。现有的有害内容检测资源主要集中在英语，中文数据集相对稀缺且范围有限。我们还构建了一个知识增强的基线模型，结合了人工标注的知识规则和大型语言模型的隐性知识，使得较小的模型在性能上能够与最先进的模型相媲美。该研究为中文有害内容检测提供了重要的资源和方法，提升了内容审核的效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.10890', 'title': 'CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation', 'url': 'https://huggingface.co/papers/2506.10890', 'abstract': 'CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter', 'score': 10, 'issue_id': 4272, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '22fffd0088d280e0', 'authors': ['Zhao Zhang', 'Yutao Cheng', 'Dexiang Hong', 'Maoke Yang', 'Gonglei Shi', 'Lei Ma', 'Hui Zhang', 'Jie Shao', 'Xinglong Wu'], 'affiliations': ['ByteDance, Fudan University', 'ByteDance, Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10890.jpg', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'CreatiPoster: ИИ-революция в графическом дизайне', 'desc': 'CreatiPoster - это новая система искусственного интеллекта для генерации высококачественных графических композиций на основе текста или готовых ресурсов. Она использует модель протокола для создания JSON-спецификации каждого слоя и условную модель фона для синтеза согласованного фона. CreatiPoster превосходит существующие инструменты и шаблоны, обеспечивая редактируемость и профессиональный визуальный appeal. Система поддерживает различные приложения, включая редактирование холста, наложение текста и адаптивное изменение размера.'}, 'en': {'title': 'Revolutionizing Graphic Design with AI-Generated Custom Compositions', 'desc': 'CreatiPoster is a novel framework that generates high-quality, editable graphic designs from user inputs like text or images. It utilizes a protocol model to create a detailed JSON specification for each design layer, ensuring precise layout and style. A conditional background model then generates a cohesive background that complements the foreground elements. This approach not only enhances the editability and visual appeal of designs but also outperforms existing tools and templates in the market.'}, 'zh': {'title': 'CreatiPoster：让图形设计更简单', 'desc': 'CreatiPoster 是一个生成高质量、可编辑和可定制图形作品的框架，能够从文本或资产中创建多层次的图形设计。与现有工具相比，它在用户提供的资产整合、可编辑性和视觉吸引力方面表现更佳。该框架使用协议模型生成详细的 JSON 规范，描述每一层的布局、层次、内容和风格。通过提供一个无版权的 100,000 个多层设计的语料库，CreatiPoster 促进了 AI 辅助图形设计的进一步研究和应用。'}}}, {'id': 'https://huggingface.co/papers/2506.06952', 'title': 'LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer', 'url': 'https://huggingface.co/papers/2506.06952', 'abstract': 'LaTtE-Flow, a new architecture, unifies image understanding and generation with high performance and faster inference by using a Layerwise Timestep Experts flow-based Transformer and Timestep-Conditioned Residual Attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.', 'score': 9, 'issue_id': 4288, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'e034c1178056e190', 'authors': ['Ying Shen', 'Zhiyang Xu', 'Jiuhai Chen', 'Shizhe Diao', 'Jiaxin Zhang', 'Yuguang Yao', 'Joy Rimchala', 'Ismini Lourentzou', 'Lifu Huang'], 'affiliations': ['Intuit AI Research', 'Nvidia', 'UC Davis', 'University of Illinois Urbana-Champaign', 'University of Maryland', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06952.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#architecture', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Единая архитектура для быстрого понимания и генерации изображений', 'desc': 'LaTtE-Flow - это новая архитектура, объединяющая понимание и генерацию изображений с высокой производительностью и более быстрым выводом. Она использует поточный трансформер с экспертами по временным шагам и слоям, а также механизм остаточного внимания, обусловленного временными шагами. LaTtE-Flow основывается на предобученных мультимодальных моделях для наследования сильных возможностей понимания. Эксперименты показывают, что LaTtE-Flow достигает высокой производительности в задачах мультимодального понимания и конкурентоспособного качества генерации изображений примерно в 6 раз быстрее по сравнению с недавними унифицированными мультимодальными моделями.'}, 'en': {'title': 'Unifying Image Understanding and Generation with Speed and Efficiency', 'desc': 'LaTtE-Flow is a new architecture that combines image understanding and generation into one efficient model. It uses a Layerwise Timestep Experts flow-based Transformer to improve the speed and performance of image generation tasks. By activating only specific layers for different timesteps, it enhances sampling efficiency, making it faster than previous models. Additionally, the Timestep-Conditioned Residual Attention mechanism allows for better information sharing across layers, leading to strong results in multimodal tasks.'}, 'zh': {'title': '高效统一图像理解与生成的LaTtE-Flow架构', 'desc': 'LaTtE-Flow是一种新型架构，旨在统一图像理解和生成，具有高性能和更快的推理速度。它采用了分层时间专家流式Transformer和时间条件残差注意力机制，提升了图像生成的效率。通过将流匹配过程分布到专门的Transformer层组中，LaTtE-Flow在每个采样时间步只激活少量层，从而显著提高了采样效率。实验结果表明，LaTtE-Flow在多模态理解任务上表现出色，同时在图像生成质量上也具有竞争力，推理速度比现有统一多模态模型快约6倍。'}}}, {'id': 'https://huggingface.co/papers/2506.10178', 'title': 'Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling', 'url': 'https://huggingface.co/papers/2506.10178', 'abstract': 'Efficient probing, a simplified multi-query cross-attention mechanism, enhances evaluation of self-supervised learning models by improving speed, performance, and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t As fine-tuning (FT) becomes increasingly impractical at scale, probing is emerging as the preferred evaluation protocol for self-supervised learning (SSL). Yet, the standard linear probing (LP) fails to adequately reflect the potential of models trained with Masked Image Modeling (MIM), due to the distributed nature of patch tokens. This motivates the need for attentive probing, an alternative that uses attention to selectively aggregate patch-level features. Despite its growing adoption, attentive probing remains under-explored, with existing methods suffering from excessive parameterization and poor computational efficiency.   In this work, we revisit attentive probing through the lens of the accuracy-efficiency trade-off. We conduct a systematic study of existing methods, analyzing their mechanisms and benchmarking their performance. We introduce efficient probing (EP), a multi-query cross-attention mechanism that eliminates redundant projections, reduces the number of trainable parameters, and achieves up to a 10times speed-up over conventional multi-head attention. Despite its simplicity, EP outperforms LP and prior attentive probing approaches across seven benchmarks, generalizes well beyond MIM to diverse pre-training paradigms, produces interpretable attention maps, and achieves strong gains in low-shot and layer-wise settings. Code available at https://github.com/billpsomas/efficient-probing.', 'score': 7, 'issue_id': 4280, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'e98eb04f1204de95', 'authors': ['Bill Psomas', 'Dionysis Christopoulos', 'Eirini Baltzi', 'Ioannis Kakogeorgiou', 'Tilemachos Aravanis', 'Nikos Komodakis', 'Konstantinos Karantzalos', 'Yannis Avrithis', 'Giorgos Tolias'], 'affiliations': ['Archimedes, Athena RC', 'Czech Technical University in Prague', 'IACM-FORTH', 'IARAI', 'IIT, NCSR Demokritos', 'National Technical University of Athens', 'University of Crete'], 'pdf_title_img': 'assets/pdf/title_img/2506.10178.jpg', 'data': {'categories': ['#interpretability', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зондирование: быстрый и точный метод оценки самообучающихся моделей', 'desc': 'Статья представляет эффективный метод зондирования (efficient probing) для оценки моделей самообучения. Этот метод использует упрощенный механизм кросс-внимания с множественными запросами, что улучшает скорость, производительность и интерпретируемость оценки. Эффективное зондирование превосходит линейное зондирование и предыдущие подходы с использованием внимания на семи эталонных тестах. Метод хорошо обобщается на различные парадигмы предварительного обучения и показывает сильные результаты в условиях малого количества обучающих примеров.'}, 'en': {'title': 'Efficient Probing: Speed and Performance in Self-Supervised Learning', 'desc': 'This paper introduces efficient probing (EP), a new method that enhances the evaluation of self-supervised learning (SSL) models by using a simplified multi-query cross-attention mechanism. Traditional linear probing (LP) does not fully capture the capabilities of models trained with Masked Image Modeling (MIM) due to the complexity of patch tokens. EP addresses this by reducing unnecessary parameters and improving computational efficiency, achieving up to a 10x speed increase compared to standard multi-head attention. The results show that EP not only outperforms LP and previous probing methods but also provides interpretable attention maps and performs well in various settings.'}, 'zh': {'title': '高效探测：提升自监督学习评估的利器', 'desc': '本论文提出了一种高效探测（Efficient Probing）的方法，旨在提升自监督学习模型的评估效率和性能。传统的线性探测方法无法充分反映使用遮挡图像建模训练的模型的潜力，因此需要一种新的关注机制来选择性地聚合特征。高效探测通过多查询交叉注意力机制，减少冗余投影和可训练参数，从而实现更快的计算速度。实验结果表明，高效探测在多个基准测试中表现优于传统方法，并且在低样本和逐层设置中也取得了显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2506.08234', 'title': 'Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions', 'url': 'https://huggingface.co/papers/2506.08234', 'abstract': 'Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.', 'score': 7, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a098935231ad146d', 'authors': ['Yu-Ang Lee', 'Guan-Ting Yi', 'Mei-Yi Liu', 'Jui-Chao Lu', 'Guan-Bo Yang', 'Yun-Nung Chen'], 'affiliations': ['National Taiwan University, Taipei, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2506.08234.jpg', 'data': {'categories': ['#survey', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты оптимизации сложных систем ИИ', 'desc': 'Статья рассматривает последние достижения в оптимизации сложных систем искусственного интеллекта. Авторы анализируют проблемы интеграции различных компонентов, уделяя особое внимание методам обратной связи на естественном языке для недифференцируемых систем. В работе представлен систематический обзор современных подходов к оптимизации составных систем ИИ, включая как численные, так и языковые методы. Исследователи формализуют понятие оптимизации составных систем ИИ, классифицируют существующие методы и выделяют открытые проблемы в этой быстро развивающейся области.'}, 'en': {'title': 'Optimizing Complex AI Systems with Natural Language Feedback', 'desc': 'This paper discusses the recent progress in optimizing compound AI systems, which are complex systems made up of multiple interacting components. It highlights the challenges faced in integrating these components, particularly when using natural language feedback methods for systems that are not easily differentiable. The authors review traditional optimization techniques like supervised fine-tuning and reinforcement learning, while also exploring new approaches that leverage natural language. They aim to formalize the concept of compound AI system optimization and identify future research directions in this evolving field.'}, 'zh': {'title': '优化复合AI系统的新方法探索', 'desc': '最近在复合人工智能系统优化方面的进展突显了整合各种组件的挑战，特别是在非可微系统中使用自然语言反馈方法。随着大型语言模型和人工智能系统的发展，复合人工智能系统在执行复杂任务方面变得更加高效。尽管传统的优化方法如监督微调和强化学习仍然是基础，但自然语言反馈的兴起为优化非可微系统提供了新的可能性。本文系统回顾了复合人工智能系统优化的最新进展，分类现有方法，并强调了该领域的开放研究挑战和未来方向。'}}}, {'id': 'https://huggingface.co/papers/2506.06950', 'title': 'What Makes a Good Natural Language Prompt?', 'url': 'https://huggingface.co/papers/2506.06950', 'abstract': 'A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.', 'score': 7, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '265555e63c6771ba', 'authors': ['Do Xuan Long', 'Duy Dinh', 'Ngoc-Hai Nguyen', 'Kenji Kawaguchi', 'Nancy F. Chen', 'Shafiq Joty', 'Min-Yen Kan'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.06950.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#survey', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация промптов для улучшения рассуждений языковых моделей', 'desc': 'Предложена система оценки и оптимизации естественно-языковых промптов для больших языковых моделей. Авторы проанализировали более 150 статей и выделили 21 свойство промптов, сгруппированных в 6 категорий. Исследование показало корреляции между свойствами качественных промптов и их влиянием на задачи рассуждения. Обнаружено, что обучение моделей на улучшенных промптах повышает их способности к рассуждению.'}, 'en': {'title': 'Optimizing Prompts for Smarter AI Reasoning', 'desc': 'This paper presents a framework for evaluating and optimizing natural language prompts used in large language models (LLMs). It identifies 21 properties of prompts, organized into six dimensions, that influence their effectiveness in reasoning tasks. The authors conducted a meta-analysis of over 150 studies to highlight the inconsistencies in how prompt quality is assessed across different models and tasks. Their findings suggest that enhancing prompts based on specific properties can significantly improve LLM performance, particularly through instruction-tuning techniques.'}, 'zh': {'title': '优化提示，提升推理能力！', 'desc': '本文提出了一个评估和优化大型语言模型中自然语言提示的框架，揭示了提示属性与推理任务之间的相关性。我们通过对2022至2025年间150多篇与提示相关的论文进行元分析，探讨了自然语言提示的量化标准。该框架包含21个属性，分为六个维度，旨在评估提示质量。研究发现，单一属性的增强对推理任务的影响最大，而基于属性增强的指令调优可以提升推理模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.09952', 'title': 'UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2506.09952', 'abstract': "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", 'score': 5, 'issue_id': 4279, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'fc746da54c59982b', 'authors': ['Ziyi Wang', 'Yanran Zhang', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Department of Automation, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09952.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Единый метод предобучения для 3D-данных любого масштаба', 'desc': 'UniPre3D - это унифицированный метод предварительного обучения для облаков точек и 3D-моделей любого масштаба. Он использует гауссовы примитивы и интеграцию 2D-признаков для эффективной работы как с объектами, так и со сценами. Метод применяет дифференцируемое гауссово сплаттинг для рендеринга изображений, что позволяет осуществлять точный попиксельный контроль и сквозную оптимизацию. UniPre3D показал универсальную эффективность в различных задачах на уровне объектов и сцен с использованием разнообразных моделей облаков точек в качестве основы.'}, 'en': {'title': 'UniPre3D: Unified Pre-Training for All 3D Scales', 'desc': "UniPre3D is a novel pre-training method designed for 3D point clouds and models, addressing the challenges posed by varying scales in 3D vision. It uniquely predicts Gaussian primitives as part of its pre-training task and utilizes differentiable Gaussian splatting for accurate image rendering. By integrating 2D features from pre-trained image models, it enhances the model's understanding of geometric structures and textures. Extensive experiments demonstrate its effectiveness across both object and scene-level tasks, making it a versatile solution for 3D representation learning."}, 'zh': {'title': '统一预训练，提升3D视觉表现', 'desc': 'UniPre3D是一种统一的预训练方法，旨在处理各种规模的3D点云和模型。该方法通过预测高斯原语作为预训练任务，并使用可微分的高斯点云渲染技术，实现了精确的像素级监督。为了增强模型对几何结构的关注，UniPre3D还整合了来自预训练图像模型的2D特征，利用已有的纹理知识。通过广泛的实验验证，我们的方法在对象和场景任务中表现出普遍的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.09942', 'title': 'VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following', 'url': 'https://huggingface.co/papers/2506.09942', 'abstract': 'VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.', 'score': 5, 'issue_id': 4272, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5430c32ec46dccaa', 'authors': ['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09942.jpg', 'data': {'categories': ['#dataset', '#optimization', '#rl', '#benchmark', '#open_source', '#reasoning', '#training', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'VerIF: Гибридная верификация для улучшения RL в следовании инструкциям', 'desc': 'Статья представляет VerIF - гибридный метод верификации, сочетающий подходы на основе правил и больших языковых моделей (LLM) для улучшения обучения с подкреплением (RL) в задаче следования инструкциям. Авторы создали набор данных VerInstruct с около 22 000 примеров и сигналами верификации. Применение RL с VerIF к двум моделям показало значительное улучшение производительности на нескольких эталонных тестах и хорошую обобщаемость. Метод может быть интегрирован в существующие рецепты RL для повышения общей эффективности моделей.'}, 'en': {'title': 'VerIF: Boosting Instruction-Following RL with Hybrid Verification', 'desc': 'This paper introduces VerIF, a novel hybrid verification method that merges rule-based and large language model (LLM) approaches to improve reinforcement learning (RL) in instruction-following tasks. The authors highlight the importance of verification engineering in enhancing LLMs through reinforcement learning with verifiable rewards (RLVR). They present a new dataset, VerInstruct, which contains around 22,000 instruction-following instances with verification signals to support their method. The results show that models trained with VerIF achieve state-of-the-art performance and maintain strong generalization capabilities, indicating that this approach can effectively enhance existing RL frameworks.'}, 'zh': {'title': 'VerIF：提升指令跟随的强化学习新方法', 'desc': '本文提出了一种名为VerIF的混合验证方法，结合了基于规则的验证和基于大型语言模型（LLM）的验证，显著提升了指令跟随的强化学习（RL）性能和泛化能力。我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其验证信号，以支持这一方法。通过使用VerIF进行RL训练，我们在多个代表性的指令跟随基准上取得了显著的性能提升，训练后的模型在同类模型中达到了最先进的表现，并且对未见约束具有良好的泛化能力。我们的研究表明，VerIF可以与现有的RL方法结合，进一步增强模型的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08060', 'title': 'Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques', 'url': 'https://huggingface.co/papers/2506.08060', 'abstract': 'Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.', 'score': 5, 'issue_id': 4272, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '5c9cde8c4bcbdc6e', 'authors': ['Asankhaya Sharma'], 'affiliations': ['Patched Codes, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.08060.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#rag', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Трансформеры: обучение в контексте как альтернатива тонкой настройке', 'desc': 'Статья исследует способность трансформеров аппроксимировать возможности обучения с учителем без изменения параметров модели, используя обучение в контексте. Авторы предоставляют теоретические границы и практические методы для этого подхода. Исследование охватывает сценарии с ограниченной длиной контекста и частичным доступом к набору данных. Результаты обосновывают эффективное развертывание больших языковых моделей и связывают теорию с практическими приложениями.'}, 'en': {'title': 'Transformers: Fine-Tuning Efficiency through In-Context Learning', 'desc': "This paper explores how transformers can mimic the performance of supervised fine-tuning (SFT) through a method called in-context learning (ICL) without changing the model's parameters. It provides theoretical proofs that under certain ideal conditions, a base transformer can achieve results similar to those obtained through SFT. The authors extend their findings to practical situations, showing that smaller datasets can still approximate fine-tuned behavior effectively. This research highlights the potential for more efficient use of large language models in real-world applications by leveraging retrieval-augmented generation techniques."}, 'zh': {'title': '变换器模型：高效近似监督微调的未来', 'desc': '本论文探讨了变换器模型如何通过上下文学习（ICL）在不改变模型参数的情况下，近似监督微调（SFT）的能力。研究表明，在理想条件下，变换器模型可以利用推理时的技术来模拟SFT的效果。我们还扩展了这些结果到实际场景，考虑有限的上下文长度和部分数据集访问。通过理论证明，这为大语言模型的资源高效部署提供了基础，结合检索增强生成等实用技术，将理论与实际应用相结合。'}}}, {'id': 'https://huggingface.co/papers/2506.10920', 'title': 'Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization', 'url': 'https://huggingface.co/papers/2506.10920', 'abstract': "SNMF is used to identify interpretable features in LLMs by directly decomposing MLP activations, outperforming SAEs and supervised methods in causal evaluations and aligning with human-interpretable concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.", 'score': 4, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'ce3cd0a96d1ecc71', 'authors': ['Or Shafran', 'Atticus Geiger', 'Mor Geva'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.10920.jpg', 'data': {'categories': ['#architecture', '#data', '#training', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'SNMF: ключ к интерпретации больших языковых моделей', 'desc': 'В статье предлагается метод полуотрицательной матричной факторизации (SNMF) для выявления интерпретируемых признаков в больших языковых моделях (LLM). SNMF напрямую разлагает активации многослойного перцептрона (MLP), что позволяет находить более интерпретируемые и каузально значимые признаки по сравнению с разреженными автоэнкодерами (SAE) и методами обучения с учителем. Эксперименты на моделях Llama 3.1, Gemma 2 и GPT-2 показывают, что признаки, полученные с помощью SNMF, лучше поддаются каузальному управлению и соответствуют понятным человеку концепциям. Анализ также выявляет иерархическую структуру в пространстве активаций MLP, где определенные комбинации нейронов повторно используются для семантически связанных признаков.'}, 'en': {'title': 'Unlocking Interpretability in LLMs with SNMF', 'desc': 'This paper introduces semi-nonnegative matrix factorization (SNMF) as a method to extract interpretable features from large language models (LLMs) by analyzing multi-layer perceptron (MLP) activations. Unlike sparse autoencoders (SAEs), which often fail in causal evaluations, SNMF directly decomposes activations into sparse linear combinations of neurons, making the features more interpretable. The study demonstrates that SNMF outperforms SAEs and supervised methods in identifying causal relationships and aligns better with human-understandable concepts. Additionally, it reveals a hierarchical structure in the activation space, showing how certain neuron combinations are reused across related features.'}, 'zh': {'title': 'SNMF：揭示大型语言模型的可解释特征', 'desc': '本论文提出了一种名为半非负矩阵分解（SNMF）的方法，用于在大型语言模型（LLMs）中识别可解释的特征。与稀疏自编码器（SAEs）和监督方法相比，SNMF在因果评估中表现更好，并且与人类可解释的概念对齐。该方法通过直接分解多层感知器（MLP）的激活，学习到的特征是稀疏的线性组合，并且可以直接映射到其激活输入上，从而提高了可解释性。实验结果表明，SNMF在识别可解释特征和解析概念表示方面是一个简单而有效的工具。'}}}, {'id': 'https://huggingface.co/papers/2506.10911', 'title': 'NoLoCo: No-all-reduce Low Communication Training Method for Large Models', 'url': 'https://huggingface.co/papers/2506.10911', 'abstract': 'NoLoCo is a novel optimization method that eliminates explicit parameter synchronization and reduces communication overhead during the training of large language models, achieving faster convergence rates and reduced idling time compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to 4% faster convergence rate with wide range of model sizes and accelerator counts.', 'score': 4, 'issue_id': 4288, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f39f533144e06990', 'authors': ['Jari Kolehmainen', 'Nikolay Blagoev', 'John Donaghy', 'Oğuzhan Ersoy', 'Christopher Nies'], 'affiliations': ['Gensyn'], 'pdf_title_img': 'assets/pdf/title_img/2506.10911.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'NoLoCo: Эффективное обучение больших языковых моделей без явной синхронизации', 'desc': 'NoLoCo - это новый метод оптимизации для обучения больших языковых моделей, который устраняет необходимость явной синхронизации параметров и снижает накладные расходы на коммуникацию. Метод использует вариант оптимизатора Нестерова, частично усредняя веса модели со случайно выбранной другой моделью. NoLoCo демонстрирует более быструю сходимость и меньшее время простоя по сравнению с существующими методами. Эксперименты показали эффективность NoLoCo для моделей размером от 125 млн до 6,8 млрд параметров на различном количестве ускорителей.'}, 'en': {'title': 'NoLoCo: Faster Training with Less Communication!', 'desc': 'NoLoCo is an innovative optimization technique designed for training large language models without the need for explicit parameter synchronization. By avoiding collective communication, it significantly reduces communication overhead and minimizes idling time among accelerators. The method utilizes a modified Nesterov momentum optimizer that implicitly synchronizes model weights through partial averaging with randomly selected weights. Empirical results demonstrate that NoLoCo achieves faster convergence rates and is more efficient than existing low communication training methods, such as DiLoCo.'}, 'zh': {'title': 'NoLoCo：高效的无同步优化方法', 'desc': 'NoLoCo是一种新颖的优化方法，旨在消除显式的参数同步，从而减少在大型语言模型训练过程中的通信开销。与现有方法相比，NoLoCo实现了更快的收敛速度和更少的空闲时间。该方法通过一种新型的Nesterov动量优化器变体，隐式地同步模型权重，部分平均与随机选择的其他权重。我们的实验结果表明，NoLoCo在不同的加速器数量和模型规模下，通信开销显著低于传统的全分片数据并行训练方法。'}}}, {'id': 'https://huggingface.co/papers/2506.10568', 'title': 'DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.10568', 'abstract': 'A Diffusion Transformer-based framework generates high-fidelity human-product demonstration videos by preserving identities and spatial relationships, using masked cross-attention and structured text encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.', 'score': 4, 'issue_id': 4281, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '83c202462f081ecf', 'authors': ['Lizhen Wang', 'Zhurong Xia', 'Tianshu Hu', 'Pengrui Wang', 'Pengfei Wang', 'Zerong Zheng', 'Ming Zhou'], 'affiliations': ['ByteDance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2506.10568.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Реалистичные видео-демонстрации продуктов с помощью диффузионных трансформеров', 'desc': 'Предложена новая архитектура на основе Diffusion Transformer для генерации высококачественных видео с демонстрацией продуктов. Метод использует маскированное кросс-внимание и структурированное кодирование текста для сохранения идентичности людей и продуктов, а также их пространственных отношений. Применяется шаблон 3D-меша тела и ограничивающие рамки продуктов для точного управления движениями. Подход превосходит современные методы в сохранении идентичности и генерации реалистичных демонстрационных движений.'}, 'en': {'title': 'Realistic Human-Product Videos with Diffusion Transformers', 'desc': 'This paper presents a Diffusion Transformer-based framework designed to create realistic human-product demonstration videos for e-commerce. The framework addresses the common issues of identity preservation and spatial relationships between humans and products by using masked cross-attention and structured text encoding. By incorporating 3D body mesh templates and product bounding boxes, the method ensures accurate motion guidance and alignment of gestures with products. The approach is trained on a diverse dataset, achieving superior results in generating high-fidelity videos that maintain the integrity of both human and product identities.'}, 'zh': {'title': '生成高保真演示视频的创新框架', 'desc': '本文提出了一种基于扩散变换器（Diffusion Transformer, DiT）的框架，用于生成高保真的人类与产品演示视频。该方法通过注入配对的人类与产品参考信息，结合掩蔽交叉注意力机制，能够同时保留人类身份和产品细节。我们使用3D身体网格模板和产品边界框提供精确的运动指导，从而实现手势与产品位置的直观对齐。此外，结构化文本编码用于引入类别级语义，增强了在小旋转变化下的3D一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.10036', 'title': 'Token Perturbation Guidance for Diffusion Models', 'url': 'https://huggingface.co/papers/2506.10036', 'abstract': 'Token Perturbation Guidance (TPG) enhances diffusion model generation quality without training, by perturbing intermediate token representations, achieving CFG-like performance and improving unconditional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance', 'score': 4, 'issue_id': 4277, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '3637011ee12cd77a', 'authors': ['Javad Rajabi', 'Soroush Mehraban', 'Seyedmorteza Sadat', 'Babak Taati'], 'affiliations': ['ETH Zürich', 'KITE Research Institute', 'University of Toronto', 'Vector Institute for Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.10036.jpg', 'data': {'categories': ['#optimization', '#training', '#diffusion', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Улучшение генерации диффузионных моделей без переобучения', 'desc': 'Статья представляет новый метод под названием Token Perturbation Guidance (TPG) для улучшения качества генерации диффузионных моделей. TPG применяет матрицы возмущения к промежуточным токенным представлениям в сети диффузии, что позволяет улучшить качество генерации без изменения архитектуры или дополнительного обучения. Метод эффективен как для условной, так и для безусловной генерации, достигая производительности, сравнимой с Classifier-free guidance (CFG). Эксперименты на моделях SDXL и Stable Diffusion 2.1 показывают значительное улучшение метрики FID для безусловной генерации.'}, 'en': {'title': 'Enhancing Diffusion Models with Token Perturbation Guidance', 'desc': 'Token Perturbation Guidance (TPG) is a new method that improves the quality of images generated by diffusion models without needing additional training. It works by applying perturbation matrices to the intermediate token representations, which helps guide the generation process effectively. Unlike Classifier-free Guidance (CFG), TPG does not require specific training and can be used for both conditional and unconditional generation tasks. Experiments show that TPG significantly enhances generation quality, achieving nearly double the improvement in FID scores compared to existing methods, while maintaining alignment with prompts.'}, 'zh': {'title': '令牌扰动引导：无训练的生成质量提升', 'desc': '本文提出了一种新的方法，称为令牌扰动引导（TPG），旨在提高扩散模型的生成质量，而无需进行训练。TPG通过对扩散网络中间令牌表示施加扰动矩阵，提供有效且稳定的引导信号，从而改善生成效果。与传统的无分类器引导方法相比，TPG在无条件生成任务中表现出接近分类器无关引导的性能。实验结果表明，TPG在生成质量上显著优于现有基线，且适用于条件和无条件生成。'}}}, {'id': 'https://huggingface.co/papers/2506.10674', 'title': 'TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving', 'url': 'https://huggingface.co/papers/2506.10674', 'abstract': 'A benchmark dataset called TeleMath evaluates Large Language Models in domain-specific mathematical problems within telecommunications, showing that models designed for mathematical reasoning perform better than general-purpose models.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing adoption of artificial intelligence in telecommunications has raised interest in the capability of Large Language Models (LLMs) to address domain-specific, mathematically intensive tasks. Although recent advancements have improved the performance of LLMs in general mathematical reasoning, their effectiveness within specialized domains, such as signal processing, network optimization, and performance analysis, remains largely unexplored. To address this gap, we introduce TeleMath, the first benchmark dataset specifically designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain. Comprising 500 question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the telecommunications field. This paper outlines the proposed QnAs generation pipeline, starting from a selected seed of problems crafted by Subject Matter Experts. The evaluation of a wide range of open-source LLMs reveals that best performance on TeleMath is achieved by recent models explicitly designed for mathematical or logical reasoning. In contrast, general-purpose models, even those with a large number of parameters, often struggle with these challenges. We have released the dataset and the evaluation code to ease result reproducibility and support future research.', 'score': 3, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'fc14281fcee53b0d', 'authors': ['Vincenzo Colle', 'Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Fadhel Ayed', 'Merouane Debbah'], 'affiliations': ['Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France', 'Università degli Studi di Cassino del Lazio Meridionale, Cassino, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2506.10674.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#open_source', '#math', '#benchmark'], 'emoji': '📡', 'ru': {'title': 'TeleMath: Измеряем математические способности ИИ в телекоммуникациях', 'desc': 'Представлен новый набор данных TeleMath для оценки способности больших языковых моделей (LLM) решать математические задачи в области телекоммуникаций. Набор содержит 500 пар вопрос-ответ, охватывающих широкий спектр тем в телекоммуникационной сфере. Исследование показало, что модели, специально разработанные для математических рассуждений, превосходят модели общего назначения в решении этих задач. Авторы опубликовали набор данных и код для оценки, чтобы облегчить воспроизводимость результатов и поддержать дальнейшие исследования.'}, 'en': {'title': 'TeleMath: Evaluating LLMs in Telecommunications Mathematics', 'desc': 'The paper introduces TeleMath, a benchmark dataset aimed at assessing Large Language Models (LLMs) on mathematical problems specific to the telecommunications sector. It highlights that LLMs tailored for mathematical reasoning outperform general-purpose models when tackling domain-specific tasks. The dataset consists of 500 question-answer pairs covering various telecommunications topics, created with input from Subject Matter Experts. The findings suggest that specialized models are more effective in solving these complex mathematical challenges compared to their general counterparts.'}, 'zh': {'title': '专注电信数学，提升模型表现', 'desc': '本文介绍了一个名为TeleMath的基准数据集，旨在评估大型语言模型（LLMs）在电信领域特定数学问题上的表现。研究表明，专为数学推理设计的模型在解决这些问题时表现优于通用模型。TeleMath包含500个问答对，涵盖电信领域的广泛主题，填补了LLMs在专业领域应用的空白。我们还发布了数据集和评估代码，以支持未来的研究和结果的可重复性。'}}}, {'id': 'https://huggingface.co/papers/2506.08373', 'title': 'Draft-based Approximate Inference for LLMs', 'url': 'https://huggingface.co/papers/2506.08373', 'abstract': "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.", 'score': 3, 'issue_id': 4272, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '02a9a3f798ba1509', 'authors': ['Kevin Galim', 'Ethan Ewer', 'Wonjun Kang', 'Minjae Lee', 'Hyung Il Koo', 'Kangwook Lee'], 'affiliations': ['Ajou University', 'FuriosaAI', 'Seoul National University', 'UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.08373.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#training', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение вывода ИИ с помощью умных черновиков', 'desc': 'Предложена новая система для приближенного вывода в больших языковых моделях с длинным контекстом, использующая вспомогательные модели-черновики. Система точнее предсказывает важность токенов и пар ключ-значение, что повышает точность при сохранении эффективности использования памяти и вычислений. Представлены два варианта реализации: SpecKV для более эффективного отбрасывания кэша ключ-значение и SpecPC для идентификации и удаления неважных токенов запроса. Эксперименты показывают, что предложенные методы превосходят существующие базовые подходы по точности при сохранении преимуществ в использовании памяти, задержке и пропускной способности.'}, 'en': {'title': 'Enhancing LLM Inference with Draft Models for Efficiency and Accuracy', 'desc': 'This paper introduces a new framework that uses draft models to enhance approximate inference in long-context Large Language Models (LLMs). By accurately predicting the importance of tokens and key-value (KV) pairs, the framework improves the accuracy of LLMs while keeping memory and computational efficiency in check. The authors present two specific implementations: SpecKV for effective KV cache dropping and SpecPC for identifying unimportant prompt tokens. Their experiments demonstrate that this approach outperforms existing methods in accuracy while maintaining low resource usage.'}, 'zh': {'title': '利用草稿模型提升长上下文LLM推理效率', 'desc': '本文提出了一种新的框架，利用草稿模型来增强长上下文大语言模型（LLM）的近似推理能力。通过更准确地预测令牌和键值对的重要性，该方法提高了推理的准确性，同时保持了内存和计算效率。我们介绍了两种具体实现：SpecKV和SpecPC，分别用于优化键值缓存和提示令牌的选择。实验结果表明，该方法在准确性、内存使用、延迟和吞吐量方面均优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2506.07795', 'title': 'LLM Unlearning Should Be Form-Independent', 'url': 'https://huggingface.co/papers/2506.07795', 'abstract': "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.", 'score': 3, 'issue_id': 4279, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '7314028832938fb2', 'authors': ['Xiaotian Ye', 'Mengqi Zhang', 'Shu Wu'], 'affiliations': ['New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Beijing University of Posts and Telecommunications', 'Shandong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07795.jpg', 'data': {'categories': ['#security', '#rlhf', '#ethics', '#benchmark', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'Преодоление зависимости от формы в разобучении языковых моделей', 'desc': 'Исследование выявило проблему зависимости эффективности методов разобучения больших языковых моделей от формы обучающих примеров. Это явление, названное Form-Dependent Bias, ограничивает возможности подавления нежелательных знаний в моделях. Для решения этой проблемы авторы предлагают новый метод Rank-one Concept Redirection (ROCR), который не зависит от формы представления знаний. ROCR перенаправляет восприятие модели от опасных концепций к безвредным, значительно улучшая эффективность разобучения по сравнению с традиционными методами.'}, 'en': {'title': 'Unlearning Without Limits: ROCR for Form-Independent Knowledge Management', 'desc': 'This paper addresses the challenge of unlearning in Large Language Models (LLMs), specifically focusing on the limitations caused by Form-Dependent Bias, which affects the effectiveness of unlearning methods across different knowledge expressions. The authors propose a new method called Rank-one Concept Redirection (ROCR) that aims to enhance unlearning efficacy by being form-independent, allowing it to generalize better across various tasks. They introduce a benchmark called ORT to evaluate the robustness of unlearning techniques against different expressions of knowledge. Experimental results show that ROCR outperforms traditional unlearning methods, providing a more effective and efficient way to manage harmful or private information in LLMs.'}, 'zh': {'title': '形式无关的去学习新方法', 'desc': '本文探讨了大型语言模型（LLM）在去除不良知识时面临的挑战，特别是形式依赖偏差的问题。研究表明，现有的去学习方法在不同知识表达形式下的有效性有限，导致其在实际应用中的效果不佳。为了解决这一问题，提出了一种新的方法——Rank-one Concept Redirection（ROCR），旨在实现形式无关的去学习。实验结果显示，ROCR在去学习的有效性上显著优于传统方法，同时生成的输出也更加自然。'}}}, {'id': 'https://huggingface.co/papers/2506.10737', 'title': 'TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora', 'url': 'https://huggingface.co/papers/2506.10737', 'abstract': "TaxoAdapt dynamically adapts an LLM-generated taxonomy for scientific literature across multiple dimensions, improving granularity and coherence compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models (LLMs) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a framework that dynamically adapts an LLM-generated taxonomy to a given corpus across multiple dimensions. TaxoAdapt performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more granularity-preserving and 50.41% more coherent than the most competitive baselines judged by LLMs.", 'score': 2, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'ba81aecb13322a55', 'authors': ['Priyanka Kargupta', 'Nan Zhang', 'Yunyi Zhang', 'Rui Zhang', 'Prasenjit Mitra', 'Jiawei Han'], 'affiliations': ['The Pennsylvania State University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10737.jpg', 'data': {'categories': ['#science', '#multimodal', '#dataset'], 'emoji': '🌳', 'ru': {'title': 'Динамическая адаптация таксономий для эволюционирующей научной литературы', 'desc': 'TaxoAdapt - это фреймворк для динамической адаптации таксономии, сгенерированной большой языковой моделью (LLM), к заданному корпусу научной литературы по нескольким измерениям. Он использует итеративную иерархическую классификацию для расширения ширины и глубины таксономии на основе тематического распределения корпуса. TaxoAdapt демонстрирует современные результаты на разнообразных компьютерных конференциях, отражая эволюцию научных областей. По сравнению с конкурентными методами, таксономии TaxoAdapt на 26.51% лучше сохраняют гранулярность и на 50.41% более согласованны по оценке LLM.'}, 'en': {'title': 'Dynamic Taxonomy Adaptation for Evolving Science', 'desc': "TaxoAdapt is a novel framework that enhances the organization of scientific literature by dynamically adapting taxonomies generated by large language models (LLMs). It addresses the limitations of traditional expert-curated taxonomies and existing automatic methods, which often lack generalizability and fail to capture the evolving nature of scientific fields. By employing iterative hierarchical classification, TaxoAdapt expands the taxonomy's width and depth based on the topical distribution of the corpus, allowing for a more nuanced representation of research contributions. The results show that TaxoAdapt achieves significantly higher granularity and coherence compared to leading methods, making it a powerful tool for structuring scientific knowledge."}, 'zh': {'title': '动态适应科学文献分类法的创新方法', 'desc': 'TaxoAdapt 是一种动态调整大型语言模型（LLM）生成的分类法的方法，旨在提高科学文献的组织和检索效率。它通过迭代的层次分类，基于文献的主题分布扩展分类法的宽度和深度，从而更好地适应快速发展的科学领域。与传统的专家策划分类法相比，TaxoAdapt 在细粒度和一致性方面表现出色，分别提高了 26.51% 和 50.41%。该方法能够处理科学文献的多维特性，使得单篇研究论文可以在多个维度上进行贡献。'}}}, {'id': 'https://huggingface.co/papers/2506.10728', 'title': 'Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims', 'url': 'https://huggingface.co/papers/2506.10728', 'abstract': 'ClaimSpect is a retrieval-augmented generation-based framework that constructs a hierarchical structure of aspects for claims, enriching them with diverse perspectives from a corpus.  \t\t\t\t\tAI-generated summary \t\t\t\t Claims made by individuals or entities are oftentimes nuanced and cannot be clearly labeled as entirely "true" or "false" -- as is frequently the case with scientific and political claims. However, a claim (e.g., "vaccine A is better than vaccine B") can be dissected into its integral aspects and sub-aspects (e.g., efficacy, safety, distribution), which are individually easier to validate. This enables a more comprehensive, structured response that provides a well-rounded perspective on a given problem while also allowing the reader to prioritize specific angles of interest within the claim (e.g., safety towards children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based framework for automatically constructing a hierarchy of aspects typically considered when addressing a claim and enriching them with corpus-specific perspectives. This structure hierarchically partitions an input corpus to retrieve relevant segments, which assist in discovering new sub-aspects. Moreover, these segments enable the discovery of varying perspectives towards an aspect of the claim (e.g., support, neutral, or oppose) and their respective prevalence (e.g., "how many biomedical papers believe vaccine A is more transportable than B?"). We apply ClaimSpect to a wide variety of real-world scientific and political claims featured in our constructed dataset, showcasing its robustness and accuracy in deconstructing a nuanced claim and representing perspectives within a corpus. Through real-world case studies and human evaluation, we validate its effectiveness over multiple baselines.', 'score': 2, 'issue_id': 4283, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'f5822dc8f4d101b9', 'authors': ['Priyanka Kargupta', 'Runchu Tian', 'Jiawei Han'], 'affiliations': ['Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.10728.jpg', 'data': {'categories': ['#science', '#reasoning', '#rag', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Разложение сложных утверждений на структурированные аспекты', 'desc': 'ClaimSpect - это фреймворк, основанный на генерации с усилением извлечения, который создает иерархическую структуру аспектов для утверждений, обогащая их разнообразными перспективами из корпуса. Он использует рекурсивный подход для разбиения сложных утверждений на более простые аспекты и подаспекты, которые легче проверить. ClaimSpect применяет методы обработки естественного языка и машинного обучения для автоматического построения иерархии аспектов и извлечения релевантных сегментов из корпуса. Система позволяет получить более полное и структурированное представление о сложных утверждениях, особенно в научной и политической сферах.'}, 'en': {'title': 'Deconstructing Claims for Clearer Perspectives', 'desc': 'ClaimSpect is a framework that uses retrieval-augmented generation to break down complex claims into a structured hierarchy of aspects and sub-aspects. This approach allows for a nuanced analysis of claims, such as those in science and politics, by focusing on individual components like efficacy and safety. By retrieving relevant information from a corpus, ClaimSpect enriches these aspects with diverse perspectives, helping users understand varying viewpoints and their prevalence. The framework has been tested on real-world claims, demonstrating its ability to provide comprehensive and accurate insights into complex issues.'}, 'zh': {'title': 'ClaimSpect：解构声明的智能框架', 'desc': 'ClaimSpect 是一个基于检索增强生成的框架，旨在为声明构建层次结构的方面，并从语料库中丰富多样的视角。声明通常是复杂的，不能简单地标记为“真”或“假”，但可以将其分解为更易验证的基本方面。该框架通过分层划分输入语料库，检索相关片段，帮助发现新的子方面和不同的观点。我们在多个真实的科学和政治声明中应用 ClaimSpect，展示了其在解构复杂声明和表示语料库中观点的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.10600', 'title': 'EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence', 'url': 'https://huggingface.co/papers/2506.10600', 'abstract': 'EmbodiedGen is a platform that generates high-quality, photorealistic 3D assets at low cost, enabling scalable and realistic embodied AI research through generative AI techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.', 'score': 2, 'issue_id': 4285, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '34b11f20c290edff', 'authors': ['Wang Xinjie', 'Liu Liu', 'Cao Yu', 'Wu Ruiqi', 'Qin Wenkang', 'Wang Dehui', 'Sui Wei', 'Su Zhizhong'], 'affiliations': ['D-Robotics', 'GigaAI', 'Horizon Robotics', 'Shanghai Jiao Tong University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10600.jpg', 'data': {'categories': ['#games', '#3d', '#agents', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Генеративный ИИ для создания реалистичных 3D-миров', 'desc': 'EmbodiedGen - это платформа для генерации фотореалистичных 3D-активов высокого качества с низкой стоимостью. Она использует методы генеративного искусственного интеллекта для создания масштабируемых и реалистичных сред для исследований в области воплощенного ИИ. Платформа состоит из шести ключевых модулей, включая преобразование изображений и текста в 3D, генерацию текстур и сцен. EmbodiedGen позволяет создавать разнообразные интерактивные 3D-миры, решая проблемы обобщения и оценки для исследований воплощенного интеллекта.'}, 'en': {'title': 'Revolutionizing 3D Asset Generation for Embodied AI', 'desc': 'EmbodiedGen is a platform designed to create high-quality, photorealistic 3D assets efficiently, which is essential for training embodied AI systems. It addresses the limitations of traditional 3D graphics by providing a scalable and cost-effective solution for generating diverse 3D environments. The platform includes six modules that facilitate the generation of 3D objects and scenes, ensuring they have accurate physical properties for realistic simulations. By leveraging generative AI techniques, EmbodiedGen enhances the generalization and evaluation capabilities of embodied intelligence research.'}, 'zh': {'title': 'EmbodiedGen：低成本生成高质量3D资产的解决方案', 'desc': 'EmbodiedGen是一个生成高质量、逼真的3D资产的平台，旨在降低成本并促进可扩展的具身人工智能研究。该平台通过生成式人工智能技术，构建物理真实且准确缩放的3D世界，以支持具身智能任务的训练和评估。EmbodiedGen提供了六个关键模块，能够生成多样化和互动的3D世界，解决了传统3D图形资产的高成本和有限真实感的问题。通过使用EmbodiedGen，研究人员可以更高效地生成所需的3D数据资产，从而推动具身智能领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.10378', 'title': 'Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning', 'url': 'https://huggingface.co/papers/2506.10378', 'abstract': 'The study proposes a causal representation learning framework to evaluate language model capabilities through latent factors, emphasizing the importance of controlling for base model variations to uncover underlying causal relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.', 'score': 2, 'issue_id': 4275, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '50915ea9038be86d', 'authors': ['Jikai Jin', 'Vasilis Syrgkanis', 'Sham Kakade', 'Hanlin Zhang'], 'affiliations': ['Harvard University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10378.jpg', 'data': {'categories': ['#science', '#dataset', '#interpretability', '#benchmark', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие причинно-следственных связей в способностях языковых моделей', 'desc': 'Исследование предлагает каузальную модель представления для оценки возможностей языковых моделей через латентные факторы. Авторы подчеркивают важность контроля вариаций базовой модели для выявления причинно-следственных связей. Применяя этот подход к большому набору данных, они выявили трехузловую линейную каузальную структуру, объясняющую наблюдаемые различия в производительности. Результаты показывают четкую причинную связь от общих способностей решения задач через следование инструкциям к математическим рассуждениям.'}, 'en': {'title': 'Uncovering Causal Relationships in Language Model Performance', 'desc': 'This paper introduces a causal representation learning framework designed to assess the capabilities of language models by examining latent factors. It highlights the necessity of controlling for variations in base models to accurately identify causal relationships. The authors analyze a dataset of over 1500 models across six benchmarks, revealing a three-node linear causal structure that explains performance differences. Their findings emphasize the importance of understanding the causal pathways from general problem-solving to specific abilities like instruction-following and mathematical reasoning.'}, 'zh': {'title': '揭示语言模型能力的因果关系', 'desc': '本研究提出了一种因果表示学习框架，用于通过潜在因素评估语言模型的能力。我们强调控制基础模型变异的重要性，以揭示潜在的因果关系。通过对超过1500个模型在六个基准测试中的表现进行分析，我们识别出一个简洁的三节点线性因果结构，能够可靠地解释观察到的性能变化。我们的结果表明，在评估过程中仔细控制基础模型的变异是揭示潜在模型能力之间因果关系的关键步骤。'}}}, {'id': 'https://huggingface.co/papers/2506.06694', 'title': 'Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning', 'url': 'https://huggingface.co/papers/2506.06694', 'abstract': 'MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.', 'score': 2, 'issue_id': 4275, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': 'f878d8e46d64a439', 'authors': ['Yuan Yuan', 'Yukun Liu', 'Chonghua Han', 'Jie Feng', 'Yong Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06694.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data', '#training', '#open_source', '#architecture'], 'emoji': '🚶', 'ru': {'title': 'Защита приватности при обучении моделей мобильности', 'desc': 'MoveGCL - это фреймворк для обучения фундаментальных моделей мобильности с сохранением конфиденциальности данных. Он использует генеративное непрерывное обучение и трансформер с механизмом смеси экспертов для работы с разнородными паттернами мобильности. MoveGCL позволяет развивать модель децентрализованно, воспроизводя синтетические траектории из замороженной учительской модели. Эксперименты показали, что MoveGCL достигает результатов, сравнимых с совместным обучением, значительно превосходя базовые методы федеративного обучения.'}, 'en': {'title': 'Unlocking Mobility Models with Privacy-Preserving Learning', 'desc': 'MoveGCL is a framework designed to train mobility foundation models while ensuring data privacy. It uses generative continual learning to create synthetic data from a teacher model, allowing for model updates without sharing sensitive raw data. The framework employs a Mixture-of-Experts Transformer to adapt to various mobility patterns and includes strategies to prevent catastrophic forgetting during training. Experiments show that MoveGCL performs well compared to traditional methods while maintaining strong privacy protections.'}, 'zh': {'title': 'MoveGCL：隐私保护的移动基础模型训练框架', 'desc': 'MoveGCL是一个保护隐私的框架，利用生成持续学习和混合专家Transformer来训练移动基础模型，而无需共享原始数据。该框架通过重放从冻结教师模型生成的合成轨迹，实现去中心化和渐进式模型演化，并通过定制的蒸馏策略增强知识保留，减少灾难性遗忘。为了应对移动模式的异质性，MoveGCL结合了移动感知的专家路由机制和逐层渐进适应策略，以稳定持续更新。实验结果表明，MoveGCL在六个真实城市数据集上的表现与联合训练相当，显著优于联邦学习基线，同时提供强有力的隐私保护。'}}}, {'id': 'https://huggingface.co/papers/2506.06561', 'title': 'LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles', 'url': 'https://huggingface.co/papers/2506.06561', 'abstract': "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.", 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '12942392f309be51', 'authors': ["Ho Yin 'Sam' Ng", 'Ting-Yao Hsu', 'Aashish Anantha Ramakrishnan', 'Branislav Kveton', 'Nedim Lipka', 'Franck Dernoncourt', 'Dongwon Lee', 'Tong Yu', 'Sungchul Kim', 'Ryan A. Rossi', "Ting-Hao 'Kenneth' Huang"], 'affiliations': ['Adobe Research', 'Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06561.jpg', 'data': {'categories': ['#optimization', '#dataset', '#multimodal', '#interpretability', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Персонализированные подписи к изображениям: мультимодальный подход', 'desc': 'LaMP-Cap представляет датасет для персонализированной генерации подписей к изображениям с использованием мультимодальных профилей. Эксперименты показали, что использование профильной информации помогает генерировать подписи, более близкие к оригинальным авторским. Исследование выявило, что изображения в профиле более полезны, чем текстовые параграфы, упоминающие рисунки. Это подчеркивает преимущество использования мультимодальных профилей по сравнению с чисто текстовыми.'}, 'en': {'title': 'Personalized Captions Through Multimodal Contexts', 'desc': "LaMP-Cap is a new dataset designed to enhance the generation of personalized figure captions by utilizing multimodal profiles. It provides not only the target figure images but also additional contextual figures and their associated captions and paragraphs. This approach allows AI models to create captions that better reflect the author's style and the specific domain. Experiments demonstrate that incorporating profile information, especially images, significantly improves the quality of AI-generated captions compared to traditional text-only methods."}, 'zh': {'title': '个性化图形标题生成的新突破', 'desc': 'LaMP-Cap是一个用于个性化图形标题生成的数据集，旨在通过多模态资料提高AI生成标题的质量。图形标题对于帮助读者理解和记住图形的关键信息至关重要。尽管已有许多模型可以生成这些标题，但作者通常需要修改通用的AI生成标题以匹配他们的写作风格。LaMP-Cap提供了图像和相关图形的上下文资料，实验表明，使用这些多模态资料可以生成更接近作者原始写作的标题。'}}}, {'id': 'https://huggingface.co/papers/2506.05982', 'title': 'MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks', 'url': 'https://huggingface.co/papers/2506.05982', 'abstract': 'MCA-Bench is a multimodal benchmark suite for CAPTCHA security evaluation which fine-tunes specialized cracking agents using a shared vision-language model.  \t\t\t\t\tAI-generated summary \t\t\t\t As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.', 'score': 1, 'issue_id': 4272, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '6cf7938ff751b2ff', 'authors': ['Zonglin Wu', 'Yule Xue', 'Xin Wei', 'Yiren Song'], 'affiliations': ['National University of Singapore', 'Southwest University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05982.jpg', 'data': {'categories': ['#agents', '#benchmark', '#security', '#open_source', '#multimodal'], 'emoji': '🔐', 'ru': {'title': 'Единый мультимодальный бенчмарк для оценки безопасности CAPTCHA', 'desc': 'MCA-Bench - это комплексный набор инструментов для оценки безопасности CAPTCHA, использующий мультимодальный подход. Он объединяет различные типы CAPTCHA в единый протокол оценки, используя общую основу модели машинного зрения и обработки естественного языка. MCA-Bench позволяет дообучать специализированных агентов для взлома каждой категории CAPTCHA, обеспечивая последовательную оценку между различными модальностями. Эксперименты показывают, что MCA-Bench эффективно отображает спектр уязвимостей современных CAPTCHA и предлагает количественный анализ взаимосвязи между сложностью задачи, глубиной взаимодействия и способностью модели решать задачи.'}, 'en': {'title': 'Strengthening CAPTCHA Security with MCA-Bench', 'desc': 'MCA-Bench is a new tool designed to evaluate the security of different types of CAPTCHAs against automated attacks. It combines various CAPTCHA formats, such as text, images, and interactive puzzles, into one comprehensive testing framework. By using a shared vision-language model, it fine-tunes specific agents to crack each type of CAPTCHA, allowing for consistent comparisons across different modalities. The results provide insights into how the complexity and interaction of CAPTCHAs affect their vulnerability, helping to improve their design and security.'}, 'zh': {'title': 'MCA-Bench：CAPTCHA安全评估的新基准', 'desc': 'MCA-Bench是一个多模态基准测试套件，用于评估CAPTCHA的安全性。它通过共享的视觉-语言模型微调专门的破解代理，以便对不同类型的CAPTCHA进行一致的评估。该研究填补了现有CAPTCHA评估中缺乏统一大规模基准的空白，提供了对现代CAPTCHA设计脆弱性的定量分析。基于实验结果，提出了三个可行的设计原则，并识别了关键的开放挑战，为CAPTCHA的系统性强化和公平基准测试奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.08862', 'title': 'StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams', 'url': 'https://huggingface.co/papers/2506.08862', 'abstract': 'StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.', 'score': 0, 'issue_id': 4279, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'fc6ac5b21c6b00ff', 'authors': ['Zike Wu', 'Qi Yan', 'Xuanyu Yi', 'Lele Wang', 'Renjie Liao'], 'affiliations': ['Canada CIFAR AI Chair', 'Nanyang Technological University', 'University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.08862.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в 3D-реконструкции: от видео к динамическим сценам в реальном времени', 'desc': 'StreamSplat - это новая система для реконструкции динамических 3D-сцен из неоткалиброванного видео в режиме реального времени. Она использует полностью прямую архитектуру нейронной сети для преобразования видеопотока в динамическое представление на основе 3D Gaussian Splatting. Ключевые инновации включают вероятностный механизм выборки для предсказания позиций 3DGS и двунаправленное поле деформации для моделирования динамики. StreamSplat превосходит существующие методы по качеству реконструкции и эффективности, поддерживая обработку видеопотоков произвольной длины.'}, 'en': {'title': 'StreamSplat: Real-Time 3D Scene Reconstruction Made Easy!', 'desc': 'StreamSplat is a novel framework designed for real-time 3D scene reconstruction from uncalibrated video inputs. It effectively addresses the challenges of processing uncalibrated data, accurately modeling dynamic changes in scenes, and ensuring long-term stability. The framework utilizes a feed-forward approach, incorporating a probabilistic sampling mechanism for predicting 3D positions and a bidirectional deformation field for dynamic modeling. Experimental results show that StreamSplat outperforms existing methods in both reconstruction quality and the ability to handle long video streams.'}, 'zh': {'title': 'StreamSplat：实时动态三维场景重建的新突破', 'desc': 'StreamSplat 是一个完全前馈的框架，旨在从未校准的视频中实时重建动态三维场景。该方法解决了处理未校准输入、准确建模动态场景演变以及保持长期稳定性等三个关键挑战。通过引入静态编码器中的概率采样机制和动态解码器中的双向变形场，StreamSplat 实现了高效的动态建模。实验结果表明，StreamSplat 在重建质量和动态场景建模方面均优于现有方法，并支持任意长度视频流的在线重建。'}}}, {'id': 'https://huggingface.co/papers/2506.06395', 'title': 'Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models', 'url': 'https://huggingface.co/papers/2506.06395', 'abstract': "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps, RLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on Minerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a simple, scalable post-training method for inference models, requiring only a small number of samples and unlabelled supervision.", 'score': 70, 'issue_id': 4258, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '0bd4f12f6c85c81f', 'authors': ['Pengyi Li', 'Matvey Skripkin', 'Alexander Zubrey', 'Andrey Kuznetsov', 'Ivan Oseledets'], 'affiliations': ['AIRI, Skoltech Moscow'], 'pdf_title_img': 'assets/pdf/title_img/2506.06395.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#optimization', '#inference', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Самоуверенность как ключ к обучению языковых моделей', 'desc': 'Метод Reinforcement Learning via Self-Confidence (RLSC) использует уверенность модели в качестве сигнала вознаграждения для улучшения точности больших языковых моделей. RLSC устраняет необходимость в человеческой разметке или инженерии вознаграждений. Применение RLSC к модели Qwen2.5-Math-7B значительно повысило ее точность на различных математических наборах данных. Этот метод обеспечивает простой и масштабируемый способ пост-обучения моделей, требуя лишь небольшого количества образцов и неразмеченного надзора.'}, 'en': {'title': 'Boosting Model Accuracy with Self-Confidence Rewards', 'desc': "Reinforcement Learning via Self-Confidence (RLSC) is a novel approach that enhances the accuracy of large language models (LLMs) by utilizing the model's own confidence as a reward signal. This method eliminates the reliance on human labels or complex reward engineering, making it more efficient and scalable. RLSC has been tested on the Qwen2.5-Math-7B model, showing significant accuracy improvements across various math benchmarks with minimal training data. By requiring only a few samples and no labeled data, RLSC offers a straightforward solution for post-training alignment of LLMs with task objectives."}, 'zh': {'title': '自信驱动的强化学习，提升模型准确性！', 'desc': '强化学习通过自信（RLSC）是一种新方法，它利用模型自身的自信度作为奖励信号，从而提高大型语言模型的准确性。这种方法不再依赖于人工标签或外部奖励模型，简化了训练过程。RLSC在多个数学任务上表现出色，显著提高了模型的准确率。该方法简单且可扩展，只需少量样本和无标签的监督即可实现。'}}}, {'id': 'https://huggingface.co/papers/2506.09113', 'title': 'Seedance 1.0: Exploring the Boundaries of Video Generation Models', 'url': 'https://huggingface.co/papers/2506.09113', 'abstract': 'Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.', 'score': 47, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'd44df125bd718f2f', 'authors': ['Yu Gao', 'Haoyuan Guo', 'Tuyen Hoang', 'Weilin Huang', 'Lu Jiang', 'Fangyuan Kong', 'Huixia Li', 'Jiashi Li', 'Liang Li', 'Xiaojie Li', 'Xunsong Li', 'Yifu Li', 'Shanchuan Lin', 'Zhijie Lin', 'Jiawei Liu', 'Shu Liu', 'Xiaonan Nie', 'Zhiwu Qing', 'Yuxi Ren', 'Li Sun', 'Zhi Tian', 'Rui Wang', 'Sen Wang', 'Guoqiang Wei', 'Guohong Wu', 'Jie Wu', 'Ruiqi Xia', 'Fei Xiao', 'Xuefeng Xiao', 'Jiangqiao Yan', 'Ceyuan Yang', 'Jianchao Yang', 'Runkai Yang', 'Tao Yang', 'Yihang Yang', 'Zilyu Ye', 'Xuejiao Zeng', 'Yan Zeng', 'Heng Zhang', 'Yang Zhao', 'Xiaozheng Zheng', 'Peihao Zhu', 'Jiaxin Zou', 'Feilong Zuo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.09113.jpg', 'data': {'categories': ['#video', '#dataset', '#optimization', '#architecture', '#benchmark', '#data', '#training', '#rlhf', '#diffusion', '#inference'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: быстро, качественно, многозадачно', 'desc': 'Seedance 1.0 - это высокопроизводительная модель генерации видео, объединяющая несколько ключевых технических улучшений. Она включает в себя многоисточниковую курацию данных с точными видеоподписями, эффективную архитектуру с поддержкой многокадровой генерации и совместным обучением задачам текст-в-видео и изображение-в-видео. Модель использует оптимизированные пост-тренировочные подходы, включая тонкую настройку под наблюдением и видео-специфичное RLHF. Благодаря многоэтапным стратегиям дистилляции и системным оптимизациям, Seedance 1.0 достигает ~10-кратного ускорения вывода.'}, 'en': {'title': 'Seedance 1.0: Fast and High-Quality Video Generation Revolutionized', 'desc': 'Seedance 1.0 is a cutting-edge video generation model that enhances performance through advanced data curation and an efficient architecture. It addresses key challenges in video generation, such as prompt adherence and visual quality, by integrating multi-source data and a novel training paradigm. The model employs optimized post-training techniques, including fine-tuning and reinforcement learning with multi-dimensional rewards, to boost its capabilities. With a remarkable inference speedup of approximately 10 times, Seedance 1.0 can produce high-quality 5-second videos at 1080p resolution in just 41.4 seconds.'}, 'zh': {'title': 'Seedance 1.0：高效视频生成的新标杆', 'desc': 'Seedance 1.0 是一种高性能的视频生成模型，结合了先进的数据整理、有效的架构设计、后训练优化和模型加速技术，提供了卓越的质量和速度。该模型通过多源数据整理和精准的视频字幕，增强了对多样场景的全面学习能力。它的高效架构支持多镜头生成，并同时学习文本到视频和图像到视频的任务。Seedance 1.0 通过多阶段蒸馏策略实现了约10倍的推理加速，能够在41.4秒内生成5秒的1080p视频。'}}}, {'id': 'https://huggingface.co/papers/2506.09350', 'title': 'Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.09350', 'abstract': 'Autoregressive adversarial post-training transforms pre-trained latent video diffusion models into real-time, interactive video generators with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2', 'score': 35, 'issue_id': 4256, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'f2ebb5225d0061fa', 'authors': ['Shanchuan Lin', 'Ceyuan Yang', 'Hao He', 'Jianwen Jiang', 'Yuxi Ren', 'Xin Xia', 'Yang Zhao', 'Xuefeng Xiao', 'Lu Jiang'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2506.09350.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Интерактивная генерация видео в реальном времени с помощью AAPT', 'desc': 'Статья представляет метод автореgressivного adversarial post-training (AAPT) для преобразования предобученных латентных видео-диффузионных моделей в генераторы видео реального времени. Модель генерирует латентные кадры автореgressивно, используя одно вычисление нейронной функции (1NFE) на кадр. Это позволяет осуществлять потоковую передачу результата пользователю в реальном времени и получать интерактивные отклики для управления генерацией следующего кадра. Эксперименты показывают, что модель размером 8 млрд параметров достигает генерации видео в реальном времени с частотой 24 кадра в секунду и разрешением до 1280x720 на 8 GPU H100.'}, 'en': {'title': 'Real-Time Video Generation Made Easy!', 'desc': 'This paper introduces a method called autoregressive adversarial post-training (AAPT) to enhance pre-trained latent video diffusion models for real-time video generation. The AAPT approach allows the model to generate video frames one at a time, using a single neural function evaluation, which significantly reduces computational demands. By incorporating adversarial training, the model improves its efficiency and reduces errors during the generation of longer videos. The results show that the model can produce high-quality video at 24 frames per second, making it suitable for interactive applications.'}, 'zh': {'title': '实时交互视频生成的新方法', 'desc': '本文提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转变为实时交互式视频生成器。该模型通过自回归方式一次生成一个潜在帧，使用单次神经网络函数评估（1NFE），实现实时流式传输。与现有方法不同，我们的方法利用对抗训练作为自回归生成的有效范式，设计出更高效的架构，减少长视频生成中的误差积累。实验结果表明，我们的8B模型在单个H100上实现了736x416分辨率的实时24fps视频生成，或在8个H100上生成1280x720分辨率的视频，最长可达一分钟（1440帧）。'}}}, {'id': 'https://huggingface.co/papers/2506.09790', 'title': 'ComfyUI-R1: Exploring Reasoning Models for Workflow Generation', 'url': 'https://huggingface.co/papers/2506.09790', 'abstract': 'ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.', 'score': 28, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '6fb3fee31c3739a5', 'authors': ['Zhenran Xu', 'Yiyu Wang', 'Xue Yang', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Digital Commerce, China', 'Harbin Institute of Technology (Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09790.jpg', 'data': {'categories': ['#rl', '#dataset', '#optimization', '#architecture', '#reasoning', '#training', '#long_context'], 'emoji': '🎨', 'ru': {'title': 'ComfyUI-R1: ИИ-художник нового поколения', 'desc': 'ComfyUI-R1 - это крупная модель машинного обучения для автоматической генерации рабочих процессов в сфере ИИ-искусства. Модель использует длинные цепочки рассуждений и обучение с подкреплением для создания эффективных рабочих процессов. ComfyUI-R1 обучается на наборе данных из 4000 рабочих процессов с использованием двухэтапного подхода: тонкая настройка цепочек рассуждений и обучение с подкреплением. Эксперименты показывают, что 7-миллиардная модель значительно превосходит существующие методы по различным метрикам.'}, 'en': {'title': 'Automating AI Art Workflows with ComfyUI-R1', 'desc': 'ComfyUI-R1 is a large reasoning model designed to automate the generation of workflows for creating AI art. It utilizes long chain-of-thought (CoT) reasoning and reinforcement learning to improve the process of workflow creation, making it easier for users to customize their creative pipelines. The model is trained on a dataset of 4,000 workflows and employs a two-stage framework that includes CoT fine-tuning and reinforcement learning with a hybrid reward system. Experimental results show that ComfyUI-R1 outperforms existing models in terms of format validity and overall workflow quality, highlighting the effectiveness of its reasoning capabilities.'}, 'zh': {'title': 'ComfyUI-R1：自动化工作流生成的推理模型', 'desc': 'ComfyUI-R1 是一个大型推理模型，专注于自动化工作流生成，特别是在 AI 艺术创作中表现出色。它通过长链推理和强化学习，帮助用户创建定制化的工作流，降低了学习曲线。该模型使用了 4K 工作流的数据集，经过两阶段的训练，确保了工作流的格式有效性和结构完整性。实验结果显示，ComfyUI-R1 在格式有效性和 F1 分数上显著超越了之前的先进方法，展示了长链推理在 AI 艺术创作中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.09991', 'title': 'Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation', 'url': 'https://huggingface.co/papers/2506.09991', 'abstract': 'Multiverse, a parallel generative model incorporating a MapReduce paradigm, achieves performance comparable to autoregressive LLMs while offering superior scaling and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model that enables natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. Starting from sequential reasoning chains, we create Multiverse 1K by converting them into structured training data using an automated LLM-assisted pipeline, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to enable parallel inference. It features a dedicated scheduler that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, engine, supporting tools, as well as complete data curation prompts and detailed training and evaluation recipes.', 'score': 26, 'issue_id': 4261, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '3ffc5ce1d43056b5', 'authors': ['Xinyu Yang', 'Yuwei An', 'Hongyi Liu', 'Tianqi Chen', 'Beidi Chen'], 'affiliations': ['Carnegie Mellon University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2506.09991.jpg', 'data': {'categories': ['#data', '#architecture', '#optimization', '#training', '#dataset', '#open_source'], 'emoji': '🌌', 'ru': {'title': 'Multiverse: Параллельная генерация на уровне ведущих языковых моделей', 'desc': 'Статья представляет Multiverse - новую генеративную модель, использующую парадигму MapReduce для параллельной генерации. Модель включает три этапа: Map для декомпозиции задачи, Process для параллельного выполнения подзадач и Reduce для синтеза результатов. Multiverse-32B достигает производительности, сравнимой с авторегрессионными моделями того же масштаба, но демонстрирует лучшую масштабируемость и скорость. Вся экосистема Multiverse, включая данные, веса модели и инструменты, доступна в открытом доступе.'}, 'en': {'title': 'Multiverse: Parallel Power in Generative Modeling', 'desc': 'The paper introduces Multiverse, a novel generative model that leverages a MapReduce paradigm to achieve efficient parallel generation, competing with autoregressive large language models (AR-LLMs). It operates through three stages: Map for task decomposition, Process for executing subtasks in parallel, and Reduce for synthesizing results without loss. The model incorporates Multiverse Attention to maintain compatibility with causal attention while allowing for parallel reasoning steps. After fine-tuning, Multiverse-32B demonstrates comparable performance to leading AR-LLMs, with significant improvements in scaling and speed, making it a promising alternative in the field of generative modeling.'}, 'zh': {'title': 'Multiverse：并行生成的未来', 'desc': 'Multiverse是一种新型生成模型，采用MapReduce范式，实现了原生的并行生成。该模型通过三个阶段进行自动生成：Map阶段用于自适应任务分解，Process阶段用于并行子任务执行，Reduce阶段用于无损结果合成。我们设计了Multiverse Attention，以分离并行推理步骤，同时保持与因果注意力的兼容性，从而实现高效训练。经过3小时的微调，Multiverse-32B在性能上与同规模的领先自回归大语言模型相当，并且在预算控制实验中显示出更优的扩展性和速度。'}}}, {'id': 'https://huggingface.co/papers/2506.09995', 'title': 'PlayerOne: Egocentric World Simulator', 'url': 'https://huggingface.co/papers/2506.09995', 'abstract': 'PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.', 'score': 24, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'e7cba5eb3e340a0f', 'authors': ['Yuanpeng Tu', 'Hao Luo', 'Xi Chen', 'Xiang Bai', 'Fan Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group', 'HKU', 'HUST', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.09995.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#training', '#games'], 'emoji': '🎥', 'ru': {'title': 'Погружение в реальность: эгоцентрическая симуляция мира с PlayerOne', 'desc': 'PlayerOne - это первый эгоцентрический симулятор реалистичного мира, способный генерировать видео на основе изображений, снятых пользователем. Система использует каскадный процесс обучения, начиная с предварительной тренировки на больших наборах эгоцентрических видео с текстовыми описаниями, а затем выполняет тонкую настройку на синхронизированных данных движения и видео. PlayerOne применяет усовершенствованные методы внедрения движения и реконструкции сцены для обеспечения точного контроля над движениями и согласованности сцены при генерации длительных видео. Эта система открывает новые возможности для моделирования мира и имеет широкий спектр потенциальных применений.'}, 'en': {'title': 'Revolutionizing Egocentric World Simulation with PlayerOne', 'desc': 'PlayerOne is an innovative egocentric realistic world simulator that generates videos based on user-captured images. It employs a coarse-to-fine training approach, initially pretraining on large datasets of egocentric text-video pairs, followed by fine-tuning with motion-video data for enhanced accuracy. The system features a part-disentangled motion injection scheme, allowing for detailed control over individual movements, and a joint reconstruction framework that maintains scene consistency across video frames. This pioneering work opens new avenues for world modeling and applications in immersive environments.'}, 'zh': {'title': '开创自我中心现实世界模拟的新纪元', 'desc': 'PlayerOne是一个以自我为中心的现实世界模拟器，能够根据用户捕捉的图像构建和生成视频。它采用粗到细的训练流程，首先在大规模的自我中心文本-视频对上进行预训练，然后在同步运动-视频数据上进行微调。该系统设计了部分解耦的运动注入方案，以实现对部分运动的精确控制，并通过联合重建框架确保长视频生成中的场景一致性。实验结果表明，PlayerOne在控制人类运动和建模多样场景方面具有出色的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.08570', 'title': 'Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation', 'url': 'https://huggingface.co/papers/2506.08570', 'abstract': 'A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM', 'score': 21, 'issue_id': 4260, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '1ad6cb6752b933a6', 'authors': ['Or Tal', 'Felix Kreuk', 'Yossi Adi'], 'affiliations': ['mail.huji.ac.il', 'meta.com'], 'pdf_title_img': 'assets/pdf/title_img/2506.08570.jpg', 'data': {'categories': ['#games', '#synthetic', '#architecture', '#training', '#audio'], 'emoji': '🎵', 'ru': {'title': 'Сравнение парадигм: ключ к совершенствованию генерации музыки по тексту', 'desc': 'Статья представляет систематическое сравнение двух основных парадигм моделирования в генерации музыки по тексту: авторегрессионного декодирования и условного согласования потоков. Исследователи провели контролируемое сравнение, используя идентичные наборы данных и конфигурации обучения. Оценка производительности проводилась по нескольким параметрам, включая качество генерации, устойчивость к конфигурациям вывода и масштабируемость. Результаты исследования выявляют различные сильные и слабые стороны каждой парадигмы, что может помочь в принятии архитектурных и обучающих решений в будущем.'}, 'en': {'title': 'Decoding the Future of Music Generation: A Paradigm Comparison', 'desc': 'This paper systematically compares two popular modeling paradigms in text-to-music generation: Auto-Regressive decoding and Conditional Flow-Matching. The authors focus on how these paradigms affect performance by conducting controlled experiments with identical datasets and training setups. They evaluate the models on various criteria, including generation quality, robustness, and editing capabilities. The findings reveal the unique strengths and weaknesses of each approach, providing valuable insights for future developments in music generation systems.'}, 'zh': {'title': '比较自回归与条件流匹配：文本到音乐生成的未来', 'desc': '本文系统比较了自回归解码和条件流匹配在文本到音乐生成中的应用，揭示了各自的优势和局限性。研究集中在建模范式上，通过相同的数据集和训练配置对模型进行控制比较。评估指标包括生成质量、对推理配置的鲁棒性、可扩展性以及对文本和时间对齐条件的遵循能力。此研究为未来文本到音乐生成系统的架构和训练决策提供了有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2506.08889', 'title': 'SeerAttention-R: Sparse Attention Adaptation for Long Reasoning', 'url': 'https://huggingface.co/papers/2506.08889', 'abstract': 'SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.', 'score': 18, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'a6e46d58b91f0fad', 'authors': ['Yizhao Gao', 'Shuming Guo', 'Shijie Cao', 'Yuqing Xia', 'Yu Cheng', 'Lei Wang', 'Lingxiao Ma', 'Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Hayden Kwok-Hay So', 'Yu Hua', 'Ting Cao', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Huazhong University of Science and Technology', 'Microsoft Research', 'Peking University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08889.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#reasoning', '#training', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Эффективное разреженное внимание для моделей рассуждения', 'desc': 'SeerAttention-R - это разреженная система внимания для моделей рассуждения, которая сохраняет высокую точность и достигает значительного ускорения с помощью оптимизированных ядер разреженного декодирования. Она обучается на всего 0,4 миллиардах токенов и поддерживает точность рассуждений почти без потерь с бюджетом в 4000 токенов в тесте AIME при больших размерах блоков разреженного внимания. Используя TileLang, авторы разработали высокооптимизированное ядро разреженного декодирования, которое достигает почти теоретического ускорения до 9 раз по сравнению с FlashAttention-3 на GPU H100 при 90% разреженности. SeerAttention-R можно легко интегрировать в существующие предобученные модели без изменения исходных параметров.'}, 'en': {'title': 'SeerAttention-R: Speed and Accuracy in Sparse Attention for Reasoning Models', 'desc': 'SeerAttention-R is a new framework designed for sparse attention in reasoning models, focusing on efficient long decoding. It builds on the original SeerAttention by using a self-distilled gating mechanism to learn attention sparsity while eliminating query pooling for better auto-regressive decoding. This framework is lightweight and can be easily integrated into existing pretrained models without altering their parameters. Our experiments show that SeerAttention-R achieves high accuracy with significant speed improvements, processing up to 4K tokens efficiently on advanced hardware.'}, 'zh': {'title': 'SeerAttention-R：高效稀疏注意力推理框架', 'desc': 'SeerAttention-R是一种稀疏注意力框架，专为推理模型的长解码而设计。它通过自蒸馏门控机制学习注意力稀疏性，同时去除了查询池化，以适应自回归解码。该框架轻量且灵活，可以无缝集成到现有的预训练模型中，而无需修改原始参数。实验表明，SeerAttention-R在AIME基准测试中以4K令牌预算保持接近无损的推理准确性，并在H100 GPU上实现了高达9倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2506.09003', 'title': 'SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner', 'url': 'https://huggingface.co/papers/2506.09003', 'abstract': 'A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce **SWE-Flow**, a novel data synthesis framework grounded in Test-Driven Development (TDD). Unlike existing software engineering data that rely on human-submitted issues, **SWE-Flow** automatically infers incremental development steps directly from unit tests, which inherently encapsulate high-level requirements. The core of **SWE-Flow** is the construction of a Runtime Dependency Graph (RDG), which precisely captures function interactions, enabling the generation of a structured, step-by-step *development schedule*. At each step, **SWE-Flow** produces a partial codebase, the corresponding unit tests, and the necessary code modifications, resulting in fully verifiable TDD tasks. With this approach, we generated 16,061 training instances and 2,020 test instances from real-world GitHub projects, creating the **SWE-Flow-Eval** benchmark. Our experiments show that fine-tuning open model on this dataset significantly improves performance in TDD-based coding. To facilitate further research, we release all code, datasets, models, and Docker images at [Github](https://github.com/Hambaobao/SWE-Flow).', 'score': 14, 'issue_id': 4251, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '794ea1a282cfa727', 'authors': ['Lei Zhang', 'Jiaxi Yang', 'Min Yang', 'Jian Yang', 'Mouxiang Chen', 'Jiajun Zhang', 'Zeyu Cui', 'Binyuan Hui', 'Junyang Lin'], 'affiliations': ['Alibaba Group, Beijing, China', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China', 'University of Science and Technology of China, Hefei, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09003.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#data', '#training', '#synthetic'], 'emoji': '🧪', 'ru': {'title': 'SWE-Flow: автоматизация TDD для улучшения ИИ-моделей в программировании', 'desc': 'SWE-Flow - это новая система синтеза данных для разработки программного обеспечения, основанная на разработке через тестирование (TDD). Она автоматически выводит этапы разработки из модульных тестов, создавая структурированный график разработки с помощью графа зависимостей времени выполнения (RDG). SWE-Flow генерирует частичную кодовую базу, соответствующие модульные тесты и необходимые изменения кода на каждом этапе. Эксперименты показывают, что дообучение открытых моделей на созданном наборе данных значительно улучшает их производительность в TDD-задачах.'}, 'en': {'title': 'Automating TDD with SWE-Flow: Smarter Development Schedules', 'desc': 'SWE-Flow is a new framework designed to enhance Test-Driven Development (TDD) by automatically generating development schedules from unit tests. It infers development steps directly from these tests, which represent high-level requirements, rather than relying on human-submitted issues. The framework constructs a Runtime Dependency Graph (RDG) to capture function interactions, allowing for a structured approach to coding tasks. By generating a large dataset from real-world projects, SWE-Flow significantly improves the performance of models fine-tuned for TDD-based coding.'}, 'zh': {'title': 'SWE-Flow：自动化测试驱动开发的创新框架', 'desc': 'SWE-Flow是一种新颖的数据合成框架，基于测试驱动开发（TDD）方法。它通过自动推断单元测试中的开发步骤，生成结构化的开发计划，从而提高了在真实项目上微调开放模型的性能。SWE-Flow的核心是构建运行时依赖图（RDG），准确捕捉函数之间的交互，确保每一步生成部分代码库及相应的单元测试。通过这种方法，我们从真实的GitHub项目中生成了大量的训练和测试实例，显著提升了基于TDD的编码性能。'}}}, {'id': 'https://huggingface.co/papers/2506.09984', 'title': 'InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions', 'url': 'https://huggingface.co/papers/2506.09984', 'abstract': "A novel framework for end-to-end human animation with multi-modal conditions enables high-quality video generation through explicit layout control and region-specific modality matching.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods.", 'score': 11, 'issue_id': 4252, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '97a59a9be9ba0dc3', 'authors': ['Zhenzhi Wang', 'Jiaqi Yang', 'Jianwen Jiang', 'Chao Liang', 'Gaojie Lin', 'Zerong Zheng', 'Ceyuan Yang', 'Dahua Lin'], 'affiliations': ['ByteDance', 'CUHK MMLab'], 'pdf_title_img': 'assets/pdf/title_img/2506.09984.jpg', 'data': {'categories': ['#multimodal', '#video', '#games', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Мультимодальная анимация множества людей с точным контролем расположения', 'desc': 'Статья представляет новую систему для анимации людей с использованием мультимодальных условий. Эта система позволяет генерировать высококачественные видео с точным контролем расположения и сопоставлением модальностей для конкретных областей. В отличие от существующих методов, данный подход может анимировать несколько субъектов одновременно, учитывая взаимодействия между людьми и объектами. Система использует маскированный предиктор для автоматического определения информации о расположении, что обеспечивает точное соответствие между сгенерированным видео и эталонными изображениями.'}, 'en': {'title': 'Revolutionizing Human Animation with Multi-Modal Control', 'desc': "This paper presents a new framework for creating human animations that can incorporate multiple types of input, such as text, images, and audio. Unlike previous methods that only animate one subject at a time, this approach allows for complex interactions between multiple characters and objects in a video. The framework uses a mask predictor to accurately match visual elements to specific regions in the video, ensuring that each character's appearance is correctly represented. Additionally, it integrates audio cues in a way that aligns with the visual layout, resulting in high-quality, controllable animations that reflect rich human interactions."}, 'zh': {'title': '多模态条件下的高质量人类动画生成', 'desc': '本文提出了一种新颖的端到端人类动画框架，能够在多模态条件下生成高质量视频。与传统方法不同，该框架支持多个概念的精确控制，允许人类与物体之间的丰富交互。通过区域特定的条件绑定，我们的方法能够自动推断布局信息，并确保不同模态之间的匹配。实验结果表明，该框架在多模态条件下的显式布局控制优于现有的隐式方法。'}}}, {'id': 'https://huggingface.co/papers/2506.09501', 'title': 'Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09501', 'abstract': 'The study investigates reproducibility issues in Large Language Models (LLMs) arising from hardware and precision variations, proposing a lightweight inference pipeline to enhance numerical stability while maintaining memory efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.', 'score': 9, 'issue_id': 4264, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'b14433f51b294499', 'authors': ['Jiayi Yuan', 'Hao Li', 'Xinheng Ding', 'Wenya Xie', 'Yu-Jhe Li', 'Wentian Zhao', 'Kun Wan', 'Jing Shi', 'Xia Hu', 'Zirui Liu'], 'affiliations': ['Adobe Inc.', 'Rice University', 'University of Minnesota Twin Cities'], 'pdf_title_img': 'assets/pdf/title_img/2506.09501.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#inference', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Стабильность в нестабильном мире: повышение воспроизводимости LLM', 'desc': 'Исследование посвящено проблемам воспроизводимости результатов больших языковых моделей (LLM) из-за вариаций в аппаратном обеспечении и точности вычислений. Авторы демонстрируют, что изменение конфигурации системы, такой как размер пакета при оценке, количество и версия GPU, может значительно влиять на генерируемые ответы, особенно в моделях для рассуждений. Основной причиной этой вариативности названа неассоциативность арифметики с плавающей запятой при ограниченной числовой точности. Для решения проблемы предложен легковесный конвейер вывода LayerCast, который хранит веса в 16-битной точности, но выполняет все вычисления в FP32, балансируя между эффективностью памяти и числовой стабильностью.'}, 'en': {'title': 'Enhancing Reproducibility in LLMs with LayerCast', 'desc': 'This paper explores the challenges of reproducibility in Large Language Models (LLMs) caused by variations in hardware and numerical precision. It highlights how changes in system configurations, such as GPU type and batch size, can lead to significant differences in model outputs, particularly in reasoning tasks. The authors identify that the non-associative nature of floating-point arithmetic contributes to this variability, which can affect accuracy and response length. To address these issues, they propose a new inference pipeline called LayerCast, which optimizes memory usage while ensuring numerical stability by using mixed precision during computations.'}, 'zh': {'title': '提升大型语言模型的可重复性与稳定性', 'desc': '本研究探讨了大型语言模型（LLMs）在硬件和精度变化下的可重复性问题，并提出了一种轻量级推理管道，以提高数值稳定性，同时保持内存效率。我们发现，LLM性能的可重复性非常脆弱，系统配置的变化（如评估批量大小、GPU数量和版本）会显著影响生成的响应。尤其是在推理模型中，早期标记的微小舍入差异可能导致思维链的分歧，从而影响准确性。为了解决这一问题，我们开发了LayerCast推理管道，使用16位精度存储权重，但在计算时使用FP32，以平衡内存效率和数值稳定性。'}}}, {'id': 'https://huggingface.co/papers/2506.05309', 'title': 'Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games', 'url': 'https://huggingface.co/papers/2506.05309', 'abstract': "An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.", 'score': 9, 'issue_id': 4260, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '474c2c6a688311bf', 'authors': ['Niv Eckhaus', 'Uri Berger', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'School of Computing and Information Systems, University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2506.05309.jpg', 'data': {'categories': ['#games', '#agents', '#open_source', '#multimodal', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'ЛЛМ-агент осваивает искусство асинхронного общения в Мафии', 'desc': 'Исследователи разработали адаптивного асинхронного ЛЛМ-агента для участия в онлайн-играх в Мафию. Агент продемонстрировал способность принимать решения не только о том, что сказать, но и когда это сделать, что важно для асинхронной коммуникации. Эксперименты показали, что агент выступает на уровне человеческих игроков как по игровым показателям, так и по способности вписаться в коллектив. Это исследование открывает путь к интеграции больших языковых моделей в реалистичные групповые взаимодействия со сложной социальной динамикой.'}, 'en': {'title': 'Bridging AI and Human Interaction in Asynchronous Settings', 'desc': 'This paper presents an adaptive asynchronous LLM-agent designed to participate in online Mafia games, showcasing its ability to mimic human players in complex social interactions. Unlike traditional LLMs that operate in synchronous settings, this agent can decide both what to say and when to say it, reflecting the nuances of real-world communication. The evaluation reveals that the agent performs comparably to human participants, effectively blending into the social dynamics of the game. The findings highlight the potential for LLMs to be integrated into various asynchronous environments, enhancing collaborative efforts in educational and professional contexts.'}, 'zh': {'title': '自适应异步代理：让AI融入人类社交游戏', 'desc': '这篇论文介绍了一种自适应的异步大型语言模型（LLM）代理，它在在线 Mafia 游戏中表现出与人类玩家相似的能力。这种代理不仅决定说什么，还决定何时说，这在许多现实世界的社交场合中至关重要。研究表明，该代理在游戏表现和与人类玩家的互动中都表现良好，能够有效融入人类社交动态。作者还发布了相关数据和代码，以促进对更真实的异步沟通的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2506.09937', 'title': 'SAFE: Multitask Failure Detection for Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.09937', 'abstract': 'SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.  \t\t\t\t\tAI-generated summary \t\t\t\t While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out-of-the-box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, pi_0, and pi_0-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results can be found at https://vla-safe.github.io/.', 'score': 7, 'issue_id': 4251, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '2a59fafef3ba118c', 'authors': ['Qiao Gu', 'Yuanliang Ju', 'Shengxiang Sun', 'Igor Gilitschenski', 'Haruki Nishimura', 'Masha Itkina', 'Florian Shkurti'], 'affiliations': ['Toyota Research Institute (TRI)', 'University of Toronto (UofT)', 'UofT Robotics Institute', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.09937.jpg', 'data': {'categories': ['#security', '#optimization', '#agents', '#robotics', '#agi'], 'emoji': '🤖', 'ru': {'title': 'SAFE: универсальный детектор ошибок для роботов-генералистов', 'desc': 'SAFE - это детектор ошибок для моделей зрения-языка-действия (VLA), который обобщается на новые задачи, обучаясь на высокоуровневых внутренних признаках моделей. Он анализирует пространство признаков VLA и обнаруживает, что эти модели имеют достаточные знания об успехе и неудаче задач, которые являются общими для разных задач. SAFE обучается на успешных и неудачных прогонах и оценивается на невиденных задачах. Детектор показывает лучшие результаты по сравнению с базовыми методами и обеспечивает оптимальный компромисс между точностью и временем обнаружения.'}, 'en': {'title': 'SAFE: Generalizing Failure Detection for Vision-Language-Action Models', 'desc': 'SAFE is a novel failure detector designed for vision-language-action models (VLAs) that enables them to generalize to new tasks. It leverages high-level internal features of VLAs to predict the likelihood of task failure, allowing robots to respond appropriately in unfamiliar environments. Unlike traditional failure detectors that are limited to specific tasks, SAFE is trained on both successful and failed attempts across various tasks, enhancing its adaptability. The effectiveness of SAFE is demonstrated through extensive testing on multiple policy architectures, achieving superior performance in failure detection compared to existing methods.'}, 'zh': {'title': 'SAFE：智能机器人故障检测的新方法', 'desc': 'SAFE是一个用于视觉-语言-行动模型的故障检测器，它能够通过学习模型的高层内部特征来推广到未见过的任务。尽管视觉-语言-行动模型在多种操作任务中表现出色，但在新任务上的成功率有限。为了让机器人安全地与环境互动，我们需要一个能够及时发出警报的故障检测器，以便机器人可以停止、回溯或请求帮助。我们提出的SAFE能够从VLA的内部特征中学习，并预测任务失败的可能性，经过广泛测试，显示出优越的故障检测性能。'}}}, {'id': 'https://huggingface.co/papers/2506.09736', 'title': 'Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.09736', 'abstract': "Visual perturbation framework enhances multimodal models' mathematical reasoning performance without additional training or algorithmic changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at https://github.com/YutingLi0606/Vision-Matters.", 'score': 3, 'issue_id': 4262, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'f9912b183b6548a4', 'authors': ['Yuting Li', 'Lai Wei', 'Kaipeng Zheng', 'Jingyuan Huang', 'Linghe Kong', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2506.09736.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#math', '#multimodal', '#training', '#open_source', '#cv'], 'emoji': '🧮', 'ru': {'title': 'Лучше видеть - лучше рассуждать: визуальные искажения улучшают математические способности ИИ', 'desc': 'Исследователи предложили простую систему визуальных искажений для улучшения математических рассуждений мультимодальных моделей без дополнительного обучения. Эксперименты показали, что языковые модели с описаниями изображений могут превзойти мультимодальные модели, работающие с необработанными визуальными данными. Предложенный подход включает три типа искажений: конкатенацию отвлекающих элементов, сохраняющее доминантность смешивание и случайное вращение. Результаты демонстрируют стабильное улучшение производительности в задачах математических рассуждений на нескольких наборах данных.'}, 'en': {'title': 'Enhancing Reasoning with Visual Perturbations', 'desc': "This paper introduces a visual perturbation framework that improves the mathematical reasoning abilities of multimodal large language models (MLLMs) without needing extra training or changes to the algorithms. The authors found that language-only models can perform as well as or better than MLLMs when given image captions, indicating a gap in how MLLMs process visual information. The proposed framework includes three types of visual perturbations that enhance the models' robustness and can be easily added to existing post-training processes. Through various experiments, the study shows that these perturbations lead to significant improvements in reasoning performance, emphasizing the importance of effective visual integration in multimodal models."}, 'zh': {'title': '视觉扰动提升多模态模型推理能力', 'desc': '这篇论文提出了一种视觉扰动框架，可以在不增加训练或算法修改的情况下，提高多模态模型的数学推理性能。研究发现，仅使用语言模型并结合图像描述，能够达到与多模态大语言模型相当甚至更好的表现。这表明当前的多模态模型在生成视觉描述时可能存在整合不足的问题。通过引入三种特定的扰动策略，论文展示了在多个数据集上数学推理性能的一致提升，强调了视觉扰动在多模态数学推理中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.09278', 'title': 'UFM: A Simple Path towards Unified Dense Correspondence with Flow', 'url': 'https://huggingface.co/papers/2506.09278', 'abstract': 'A Unified Flow & Matching model (UFM) improves dense image correspondence accuracy and speed by using a transformer architecture for unified data training, outperforming specialized methods for both optical flow and wide-baseline scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense image correspondence is central to many applications, such as visual odometry, 3D reconstruction, object association, and re-identification. Historically, dense correspondence has been tackled separately for wide-baseline scenarios and optical flow estimation, despite the common goal of matching content between two images. In this paper, we develop a Unified Flow & Matching model (UFM), which is trained on unified data for pixels that are co-visible in both source and target images. UFM uses a simple, generic transformer architecture that directly regresses the (u,v) flow. It is easier to train and more accurate for large flows compared to the typical coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than state-of-the-art flow methods (Unimatch), while also having 62% less error and 6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to demonstrate that unified training can outperform specialized approaches across both domains. This result enables fast, general-purpose correspondence and opens new directions for multi-modal, long-range, and real-time correspondence tasks.', 'score': 3, 'issue_id': 4263, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '3926eda7133e3fbf', 'authors': ['Yuchen Zhang', 'Nikhil Keetha', 'Chenwei Lyu', 'Bhuvan Jhamb', 'Yutian Chen', 'Yuheng Qiu', 'Jay Karhade', 'Shreyas Jha', 'Yaoyu Hu', 'Deva Ramanan', 'Sebastian Scherer', 'Wenshan Wang'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09278.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Единая модель для точного сопоставления изображений во всех сценариях', 'desc': 'Статья представляет Unified Flow & Matching model (UFM) - модель для улучшения точности и скорости плотного сопоставления изображений. UFM использует архитектуру трансформера для унифицированного обучения данных, превосходя специализированные методы как для оптического потока, так и для сценариев с широкой базовой линией. Модель на 28% точнее современных методов оптического потока и имеет на 62% меньше ошибок по сравнению с методами сопоставления для широкой базовой линии. UFM демонстрирует, что унифицированное обучение может превзойти специализированные подходы в обеих областях.'}, 'en': {'title': 'Unified Training for Superior Image Correspondence', 'desc': 'The Unified Flow & Matching model (UFM) enhances the accuracy and speed of dense image correspondence by employing a transformer architecture for unified training. It addresses the common challenge of matching content between images in both optical flow and wide-baseline scenarios, which have traditionally been treated separately. UFM directly regresses the (u,v) flow, making it simpler to train and more effective for large flows compared to previous methods that relied on coarse-to-fine cost volumes. This model achieves a 28% improvement in accuracy and is significantly faster, paving the way for advancements in multi-modal and real-time correspondence applications.'}, 'zh': {'title': '统一流与匹配模型：提升图像对应的速度与准确性', 'desc': '本文提出了一种统一流与匹配模型（UFM），旨在提高密集图像对应的准确性和速度。UFM采用了变换器架构，通过统一数据训练，能够同时处理光流估计和宽基线场景的匹配问题。与传统的粗到细成本体积方法相比，UFM在处理大流时更易于训练且更为准确。实验结果表明，UFM在准确性上比现有最先进的流方法提高了28%，并且在速度上比密集宽基线匹配器快了6.7倍。'}}}, {'id': 'https://huggingface.co/papers/2506.08900', 'title': 'MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis', 'url': 'https://huggingface.co/papers/2506.08900', 'abstract': 'MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT/SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: https://github.com/j-morano/MIRAGE.', 'score': 2, 'issue_id': 4258, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'd3860686fd37639a', 'authors': ['José Morano', 'Botond Fazekas', 'Emese Sükei', 'Ronald Fecso', 'Taha Emre', 'Markus Gumpinger', 'Georg Faustmann', 'Marzieh Oghbaie', 'Ursula Schmidt-Erfurth', 'Hrvoje Bogunović'], 'affiliations': ['Christian Doppler Laboratory for Artificial Intelligence in Retina, Institute of Artificial Intelligence, Center for Medical Data Science, Medical University of Vienna, Vienna, Austria', 'Comprehensive Center for AI in Medicine, Medical University of Vienna, Vienna, Austria', 'OPTIMA Lab, Department of Ophthalmology, Medical University of Vienna, Vienna, Austria'], 'pdf_title_img': 'assets/pdf/title_img/2506.08900.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#science', '#open_source', '#cv'], 'emoji': '👁️', 'ru': {'title': 'MIRAGE: Революция в ИИ-анализе офтальмологических изображений', 'desc': 'MIRAGE - это новая мультимодальная модель-основа (foundation model) для анализа изображений ОКТ и СЛО в офтальмологии. Она превосходит существующие общие и специализированные модели в задачах классификации и сегментации. MIRAGE обучена на большом наборе немаркированных данных, что позволяет ей лучше обобщаться на новые данные. Авторы также предлагают новый набор данных для оценки подобных моделей, который доступен публично вместе с самой MIRAGE.'}, 'en': {'title': 'MIRAGE: Revolutionizing Ophthalmic Image Analysis with Multimodal AI', 'desc': "MIRAGE is a multimodal foundation model designed to improve the classification and segmentation of ophthalmic images, specifically optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) images. It addresses the limitations of existing AI models that often require extensive labeled data and struggle with unseen datasets. By leveraging large amounts of unlabeled data, MIRAGE outperforms both general and specialized models in various tasks. The paper also introduces a new evaluation benchmark for OCT and SLO, demonstrating MIRAGE's effectiveness and potential for advancing AI in retinal image analysis."}, 'zh': {'title': 'MIRAGE：眼科图像分析的新标杆', 'desc': 'MIRAGE是一种多模态基础模型，专注于光学相干断层扫描（OCT）和扫描激光眼底照相（SLO）图像的分类和分割。该模型在这些任务上表现优于现有的通用和专业模型，解决了传统模型在独立未见数据上的性能不足问题。MIRAGE通过在大规模未标记数据集上训练，克服了对大量标注的依赖，并且在分割任务上进行了广泛验证。我们还提出了一个新的评估基准，进一步证明了MIRAGE在眼底OCT图像分析中的适用性和优越性。'}}}, {'id': 'https://huggingface.co/papers/2506.08008', 'title': 'Hidden in plain sight: VLMs overlook their visual representations', 'url': 'https://huggingface.co/papers/2506.08008', 'abstract': "Vision language models perform poorly on vision-centric tasks compared to their visual encoders, primarily due to ineffective utilization of visual information and inherited language priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.", 'score': 2, 'issue_id': 4262, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '4183c96f8f03c207', 'authors': ['Stephanie Fu', 'Tyler Bonnen', 'Devin Guillory', 'Trevor Darrell'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.08008.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#cv', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'VLM не дотягивают до своих визуальных энкодеров в задачах компьютерного зрения', 'desc': 'Исследование показывает, что модели компьютерного зрения и языка (VLM) значительно уступают своим визуальным энкодерам в задачах, ориентированных на зрение. Основные проблемы заключаются в неэффективном использовании визуальной информации и унаследованных языковых предубеждениях. Анализ выявил, что узким местом является роль языковой модели в решении задачи, а не деградация визуальных представлений или чувствительность к формулировке задачи. Работа помогает диагностировать режимы отказа VLM с открытым исходным кодом и предлагает ряд оценок для будущих исследований визуального понимания в этих моделях.'}, 'en': {'title': 'Unlocking Visual Potential in Vision Language Models', 'desc': 'This paper examines the performance of vision language models (VLMs) on tasks that are primarily visual in nature. It finds that VLMs struggle to effectively integrate visual and linguistic information, leading to significantly poorer performance compared to their visual encoders. The research identifies key issues such as the degradation of visual representations and the influence of language model priors on task performance. Ultimately, the study highlights that VLMs fail to utilize available visual information effectively, which hampers their ability to perform well on vision-centric benchmarks.'}, 'zh': {'title': '视觉语言模型的挑战与瓶颈', 'desc': '本文探讨了视觉语言模型（VLMs）在视觉任务中的表现不佳，主要原因在于它们未能有效利用视觉信息和继承的语言先验。我们通过比较VLMs与其视觉编码器的直接输出，分析了它们在视觉和语言信息整合方面的能力。研究发现，VLMs在视觉中心基准测试中的表现显著低于视觉编码器，接近随机水平。我们指出，VLMs在执行这些任务时的瓶颈主要在于语言模型未能有效利用可用的视觉信息。'}}}, {'id': 'https://huggingface.co/papers/2506.08001', 'title': 'Reparameterized LLM Training via Orthogonal Equivalence Transformation', 'url': 'https://huggingface.co/papers/2506.08001', 'abstract': "A new reParameterized training algorithm named POET uses Orthogonal Equivalence Transformation to optimize neurons, providing stable optimization and improved generalization for training large-scale neural networks including LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.", 'score': 2, 'issue_id': 4255, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'b47d919d8313c30a', 'authors': ['Zeju Qiu', 'Simon Buchholz', 'Tim Z. Xiao', 'Maximilian Dax', 'Bernhard Schölkopf', 'Weiyang Liu'], 'affiliations': ['Max Planck Institute for Intelligent Systems, Tübingen', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.08001.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'POET: стабильная оптимизация нейронов для обучения крупных языковых моделей', 'desc': 'POET - это новый алгоритм обучения нейронных сетей, использующий ортогональное эквивалентное преобразование для оптимизации нейронов. Он обеспечивает стабильную оптимизацию и улучшенную генерализацию при обучении крупномасштабных нейронных сетей, включая языковые модели. POET репараметризует каждый нейрон с помощью двух обучаемых ортогональных матриц и фиксированной случайной весовой матрицы. Эксперименты подтверждают эффективность и масштабируемость POET при обучении больших языковых моделей.'}, 'en': {'title': 'POET: Optimizing Neurons for Stable and Scalable Training', 'desc': 'The paper introduces POET, a new training algorithm designed to enhance the optimization of neurons in large-scale neural networks, particularly large language models (LLMs). POET employs Orthogonal Equivalence Transformation, which reparameterizes neurons using two learnable orthogonal matrices alongside a fixed random weight matrix. This method ensures the stability of the optimization process and improves the generalization capabilities of the models. The authors also present efficient approximations that allow POET to be flexible and scalable, demonstrating its effectiveness through extensive experiments.'}, 'zh': {'title': 'POET：优化神经元的重参数化训练新算法', 'desc': 'POET是一种新的重参数化训练算法，利用正交等价变换来优化神经元。该算法通过使用两个可学习的正交矩阵和一个固定的随机权重矩阵，对每个神经元进行重参数化。POET能够稳定地优化目标函数，并提高模型的泛化能力，特别适用于大规模神经网络的训练。实验结果验证了POET在训练大型语言模型（LLMs）方面的有效性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.09958', 'title': 'Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy', 'url': 'https://huggingface.co/papers/2506.09958', 'abstract': "Kvasir-VQA-x1, an expanded dataset for gastrointestinal endoscopy, addresses clinical complexity and visual diversity with large-scale question-answer pairs and visual augmentations to enhance multimodal AI system reliability in clinical settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a model's inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: https://github.com/Simula/Kvasir-VQA-x1 and https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", 'score': 1, 'issue_id': 4263, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '127078517320f2b7', 'authors': ['Sushant Gautam', 'Michael A. Riegler', 'Pål Halvorsen'], 'affiliations': ['Oslo Metropolitan University (OsloMet), Norway', 'Simula Metropolitan Center for Digital Engineering (SimulaMet), Norway', 'Simula Research Laboratory, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2506.09958.jpg', 'data': {'categories': ['#open_source', '#dataset', '#science', '#benchmark', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Революция в медицинском компьютерном зрении: Kvasir-VQA-x1 для надежной эндоскопической диагностики', 'desc': 'Датасет Kvasir-VQA-x1 расширяет возможности для разработки систем поддержки принятия решений в гастроэнтерологической эндоскопии. Он включает 159,549 новых пар вопрос-ответ, созданных с помощью больших языковых моделей для тестирования глубокого клинического мышления. Датасет содержит визуальные аугментации, имитирующие распространенные артефакты изображений, что повышает надежность мультимодальных систем искусственного интеллекта в клинических условиях. Kvasir-VQA-x1 предоставляет более сложный и клинически релевантный бенчмарк для ускорения разработки эффективных систем медицинского компьютерного зрения.'}, 'en': {'title': 'Enhancing Clinical AI with Kvasir-VQA-x1: A New Benchmark for MedVQA', 'desc': "Kvasir-VQA-x1 is a newly developed dataset aimed at improving Medical Visual Question Answering (MedVQA) systems for gastrointestinal endoscopy. It includes 159,549 question-answer pairs that enhance clinical reasoning and assess AI models' inference capabilities. The dataset also features visual augmentations to simulate common imaging artifacts, ensuring models are tested under realistic conditions. By providing a more complex and diverse benchmark, Kvasir-VQA-x1 seeks to foster the development of robust multimodal AI systems in clinical environments."}, 'zh': {'title': 'Kvasir-VQA-x1：提升临床决策支持的多模态数据集', 'desc': 'Kvasir-VQA-x1是一个扩展的数据集，专注于胃肠内窥镜检查，旨在解决临床复杂性和视觉多样性的问题。该数据集包含159,549个新的问答对，旨在测试更深层次的临床推理能力。我们使用大型语言模型系统生成这些问题，并通过视觉增强技术模拟常见的成像伪影，以提高模型在真实临床场景中的可靠性。Kvasir-VQA-x1为多模态人工智能系统的开发提供了更具挑战性和临床相关性的基准，促进了更可靠和有效的临床决策支持系统的进步。'}}}, {'id': 'https://huggingface.co/papers/2506.09669', 'title': 'Query-Level Uncertainty in Large Language Models', 'url': 'https://huggingface.co/papers/2506.09669', 'abstract': 'A method using Query-Level Uncertainty and Internal Confidence enables Large Language Models to determine knowledge boundaries efficiently, improving adaptability and reducing inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called Internal Confidence, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance.', 'score': 1, 'issue_id': 4262, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'b1281e93419bc3d5', 'authors': ['Lihu Chen', 'Gaël Varoquaux'], 'affiliations': ['Imperial College London, UK', 'Soda, Inria Saclay, France'], 'pdf_title_img': 'assets/pdf/title_img/2506.09669.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#math', '#rag', '#inference', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Умные границы: как научить ИИ знать свои пределы', 'desc': 'Предложен метод определения границ знаний для больших языковых моделей с использованием неопределенности на уровне запроса и внутренней уверенности. Этот подход позволяет моделям эффективно определять, могут ли они ответить на заданный вопрос без генерации токенов. Метод продемонстрировал превосходство над базовыми подходами в задачах фактического ответа на вопросы и математических рассуждений. Применение данного метода позволяет оптимизировать использование RAG и каскадирования моделей, снижая вычислительные затраты при сохранении производительности.'}, 'en': {'title': 'Enhancing LLMs with Knowledge Boundary Awareness', 'desc': "This paper presents a method that helps Large Language Models (LLMs) identify the limits of their knowledge using Query-Level Uncertainty and Internal Confidence. By understanding which queries they can answer confidently, LLMs can adapt their responses, engage in deeper reasoning, or choose to abstain from answering when uncertain. The proposed Internal Confidence method evaluates the model's performance across different layers and tokens without requiring additional training. Experimental results show that this approach not only improves the model's adaptability but also reduces inference costs while maintaining high performance in tasks like factual question answering and mathematical reasoning."}, 'zh': {'title': '识别知识边界，提升推理效率', 'desc': '本研究提出了一种利用查询级不确定性和内部信心的方法，帮助大型语言模型有效识别知识边界。这种机制使模型能够适应性推理，能够选择性地调用相关知识或采取保留机制，从而提高AI的效率和可信度。我们的方法通过自我评估来检测模型是否能够处理特定查询，而无需生成任何输出。实验结果表明，我们的内部信心方法在事实问答和数学推理任务中表现优于多个基线模型，同时降低了推理成本。'}}}, {'id': 'https://huggingface.co/papers/2506.09229', 'title': 'Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.09229', 'abstract': 'Cross-frame Representation Alignment improves video diffusion model fine-tuning by enhancing convergence and semantic coherence across frames.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning Video Diffusion Models (VDMs) at the user level to generate videos that reflect specific attributes of training data presents notable challenges, yet remains underexplored despite its practical importance. Meanwhile, recent work such as Representation Alignment (REPA) has shown promise in improving the convergence and quality of DiT-based image diffusion models by aligning, or assimilating, its internal hidden states with external pretrained visual features, suggesting its potential for VDM fine-tuning. In this work, we first propose a straightforward adaptation of REPA for VDMs and empirically show that, while effective for convergence, it is suboptimal in preserving semantic consistency across frames. To address this limitation, we introduce Cross-frame Representation Alignment (CREPA), a novel regularization technique that aligns hidden states of a frame with external features from neighboring frames. Empirical evaluations on large-scale VDMs, including CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA. We further validate CREPA across diverse datasets with varying attributes, confirming its broad applicability. Project page: https://crepavideo.github.io', 'score': 1, 'issue_id': 4262, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'c5fdb98160ab01fe', 'authors': ['Sungwon Hwang', 'Hyojin Jang', 'Kinam Kim', 'Minho Park', 'Jaegul choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09229.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#video'], 'emoji': '🎞️', 'ru': {'title': 'CREPA: Повышение качества и согласованности при дообучении видео-диффузионных моделей', 'desc': 'Статья представляет новый метод улучшения дообучения видео-диффузионных моделей (VDM) - Cross-frame Representation Alignment (CREPA). CREPA выравнивает скрытые состояния кадра с внешними признаками соседних кадров, что улучшает визуальное качество и семантическую согласованность между кадрами. Эксперименты на крупномасштабных VDM, таких как CogVideoX-5B и Hunyuan Video, показали эффективность CREPA при использовании с методами эффективного обучения параметров, например LoRA. Метод был успешно протестирован на различных наборах данных, подтверждая его широкую применимость.'}, 'en': {'title': 'Aligning Frames for Better Video Generation', 'desc': 'This paper introduces Cross-frame Representation Alignment (CREPA), a new technique designed to enhance the fine-tuning of Video Diffusion Models (VDMs). CREPA improves the convergence of VDMs by aligning the hidden states of individual frames with features from neighboring frames, which helps maintain semantic coherence across the video. The authors demonstrate that this method not only boosts visual quality but also ensures that the generated videos are more consistent in meaning throughout. Their experiments on various large-scale VDMs show that CREPA is effective and applicable across different datasets, making it a significant advancement in video generation.'}, 'zh': {'title': '跨帧表示对齐提升视频生成质量', 'desc': '本论文提出了一种新的正则化技术，称为跨帧表示对齐（CREPA），用于改进视频扩散模型（VDMs）的微调过程。CREPA通过将当前帧的隐藏状态与相邻帧的外部特征对齐，增强了帧之间的语义一致性。实验结果表明，CREPA在视觉保真度和跨帧语义一致性方面均有显著提升，尤其是在使用高效参数微调方法时。该方法在多个数据集上验证了其广泛适用性，展示了其在视频生成中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.09007', 'title': 'Branched Schrödinger Bridge Matching', 'url': 'https://huggingface.co/papers/2506.09007', 'abstract': 'BranchSBM, a novel generative modeling framework, extends Schr\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.', 'score': 1, 'issue_id': 4252, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '8f3c4a6be505cd98', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer and Information Science, University of Pennsylvania', 'Mila, Quebec AI Institute', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2506.09007.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#dataset', '#data'], 'emoji': '🌳', 'ru': {'title': 'BranchSBM: моделирование разветвленных путей в генеративных моделях', 'desc': 'BranchSBM - это новая система генеративного моделирования, расширяющая метод Schrödinger Bridge Matching для моделирования разветвленных стохастических путей. Она позволяет моделировать эволюцию от одного начального распределения к нескольким конечным состояниям. BranchSBM параметризует несколько зависящих от времени полей скоростей и процессов роста, что дает возможность представлять расхождение на уровне популяции в несколько конечных распределений. Этот метод особенно полезен для задач, связанных с навигацией по поверхности по нескольким путям, моделированием бифуркаций клеточных судеб и симуляцией расходящихся клеточных реакций на возмущения.'}, 'en': {'title': 'Branching Out: Modeling Multiple Outcomes with BranchSBM', 'desc': 'BranchSBM is a new framework in generative modeling that enhances the traditional Schr"odinger Bridge Matching by allowing for branched stochastic paths. Unlike previous methods that only model single paths between two distributions, BranchSBM can represent multiple outcomes from a single starting point. This is achieved by using multiple time-dependent velocity fields and growth processes, which capture the complexity of population-level divergence. The framework is particularly useful for applications like simulating cell fate decisions and navigating multi-path scenarios.'}, 'zh': {'title': '分支薛定谔桥匹配：捕捉多路径演化的生成建模新框架', 'desc': 'BranchSBM是一种新颖的生成建模框架，扩展了薛定谔桥匹配方法，以建模从单一初始分布到多个结果的分支随机路径和多路径演化。该方法通过参数化多个时间依赖的速度场和生长过程，能够表示从共同起源到多个不同结果的人口级别的分歧。与现有的单模态过渡方法相比，BranchSBM在处理多路径表面导航、细胞命运分叉建模以及模拟细胞对扰动的不同反应等任务中表现出更强的表达能力。总之，BranchSBM为生成建模提供了更丰富的工具，能够捕捉复杂的演化过程。'}}}, {'id': 'https://huggingface.co/papers/2506.06020', 'title': 'When to Trust Context: Self-Reflective Debates for Context Reliability', 'url': 'https://huggingface.co/papers/2506.06020', 'abstract': "A lightweight framework integrating token-level self-confidence and an asymmetric debate between agents enhances the robustness of large language models to contextual inconsistencies with minimal computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently encounter conflicts between their parametric knowledge and contextual input, often resulting in factual inconsistencies or hallucinations. We propose Self-Reflective Debate for Contextual Reliability (SR-DCR), a lightweight framework that integrates token-level self-confidence with an asymmetric multi-agent debate to adjudicate such conflicts. A critic, deprived of context, challenges a defender who argues from the given passage; a judge model evaluates the debate and determines the context's reliability. The final answer is selected by combining the verdict with model confidence. Experiments on the ClashEval benchmark demonstrate that SR-DCR consistently enhances robustness to misleading context while maintaining accuracy on trustworthy inputs, outperforming both classical debate and confidence-only baselines with minimal computational overhead. The code is available at https://github.com/smiles724/Self-Reflective-Debates.", 'score': 1, 'issue_id': 4264, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '2899485806a0e65b', 'authors': ['Zeqi Zhou', 'Fang Wu', 'Shayan Talaei', 'Haokai Zhao', 'Cheng Meixin', 'Tinson Xu', 'Amin Saberi', 'Yejin Choi'], 'affiliations': ['Brown University', 'Stanford University', 'University of Chicago', 'University of New South Wales', 'Xian University of Electronic Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.06020.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#training', '#hallucinations', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ и дебаты для повышения надежности языковых моделей', 'desc': 'Статья представляет легковесный фреймворк SR-DCR для улучшения устойчивости больших языковых моделей к контекстуальным несоответствиям. Он интегрирует самоуверенность на уровне токенов с асимметричными дебатами между агентами. Эксперименты показывают, что SR-DCR повышает устойчивость к вводящему в заблуждение контексту, сохраняя точность на надежных входных данных. Фреймворк превосходит как классические дебаты, так и базовые линии, основанные только на уверенности, с минимальными вычислительными затратами.'}, 'en': {'title': 'Debating for Better Contextual Understanding in AI', 'desc': "This paper introduces a new framework called Self-Reflective Debate for Contextual Reliability (SR-DCR) that improves the reliability of large language models when faced with conflicting information. It combines token-level self-confidence with a debate between two agents: a defender who uses the context and a critic who does not. A judge model evaluates their arguments to determine the reliability of the context. The results show that SR-DCR enhances the model's ability to handle misleading information while keeping its performance on accurate inputs intact, all with low computational costs."}, 'zh': {'title': '自我反思辩论：提升语言模型鲁棒性的新方法', 'desc': '本文提出了一种名为自我反思辩论的框架（SR-DCR），旨在提高大型语言模型在面对上下文不一致时的鲁棒性。该框架结合了基于标记的自信度和不对称的多智能体辩论，通过让批评者和辩护者进行辩论来解决知识与上下文之间的冲突。最终，评判模型会根据辩论结果和模型自信度来选择答案。实验结果表明，SR-DCR在处理误导性上下文时表现出更强的鲁棒性，同时在可信输入上保持准确性，且计算成本极低。'}}}, {'id': 'https://huggingface.co/papers/2506.13585', 'title': 'MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention', 'url': 'https://huggingface.co/papers/2506.13585', 'abstract': "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.", 'score': 184, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '05163c188bd37051', 'authors': ['MiniMax', ':', 'Aili Chen', 'Aonian Li', 'Bangwei Gong', 'Binyang Jiang', 'Bo Fei', 'Bo Yang', 'Boji Shan', 'Changqing Yu', 'Chao Wang', 'Cheng Zhu', 'Chengjun Xiao', 'Chengyu Du', 'Chi Zhang', 'Chu Qiao', 'Chunhao Zhang', 'Chunhui Du', 'Congchao Guo', 'Da Chen', 'Deming Ding', 'Dianjun Sun', 'Dong Li', 'Enwei Jiao', 'Haigang Zhou', 'Haimo Zhang', 'Han Ding', 'Haohai Sun', 'Haoyu Feng', 'Huaiguang Cai', 'Haichao Zhu', 'Jian Sun', 'Jiaqi Zhuang', 'Jiaren Cai', 'Jiayuan Song', 'Jin Zhu', 'Jingyang Li', 'Jinhao Tian', 'Jinli Liu', 'Junhao Xu', 'Junjie Yan', 'Junteng Liu', 'Junxian He', 'Kaiyi Feng', 'Ke Yang', 'Kecheng Xiao', 'Le Han', 'Leyang Wang', 'Lianfei Yu', 'Liheng Feng', 'Lin Li', 'Lin Zheng', 'Linge Du', 'Lingyu Yang', 'Lunbin Zeng', 'Minghui Yu', 'Mingliang Tao', 'Mingyuan Chi', 'Mozhi Zhang', 'Mujie Lin', 'Nan Hu', 'Nongyu Di', 'Peng Gao', 'Pengfei Li', 'Pengyu Zhao', 'Qibing Ren', 'Qidi Xu', 'Qile Li', 'Qin Wang', 'Rong Tian', 'Ruitao Leng', 'Shaoxiang Chen', 'Shaoyu Chen', 'Shengmin Shi', 'Shitong Weng', 'Shuchang Guan', 'Shuqi Yu', 'Sichen Li', 'Songquan Zhu', 'Tengfei Li', 'Tianchi Cai', 'Tianrun Liang', 'Weiyu Cheng', 'Weize Kong', 'Wenkai Li', 'Xiancai Chen', 'Xiangjun Song', 'Xiao Luo', 'Xiao Su', 'Xiaobo Li', 'Xiaodong Han', 'Xinzhu Hou', 'Xuan Lu', 'Xun Zou', 'Xuyang Shen', 'Yan Gong', 'Yan Ma', 'Yang Wang', 'Yiqi Shi', 'Yiran Zhong', 'Yonghong Duan', 'Yongxiang Fu', 'Yongyi Hu', 'Yu Gao', 'Yuanxiang Fan', 'Yufeng Yang', 'Yuhao Li', 'Yulin Hu', 'Yunan Huang', 'Yunji Li', 'Yunzhi Xu', 'Yuxin Mao', 'Yuxuan Shi', 'Yuze Wenren', 'Zehan Li', 'Zelin Li', 'Zhanxu Tian', 'Zhengmao Zhu', 'Zhenhua Fan', 'Zhenzhen Wu', 'Zhichao Xu', 'Zhihang Yu', 'Zhiheng Lyu', 'Zhuo Jiang', 'Zibo Gao', 'Zijia Wu', 'Zijian Song', 'Zijun Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.13585.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#architecture', '#reasoning', '#long_context', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'MiniMax-M1: Гибридный ИИ для эффективной обработки сложных задач', 'desc': 'MiniMax-M1 - это гибридная модель рассуждений с архитектурой Mixture-of-Experts и механизмом молниеносного внимания. Модель поддерживает контекст длиной 1 миллион токенов и эффективно обрабатывает длинные входные данные. MiniMax-M1 обучена с помощью масштабного обучения с подкреплением на разнообразных задачах, включая реальные среды разработки программного обеспечения. Эксперименты показывают, что модель превосходит аналоги в сложных задачах программирования, использовании инструментов и работе с длинным контекстом.'}, 'en': {'title': 'Revolutionizing Long-Input Processing with MiniMax-M1', 'desc': 'MiniMax-M1 is a groundbreaking hybrid-attention reasoning model that utilizes a Mixture-of-Experts architecture and a lightning attention mechanism to efficiently handle long input sequences. With 456 billion parameters and the ability to process up to 1 million tokens, it significantly outperforms previous models in context length. The model is trained using a novel reinforcement learning algorithm called CISPO, which enhances training efficiency by clipping importance sampling weights. MiniMax-M1 demonstrates superior performance in complex tasks, particularly in software engineering and long-context applications, making it a valuable tool for various AI challenges.'}, 'zh': {'title': '高效长输入处理的混合注意力模型', 'desc': 'MiniMax-M1是一种混合注意力推理模型，采用混合专家架构和闪电注意力机制，旨在高效处理长输入和强化学习任务。该模型基于之前的MiniMax-Text-01模型，具有4560亿个参数，并支持高达100万个token的上下文长度。MiniMax-M1在复杂任务中表现出色，尤其是在软件工程和工具利用方面。我们还提出了一种新颖的强化学习算法CISPO，进一步提高了训练效率。'}}}, {'id': 'https://huggingface.co/papers/2506.10521', 'title': "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning", 'url': 'https://huggingface.co/papers/2506.10521', 'abstract': "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.", 'score': 60, 'issue_id': 4325, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '3e2672b026127b5e', 'authors': ['Yuhao Zhou', 'Yiheng Wang', 'Xuming He', 'Ruoyao Xiao', 'Zhiwei Li', 'Qiantai Feng', 'Zijie Guo', 'Yuejin Yang', 'Hao Wu', 'Wenxuan Huang', 'Jiaqi Wei', 'Dan Si', 'Xiuqi Yao', 'Jia Bu', 'Haiwen Huang', 'Tianfan Fu', 'Shixiang Tang', 'Ben Fei', 'Dongzhan Zhou', 'Fenghua Ling', 'Yan Lu', 'Siqi Sun', 'Chenhui Li', 'Guanjie Zheng', 'Jiancheng Lv', 'Wenlong Zhang', 'Lei Bai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10521.jpg', 'data': {'categories': ['#reasoning', '#science', '#multimodal', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Новый бенчмарк для оценки научного мышления ИИ', 'desc': "Учёные разработали новый бенчмарк под названием Scientists' First Exam (SFE) для оценки научных когнитивных способностей мультимодальных больших языковых моделей (MLLM). SFE оценивает восприятие, понимание и сравнительное рассуждение в научном контексте. Бенчмарк состоит из 830 экспертно проверенных пар вопросов-ответов по визуальным данным, охватывающих 66 мультимодальных задач в пяти важных научных дисциплинах. Эксперименты показали, что современные модели GPT-4 и InternVL-3 достигают лишь 34.08% и 26.52% соответственно на SFE, что указывает на значительный потенциал для улучшения MLLM в научной сфере."}, 'en': {'title': 'Enhancing Scientific Discovery with MLLMs: The SFE Benchmark', 'desc': "The Scientists' First Exam (SFE) benchmark evaluates the cognitive abilities of Multimodal Large Language Models (MLLMs) in scientific contexts. It focuses on three key areas: perception of scientific signals, understanding of scientific attributes, and comparative reasoning. The benchmark includes 830 expert-verified visual question-answering pairs across various multimodal tasks in five important scientific disciplines. Results show that leading models like GPT-o3 and InternVL-3 perform below expectations, indicating a need for improvement in their scientific reasoning capabilities."}, 'zh': {'title': '科学认知能力的新评估标准', 'desc': '科学家首次考试（SFE）基准测试评估多模态大型语言模型（MLLMs）的科学认知能力，主要通过感知、理解和比较推理三个方面进行评估。当前的科学基准主要关注MLLMs的知识理解能力，未能充分评估其感知和推理能力。SFE基准包含830个经过专家验证的视觉问答对，涵盖66个多模态任务，涉及五个高价值学科。实验结果显示，现有的最先进模型在SFE上的表现仍有很大提升空间，表明MLLMs在科学领域的应用潜力巨大。'}}}, {'id': 'https://huggingface.co/papers/2506.11763', 'title': 'DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents', 'url': 'https://huggingface.co/papers/2506.11763', 'abstract': "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.", 'score': 42, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '197213635094ee83', 'authors': ['Mingxuan Du', 'Benfeng Xu', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['MetastoneTechnology, Beijing, China', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.11763.jpg', 'data': {'categories': ['#open_source', '#science', '#agents', '#alignment', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Комплексная оценка ИИ-агентов для глубоких исследований', 'desc': 'DeepResearch Bench - это система оценки возможностей агентов глубоких исследований в области качества исследований и точности поиска информации. Бенчмарк включает 100 исследовательских задач уровня PhD по 22 различным областям, разработанных экспертами. Предложены две новые методологии оценки, хорошо согласующиеся с человеческим суждением: метод на основе эталонов и оценка возможностей поиска информации. DeepResearch Bench доступен в открытом доступе для ускорения разработки практических агентов на основе языковых моделей.'}, 'en': {'title': 'Benchmarking Deep Research Agents for Superior Research Quality', 'desc': "DeepResearch Bench is a new framework designed to evaluate the performance of Deep Research Agents (DRAs) in generating high-quality research outputs. It includes 100 PhD-level tasks across 22 fields, allowing for a comprehensive assessment of DRAs' capabilities in information retrieval and report synthesis. The framework introduces two innovative evaluation methods: one that uses reference-based criteria to judge report quality, and another that measures citation effectiveness and accuracy. By providing these tools, DeepResearch Bench aims to enhance the development of LLM-based agents and improve their research efficiency."}, 'zh': {'title': '评估深度研究代理的新基准', 'desc': 'DeepResearch Bench是一个基准框架，用于评估深度研究代理在研究质量和信息检索准确性方面的能力。该框架包含100个由领域专家精心设计的博士级研究任务，涵盖22个不同领域。我们提出了两种新方法来评估这些代理的能力，一种是基于参考的评估方法，另一种是评估信息检索和引用准确性的框架。通过开源DeepResearch Bench及其关键组件，我们希望加速基于大型语言模型的代理的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.12571', 'title': 'DoTA-RAG: Dynamic of Thought Aggregation RAG', 'url': 'https://huggingface.co/papers/2506.12571', 'abstract': "DoTA-RAG improves retrieval and generation accuracy over massive web datasets using a dynamic routing pipeline and optimized embedding models, achieving high correctness scores while maintaining low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources.", 'score': 35, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '3676dd66819fb868', 'authors': ['Saksorn Ruangtanusak', 'Natthapath Rungseesiripak', 'Peerawat Rojratchadakorn', 'Monthol Charattrakool', 'Natapong Nitarach'], 'affiliations': ['SCB 10X Bangkok, Thailand', 'SCBX Bangkok, Thailand'], 'pdf_title_img': 'assets/pdf/title_img/2506.12571.jpg', 'data': {'categories': ['#optimization', '#survey', '#data', '#dataset', '#rag'], 'emoji': '🌐', 'ru': {'title': 'DoTA-RAG: Динамическая оптимизация поиска и генерации для масштабных веб-знаний', 'desc': 'В статье представлена система DoTA-RAG, улучшающая точность поиска и генерации в больших веб-датасетах. Система использует динамическую маршрутизацию и оптимизированные модели эмбеддингов. DoTA-RAG включает трехэтапный конвейер: переписывание запросов, динамическую маршрутизацию и многоступенчатый поиск. Система значительно повышает оценку корректности ответов при сохранении низкой задержки.'}, 'en': {'title': 'Boosting Retrieval and Generation with DoTA-RAG!', 'desc': 'DoTA-RAG is a new system designed to improve how we retrieve and generate information from large web datasets. It uses a three-stage process that includes rewriting queries, dynamically routing them to specialized sub-indexes, and retrieving and ranking results in multiple stages. By optimizing the embedding models and re-embedding a large dataset, DoTA-RAG significantly increases the accuracy of answers while keeping response times low. This makes it a promising tool for applications that need quick and reliable access to vast amounts of information.'}, 'zh': {'title': 'DoTA-RAG：快速可靠的知识检索与生成系统', 'desc': '本文介绍了DoTA-RAG（动态思维聚合RAG），这是一种优化的检索增强生成系统，旨在处理大规模网络知识索引。传统的RAG管道在处理庞大多样的数据集时，常常面临高延迟和准确性不足的问题。DoTA-RAG通过查询重写、动态路由到专业子索引以及多阶段检索和排名的三阶段管道来解决这些挑战。该系统在FineWeb-10BT语料库上重新嵌入并选择了优越的嵌入模型，从而显著提高了答案的正确性分数，同时保持了低延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.13654', 'title': 'Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning', 'url': 'https://huggingface.co/papers/2506.13654', 'abstract': 'Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.', 'score': 32, 'issue_id': 4326, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '52b4ddc8a62e646b', 'authors': ['Shulin Tian', 'Ruiqi Wang', 'Hongming Guo', 'Penghao Wu', 'Yuhao Dong', 'Xiuying Wang', 'Jingkang Yang', 'Hao Zhang', 'Hongyuan Zhu', 'Ziwei Liu'], 'affiliations': ['A*STAR, Singapore', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13654.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#training', '#multimodal', '#long_context', '#rl'], 'emoji': '🎥', 'ru': {'title': 'Ego-R1: Революция в анализе длительных эгоцентрических видео', 'desc': 'Ego-R1 - это новая система обработки сверхдлинных эгоцентрических видео, использующая структурированный процесс цепочки рассуждений с инструментами (Chain-of-Tool-Thought). Система применяет агента, обученного с помощью обучения с подкреплением, для декомпозиции сложных задач на модульные шаги. Ego-R1 использует двухэтапную парадигму обучения, включающую тонкую настройку предобученной языковой модели и обучение с подкреплением. Система значительно расширяет временной охват анализа видео с нескольких часов до недели, превосходя существующие методы.'}, 'en': {'title': 'Revolutionizing Video Understanding with Ego-R1', 'desc': 'Ego-R1 is a new framework designed to analyze ultra-long egocentric videos, which can last for days or even weeks. It employs a structured Chain-of-Tool-Thought (CoTT) process, allowing the system to break down complex reasoning tasks into manageable steps. The framework is powered by a reinforcement learning agent that learns to select and use specific tools for each step, enhancing its ability to answer questions about the video content. By training on a specially created dataset and evaluating on a new benchmark, Ego-R1 demonstrates improved performance in understanding long-duration videos compared to existing methods.'}, 'zh': {'title': 'Ego-R1：超长视频推理的新突破', 'desc': 'Ego-R1是一个基于强化学习的框架，旨在处理超长的自我中心视频。它采用了一种结构化的工具增强思维链（CoTT）过程，将复杂的推理分解为模块化步骤。通过强化学习训练的Ego-R1代理能够动态地提出逐步工具，以应对长时间范围内的推理任务。实验结果表明，Ego-R1在理解超长视频方面表现优异，时间覆盖范围从几小时扩展到一周。'}}}, {'id': 'https://huggingface.co/papers/2506.08343', 'title': 'Wait, We Don\'t Need to "Wait"! Removing Thinking Tokens Improves\n  Reasoning Efficiency', 'url': 'https://huggingface.co/papers/2506.08343', 'abstract': 'NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning.', 'score': 29, 'issue_id': 4324, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'bc2b3e7cb2a8d002', 'authors': ['Chenlong Wang', 'Yuanning Feng', 'Dongping Chen', 'Zhaoyang Chu', 'Ranjay Krishna', 'Tianyi Zhou'], 'affiliations': ['University College London', 'University of Maryland', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.08343.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#inference', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение без лишних слов', 'desc': 'Исследование представляет метод NoWait, который подавляет токены явной саморефлексии в больших языковых моделях во время вывода. Это позволяет сократить длину цепочки рассуждений на 27-51% без ущерба для полезности модели. Эксперименты проводились на десяти бенчмарках, охватывающих задачи рассуждения с текстом, изображениями и видео. NoWait предлагает простое решение для повышения эффективности мультимодального рассуждения в ИИ-системах.'}, 'en': {'title': 'NoWait: Streamlining Multimodal Reasoning for Efficiency', 'desc': "The paper introduces NoWait, a method that improves the efficiency of multimodal reasoning models by suppressing explicit self-reflection tokens during inference. These tokens, like 'Wait' and 'Hmm', often lead to unnecessary verbosity and can slow down the reasoning process. By removing these tokens, NoWait significantly shortens the reasoning paths while maintaining the effectiveness of the model. The results from various benchmarks demonstrate that this approach can reduce the length of reasoning trajectories by up to 51%, making it a valuable enhancement for large reasoning models."}, 'zh': {'title': 'NoWait：提升多模态推理效率的创新方法', 'desc': 'NoWait是一种新方法，通过在推理过程中抑制显式自我反思的标记（如“等一下”和“嗯”），来提高多模态推理的效率，而不降低模型的实用性。研究表明，传统的推理模型在复杂推理时常常会出现过度思考，导致输出冗长且重复，影响效率。通过在十个基准测试中进行广泛实验，NoWait能够将思维链的长度减少27%到51%。因此，NoWait为高效且保持实用性的多模态推理提供了一种简单有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.10055', 'title': 'TaskCraft: Automated Generation of Agentic Tasks', 'url': 'https://huggingface.co/papers/2506.10055', 'abstract': 'TaskCraft automates the generation of scalable, multi-tool, and complex agentic tasks to enhance prompt optimization and fine-tuning of agentic models.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic tasks, which require multi-step problem solving with autonomy, tool use, and adaptive reasoning, are becoming increasingly central to the advancement of NLP and AI. However, existing instruction data lacks tool interaction, and current agentic benchmarks rely on costly human annotation, limiting their scalability. We introduce TaskCraft, an automated workflow for generating difficulty-scalable, multi-tool, and verifiable agentic tasks with execution trajectories. TaskCraft expands atomic tasks using depth-based and width-based extensions to create structurally and hierarchically complex challenges. Empirical results show that these tasks improve prompt optimization in the generation workflow and enhance supervised fine-tuning of agentic foundation models. We present a large-scale synthetic dataset of approximately 36,000 tasks with varying difficulty to support future research on agent tuning and evaluation.', 'score': 24, 'issue_id': 4329, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'cc5a94a8870a39e9', 'authors': ['Dingfeng Shi', 'Jingyi Cao', 'Qianben Chen', 'Weichen Sun', 'Weizhen Li', 'Hongxuan Lu', 'Fangchen Dong', 'Tianrui Qin', 'King Zhu', 'Minghao Yang', 'Jian Yang', 'Ge Zhang', 'Jiaheng Liu', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO'], 'pdf_title_img': 'assets/pdf/title_img/2506.10055.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#optimization', '#training', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'TaskCraft: автоматизация создания сложных агентных задач для улучшения ИИ', 'desc': 'TaskCraft - это автоматизированный метод создания масштабируемых агентных задач для улучшения оптимизации промптов и дообучения агентных моделей. Он генерирует сложные многоэтапные задачи с использованием различных инструментов и адаптивных рассуждений. TaskCraft расширяет атомарные задачи, создавая структурно и иерархически сложные проблемы. Эмпирические результаты показывают, что такие задачи улучшают оптимизацию промптов и повышают качество обучения агентных фундаментальных моделей.'}, 'en': {'title': 'Automating Complex Tasks for Smarter AI Agents', 'desc': 'TaskCraft is a system designed to automatically create complex tasks that require agents to use multiple tools and solve problems independently. It addresses the limitations of current instruction data, which often lacks examples of tool interaction and relies on expensive human-created benchmarks. By generating scalable and verifiable agentic tasks, TaskCraft enhances the training and fine-tuning of AI models, making them more effective in handling multi-step challenges. The system has produced a large dataset of around 36,000 tasks of varying difficulty to aid future research in improving agent performance.'}, 'zh': {'title': 'TaskCraft：自动化生成复杂代理任务的利器', 'desc': 'TaskCraft 是一种自动化工具，旨在生成可扩展的多工具复杂任务，以优化代理模型的提示和微调。代理任务需要多步骤的问题解决能力、工具使用和自适应推理，这在自然语言处理和人工智能的发展中变得越来越重要。现有的指令数据缺乏工具交互，而当前的代理基准依赖昂贵的人类标注，限制了其可扩展性。TaskCraft 通过深度和宽度扩展来生成结构复杂的任务，并提供约36,000个不同难度的合成任务数据集，以支持未来的代理调优和评估研究。'}}}, {'id': 'https://huggingface.co/papers/2506.09482', 'title': 'Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression', 'url': 'https://huggingface.co/papers/2506.09482', 'abstract': "TransDiff, combining an Autoregressive Transformer and diffusion models, achieves superior image generation performance and speed, while Multi-Reference Autoregression further enhances its quality and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation.", 'score': 23, 'issue_id': 4336, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '44097add463b0792', 'authors': ['Dingcheng Zhen', 'Qian Qiao', 'Tan Yu', 'Kangxi Wu', 'Ziwei Zhang', 'Siyuan Liu', 'Shunshun Yin', 'Ming Tao'], 'affiliations': ['Soul AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.09482.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#benchmark', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'TransDiff: Революция в генерации изображений с помощью гибридных моделей', 'desc': 'TransDiff - это новая модель генерации изображений, объединяющая авторегрессионный трансформер и диффузионные модели. Она достигает превосходных результатов по качеству и скорости генерации изображений на бенчмарке ImageNet 256x256. Модель использует кодирование меток и изображений в семантические признаки высокого уровня и применяет диффузионную модель для оценки распределения сэмплов изображений. Дополнительно предложен метод мультиреференсной авторегрессии (MRAR), который улучшает качество и разнообразие генерируемых изображений.'}, 'en': {'title': 'TransDiff: Revolutionizing Image Generation with Speed and Quality', 'desc': 'TransDiff is a novel image generation model that integrates Autoregressive Transformers with diffusion models to enhance both performance and speed. It effectively encodes images and labels into high-level features, using a diffusion model to sample images from a learned distribution. On the ImageNet benchmark, TransDiff achieves impressive metrics, including a Fréchet Inception Distance (FID) of 1.61 and a significant reduction in inference time compared to existing models. Additionally, the introduction of Multi-Reference Autoregression (MRAR) allows the model to generate images by referencing multiple previous outputs, leading to improved diversity and quality in generated images.'}, 'zh': {'title': 'TransDiff：图像生成的新纪元', 'desc': 'TransDiff是一种结合自回归变换器和扩散模型的图像生成模型，具有更高的生成性能和速度。该模型通过编码标签和图像为高级语义特征，并利用扩散模型来估计图像样本的分布。在ImageNet 256x256基准测试中，TransDiff的表现显著优于单独使用自回归变换器或扩散模型的其他图像生成模型。通过引入多参考自回归（MRAR）方法，TransDiff进一步提高了生成图像的质量和多样性。'}}}, {'id': 'https://huggingface.co/papers/2506.13759', 'title': 'Discrete Diffusion in Large Language and Multimodal Models: A Survey', 'url': 'https://huggingface.co/papers/2506.13759', 'abstract': 'Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey', 'score': 21, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0a523ab9b7563360', 'authors': ['Runpeng Yu', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13759.jpg', 'data': {'categories': ['#math', '#training', '#diffusion', '#inference', '#multimodal', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Революция в языковом моделировании: дискретные диффузионные модели', 'desc': 'Статья представляет собой систематический обзор дискретных диффузионных языковых моделей (dLLMs) и мультимодальных языковых моделей (dMLLMs). В отличие от авторегрессионных моделей, dLLMs и dMLLMs используют параллельное декодирование и стратегию генерации на основе шумоподавления. Эти модели обеспечивают более быстрый вывод и лучшую контролируемость по сравнению с авторегрессионными аналогами. В работе рассматриваются математические основы, ключевые техники обучения и вывода, а также применения dLLMs и dMLLMs в различных областях.'}, 'en': {'title': 'Accelerating Language Generation with Discrete Diffusion Models', 'desc': 'This paper surveys Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs), highlighting their advantages over traditional autoregressive models. dLLMs and dMLLMs utilize a parallel decoding approach with full attention and denoising strategies, allowing for faster generation and improved output control. The paper reviews the historical development, mathematical foundations, and key techniques for training these models, as well as their applications in various domains. It also discusses the future potential of dLLMs and dMLLMs in advancing machine learning research and deployment.'}, 'zh': {'title': '离散扩散模型：加速生成与控制的未来', 'desc': '本文系统性地调查了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。与自回归模型不同，dLLMs和dMLLMs采用多标记并行解码的范式，利用全注意力机制和去噪生成策略，从而实现并行生成和更快的推理速度。这种新方法使得细粒度的输出控制和动态响应感知成为可能，这在自回归模型中是难以实现的。研究表明，dLLMs和dMLLMs在推理速度上可实现高达10倍的加速，且在多个领域的应用中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.13750', 'title': 'Test3R: Learning to Reconstruct 3D at Test Time', 'url': 'https://huggingface.co/papers/2506.13750', 'abstract': 'Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R.', 'score': 18, 'issue_id': 4324, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '68d521856e78273a', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Shizun Wang', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.13750.jpg', 'data': {'categories': ['#3d', '#training', '#optimization'], 'emoji': '🏛️', 'ru': {'title': 'Test3R: Повышение точности 3D-реконструкции через самообучение на тестовых данных', 'desc': 'Test3R - это новая техника обучения во время тестирования для 3D-реконструкции, которая улучшает геометрическую точность. Метод оптимизирует согласованность нейронной сети, используя самоконтролируемое обучение на триплетах изображений. Test3R генерирует реконструкции из пар изображений и максимизирует их геометрическую согласованность относительно общего изображения. Эксперименты показывают, что техника значительно превосходит предыдущие методы в задачах 3D-реконструкции и многоракурсной оценки глубины.'}, 'en': {'title': 'Boosting 3D Reconstruction Accuracy with Test3R', 'desc': "Test3R is a novel test-time learning approach designed to improve the accuracy of 3D reconstruction by leveraging self-supervised learning on image triplets. It addresses the limitations of traditional dense matching methods that rely on pairwise predictions, which can lead to inconsistencies in global geometry. By optimizing the network's output during testing, Test3R ensures that reconstructions from different image pairs maintain geometric consistency relative to a common reference image. This technique not only enhances performance on 3D reconstruction tasks but is also easy to implement and applicable to various models with minimal additional training requirements."}, 'zh': {'title': 'Test3R：提升3D重建精度的简单方法', 'desc': 'Test3R是一种用于3D重建的测试时学习技术，通过自监督学习优化网络一致性，从而提高几何精度。该方法利用图像三元组生成重建，确保不同图像对之间的一致性。Test3R的核心思想是在测试时最大化重建之间的几何一致性，确保模型输出在不同输入下保持一致。实验结果表明，Test3R在3D重建和多视角深度估计任务中显著优于现有的最先进方法，且适用性广泛，几乎不增加成本。'}}}, {'id': 'https://huggingface.co/papers/2506.11991', 'title': 'VGR: Visual Grounded Reasoning', 'url': 'https://huggingface.co/papers/2506.11991', 'abstract': 'VGR, a novel multimodal large language model, improves visual reasoning by detecting relevant image regions and integrating them into the reasoning process, outperforming existing models on multimodal benchmarks with reduced resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.', 'score': 16, 'issue_id': 4330, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '48fd0bae72ad378e', 'authors': ['Jiacong Wang', 'Zijian Kang', 'Haochen Wang', 'Haiyong Jiang', 'Jiawen Li', 'Bohong Wu', 'Ya Wang', 'Jiao Ran', 'Xiao Liang', 'Chao Feng', 'Jun Xiao'], 'affiliations': ['ByteDance Inc.', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.11991.jpg', 'data': {'categories': ['#reasoning', '#games', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'VGR: Точное визуальное рассуждение с меньшими затратами', 'desc': 'VGR - это новая мультимодальная большая языковая модель, которая улучшает визуальное рассуждение путем обнаружения релевантных областей изображения и интеграции их в процесс рассуждения. Модель использует комбинацию визуального обоснования и языковой дедукции, что позволяет ей лучше понимать детали изображений. VGR превосходит существующие модели на мультимодальных тестах, используя при этом меньше вычислительных ресурсов. Эксперименты показали значительное улучшение производительности на нескольких бенчмарках, требующих комплексного понимания деталей изображений.'}, 'en': {'title': 'VGR: Revolutionizing Visual Reasoning with Multimodal Integration', 'desc': 'The paper presents VGR, a new multimodal large language model designed to enhance visual reasoning by effectively integrating relevant image regions into its reasoning process. Unlike traditional models that rely solely on language, VGR detects important areas in images to improve its understanding and answers. This model is trained on a large-scale dataset that combines visual grounding with language reasoning, allowing it to perform better on complex visual tasks. VGR demonstrates significant improvements on multimodal benchmarks while using fewer resources, showcasing its efficiency and effectiveness in visual reasoning.'}, 'zh': {'title': 'VGR：提升视觉推理的新型多模态模型', 'desc': 'VGR是一种新型的多模态大型语言模型，旨在通过检测相关图像区域来改善视觉推理能力。与传统模型不同，VGR在推理过程中首先识别可能有助于解决问题的图像区域，然后基于这些区域提供准确的答案。该模型使用了一种名为VGR-SFT的大规模数据集，结合了视觉基础和语言推理的数据。实验结果表明，VGR在多模态基准测试中表现优异，同时资源使用效率更高。'}}}, {'id': 'https://huggingface.co/papers/2506.06962', 'title': 'AR-RAG: Autoregressive Retrieval Augmentation for Image Generation', 'url': 'https://huggingface.co/papers/2506.06962', 'abstract': 'Autoregressive Retrieval Augmentation enhances image generation through context-aware patch-level retrievals, improving performance over existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating knearest neighbor retrievals at the patch level. Unlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images, AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, enabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.', 'score': 16, 'issue_id': 4338, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '0d850cd9a51e6a48', 'authors': ['Jingyuan Qi', 'Zhiyang Xu', 'Qifan Wang', 'Lifu Huang'], 'affiliations': ['Meta', 'UC Davis', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06962.jpg', 'data': {'categories': ['#cv', '#benchmark', '#games', '#rag', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Умное расширение контекста для улучшения генерации изображений', 'desc': 'Авторегрессивное расширение поиска (AR-RAG) - это новый подход к генерации изображений, использующий контекстно-зависимый поиск на уровне фрагментов. В отличие от предыдущих методов, AR-RAG выполняет поиск на каждом шаге генерации, используя ранее сгенерированные фрагменты в качестве запросов. Предложены два фреймворка: Distribution-Augmentation in Decoding (DAiD) и Feature-Augmentation in Decoding (FAiD). Эксперименты на стандартных бенчмарках показали значительное улучшение производительности по сравнению с современными моделями генерации изображений.'}, 'en': {'title': 'Dynamic Patch Retrieval for Enhanced Image Generation', 'desc': 'The paper presents Autoregressive Retrieval Augmentation (AR-RAG), a new approach to improve image generation by using context-aware retrievals at the patch level. Unlike traditional methods that rely on a single static reference image, AR-RAG dynamically retrieves relevant patches during each generation step, allowing the model to adapt to changing requirements. This method helps to reduce common issues like over-copying and stylistic bias found in previous techniques. The authors introduce two frameworks, DAiD and FAiD, to implement AR-RAG effectively, showing significant improvements in performance on established benchmarks.'}, 'zh': {'title': '自回归检索增强：提升图像生成的新方法', 'desc': '本文介绍了一种新颖的图像生成增强方法，称为自回归检索增强（AR-RAG）。该方法通过在每个生成步骤中进行上下文感知的检索，动态地引入最相关的图像块，从而提高生成效果。与传统方法不同，AR-RAG避免了固定参考图像的限制，能够更好地适应生成过程中的变化需求。我们提出了两种并行框架，分别是解码中的分布增强（DAiD）和解码中的特征增强（FAiD），并在多个基准测试中验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.12915', 'title': 'PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization', 'url': 'https://huggingface.co/papers/2506.12915', 'abstract': "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization.", 'score': 14, 'issue_id': 4325, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '7f12645dbf1aa58d', 'authors': ['Meiling Tao', 'Chenghao Zhu', 'Dongyi Ding', 'Tiannan Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'South China Agricultural University', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12915.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'PersonaFeedback: новый стандарт оценки персонализации языковых моделей', 'desc': 'Представлен новый бенчмарк PersonaFeedback для оценки способности больших языковых моделей (LLM) генерировать персонализированные ответы на основе явно заданных пользовательских персон. Бенчмарк состоит из 8298 аннотированных тестовых примеров, разделенных на легкие, средние и сложные уровни. Результаты показывают, что даже современные LLM, способные решать сложные задачи рассуждений, испытывают трудности на сложном уровне PersonaFeedback. Анализ выявил ограничения текущих подходов к персонализации LLM, включая retrieval-augmented фреймворки.'}, 'en': {'title': 'Enhancing Personalization in LLMs with PersonaFeedback', 'desc': 'The paper introduces PersonaFeedback, a new benchmark designed to assess the ability of Large Language Models (LLMs) to generate personalized responses based on explicit user personas. This benchmark addresses the gap in evaluating LLM personalization, which has been overlooked compared to general reasoning capabilities. PersonaFeedback includes 8,298 human-annotated test cases categorized by complexity, revealing that even advanced LLMs struggle with nuanced personalization tasks. The findings highlight the limitations of current models and emphasize the need for improved frameworks in LLM personalization.'}, 'zh': {'title': '个性化生成的新基准：PersonaFeedback', 'desc': '本文介绍了一个新的基准测试，名为PersonaFeedback，旨在评估大型语言模型（LLM）生成个性化响应的能力。随着LLM能力的快速提升，个性化生成已成为一个重要的研究问题，但缺乏高质量的基准测试限制了这一领域的进展。PersonaFeedback通过提供预定义的用户角色和查询，直接评估模型生成针对特定用户角色的响应能力。研究结果表明，即使是最先进的LLM在处理复杂的个性化任务时也可能表现不佳，强调了当前模型在个性化生成方面的局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.03968', 'title': 'From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding', 'url': 'https://huggingface.co/papers/2506.03968', 'abstract': 'The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.', 'score': 13, 'issue_id': 4324, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '991991c3f686afa8', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.03968.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#alignment', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Синтез сложных инструкций для эффективного обучения языковых моделей', 'desc': 'Статья представляет метод генерации разнообразных и сложных инструкций для обучения больших языковых моделей (LLM) с использованием атрибутивного заземления. Авторы создали набор данных SynthQuestions, содержащий 1 миллион синтетических инструкций. Метод включает в себя нисходящий процесс атрибуции, который связывает реальные инструкции с конкретными пользователями, и восходящий процесс синтеза, использующий веб-документы для создания ситуаций и соответствующих инструкций. Модели, обученные на этом наборе данных, показали ведущие результаты на нескольких стандартных тестах.'}, 'en': {'title': 'Harnessing Attributed Grounding for Diverse Instruction Generation', 'desc': 'This paper introduces a novel method for generating diverse and complex instruction data for large language models (LLMs) through a technique called attributed grounding. The approach combines a top-down attribution process, which connects real-world instructions to specific user contexts, with a bottom-up synthesis process that creates meaningful instructions from web documents. By leveraging this framework, the authors successfully produce a large dataset of 1 million synthesized instructions, named SynthQuestions, which significantly enhances the performance of LLMs on various benchmarks. The results indicate that as more web data is utilized, the effectiveness of the models continues to improve, showcasing the importance of rich and varied instruction data for model alignment.'}, 'zh': {'title': '通过属性基础生成复杂指令数据，提升语言模型性能', 'desc': '本文提出了一种通过属性基础生成多样化和复杂的指令数据的方法，以提高大型语言模型的性能。该方法结合了自上而下的归因过程和自下而上的合成过程，能够有效地从真实指令和网络文档中生成有意义的指令。通过这种框架，我们构建了一个包含100万个指令的数据集SynthQuestions，并在多个基准测试中取得了领先的表现。该研究表明，利用丰富的网络文档可以大规模收集复杂的指令，从而提升模型的对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2506.13284', 'title': 'AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy', 'url': 'https://huggingface.co/papers/2506.13284', 'abstract': 'Combining supervised fine-tuning and reinforcement learning enhances reasoning models, especially when optimizing sampling temperature and leveraging strong initial fine-tuning, as demonstrated by the improved AceReason-Nemotron-1.1 model.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B', 'score': 11, 'issue_id': 4340, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'a8bb5987fc0b396b', 'authors': ['Zihan Liu', 'Zhuolin Yang', 'Yang Chen', 'Chankyu Lee', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.13284.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Синергия обучения с учителем и с подкреплением повышает эффективность моделей рассуждений', 'desc': 'Это исследование изучает синергию между обучением с учителем (SFT) и обучением с подкреплением (RL) для создания мощных моделей рассуждений. Авторы обнаружили, что сильная модель SFT приводит к лучшим результатам после масштабного RL-обучения при правильном подборе температуры сэмплирования. Модель AceReason-Nemotron-1.1 7B, созданная с использованием этих выводов, значительно превзошла предыдущую версию на сложных математических и кодовых бенчмарках. Исследование демонстрирует эффективность комбинирования SFT и RL для улучшения способностей модели к рассуждениям.'}, 'en': {'title': 'Boosting Reasoning Models with SFT and RL Synergy', 'desc': 'This paper explores how combining supervised fine-tuning (SFT) with reinforcement learning (RL) can improve reasoning models. The authors enhance SFT by increasing the number of prompts and responses, leading to better reasoning performance, especially when more prompts are used. They investigate whether a stronger SFT model results in better performance after RL training and how to set the sampling temperature for optimal exploration and exploitation. Their findings confirm that a robust SFT foundation, along with careful temperature management, leads to significant improvements in the AceReason-Nemotron-1.1 model, achieving state-of-the-art results in reasoning tasks.'}, 'zh': {'title': '结合监督微调与强化学习，提升推理模型的性能', 'desc': '本研究探讨了监督微调（SFT）与强化学习（RL）在开发强大推理模型中的协同作用。我们通过增加收集的提示数量和每个提示生成的响应数量来优化SFT训练数据，从而显著提升推理性能。研究表明，强大的SFT模型在经过大规模RL训练后，能够持续提高最终性能，尤其是在适当选择采样温度的情况下。我们的AceReason-Nemotron-1.1模型在数学和代码基准测试中表现优异，展示了后训练方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.12450', 'title': 'Language Surgery in Multilingual Large Language Models', 'url': 'https://huggingface.co/papers/2506.12450', 'abstract': "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance.", 'score': 10, 'issue_id': 4325, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '6c05e4a1a8b705dc', 'authors': ['Joanito Agili Lopo', 'Muhammad Ravi Shulthan Habibi', 'Tack Hwa Wong', 'Muhammad Ilham Ghozali', 'Fajri Koto', 'Genta Indra Winata', 'Peerat Limkonchotiwat', 'Alham Fikri Aji', 'Samuel Cahyawijaya'], 'affiliations': ['AI Singapore', 'Capital One', 'Cohere', 'Kreasof AI', 'MBZUAI', 'SEACrowd', 'Universitas Indonesia'], 'pdf_title_img': 'assets/pdf/title_img/2506.12450.jpg', 'data': {'categories': ['#alignment', '#machine_translation', '#inference', '#multilingual'], 'emoji': '🌐', 'ru': {'title': 'Улучшение кросс-языковых возможностей LLM через контроль скрытых представлений', 'desc': 'Исследование подтверждает естественное выравнивание представлений в больших языковых моделях (LLM), особенно в средних слоях. Авторы предлагают метод Inference-Time Language Control (ITLC), который использует латентную инъекцию для точного межъязыкового контроля. ITLC позволяет улучшить кросс-языковую производительность LLM и уменьшить языковую путаницу. Эксперименты показывают эффективность ITLC в сохранении семантической целостности при переключении между языками.'}, 'en': {'title': 'Enhancing Cross-Lingual Performance with Natural Representation Alignment', 'desc': 'This paper explores how large language models (LLMs) naturally align their representations across different languages, particularly in their middle layers. It confirms that this alignment allows for the separation of language-specific and language-agnostic information, which can be manipulated without losing meaning. The authors introduce a new technique called Inference-Time Language Control (ITLC) that uses latent injection to improve control over language generation in cross-lingual contexts. Their experiments show that ITLC effectively reduces language confusion while maintaining semantic integrity, enhancing the overall performance of LLMs in multilingual tasks.'}, 'zh': {'title': '提升跨语言性能的推理时语言控制', 'desc': '本研究确认了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现。我们实证验证了这种对齐的存在，并分析了其与显式设计的对齐模型的行为比较。基于这些发现，我们提出了一种新方法——推理时语言控制（ITLC），它利用潜在注入技术实现精确的跨语言控制。实验结果表明，ITLC在保持目标语言语义完整性的同时，显著提升了跨语言控制能力，解决了当前大型LLMs中存在的语言混淆问题。'}}}, {'id': 'https://huggingface.co/papers/2506.07961', 'title': 'BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.07961', 'abstract': 'BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, leveraging pre-trained vision-language models (VLMs) for building vision-language-action (VLA) models has emerged as a promising approach to effective robot manipulation learning. However, only few methods incorporate 3D signals into VLMs for action prediction, and they do not fully leverage the spatial structure inherent in 3D data, leading to low sample efficiency. In this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D inputs to multiple 2D images, ensuring input alignment with the VLM backbone, and (2) utilizes 2D heatmaps for action prediction, unifying the input and output spaces within a consistent 2D image space. In addition, we propose a scalable pre-training method that equips the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning. Extensive experiments show the proposed method is able to learn 3D manipulation efficiently and effectively. BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks. In RLBench, it improves the average success rate from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better performance in challenging generalization settings, boosting the average success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing baseline methods in terms of average success rate. In real-robot experiments, BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It generalizes robustly in multiple out-of-distribution settings, including visual disturbances and unseen instructions. Remarkably, it is able to achieve a success rate of 96.8% on 10+ tasks with only 3 trajectories per task, highlighting its extraordinary sample efficiency. Project Website:https://bridgevla.github.io/', 'score': 10, 'issue_id': 4324, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '3fcf8d6329af3962', 'authors': ['Peiyan Li', 'Yixiang Chen', 'Hongtao Wu', 'Xiao Ma', 'Xiangnan Wu', 'Yan Huang', 'Liang Wang', 'Tao Kong', 'Tieniu Tan'], 'affiliations': ['ByteDance Seed', 'CASIA', 'FiveAges', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2506.07961.jpg', 'data': {'categories': ['#rl', '#3d', '#games', '#optimization', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'BridgeVLA: Эффективное обучение роботов через проекцию 3D в 2D', 'desc': 'BridgeVLA - это новая модель машинного обучения для роботизированных манипуляций, объединяющая 3D-зрение, язык и действия. Она проецирует 3D-входы на 2D-изображения и использует 2D-тепловые карты для эффективного прогнозирования действий. Модель превосходит существующие методы в различных тестах, включая симуляции и эксперименты с реальными роботами. BridgeVLA демонстрирует высокую эффективность обучения и способность к обобщению в нестандартных ситуациях.'}, 'en': {'title': 'BridgeVLA: Bridging 3D Vision and Action with 2D Heatmaps', 'desc': "BridgeVLA is a novel model that integrates 3D vision with language and action prediction by projecting 3D inputs into 2D images. This approach allows it to utilize 2D heatmaps for more efficient action prediction, enhancing the model's performance in robot manipulation tasks. The model is pre-trained to predict these heatmaps, which helps it learn effectively from fewer samples. Extensive testing shows that BridgeVLA significantly outperforms existing methods in various benchmarks, demonstrating its robustness and efficiency in real-world applications."}, 'zh': {'title': 'BridgeVLA：高效的3D视觉-语言-动作模型', 'desc': 'BridgeVLA是一种3D视觉-语言-动作模型，它将3D输入投影到2D图像，并利用2D热图进行高效的动作预测。该模型通过将3D信号整合到视觉-语言模型中，充分利用了3D数据的空间结构，从而提高了样本效率。我们提出了一种可扩展的预训练方法，使得视觉-语言模型能够在下游策略学习之前预测2D热图。实验结果表明，BridgeVLA在多个基准测试中超越了现有的最先进方法，展现了卓越的学习效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2506.06366', 'title': 'AI Agent Behavioral Science', 'url': 'https://huggingface.co/papers/2506.06366', 'abstract': 'A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.', 'score': 7, 'issue_id': 4326, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '683be64d015db51c', 'authors': ['Lin Chen', 'Yunke Zhang', 'Jie Feng', 'Haoye Chai', 'Honglin Zhang', 'Bingbing Fan', 'Yibo Ma', 'Shiyuan Zhang', 'Nian Li', 'Tianhui Liu', 'Nicholas Sukiennik', 'Keyu Zhao', 'Yu Li', 'Ziyi Liu', 'Fengli Xu', 'Yong Li'], 'affiliations': ['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.06366.jpg', 'data': {'categories': ['#agi', '#healthcare', '#ethics', '#multimodal', '#agents', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'От модели к поведению: новый взгляд на изучение ИИ-агентов', 'desc': 'Предлагается новая область исследований - поведенческая наука ИИ-агентов. Она направлена на систематическое изучение поведения ИИ-агентов в различных контекстах, уделяя особое внимание внешним факторам и их взаимодействию. Исследования охватывают индивидуальных агентов, многоагентные системы и взаимодействие человека с агентами. Этот подход дополняет традиционные модельно-ориентированные методы и предоставляет инструменты для понимания, оценки и управления поведением автономных ИИ-систем в реальном мире.'}, 'en': {'title': 'Understanding AI Behavior: A New Scientific Approach', 'desc': "The paper introduces AI Agent Behavioral Science, a new field focused on studying the behaviors of AI agents in various contexts. It highlights that these behaviors arise not just from the AI's internal design but also from interactions with their environment and social dynamics. The approach emphasizes systematic observation, hypothesis testing, and theory-driven analysis to understand how AI agents adapt and interact over time. This perspective also addresses responsible AI considerations, such as fairness and accountability, making it a vital complement to traditional model-centric methods."}, 'zh': {'title': '探索人工智能代理的行为科学', 'desc': '本文提出了一个新的领域——人工智能代理行为科学，旨在系统研究人工智能代理在不同环境中的行为。随着大型语言模型的发展，AI代理展现出越来越人性化的行为，如规划、适应和社交动态。这些行为不仅源于模型的内部结构，还受到环境因素、社交线索和互动反馈的影响。该领域强调对行为的系统观察和干预设计，以促进对AI代理行为的理解和负责任的应用。'}}}, {'id': 'https://huggingface.co/papers/2506.09050', 'title': 'ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering', 'url': 'https://huggingface.co/papers/2506.09050', 'abstract': 'ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.  \t\t\t\t\tAI-generated summary \t\t\t\t How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements.', 'score': 6, 'issue_id': 4328, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f30a01d616f75ed3', 'authors': ['Yuki Imajuku', 'Kohki Horie', 'Yoichi Iwata', 'Kensho Aoki', 'Naohiro Takahashi', 'Takuya Akiba'], 'affiliations': ['AtCoder, Japan', 'Sakana AI, Japan', 'The University of Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.09050.jpg', 'data': {'categories': ['#benchmark', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'ALE-Bench: Испытание ИИ в алгоритмической оптимизации', 'desc': 'ALE-Bench - это новый эталонный тест для оценки систем искусственного интеллекта на основе алгоритмических соревнований по программированию. Он фокусируется на задачах оптимизации в таких областях, как маршрутизация доставки, планирование экипажей и балансировка электросетей. В отличие от кратковременных тестов на прохождение/непрохождение, ALE-Bench поощряет итеративное улучшение решений в течение длительного времени. Оценка современных языковых моделей показала, что хотя они демонстрируют высокую производительность на отдельных задачах, по сравнению с людьми остается заметный разрыв в последовательности решения задач и способности к долгосрочному решению проблем.'}, 'en': {'title': "Evaluating AI's Long-Term Problem-Solving with ALE-Bench", 'desc': 'ALE-Bench is a benchmark designed to assess AI systems on complex optimization problems derived from real-world algorithmic programming contests. It focuses on long-term iterative problem-solving in various domains, such as package delivery and factory production. The benchmark emphasizes the importance of refining solutions over extended periods, rather than just achieving quick pass/fail results. Our findings indicate that while advanced language models perform well on certain tasks, they still lag behind human consistency and long-term problem-solving abilities, underscoring the need for further development in AI.'}, 'zh': {'title': 'ALE-Bench：推动AI在复杂优化问题上的进步', 'desc': 'ALE-Bench是一个新的基准，用于评估人工智能系统在基于分数的算法编程竞赛中的表现，特别是在复杂的优化问题上。它关注于长期的迭代问题解决，涉及包裹投递、人员调度、工厂生产和电网平衡等领域。与短期的通过/不通过编码基准不同，ALE-Bench鼓励在较长时间内对解决方案进行细化。我们的评估显示，尽管前沿的大型语言模型在特定问题上表现良好，但在一致性和长期问题解决能力方面与人类相比仍存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2506.13404', 'title': 'A Technical Study into Small Reasoning Language Models', 'url': 'https://huggingface.co/papers/2506.13404', 'abstract': 'The research explores training strategies such as supervised fine-tuning, knowledge distillation, and reinforcement learning to enhance the performance of resource-efficient Small Reasoning Language Models with limited capacity.  \t\t\t\t\tAI-generated summary \t\t\t\t The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models.', 'score': 5, 'issue_id': 4336, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '1a80e44800fb97ab', 'authors': ['Xialie Zhuang', 'Peixian Ma', 'Zhikai Jia', 'Zheng Cao', 'Shiwei Liu'], 'affiliations': ['SCITIX (SGP) TECH PTE. LTD., Singapore', 'The Hong Kong University of Science and Technology (Guangzhou), China', 'University of Chinese Academy of Sciences, China', 'University of Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.13404.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Маленькие модели, большие возможности: оптимизация обучения SRLM', 'desc': 'Исследование посвящено изучению стратегий обучения малых языковых моделей рассуждения (SRLM) с около 0,5 миллиардами параметров. Авторы рассматривают методы контролируемой тонкой настройки (SFT), дистилляции знаний (KD) и обучения с подкреплением (RL) для улучшения производительности SRLM. Цель работы - преодолеть разрыв в эффективности между SRLM и более крупными моделями. Результаты исследования предлагают рекомендации по максимизации способностей рассуждения 0,5B моделей.'}, 'en': {'title': 'Boosting Small Models: Smart Training for Big Performance', 'desc': 'This research focuses on improving Small Reasoning Language Models (SRLMs) with around 0.5 billion parameters, which are efficient but struggle with complex tasks. It explores various training strategies like supervised fine-tuning, knowledge distillation, and reinforcement learning to boost their performance. The study aims to find effective methods to close the performance gap between these smaller models and larger, more powerful ones. By conducting extensive experiments, the authors provide practical recommendations for enhancing the reasoning abilities of SRLMs in resource-limited settings.'}, 'zh': {'title': '提升小型推理模型性能的训练策略', 'desc': '本研究探讨了多种训练策略，包括监督微调、知识蒸馏和强化学习，以提升资源高效的小型推理语言模型（SRLMs）的性能。这些模型具有约5亿个参数，尽管在处理复杂任务时面临挑战，但在计算效率和成本效益方面表现出色。研究还分析了如何缩小小型模型与大型模型之间的性能差距，并提出了针对小型架构的最佳训练流程。通过广泛的实验验证，我们的工作旨在为最大化5亿参数模型的推理能力提供可行的建议。'}}}, {'id': 'https://huggingface.co/papers/2506.06454', 'title': 'LETS Forecast: Learning Embedology for Time Series Forecasting', 'url': 'https://huggingface.co/papers/2506.06454', 'abstract': "DeepEDM integrates empirical dynamic modeling with deep neural networks to learn latent spaces and approximate complex nonlinear dynamics for improved time series forecasting.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world time series are often governed by complex nonlinear dynamics. Understanding these underlying dynamics is crucial for precise future prediction. While deep learning has achieved major success in time series forecasting, many existing approaches do not explicitly model the dynamics. To bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear dynamical systems modeling with deep neural networks. Inspired by empirical dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel deep model that learns a latent space from time-delayed embeddings, and employs kernel regression to approximate the underlying dynamics, while leveraging efficient implementation of softmax attention and allowing for accurate prediction of future time steps. To evaluate our method, we conduct comprehensive experiments on synthetic data of nonlinear dynamical systems as well as real-world time series across domains. Our results show that DeepEDM is robust to input noise, and outperforms state-of-the-art methods in forecasting accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.", 'score': 4, 'issue_id': 4329, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '4d7af8be98618ff0', 'authors': ['Abrar Majeedi', 'Viswanatha Reddy Gajjala', 'Satya Sai Srinath Namburi GNVV', 'Nada Magdi Elkordi', 'Yin Li'], 'affiliations': ['Department of Biostatistics and Medical Informatics, University of Wisconsin-Madison', 'Department of Computer Sciences, University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.06454.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#data'], 'emoji': '🔮', 'ru': {'title': 'DeepEDM: Глубокое обучение встречается с нелинейной динамикой для точного прогнозирования', 'desc': 'DeepEDM - это новый фреймворк, объединяющий эмпирическое динамическое моделирование с глубокими нейронными сетями для прогнозирования временных рядов. Он обучает латентное пространство на основе вложений с временной задержкой и использует ядерную регрессию для аппроксимации нелинейной динамики. DeepEDM применяет эффективную реализацию механизма внимания softmax и превосходит современные методы по точности прогнозирования. Эксперименты показали устойчивость метода к входному шуму на синтетических и реальных данных из разных областей.'}, 'en': {'title': 'DeepEDM: Bridging Dynamics and Deep Learning for Better Forecasting', 'desc': 'DeepEDM is a novel framework that combines empirical dynamic modeling with deep neural networks to enhance time series forecasting. It effectively learns latent spaces from time-delayed embeddings, allowing it to capture complex nonlinear dynamics in data. By utilizing kernel regression and softmax attention, DeepEDM can accurately predict future time steps while being robust to input noise. Comprehensive experiments demonstrate that it outperforms existing state-of-the-art forecasting methods across various domains.'}, 'zh': {'title': '深度动态建模，精准时间预测', 'desc': 'DeepEDM是一种将经验动态建模与深度神经网络相结合的框架，旨在提高时间序列预测的准确性。该方法通过学习时间延迟嵌入的潜在空间，利用核回归来近似复杂的非线性动态。DeepEDM受到Takens定理的启发，能够有效地实现softmax注意力机制，从而准确预测未来的时间步。实验结果表明，DeepEDM在处理输入噪声时表现出色，并在预测准确性上超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.12189', 'title': "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis", 'url': 'https://huggingface.co/papers/2506.12189', 'abstract': "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications.", 'score': 3, 'issue_id': 4324, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '952c0d68aa23cbda', 'authors': ['Pranav Agarwal', 'Ioana Ciucă'], 'affiliations': ['Google Deep Research', 'Institute', 'Mila', 'Quebec AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12189.jpg', 'data': {'categories': ['#dataset', '#small_models', '#reasoning', '#long_context', '#interpretability', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая личность искусственного интеллекта: новый подход к интерпретации языковых моделей', 'desc': "Исследование оценивает различные большие языковые модели (LLM) на разнообразных текстовых задачах с использованием нового набора данных, выявляя их отличительные личностные черты и улучшая интерпретируемость моделей. Авторы предлагают набор данных Supernova Event Dataset, содержащий разнообразные статьи, и используют его для оценки способности LLM извлекать и ранжировать ключевые события из текста. Анализ показывает, что разные модели демонстрируют различные подходы к рассуждению и анализу информации. Это исследование улучшает понимание 'личности' языковых моделей, делая их более удобными для широкого спектра приложений."}, 'en': {'title': 'Unveiling LLM Personalities for Better Interpretability', 'desc': 'This study investigates how different Large Language Models (LLMs) perform on various text tasks using a new dataset called the Supernova Event Dataset. The dataset includes a wide range of articles, allowing for the evaluation of LLMs in extracting and ranking key events, which requires complex reasoning and understanding of context. The research reveals distinct personality traits among the models, such as emotional reasoning in Orca 2 and strategic thinking in Qwen 2.5, enhancing our understanding of their decision-making processes. By using another LLM as a judge to assess these traits, the study improves the interpretability of models, making them more accessible for diverse applications.'}, 'zh': {'title': '揭示大型语言模型的个性特征', 'desc': '本研究评估了多种大型语言模型（LLMs）在不同文本任务上的表现，使用了一个新的数据集。通过分析模型的选择和分类事件，我们揭示了模型的个性特征，并提高了模型的可解释性。我们提出的超新星事件数据集包含多样的文章，帮助我们基准测试模型在提取和排序关键事件方面的能力。研究结果显示，不同模型在处理情感推理、战略分析和因果推理等方面表现出明显的个性差异。'}}}, {'id': 'https://huggingface.co/papers/2506.13752', 'title': 'Steering LLM Thinking with Budget Guidance', 'url': 'https://huggingface.co/papers/2506.13752', 'abstract': 'Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance.', 'score': 2, 'issue_id': 4325, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'c3d9b714e91736d6', 'authors': ['Junyan Li', 'Wenshuo Zhao', 'Yang Zhang', 'Chuang Gan'], 'affiliations': ['MIT-IBM Watson AI Lab', 'UMass Amherst', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13752.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#reasoning', '#math'], 'emoji': '💡', 'ru': {'title': 'Эффективное управление рассуждениями ИИ в рамках бюджета', 'desc': "Метод 'бюджетного руководства' позволяет управлять рассуждениями языковых моделей в рамках заданного бюджета без дополнительного обучения. Он использует легковесный предиктор, моделирующий гамма-распределение оставшейся длины рассуждения при генерации следующего токена. Этот подход обеспечивает естественный контроль длины рассуждения и значительно повышает эффективность использования токенов на сложных математических тестах. Метод также демонстрирует обобщающую способность на более широкий спектр задач и проявляет эмерджентные свойства, такие как оценка сложности вопросов."}, 'en': {'title': 'Steering LLM Reasoning with Budget Guidance for Efficiency and Performance', 'desc': 'This paper introduces a method called budget guidance, which helps large language models (LLMs) reason effectively within a specified budget of thinking tokens. By using a lightweight predictor that models a Gamma distribution, the method controls the reasoning length during the generation of each token without needing to fine-tune the LLM. This approach not only improves efficiency but also enhances performance on math benchmarks, achieving significant accuracy gains while using fewer tokens. Additionally, budget guidance shows versatility across different tasks and can even estimate the difficulty of questions.'}, 'zh': {'title': '预算引导：高效推理的新方法', 'desc': '预算引导是一种方法，可以在不进行微调的情况下，引导大型语言模型（LLM）在目标预算内进行推理，从而提高效率和性能。该方法通过引入一个轻量级预测器，建模剩余思考长度的伽马分布，来控制推理长度。预算引导确保生成过程遵循指定的思考预算，同时在数学基准测试中显著提高了令牌效率。与基线方法相比，在紧张预算下，预算引导在MATH-500基准上实现了高达26%的准确率提升，同时仅使用全思考模型63%的思考令牌。'}}}, {'id': 'https://huggingface.co/papers/2506.12953', 'title': 'Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition', 'url': 'https://huggingface.co/papers/2506.12953', 'abstract': 'PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions.', 'score': 2, 'issue_id': 4324, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'fb2789e38592ff5a', 'authors': ['Mayank Bumb', 'Anshul Vemulapalli', 'Sri Harsha Vardhan Prasad Jella', 'Anish Gupta', 'An La', 'Ryan A. Rossi', 'Hongjie Chen', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Yu Wang'], 'affiliations': ['Adobe', 'Dolby Labs', 'Intel', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2506.12953.jpg', 'data': {'categories': ['#data', '#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Точное прогнозирование временных рядов с помощью языковых моделей', 'desc': 'Статья представляет метод PatchInstruct для улучшения качества прогнозирования временных рядов с помощью больших языковых моделей (LLM). Метод использует специализированные техники промптинга, включая декомпозицию временных рядов, токенизацию на основе патчей и расширение данных с помощью похожих соседей. PatchInstruct позволяет LLM делать точные прогнозы без сложной архитектуры или масштабного дообучения. Это простой и гибкий подход, требующий минимальной предобработки данных.'}, 'en': {'title': 'Enhancing LLM Forecasting with Simple Prompting Techniques', 'desc': 'PatchInstruct is a method that improves the forecasting abilities of Large Language Models (LLMs) by using innovative prompting techniques. It incorporates time series decomposition to break down data into manageable parts, patch-based tokenization to efficiently handle input, and similarity-based neighbor augmentation to enhance predictions by considering related data points. This approach allows LLMs to perform time series forecasting without the need for extensive fine-tuning or complex architectures. Overall, PatchInstruct simplifies the process while boosting the accuracy of predictions in time series analysis.'}, 'zh': {'title': 'PatchInstruct：简化时间序列预测的有效方法', 'desc': 'PatchInstruct是一种增强大型语言模型（LLM）时间序列预测质量的方法。它通过专门的提示策略，如时间序列分解、基于补丁的标记化和相似性邻居增强，来实现这一目标。与以往需要大量微调的方法不同，PatchInstruct能够在不复杂重训练的情况下，灵活地进行时间序列预测。该方法保持了简单性，并且对数据的预处理要求最低。'}}}, {'id': 'https://huggingface.co/papers/2506.12623', 'title': 'MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos', 'url': 'https://huggingface.co/papers/2506.12623', 'abstract': 'A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization.', 'score': 2, 'issue_id': 4324, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': 'ef83eb4ade9dc4bf', 'authors': ['Yuan Zang', 'Hao Tan', 'Seunghyun Yoon', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Kushal Kafle', 'Chen Sun', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Brown University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12623.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#video', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Новый подход к сумматизации обучающих видео по UI', 'desc': 'Предложен новый бенчмарк и набор данных для мультимодальной сумматизации обучающих видео по пользовательским интерфейсам. Исследование направлено на создание пошаговых исполняемых инструкций и выделение ключевых кадров видео. Собран датасет из 2413 обучающих видео общей продолжительностью более 167 часов. Эксперименты показали, что современные методы мультимодальной сумматизации испытывают трудности с обработкой таких видео, что подчеркивает важность разработки новых подходов.'}, 'en': {'title': 'Enhancing Learning with UI Video Summarization', 'desc': 'This paper introduces a new benchmark and dataset specifically designed for multi-modal summarization of user interface (UI) instructional videos. The goal is to create efficient summaries that include step-by-step text instructions and key video frames, which are essential for effective learning. The authors highlight that existing benchmarks are inadequate for this purpose, as they focus on general video summarization rather than instructional content. Through extensive experiments on their dataset of 2,413 annotated UI instructional videos, they demonstrate that current multi-modal summarization techniques are not effective for this specific type of video, indicating a need for improved methods.'}, 'zh': {'title': '提升UI教学视频的多模态总结能力', 'desc': '本文提出了一种新的基准和数据集，用于多模态总结用户界面（UI）教学视频，旨在提供逐步可执行的指令和关键视频帧。现有的基准主要关注一般的语义级视频总结，无法满足教学视频中对逐步指令和插图的需求。我们收集了2413个UI教学视频的数据集，手动标注了视频分割、文本总结和视频总结，以便进行全面评估。实验结果表明，现有的多模态总结方法在UI视频总结上表现不佳，强调了开发新方法的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.12552', 'title': 'Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts', 'url': 'https://huggingface.co/papers/2506.12552', 'abstract': 'A novel methodology using large language models with curated prompts improves predictions of media outlet factuality and political bias, validated through experiments and error analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t In an age characterized by the proliferation of mis- and disinformation online, it is critical to empower readers to understand the content they are reading. Important efforts in this direction rely on manual or automatic fact-checking, which can be challenging for emerging claims with limited information. Such scenarios can be handled by assessing the reliability and the political bias of the source of the claim, i.e., characterizing entire news outlets rather than individual claims or articles. This is an important but understudied research direction. While prior work has looked into linguistic and social contexts, we do not analyze individual articles or information in social media. Instead, we propose a novel methodology that emulates the criteria that professional fact-checkers use to assess the factuality and political bias of an entire outlet. Specifically, we design a variety of prompts based on these criteria and elicit responses from large language models (LLMs), which we aggregate to make predictions. In addition to demonstrating sizable improvements over strong baselines via extensive experiments with multiple LLMs, we provide an in-depth error analysis of the effect of media popularity and region on model performance. Further, we conduct an ablation study to highlight the key components of our dataset that contribute to these improvements. To facilitate future research, we released our dataset and code at https://github.com/mbzuai-nlp/llm-media-profiling.', 'score': 2, 'issue_id': 4332, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '6c852f01e3464d88', 'authors': ['Zain Muhammad Mujahid', 'Dilshod Azizov', 'Maha Tufail Agro', 'Preslav Nakov'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, UAE', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2506.12552.jpg', 'data': {'categories': ['#alignment', '#data', '#ethics', '#open_source', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ИИ на страже достоверности: новый подход к оценке СМИ', 'desc': 'Статья представляет новую методологию использования больших языковых моделей (LLM) с специально подобранными промптами для улучшения прогнозирования фактической точности и политической предвзятости СМИ. Авторы предлагают подход, имитирующий критерии, используемые профессиональными фактчекерами для оценки целых новостных изданий, а не отдельных статей. Эксперименты показывают значительное улучшение по сравнению с сильными базовыми моделями, а также включают подробный анализ ошибок. Исследователи предоставляют набор данных и код для дальнейших исследований в этой области.'}, 'en': {'title': 'Empowering Readers: Assessing Media Factuality with LLMs', 'desc': 'This paper presents a new method that uses large language models (LLMs) to evaluate the factuality and political bias of media outlets. Instead of focusing on individual articles, the approach assesses the overall reliability of news sources by using curated prompts that mimic professional fact-checking criteria. The authors conducted experiments that showed significant improvements in prediction accuracy compared to existing methods, along with a detailed error analysis. They also released their dataset and code to support further research in this area.'}, 'zh': {'title': '利用大型语言模型提升媒体事实性与偏见预测', 'desc': '本文提出了一种新方法，利用大型语言模型和精心设计的提示来提高媒体来源的事实性和政治偏见预测。研究表明，评估整个新闻机构的可靠性比单独分析个别文章更有效。通过大量实验和错误分析，验证了该方法在多个大型语言模型上的显著改进。我们还发布了数据集和代码，以促进未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2506.10341', 'title': 'Provably Learning from Language Feedback', 'url': 'https://huggingface.co/papers/2506.10341', 'abstract': 'A formal framework and no-regret algorithm are introduced for learning from language feedback, addressing challenges in interactive learning with large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce transfer eluder dimension as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called HELiX, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that HELiX performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.', 'score': 2, 'issue_id': 4343, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'bbe8c2437ddea995', 'authors': ['Wanqiao Xu', 'Allen Nie', 'Ruijie Zheng', 'Aditya Modi', 'Adith Swaminathan', 'Ching-An Cheng'], 'affiliations': ['Microsoft Research', 'Netflix Research', 'Stanford University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.10341.jpg', 'data': {'categories': ['#agi', '#agents', '#training', '#rlhf', '#transfer_learning'], 'emoji': '🗣️', 'ru': {'title': 'Формализация обучения с языковой обратной связью для создания более эффективных интерактивных ИИ-агентов', 'desc': 'В статье представлена формальная структура и алгоритм без сожалений для обучения на основе языковой обратной связи. Авторы вводят понятие transfer eluder dimension как меру сложности для характеристики трудности задач обучения с языковой обратной связью. Разработан алгоритм HELiX, который решает такие задачи через последовательные взаимодействия с гарантиями производительности. Эмпирические эксперименты показывают, что HELiX работает хорошо даже когда многократные запросы к языковым моделям не дают надежных результатов.'}, 'en': {'title': 'Unlocking Learning Efficiency with Language Feedback', 'desc': "This paper introduces a formal framework for Learning from Language Feedback (LLF), which is essential for improving interactive learning with large language models (LLMs). It addresses the challenges of learning from language feedback by defining the transfer eluder dimension, a measure that helps understand the complexity of LLF problems. The authors present a no-regret algorithm named HELiX, which effectively learns from language feedback through sequential interactions, ensuring performance that scales with the problem's complexity. The findings suggest that learning from language feedback can be significantly more efficient than traditional reward-based learning methods."}, 'zh': {'title': '从语言反馈中高效学习的创新算法', 'desc': '本文提出了一个正式框架和无悔算法，用于从语言反馈中学习，解决了与大型语言模型的交互学习中的挑战。我们正式化了语言反馈学习（LLF）问题，并提出了足够的假设，以便在潜在奖励的情况下进行学习。引入了转移逃避维度作为复杂性度量，表征LLF问题的难度。我们展示了从丰富的语言反馈中学习可以比从奖励中学习快得多，并开发了名为HELiX的无悔算法，能够通过顺序交互有效解决LLF问题。'}}}, {'id': 'https://huggingface.co/papers/2506.09968', 'title': 'SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance', 'url': 'https://huggingface.co/papers/2506.09968', 'abstract': 'A gamified LLM-assisted system, SRLAgent, significantly improves self-regulated learning skills in college students through interactive, goal-setting, and real-time AI feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-regulated learning (SRL) is crucial for college students navigating increased academic demands and independence. Insufficient SRL skills can lead to disorganized study habits, low motivation, and poor time management, undermining learners ability to thrive in challenging environments. Through a formative study involving 59 college students, we identified key challenges students face in developing SRL skills, including difficulties with goal-setting, time management, and reflective learning. To address these challenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL skills through gamification and adaptive support from large language models (LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables students to engage in goal-setting, strategy execution, and self-reflection within an interactive game-based environment. The system offers real-time feedback and scaffolding powered by LLMs to support students independent study efforts. We evaluated SRLAgent using a between-subjects design, comparing it to a baseline system (SRL without Agent features) and a traditional multimedia learning condition. Results showed significant improvements in SRL skills within the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement compared to the baselines. This work highlights the value of embedding SRL scaffolding and real-time AI support within gamified environments, offering design implications for educational technologies that aim to promote deeper learning and metacognitive skill development.', 'score': 2, 'issue_id': 4330, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'ea72b7b36234ccbd', 'authors': ['Wentao Ge', 'Yuqing Sun', 'Ziyan Wang', 'Haoyue Zheng', 'Weiyang He', 'Piaohong Wang', 'Qianyu Zhu', 'Benyou Wang'], 'affiliations': ['City University of Hong Kong China', 'The Chinese University of Hong Kong, Shenzhen China'], 'pdf_title_img': 'assets/pdf/title_img/2506.09968.jpg', 'data': {'categories': ['#healthcare', '#games', '#multimodal', '#science', '#agents'], 'emoji': '🎓', 'ru': {'title': 'ИИ-ассистент для развития навыков самообучения', 'desc': 'Статья представляет SRLAgent - систему на основе больших языковых моделей (LLM), которая помогает студентам колледжей развивать навыки саморегулируемого обучения (SRL). SRLAgent использует геймификацию и адаптивную поддержку LLM для улучшения постановки целей, управления временем и рефлексивного обучения. Система основана на трехфазной модели SRL Циммермана и предоставляет интерактивную игровую среду с обратной связью в реальном времени. Результаты исследования показали значительное улучшение навыков SRL и вовлеченности студентов при использовании SRLAgent по сравнению с контрольными группами.'}, 'en': {'title': 'Empowering Students with Gamified AI for Self-Regulated Learning', 'desc': "The paper presents SRLAgent, a gamified system that enhances self-regulated learning (SRL) skills in college students by utilizing large language models (LLMs) for real-time feedback. It addresses common challenges students face, such as goal-setting and time management, by providing an interactive environment based on Zimmerman's SRL framework. A study with 59 participants demonstrated that SRLAgent significantly improved SRL skills and engagement compared to traditional learning methods. This research emphasizes the importance of integrating AI support and gamification in educational tools to foster independent learning and metacognitive development."}, 'zh': {'title': '游戏化系统提升大学生自我调节学习技能', 'desc': '本研究介绍了一种名为SRLAgent的系统，该系统通过游戏化和大型语言模型（LLM）的实时反馈，显著提升大学生的自我调节学习（SRL）技能。研究发现，许多学生在目标设定、时间管理和反思学习方面面临挑战。SRLAgent基于Zimmerman的三阶段SRL框架，帮助学生在互动游戏环境中进行目标设定、策略执行和自我反思。实验结果表明，使用SRLAgent的学生在SRL技能和参与度上均有显著提高。'}}}, {'id': 'https://huggingface.co/papers/2506.11115', 'title': 'Incorporating Domain Knowledge into Materials Tokenization', 'url': 'https://huggingface.co/papers/2506.11115', 'abstract': 'MATTER, a novel tokenization approach incorporating material knowledge, improves performance in scientific text processing tasks by maintaining structural and semantic material integrity.  \t\t\t\t\tAI-generated summary \t\t\t\t While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of 4% and 2% in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER', 'score': 2, 'issue_id': 4330, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'dfe8867cd5b71249', 'authors': ['Yerim Oh', 'Jun-Hyung Park', 'Junho Kim', 'SungHo Kim', 'SangKeun Lee'], 'affiliations': ['Department of Artificial Intelligence, Korea University', 'Department of Computer Science and Engineering, Korea University', 'Division of Language & AI, Hankuk University of Foreign Studies'], 'pdf_title_img': 'assets/pdf/title_img/2506.11115.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#multimodal', '#science'], 'emoji': '🧪', 'ru': {'title': 'MATTER: Токенизация с пониманием материаловедения', 'desc': 'MATTER - это новый подход к токенизации, который учитывает знания о материалах и улучшает обработку научных текстов. В отличие от стандартных методов токенизации, MATTER сохраняет структурную и семантическую целостность концепций материалов. Этот метод основан на модели MatDetector, обученной на базе знаний о материалах, и использует переранжирование для приоритизации концепций материалов при объединении токенов. Эксперименты показали, что MATTER превосходит существующие методы токенизации, улучшая результаты на 4% в задачах генерации и на 2% в задачах классификации.'}, 'en': {'title': 'MATTER: Tokenization that Understands Materials!', 'desc': 'The paper introduces MATTER, a new tokenization method designed specifically for scientific texts in materials science. Unlike traditional tokenization methods that often lead to loss of meaning and structure, MATTER incorporates material knowledge to preserve the integrity of material concepts. It utilizes a trained model called MatDetector and a re-ranking technique to effectively merge tokens while maintaining their semantic significance. Experimental results show that MATTER significantly improves performance in text generation and classification tasks compared to existing methods.'}, 'zh': {'title': 'MATTER：提升科学文本处理的分词新方法', 'desc': 'MATTER是一种新颖的分词方法，它结合了材料知识，旨在提高科学文本处理任务的性能。传统的分词方法往往基于频率，容易导致语义丢失和结构破坏，无法有效保持材料概念的完整性。MATTER通过使用训练好的MatDetector和优先考虑材料概念的重排序方法，确保在分词过程中保持材料概念的结构和语义完整。实验结果表明，MATTER在生成和分类任务中分别提高了4%和2%的性能，强调了领域知识在科学文本处理中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.13277', 'title': 'SeqPE: Transformer with Sequential Position Encoding', 'url': 'https://huggingface.co/papers/2506.13277', 'abstract': "SeqPE, a fully learnable position encoding framework, enhances the adaptability and scalability of positional encodings in Transformers, improving performance in various tasks and seamless multi-dimensional generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each n-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.", 'score': 1, 'issue_id': 4336, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '9c0c6eb35f5707c3', 'authors': ['Huyang Li', 'Yahui Liu', 'Hongyu Sun', 'Deng Cai', 'Leyang Cui', 'Wei Bi', 'Peilin Zhao', 'Taro Watanabe'], 'affiliations': ['Kuaishou Technology', 'Nara Institute of Science and Technology (NAIST)', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2506.13277.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#long_context', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SeqPE: гибкое кодирование позиций для универсальных трансформеров', 'desc': 'SeqPE - это новая система кодирования позиций для трансформеров, которая улучшает их адаптивность и масштабируемость. В отличие от традиционных методов с фиксированными таблицами поиска, SeqPE представляет каждый n-мерный индекс позиции как символьную последовательность и использует легковесный энкодер для обучения их эмбеддингов. Система включает контрастивную цель и дистилляцию знаний для улучшения экстраполяции. Эксперименты показали превосходство SeqPE над базовыми методами в задачах языкового моделирования, ответов на вопросы и классификации изображений.'}, 'en': {'title': 'SeqPE: Revolutionizing Positional Encoding for Transformers', 'desc': 'SeqPE is a novel position encoding framework designed for Transformers that enhances their adaptability and scalability. Unlike traditional fixed-size positional encodings, SeqPE uses a fully learnable approach that allows for better extrapolation beyond pre-trained sequence lengths. It incorporates a contrastive objective and knowledge distillation to improve the embedding space, ensuring that the model can generalize effectively to new tasks and dimensions. Experiments show that SeqPE outperforms existing methods in various applications, including language modeling and image classification, without needing extensive modifications to the architecture.'}, 'zh': {'title': 'SeqPE：提升Transformer位置编码的适应性与可扩展性', 'desc': 'SeqPE是一种完全可学习的位置编码框架，旨在提高Transformer模型中位置编码的适应性和可扩展性。传统的可学习位置嵌入方法由于固定大小的查找表，限制了超出预训练序列长度的外推能力。SeqPE通过将每个n维位置索引表示为符号序列，并使用轻量级的顺序位置编码器进行端到端学习，克服了这一限制。实验结果表明，SeqPE在语言建模、长上下文问答和二维图像分类等任务中表现优异，尤其在上下文长度外推方面，能够无缝地推广到多维输入。'}}}, {'id': 'https://huggingface.co/papers/2506.12299', 'title': 'QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety', 'url': 'https://huggingface.co/papers/2506.12299', 'abstract': 'QGuard, a safety guard method using question prompting, effectively defends LLMs against harmful and multi-modal malicious prompts without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts.', 'score': 1, 'issue_id': 4330, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': '52cafcd463738b45', 'authors': ['Taegyeong Lee', 'Jeonghwa Yoo', 'Hyoungseo Cho', 'Soo Yong Kim', 'Yunho Maeng'], 'affiliations': ['A.I.MATICS Inc.', 'Ewha Womans University', 'FnGuide Inc.', 'Safe Generative AI Lab, MODULABS'], 'pdf_title_img': 'assets/pdf/title_img/2506.12299.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#security', '#hallucinations'], 'emoji': '🛡️', 'ru': {'title': 'QGuard: Защита языковых моделей вопросами', 'desc': 'QGuard - это метод защиты больших языковых моделей (LLM) от вредоносных и мультимодальных атак с использованием вопросных промптов. Он работает без дополнительного обучения модели и эффективен как для текстовых, так и для мультимодальных вредоносных запросов. QGuard использует разнообразные защитные вопросы, что делает его устойчивым к новейшим вредоносным промптам. Экспериментальные результаты показывают конкурентоспособную эффективность метода на текстовых и мультимодальных наборах данных.'}, 'en': {'title': 'QGuard: Safeguarding LLMs with Smart Question Prompting', 'desc': 'QGuard is a novel safety guard method designed to protect Large Language Models (LLMs) from harmful and multi-modal malicious prompts without the need for fine-tuning. It employs question prompting to effectively block these harmful inputs in a zero-shot manner, making it versatile against various types of attacks. The method not only addresses text-based threats but also extends its defense to multi-modal prompt attacks, showcasing its robustness. Experimental results indicate that QGuard performs competitively across different datasets, providing valuable insights for enhancing the security of LLM services.'}, 'zh': {'title': 'QGuard：保护LLMs的安全防护新方法', 'desc': 'QGuard是一种使用问题提示的安全防护方法，能够有效防御大型语言模型（LLMs）对抗有害和多模态恶意提示，而无需进行微调。该方法通过零-shot的方式阻止有害提示，保护LLMs免受文本和多模态攻击。通过多样化和修改防护问题，QGuard在面对最新的有害提示时依然保持强大的鲁棒性。实验结果表明，该模型在文本和多模态有害数据集上表现出色，为实际的LLM服务提供了重要的安全风险缓解方案。'}}}, {'id': 'https://huggingface.co/papers/2506.12258', 'title': 'EgoPrivacy: What Your First-Person Camera Says About You?', 'url': 'https://huggingface.co/papers/2506.12258', 'abstract': "EgoPrivacy evaluates privacy risks in egocentric vision through a large-scale benchmark, revealing that foundation models can infer private information about camera wearers with high accuracy in zero-shot settings.  \t\t\t\t\tAI-generated summary \t\t\t\t While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70-80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy.", 'score': 1, 'issue_id': 4332, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '80aaa11f21bae6d3', 'authors': ['Yijiang Li', 'Genpei Zhang', 'Jiacheng Cheng', 'Yi Li', 'Xiaojun Shan', 'Dashan Gao', 'Jiancheng Lyu', 'Yuan Li', 'Ning Bi', 'Nuno Vasconcelos'], 'affiliations': ['Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.', 'University of California, San Diego', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12258.jpg', 'data': {'categories': ['#leakage', '#rag', '#ethics', '#benchmark'], 'emoji': '🕵️', 'ru': {'title': 'Эгоцентрическое зрение: скрытая угроза приватности', 'desc': 'Исследование EgoPrivacy оценивает риски конфиденциальности в эгоцентрическом зрении с помощью масштабного бенчмарка. Оно показывает, что фундаментальные модели могут с высокой точностью выводить личную информацию о носителях камер в режиме нулевого обучения. Работа охватывает три типа конфиденциальности: демографическую, индивидуальную и ситуационную, определяя семь задач для восстановления личной информации. Предложена новая стратегия атаки с использованием ретривала, которая повышает эффективность атак на демографическую конфиденциальность.'}, 'en': {'title': 'EgoPrivacy: Unveiling Hidden Risks in Wearable Camera Data', 'desc': 'EgoPrivacy is a benchmark designed to assess privacy risks associated with egocentric vision, particularly from wearable cameras. The study reveals that foundation models can accurately infer sensitive information about the camera wearer, such as identity and demographic details, even without prior training on specific data. It introduces a novel attack method called Retrieval-Augmented Attack, which enhances the effectiveness of privacy attacks by utilizing external video data. The results show that private information can be compromised with high accuracy, highlighting significant privacy concerns in the use of wearable cameras.'}, 'zh': {'title': 'EgoPrivacy：揭示自我中心视觉中的隐私风险', 'desc': 'EgoPrivacy是一个用于评估自我中心视觉隐私风险的大规模基准。研究表明，基础模型能够在零样本设置下高效推断摄像头佩戴者的私人信息。该研究定义了三种隐私类型，并提出了七个任务，以恢复从佩戴者身份到年龄组等不同层次的私人信息。此外，研究还提出了一种新颖的攻击策略，通过外部视频库增强人口统计隐私攻击的效果，显示出佩戴者的私人信息极易泄露。'}}}, {'id': 'https://huggingface.co/papers/2506.13430', 'title': 'Uncertainty-Aware Remaining Lifespan Prediction from Images', 'url': 'https://huggingface.co/papers/2506.13430', 'abstract': 'Vision transformer models predict remaining lifespan from images with high accuracy and well-calibrated uncertainty estimates.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research.', 'score': 0, 'issue_id': 4331, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'ce0b4c81ed1c657b', 'authors': ['Tristan Kenneweg', 'Philip Kenneweg', 'Barbara Hammer'], 'affiliations': ['University of Bielefeld'], 'pdf_title_img': 'assets/pdf/title_img/2506.13430.jpg', 'data': {'categories': ['#optimization', '#dataset', '#healthcare', '#science', '#open_source', '#cv'], 'emoji': '🔮', 'ru': {'title': 'ИИ предсказывает продолжительность жизни по фото', 'desc': 'Исследователи разработали метод, использующий предобученные модели Vision Transformer для оценки оставшейся продолжительности жизни по изображениям лица и тела. Модель достигает наилучшей точности предсказания со средней абсолютной ошибкой 4.79-7.48 лет на разных наборах данных. Важно, что модель предоставляет хорошо калиброванные оценки неопределенности. Хотя метод не предназначен для клинического применения, он демонстрирует потенциал извлечения медицински значимых сигналов из изображений.'}, 'en': {'title': 'Transforming Images into Lifespan Predictions with Uncertainty', 'desc': "This paper presents a method using vision transformer models to predict remaining lifespan from facial and whole-body images. The approach not only provides accurate lifespan estimates but also quantifies uncertainty in these predictions, which is crucial for understanding the reliability of the model's outputs. By learning a Gaussian distribution for each sample, the model effectively captures how uncertainty varies with the true remaining lifespan. The results show a significant improvement in prediction accuracy on new datasets, emphasizing the potential of using image analysis for health screening."}, 'zh': {'title': '从图像预测剩余寿命的创新方法', 'desc': '本文提出了一种利用预训练的视觉变换器模型，从面部和全身图像中预测剩余寿命的方法。该方法不仅具有高准确性，还能有效量化预测的不确定性。研究表明，预测的不确定性与真实的剩余寿命之间存在系统性的变化，并且可以通过为每个样本学习高斯分布来有效建模。我们的模型在多个数据集上实现了最先进的平均绝对误差，展示了从图像中提取医学相关信号的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.13172', 'title': 'Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns', 'url': 'https://huggingface.co/papers/2506.13172', 'abstract': "Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.  \t\t\t\t\tAI-generated summary \t\t\t\t We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing.", 'score': 0, 'issue_id': 4328, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '298ec00caed4ffd4', 'authors': ['Evgeny Markhasin'], 'affiliations': ['Lobachevsky State University of Nizhny Novgorod'], 'pdf_title_img': 'assets/pdf/title_img/2506.13172.jpg', 'data': {'categories': ['#science', '#multimodal', '#training', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Структурированные промпты улучшают анализ текста, но требуют индивидуального подхода', 'desc': 'Исследователи разработали структурированные рабочие процессы для улучшения иерархического рассуждения в больших языковых моделях (LLM) при анализе научных рукописей. Они оценили эффективность этих процессов на двух моделях (Gemini Pro 2.5 Pro и ChatGPT Plus o3) для задач выявления необоснованных утверждений и неоднозначных местоимений. Результаты показали, что эффективность структурированных промптов зависит от конкретной модели, типа задачи и контекста. Исследование подчеркивает необходимость тщательного тестирования для каждой конкретной модели и задачи.'}, 'en': {'title': 'Enhancing Scholarly Analysis with Structured Prompts in LLMs', 'desc': 'This paper explores how structured workflow prompts can enhance hierarchical reasoning in Large Language Models (LLMs) when analyzing scholarly manuscripts. The authors designed prompts to help models identify unsubstantiated claims and ambiguous pronoun references, which are crucial for maintaining informational integrity and linguistic clarity. They evaluated two advanced models, Gemini Pro 2.5 Pro and ChatGPT Plus o3, and found significant differences in their performance based on the task and context. The results indicate that while structured prompting can improve analysis, its effectiveness varies greatly depending on the model and the specific analytical task.'}, 'zh': {'title': '结构化提示提升学术分析中的层次推理', 'desc': '本研究探讨了结构化工作流程提示在大型语言模型（LLMs）中促进层次推理的效果，特别是在学术手稿分析中。我们设计了一系列概念验证的提示，旨在引导模型进行高水平的语义和语言分析，重点关注识别未证实的主张和模糊的代词引用。通过对两种前沿模型的系统评估，我们发现模型在不同任务和上下文中的表现差异显著，尤其是在处理句法角色时。研究结果表明，结构化提示是一种有效的复杂文本分析方法，但其效果受模型、任务类型和上下文的相互影响。'}}}, {'id': 'https://huggingface.co/papers/2506.12148', 'title': "Hatevolution: What Static Benchmarks Don't Tell Us", 'url': 'https://huggingface.co/papers/2506.12148', 'abstract': 'Empirical evaluation reveals temporal misalignment in the robustness of language models on evolving hate speech benchmarks, highlighting the need for time-sensitive linguistic assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain.', 'score': 0, 'issue_id': 4333, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'f0e52e354d5e052b', 'authors': ['Chiara Di Bonaventura', 'Barbara McGillivray', 'Yulan He', 'Albert Meroño-Peñuela'], 'affiliations': ['Imperial College London', 'Kings College London'], 'pdf_title_img': 'assets/pdf/title_img/2506.12148.jpg', 'data': {'categories': ['#ethics', '#dataset', '#security', '#benchmark'], 'emoji': '⏳', 'ru': {'title': 'Время имеет значение: необходимость динамической оценки языковых моделей', 'desc': 'Исследование показывает временное несоответствие в устойчивости языковых моделей при оценке на эволюционирующих тестах по выявлению языка вражды. Авторы провели эмпирическую оценку 20 языковых моделей в двух экспериментах с изменяющимися во времени данными. Результаты выявили расхождение между статичными и учитывающими временной фактор методами оценки. Исследование подчеркивает необходимость разработки лингвистических тестов, чувствительных к временным изменениям, для корректной оценки языковых моделей в области выявления языка вражды.'}, 'en': {'title': 'Evolving Language, Evolving Benchmarks: Time-Sensitive Evaluations for Hate Speech Models', 'desc': "This paper investigates how language models perform on hate speech benchmarks that change over time. It highlights that as language evolves, especially in sensitive areas like hate speech, the effectiveness of static evaluations may not accurately reflect a model's robustness. The authors tested 20 different language models against two evolving hate speech datasets and found significant misalignment in their performance. The study emphasizes the importance of developing time-sensitive benchmarks to ensure that language models are evaluated accurately and safely in the context of evolving language."}, 'zh': {'title': '仇恨言论评估需与时俱进', 'desc': '本论文探讨了语言模型在不断变化的仇恨言论基准上的鲁棒性，发现了时间上的不一致性。随着社会动态和文化变迁，仇恨言论的语言也在不断演变。尽管自然语言处理研究已经关注语言演变对模型训练的影响，但对模型基准测试的影响仍然研究不足。我们建议在仇恨言论领域中采用时间敏感的语言基准，以便更准确地评估语言模型的安全性。'}}}, {'id': 'https://huggingface.co/papers/2505.24864', 'title': 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.24864', 'abstract': "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", 'score': 77, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '390a294f460cedfc', 'authors': ['Mingjie Liu', 'Shizhe Diao', 'Ximing Lu', 'Jian Hu', 'Xin Dong', 'Yejin Choi', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24864.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl', '#alignment', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Расширение границ рассуждений языковых моделей с помощью длительного обучения с подкреплением', 'desc': 'Статья описывает новый метод обучения языковых моделей с подкреплением (RL) под названием ProRL. Авторы показывают, что длительное RL-обучение может раскрыть новые стратегии рассуждений, недоступные базовым моделям. ProRL включает контроль расхождения KL, сброс эталонной политики и набор разнообразных задач. Эмпирический анализ демонстрирует, что модели, обученные с помощью RL, превосходят базовые модели в различных оценках pass@k.'}, 'en': {'title': 'Unlocking New Reasoning Strategies with ProRL', 'desc': 'This paper explores the effectiveness of reinforcement learning (RL) in enhancing the reasoning capabilities of language models. The authors introduce a new training method called ProRL, which employs techniques like KL divergence control and reference policy resetting to improve model performance. Their experiments show that models trained with ProRL outperform base models in various reasoning tasks, even in cases where base models struggle. The study suggests that prolonged RL training can help discover new reasoning strategies, indicating that RL can significantly expand the reasoning abilities of language models over time.'}, 'zh': {'title': '强化学习扩展推理能力的新方法', 'desc': '最近的研究表明，基于推理的语言模型在强化学习（RL）方面取得了进展，这被认为是一种有效的方法来使模型与可验证的奖励对齐。然而，关于RL是否真正增强了模型的推理能力，还是仅仅放大了基础模型分布中已经存在的高奖励输出，仍然存在争议。本文提出了一种新的训练方法ProRL，证明了经过长时间的RL训练可以发现基础模型无法访问的新推理策略。我们的实证分析显示，经过RL训练的模型在多种评估任务中表现优于基础模型，尤其是在基础模型完全失败的情况下。'}}}, {'id': 'https://huggingface.co/papers/2505.24867', 'title': "Time Blindness: Why Video-Language Models Can't See What Humans Can?", 'url': 'https://huggingface.co/papers/2505.24867', 'abstract': 'Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.', 'score': 53, 'issue_id': 4070, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6870a65f3ad54877', 'authors': ['Ujjwal Upadhyay', 'Mukul Ranjan', 'Zhiqiang Shen', 'Mohamed Elhoseiny'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST)', 'Mohamed bin Zayed University of AI (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24867.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#survey', '#reasoning', '#multimodal', '#architecture', '#cv', '#games', '#dataset'], 'emoji': '⏳', 'ru': {'title': 'Временная слепота: раскрывая ограничения современных моделей компьютерного зрения', 'desc': 'Исследователи представили SpookyBench - новый бенчмарк для оценки способности моделей компьютерного зрения и обработки естественного языка (VLM) распознавать временные паттерны в видео. Эксперименты показали, что современные VLM не способны извлекать смысл из чисто временных последовательностей, в то время как люди справляются с этой задачей с точностью более 98%. Авторы отмечают, что эта проблема связана с чрезмерной зависимостью моделей от пространственных признаков в отдельных кадрах. SpookyBench призван стимулировать исследования в области распознавания временных паттернов и улучшить понимание видео машинными системами.'}, 'en': {'title': 'Bridging the Gap: Enhancing Temporal Understanding in Vision-Language Models', 'desc': 'This paper introduces SpookyBench, a new benchmark designed to test vision-language models (VLMs) on their ability to understand temporal patterns in videos when spatial information is not available. The study reveals that while humans can accurately identify shapes and patterns in noisy temporal sequences, current state-of-the-art VLMs fail to do so, achieving 0% accuracy. This highlights a significant limitation in VLMs, which tend to rely heavily on spatial features and struggle with temporal reasoning, especially in low spatial signal-to-noise ratio scenarios. The authors suggest that addressing this issue may require innovative model architectures or training methods that separate spatial and temporal processing, and they aim to stimulate further research in this area by releasing the SpookyBench dataset.'}, 'zh': {'title': '突破时序理解的瓶颈', 'desc': '最近，视觉语言模型（VLMs）在理解视频中的时空关系方面取得了显著进展。然而，当空间信息被遮蔽时，这些模型在捕捉纯粹的时间模式方面表现不佳。我们提出了SpookyBench，这是一个基准测试，信息仅通过噪声帧的时间序列编码，反映了从生物信号到隐蔽通信的自然现象。我们的研究表明，尽管人类在这些序列中识别形状、文本和模式的准确率超过98%，但最先进的VLMs的准确率却为0%，这突显了模型在时序理解上的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2505.24863', 'title': 'AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time', 'url': 'https://huggingface.co/papers/2505.24863', 'abstract': "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/", 'score': 53, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a30c2004fdd2d154', 'authors': ['Junyu Zhang', 'Runpei Dong', 'Han Wang', 'Xuying Ning', 'Haoran Geng', 'Peihao Li', 'Xialin He', 'Yutong Bai', 'Jitendra Malik', 'Saurabh Gupta', 'Huan Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24863.jpg', 'data': {'categories': ['#math', '#reasoning', '#training', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'AlphaOne: Универсальная модуляция рассуждений в ИИ', 'desc': 'AlphaOne (alpha1) - это универсальная система для модуляции процесса рассуждений в крупных моделях рассуждений (LRM) во время тестирования. Она вводит понятие альфа-момента, представляющего масштабированную фазу мышления с универсальным параметром альфа. Система динамически планирует переходы между медленным и быстрым мышлением, моделируя вставку токенов перехода рассуждений как стохастический процесс Бернулли. AlphaOne превосходит существующие методы монотонного масштабирования, обеспечивая гибкую модуляцию рассуждений.'}, 'en': {'title': 'AlphaOne: Revolutionizing Reasoning in Large Models', 'desc': "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."}, 'zh': {'title': '灵活调节推理进程的AlphaOne框架', 'desc': '本文提出了AlphaOne（alpha1），这是一个在测试时调节大型推理模型（LRMs）推理进程的通用框架。alpha1首先引入了alpha时刻，表示带有通用参数alpha的缩放思维阶段。在这个缩放的前alpha时刻阶段中，它通过将推理过渡标记的插入建模为伯努利随机过程，动态调度缓慢思维的过渡。在alpha时刻之后，alpha1通过思维结束标记确定性地终止缓慢思维，从而促进快速推理和高效答案生成。'}}}, {'id': 'https://huggingface.co/papers/2505.24098', 'title': 'HardTests: Synthesizing High-Quality Test Cases for LLM Coding', 'url': 'https://huggingface.co/papers/2505.24098', 'abstract': 'Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.', 'score': 32, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '1a86a8aadff22dbb', 'authors': ['Zhongmou He', 'Yee Man Choi', 'Kexun Zhang', 'Jiabao Ji', 'Junting Zhou', 'Dejia Xu', 'Ivan Bercovich', 'Aidan Zhang', 'Lei Li'], 'affiliations': ['Carnegie Mellon University', 'UC Santa Barbara', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.24098.jpg', 'data': {'categories': ['#synthetic', '#reasoning', '#training', '#data', '#open_source', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Улучшение верификации кода с помощью синтетических тестов', 'desc': 'Статья представляет HARDTESTGEN - пайплайн для синтеза высококачественных тестов с использованием больших языковых моделей (LLM). На его основе создан набор данных HARDTESTS, содержащий 47 тысяч задач по соревновательному программированию с синтетическими тестами высокого качества. Тесты HARDTESTGEN показывают значительно более высокую точность и полноту при оценке кода, сгенерированного LLM, по сравнению с существующими тестами. HARDTESTS также оказывается более эффективным для обучения моделей, что измеряется производительностью генерации кода.'}, 'en': {'title': 'Enhancing LLM Evaluation with Synthetic Test Cases', 'desc': 'This paper introduces HARDTESTGEN, a new method for generating high-quality test cases for evaluating large language models (LLMs) in coding tasks. The challenge with existing verifiers is that they often fail to catch subtle errors in code, which can only be identified through complex human-written edge cases. HARDTESTGEN addresses this by synthesizing a dataset called HARDTESTS, which includes 47,000 programming problems along with high-quality tests generated by LLMs. The results show that tests from HARDTESTGEN significantly improve the precision and recall of evaluating LLM-generated code, making it a valuable tool for enhancing model training and performance.'}, 'zh': {'title': '高质量测试合成，提升LLM推理能力', 'desc': '本文提出了一种名为HARDTESTGEN的高质量测试合成管道，旨在解决大型语言模型（LLM）推理中的验证器问题。由于难以为复杂编码问题获取可靠的验证器，HARDTESTGEN能够生成高质量的测试用例，帮助评估LLM生成的代码。我们创建了一个包含47,000个问题的竞争编程数据集HARDTESTS，并且与现有测试相比，HARDTESTGEN的测试在精确度和召回率上都有显著提升。该数据集和合成管道将开源，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2505.14752', 'title': 'Large Language Models for Data Synthesis', 'url': 'https://huggingface.co/papers/2505.14752', 'abstract': 'LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.', 'score': 31, 'issue_id': 4067, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '202c77d3d43de6f6', 'authors': ['Yihong Tang', 'Menglin Kong', 'Lijun Sun'], 'affiliations': ['McGill University'], 'pdf_title_img': 'assets/pdf/title_img/2505.14752.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#data'], 'emoji': '🧬', 'ru': {'title': 'LLMSynthor: Превращение языковых моделей в точные генераторы синтетических данных', 'desc': 'LLMSynthor - это фреймворк для синтеза данных, который превращает большие языковые модели (LLM) в симуляторы, учитывающие структуру данных и использующие распределительную обратную связь. Он применяет LLM как непараметрический копула-симулятор для моделирования зависимостей высокого порядка и вводит LLM Proposal Sampling для создания обоснованных предлагаемых распределений. LLMSynthor итеративно минимизирует расхождения в пространстве сводных статистик, выравнивая реальные и синтетические данные. Фреймворк показывает высокую статистическую точность и практическую полезность на гетерогенных наборах данных в конфиденциальных областях.'}, 'en': {'title': 'Transforming LLMs into Efficient Data Synthesizers', 'desc': 'LLMSynthor is a framework that enhances Large Language Models (LLMs) for creating synthetic data that accurately reflects real-world statistical distributions. It addresses the limitations of traditional data synthesis methods, which often rely on rigid assumptions and struggle with complex data types. By using distributional feedback and a novel LLM Proposal Sampling technique, LLMSynthor improves the efficiency and accuracy of data generation without the need for rejection sampling. The framework has been tested in various real-world scenarios, demonstrating its ability to produce high-quality synthetic data suitable for diverse applications.'}, 'zh': {'title': 'LLMSynthor：高效的统计数据合成新工具', 'desc': 'LLMSynthor 是一种增强大型语言模型（LLM）以实现高效和统计准确的数据合成的方法。它通过分布反馈和提议采样，将 LLM 转变为结构感知的模拟器，能够更好地捕捉真实世界分布的统计特征。该框架通过最小化摘要统计空间中的差异，逐步对齐真实数据和合成数据，同时揭示和优化潜在的生成结构。LLMSynthor 在隐私敏感领域的异构数据集上进行了评估，显示出高统计保真度和实用性，适用于经济学、社会科学和城市研究等多个领域。'}}}, {'id': 'https://huggingface.co/papers/2505.18842', 'title': "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation", 'url': 'https://huggingface.co/papers/2505.18842', 'abstract': "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.", 'score': 28, 'issue_id': 4069, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'a97f1e174b838d2d', 'authors': ['Jiwan Chung', 'Junhyeok Kim', 'Siyeol Kim', 'Jaeyoung Lee', 'Min Soo Kim', 'Youngjae Yu'], 'affiliations': ['Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18842.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#games', '#architecture', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Динамический визуальный доступ для улучшения мультимодальных рассуждений', 'desc': 'v1 - это расширение для мультимодальных больших языковых моделей (MLLM), которое позволяет избирательно обращаться к визуальным данным во время вывода. Оно вводит механизм указания и копирования, позволяющий модели динамически извлекать релевантные области изображения в процессе рассуждения. Для обучения этой возможности был создан набор данных v1g из 300 тысяч трасс мультимодальных рассуждений с аннотациями визуальной привязки. Эксперименты на трех эталонных тестах по мультимодальным математическим рассуждениям показали, что v1 стабильно улучшает производительность по сравнению с базовыми моделями.'}, 'en': {'title': 'Dynamic Visual Retrieval for Enhanced Multimodal Reasoning', 'desc': "The paper introduces v1, an enhancement to Multimodal Large Language Models (MLLMs) that allows for selective and dynamic retrieval of visual information during inference. Unlike traditional MLLMs that process visual inputs only once, v1 employs a point-and-copy mechanism to revisit relevant image regions as the model generates responses. This approach improves the model's ability to perform multimodal reasoning tasks by providing contextual access to visual data based on its ongoing hypotheses. The authors validate v1's effectiveness through experiments on multiple benchmarks, showing significant performance gains in tasks that require detailed visual references and complex reasoning steps."}, 'zh': {'title': '动态视觉访问提升多模态推理能力', 'desc': 'v1是对多模态大型语言模型（MLLMs）的轻量级扩展，能够在推理过程中实现选择性视觉区域的动态检索。与传统的MLLMs仅在内部记忆中进行推理不同，v1引入了一种简单的点对点复制机制，使模型能够在推理过程中动态获取相关的图像区域。通过构建包含30万条多模态推理轨迹的数据集v1g，模型得以训练这种能力。实验结果表明，v1在多个多模态数学推理基准上表现优异，尤其是在需要细致视觉参考和多步骤推理的任务中。'}}}, {'id': 'https://huggingface.co/papers/2505.24862', 'title': 'ViStoryBench: Comprehensive Benchmark Suite for Story Visualization', 'url': 'https://huggingface.co/papers/2505.24862', 'abstract': "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.", 'score': 21, 'issue_id': 4072, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '40afeafebffbd255', 'authors': ['Cailin Zhuang', 'Ailin Huang', 'Wei Cheng', 'Jingwei Wu', 'Yaoqi Hu', 'Jiaqi Liao', 'Zhewei Huang', 'Hongyuan Wang', 'Xinyao Liao', 'Weiwei Cai', 'Hengyuan Xu', 'Xuanyang Zhang', 'Xianfang Zeng', 'Gang Yu', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'AIGC Research', 'ShanghaiTech University', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2505.24862.jpg', 'data': {'categories': ['#dataset', '#cv', '#story_generation', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'ViStoryBench: комплексная оценка визуализации историй', 'desc': 'Статья представляет новый набор данных и систему оценки для задачи визуализации историй - ViStoryBench. Этот бенчмарк включает разнообразные типы сюжетов и художественных стилей, позволяя оценивать модели по различным аспектам генерации изображений. ViStoryBench содержит истории с одним и несколькими персонажами, сложными сюжетами и детальными мирами. Система включает набор метрик для всесторонней оценки качества визуализации, что позволяет выявлять сильные и слабые стороны различных моделей машинного обучения.'}, 'en': {'title': 'Enhancing Story Visualization with ViStoryBench', 'desc': 'This paper introduces ViStoryBench, a new evaluation benchmark designed to improve story visualization models that generate images based on narratives. It features a diverse dataset that includes various story types and artistic styles, allowing for a comprehensive assessment of model performance across different plots and visual aesthetics. The benchmark tests models on their ability to maintain character consistency and handle complex narratives with multiple protagonists. By providing a structured framework and a variety of evaluation metrics, ViStoryBench helps researchers identify strengths and weaknesses in their models, promoting targeted enhancements in story visualization.'}, 'zh': {'title': '提升故事可视化的评估基准', 'desc': '故事可视化旨在生成与给定叙述和参考图像一致的视觉图像序列。为了提升故事可视化框架在实际场景中的表现，我们引入了一个全面的评估基准，称为ViStoryBench。该基准收集了多样化的数据集，涵盖不同类型的故事和艺术风格，确保模型在不同情节和视觉美学上进行评估。ViStoryBench经过精心策划，平衡了叙事结构和视觉元素，帮助研究人员识别不同模型的优缺点，促进有针对性的改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24878', 'title': 'Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents', 'url': 'https://huggingface.co/papers/2505.24878', 'abstract': 'CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.', 'score': 15, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '9cceceaf09c77468', 'authors': ['Yaxin Luo', 'Zhaoyi Li', 'Jiacheng Liu', 'Jiacheng Cui', 'Xiaohan Zhao', 'Zhiqiang Shen'], 'affiliations': ['MetaAgentX', 'VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24878.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Open CaptchaWorld: вызов для мультимодальных ИИ-агентов', 'desc': 'Статья представляет Open CaptchaWorld - первый веб-бенчмарк для оценки возможностей мультимодальных языковых моделей (MLLM) в решении CAPTCHA. Бенчмарк включает 20 типов современных CAPTCHA, всего 225 задач, с новой метрикой - глубиной рассуждения CAPTCHA. Эксперименты показали, что люди достигают почти идеальных результатов, в то время как лучшие MLLM-агенты справляются максимум с 40% задач. Это подчеркивает важность Open CaptchaWorld для диагностики ограничений современных мультимодальных агентов и разработки более надежных систем рассуждений.'}, 'en': {'title': 'Unlocking the Future: Evaluating MLLM Agents with Open CaptchaWorld', 'desc': 'This paper introduces Open CaptchaWorld, a new benchmark designed to test the capabilities of multimodal large language model (MLLM) agents in solving CAPTCHA puzzles. It evaluates the visual reasoning and interaction skills of these agents through a variety of 225 CAPTCHA types, measuring their performance with a novel metric called CAPTCHA Reasoning Depth. Experimental results reveal that while humans achieve high success rates, MLLM agents struggle significantly, with a maximum success rate of only 40%. This underscores the need for improved multimodal reasoning systems and positions Open CaptchaWorld as a crucial tool for assessing and enhancing agent performance in complex tasks.'}, 'zh': {'title': '突破CAPTCHA瓶颈，提升多模态推理能力！', 'desc': 'CAPTCHA在实际应用中是部署网络代理的一个重要瓶颈，常常阻碍它们完成端到端的自动化任务。虽然现代多模态大语言模型（MLLM）在静态感知任务中表现出色，但它们在处理交互式、多步骤推理挑战（如CAPTCHA）方面的能力尚未得到充分测试。为了解决这个问题，我们推出了Open CaptchaWorld，这是第一个专门设计用于评估MLLM代理的视觉推理和交互能力的网络基准平台，涵盖20种现代CAPTCHA类型，共225个CAPTCHA，并引入了一种新的度量标准：CAPTCHA推理深度。实验结果表明，人类的成功率接近完美，而最先进的MLLM代理的成功率最高仅为40.0%，远低于人类的93.3%，这突显了Open CaptchaWorld作为诊断当前多模态代理局限性的重要基准。'}}}, {'id': 'https://huggingface.co/papers/2505.23941', 'title': 'Vision Language Models are Biased', 'url': 'https://huggingface.co/papers/2505.23941', 'abstract': 'Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.', 'score': 15, 'issue_id': 4069, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '1c96442d8acb3ec5', 'authors': ['An Vo', 'Khai-Nguyen Nguyen', 'Mohammad Reza Taesiri', 'Vy Tuong Dang', 'Anh Totti Nguyen', 'Daeyoung Kim'], 'affiliations': ['Auburn University', 'College of William and Mary', 'KAIST', 'University of Alberta'], 'pdf_title_img': 'assets/pdf/title_img/2505.23941.jpg', 'data': {'categories': ['#multimodal', '#cv', '#hallucinations', '#ethics', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Предвзятость визуально-языковых моделей: когда знания мешают точности', 'desc': 'Это исследование показывает, что крупные языковые модели (LLM) и визуально-языковые модели (VLM) могут быть сильно предвзяты из-за предварительных знаний, полученных из интернета. Авторы тестируют VLM на задачах подсчета и идентификации объектов, обнаруживая низкую точность (в среднем 17,05%) в различных доменах. Интересно, что добавление текстовой информации о предмете еще больше снижает точность моделей. Инструктирование моделей перепроверять свои результаты или полагаться только на детали изображения лишь незначительно улучшает точность подсчета.'}, 'en': {'title': 'Unveiling Biases in Vision Language Models', 'desc': 'This paper investigates how large language models (LLMs) influence the performance of vision language models (VLMs) on visual tasks like counting and identification. The authors find that VLMs exhibit significant biases, leading to poor accuracy when recognizing visual elements, such as miscounting stripes on logos. Even when provided with counterfactual information, such as the name of the subject, the accuracy of VLMs decreases further. The study highlights a critical failure mode in VLMs and introduces a framework for assessing these biases systematically.'}, 'zh': {'title': '视觉语言模型的偏见问题', 'desc': '大型语言模型（LLMs）从互联网中记忆了大量知识，这对下游任务有帮助，但也可能导致输出偏向错误或有偏见的答案。我们研究了流行主题的知识如何影响视觉语言模型（VLMs）在标准视觉任务（如计数和识别）上的准确性。结果显示，最先进的VLMs在计数任务中的平均准确率仅为17.05%，并且在识别图案时存在明显的偏见。我们的研究揭示了VLMs中的一种有趣的失败模式，并提供了一个自动化框架来测试VLM的偏见。'}}}, {'id': 'https://huggingface.co/papers/2505.24025', 'title': 'DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models', 'url': 'https://huggingface.co/papers/2505.24025', 'abstract': 'DINO-R1 incorporates reinforcement learning to enhance visual in-context reasoning capabilities in vision foundation models, achieving better performance than supervised fine-tuning across various visual prompting scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose DINO-R1, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces Group Relative Query Optimization (GRQO), a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios.', 'score': 13, 'issue_id': 4081, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '88443982cc458e0e', 'authors': ['Chenbin Pan', 'Wenbin He', 'Zhengzhong Tu', 'Liu Ren'], 'affiliations': ['Bosch Center for Artificial Intelligence (BCAI)', 'Bosch Research North America', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24025.jpg', 'data': {'categories': ['#rl', '#reasoning', '#cv', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'DINO-R1: Революция в визуальном рассуждении с помощью обучения с подкреплением', 'desc': 'DINO-R1 - это новый подход к обучению моделей компьютерного зрения с использованием обучения с подкреплением. Он применяет метод Group Relative Query Optimization (GRQO) для улучшения способностей визуального рассуждения в контексте. DINO-R1 превосходит базовые модели с обучением с учителем в различных сценариях визуальных подсказок. Эксперименты на наборах данных COCO, LVIS и ODinW демонстрируют сильную обобщающую способность модели.'}, 'en': {'title': 'Reinforcement Learning Boosts Visual Reasoning in DINO-R1', 'desc': 'DINO-R1 is a novel approach that uses reinforcement learning to improve the visual reasoning abilities of vision foundation models. It introduces a new training strategy called Group Relative Query Optimization (GRQO), which focuses on enhancing query-level performance by providing rewards based on alignment quality. Additionally, KL-regularization is applied to stabilize the training process and prevent issues like overfitting. The results show that DINO-R1 outperforms traditional supervised fine-tuning methods, demonstrating its effectiveness in various visual prompting tasks.'}, 'zh': {'title': 'DINO-R1：强化学习提升视觉推理能力的创新尝试', 'desc': 'DINO-R1 是一种结合强化学习的视觉基础模型，旨在增强视觉上下文推理能力。它引入了一种新的训练策略，称为群体相对查询优化（GRQO），通过计算基于查询的奖励来提高模型的表现。该模型在 COCO、LVIS 和 ODinW 数据集上的实验结果显示，DINO-R1 在多种视觉提示场景中显著优于传统的监督微调方法。通过这种方法，DINO-R1 能够实现更强的泛化能力，适应开放词汇和封闭集合的视觉提示任务。'}}}, {'id': 'https://huggingface.co/papers/2505.21437', 'title': 'CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects', 'url': 'https://huggingface.co/papers/2505.21437', 'abstract': 'Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.', 'score': 13, 'issue_id': 4073, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3b71d1aa0666ba54', 'authors': ['Huaijin Pi', 'Zhi Cen', 'Zhiyang Dou', 'Taku Komura'], 'affiliations': ['The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21437.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'Синтез реалистичных движений всего тела для манипуляции сложными объектами', 'desc': 'Статья представляет новый метод синтеза целостных движений тела при манипуляции сочлененными объектами, включая движения тела, рук и объекта. Авторы предлагают координированную оптимизацию шума диффузии с использованием трех специализированных диффузионных моделей для тела и рук. Для повышения точности взаимодействия руки с объектом применяется унифицированное представление на основе набора базисных точек (BPS). Эксперименты показывают, что метод превосходит существующие подходы по качеству движения и физической достоверности.'}, 'en': {'title': 'Coordinated Motion Synthesis for Realistic Manipulation', 'desc': 'This paper addresses the complex task of synthesizing whole-body movements for manipulating articulated objects in robotics and virtual humans. The authors introduce a coordinated diffusion noise optimization framework that enhances the synchronization between hand and body motions, which is crucial for realistic manipulation. By utilizing specialized diffusion models for different body parts and a unified representation of hand-object interactions, the method improves precision and generalization in motion generation. Experimental results show that this approach surpasses existing methods in terms of motion quality and physical realism, enabling advanced capabilities like simultaneous walking and manipulation.'}, 'zh': {'title': '全身操控的协调优化新方法', 'desc': '本文提出了一种新颖的协调扩散噪声优化框架，用于合成全身操控关节物体的运动，包括身体、手和物体的运动。该方法通过对三个专门的扩散模型进行噪声空间优化，分别针对身体、左手和右手进行训练，以提高模型的泛化能力。通过沿着人体运动链的梯度流动，协调性自然地出现，使得全身姿态能够高保真地响应手部运动目标。实验结果表明，该方法在运动质量和物理合理性方面优于现有方法，并能够实现物体姿态控制、同时行走和操控等多种能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24196', 'title': 'CLaSp: In-Context Layer Skip for Self-Speculative Decoding', 'url': 'https://huggingface.co/papers/2505.24196', 'abstract': 'Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.', 'score': 12, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '004f8eaa8c2fe087', 'authors': ['Longze Chen', 'Renke Shan', 'Huiming Wang', 'Lu Wang', 'Ziqiang Liu', 'Run Luo', 'Jiawei Wang', 'Hamid Alinejad-Rokny', 'Min Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24196.jpg', 'data': {'categories': ['#inference', '#architecture', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение LLM без компромиссов: CLaSp - новый метод спекулятивного декодирования', 'desc': 'В статье представлен метод CLaSp для ускорения декодирования больших языковых моделей (LLM) с помощью спекулятивного декодирования. CLaSp использует стратегию пропуска слоев в контексте для самоспекулятивного декодирования, не требуя дополнительных модулей или обучения. Метод применяет алгоритм динамического программирования для оптимизации процесса пропуска слоев, используя скрытые состояния последней стадии верификации. Эксперименты показывают, что CLaSp достигает ускорения в 1.3-1.7 раза на моделях серии LLaMA3 без изменения исходного распределения генерируемого текста.'}, 'en': {'title': 'Accelerating LLM Decoding with Layer-Skipping Efficiency', 'desc': 'This paper introduces CLaSp, a novel approach to speculative decoding that enhances the efficiency of Large Language Models (LLMs) without the need for additional training modules. CLaSp utilizes an in-context layer-skipping strategy, allowing it to create a compressed draft model by skipping certain layers in the verify model. The method employs a dynamic programming algorithm to optimize the layer-skipping process, adapting after each verification stage based on the hidden states. Experimental results show that CLaSp can speed up the decoding process by 1.3x to 1.7x on LLaMA3 models while maintaining the quality of the generated text.'}, 'zh': {'title': 'CLaSp：加速解码的新策略', 'desc': '本文提出了一种名为CLaSp的自我推测解码策略，旨在加速大型语言模型的解码过程。CLaSp通过跳过验证模型的中间层，构建一个压缩的草稿模型，从而避免了额外模块的训练需求。该方法利用动态规划算法优化层跳过过程，使其能够在每个验证阶段后动态调整策略。实验结果表明，CLaSp在LLaMA3系列模型上实现了1.3倍到1.7倍的加速，同时保持生成文本的原始分布不变。'}}}, {'id': 'https://huggingface.co/papers/2505.23009', 'title': 'EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge', 'url': 'https://huggingface.co/papers/2505.23009', 'abstract': "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on EmergentTTS, we introduce EmergentTTS-Eval, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation https://github.com/boson-ai/EmergentTTS-Eval-public{code} and the https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.", 'score': 12, 'issue_id': 4067, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8ed75b2649e36558', 'authors': ['Ruskin Raj Manku', 'Yuzhi Tang', 'Xingjian Shi', 'Mu Li', 'Alex Smola'], 'affiliations': ['Boson AI, Santa Clara, CA 95054'], 'pdf_title_img': 'assets/pdf/title_img/2505.23009.jpg', 'data': {'categories': ['#games', '#benchmark', '#open_source', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Автоматизированная оценка сложных аспектов синтеза речи с помощью ИИ', 'desc': "EmergentTTS-Eval - это новый комплексный бенчмарк для оценки систем Text-to-Speech (TTS). Он использует языковые модели (LLM) и аудио-языковые модели (LALM) для автоматической генерации тестовых случаев и оценки качества синтезированной речи. Бенчмарк охватывает шесть сложных сценариев, включая эмоции, паралингвистику, иностранные слова и сложное произношение. Результаты показывают, что подход 'модель-как-судья' обеспечивает надежную оценку TTS систем и высокую корреляцию с предпочтениями людей."}, 'en': {'title': 'Automating TTS Evaluation for Nuanced Speech Outputs', 'desc': 'The paper presents EmergentTTS-Eval, a new benchmark for evaluating Text-to-Speech (TTS) systems that focuses on complex and nuanced text. It automates the generation of test cases using Large Language Models (LLMs) and evaluates the outputs with a Large Audio Language Model (LALM). The benchmark includes six challenging scenarios, such as emotional expression and complex pronunciation, and generates 1,645 diverse test cases from a small set of human-written prompts. The results show that this automated approach provides a reliable assessment of TTS systems, correlating well with human evaluations.'}, 'zh': {'title': '全面评估文本到语音系统的EmergentTTS-Eval', 'desc': '本文介绍了一个全面的文本到语音（TTS）基准测试工具EmergentTTS-Eval，旨在自动生成和评估测试案例，以评估模型在处理复杂语义文本时的表现。该基准涵盖六种具有挑战性的TTS场景，包括情感、旁语言、外语、句法复杂性、复杂发音（如网址、公式）和问题。通过使用大型语言模型（LLM）迭代扩展人类编写的种子提示，生成了1645个多样化的测试案例。我们还采用了模型作为评判者的方法，利用大型音频语言模型（LALM）从多个维度评估语音输出，结果显示该方法能够有效揭示不同TTS系统之间的细微性能差异。'}}}, {'id': 'https://huggingface.co/papers/2505.24858', 'title': 'MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs', 'url': 'https://huggingface.co/papers/2505.24858', 'abstract': "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.", 'score': 10, 'issue_id': 4069, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '5cc52e721279a4e1', 'authors': ['Gabrielle Kaili-May Liu', 'Gal Yona', 'Avi Caciularu', 'Idan Szpektor', 'Tim G. J. Rudner', 'Arman Cohan'], 'affiliations': ['Google Research', 'New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24858.jpg', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#interpretability', '#hallucinations', '#dataset'], 'emoji': '🎯', 'ru': {'title': 'Научить ИИ честно выражать неуверенность', 'desc': 'Исследование посвящено проблеме надежной коммуникации неопределенности в больших языковых моделях (LLM). Авторы провели систематический анализ способности моделей выражать неуверенность, соответствующую их внутренней неопределенности. Результаты показали, что существующие LLM и методы в основном не справляются с этой задачей. Предложен новый метод калибровки MetaFaith, вдохновленный человеческой метакогнитивностью, который значительно улучшает верную калибровку моделей.'}, 'en': {'title': 'Enhancing Trust in LLMs through Better Uncertainty Communication', 'desc': 'This paper addresses the issue of how large language models (LLMs) communicate uncertainty, which is crucial for building trust in their outputs. The authors conduct a systematic study to evaluate how well LLMs express their confidence levels in a way that matches their actual uncertainty. They find that current methods for improving this communication are largely ineffective, and some can even worsen the situation. To solve this problem, they propose a new method called MetaFaith, which significantly enhances the ability of LLMs to convey uncertainty accurately, leading to better trustworthiness in their responses.'}, 'zh': {'title': '提升大型语言模型的不确定性表达信任度', 'desc': '本研究探讨了大型语言模型（LLMs）在不确定性传达方面的可靠性，指出它们在表达错误信息时常使用过于自信的语言，从而导致用户过度依赖并削弱信任。我们系统地评估了LLMs在使用不确定性语言表达其内在不确定性方面的能力，结果显示大多数模型在这方面表现不佳。现有的干预措施效果有限，标准提示方法仅带来微小改进，而基于事实的校准技术甚至可能对忠实校准产生负面影响。为了解决这一问题，我们提出了MetaFaith，这是一种基于提示的新型校准方法，能够显著提高不同模型和任务领域的忠实校准效果。'}}}, {'id': 'https://huggingface.co/papers/2505.24521', 'title': 'UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation', 'url': 'https://huggingface.co/papers/2505.24521', 'abstract': 'Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.', 'score': 10, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4ae43b7cdb482867', 'authors': ['Yang-Tian Sun', 'Xin Yu', 'Zehuan Huang', 'Yi-Hua Huang', 'Yuan-Chen Guo', 'Ziyi Yang', 'Yan-Pei Cao', 'Xiaojuan Qi'], 'affiliations': ['Beihang University', 'The University of Hong Kong', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2505.24521.jpg', 'data': {'categories': ['#video', '#diffusion', '#optimization', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Согласованная геометрическая оценка видео с помощью диффузионных моделей', 'desc': 'Статья представляет новый подход к оценке геометрических свойств в видео с использованием диффузионных моделей. Авторы предлагают метод, который позволяет использовать внутреннюю согласованность моделей генерации видео для последовательной геометрической оценки. Они вводят новый метод кондиционирования, переиспользуя позиционные кодировки, и улучшают производительность путем совместного обучения на нескольких геометрических атрибутах. Результаты показывают превосходную производительность в предсказании глобальных геометрических атрибутов в видео и могут быть применены к задачам реконструкции.'}, 'en': {'title': 'Harnessing Diffusion Models for Consistent Geometric Estimation in Videos', 'desc': "This paper explores the use of diffusion models to improve the estimation of geometric properties like depth and normals in videos. Unlike previous methods that focus on individual frames, this approach leverages the relationships between frames to enhance consistency in geometric estimation. The authors propose a novel conditioning method that reuses positional encodings and advocate for joint training on multiple geometric attributes. Their results show improved performance in predicting global geometric attributes, demonstrating the model's ability to generalize even from static video data to dynamic scenes."}, 'zh': {'title': '利用扩散模型提升视频几何估计的一致性', 'desc': '最近，利用扩散模型先验来辅助单目几何估计（如深度和法线）的方法受到了广泛关注，因为它们具有很强的泛化能力。然而，大多数现有工作集中在单个视频帧的相机坐标系内估计几何属性，忽视了扩散模型在确定帧间对应关系方面的固有能力。在本研究中，我们展示了通过适当的设计和微调，可以有效利用视频生成模型的内在一致性来进行一致的几何估计。具体而言，我们选择在全局坐标系中与视频帧共享相同对应关系的几何属性作为预测目标，并引入了一种新颖高效的条件方法，通过重用位置编码来增强性能。'}}}, {'id': 'https://huggingface.co/papers/2505.20873', 'title': 'Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.20873', 'abstract': 'The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.', 'score': 9, 'issue_id': 4072, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '12352ddbed62a761', 'authors': ['Chaeyoung Jung', 'Youngjoon Jang', 'Jongmin Choi', 'Joon Son Chung'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20873.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🍴', 'ru': {'title': 'Разделяй и властвуй: новый подход к мультимодальному анализу', 'desc': 'Статья представляет стратегию Fork-Merge Decoding (FMD) для улучшения сбалансированного мультимодального понимания в аудио-визуальных больших языковых моделях (AV-LLM). FMD разделяет обработку аудио и видео на ранних слоях декодера, а затем объединяет их для совместного анализа. Этот метод не требует дополнительного обучения или изменения архитектуры модели. Эксперименты показали, что FMD улучшает производительность моделей в задачах, связанных с аудио, видео и комбинированным аудио-визуальным анализом.'}, 'en': {'title': 'Fork-Merge Decoding: Balancing Audio-Visual Insights for Better Understanding', 'desc': 'This paper introduces the Fork-Merge Decoding (FMD) strategy to enhance multimodal understanding in audio-visual large language models (AV-LLMs). The method addresses the issue of modality bias, which occurs when models overly depend on one type of input, like audio or video. FMD operates by first analyzing audio and video inputs separately in the early decoder layers (fork phase) and then combining the insights in later layers (merge phase). This approach allows for a more balanced contribution from both modalities, improving performance on various tasks without needing extra training or changes to the model architecture.'}, 'zh': {'title': '叉合解码：提升多模态理解的有效策略', 'desc': '本文提出了一种名为叉合解码（Fork-Merge Decoding, FMD）的策略，旨在改善音视频大型语言模型（AV-LLMs）的平衡多模态理解。该方法通过在推理阶段分开处理音频和视频输入，先进行模态特定的推理，然后再合并结果，避免了模态偏差的问题。FMD不需要额外的训练或架构修改，简单有效。实验结果表明，该策略在音频、视频及音视频结合推理任务上均表现出一致的性能提升，证明了推理时干预对增强多模态理解的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.21523', 'title': 'More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models', 'url': 'https://huggingface.co/papers/2505.21523', 'abstract': "A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.", 'score': 9, 'issue_id': 4074, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '26f1abb0ea843b21', 'authors': ['Chengzhi Liu', 'Zhongxing Xu', 'Qingyue Wei', 'Juncheng Wu', 'James Zou', 'Xin Eric Wang', 'Yuyin Zhou', 'Sheng Liu'], 'affiliations': ['Stanford University', 'UC Santa Barbara', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2505.21523.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Балансируя между рассуждениями и восприятием в мультимодальных ИИ-моделях', 'desc': 'Представлена новая метрика RH-AUC и тестовый набор RH-Bench для оценки способности мультимодальных больших языковых моделей сохранять визуальную привязку при выполнении расширенных рассуждений. Анализ внимания показывает, что более длинные цепочки рассуждений приводят к уменьшению фокуса на визуальных входных данных, что способствует галлюцинациям. Исследование выявило, что более крупные модели обычно достигают лучшего баланса между рассуждениями и восприятием. Также было обнаружено, что на этот баланс больше влияют типы и домены обучающих данных, чем их общий объем.'}, 'en': {'title': 'Balancing Reasoning and Visual Grounding in Multimodal Models', 'desc': 'This paper introduces a new metric called RH-AUC to evaluate how well multimodal large language models maintain visual grounding while performing extended reasoning tasks. It highlights that as reasoning chains become longer, models often lose focus on visual inputs, leading to increased hallucination. The study also presents RH-Bench, a benchmark for assessing the trade-off between reasoning ability and hallucination across various multimodal tasks. Findings indicate that larger models and specific types of training data enhance the balance between reasoning and perception, emphasizing the need for evaluation methods that consider both aspects together.'}, 'zh': {'title': '提升推理与视觉感知的平衡', 'desc': '本文介绍了一种新的评估指标和基准，用于评估多模态大型语言模型在进行扩展推理时保持视觉基础的能力。研究发现，较大的模型和特定类型的训练数据可以改善推理与视觉感知之间的平衡。通过引入RH-AUC指标，能够量化模型在推理长度变化时的感知准确性，从而评估模型在推理过程中是否保持视觉基础。我们的分析表明，模型的大小和训练数据的类型对推理能力和幻觉之间的权衡有显著影响。'}}}, {'id': 'https://huggingface.co/papers/2505.24850', 'title': 'Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24850', 'abstract': "Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.", 'score': 8, 'issue_id': 4067, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e946031c286b5bf4', 'authors': ['Shuyao Xu', 'Cheng Peng', 'Jiangxuan Long', 'Weidi Xu', 'Wei Chu', 'Yuan Qi'], 'affiliations': ['AI3 Institute of Fudan University', 'INFLY TECH (Shanghai) Co., Ltd.', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.24850.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#dataset', '#math'], 'emoji': '🧠', 'ru': {'title': 'REDI: Эффективное обучение рассуждениям на положительных и отрицательных примерах', 'desc': 'Статья представляет новый метод дистилляции моделей машинного обучения под названием Reinforcement Distillation (REDI). REDI использует как положительные, так и отрицательные примеры рассуждений для улучшения способностей языковых моделей к логическому мышлению. Метод состоит из двух этапов: обучение на положительных примерах и дальнейшая оптимизация с использованием специальной целевой функции REDI. Эксперименты показывают превосходство REDI над базовыми методами на задачах математических рассуждений, особенно для моделей среднего размера.'}, 'en': {'title': 'Maximizing Reasoning Performance with Reinforcement Distillation', 'desc': 'This paper introduces Reinforcement Distillation (REDI), a two-stage framework designed to enhance the reasoning capabilities of smaller models by utilizing both positive and negative reasoning examples. In the first stage, the model is fine-tuned using positive reasoning traces through Supervised Fine-Tuning (SFT). The second stage refines the model further by incorporating both types of traces with a novel, reference-free loss function that improves performance over traditional methods like DPO and SimPO. Empirical results show that the Qwen-REDI-1.5B model achieves impressive scores on mathematical reasoning tasks, outperforming larger models trained on more extensive proprietary datasets.'}, 'zh': {'title': '强化蒸馏：提升推理性能的新方法', 'desc': '本论文探讨了如何有效利用正负推理轨迹来提升大型语言模型（LLM）的推理性能。我们提出了一种名为强化蒸馏（REDI）的两阶段框架，第一阶段通过监督微调（SFT）学习正推理轨迹，第二阶段则结合正负推理轨迹进一步优化模型。我们的REDI目标是一个简单的无参考损失函数，在蒸馏任务中优于传统方法如DPO和SimPO。实验结果表明，经过131k正负样本训练的Qwen-REDI-1.5B模型在数学推理任务上达到了83.1%的得分，创造了1.5B模型的新状态。'}}}, {'id': 'https://huggingface.co/papers/2505.24417', 'title': 'EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering', 'url': 'https://huggingface.co/papers/2505.24417', 'abstract': 'Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.', 'score': 8, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f28c426fafe8156a', 'authors': ['Runnan Lu', 'Yuxuan Zhang', 'Jailing Liu', 'Haifa Wang', 'Yiren Song'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'The Chinese University of Hong Kong', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24417.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#multilingual', '#cv', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'EasyText: прорыв в многоязычном рендеринге текста с помощью диффузионных моделей', 'desc': 'Статья представляет EasyText - фреймворк для рендеринга многоязычного текста, основанный на модели диффузии DiT. Авторы предлагают методы кодирования позиций символов и интерполяции позиционного кодирования для точного рендеринга текста. Для обучения модели был создан большой синтетический датасет с 1 миллионом аннотаций изображений с текстом на разных языках. Эксперименты показывают эффективность подхода в многоязычном рендеринге текста, визуальном качестве и интеграции текста с учетом макета.'}, 'en': {'title': 'EasyText: Multilingual Text Rendering Made Simple', 'desc': "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."}, 'zh': {'title': '多语言文本渲染的新突破', 'desc': '本论文介绍了一种名为EasyText的文本渲染框架，基于扩散变换器（DiT）技术。该框架通过将去噪潜变量与多语言字符令牌连接，实现了对多语言文本的精确渲染。我们提出了字符位置编码和位置编码插值技术，以实现可控和精确的文本渲染。此外，我们构建了一个包含100万条多语言图像-文本注释的大规模合成文本图像数据集，用于预训练和微调，实验结果表明我们的方法在多语言文本渲染和视觉质量方面具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2505.24871', 'title': 'MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.24871', 'abstract': "A framework for post-training multimodal large language models using reinforcement learning with verifiable rewards introduces a data mixture strategy to enhance general reasoning abilities and benchmark performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for post-training large language models (LLMs), achieving state-of-the-art performance on tasks with structured, verifiable answers. Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but is complicated by the broader, heterogeneous nature of vision-language tasks that demand nuanced visual, logical, and spatial capabilities. As such, training MLLMs using RLVR on multiple datasets could be beneficial but creates challenges with conflicting objectives from interaction among diverse datasets, highlighting the need for optimal dataset mixture strategies to improve generalization and reasoning. We introduce a systematic post-training framework for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation and benchmark implementation. Specifically, (1) We developed a multimodal RLVR framework for multi-dataset post-training by curating a dataset that contains different verifiable vision-language problems and enabling multi-domain online RL learning with different verifiable rewards; (2) We proposed a data mixture strategy that learns to predict the RL fine-tuning outcome from the data mixture distribution, and consequently optimizes the best mixture. Comprehensive experiments showcase that multi-domain RLVR training, when combined with mixture prediction strategies, can significantly boost MLLM general reasoning capacities. Our best mixture improves the post-trained model's accuracy on out-of-distribution benchmarks by an average of 5.24% compared to the same model post-trained with uniform data mixture, and by a total of 20.74% compared to the pre-finetuning baseline.", 'score': 7, 'issue_id': 4081, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a9ad14edf2493ec8', 'authors': ['Yiqing Liang', 'Jielin Qiu', 'Wenhao Ding', 'Zuxin Liu', 'James Tompkin', 'Mengdi Xu', 'Mengzhou Xia', 'Zhengzhong Tu', 'Laixi Shi', 'Jiacheng Zhu'], 'affiliations': ['Brown University', 'California Institute of Technology', 'Carnegie Mellon University', 'MIT CSAIL', 'NVIDIA Research', 'Princeton University', 'Salesforce AI Research', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24871.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#optimization', '#dataset', '#training', '#rag', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодальных ИИ через смешивание данных и обучение с подкреплением', 'desc': 'Статья представляет фреймворк для пост-обучения мультимодальных больших языковых моделей (MLLM) с использованием обучения с подкреплением с проверяемыми наградами (RLVR). Авторы разработали стратегию смешивания данных для улучшения способностей к обобщению и рассуждению у MLLM. Фреймворк включает в себя формулировку проблемы смешивания данных и реализацию бенчмарка для мультидоменного онлайн-обучения с подкреплением. Эксперименты показывают, что предложенный подход значительно повышает точность модели на тестах вне обучающей выборки.'}, 'en': {'title': 'Boosting Multimodal Models with Smart Data Mixing', 'desc': 'This paper presents a new framework for enhancing multimodal large language models (MLLMs) using Reinforcement Learning with Verifiable Rewards (RLVR). The authors address the challenges of training MLLMs on diverse datasets that require complex reasoning across visual and textual information. They propose a systematic approach that includes a data mixture strategy to optimize the training process and improve generalization. Experimental results demonstrate that their method significantly increases the accuracy of MLLMs on various benchmarks, outperforming traditional uniform data mixtures.'}, 'zh': {'title': '优化数据混合，提升多模态推理能力', 'desc': '本文提出了一种后训练多模态大语言模型的框架，利用可验证奖励的强化学习来增强推理能力和基准性能。通过引入数据混合策略，解决了多数据集训练中目标冲突的问题，从而提高了模型的泛化能力。研究表明，结合多领域的强化学习训练和混合预测策略，可以显著提升多模态大语言模型的推理能力。实验结果显示，最佳数据混合策略使得后训练模型在分布外基准上的准确率平均提高了5.24%。'}}}, {'id': 'https://huggingface.co/papers/2505.24785', 'title': 'EXP-Bench: Can AI Conduct AI Research Experiments?', 'url': 'https://huggingface.co/papers/2505.24785', 'abstract': "EXP-Bench evaluates AI agents' end-to-end research experiment capabilities through curated tasks from top AI papers, highlighting current limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.", 'score': 7, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6aafc5c29245622f', 'authors': ['Patrick Tser Jern Kon', 'Jiachen Liu', 'Xinyi Zhu', 'Qiuyi Ding', 'Jingjia Peng', 'Jiarong Xing', 'Yibo Huang', 'Yiming Qiu', 'Jayanth Srinivasa', 'Myungjin Lee', 'Mosharaf Chowdhury', 'Matei Zaharia', 'Ang Chen'], 'affiliations': ['Cisco Research', 'Rice University', 'UC Berkeley', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.24785.jpg', 'data': {'categories': ['#agents', '#benchmark', '#science', '#open_source'], 'emoji': '🧪', 'ru': {'title': 'EXP-Bench: Оценка исследовательских способностей ИИ в реальных экспериментах', 'desc': 'EXP-Bench - это новый бенчмарк для оценки способностей ИИ-агентов проводить полноценные исследовательские эксперименты в области искусственного интеллекта. Он содержит 461 задачу, извлеченную из 51 ведущей научной статьи по ИИ, и требует от агентов формулировать гипотезы, разрабатывать и выполнять экспериментальные процедуры, а также анализировать результаты. Оценка современных агентов на основе больших языковых моделей показала, что они способны частично справляться с отдельными аспектами экспериментов, но полностью выполнить эксперимент удается крайне редко. EXP-Bench выявляет текущие ограничения ИИ-агентов и предоставляет реалистичные пошаговые процедуры для улучшения их исследовательских возможностей.'}, 'en': {'title': 'EXP-Bench: Elevating AI Agents in Research Experimentation', 'desc': 'EXP-Bench is a benchmark designed to evaluate the capabilities of AI agents in conducting end-to-end research experiments. It presents AI agents with tasks derived from top AI papers, requiring them to formulate hypotheses, design experiments, implement procedures, and analyze results. The benchmark highlights the current limitations of AI agents, as they often struggle to execute complete experiments successfully. By providing a structured approach to experimental tasks, EXP-Bench aims to enhance the ability of future AI agents in performing rigorous scientific research.'}, 'zh': {'title': 'EXP-Bench：提升AI研究实验能力的基准测试', 'desc': 'EXP-Bench是一个新颖的基准测试，旨在系统地评估人工智能代理在完整研究实验中的能力。它通过从顶级AI论文中提取和结构化实验细节，创建了461个AI研究任务。尽管一些基于大型语言模型的代理在实验设计和实现的某些方面得分达到20-35%，但完整可执行实验的成功率仅为0.5%。通过识别这些瓶颈，EXP-Bench为未来的AI代理提供了改进其进行AI研究实验能力的重要工具。'}}}, {'id': 'https://huggingface.co/papers/2505.24293', 'title': 'Large Language Models are Locally Linear Mappings', 'url': 'https://huggingface.co/papers/2505.24293', 'abstract': 'We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.', 'score': 7, 'issue_id': 4066, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '42a9e20ff9742560', 'authors': ['James R. Golden'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24293.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#inference', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Линейное представление нелинейных языковых моделей', 'desc': 'Исследователи показали, что операции вывода нескольких открытых большим языковых моделей (LLM) можно отобразить в эквивалентную линейную систему для входной последовательности без изменения весов модели или предсказаний. Они расширили методы из моделей диффузии изображений, проявляющих локальную или кусочную линейность, стратегически изменив вычисление градиента для предсказания следующего токена. Этот подход был продемонстрирован на различных моделях, включая Llama 3, Gemma 3 и другие. Анализ сингулярного разложения отсоединенного якобиана показал, что эти LLM работают в экстремально низкоразмерных подпространствах, где многие из крупнейших сингулярных векторов декодируются в концепции, связанные с наиболее вероятным выходным токеном.'}, 'en': {'title': 'Unlocking LLMs: Linear Insights into Complex Predictions', 'desc': "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."}, 'zh': {'title': '揭示大型语言模型的线性本质', 'desc': '本文展示了多个开放权重的大型语言模型（LLMs）的推理操作可以映射到一个完全等价的线性系统，而无需修改模型权重或改变输出预测。我们借鉴了图像扩散模型的技术，通过战略性地改变相对于给定输入序列的梯度计算，使得模型的雅可比矩阵几乎完全重现了线性系统的前向预测。我们在多个模型上验证了这种方法，并通过对分离雅可比矩阵的奇异值分解，发现这些LLMs在极低维的子空间中操作，许多最大的奇异向量解码出与最可能输出标记相关的概念。尽管现代LLMs具有强大的表达能力和全局非线性，但可以通过几乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察，并揭示下一个标记预测过程中的可解释语义结构。'}}}, {'id': 'https://huggingface.co/papers/2505.13157', 'title': 'Role-Playing Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2505.13157', 'abstract': 'A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval', 'score': 5, 'issue_id': 4073, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': 'c1362083ff11ec99', 'authors': ['Yassine El Boudouri', 'Walter Nuninger', 'Julian Alvarez', 'Yvan Peter'], 'affiliations': ['Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.13157.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'RPEval: новый подход к оценке ролевых способностей языковых моделей', 'desc': 'В статье представлен новый бенчмарк Role-Playing Eval (RPEval) для оценки способностей больших языковых моделей (LLM) к ролевой игре. RPEval оценивает четыре ключевых аспекта: понимание эмоций, принятие решений, моральное соответствие и последовательность характера. Этот инструмент призван преодолеть ограничения существующих методов оценки, таких как ресурсоемкость человеческих оценок и потенциальная предвзятость автоматизированных подходов. Авторы описывают процесс создания RPEval и приводят результаты базовых оценок.'}, 'en': {'title': 'Assessing Role-Playing Skills in AI with RPEval', 'desc': 'The paper introduces Role-Playing Eval (RPEval), a benchmark for evaluating Large Language Models (LLMs) in their ability to role-play. It focuses on four critical aspects: emotional understanding, decision-making, moral alignment, and in-character consistency. The authors highlight the challenges of traditional evaluation methods, which can be resource-intensive and biased. RPEval aims to provide a standardized approach to assess these capabilities in LLMs, with the code and dataset made publicly available for further research.'}, 'zh': {'title': '角色扮演评估：评估大型语言模型的新基准', 'desc': '本文介绍了一种新的基准测试工具，称为角色扮演评估（Role-Playing Eval，RPEval），用于评估大型语言模型（LLMs）在角色扮演中的能力。该评估涵盖了四个关键维度：情感理解、决策能力、道德一致性和角色一致性。由于人类评估资源消耗大且自动评估可能存在偏见，RPEval旨在提供一种更有效的评估方法。文章详细描述了RPEval的构建过程，并提供了基准评估结果。'}}}, {'id': 'https://huggingface.co/papers/2505.24875', 'title': 'ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL', 'url': 'https://huggingface.co/papers/2505.24875', 'abstract': 'Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen.', 'score': 4, 'issue_id': 4078, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f15f9f746431348a', 'authors': ['Yu Zhang', 'Yunqi Li', 'Yifan Yang', 'Rui Wang', 'Yuqing Yang', 'Dai Qi', 'Jianmin Bao', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu'], 'affiliations': ['Fudan University', 'Microsoft Corporation', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24875.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#architecture', '#optimization', '#reasoning', '#rag', '#rl', '#training'], 'emoji': '🎨', 'ru': {'title': 'Рассуждающий генератор изображений: мышление перед созданием', 'desc': "ReasonGen-R1 - это двухэтапная система, объединяющая рассуждения по цепочке мыслей и обучение с подкреплением для генерации изображений. На первом этапе модель обучается явному 'мышлению' на основе текста с помощью нового набора данных с письменными обоснованиями. Второй этап использует алгоритм Group Relative Policy Optimization для улучшения качества выходных изображений. Оценки на нескольких бенчмарках показывают, что ReasonGen-R1 превосходит современные базовые модели."}, 'en': {'title': 'Empowering Image Generation with Reasoning Skills', 'desc': "This paper presents ReasonGen-R1, a novel framework that combines chain-of-thought reasoning with reinforcement learning to enhance generative vision models. The first stage involves fine-tuning an autoregressive image generator using a new dataset of text-based rationales, allowing the model to 'think' before creating images. The second stage employs Group Relative Policy Optimization (GRPO) to refine the generated images based on feedback from a pretrained vision language model, ensuring high visual quality. The results show that ReasonGen-R1 outperforms existing models on various benchmarks, demonstrating its effectiveness in integrating reasoning into image generation."}, 'zh': {'title': '推理与生成的完美结合', 'desc': '本文介绍了一种名为ReasonGen-R1的两阶段框架，旨在将链式思维推理与生成视觉模型结合起来。首先，通过在新生成的推理数据集上进行监督微调，使自回归图像生成器具备明确的文本推理能力。然后，利用群体相对策略优化（GRPO）算法对生成的图像进行优化，以提高视觉质量。实验结果表明，ReasonGen-R1在多个基准测试中表现优于强基线和之前的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.24615', 'title': 'Harnessing Large Language Models for Scientific Novelty Detection', 'url': 'https://huggingface.co/papers/2505.24615', 'abstract': 'In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.', 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'e2151a4cb797ec9b', 'authors': ['Yan Liu', 'Zonglin Yang', 'Soujanya Poria', 'Thanh-Son Nguyen', 'Erik Cambria'], 'affiliations': ['Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.24615.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#dataset', '#science'], 'emoji': '💡', 'ru': {'title': 'LLM на страже научной новизны', 'desc': 'Статья представляет новый подход к обнаружению научной новизны с использованием больших языковых моделей (LLM). Авторы создали два новых набора данных в областях маркетинга и обработки естественного языка для оценки методов определения новизны. Они предлагают обучать легковесную модель-ретривер, дистиллируя знания об идеях из LLM, чтобы эффективно сопоставлять похожие концепции. Эксперименты показывают, что предложенный метод превосходит другие подходы в задачах поиска идей и обнаружения новизны на созданных наборах данных.'}, 'en': {'title': 'Harnessing LLMs for Effective Novelty Detection in Research', 'desc': 'This paper addresses the challenge of identifying novel research ideas in academia, which is hindered by the lack of suitable benchmark datasets for novelty detection (ND). The authors propose using large language models (LLMs) to enhance ND by creating two new datasets focused on marketing and NLP. They introduce a method to extract closure sets of related papers and summarize their main ideas using LLMs, which helps in understanding idea conception. Additionally, a lightweight retriever is trained to distill idea-level knowledge from LLMs, improving the efficiency and accuracy of idea retrieval for ND tasks, with experimental results showing superior performance over existing methods.'}, 'zh': {'title': '利用大型语言模型提升科学新颖性检测', 'desc': '在科学快速发展的时代，识别新颖的研究想法变得至关重要但也充满挑战。现有的自然语言处理技术无法有效解决新颖性检测的问题，因为文本相似性与想法构思之间存在差距。本文提出利用大型语言模型（LLMs）进行科学新颖性检测，并引入了两个新的数据集，分别来自市场营销和自然语言处理领域。我们的方法通过提取论文之间的关系构建数据集，并训练轻量级检索器，从而实现高效准确的想法检索，实验结果表明该方法在新颖性检测任务中优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2505.24517', 'title': "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP", 'url': 'https://huggingface.co/papers/2505.24517', 'abstract': "Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.", 'score': 4, 'issue_id': 4071, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '7cec3d5f1feec6d4', 'authors': ['Yinqi Li', 'Jiahe Zhao', 'Hong Chang', 'Ruibing Hou', 'Shiguang Shan', 'Xilin Chen'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.24517.jpg', 'data': {'categories': ['#games', '#multimodal', '#benchmark', '#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Улучшение CLIP через инвертирование генеративной модели', 'desc': 'Данная статья посвящена улучшению модели CLIP (Contrastive Language-Image Pre-training) для более детального распознавания изображений. Авторы предлагают метод un^2CLIP, основанный на инвертировании генеративной модели unCLIP. Этот подход позволяет улучшенному энкодеру изображений сохранить выравнивание с текстовым энкодером, одновременно приобретая способность захватывать больше визуальных деталей. Эксперименты показывают значительное улучшение производительности un^2CLIP по сравнению с оригинальным CLIP на различных задачах, включая мультимодальные и задачи плотного предсказания.'}, 'en': {'title': 'Enhancing CLIP with Un^2CLIP for Better Image Detail Capture', 'desc': 'This paper addresses the limitations of the Contrastive Language-Image Pre-training (CLIP) model, particularly its inability to capture fine details in images and its performance on dense-prediction tasks. The authors propose a novel approach called un^2CLIP, which utilizes a generative model known as unCLIP to enhance the image encoder of CLIP. By inverting the unCLIP model, the authors aim to improve the detail-capturing capabilities of CLIP while maintaining its alignment with text embeddings. Experimental results demonstrate that un^2CLIP outperforms both the original CLIP and previous enhancement methods across various multimodal tasks.'}, 'zh': {'title': '提升CLIP模型，捕捉更多视觉细节', 'desc': '对比语言-图像预训练（CLIP）已成为基础模型，并广泛应用于各种视觉和多模态任务。然而，最近的研究表明，CLIP在区分图像的细微差别方面存在不足，并且在密集预测和以视觉为中心的多模态任务上表现不佳。因此，本研究旨在改进现有的CLIP模型，以尽可能捕捉图像中的视觉细节。我们发现，一种特定类型的生成模型unCLIP为实现这一目标提供了合适的框架。'}}}, {'id': 'https://huggingface.co/papers/2505.23926', 'title': 'Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2505.23926', 'abstract': 'While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.', 'score': 4, 'issue_id': 4070, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '52d11240fa18198b', 'authors': ['Xuweiyi Chen', 'Wentao Zhou', 'Aruni RoyChowdhury', 'Zezhou Cheng'], 'affiliations': ['The MathWorks, Inc.', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23926.jpg', 'data': {'categories': ['#3d', '#architecture', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'Автоматическая специализация экспертов для масштабируемого 3D восприятия', 'desc': 'Статья представляет Point-MoE - архитектуру Mixture-of-Experts для масштабируемого понимания 3D точечных облаков. В отличие от стандартных моделей, Point-MoE способна автоматически специализировать экспертов для разных доменов данных без явных меток. Эксперименты показывают, что Point-MoE превосходит сильные мультидоменные базовые модели и лучше обобщается на новые домены. Это открывает путь к масштабируемому 3D пониманию, позволяя модели самостоятельно обнаруживать структуру в разнородных 3D данных.'}, 'en': {'title': 'Unlocking 3D Perception with Point-MoE: A Scalable Solution for Diverse Data', 'desc': 'This paper addresses the challenges in understanding 3D point clouds due to the diverse sources and characteristics of the data. It introduces Point-MoE, a Mixture-of-Experts architecture that enhances cross-domain generalization in 3D perception without needing domain labels during inference. The proposed method allows the model to automatically specialize its experts based on the data it encounters, improving performance on mixed-domain datasets. Experimental results show that Point-MoE outperforms existing models and effectively generalizes to new, unseen domains, paving the way for scalable 3D understanding.'}, 'zh': {'title': '让3D理解更智能：Point-MoE架构的创新之路', 'desc': '本论文提出了一种名为Point-MoE的混合专家架构，旨在实现3D点云理解的跨域泛化。由于3D数据集规模较小且来源多样，传统的点云模型在混合域数据上表现不佳。Point-MoE通过简单的top-k路由策略，能够在没有域标签的情况下自动专门化专家，从而提高模型的性能。实验结果表明，Point-MoE在多个域的基准测试中表现优于现有方法，并且在未见域上具有更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23844', 'title': 'Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation', 'url': 'https://huggingface.co/papers/2505.23844', 'abstract': 'Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration', 'score': 4, 'issue_id': 4066, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '252af3d7c602c2c3', 'authors': ['Zhenglun Kong', 'Zheng Zhan', 'Shiyue Hou', 'Yifan Gong', 'Xin Meng', 'Pengwei Sui', 'Peiyan Dong', 'Xuan Shen', 'Zifeng Wang', 'Pu Zhao', 'Hao Tang', 'Stratis Ioannidis', 'Yanzhi Wang'], 'affiliations': ['Google', 'Harvard University', 'Northeastern University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23844.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Умное слияние языковых моделей: адаптивный подход к интеграции знаний', 'desc': 'Эта статья представляет новый подход к улучшению больших языковых моделей (LLM) путем адаптивного отбора и объединения знаний из различных LLM. Авторы предлагают фреймворк, который использует сеть адаптивного выбора для определения наиболее релевантных исходных моделей и динамическую стратегию взвешенного слияния для учета сильных сторон каждой модели. Метод позволяет снизить интерференцию знаний на 50% по сравнению с существующими подходами. Экспериментальные результаты показывают, что предложенный метод обеспечивает более стабильный и масштабируемый процесс агрегации знаний.'}, 'en': {'title': 'Adaptive Knowledge Aggregation for Enhanced LLM Performance', 'desc': 'This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation.'}, 'zh': {'title': '自适应知识聚合，构建更强大的语言模型', 'desc': '大型语言模型（LLMs）在性能上表现出色，但通过传统的微调方法持续改进仍然具有挑战性，尤其是在整合其他专业LLMs的能力时。现有的方法如集成和权重合并需要大量内存，并且难以适应变化的数据环境。我们提出了一种框架，能够自适应地选择和聚合来自不同LLMs的知识，以构建一个更强大的单一模型，避免了集成方法的高内存开销和权重合并的灵活性不足。实验结果表明，我们的方法能够实现更稳定和可扩展的知识聚合过程，同时将知识干扰减少了50%。'}}}, {'id': 'https://huggingface.co/papers/2505.21864', 'title': 'DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation', 'url': 'https://huggingface.co/papers/2505.21864', 'abstract': "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.", 'score': 4, 'issue_id': 4072, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '5d1867db4168cab6', 'authors': ['Mengda Xu', 'Han Zhang', 'Yifan Hou', 'Zhenjia Xu', 'Linxi Fan', 'Manuela Veloso', 'Shuran Song'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'J.P. Morgan AI Research', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21864.jpg', 'data': {'categories': ['#robotics', '#transfer_learning', '#agents'], 'emoji': '🦾', 'ru': {'title': 'Перенос навыков ловкой манипуляции от человека к роботу', 'desc': 'DexUMI - это фреймворк для сбора данных и обучения политик, который использует человеческую руку в качестве естественного интерфейса для передачи навыков ловкой манипуляции роботизированным рукам. Фреймворк включает аппаратную адаптацию в виде носимого экзоскелета руки и программную адаптацию для замены изображения человеческой руки на робототехническую в видеоданных. DexUMI позволяет минимизировать разрыв между воплощением человеческой и роботизированной руки. В экспериментах на двух различных платформах дексетрозных роботизированных рук была достигнута средняя успешность выполнения задач 86%.'}, 'en': {'title': 'Bridging Human and Robot Dexterity with DexUMI', 'desc': 'The DexUMI framework is designed to enhance the transfer of dexterous manipulation skills from human hands to robotic hands. It combines a wearable hand exoskeleton for direct haptic feedback and a high-fidelity robot hand inpainting technique to create realistic training data. This approach minimizes the embodiment gap by adapting human movements to be compatible with robot hand kinematics. Through extensive real-world testing, DexUMI achieves an impressive average task success rate of 86%, showcasing its effectiveness in robotic skill transfer.'}, 'zh': {'title': 'DexUMI：将人类灵巧技能转移到机器人手的创新框架', 'desc': 'DexUMI框架利用可穿戴手部外骨骼和高保真机器人手部重绘技术，将人类的灵巧操作技能转移到机器人手上，从而实现高任务成功率。该框架通过硬件和软件的适配，缩小了人类手与各种机器人手之间的体现差距。硬件适配使用可穿戴手部外骨骼，允许在数据收集过程中直接获得触觉反馈，并将人类动作适配为可行的机器人手动作。软件适配则通过在视频数据中用高保真机器人手重绘替换人类手，弥补了视觉差距。'}}}, {'id': 'https://huggingface.co/papers/2505.24189', 'title': 'Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows', 'url': 'https://huggingface.co/papers/2505.24189', 'abstract': 'Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.', 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '6fcc18713ed36918', 'authors': ['Orlando Marquez Ayala', 'Patrice Bechard', 'Emily Chen', 'Maggie Baird', 'Jingfei Chen'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2505.24189.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#small_models'], 'emoji': '🤖', 'ru': {'title': 'Малые языковые модели все еще имеют преимущество в специализированных задачах', 'desc': 'Исследование сравнивает эффективность малых языковых моделей (SLM) и больших языковых моделей (LLM) для задач, требующих структурированного вывода. Авторы обнаружили, что для специфических задач, таких как генерация низкокодовых рабочих процессов в формате JSON, дообучение SLM дает преимущество в качестве на 10% по сравнению с промптингом LLM. Несмотря на снижение стоимости токенов для LLM, SLM все еще могут быть предпочтительнее из-за более быстрого вывода и меньших затрат. Проведен систематический анализ ошибок для выявления ограничений моделей.'}, 'en': {'title': 'Fine-Tuning SLMs: The Key to Quality in Domain-Specific Tasks', 'desc': 'This paper investigates the effectiveness of Small Language Models (SLMs) compared to Large Language Models (LLMs) like GPT-4o for specific tasks that require structured outputs. It highlights that while LLMs can perform well with appropriate prompts, fine-tuning SLMs can lead to a significant quality improvement, averaging a 10% increase in performance for generating low-code workflows in JSON format. The authors conduct a thorough error analysis to identify the limitations of both model types. Ultimately, the findings suggest that SLMs retain a quality advantage for certain domain-specific applications despite the reduced costs of using LLMs.'}, 'zh': {'title': '小型语言模型在特定任务中的质量优势', 'desc': '大型语言模型（LLMs）如GPT-4o能够通过合适的提示处理多种复杂任务。随着每个token的成本降低，针对实际应用微调小型语言模型（SLMs）的优势可能不再明显。本文提供证据表明，对于需要结构化输出的特定领域任务，SLMs仍然具有质量优势。我们比较了微调SLM与使用提示的LLM在生成JSON格式低代码工作流任务上的表现，发现微调平均提高了10%的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.20047', 'title': 'Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.20047', 'abstract': "This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.", 'score': 3, 'issue_id': 4075, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '86bdf56529c01563', 'authors': ['Debargha Ganguly', 'Vikash Singh', 'Sreehari Sankar', 'Biyao Zhang', 'Xuecen Zhang', 'Srinivasan Iyengar', 'Xiaotian Han', 'Amit Sharma', 'Shivkumar Kalyanaraman', 'Vipin Chaudhary'], 'affiliations': ['Case Western Reserve University', 'Microsoft Corporation', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.20047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#interpretability', '#data', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности формальных спецификаций, генерируемых нейросетями', 'desc': 'Исследование посвящено квантификации неопределенности в больших языковых моделях (LLM) при генерации формальных спецификаций. Авторы представляют фреймворк на основе вероятностных контекстно-свободных грамматик (PCFG) для улучшения обнаружения ошибок и выборочной верификации. Оценка пяти современных LLM показала значительное влияние автоформализации на основе SMT на точность в зависимости от домена. Предложенный подход позволяет существенно снизить количество ошибок при минимальном отказе от верификации, делая формализацию на основе LLM более надежной.'}, 'en': {'title': 'Bridging Uncertainty and Formal Verification in LLMs', 'desc': 'This paper investigates how to measure and manage uncertainty in large language models (LLMs) when they generate formal specifications. It highlights the challenge that LLMs are probabilistic, while formal verification requires certainty. The authors propose a new framework using probabilistic context-free grammar (PCFG) to better understand and categorize the uncertainty in LLM outputs. Their findings show that different tasks exhibit unique uncertainty patterns, and by combining these signals, they can significantly improve the accuracy of formal verification processes.'}, 'zh': {'title': '提升LLM生成规范的可靠性', 'desc': '本研究探讨了在大型语言模型中进行不确定性量化，以生成正式规范。我们引入了一种概率上下文无关文法（PCFG）框架，以提高错误检测和选择性验证的能力。研究表明，LLM生成的正式文档在不同任务中的不确定性信号是依赖于任务的，且现有的不确定性量化技术未能有效识别这些错误。通过轻量级融合这些信号，我们显著减少了错误率，使LLM驱动的形式化过程变得更加可靠。'}}}, {'id': 'https://huggingface.co/papers/2505.24581', 'title': 'GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training', 'url': 'https://huggingface.co/papers/2505.24581', 'abstract': 'Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '380d404889c022d3', 'authors': ['Omer Nacar', 'Anis Koubaa', 'Serry Sibaee', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Alfaisal University, Riyadh, Saudi Arabia', 'Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2505.24581.jpg', 'data': {'categories': ['#low_resource', '#benchmark', '#multilingual', '#science', '#training', '#transfer_learning', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Прорыв в семантическом анализе арабского текста', 'desc': 'Статья представляет модели GATE (General Arabic Text Embedding) для семантического анализа арабского текста. Эти модели достигают наилучших результатов в задаче определения семантической близости текстов (STS) в рамках бенчмарка MTEB. GATE использует технику Matryoshka Representation Learning и гибридный подход к обучению с арабскими триплетами для задачи логического вывода на естественном языке. Модели GATE превосходят более крупные модели, включая OpenAI, на 20-25% в тестах STS, эффективно улавливая семантические нюансы арабского языка.'}, 'en': {'title': 'Unlocking Arabic Semantics with GATE Models', 'desc': 'This paper addresses the challenge of semantic textual similarity (STS) in the Arabic language, which has been under-researched due to limited datasets and models. It presents the General Arabic Text Embedding (GATE) models, which utilize advanced techniques like Matryoshka Representation Learning and a hybrid loss training approach. GATE demonstrates significant improvements in STS tasks, outperforming larger models by 20-25% on benchmarks. This advancement is crucial for better understanding and processing the unique semantic characteristics of Arabic text.'}, 'zh': {'title': '提升阿拉伯语语义理解的GATE模型', 'desc': '语义文本相似性（STS）是自然语言处理（NLP）中的一个重要任务，能够支持检索、聚类和理解文本之间的语义关系。然而，由于缺乏高质量的数据集和预训练模型，阿拉伯语领域的研究仍然有限。这种资源的匮乏限制了阿拉伯文本语义相似性的准确评估和进展。本文介绍了一种通用阿拉伯文本嵌入（GATE）模型，在MTEB基准测试中实现了语义文本相似性任务的最先进性能，显著提升了阿拉伯语的语义理解能力。'}}}, {'id': 'https://huggingface.co/papers/2505.23832', 'title': 'LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation', 'url': 'https://huggingface.co/papers/2505.23832', 'abstract': 'Legal Case Retrieval (LCR), which retrieves relevant cases from a query case, is a fundamental task for legal professionals in research and decision-making. However, existing studies on LCR face two major limitations. First, they are evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and use a narrow range of criminal query types, which cannot sufficiently reflect the complexity of real-world legal retrieval scenarios. Second, their reliance on embedding-based or lexical matching methods often results in limited representations and legally irrelevant matches. To address these issues, we present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering 411 diverse crime types in queries over 1.2M legal cases; and (2) LegalSearchLM, a retrieval model that performs legal element reasoning over the query case and directly generates content grounded in the target cases through constrained decoding. Experimental results show that LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It also demonstrates strong generalization to out-of-domain cases, outperforming naive generative models trained on in-domain data by 15%.', 'score': 2, 'issue_id': 4073, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '8778ede47e6e60db', 'authors': ['Chaeeun Kim', 'Jinu Lee', 'Wonseok Hwang'], 'affiliations': ['LBOX', 'University of Illinois Urbana-Champaign', 'University of Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2505.23832.jpg', 'data': {'categories': ['#benchmark', '#science', '#multimodal', '#reasoning', '#transfer_learning', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LegalSearchLM: Умный поиск в море юридических дел', 'desc': 'Статья представляет новый подход к поиску релевантных юридических дел - LegalSearchLM. Эта модель выполняет рассуждения над элементами запроса и генерирует ответ на основе целевых дел с помощью ограниченного декодирования. Авторы также создали первый крупномасштабный корейский бенчмарк для оценки поиска юридических дел - LEGAR BENCH. LegalSearchLM превосходит базовые модели на 6-20% на LEGAR BENCH и демонстрирует сильную обобщающую способность на новых типах дел.'}, 'en': {'title': 'Revolutionizing Legal Case Retrieval with LEGAR BENCH and LegalSearchLM', 'desc': 'This paper addresses the challenges in Legal Case Retrieval (LCR) by introducing a new benchmark and a novel retrieval model. The authors present LEGAR BENCH, a comprehensive dataset that includes over 1.2 million legal cases and 411 different crime types, which enhances the evaluation of LCR systems. They also propose LegalSearchLM, a model that utilizes legal element reasoning to improve the relevance of retrieved cases by generating content based on the query case. Experimental results indicate that LegalSearchLM significantly outperforms existing methods, demonstrating better accuracy and generalization to diverse legal scenarios.'}, 'zh': {'title': '法律检索的新突破：超越传统方法', 'desc': '法律案件检索（LCR）是法律专业人员在研究和决策中获取相关案例的基本任务。现有的LCR研究存在两个主要限制：一是评估使用的检索语料库规模较小，无法反映真实法律检索场景的复杂性；二是依赖于嵌入或词汇匹配方法，导致表示能力有限和法律无关的匹配。为了解决这些问题，本文提出了LEGAR BENCH，这是第一个涵盖411种犯罪类型和120万法律案例的大规模韩国LCR基准，以及LegalSearchLM，一个能够进行法律元素推理并生成与目标案例相关内容的检索模型。实验结果表明，LegalSearchLM在LEGAR BENCH上比基线模型提高了6-20%的性能，并在跨领域案例中表现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20977', 'title': 'Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model', 'url': 'https://huggingface.co/papers/2505.20977', 'abstract': 'MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a MC\\textsuperscript{2} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.', 'score': 2, 'issue_id': 4076, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e3f911b43ad30492', 'authors': ['Yu Zhang', 'Jinlong Ma', 'Yongshuai Hou', 'Xuefeng Bai', 'Kehai Chen', 'Yang Xiang', 'Jun Yu', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20977.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#machine_translation', '#hallucinations'], 'emoji': '🔬', 'ru': {'title': 'Контроль предвзятости модальностей в мультимодальных языковых моделях', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) демонстрируют предвзятость к определенным модальностям при обработке мультимодального контекста. Авторы разработали бенчмарк MC² для оценки предпочтений модальностей в сценариях с конфликтующими свидетельствами. Анализ выявил, что направление предпочтений может быть обнаружено в скрытых представлениях MLLM. На основе этого предложен метод инженерии представлений для контроля предпочтений модальностей, который улучшает такие задачи как смягчение галлюцинаций и мультимодальный машинный перевод.'}, 'en': {'title': 'Controlling Modality Bias in Multimodal Models', 'desc': 'This paper investigates how multimodal large language models (MLLMs) show a tendency to favor one type of input (modality) over another when processing mixed information. The authors create a benchmark called MC² to evaluate this modality bias under specific conditions where evidence from different modalities conflicts. They find that all tested MLLMs exhibit clear modality preferences, which can be adjusted using a new method based on representation engineering. This method allows for the control of modality bias without needing to retrain the models, leading to better performance in tasks like reducing hallucinations and improving multimodal translations.'}, 'zh': {'title': '控制模态偏好，提升多模态任务表现', 'desc': '多模态大型语言模型（MLLMs）在处理复杂的多模态任务时表现出色，但它们在处理多模态上下文时可能存在模态偏好。我们建立了一个MC²基准，以系统评估在证据冲突场景下的模态偏好。研究发现，所有测试的18个MLLMs普遍表现出明显的模态偏见，并且这种偏好可以通过外部干预进行调整。基于此，我们提出了一种基于表示工程的探测和引导方法，可以在不进行额外微调的情况下显式控制模态偏好，从而在幻觉缓解和多模态机器翻译等下游任务中取得显著改进。'}}}, {'id': 'https://huggingface.co/papers/2505.24869', 'title': 'SiLVR: A Simple Language-based Video Reasoning Framework', 'url': 'https://huggingface.co/papers/2505.24869', 'abstract': "SiLVR, a language-based framework, enhances multimodal LLMs' video reasoning by leveraging adaptive token reduction, achieving top results on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.", 'score': 1, 'issue_id': 4082, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'd03545b1ac06e77f', 'authors': ['Ce Zhang', 'Yan-Bo Lin', 'Ziyang Wang', 'Mohit Bansal', 'Gedas Bertasius'], 'affiliations': ['Department of Computer Science, UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.24869.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#reasoning', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'SiLVR: Прорыв в видеорассуждениях для мультимодальных ИИ', 'desc': 'SiLVR - это фреймворк для улучшения видеорассуждений мультимодальных языковых моделей. Он использует двухэтапный подход: сначала преобразует видео в языковые представления, а затем применяет мощную LLM для решения сложных задач понимания видео. SiLVR использует адаптивное сокращение токенов для обработки длинного контекста. Фреймворк достиг лучших результатов на нескольких бенчмарках видеопонимания.'}, 'en': {'title': 'Enhancing Video Reasoning with SiLVR: A Language-Based Approach', 'desc': 'SiLVR is a novel framework designed to improve the video reasoning abilities of multimodal large language models (MLLMs). It breaks down complex video understanding into two main stages: first, converting raw video into language-based representations using various sensory inputs, and second, utilizing a powerful reasoning LLM to interpret these representations. To efficiently manage long-context inputs, SiLVR employs an adaptive token reduction method that optimizes how tokens are sampled. This approach has led to SiLVR achieving top performance on multiple video-language benchmarks, demonstrating the potential of strong reasoning LLMs in processing and understanding video content.'}, 'zh': {'title': 'SiLVR：提升视频推理的语言框架', 'desc': 'SiLVR是一个基于语言的视频推理框架，旨在提升多模态大语言模型（MLLMs）在视频理解方面的能力。该框架将复杂的视频理解任务分为两个阶段：首先，通过多感官输入将原始视频转换为基于语言的表示；其次，将这些语言描述输入到强大的推理大语言模型中，以解决复杂的视频语言理解任务。SiLVR采用自适应的令牌减少方案，动态确定采样令牌的时间粒度，从而有效处理长上下文的多感官输入。该框架在多个基准测试中取得了最佳结果，展示了强大的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24782', 'title': 'Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings', 'url': 'https://huggingface.co/papers/2505.24782', 'abstract': 'A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '54b58cb49740c12b', 'authors': ['Max Conti', 'Manuel Faysse', 'Gautier Viaud', 'Antoine Bosselut', 'Céline Hudelot', 'Pierre Colombo'], 'affiliations': ['CentraleSupélec, Paris-Saclay', 'EPFL Lausanne', 'Equall.ai', 'Illuin Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.24782.jpg', 'data': {'categories': ['#open_source', '#optimization', '#data', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекстное обогащение для умного поиска документов', 'desc': 'Авторы представили новый бенчмарк ConTEB для оценки способности моделей извлечения документов использовать контекст всего документа. Они также предложили метод контрастивного дообучения InSeNT, который улучшает контекстуальные представления, сохраняя вычислительную эффективность. Результаты показывают, что их подход значительно повышает качество извлечения на ConTEB без ухудшения базовой производительности модели. Метод также делает вложения фрагментов более устойчивыми к неоптимальным стратегиям разбиения и увеличению размера корпуса для поиска.'}, 'en': {'title': 'Enhancing Document Retrieval with Contextual Awareness', 'desc': 'This paper addresses the limitations of current document retrieval methods that often treat text passages independently, missing out on important context from the entire document. It introduces ConTEB, a benchmark that evaluates how well retrieval models utilize document-wide context. The authors propose InSeNT, a new contrastive training method that enhances the learning of contextual representations while maintaining efficiency. Their findings demonstrate that this approach significantly improves retrieval quality, making the models more robust to various chunking strategies and larger datasets.'}, 'zh': {'title': '提升文档检索质量的上下文感知方法', 'desc': '本文提出了一种上下文感知的基准测试和对比训练方法，以提高文档检索的质量。现有的文档检索嵌入方法通常独立编码文档中的段落，忽视了文档整体的上下文信息。我们引入了ConTEB（上下文感知文本嵌入基准），用于评估检索模型利用文档上下文的能力。通过提出InSeNT（序列内负训练），我们的方法在保持计算效率的同时，显著提升了检索质量。'}}}, {'id': 'https://huggingface.co/papers/2505.24119', 'title': 'The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It', 'url': 'https://huggingface.co/papers/2505.24119', 'abstract': 'This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.', 'score': 1, 'issue_id': 4076, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'f5b6ed3de0b4cc5b', 'authors': ['Zheng-Xin Yong', 'Beyza Ermis', 'Marzieh Fadaee', 'Stephen H. Bach', 'Julia Kreutzer'], 'affiliations': ['Brown University', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2505.24119.jpg', 'data': {'categories': ['#low_resource', '#survey', '#ethics', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языкового барьера в исследованиях безопасности ИИ', 'desc': 'Данная статья представляет всесторонний анализ лингвистического разнообразия в исследованиях безопасности больших языковых моделей (LLM), подчеркивая их англоцентричность. Авторы провели систематический обзор почти 300 публикаций за 2020-2024 годы на крупных конференциях и семинарах по обработке естественного языка. Исследование выявило значительный и растущий языковой разрыв в исследованиях безопасности LLM, при этом даже высокоресурсные неанглийские языки получают минимальное внимание. На основе своего анализа авторы предлагают рекомендации и конкретные направления для будущих исследований в области многоязычной безопасности ИИ.'}, 'en': {'title': 'Bridging the Language Gap in LLM Safety Research', 'desc': 'This paper analyzes the focus on English in large language model (LLM) safety research, revealing a significant language gap. It reviews nearly 300 publications from major NLP conferences between 2020 and 2024, showing that non-English languages, even those with ample resources, are largely overlooked. The authors highlight the lack of standalone studies on non-English languages and poor documentation practices in English safety research. They propose recommendations and future directions to enhance multilingual safety evaluation, data generation, and cross-lingual safety generalization, aiming for more inclusive AI safety practices.'}, 'zh': {'title': '推动多语言LLM安全研究的未来方向', 'desc': '这篇论文对大型语言模型（LLM）安全研究的语言多样性进行了全面分析，强调了该领域以英语为中心的特点。通过对2020年至2024年间近300篇来自主要自然语言处理（NLP）会议和研讨会的出版物进行系统审查，我们发现LLM安全研究中存在显著且日益扩大的语言差距，甚至高资源的非英语语言也受到的关注很少。我们进一步观察到，非英语语言很少作为独立语言进行研究，而英语安全研究的文献记录实践也很差。为了激励未来的多语言安全研究，我们根据调查结果提出了几项建议，并提出了关于安全评估、训练数据生成和跨语言安全泛化的三个具体未来方向。'}}}, {'id': 'https://huggingface.co/papers/2505.23923', 'title': 'ChARM: Character-based Act-adaptive Reward Modeling for Advanced\n  Role-Playing Language Agents', 'url': 'https://huggingface.co/papers/2505.23923', 'abstract': 'ChARM, a character-focused adaptive reward model, improves preference learning for role-playing language agents by using an act-adaptive margin and self-evolution with unlabeled data, achieving superior results on dedicated benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.', 'score': 1, 'issue_id': 4079, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '761912e5ab3c056e', 'authors': ['Feiteng Fang', 'Ting-En Lin', 'Yuchuan Wu', 'Xiong Liu', 'Xiang Huang', 'Dingwei Chen', 'Jing Ye', 'Haonan Zhang', 'Liang Zhu', 'Hamid Alinejad-Rokny', 'Min Yang', 'Fei Huang', 'Yongbin Li'], 'affiliations': ['Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China', 'Tongji University', 'Tongyi Laboratory', 'University of New South Wales'], 'pdf_title_img': 'assets/pdf/title_img/2505.23923.jpg', 'data': {'categories': ['#optimization', '#open_source', '#benchmark', '#rlhf', '#dataset', '#games'], 'emoji': '🎭', 'ru': {'title': 'ChARM: Революция в обучении ролевых языковых агентов', 'desc': 'ChARM - это модель вознаграждения для ролевых языковых агентов, использующая адаптивную границу для действий и самоэволюцию с неразмеченными данными. Она улучшает обучение предпочтениям для более реалистичного взаимодействия человека с компьютером. Авторы также представили RoleplayPref - крупномасштабный набор данных предпочтений для ролевых агентов, и RoleplayEval - специализированный оценочный бенчмарк. Эксперименты показали 13% улучшение по сравнению с традиционной моделью Брэдли-Терри в ранжировании предпочтений.'}, 'en': {'title': 'ChARM: Elevating Role-Playing Agents with Adaptive Rewards', 'desc': 'ChARM is a novel reward model designed to enhance preference learning for role-playing language agents (RPLAs). It introduces an act-adaptive margin that improves the efficiency and adaptability of learning from user preferences. Additionally, ChARM employs a self-evolution mechanism that utilizes large amounts of unlabeled data to broaden the training scope. The model demonstrates significant performance improvements over traditional methods, achieving state-of-the-art results on specialized evaluation benchmarks.'}, 'zh': {'title': '角色扮演语言代理的新突破：ChARM模型', 'desc': 'ChARM是一种以角色为中心的自适应奖励模型，旨在改善角色扮演语言代理的偏好学习。它通过引入行为自适应边际和利用未标记数据的自我进化机制，解决了传统奖励模型在可扩展性和适应主观对话偏好方面的挑战。我们还推出了RoleplayPref，这是第一个专门针对角色扮演语言代理的大规模偏好数据集，包含1108个角色和16888个双语对话。实验结果表明，ChARM在偏好排名上比传统的Bradley-Terry模型提高了13%。'}}}, {'id': 'https://huggingface.co/papers/2505.21749', 'title': 'Revisiting Bi-Linear State Transitions in Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2505.21749', 'abstract': 'Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.  \t\t\t\t\tAI-generated summary \t\t\t\t The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.', 'score': 1, 'issue_id': 4077, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cb7a7df613a84e6e', 'authors': ['M. Reza Ebrahimi', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.21749.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Билинейность - ключ к эффективному отслеживанию состояний в РНС', 'desc': 'В статье рассматриваются билинейные операции в рекуррентных нейронных сетях как естественное предрасположение для задач отслеживания состояний. Авторы демонстрируют, что скрытые слои активно участвуют в вычислениях, а не просто хранят информацию. Теоретически и эмпирически показано, что билинейные обновления состояний образуют естественную иерархию, соответствующую задачам отслеживания состояний возрастающей сложности. Линейные рекуррентные сети, такие как Mamba, находятся в центре этой иерархии с наименьшей сложностью.'}, 'en': {'title': 'Bilinear Operations: Enhancing RNNs for State Tracking', 'desc': 'This paper explores the role of bilinear operations in recurrent neural networks (RNNs) for state tracking tasks. It argues that hidden units should be seen as active contributors to computations rather than just memory stores. The authors demonstrate that bilinear interactions between hidden units and input embeddings provide a beneficial inductive bias for evolving hidden states. Additionally, they establish a hierarchy of state tracking tasks, with linear RNNs like Mamba representing the simplest form of this structure.'}, 'zh': {'title': '双线性操作：状态跟踪的自然偏置', 'desc': '在递归神经网络中，双线性操作被证明是状态跟踪任务的自然偏置，形成了一种层次结构，其中线性递归网络是最简单的形式。隐藏单元的角色通常被视为建模记忆，研究主要集中在通过门控机制增强信息保留。本文重新审视双线性操作，展示它们在状态跟踪任务中如何自然地表示隐藏状态的演变。我们还表明，双线性状态更新形成了一个自然的层次结构，适用于复杂性逐渐增加的状态跟踪任务。'}}}, {'id': 'https://huggingface.co/papers/2505.23856', 'title': 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across\n  Modalities', 'url': 'https://huggingface.co/papers/2505.23856', 'abstract': 'OMNIGUARD detects harmful prompts across languages and modalities by identifying aligned internal representations in large language models, achieving high accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\% over the strongest baseline in a multilingual setting, by 20.44\\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient (approx 120 times faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.', 'score': 0, 'issue_id': 4082, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '365a6ed14a1515e6', 'authors': ['Sahil Verma', 'Keegan Hines', 'Jeff Bilmes', 'Charlotte Siska', 'Luke Zettlemoyer', 'Hila Gonen', 'Chandan Singh'], 'affiliations': ['Microsoft', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.23856.jpg', 'data': {'categories': ['#security', '#low_resource', '#dataset', '#multimodal', '#multilingual'], 'emoji': '🛡️', 'ru': {'title': 'OMNIGUARD: Универсальная защита языковых моделей от вредоносных запросов', 'desc': 'OMNIGUARD - это подход к обнаружению вредоносных запросов к языковым моделям на разных языках и в разных модальностях. Он использует внутренние представления модели, согласованные между языками и модальностями, для создания универсального классификатора. OMNIGUARD значительно повышает точность классификации вредоносных запросов по сравнению с существующими методами. Метод также отличается высокой эффективностью за счет повторного использования вычисленных эмбеддингов.'}, 'en': {'title': 'OMNIGUARD: Safeguarding LLMs Across Languages and Modalities', 'desc': 'OMNIGUARD is a novel approach designed to detect harmful prompts in large language models (LLMs) across different languages and modalities, such as text, images, and audio. It works by identifying aligned internal representations within the LLMs, which allows it to create a classifier that is effective regardless of the language or type of input. This method significantly improves the accuracy of harmful prompt detection, outperforming existing techniques by notable margins in multilingual and multimodal contexts. Additionally, OMNIGUARD is highly efficient, operating approximately 120 times faster than previous methods, making it a practical solution for real-time applications.'}, 'zh': {'title': 'OMNIGUARD：跨语言和模态的有害提示检测新方法', 'desc': 'OMNIGUARD是一种检测有害提示的技术，能够跨语言和模态识别大型语言模型中的对齐内部表示。该方法通过识别不同语言或模态中对齐的内部表示，构建一种不依赖语言或模态的分类器，从而提高有害提示的检测准确性。与现有方法相比，OMNIGUARD在多语言环境下的分类准确性提高了11.57%，在基于图像的提示中提高了20.44%，并在基于音频的提示中设定了新的最优性能。该方法还利用生成过程中计算的嵌入，使得检测过程非常高效，速度比下一个最快的基线快约120倍。'}}}, {'id': 'https://huggingface.co/papers/2506.08007', 'title': 'Reinforcement Pre-Training', 'url': 'https://huggingface.co/papers/2506.08007', 'abstract': 'Reinforcement Pre-Training (RPT) enhances language model pre-training by framing next-token prediction as a reinforcement learning task, improving accuracy and providing a strong foundation for further fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.', 'score': 142, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a1ef74d27d1d9d50', 'authors': ['Qingxiu Dong', 'Li Dong', 'Yao Tang', 'Tianzhu Ye', 'Yutao Sun', 'Zhifang Sui', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08007.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты для языковых моделей', 'desc': 'Эта статья представляет новый подход к предварительному обучению больших языковых моделей, называемый Reinforcement Pre-Training (RPT). RPT переосмысливает задачу предсказания следующего токена как задачу обучения с подкреплением, где модель получает вознаграждения за правильные предсказания. Этот метод позволяет эффективно использовать огромные объемы текстовых данных для обучения с подкреплением общего назначения. Результаты показывают, что RPT значительно улучшает точность языкового моделирования и создает прочную основу для дальнейшей тонкой настройки.'}, 'en': {'title': 'Reinforcement Learning Boosts Language Model Training', 'desc': 'Reinforcement Pre-Training (RPT) is a novel approach that enhances the pre-training of language models by treating next-token prediction as a reinforcement learning (RL) task. This method allows the model to receive rewards for accurately predicting the next token based on the context, which improves its reasoning capabilities. RPT utilizes large amounts of unannotated text data, making it scalable and effective for general-purpose RL applications. The results demonstrate that RPT not only boosts prediction accuracy but also lays a solid groundwork for subsequent fine-tuning in reinforcement learning tasks.'}, 'zh': {'title': '强化预训练：提升语言模型的下一步预测能力', 'desc': '强化预训练（RPT）通过将下一个标记预测视为强化学习任务，增强了语言模型的预训练。这种方法通过为正确预测给定上下文的下一个标记提供可验证的奖励，从而提高了准确性。RPT利用大量文本数据进行通用强化学习，而不依赖于特定领域的标注答案。通过激励下一个标记推理的能力，RPT显著提高了语言建模的准确性，并为进一步的强化微调提供了坚实的基础。'}}}, {'id': 'https://huggingface.co/papers/2506.07044', 'title': 'Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2506.07044', 'abstract': "Lingshu, a medical-specialized MLLM, enhances performance in medical tasks through comprehensive data curation, multi-stage training, and reinforcement learning with verifiable rewards, outperforming existing open-source models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshu's medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks ...", 'score': 79, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '475506f0d0cabb39', 'authors': ['LASA Team', 'Weiwen Xu', 'Hou Pong Chan', 'Long Li', 'Mahani Aljunied', 'Ruifeng Yuan', 'Jianyu Wang', 'Chenghao Xiao', 'Guizhen Chen', 'Chaoqun Liu', 'Zhaodonghui Li', 'Yu Sun', 'Junao Shen', 'Chaojun Wang', 'Jie Tan', 'Deli Zhao', 'Tingyang Xu', 'Hao Zhang', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2506.07044.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#hallucinations', '#benchmark', '#medical', '#rl', '#reasoning', '#data', '#healthcare', '#multimodal'], 'emoji': '🩺', 'ru': {'title': 'Lingshu: мультимодальная ИИ-модель нового поколения для медицины', 'desc': 'Lingshu - это специализированная мультимодальная языковая модель для медицины, разработанная для улучшения производительности в медицинских задачах. Модель использует комплексный подход к курированию данных, включая медицинские изображения, тексты и общие данные. Lingshu проходит многоэтапное обучение для встраивания медицинских знаний и улучшения способностей решения задач. Модель также применяет обучение с подкреплением с проверяемыми наградами для усиления способностей медицинского рассуждения.'}, 'en': {'title': 'Lingshu: Revolutionizing Medical AI with Enhanced Learning and Reasoning', 'desc': 'Lingshu is a specialized Multimodal Large Language Model (MLLM) designed to improve medical task performance through enhanced data curation and training techniques. It addresses key limitations of existing medical models, such as insufficient medical knowledge coverage and a tendency to produce hallucinations. By utilizing a comprehensive dataset that includes both medical texts and images, Lingshu is trained in multiple stages to develop strong reasoning capabilities for complex medical scenarios. The model is evaluated using the MedEvalKit framework, demonstrating superior performance in tasks like multimodal question answering and medical report generation compared to other open-source models.'}, 'zh': {'title': 'Lingshu：医疗任务的智能助手', 'desc': 'Lingshu是一种专门针对医疗任务的多模态大语言模型（MLLM），通过全面的数据整理、多阶段训练和可验证奖励的强化学习来提升其性能。该模型解决了现有医疗MLLM在知识覆盖、数据整理和推理能力方面的不足。通过高效获取丰富的医疗知识数据，Lingshu能够在多种医疗任务中表现优异。最终，Lingshu在多模态问答、基于文本的问答和医疗报告生成等任务中超越了现有的开源模型。'}}}, {'id': 'https://huggingface.co/papers/2506.06444', 'title': 'Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance', 'url': 'https://huggingface.co/papers/2506.06444', 'abstract': "SAFFRON introduces multifurcation reward models to improve safety assurance in large language models by addressing the exploration-efficiency dilemma in inference scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .", 'score': 59, 'issue_id': 4209, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '86bb27c97ab5d915', 'authors': ['Ruizhong Qiu', 'Gaotang Li', 'Tianxin Wei', 'Jingrui He', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06444.jpg', 'data': {'categories': ['#open_source', '#security', '#dataset', '#alignment', '#rl', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'SAFFRON: Революция в обеспечении безопасности языковых моделей', 'desc': 'Статья представляет SAFFRON - новую парадигму масштабирования вывода для обеспечения безопасности больших языковых моделей (LLM). Авторы вводят концепцию многовариантной модели вознаграждения (MRM), которая значительно сокращает количество необходимых оценок модели вознаграждения. Предлагаемый подход включает частично контролируемую цель обучения для MRM, ограничение консервативного исследования и стратегию кэширования на основе префиксного дерева. Эксперименты подтверждают эффективность метода в повышении безопасности LLM против возникающих угроз.'}, 'en': {'title': 'SAFFRON: Enhancing LLM Safety with Multifurcation Reward Models', 'desc': 'SAFFRON presents a new approach to enhance safety in large language models (LLMs) by introducing multifurcation reward models that tackle the exploration-efficiency dilemma during inference scaling. Traditional safety methods have struggled against jailbreak attacks and have not effectively integrated with the advancements in LLM reasoning capabilities. The paper identifies that existing inference scaling techniques are inadequate for safety assurance, often performing worse than simpler methods. To address this, SAFFRON proposes a novel paradigm that reduces the number of necessary reward model evaluations, ensuring safer and more efficient LLM operations.'}, 'zh': {'title': 'SAFFRON：提升大型语言模型安全性的创新方案', 'desc': 'SAFFRON提出了一种多分叉奖励模型，以提高大型语言模型的安全性保障，解决推理扩展中的探索效率困境。现有的安全保障研究主要集中在训练阶段的对齐，以培养LLM的安全行为，但这些方法在面对各种攻击时表现出脆弱性。我们的研究首次探讨了推理扩展在安全保障中的应用，发现传统的推理扩展技术在安全上下文中表现不佳。为此，我们提出了一种新的推理扩展范式，结合多分叉奖励模型，显著减少了奖励模型评估的次数，从而提高了安全性。'}}}, {'id': 'https://huggingface.co/papers/2506.07900', 'title': 'MiniCPM4: Ultra-Efficient LLMs on End Devices', 'url': 'https://huggingface.co/papers/2506.07900', 'abstract': 'MiniCPM4, a large language model optimized for end-side devices, achieves efficiency and effectiveness through innovations in model architecture, training data, algorithms, and inference systems.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.', 'score': 53, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'edbb02209bdb677b', 'authors': ['MiniCPM Team', 'Chaojun Xiao', 'Yuxuan Li', 'Xu Han', 'Yuzhuo Bai', 'Jie Cai', 'Haotian Chen', 'Wentong Chen', 'Xin Cong', 'Ganqu Cui', 'Ning Ding', 'Shengdan Fan', 'Yewei Fang', 'Zixuan Fu', 'Wenyu Guan', 'Yitong Guan', 'Junshao Guo', 'Yufeng Han', 'Bingxiang He', 'Yuxiang Huang', 'Cunliang Kong', 'Qiuzuo Li', 'Siyuan Li', 'Wenhao Li', 'Yanghao Li', 'Yishan Li', 'Zhen Li', 'Dan Liu', 'Biyuan Lin', 'Yankai Lin', 'Xiang Long', 'Quanyu Lu', 'Yaxi Lu', 'Peiyan Luo', 'Hongya Lyu', 'Litu Ou', 'Yinxu Pan', 'Zekai Qu', 'Qundong Shi', 'Zijun Song', 'Jiayuan Su', 'Zhou Su', 'Ao Sun', 'Xianghui Sun', 'Peijun Tang', 'Fangzheng Wang', 'Feng Wang', 'Shuo Wang', 'Yudong Wang', 'Yesai Wu', 'Zhenyu Xiao', 'Jie Xie', 'Zihao Xie', 'Yukun Yan', 'Jiarui Yuan', 'Kaihuo Zhang', 'Lei Zhang', 'Linyue Zhang', 'Xueren Zhang', 'Yudi Zhang', 'Hengyu Zhao', 'Weilin Zhao', 'Weilun Zhao', 'Yuanqian Zhao', 'Zhi Zheng', 'Ge Zhou', 'Jie Zhou', 'Wei Zhou', 'Zihan Zhou', 'Zixuan Zhou', 'Zhiyuan Liu', 'Guoyang Zeng', 'Chao Jia', 'Dahai Li', 'Maosong Sun'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.07900.jpg', 'data': {'categories': ['#optimization', '#dataset', '#long_context', '#training', '#architecture', '#inference', '#open_source', '#data', '#games'], 'emoji': '🚀', 'ru': {'title': 'MiniCPM4: Мощь большой языковой модели в компактном исполнении', 'desc': 'MiniCPM4 - это эффективная большая языковая модель (LLM), оптимизированная для использования на конечных устройствах. Модель достигает высокой производительности благодаря инновациям в архитектуре, обучающих данных, алгоритмах и системах вывода. Ключевые особенности включают механизм разреженного внимания InfLLM v2, стратегию фильтрации данных UltraClean и набор данных UltraChat v2 для обучения. MiniCPM4 превосходит модели аналогичного размера по нескольким бенчмаркам и демонстрирует значительное ускорение при обработке длинных последовательностей.'}, 'en': {'title': 'MiniCPM4: Efficiency Meets Performance for On-Device AI', 'desc': 'MiniCPM4 is a large language model specifically designed for efficient use on end-side devices. It incorporates innovations in model architecture, such as a new sparse attention mechanism, and utilizes advanced training data strategies to enhance performance. The model employs unique training algorithms for effective pre-training and post-training, ensuring it can handle diverse tasks with minimal resources. Evaluation results indicate that MiniCPM4 outperforms similar-sized models, demonstrating its speed and effectiveness in real-world applications.'}, 'zh': {'title': 'MiniCPM4：终端设备的高效语言模型', 'desc': '本文介绍了MiniCPM4，这是一种专为终端设备优化的大型语言模型。通过在模型架构、训练数据、训练算法和推理系统四个关键方面的创新，MiniCPM4实现了高效性和有效性。特别是，采用了可训练的稀疏注意力机制和高效的数据过滤策略，使得模型在处理长上下文时表现出色。评估结果显示，MiniCPM4在多个基准测试中超越了同类开源模型，展现了其广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.07977', 'title': 'OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation', 'url': 'https://huggingface.co/papers/2506.07977', 'abstract': 'OneIG-Bench is a benchmark framework that comprehensively evaluates text-to-image models across prompt-image alignment, text rendering, reasoning, stylization, and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community.', 'score': 37, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'f87355dfc9390cc7', 'authors': ['Jingjing Chang', 'Yixiao Fang', 'Peng Xing', 'Shuhan Wu', 'Wei Cheng', 'Rui Wang', 'Xianfang Zeng', 'Gang Yu', 'Hai-Bao Chen'], 'affiliations': ['SJTU', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2506.07977.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#open_source', '#survey'], 'emoji': '🖼️', 'ru': {'title': 'Всесторонняя оценка моделей текст-изображение', 'desc': 'OneIG-Bench - это комплексная система оценки моделей преобразования текста в изображение. Она оценивает соответствие изображений текстовым подсказкам, точность отображения текста, способность к рассуждению, стилизацию и разнообразие. Этот бенчмарк позволяет проводить детальный анализ производительности моделей, выявляя их сильные и слабые стороны. OneIG-Bench предоставляет гибкую оценку, позволяя пользователям сосредоточиться на конкретных аспектах генерации изображений.'}, 'en': {'title': 'OneIG-Bench: A Comprehensive Evaluation for Text-to-Image Models', 'desc': 'OneIG-Bench is a new framework designed to evaluate text-to-image (T2I) models in a detailed way. It focuses on several important aspects like how well the images match the text prompts, the quality of text rendering, reasoning capabilities, artistic style, and diversity of generated images. This benchmark addresses the shortcomings of previous evaluation methods by providing a structured approach that allows researchers to analyze specific areas of model performance. By making the code and dataset publicly available, OneIG-Bench promotes reproducibility and encourages comparisons among different T2I models.'}, 'zh': {'title': '全面评估文本到图像模型的基准框架', 'desc': 'OneIG-Bench是一个基准框架，旨在全面评估文本到图像（T2I）模型的性能。它涵盖了多个维度，包括提示与图像的对齐、文本渲染、推理能力、风格化和多样性。通过系统化的评估，OneIG-Bench帮助研究人员深入分析模型的优缺点，识别图像生成过程中的瓶颈。该框架的代码库和数据集已公开，便于在T2I研究社区中进行可重复的评估研究和跨模型比较。'}}}, {'id': 'https://huggingface.co/papers/2506.07491', 'title': 'SpatialLM: Training Large Language Models for Structured Indoor Modeling', 'url': 'https://huggingface.co/papers/2506.07491', 'abstract': 'SpatialLM is a large language model capable of processing 3D point cloud data to produce structured outputs for spatial understanding, outperforming previous methods on layout estimation and object detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.', 'score': 28, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '604b98825a94f8d1', 'authors': ['Yongsen Mao', 'Junhao Zhong', 'Chuan Fang', 'Jia Zheng', 'Rui Tang', 'Hao Zhu', 'Ping Tan', 'Zihan Zhou'], 'affiliations': ['Hong Kong University of Science and Technology', 'Manycore Tech Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.07491.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#3d', '#open_source', '#synthetic', '#multimodal'], 'emoji': '🏠', 'ru': {'title': 'Пространственное понимание 3D-сцен с помощью языковых моделей', 'desc': 'SpatialLM - это большая языковая модель, способная обрабатывать трехмерные облака точек для пространственного понимания сцен. Модель генерирует структурированные выходные данные, включая архитектурные элементы и семантические категории объектов. В отличие от предыдущих методов, SpatialLM использует стандартную архитектуру мультимодальной LLM и дообучается напрямую из открытых LLM. Модель демонстрирует передовые результаты в оценке планировки помещений и конкурентоспособные результаты в трехмерном обнаружении объектов.'}, 'en': {'title': 'Revolutionizing 3D Spatial Understanding with SpatialLM', 'desc': 'SpatialLM is a large language model that specializes in understanding 3D point cloud data, enabling it to produce structured outputs for spatial comprehension. It identifies and categorizes architectural features such as walls, doors, and windows, as well as oriented object boxes. Unlike earlier approaches that relied on specific network designs for tasks, SpatialLM uses a standard multimodal architecture and is fine-tuned from existing open-source models. The model is trained on a comprehensive dataset of indoor scenes, achieving state-of-the-art results in layout estimation and strong performance in 3D object detection, paving the way for advancements in augmented reality and robotics.'}, 'zh': {'title': 'SpatialLM：提升空间理解的语言模型', 'desc': 'SpatialLM是一种大型语言模型，能够处理3D点云数据，并生成结构化的空间理解输出。这些输出包括建筑元素，如墙壁、门、窗户以及带有语义类别的定向物体框。与之前的方法不同，SpatialLM遵循标准的多模态LLM架构，并直接从开源LLM进行微调。通过收集包含12,328个室内场景的高质量合成数据集，我们的模型在布局估计和3D物体检测任务上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.07803', 'title': 'Image Reconstruction as a Tool for Feature Analysis', 'url': 'https://huggingface.co/papers/2506.07803', 'abstract': 'Image reconstruction reveals that vision encoders retain more image information after image-based tasks and that orthogonal rotations in feature space control color encoding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub.', 'score': 24, 'issue_id': 4213, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '0bba64237ba10a58', 'authors': ['Eduard Allakhverdov', 'Dmitrii Tarasov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2506.07803.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#cv', '#multimodal', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн внутренней структуры энкодеров компьютерного зрения', 'desc': 'Исследователи предложили новый метод интерпретации признаков в энкодерах компьютерного зрения через реконструкцию изображений. Они обнаружили, что энкодеры, предобученные на задачах, связанных с изображениями, сохраняют значительно больше информации об изображении, чем обученные на других задачах. Авторы применили свой метод к различным энкодерам и ранжировали их по информативности представлений признаков. Также было показано, что ортогональные вращения в пространстве признаков контролируют кодирование цвета.'}, 'en': {'title': 'Unlocking the Secrets of Vision Encoders Through Image Reconstruction', 'desc': 'This paper explores how vision encoders, which are used in various AI applications, represent image features internally. The authors introduce a method for interpreting these features through image reconstruction, comparing two model families with different training objectives. They find that encoders trained on image-based tasks retain more information than those trained on non-image tasks. Additionally, they show that manipulating the feature space through orthogonal rotations can control how colors are encoded in the reconstructed images.'}, 'zh': {'title': '揭示视觉编码器的特征空间结构', 'desc': '本论文探讨了视觉编码器在图像重建中的表现，发现经过图像任务训练的编码器保留了更多的图像信息。我们比较了两种模型家族，SigLIP和SigLIP2，发现前者在图像任务上的预训练使其特征表示更具信息量。通过对特征空间的操作，我们发现正交旋转可以控制颜色编码，而不是空间变换。该方法适用于任何视觉编码器，有助于理解其特征空间的内部结构。'}}}, {'id': 'https://huggingface.co/papers/2506.06205', 'title': 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning', 'url': 'https://huggingface.co/papers/2506.06205', 'abstract': 'Astra, a dual-model architecture for mobile robot navigation, uses a multimodal LLM for global localization and a multitask network for local path planning and odometry estimation, achieving high success rates in diverse indoor environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.', 'score': 20, 'issue_id': 4216, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'ff910133c6b6f7f5', 'authors': ['Sheng Chen', 'Peiyu He', 'Jiaxin Hu', 'Ziyang Liu', 'Yansheng Wang', 'Tao Xu', 'Chi Zhang', 'Chongchong Zhang', 'Chao An', 'Shiyu Cai', 'Duo Cao', 'Kangping Chen', 'Shuai Chu', 'Tianwei Chu', 'Mingdi Dan', 'Min Du', 'Weiwei Fang', 'Pengyou Fu', 'Junkai Hu', 'Xiaowei Jiang', 'Zhaodi Jiang', 'Fuxuan Li', 'Jun Li', 'Minghui Li', 'Mingyao Li', 'Yanchang Li', 'Zhibin Li', 'Guangming Liu', 'Kairui Liu', 'Lihao Liu', 'Weizhi Liu', 'Xiaoshun Liu', 'Yufei Liu', 'Yunfei Liu', 'Qiang Lu', 'Yuanfei Luo', 'Xiang Lv', 'Hongying Ma', 'Sai Ma', 'Lingxian Mi', 'Sha Sa', 'Hongxiang Shu', 'Lei Tian', 'Chengzhi Wang', 'Jiayu Wang', 'Kaijie Wang', 'Qingyi Wang', 'Renwen Wang', 'Tao Wang', 'Wei Wang', 'Xirui Wang', 'Chao Wei', 'Xuguang Wei', 'Zijun Xia', 'Zhaohao Xiao', 'Tingshuai Yan', 'Liyan Yang', 'Yifan Yang', 'Zhikai Yang', 'Zhong Yin', 'Li Yuan', 'Liuchun Yuan', 'Chi Zhang', 'Jinyang Zhang', 'Junhui Zhang', 'Linge Zhang', 'Zhenyi Zhang', 'Zheyu Zhang', 'Dongjie Zhu', 'Hang Li', 'Yangang Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.06205.jpg', 'data': {'categories': ['#robotics', '#games', '#architecture', '#agents', '#graphs', '#optimization', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Астра: двухмодельная архитектура ИИ для эффективной навигации роботов', 'desc': 'Астра - это двухмодельная архитектура для навигации мобильных роботов. Она состоит из мультимодальной большой языковой модели для глобальной локализации и многозадачной сети для локального планирования пути и оценки одометрии. Астра использует гибридную топологическо-семантическую карту в качестве глобальной карты. Система достигает высоких показателей успешности в различных внутренних средах.'}, 'en': {'title': 'Astra: Navigating Indoor Spaces with Dual-Model Intelligence', 'desc': 'Astra is a dual-model architecture designed for mobile robot navigation, addressing challenges in complex indoor environments. It consists of Astra-Global, a multimodal large language model (LLM) that combines vision and language inputs for effective global localization, and Astra-Local, a multitask network focused on local path planning and odometry estimation. The system utilizes advanced techniques like a hybrid topological-semantic graph and a 4D spatial-temporal encoder to enhance performance and adaptability. By integrating self-supervised learning and innovative loss functions, Astra achieves high success rates in navigating diverse indoor settings.'}, 'zh': {'title': 'Astra：智能移动机器人导航的新纪元', 'desc': 'Astra是一种双模型架构，专为移动机器人导航设计。它结合了多模态大语言模型（LLM）和多任务网络，分别用于全局定位和局部路径规划。Astra-Global通过处理视觉和语言输入，利用混合拓扑-语义图进行自我和目标定位，超越了传统的视觉位置识别方法。Astra-Local则通过自监督学习生成强大的4D特征，确保在复杂室内环境中实现高成功率。'}}}, {'id': 'https://huggingface.co/papers/2506.07298', 'title': 'Pre-trained Large Language Models Learn Hidden Markov Models In-context', 'url': 'https://huggingface.co/papers/2506.07298', 'abstract': 'LLMs using in-context learning can accurately predict sequences generated by HMMs, showcasing its potential for uncovering hidden structures in complex data.  \t\t\t\t\tAI-generated summary \t\t\t\t Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)x2013their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequencesx2013an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.', 'score': 15, 'issue_id': 4208, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '568113a6bc87dd15', 'authors': ['Yijia Dai', 'Zhaolin Gao', 'Yahya Satter', 'Sarah Dean', 'Jennifer J. Sun'], 'affiliations': ['Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07298.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#data', '#science', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'LLM раскрывают скрытые структуры данных через обучение в контексте', 'desc': 'Исследование демонстрирует способность больших языковых моделей (LLM) эффективно моделировать данные, генерируемые скрытыми марковскими моделями (HMM), используя обучение в контексте (ICL). LLM достигают предсказательной точности, близкой к теоретическому оптимуму, на разнообразном наборе синтетических HMM. Авторы выявляют новые тенденции масштабирования, зависящие от свойств HMM, и предлагают теоретические гипотезы для объяснения эмпирических наблюдений. Результаты исследования углубляют понимание ICL в LLM и демонстрируют его потенциал как мощного инструмента для выявления скрытых структур в сложных научных данных.'}, 'en': {'title': 'Unlocking Hidden Structures with In-Context Learning in LLMs', 'desc': 'This paper demonstrates that large language models (LLMs) can effectively use in-context learning (ICL) to predict sequences generated by Hidden Markov Models (HMMs). The authors show that LLMs can achieve high predictive accuracy on synthetic HMM data, approaching theoretical limits. They also identify scaling trends related to HMM characteristics and provide insights into using ICL as a diagnostic tool for complex datasets. Additionally, the study highlights that ICL performs competitively on real-world tasks compared to expert-designed models, showcasing its potential in revealing hidden structures in data.'}, 'zh': {'title': '利用上下文学习揭示复杂数据的潜力', 'desc': '本研究展示了预训练的大型语言模型（LLMs）能够通过上下文学习（ICL）有效地预测由隐马尔可夫模型（HMMs）生成的序列。这表明LLMs在揭示复杂数据中的隐藏结构方面具有潜力。我们在多种合成HMMs上测试了LLMs，发现其预测准确性接近理论最优值。此外，我们还提供了使用ICL作为复杂数据诊断工具的实用指南，并在真实的动物决策任务中取得了与人类专家设计模型相当的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.07986', 'title': 'Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.07986', 'abstract': 'Temperature-Adjusted Cross-modal Attention (TACA) improves text-image alignment in diffusion-based multimodal transformers by dynamically balancing multimodal interactions and provides a parameter-efficient solution.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA', 'score': 14, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '54c2bf2395cf58cc', 'authors': ['Zhengyao Lv', 'Tianlin Pan', 'Chenyang Si', 'Zhaoxi Chen', 'Wangmeng Zuo', 'Ziwei Liu', 'Kwan-Yee K. Wong'], 'affiliations': ['Harbin Institute of Technology', 'Nanjing University', 'Nanyang Technological University', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07986.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'Балансировка внимания для точной генерации изображений по тексту', 'desc': 'Статья представляет метод Temperature-Adjusted Cross-modal Attention (TACA) для улучшения согласованности текста и изображений в мультимодальных диффузионных трансформерах. TACA динамически балансирует взаимодействие между модальностями с помощью температурного масштабирования и настройки, зависящей от временного шага. Метод эффективно решает проблемы подавления кросс-модального внимания и отсутствия учета временных шагов в механизме внимания. TACA значительно улучшает соответствие текста и изображений при минимальных вычислительных затратах.'}, 'en': {'title': 'Enhancing Text-Image Alignment with TACA', 'desc': 'The paper introduces Temperature-Adjusted Cross-modal Attention (TACA), a novel approach to enhance text-image alignment in multimodal diffusion transformers. TACA addresses two main challenges: the imbalance in token representation between text and images, and the absence of attention weighting that considers the timing of inputs. By implementing temperature scaling and timestep-dependent adjustments, TACA efficiently rebalances interactions between modalities. The results show that TACA, when used with LoRA fine-tuning, significantly improves alignment in state-of-the-art models, leading to better semantic fidelity in generated images.'}, 'zh': {'title': '温度调整跨模态注意力：提升文本与图像对齐的利器', 'desc': '本文提出了一种名为温度调整跨模态注意力（TACA）的方法，旨在改善基于扩散的多模态变换器中的文本与图像对齐。我们发现现有模型在文本提示与生成内容之间存在注意力机制的不足，主要体现在视觉和文本模态之间的令牌不平衡以及缺乏时间步感知的注意力加权。TACA通过温度缩放和时间步依赖的调整，动态平衡多模态交互，从而提高文本与图像的对齐效果。实验结果表明，TACA在FLUX和SD3.5等先进模型上显著提升了图像-文本对齐的准确性，且计算开销极小。'}}}, {'id': 'https://huggingface.co/papers/2506.07553', 'title': 'GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular\n  Structure Recognition', 'url': 'https://huggingface.co/papers/2506.07553', 'abstract': "GTR-Mol-VLM, a vision-language model with graph traversal and data-centric principles, achieves superior accuracy in converting molecular images to machine-readable formats, particularly in handling complex structures and functional group abbreviations.  \t\t\t\t\tAI-generated summary \t\t\t\t Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT.", 'score': 12, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '379cf5e0efdacd27', 'authors': ['Jingchao Wang', 'Haote Yang', 'Jiang Wu', 'Yifan He', 'Xingjian Wei', 'Yinfan Wang', 'Chengjin Liu', 'Lingli Ge', 'Lijun Wu', 'Bin Wang', 'Dahua Lin', 'Conghui He'], 'affiliations': ['Chinese University of Hong Kong', 'East China Normal University', 'Fudan University', 'Northwestern Polytechnical University', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07553.jpg', 'data': {'categories': ['#dataset', '#graphs', '#benchmark', '#reasoning', '#cv', '#science', '#games'], 'emoji': '🧪', 'ru': {'title': 'Точное распознавание химических структур с помощью обхода графа и машинного обучения', 'desc': 'GTR-Mol-VLM - это новая модель компьютерного зрения и обработки естественного языка для распознавания химических структур на изображениях. Она использует механизм обхода графа и принцип точного распознавания увиденного для повышения точности. Модель превосходит существующие решения, особенно при работе со сложными структурами и сокращениями функциональных групп. Для обучения и оценки модели были созданы большой набор данных GTR-CoT-1.3M и бенчмарк MolRec-Bench.'}, 'en': {'title': 'Transforming Molecular Images into Accurate Data with GTR-Mol-VLM', 'desc': 'GTR-Mol-VLM is a new vision-language model designed to convert molecular images into machine-readable formats with high accuracy. It uses a unique Graph Traversal mechanism that mimics human reasoning by analyzing molecular graphs step-by-step. Additionally, it employs a data-centric approach to ensure that the model accurately recognizes abbreviated structures in images. The model has been tested against various benchmarks and has shown significant improvements, especially in handling complex molecular structures and functional group abbreviations.'}, 'zh': {'title': 'GTR-Mol-VLM：提升分子图像识别的智能新方法', 'desc': 'GTR-Mol-VLM是一种结合图遍历和数据中心原则的视觉语言模型，能够将分子图像准确转换为机器可读格式。该模型通过图遍历机制模拟人类推理，逐步解析分子图，解决了复杂结构和功能基团缩写的问题。为了支持模型开发，我们构建了一个大规模的指令调优数据集GTR-CoT-1.3M，并推出了MolRec-Bench基准测试，以评估图解析的准确性。实验结果表明，GTR-Mol-VLM在处理分子图像时的表现优于其他专业模型和通用模型，特别是在功能基团缩写的场景中，准确率提高了约14个百分点。'}}}, {'id': 'https://huggingface.co/papers/2506.07530', 'title': 'BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation', 'url': 'https://huggingface.co/papers/2506.07530', 'abstract': "BitVLA, a 1-bit VLA model with ternary parameters, achieves comparable performance to OpenVLA-OFT on LIBERO while using 29.8% less memory through distillation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.", 'score': 12, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '2ff752565d34986b', 'authors': ['Hongyu Wang', 'Chuyan Xiong', 'Ruiping Wang', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.07530.jpg', 'data': {'categories': ['#optimization', '#small_models', '#robotics', '#inference', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Эффективная 1-битная модель VLA для роботов с ограниченными ресурсами', 'desc': 'BitVLA - это первая 1-битная модель VLA для роботизированных манипуляций, где каждый параметр является тернарным (-1, 0, 1). Модель использует стратегию обучения с дистилляцией для сжатия полноточного энкодера до 1,58-битных весов. BitVLA достигает производительности, сопоставимой с современной моделью OpenVLA-OFT на бенчмарке LIBERO, потребляя всего 29,8% памяти. Эти результаты демонстрируют перспективность BitVLA для развертывания на устройствах с ограниченной памятью.'}, 'en': {'title': 'Efficient Robotics with BitVLA: Less Memory, Same Performance!', 'desc': 'BitVLA is a novel 1-bit Vision-Language-Action (VLA) model designed for robotics manipulation tasks, utilizing ternary parameters to optimize performance. It employs a distillation-aware training strategy that compresses a full-precision vision encoder into 1.58-bit weights, significantly reducing memory usage. Despite not being pretrained on large-scale robotics data, BitVLA achieves performance on par with the state-of-the-art OpenVLA-OFT model while using 29.8% less memory. This advancement makes BitVLA particularly suitable for deployment in resource-limited robotic systems.'}, 'zh': {'title': 'BitVLA：高效的1位视觉-语言-动作模型', 'desc': 'BitVLA是一种1位视觉-语言-动作（VLA）模型，采用三元参数，旨在解决机器人操作任务中的内存限制问题。通过使用注意蒸馏训练策略，BitVLA在保持性能的同时，显著减少了内存占用，达到仅使用29.8%的内存。尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试中表现出与最先进的OpenVLA-OFT模型相当的性能。该研究展示了BitVLA在资源受限的边缘设备上的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.05062', 'title': 'Debatable Intelligence: Benchmarking LLM Judges via Debate Speech\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.05062', 'abstract': 'This study evaluates the performance of large language models in assessing debate speeches, a task requiring deep understanding of various aspects of speech, including argument strength and coherence, and compares it to human judges.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Debate Speech Evaluation as a novel and challenging benchmark for assessing LLM judges. Evaluating debate speeches requires a deep understanding of the speech at multiple levels, including argument strength and relevance, the coherence and organization of the speech, the appropriateness of its style and tone, and so on. This task involves a unique set of cognitive abilities that have previously received limited attention in systematic LLM benchmarking. To explore such skills, we leverage a dataset of over 600 meticulously annotated debate speeches and present the first in-depth analysis of how state-of-the-art LLMs compare to human judges on this task. Our findings reveal a nuanced picture: while larger models can approximate individual human judgments in some respects, they differ substantially in their overall judgment behavior. We also investigate the ability of frontier LLMs to generate persuasive, opinionated speeches, showing that models may perform at a human level on this task.', 'score': 11, 'issue_id': 4219, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '150a859b148d8cc1', 'authors': ['Noy Sternlicht', 'Ariel Gera', 'Roy Bar-Haim', 'Tom Hope', 'Noam Slonim'], 'affiliations': ['IBM Research', 'School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'The Allen Institute for AI (AI2)'], 'pdf_title_img': 'assets/pdf/title_img/2506.05062.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#interpretability', '#reasoning', '#multimodal', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'LLM vs Человек: Кто лучший судья дебатов?', 'desc': 'Исследование оценивает способность больших языковых моделей (LLM) анализировать дебатные речи, сравнивая их с человеческими судьями. Задача требует глубокого понимания различных аспектов речи, включая силу аргументации и связность. Авторы представляют новый бенчмарк для оценки LLM на основе набора данных из более 600 аннотированных дебатных выступлений. Результаты показывают, что крупные модели могут приближаться к оценкам отдельных людей, но отличаются в общем поведении при вынесении суждений.'}, 'en': {'title': 'Evaluating Debate Speeches: Can AI Match Human Judgment?', 'desc': "This paper investigates how well large language models (LLMs) can evaluate debate speeches, which require understanding complex elements like argument strength and coherence. It introduces a new benchmark called Debate Speech Evaluation, focusing on the cognitive skills needed for this task. The study uses a dataset of over 600 annotated speeches to compare LLMs with human judges, revealing that while LLMs can mimic some human judgments, their overall evaluation patterns differ significantly. Additionally, the research explores LLMs' ability to generate persuasive speeches, indicating that they can perform at a level comparable to humans in this area."}, 'zh': {'title': '评估辩论演讲：语言模型与人类的较量', 'desc': '本研究评估了大型语言模型在辩论演讲评估中的表现，这一任务需要对演讲的多个方面有深入理解，包括论点的强度和连贯性。我们引入了辩论演讲评估作为一个新颖且具有挑战性的基准，比较了语言模型与人类评审的表现。研究发现，尽管较大的模型在某些方面可以接近个别的人类判断，但它们在整体判断行为上存在显著差异。我们还探讨了前沿语言模型生成有说服力和观点明确的演讲的能力，显示这些模型在此任务上可能达到人类水平。'}}}, {'id': 'https://huggingface.co/papers/2506.07712', 'title': 'Through the Valley: Path to Effective Long CoT Training for Small\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07712', 'abstract': 'Small language models experience significant performance declines when trained on long chain-of-thought data due to error accumulation, impacting downstream reinforcement learning but potentially mitigated by extensive supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; <=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models.', 'score': 10, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '856fa5edc302286a', 'authors': ['Renjie Luo', 'Jiaxi Li', 'Chen Huang', 'Wei Lu'], 'affiliations': ['StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2506.07712.jpg', 'data': {'categories': ['#small_models', '#long_context', '#training', '#reasoning', '#rl'], 'emoji': '📉', 'ru': {'title': 'Малые модели страдают от длинных рассуждений', 'desc': 'Статья исследует феномен деградации производительности малых языковых моделей при обучении на длинных цепочках рассуждений (CoT). Авторы обнаружили, что модели с менее чем 3 миллиардами параметров могут терять до 75% своей изначальной эффективности после такого обучения. Это явление объясняется накоплением ошибок в длинных последовательностях рассуждений. Исследование также показывает, что деградация CoT может негативно влиять на последующее обучение с подкреплением, хотя этот эффект можно смягчить масштабным дообучением под присмотром.'}, 'en': {'title': 'Mitigating Long CoT Degradation in Small Language Models', 'desc': 'This paper discusses a problem called Long CoT Degradation, which affects small language models (SLMs) when they are trained on long chain-of-thought (CoT) data. The authors found that these models can lose a significant amount of performance due to error accumulation, especially when trained on limited long CoT examples. Their experiments show that even with a large number of training examples, some small models still struggle to recover their original performance. The study suggests that while long CoT training can enhance reasoning, it may not be beneficial for smaller models without adequate supervised fine-tuning to mitigate the negative effects.'}, 'zh': {'title': '小型模型的长链思维退化问题', 'desc': '小型语言模型在长链思维数据上训练时，因错误累积而导致性能显著下降，这种现象被称为长链思维退化。尽管长链思维监督对大型模型有效，但小型模型在有限的长链思维数据上训练时，性能可能下降高达75%。我们的研究表明，错误累积是导致这种退化的主要原因，长响应虽然增加了多步推理的能力，但也加大了错误叠加的风险。此外，长链思维退化还可能对后续的强化学习产生负面影响，但通过充分规模的监督微调可以缓解这一问题。'}}}, {'id': 'https://huggingface.co/papers/2506.06941', 'title': 'The Illusion of Thinking: Understanding the Strengths and Limitations of\n  Reasoning Models via the Lens of Problem Complexity', 'url': 'https://huggingface.co/papers/2506.06941', 'abstract': "Large Reasoning Models (LRMs) undergo accuracy collapse at higher complexities and exhibit unique performance scaling behaviors compared to standard LLMs, with failures in exact computation and inconsistent reasoning across scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.", 'score': 10, 'issue_id': 4208, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '6f3991ea3357f456', 'authors': ['Parshin Shojaee', 'Iman Mirzadeh', 'Keivan Alizadeh', 'Maxwell Horton', 'Samy Bengio', 'Mehrdad Farajtabar'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2506.06941.jpg', 'data': {'categories': ['#math', '#training', '#benchmark', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Ограничения крупных моделей рассуждений: анализ производительности и масштабируемости', 'desc': 'Исследование показывает, что крупные модели рассуждений (LRM) демонстрируют резкое падение точности при повышении сложности задач. В отличие от стандартных языковых моделей, LRM имеют уникальные характеристики масштабирования производительности. Модели испытывают трудности с точными вычислениями и проявляют непоследовательность в рассуждениях при изменении масштаба. Анализ внутренних процессов рассуждений LRM выявляет их сильные и слабые стороны, поднимая вопросы о реальных возможностях этих моделей.'}, 'en': {'title': 'Understanding the Limits of Large Reasoning Models in Complex Tasks', 'desc': 'This paper explores the performance of Large Reasoning Models (LRMs) in relation to their complexity and reasoning capabilities. It reveals that LRMs experience accuracy collapse when faced with high-complexity tasks, which is a significant limitation compared to standard language models (LLMs). The study introduces a controlled environment to analyze both the final answers and the internal reasoning processes of LRMs, highlighting their inconsistent reasoning and failure to perform exact computations. The findings categorize performance into three regimes based on task complexity, providing insights into the strengths and weaknesses of LRMs in reasoning tasks.'}, 'zh': {'title': '大型推理模型的复杂性挑战', 'desc': '大型推理模型（LRMs）在处理更复杂的问题时会出现准确性崩溃，并且与标准语言模型（LLMs）相比，它们的性能扩展行为独特。尽管LRMs在推理基准测试中表现出色，但它们的基本能力和局限性仍然不够清楚。通过可控的难题环境，我们能够精确操控复杂性，并分析LRMs的内部推理过程。研究表明，LRMs在高复杂性任务中会完全崩溃，并且在问题复杂性增加时，它们的推理努力会先增加后减少，显示出反直觉的扩展限制。'}}}, {'id': 'https://huggingface.co/papers/2506.06006', 'title': 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation\n  Models', 'url': 'https://huggingface.co/papers/2506.06006', 'abstract': 'Foundation models can be fine-tuned to develop dynamics models more easily than world models, with dynamics models aiding world model development through weak supervision and inference time verification, leading to state-of-the-art performance in action-centric image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t To what extent do vision-and-language foundation models possess a realistic world model (observation times action rightarrow observation) and a dynamics model (observation times observation rightarrow action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.', 'score': 10, 'issue_id': 4217, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'fc75b0606885fd24', 'authors': ['Yifu Qiu', 'Yftah Ziser', 'Anna Korhonen', 'Shay B. Cohen', 'Edoardo M. Ponti'], 'affiliations': ['Institute for Language, Cognition and Computation, University of Edinburgh', 'Language Technology Lab, University of Cambridge', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06006.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#synthetic', '#multimodal', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Динамические модели как ключ к улучшению мировых моделей в ИИ', 'desc': 'Исследование показывает, что модели основания (foundation models) легче настроить для создания динамических моделей, чем мировых моделей. Динамические модели помогают в разработке мировых моделей через слабый надзор и верификацию во время вывода. Это приводит к улучшению результатов в редактировании изображений на основе действий. Авторы предлагают новые стратегии для обучения мировых моделей с помощью динамических, включая взвешивание токенов изображений по важности.'}, 'en': {'title': 'Boosting Image Editing with Dynamics Models', 'desc': 'This paper explores how foundation models can be fine-tuned to create dynamics models, which are easier to develop than world models. Dynamics models help in building world models by providing weak supervision and verifying actions during inference. The authors propose two strategies for using dynamics models: one involves using synthetic data to label actions in unannotated video frames, and the other uses these models to score and guide the search for better world models during inference. The results show that their approach significantly improves performance in action-centric image editing tasks, outperforming existing models by 15%.'}, 'zh': {'title': '动态模型助力世界模型，提升图像编辑性能', 'desc': '本论文探讨了基础模型在动态模型和世界模型开发中的应用。研究发现，通过微调基础模型来获取动态模型比获取世界模型要容易得多。动态模型可以通过弱监督学习和推理时验证来辅助世界模型的开发，从而在以动作为中心的图像编辑任务中实现了最先进的性能。最终，我们的最佳模型在Aurora-Bench的真实世界子集上比现有的图像编辑模型提高了15%的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08010', 'title': "Vision Transformers Don't Need Trained Registers", 'url': 'https://huggingface.co/papers/2506.08010', 'abstract': 'A training-free method shifts high-norm activations in Vision Transformers to an untrained token, enhancing attention maps and performance across visual tasks, and improving interpretability in vision-language models.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.', 'score': 9, 'issue_id': 4212, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'd833304e26f5d1ee', 'authors': ['Nick Jiang', 'Amil Dravid', 'Alexei Efros', 'Yossi Gandelsman'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.08010.jpg', 'data': {'categories': ['#cv', '#training', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Улучшение внимания в трансформерах без переобучения', 'desc': 'Исследователи обнаружили, что в моделях компьютерного зрения, основанных на трансформерах, небольшое количество нейронов создает токены с высокой нормой, что приводит к шумным картам внимания. Они разработали метод без дополнительного обучения, который перемещает активации с высокой нормой в дополнительный необученный токен. Этот подход улучшает карты внимания и признаков, повышает производительность на различных задачах компьютерного зрения и улучшает интерпретируемость мультимодальных моделей. Метод сравним по эффективности с моделями, специально обученными с регистровыми токенами, но не требует переобучения.'}, 'en': {'title': 'Enhancing Vision Transformers with Training-Free Token Shifting', 'desc': 'This paper presents a novel training-free method to improve Vision Transformers by addressing the issue of high-norm activations that lead to noisy attention maps. The authors identify that certain neurons concentrate these high-norm activations on outlier tokens, which disrupts attention patterns and affects visual task performance. Instead of retraining models with additional learned tokens, they propose shifting these activations to an untrained token, effectively simulating the benefits of register tokens without the need for retraining. Their approach not only enhances attention and feature maps but also boosts performance across various visual tasks, making it applicable to existing vision-language models for better interpretability.'}, 'zh': {'title': '无训练方法提升视觉模型性能与可解释性', 'desc': '本文提出了一种无训练的方法，通过将高范数激活转移到未训练的标记上，来增强视觉变换器中的注意力图和性能。研究发现，在多个模型中，一小部分神经元负责将高范数激活集中在异常标记上，导致注意力模式不规则，影响视觉处理。我们的方法无需重新训练模型，而是利用发现的注册神经元将高范数激活转移到额外的未训练标记上，从而改善注意力和特征图。实验结果表明，该方法在多个视觉任务中提升了性能，并且与显式训练的注册标记模型的结果相当。'}}}, {'id': 'https://huggingface.co/papers/2506.07463', 'title': 'CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.07463', 'abstract': 'A large-scale bilingual pre-training dataset, CCI4.0, enhances data quality and diverse reasoning patterns for language models, leading to improved performance in downstream tasks like math and code reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a 5.2 TB carefully curated Chinese web corpus, a 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora.', 'score': 8, 'issue_id': 4209, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '219a3c7d2fbb1e22', 'authors': ['Guang Liu', 'Liangdong Wang', 'Jijie Li', 'Yang Yu', 'Yao Xu', 'Jiabei Chen', 'Yu Bai', 'Feng Liao', 'Yonghua Lin'], 'affiliations': ['baai.ac.cn'], 'pdf_title_img': 'assets/pdf/title_img/2506.07463.jpg', 'data': {'categories': ['#hallucinations', '#data', '#dataset', '#reasoning', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'CCI4.0: Новый уровень качества данных для обучения языковых моделей', 'desc': 'CCI4.0 - это крупномасштабный двуязычный набор данных для предварительного обучения языковых моделей. Он включает в себя высококачественные данные и разнообразные шаблоны рассуждений, что улучшает производительность в задачах вроде математики и рефлексии кода. Набор данных состоит из двух поднаборов: CCI4.0-M2-Base и CCI4.0-M2-CoT, общим объемом около 35 ТБ. Авторы предлагают новый конвейер обработки данных, включающий двухэтапное удаление дубликатов, оценку качества с помощью мультиклассификатора и фильтрацию по доменам.'}, 'en': {'title': 'Enhancing Language Models with CCI4.0: Quality Data for Better Reasoning', 'desc': 'The paper presents CCI4.0, a large-scale bilingual pre-training dataset designed to improve the quality of data and enhance reasoning capabilities in language models. It consists of two main sub-datasets, CCI4.0-M2-Base and CCI4.0-M2-CoT, which together provide a diverse range of high-quality data from various domains. The authors introduce a novel data curation pipeline that includes deduplication, quality scoring, and fluency filtering to ensure the reliability of the training data. Empirical results show that language models trained on CCI4.0 exhibit significant performance improvements in tasks such as mathematics and code reflection, highlighting the importance of quality data in machine learning.'}, 'zh': {'title': '提升语言模型性能的双语数据集CCI4.0', 'desc': '本文介绍了一个名为CCI4.0的大规模双语预训练数据集，旨在提高数据质量和多样化的推理模式。CCI4.0包含约35 TB的数据，分为两个子数据集：CCI4.0-M2-Base和CCI4.0-M2-CoT。通过两阶段去重、多分类器质量评分和领域感知流畅性过滤等新颖流程，确保了数据的高质量。实验证明，使用CCI4.0预训练的语言模型在数学和代码反射等下游任务中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.07309', 'title': 'ConfQA: Answer Only If You Are Confident', 'url': 'https://huggingface.co/papers/2506.07309', 'abstract': 'ConfQA fine-tuning strategy reduces factual statement hallucination in LLMs by 80%, using a dampening prompt and factual statements from knowledge graphs to improve confidence calibration and knowledge selection.  \t\t\t\t\tAI-generated summary \t\t\t\t Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit "I am unsure". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt "answer only if you are confident" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA\'s confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%.', 'score': 8, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '8a4581c11bf35360', 'authors': ['Yin Huang', 'Yifan Ethan Xu', 'Kai Sun', 'Vera Yan', 'Alicia Sun', 'Haidar Khan', 'Jimmy Nguyen', 'Mohammad Kachuee', 'Zhaojiang Lin', 'Yue Liu', 'Aaron Colak', 'Anuj Kumar', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR at Meta', 'Meta Reality Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.07309.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'ConfQA: обуздание галлюцинаций в LLM через уверенное молчание', 'desc': 'Статья представляет стратегию дообучения под названием ConfQA, которая значительно снижает уровень галлюцинаций в больших языковых моделях (LLM) при ответах на фактические вопросы. Ключевыми элементами стратегии являются использование специального промпта для подавления неуверенных ответов и применение фактических утверждений из графов знаний для улучшения калибровки уверенности модели. Результаты показывают снижение уровня галлюцинаций с 20-40% до менее 5% на различных тестах фактической точности. Авторы также предлагают фреймворк Dual Neural Knowledge для эффективного выбора между внутренними параметризованными знаниями модели и внешними символьными знаниями.'}, 'en': {'title': 'ConfQA: Reducing Hallucinations in LLMs with Confidence Calibration', 'desc': "This paper introduces a fine-tuning strategy called ConfQA that significantly reduces the occurrence of factual statement hallucinations in Large Language Models (LLMs) by up to 80%. The approach involves training the model to confidently provide answers when it knows them and to express uncertainty when it does not. Key components of this strategy include a dampening prompt that encourages the model to answer only when confident, and the use of factual statements from knowledge graphs to enhance the model's confidence calibration. Additionally, the proposed Dual Neural Knowledge framework allows the model to effectively choose between internal neural knowledge and external symbolic knowledge based on its confidence level, leading to improved accuracy and reduced reliance on external data retrieval."}, 'zh': {'title': '减少幻觉，提高准确性！', 'desc': '本文提出了一种名为ConfQA的微调策略，旨在减少大型语言模型（LLMs）中的事实陈述幻觉现象。通过使用减弱提示和知识图谱中的事实陈述，该策略能够将幻觉率从20-40%降低到5%以下。核心思想是，当LLM正确回答问题时，继续给出答案；否则，承认“不确定”。此外，ConfQA框架通过选择内部参数化的神经知识和外部记录的符号知识，进一步提高了模型的准确性和知识选择能力。'}}}, {'id': 'https://huggingface.co/papers/2506.08012', 'title': 'GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection\n  Behavior', 'url': 'https://huggingface.co/papers/2506.08012', 'abstract': 'A novel framework, GUI-Reflection, integrates self-reflection and error correction into multimodal GUI models through specialized training stages, enabling more robust and intelligent automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly.', 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '8052bb442adf3c53', 'authors': ['Penghao Wu', 'Shengnan Ma', 'Bo Wang', 'Jiaheng Yu', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08012.jpg', 'data': {'categories': ['#optimization', '#dataset', '#agents', '#training', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самоанализ и исправление ошибок в ИИ для автоматизации графических интерфейсов', 'desc': 'Статья представляет новую структуру под названием GUI-Reflection, которая интегрирует самоанализ и исправление ошибок в мультимодальные модели графического интерфейса пользователя. Эта структура использует специализированные этапы обучения, включая предварительное обучение для GUI, офлайн-обучение с учителем и онлайн-настройку рефлексии. GUI-Reflection позволяет автоматически генерировать данные для обучения без необходимости аннотации человеком. Авторы также разработали набор задач GUI-Reflection Task Suite для оценки способностей к рефлексии и создали среду для онлайн-обучения на мобильных устройствах.'}, 'en': {'title': 'Empowering GUI Automation with Self-Reflection and Error Correction', 'desc': 'The paper introduces GUI-Reflection, a new framework that enhances multimodal GUI models by incorporating self-reflection and error correction during training. This approach addresses the limitations of existing models that primarily learn from error-free data, enabling them to recover from mistakes and improve over time. The framework includes specialized training stages such as pre-training, supervised fine-tuning, and online reflection tuning, allowing models to learn from their own errors without human intervention. By automating data generation and focusing on reflection-oriented tasks, GUI-Reflection aims to create more robust and intelligent automation for graphical user interfaces.'}, 'zh': {'title': '自我反思与错误纠正，提升GUI自动化智能', 'desc': '本文提出了一种新框架GUI-Reflection，将自我反思和错误纠正能力整合到多模态图形用户界面（GUI）模型中。该框架通过专门的训练阶段，使得模型能够在自动化过程中更具鲁棒性和智能性。我们设计了可扩展的数据管道，自动生成反思和错误纠正的数据，并提出了GUI-Reflection任务套件来评估这些能力。最终，框架使得GUI代理具备自我反思和纠正能力，为更智能的GUI自动化奠定基础。'}}}, {'id': 'https://huggingface.co/papers/2506.07434', 'title': 'Well Begun is Half Done: Low-resource Preference Alignment by\n  Weak-to-Strong Decoding', 'url': 'https://huggingface.co/papers/2506.07434', 'abstract': 'A novel Weak-to-Strong Decoding (WSD) framework enhances the alignment of large language models using a small aligned model to draft responses initially, improving alignment and performance without degrading downstream task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) require alignment with human preferences to avoid generating offensive, false, or meaningless content. Recently, low-resource methods for LLM alignment have been popular, while still facing challenges in obtaining both high-quality and aligned content. Motivated by the observation that the difficulty of generating aligned responses is concentrated at the beginning of decoding, we propose a novel framework, Weak-to-Strong Decoding (WSD), to enhance the alignment ability of base models by the guidance of a small aligned model. The small model first drafts well-aligned beginnings, followed by the large base model to continue the rest, controlled by a well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign, to fine-tune a small-sized Pilot-3B as the draft model, which effectively enhances different base models under the WSD framework to outperform all baseline methods, while avoiding degradation on downstream tasks, termed as the alignment tax. Extensive experiments are further conducted to examine the impact of different settings and time efficiency, as well as analyses on the intrinsic mechanisms of WSD in depth.', 'score': 7, 'issue_id': 4208, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '11d72d8081b838d5', 'authors': ['Feifan Song', 'Shaohang Wei', 'Wen Luo', 'Yuxuan Fan', 'Tianyu Liu', 'Guoyin Wang', 'Houfeng Wang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07434.jpg', 'data': {'categories': ['#alignment', '#dataset', '#training', '#rlhf'], 'emoji': '🔀', 'ru': {'title': 'Слабый старт, сильный финиш: новый метод выравнивания языковых моделей', 'desc': 'Статья представляет новый подход к улучшению выравнивания больших языковых моделей (LLM) с предпочтениями человека. Метод Weak-to-Strong Decoding (WSD) использует маленькую выровненную модель для создания начала ответа, после чего большая базовая модель продолжает генерацию. Авторы собрали датасет GenerAlign для обучения небольшой модели Pilot-3B, которая эффективно улучшает работу различных базовых моделей в рамках WSD. Эксперименты показывают, что этот подход превосходит базовые методы без ухудшения производительности на других задачах.'}, 'en': {'title': 'Enhancing LLM Alignment with Weak-to-Strong Decoding', 'desc': 'The paper introduces a new framework called Weak-to-Strong Decoding (WSD) that improves the alignment of large language models (LLMs) with human preferences. It uses a smaller, aligned model to generate the initial part of responses, which helps guide the larger model in producing better outputs. This approach addresses the challenge of generating high-quality and aligned content without negatively impacting the performance on other tasks, known as the alignment tax. The authors also present a new dataset, GenerAlign, which is used to fine-tune the smaller model, leading to superior results compared to existing methods.'}, 'zh': {'title': '弱到强解码：提升语言模型对齐能力的创新框架', 'desc': '本文提出了一种新的弱到强解码框架（WSD），旨在通过小型对齐模型的指导来增强大型语言模型的对齐能力。该框架首先由小模型草拟出高质量的对齐开头，然后由大型基础模型继续生成后续内容。通过设计良好的自动切换机制，WSD能够在不降低下游任务性能的情况下，显著提高模型的对齐效果。我们还收集了一个新的数据集GenerAlign，以微调小型Pilot-3B模型，实验结果表明该方法在多种基线方法中表现优越。'}}}, {'id': 'https://huggingface.co/papers/2505.23760', 'title': 'Model Immunization from a Condition Number Perspective', 'url': 'https://huggingface.co/papers/2505.23760', 'abstract': 'An algorithm with regularization terms based on the condition number of a Hessian matrix is proposed to analyze and achieve model immunization, demonstrating effectiveness on both linear models and deep-nets.  \t\t\t\t\tAI-generated summary \t\t\t\t Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num.', 'score': 7, 'issue_id': 4224, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '764c4291f581dbd9', 'authors': ['Amber Yijia Zheng', 'Cedar Site Bai', 'Brian Bullins', 'Raymond A. Yeh'], 'affiliations': ['Department of Computer Science, Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23760.jpg', 'data': {'categories': ['#training', '#security', '#architecture', '#optimization'], 'emoji': '🛡️', 'ru': {'title': 'Иммунизация моделей машинного обучения с помощью регуляризации матрицы Гессе', 'desc': 'В статье предлагается алгоритм с регуляризационными членами, основанный на числе обусловленности матрицы Гессе, для анализа и достижения иммунизации модели. Авторы разрабатывают framework для анализа иммунизации линейных моделей, используя число обусловленности. На основе этого framework создается алгоритм с регуляризацией для контроля чисел обусловленности после предварительного обучения. Эмпирические результаты на линейных моделях и нелинейных глубоких нейронных сетях демонстрируют эффективность предложенного алгоритма для иммунизации модели.'}, 'en': {'title': 'Enhancing Model Immunization with Hessian-Based Regularization', 'desc': 'This paper introduces a new algorithm that uses regularization based on the condition number of the Hessian matrix to improve model immunization. Model immunization is the process of preparing machine learning models to avoid harmful tasks while still performing well on safe tasks. The authors provide a framework to better understand when immunization is feasible and define what an immunized model is. Their empirical results show that the proposed algorithm is effective for both linear models and deep neural networks, enhancing the robustness of these models against harmful influences.'}, 'zh': {'title': '基于Hessian矩阵的模型免疫算法', 'desc': '本文提出了一种基于Hessian矩阵条件数的正则化算法，用于分析和实现模型免疫。模型免疫的目标是预训练模型，使其在有害任务上难以微调，同时在其他无害任务上保持有效性。尽管之前的研究已显示文本到图像模型的免疫效果，但免疫何时可能以及免疫模型的精确定义仍不清楚。通过实验证明，该算法在线性模型和深度网络上均有效，能够有效控制预训练后的条件数。'}}}, {'id': 'https://huggingface.co/papers/2506.08006', 'title': 'Dreamland: Controllable World Creation with Simulator and Generative\n  Models', 'url': 'https://huggingface.co/papers/2506.08006', 'abstract': 'Dreamland integrates physics simulators and large-scale pretrained generative models for controllable and photorealistic video world generation, improving image quality and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available.', 'score': 6, 'issue_id': 4210, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e9dd998a336df034', 'authors': ['Sicheng Mo', 'Ziyang Leng', 'Leon Liu', 'Weizhen Wang', 'Honglin He', 'Bolei Zhou'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08006.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#video', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Фотореалистичные видеомиры под полным контролем', 'desc': 'Dreamland - это новый подход к генерации видеомиров, объединяющий физические симуляторы и предобученные генеративные модели большого масштаба. Система использует многоуровневую абстракцию мира, кодирующую как пиксельную, так и объектную семантику и геометрию. Это позволяет улучшить контролируемость и фотореалистичность генерируемого контента. Эксперименты показывают, что Dreamland превосходит существующие методы по качеству изображения и степени контроля.'}, 'en': {'title': 'Dreamland: Bridging Physics and Generative Models for Enhanced Video World Creation', 'desc': 'Dreamland is a novel framework that combines physics simulators with large-scale pretrained generative models to create realistic and controllable video worlds. It addresses the limitations of existing video generative models by providing element-wise controllability, which is essential for editing scenes and training AI agents. The framework uses a layered world abstraction that captures both pixel-level and object-level details, allowing for better integration between the simulator and the generative model. Experiments show that Dreamland significantly improves image quality and controllability, making it a promising tool for future AI applications.'}, 'zh': {'title': '梦境：可控的真实视频世界生成', 'desc': 'Dreamland是一个结合物理模拟器和大规模预训练生成模型的混合世界生成框架。它通过设计分层世界抽象，编码像素级和物体级的语义与几何信息，从而提高了图像质量和可控性。该方法能够在生成过程中实现更细粒度的控制，减少适应成本，并支持现有和未来生成模型的使用。实验结果表明，Dreamland在图像质量和可控性方面均显著优于现有基线，具有提升智能体训练的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.05598', 'title': 'SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward\n  Models in LLMs', 'url': 'https://huggingface.co/papers/2506.05598', 'abstract': 'SynthesizeMe generates synthetic user personas from interactions for personalized reward modeling, improving LLM accuracy and achieving top performance on PersonalRewardBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent calls for pluralistic alignment of Large Language Models (LLMs) encourage adapting models to diverse user preferences. However, most prior work on personalized reward models heavily rely on additional identity information, such as demographic details or a predefined set of preference categories. To this end, we introduce SynthesizeMe, an approach to inducing synthetic user personas from user interactions for personalized reward modeling. SynthesizeMe first generates and verifies reasoning to explain user preferences, then induces synthetic user personas from that reasoning, and finally filters to informative prior user interactions in order to build personalized prompts for a particular user. We show that using SynthesizeMe induced prompts improves personalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining SynthesizeMe derived prompts with a reward model achieves top performance on PersonalRewardBench: a new curation of user-stratified interactions with chatbots collected from 854 users of Chatbot Arena and PRISM.', 'score': 6, 'issue_id': 4223, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c840a8d8856e1f61', 'authors': ['Michael J Ryan', 'Omar Shaikh', 'Aditri Bhagirath', 'Daniel Frees', 'William Held', 'Diyi Yang'], 'affiliations': ['Georgia Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05598.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#data', '#training', '#alignment', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Синтетические персоны для точной персонализации языковых моделей', 'desc': 'SynthesizeMe - это новый подход к созданию синтетических пользовательских персон для персонализированного моделирования вознаграждений в больших языковых моделях (LLM). Метод генерирует и проверяет рассуждения для объяснения предпочтений пользователей, создает синтетические персоны и фильтрует информативные взаимодействия для построения персонализированных промптов. Использование SynthesizeMe улучшает точность персонализированных LLM на 4,4% в Chatbot Arena. В сочетании с моделью вознаграждения, SynthesizeMe достигает лучших результатов на новом бенчмарке PersonalRewardBench.'}, 'en': {'title': 'Empowering LLMs with Synthetic User Personas for Personalization', 'desc': 'The paper presents SynthesizeMe, a novel method for creating synthetic user personas based on user interactions, aimed at enhancing personalized reward modeling for Large Language Models (LLMs). Unlike traditional approaches that depend on demographic data or fixed preference categories, SynthesizeMe generates user personas by analyzing and verifying reasoning behind user preferences. This method not only improves the accuracy of LLMs in personalized tasks but also achieves significant performance gains on the PersonalRewardBench benchmark. By leveraging these synthetic personas, the approach demonstrates a 4.4% increase in accuracy for LLMs acting as judges in user interactions.'}, 'zh': {'title': '合成用户角色，提升个性化奖励建模', 'desc': 'SynthesizeMe是一种通过用户交互生成合成用户角色的方法，用于个性化奖励建模。该方法首先生成并验证解释用户偏好的推理，然后从这些推理中诱导出合成用户角色，最后过滤出有用的用户交互，以构建特定用户的个性化提示。使用SynthesizeMe生成的提示可以提高个性化大型语言模型（LLM）作为评判者的准确性，提升幅度达到4.4%。结合SynthesizeMe生成的提示与奖励模型，在PersonalRewardBench上取得了最佳表现。'}}}, {'id': 'https://huggingface.co/papers/2506.01241', 'title': 'ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form\n  Generation Tasks with Structured Checklists', 'url': 'https://huggingface.co/papers/2506.01241', 'abstract': 'ExpertLongBench is a benchmark for expert-level workflows with diverse, long-form tasks, evaluated using CLEAR, a framework that assesses outputs based on domain-specific rubrics and checklists.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces ExpertLongBench, an expert-level benchmark containing 11 tasks from 9 domains that reflect realistic expert workflows and applications. Beyond question answering, the application-driven tasks in ExpertLongBench demand long-form outputs that can exceed 5,000 tokens and strict adherence to domain-specific requirements. Notably, each task in ExpertLongBench includes a rubric, designed or validated by domain experts, to specify task requirements and guide output evaluation. Furthermore, we propose CLEAR, an evaluation framework that supports accurate evaluation of long-form model outputs in our benchmark. To achieve fine-grained, expert-aligned evaluation, CLEAR derives checklists from both model outputs and references by extracting information corresponding to items in the task-specific rubric. Checklist items for model outputs are then compared with corresponding items for reference outputs to assess their correctness, enabling grounded evaluation. We benchmark 11 large language models (LLMs) and analyze components in CLEAR, showing that (1) existing LLMs, with the top performer achieving only a 26.8% F1 score, require significant improvement for expert-level tasks; (2) models can generate content corresponding to the required aspects, though often not accurately; and (3) accurate checklist extraction and comparison in CLEAR can be achieved by open-weight models for more scalable and low-cost usage.', 'score': 6, 'issue_id': 4222, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '9956d204f343761f', 'authors': ['Jie Ruan', 'Inderjeet Nair', 'Shuyang Cao', 'Amy Liu', 'Sheza Munir', 'Micah Pollens-Dempsey', 'Tiffany Chiang', 'Lucy Kates', 'Nicholas David', 'Sihan Chen', 'Ruxin Yang', 'Yuqian Yang', 'Jasmine Gump', 'Tessa Bialek', 'Vivek Sankaran', 'Margo Schlanger', 'Lu Wang'], 'affiliations': ['Biomedical Engineering, University of Michigan', 'Computer Science and Engineering, University of Michigan', 'Department of Chemistry, Carnegie Mellon University', 'Materials Science & Engineering, University of Michigan', 'School of Information, University of Michigan', 'University of Michigan Law School'], 'pdf_title_img': 'assets/pdf/title_img/2506.01241.jpg', 'data': {'categories': ['#optimization', '#data', '#benchmark', '#long_context', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'ExpertLongBench: новый стандарт оценки языковых моделей в экспертных задачах', 'desc': 'ExpertLongBench - это набор экспертных задач для оценки языковых моделей, включающий 11 заданий из 9 областей, требующих длинных ответов до 5000 токенов. Для оценки используется фреймворк CLEAR, который анализирует выводы моделей на основе экспертных рубрик и чек-листов. Тестирование 11 больших языковых моделей показало, что даже лучшая модель достигла лишь 26.8% F1-score, что говорит о необходимости значительного улучшения для экспертных задач. CLEAR позволяет проводить точную и масштабируемую оценку длинных ответов с низкими затратами.'}, 'en': {'title': 'ExpertLongBench: Elevating AI Evaluation for Expert Workflows', 'desc': 'This paper presents ExpertLongBench, a benchmark designed for evaluating expert-level workflows through 11 diverse tasks across 9 domains. The tasks require long-form outputs that can be extensive, exceeding 5,000 tokens, and must meet specific domain requirements. To assess these outputs, the authors introduce CLEAR, an evaluation framework that utilizes domain-specific rubrics and checklists for precise evaluation. The study benchmarks 11 large language models, revealing that current models struggle with expert-level tasks, highlighting the need for significant improvements in their performance.'}, 'zh': {'title': 'ExpertLongBench：专家级任务的评估新标准', 'desc': '本文介绍了ExpertLongBench，这是一个针对专家级工作流程的基准测试，包含来自9个领域的11个任务，反映了真实的专家应用。该基准测试的任务不仅限于问答，还要求生成超过5000个标记的长文本输出，并严格遵循领域特定的要求。每个任务都配有由领域专家设计或验证的评分标准，以明确任务要求并指导输出评估。此外，本文提出了CLEAR评估框架，支持对长文本模型输出的准确评估，确保评估过程的细致和与专家标准的一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.08011', 'title': 'Play to Generalize: Learning to Reason Through Game Play', 'url': 'https://huggingface.co/papers/2506.08011', 'abstract': "Post-training multimodal large language models with reinforcement learning on arcade-like games enhances their multimodal reasoning abilities without domain-specific data.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.", 'score': 5, 'issue_id': 4218, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '3ca46b3c756ab23e', 'authors': ['Yunfei Xie', 'Yinsong Ma', 'Shiyi Lan', 'Alan Yuille', 'Junfei Xiao', 'Chen Wei'], 'affiliations': ['Johns Hopkins University', 'NVIDIA', 'Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08011.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#rl', '#games', '#transfer_learning'], 'emoji': '🎮', 'ru': {'title': 'Игры как ключ к универсальному мышлению ИИ', 'desc': 'Исследователи предложили новый метод постобучения мультимодальных больших языковых моделей (MLLM) с помощью игр в стиле аркады. Этот подход, названный Visual Game Learning (ViGaL), использует обучение с подкреплением на простых играх для улучшения способностей моделей к мультимодальным рассуждениям. Эксперименты показали, что такое обучение значительно повышает производительность 7-миллиардной модели на различных мультимодальных тестах, включая математические и междисциплинарные задачи. Примечательно, что модель превзошла специализированные модели, настроенные на конкретные задачи мультимодальных рассуждений, сохранив при этом свою общую производительность.'}, 'en': {'title': 'Unlocking Reasoning Skills through Game Play', 'desc': 'This paper introduces a new method called Visual Game Learning (ViGaL) to improve the reasoning skills of multimodal large language models (MLLMs) by using arcade-like games. The authors demonstrate that by applying reinforcement learning to a 7B-parameter MLLM while it plays these games, the model can enhance its performance on complex multimodal tasks without needing specific training data. The results show that this approach allows the model to generalize its reasoning abilities effectively, outperforming specialized models that are trained on multimodal reasoning data. This suggests that engaging with simple, rule-based games can be a powerful way to develop transferable cognitive skills in MLLMs.'}, 'zh': {'title': '通过游戏提升多模态推理能力', 'desc': '这篇论文提出了一种新的后训练方法，称为视觉游戏学习（ViGaL），旨在提高多模态大语言模型（MLLMs）的推理能力。通过在简单的街机游戏上进行强化学习，模型能够在没有特定领域数据的情况下，增强其多模态推理能力。研究表明，经过这种训练的模型在多模态数学基准测试和跨学科问题上表现优异，超越了专门针对多模态推理数据调优的模型。该方法展示了合成、基于规则的游戏可以作为可控和可扩展的预训练任务，帮助MLLMs解锁可迁移的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.06485', 'title': 'What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge\n  Conflict on Large Language Models', 'url': 'https://huggingface.co/papers/2506.06485', 'abstract': "LLMs show varying performance under context-memory conflict, with conflicting knowledge impacting tasks that require knowledge utilization and model reliance on internal knowledge challenging to suppress.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.", 'score': 4, 'issue_id': 4222, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'd74da032a2f344f1', 'authors': ['Kaiser Sun', 'Fan Bai', 'Mark Dredze'], 'affiliations': ['Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.06485.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#hallucinations', '#dataset', '#long_context', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Конфликт знаний в LLM: когда контекст противоречит памяти', 'desc': 'Исследование анализирует поведение больших языковых моделей (LLM) в условиях конфликта между контекстной информацией и параметрическими знаниями модели. Авторы разработали диагностическую систему для оценки производительности LLM при таких конфликтах. Результаты показывают, что модели лучше работают при согласованности контекста и внутренних знаний, а также затрудняются полностью подавить свои внутренние знания даже при явных инструкциях. Исследование поднимает вопросы о валидности оценки моделей и необходимости учета конфликтов знаний при развертывании LLM.'}, 'en': {'title': 'Navigating Knowledge Conflicts in Large Language Models', 'desc': "This paper investigates how large language models (LLMs) perform when there is a conflict between contextual information and their internal knowledge. The authors introduce a framework to assess LLM behavior in situations where the context contradicts the model's learned knowledge. They find that while knowledge conflict does not significantly affect tasks that don't require knowledge, it does impact performance when alignment between context and knowledge is necessary. Additionally, the study shows that LLMs struggle to ignore their internal knowledge even when asked to do so, and providing explanations for conflicts can help models rely more on contextual information."}, 'zh': {'title': '理解上下文与记忆冲突对LLM的影响', 'desc': '大型语言模型（LLMs）在上下文与记忆冲突的情况下表现不一，冲突的知识会影响需要知识利用的任务。我们提出了一种诊断框架，系统评估LLM在上下文与参数知识不一致时的行为。研究发现，当上下文和参数知识一致时，模型的表现更好，而在不需要知识利用的任务中，知识冲突的影响较小。此外，模型即使在指示下也无法完全抑制其内部知识，这表明在部署LLM时需要考虑知识冲突的问题。'}}}, {'id': 'https://huggingface.co/papers/2506.06266', 'title': 'Cartridges: Lightweight and general-purpose long context representations\n  via self-study', 'url': 'https://huggingface.co/papers/2506.06266', 'abstract': "Cartridges trained with self-study replicate in-context learning functionality while reducing memory and increasing throughput, and enable longer context lengths and composability at inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.", 'score': 4, 'issue_id': 4211, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': '0d49d6022b9d6786', 'authors': ['Sabri Eyuboglu', 'Ryan Ehrlich', 'Simran Arora', 'Neel Guha', 'Dylan Zinsley', 'Emily Liu', 'Will Tennien', 'Atri Rudra', 'James Zou', 'Azalia Mirhoseini', 'Christopher Re'], 'affiliations': ['Caltech', 'Stanford University', 'University at Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2506.06266.jpg', 'data': {'categories': ['#long_context', '#data', '#inference', '#synthetic', '#training', '#optimization'], 'emoji': '💾', 'ru': {'title': 'Картриджи: эффективное обучение языковых моделей для работы с длинными контекстами', 'desc': "Статья представляет новый метод обучения языковых моделей, названный 'картриджами'. Картриджи, обученные с помощью техники 'самообучения', способны воспроизводить функциональность обучения в контексте, при этом значительно снижая потребление памяти и увеличивая пропускную способность. Этот подход позволяет работать с более длинными контекстами и обеспечивает возможность композиции картриджей во время вывода. Метод особенно эффективен для задач, требующих анализа больших текстовых корпусов, таких как кодовые базы или юридические документы."}, 'en': {'title': 'Efficient In-Context Learning with Cartridges', 'desc': "This paper introduces a novel approach called Cartridges, which are trained using a method called self-study to enhance in-context learning (ICL) in large language models. By creating a smaller key-value (KV) cache for each text corpus, Cartridges significantly reduce memory usage and increase processing speed during inference. The self-study training method involves generating synthetic conversations to improve the Cartridge's performance, allowing it to match ICL capabilities while being more efficient. The results show that Cartridges can handle longer context lengths and can be composed without the need for retraining, making them a powerful tool for various applications."}, 'zh': {'title': '自我学习：高效的上下文学习解决方案', 'desc': '本文提出了一种名为Cartridge的技术，通过自我学习训练小型KV缓存，以实现上下文学习功能，同时降低内存消耗并提高处理速度。传统的大型语言模型需要将整个文本语料放入上下文窗口，导致高昂的内存成本，而Cartridge通过离线训练缓存来解决这一问题。研究表明，使用自我学习的方法训练Cartridge，可以在保持与上下文学习相似的性能的同时，显著减少内存使用和提高吞吐量。最终，Cartridge不仅扩展了模型的有效上下文长度，还能在推理时进行组合，而无需重新训练。'}}}, {'id': 'https://huggingface.co/papers/2506.04651', 'title': 'Agents of Change: Self-Evolving LLM Agents for Strategic Planning', 'url': 'https://huggingface.co/papers/2506.04651', 'abstract': "LLM agents improve their strategic planning and adapt over time when placed in complex environments like Settlers of Catan, outperforming static baselines through self-evolving mechanisms and collaboration among specialized roles.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in LLMs have enabled their use as autonomous agents across a range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark a progression of LLM-based agents, from a simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agent's code. We introduce a multi-agent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agent's logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to game-playing agents, and demonstrating adaptive reasoning over multiple iterations.", 'score': 4, 'issue_id': 4224, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'b4947cd58776a704', 'authors': ['Nikolas Belle', 'Dakota Barnes', 'Alfonso Amayuelas', 'Ivan Bercovich', 'Xin Eric Wang', 'William Wang'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2506.04651.jpg', 'data': {'categories': ['#reasoning', '#agi', '#optimization', '#games', '#architecture', '#agents', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Самообучающиеся ИИ-агенты побеждают в стратегических играх', 'desc': "Исследование показывает, что агенты на основе больших языковых моделей (LLM) способны к самосовершенствованию в стратегически сложных средах, таких как игра 'Колонизаторы'. Авторы представляют мультиагентную архитектуру, где специализированные роли (Аналитик, Исследователь, Программист и Игрок) сотрудничают для итеративного анализа игрового процесса и адаптации стратегий. Результаты демонстрируют, что самоэволюционирующие агенты, особенно на базе моделей Claude 3.7 и GPT-4, превосходят статические базовые линии. Агенты показывают способность автономно адаптировать свои стратегии и демонстрировать адаптивное мышление на протяжении нескольких итераций."}, 'en': {'title': 'Empowering LLM Agents to Evolve and Strategize in Complex Games', 'desc': "This paper explores how large language model (LLM) agents can enhance their strategic planning abilities in complex environments, specifically using the board game Settlers of Catan. The authors introduce a multi-agent system where specialized roles work together to analyze gameplay, research strategies, and modify the agents' logic. By benchmarking LLM agents that can autonomously rewrite their own prompts and code, the study demonstrates that these self-evolving agents significantly outperform static counterparts. The findings highlight the potential of LLMs to adapt and improve over time through collaboration and iterative learning."}, 'zh': {'title': '自我进化的LLM代理：在复杂环境中的战略提升', 'desc': '本论文探讨了大型语言模型（LLM）代理在复杂环境中如何通过自我进化和角色协作来提高战略规划能力。我们使用开源的Catanatron框架，在《卡坦岛》游戏中评估了不同的LLM代理，从简单的游戏代理到能够自主重写提示和代码的系统。通过引入多代理架构，代理们在分析游戏、研究新策略和修改逻辑方面进行协作。研究结果表明，自我进化的代理在战略适应性和长期规划上显著优于静态基线。'}}}, {'id': 'https://huggingface.co/papers/2506.07982', 'title': 'τ^2-Bench: Evaluating Conversational Agents in a Dual-Control\n  Environment', 'url': 'https://huggingface.co/papers/2506.07982', 'abstract': 'tau²-bench introduces a dual-control benchmark for conversational AI agents in telecommunications, simulating user-agent collaboration and evaluating their performance in reasoning and coordination.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce tau^2-bench, with four key contributions:   1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication,   2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity,   3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity,   4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination.   In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, tau^2-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.', 'score': 3, 'issue_id': 4224, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'f5bec003306d8b3b', 'authors': ['Victor Barres', 'Honghua Dong', 'Soham Ray', 'Xujie Si', 'Karthik Narasimhan'], 'affiliations': ['Sierra', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.07982.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Двойной контроль: новый рубеж для разговорных ИИ-агентов', 'desc': 'Статья представляет tau²-bench - новый эталонный тест для оценки разговорных ИИ-агентов в сфере телекоммуникаций. Он моделирует среду с двойным контролем, где и агент, и пользователь могут активно взаимодействовать с общим миром через инструменты. Тест включает генератор задач, симулятор пользователя и детальный анализ производительности агента. Эксперименты показывают значительное снижение эффективности агентов при переходе от одиночного к двойному контролю, подчеркивая сложность координации действий с пользователем.'}, 'en': {'title': 'Empowering AI Agents through User Collaboration in Telecom', 'desc': "The paper introduces tau²-bench, a benchmark designed for evaluating conversational AI agents in telecommunications through a dual-control framework. Unlike traditional benchmarks that only involve AI agents, tau²-bench allows both users and agents to actively participate in a shared environment, modeled as a Dec-POMDP. This setup tests the agents' abilities in reasoning, coordination, and communication while interacting with users. The benchmark includes a task generator, a user simulator, and methods for detailed performance analysis, revealing significant challenges when agents operate in dual-control scenarios."}, 'zh': {'title': '双控制基准：提升对话式AI的协作能力', 'desc': 'tau²-bench 是一个针对电信领域对话式人工智能代理的双控制基准测试，模拟用户与代理的协作，并评估它们在推理和协调方面的表现。与现有的单控制环境不同，tau²-bench 允许用户积极参与，共同修改共享环境的状态。该基准测试包括一个新颖的电信双控制领域模型，使用 Dec-POMDP 进行建模，测试代理的协调和沟通能力。通过多种实验，结果显示在双控制环境中，代理的表现显著下降，突显了引导用户的挑战。'}}}, {'id': 'https://huggingface.co/papers/2506.07527', 'title': "Learning What Reinforcement Learning Can't: Interleaved Online\n  Fine-Tuning for Hardest Questions", 'url': 'https://huggingface.co/papers/2506.07527', 'abstract': "ReLIFT, a method combining reinforcement learning and supervised fine-tuning, enhances large language model reasoning by addressing limitations of RL through interleaved training, improving performance across benchmarks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, ReLIFT (Reinforcement Learning Interleaved with Online Fine-Tuning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.", 'score': 3, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '6978ee3d97028c45', 'authors': ['Lu Ma', 'Hao Liang', 'Meiyi Qiang', 'Lexiang Tang', 'Xiaochen Ma', 'Zhen Hao Wong', 'Junbo Niu', 'Chengyu Shen', 'Runming He', 'Bin Cui', 'Wentao Zhang'], 'affiliations': ['Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07527.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'ReLIFT: Улучшение рассуждений языковых моделей через комбинацию RL и SFT', 'desc': 'ReLIFT - это новый метод обучения больших языковых моделей (LLM), сочетающий обучение с подкреплением (RL) и супервизорную тонкую настройку (SFT). Он решает проблему ограниченности RL, чередуя его с SFT для приобретения новых знаний. ReLIFT улучшает способности модели к рассуждению, показывая прирост более 5.2 пунктов на различных бенчмарках. Метод эффективно использует всего 13% детальных обучающих данных, демонстрируя высокую масштабируемость.'}, 'en': {'title': 'ReLIFT: Enhancing LLM Reasoning with Smart Training Mix', 'desc': 'ReLIFT is a novel training method that combines reinforcement learning (RL) and supervised fine-tuning (SFT) to improve the reasoning capabilities of large language models (LLMs). It addresses the limitations of traditional RL by interleaving training sessions, allowing the model to learn from high-quality demonstration data when faced with difficult questions. This approach enables the model to acquire new knowledge and reasoning patterns, enhancing its performance on challenging tasks. The results show that ReLIFT significantly outperforms both standalone RL and SFT methods, achieving better results with less training data.'}, 'zh': {'title': 'ReLIFT：强化学习与微调的完美结合', 'desc': 'ReLIFT是一种结合强化学习和监督微调的方法，旨在提升大型语言模型的推理能力。该方法通过交替训练来解决强化学习的局限性，使模型在面对挑战性问题时能够有效地吸收新知识。研究表明，ReLIFT在五个竞争级基准测试中平均提高了超过5.2分，并且在使用仅13%的示范数据的情况下，超越了传统的强化学习和监督微调方法。此结果表明，ReLIFT能够克服强化学习的基本局限性，展现出巨大的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.06658', 'title': 'Self-Adapting Improvement Loops for Robotic Learning', 'url': 'https://huggingface.co/papers/2506.06658', 'abstract': 'SAIL, a self-adapting improvement loop, iteratively enhances a video model using self-produced data and pretrained models, achieving continuous performance improvement on novel robotic tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement.', 'score': 3, 'issue_id': 4224, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '0d830115e90c1ee8', 'authors': ['Calvin Luo', 'Zilai Zeng', 'Mingxi Jia', 'Yilun Du', 'Chen Sun'], 'affiliations': ['Brown University', 'Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06658.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#video', '#agents', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствующиеся роботы через обучение на собственном опыте', 'desc': 'SAIL (Self-Adapting Improvement Loop) - это метод итеративного улучшения видеомодели для решения новых робототехнических задач. Он использует самогенерируемые данные и предобученные модели для постоянного повышения производительности. SAIL применяет адаптацию с помощью предобученной на интернет-данных видеомодели и обучение на собственном опыте. Эксперименты показали, что SAIL позволяет непрерывно улучшать результаты на новых задачах, изначально не встречавшихся при обучении.'}, 'en': {'title': 'Continuous Self-Improvement for Robotic Video Models', 'desc': 'The paper introduces SAIL, a self-adapting improvement loop designed to enhance video models for robotic tasks. It allows models to iteratively learn from their own generated data and adapt using pretrained models, leading to continuous performance gains on new tasks. By leveraging self-collected behaviors and internet-scale video datasets, SAIL enables agents to improve their generalization capabilities over time. The results show that SAIL effectively boosts performance on various tasks, even when initial training data is limited or filtered.'}, 'zh': {'title': '自适应改进，持续提升机器人任务性能', 'desc': 'SAIL（自适应改进循环）是一种通过自我生成数据和预训练模型，迭代提升视频模型性能的方法。该方法旨在解决机器人任务中的泛化问题，使模型能够在未见过的任务上持续改进。通过利用互联网规模的预训练视频模型，SAIL能够从自我收集的行为中不断学习和适应。实验表明，SAIL在多次迭代中对新任务的性能有显著提升，且对自我收集经验的过滤和初始演示的质量具有较强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.07848', 'title': 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal\n  Interaction and Enhancement', 'url': 'https://huggingface.co/papers/2506.07848', 'abstract': 'PolyVivid is a multi-subject video customization framework that uses text-image fusion, 3D-RoPE enhancement, attention-inherited identity injection, and MLLM-based data processing to ensure identity consistency and realistic video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines.', 'score': 2, 'issue_id': 4214, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e91b69e6dac8c694', 'authors': ['Teng Hu', 'Zhentao Yu', 'Zhengguang Zhou', 'Jiangning Zhang', 'Yuan Zhou', 'Qinglin Lu', 'Ran Yi'], 'affiliations': ['Shanghai Jiao Tong University', 'Tencent Hunyuan', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07848.jpg', 'data': {'categories': ['#3d', '#video', '#open_source', '#multimodal', '#games', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'PolyVivid: Новый уровень кастомизации видео с множеством субъектов', 'desc': 'PolyVivid - это фреймворк для кастомизации видео с несколькими субъектами, использующий слияние текста и изображений, улучшение с помощью 3D-RoPE, внедрение идентичности на основе наследования внимания и обработку данных на основе MLLM. Система обеспечивает согласованность идентичности и реалистичную генерацию видео. PolyVivid позволяет осуществлять гибкую и согласованную по идентичности генерацию, используя модуль слияния текста и изображений на основе VLLM для точной привязки. Фреймворк также включает модуль улучшения на основе 3D-RoPE и модуль внедрения идентичности для сохранения идентичности и взаимодействия субъектов.'}, 'en': {'title': 'PolyVivid: Consistent Multi-Subject Video Generation Made Easy!', 'desc': 'PolyVivid is a framework designed for creating videos with multiple subjects while maintaining their identities and interactions. It uses advanced techniques like text-image fusion to accurately link visual identities with textual descriptions. The framework also incorporates a 3D-RoPE enhancement module to improve how text and image data interact, ensuring that identities remain consistent throughout the video. By employing a multi-level language model (MLLM) for data processing, PolyVivid enhances the quality of video generation, achieving better identity fidelity and realism compared to existing models.'}, 'zh': {'title': 'PolyVivid：多主体视频定制的新突破', 'desc': 'PolyVivid是一个多主体视频定制框架，利用文本-图像融合、3D-RoPE增强、注意力继承身份注入和基于MLLM的数据处理，确保身份一致性和真实的视频生成。该框架解决了现有视频生成模型在多主体定制中缺乏细粒度可控性的问题。通过设计基于VLLM的文本-图像融合模块，PolyVivid能够准确建立主体图像与文本实体之间的对应关系。实验结果表明，PolyVivid在身份保真度、视频真实感和主体对齐方面表现优越，超越了现有的开源和商业基线。'}}}, {'id': 'https://huggingface.co/papers/2506.07564', 'title': 'SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems', 'url': 'https://huggingface.co/papers/2506.07564', 'abstract': "SAFEFLOW secures autonomous agents by enforcing fine-grained information flow control and ensuring reliability in multi-agent, adversarial environments through transactional execution and secure scheduling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.", 'score': 2, 'issue_id': 4225, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'cf23e35fdd9c8078', 'authors': ['Peiran Li', 'Xinkai Zou', 'Zhuohang Wu', 'Ruifeng Li', 'Shuo Xing', 'Hanwen Zheng', 'Zhikai Hu', 'Yuping Wang', 'Haoxi Li', 'Qin Yuan', 'Yingmo Zhang', 'Zhengzhong Tu'], 'affiliations': ['Carnegie Mellon University', 'Columbia University', 'Meta', 'Texas A&M University', 'UC Irvine', 'UC San Diego', 'University of Michigan', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.07564.jpg', 'data': {'categories': ['#agents', '#benchmark', '#multimodal', '#security'], 'emoji': '🛡️', 'ru': {'title': 'SAFEFLOW: Надежные и безопасные автономные агенты для сложных сред', 'desc': 'SAFEFLOW - это новая система для создания надежных агентов на основе больших языковых моделей (LLM) и мультимодальных моделей. Она обеспечивает детальный контроль потоков информации, отслеживая происхождение, целостность и конфиденциальность данных между агентами, инструментами и средой. SAFEFLOW вводит транзакционное выполнение и безопасное планирование для обеспечения согласованности в многоагентных средах. Эксперименты показывают, что агенты на базе SAFEFLOW сохраняют высокую производительность и безопасность даже в неблагоприятных условиях.'}, 'en': {'title': 'Building Trustworthy Autonomous Agents with SAFEFLOW', 'desc': 'SAFEFLOW is a new framework designed to enhance the security and reliability of autonomous agents, particularly those using large language models (LLMs) and vision-language models (VLMs). It implements fine-grained information flow control (IFC) to monitor and protect the integrity, provenance, and confidentiality of data exchanged among agents and their environments. By enforcing security labels during reasoning, SAFEFLOW prevents harmful inputs from affecting decision-making processes. Additionally, it introduces mechanisms for transactional execution and secure scheduling to ensure consistency and resilience in multi-agent scenarios, even under adversarial conditions.'}, 'zh': {'title': 'SAFEFLOW：构建可信赖的自主代理框架', 'desc': 'SAFEFLOW 是一个新型协议框架，旨在为基于大语言模型（LLM）和视觉语言模型（VLM）的自主代理提供安全保障。它通过细粒度的信息流控制（IFC）来追踪数据的来源、完整性和机密性，从而防止不可信或对抗性输入影响决策。为了确保在多代理环境中的鲁棒性，SAFEFLOW 引入了事务执行、冲突解决和安全调度机制，保持代理之间的全局一致性。此外，SAFEFLOW 还通过写前日志、回滚和安全缓存等机制增强了对运行时错误和政策违规的抵御能力。'}}}, {'id': 'https://huggingface.co/papers/2506.07240', 'title': 'Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path\n  Lengths in LLMs', 'url': 'https://huggingface.co/papers/2506.07240', 'abstract': 'LLMs use progressive encoding and visualization to optimize the length of reasoning processes, enhancing accuracy and reducing inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model\'s internal "thinking" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model\'s planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this "overclocking" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.', 'score': 2, 'issue_id': 4211, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '25ea5847c07c7881', 'authors': ['Roy Eisenstadt', 'Itamar Zimerman', 'Lior Wolf'], 'affiliations': ['IBM Research', 'Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07240.jpg', 'data': {'categories': ['#inference', '#reasoning', '#optimization', '#training'], 'emoji': '⏱️', 'ru': {'title': 'Оптимизация рассуждений LLM: быстрее, точнее, эффективнее', 'desc': "Статья исследует механизмы, с помощью которых большие языковые модели (LLM) регулируют длину процесса рассуждений. Авторы демонстрируют, что LLM кодируют прогресс рассуждений и предлагают визуализацию в виде интерактивной полосы прогресса. Они манипулируют внутренним кодированием прогресса во время вывода, чтобы сократить ненужные шаги и генерировать более краткую цепочку мыслей. Эмпирические результаты показывают, что этот метод 'разгона' уменьшает излишние размышления, повышает точность ответов и сокращает задержку вывода."}, 'en': {'title': 'Optimizing Reasoning Length for Enhanced LLM Performance', 'desc': 'This paper discusses how large language models (LLMs) can improve their reasoning processes by optimizing the length of their internal thought stages. It highlights the importance of balancing the duration of reasoning: too short may miss complexities, while too long can lead to overthinking and inefficiency. The authors introduce a method for encoding reasoning progress and a visualization tool to better understand how LLMs plan their responses. By adjusting this internal encoding, they demonstrate that LLMs can enhance accuracy and speed during inference, leading to better performance overall.'}, 'zh': {'title': '优化推理长度，提升模型表现', 'desc': '本文探讨了大型语言模型（LLMs）在推理过程中如何优化思维长度，以提高准确性并减少推理时间。研究表明，推理阶段的长度对答案质量有重要影响，过短可能无法捕捉任务复杂性，过长则可能导致不必要的计算。我们引入了一种交互式进度条可视化，帮助揭示模型的思维动态，并通过操控内部进度编码来减少冗余步骤。实验证明，这种“超频”方法有效减轻了过度思考，提高了答案的准确性，并降低了推理延迟。'}}}, {'id': 'https://huggingface.co/papers/2506.07160', 'title': 'GeometryZero: Improving Geometry Solving for LLM with Group Contrastive\n  Policy Optimization', 'url': 'https://huggingface.co/papers/2506.07160', 'abstract': 'A new reinforcement learning framework, Group Contrastive Policy Optimization (GCPO), enhances geometric reasoning in large language models with judicious auxiliary constructions, outperforming existing methods on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks.', 'score': 2, 'issue_id': 4212, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '25d25cf0d2a1b0d5', 'authors': ['Yikun Wang', 'Yibin Wang', 'Dianyi Wang', 'Zimian Peng', 'Qipeng Guo', 'Dacheng Tao', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07160.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': '📐', 'ru': {'title': 'Умные вспомогательные построения для геометрических задач', 'desc': 'Новый фреймворк обучения с подкреплением, Group Contrastive Policy Optimization (GCPO), улучшает геометрические рассуждения в больших языковых моделях с помощью вспомогательных построений. GCPO использует групповое контрастное маскирование для адаптивного предоставления сигналов вознаграждения за вспомогательные построения. На основе GCPO разработано семейство моделей GeometryZero, которые эффективно определяют, когда применять вспомогательные построения. Эмпирическая оценка показывает, что GeometryZero превосходит базовые модели на популярных геометрических тестах.'}, 'en': {'title': 'Enhancing Geometric Reasoning with GCPO in Language Models', 'desc': 'The paper introduces a new reinforcement learning framework called Group Contrastive Policy Optimization (GCPO) that improves geometric reasoning in large language models. It addresses the limitations of existing methods that either underperform or require large models, which are costly to run. GCPO uses innovative techniques like Group Contrastive Masking to provide context-sensitive rewards for auxiliary constructions and a length reward to encourage longer reasoning processes. The results show that models developed using GCPO, named GeometryZero, significantly outperform previous benchmarks in geometric problem-solving tasks.'}, 'zh': {'title': '群体对比策略优化：提升几何推理的新方法', 'desc': '本文提出了一种新的强化学习框架，称为群体对比策略优化（GCPO），旨在增强大型语言模型在几何推理方面的能力。GCPO通过适应性地提供正负奖励信号，优化辅助构造的使用，从而克服了现有方法的局限性。我们还开发了GeometryZero模型系列，这些模型能够有效地结合辅助构造与稳健的几何推理。实验结果表明，GeometryZero在多个几何基准测试中表现优于现有方法，平均提升了4.29%。'}}}, {'id': 'https://huggingface.co/papers/2506.04807', 'title': 'MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character\n  Recognition with over 97K Categories', 'url': 'https://huggingface.co/papers/2506.04807', 'abstract': 'MegaHan97K, a large-scale dataset for recognizing over 97,000 Chinese characters, addresses the long-tail distribution problem and reveals new challenges in mega-category OCR.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.', 'score': 2, 'issue_id': 4212, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd9e2ab51265af386', 'authors': ['Yuyi Zhang', 'Yongxin Shi', 'Peirong Zhang', 'Yixin Zhao', 'Zhenhua Yang', 'Lianwen Jin'], 'affiliations': ['SCUT-Zhuhai Institute of Modern Industrial Innovation, Zhuhai, China', 'School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04807.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#synthetic'], 'emoji': '🈶', 'ru': {'title': 'MegaHan97K: Революция в распознавании китайских иероглифов', 'desc': 'MegaHan97K - это крупномасштабный датасет для распознавания более 97 000 китайских иероглифов. Он решает проблему распределения с длинным хвостом и раскрывает новые вызовы в оптическом распознавании символов (OCR) с мега-категориями. Датасет включает рукописные, исторические и синтетические подмножества, обеспечивая сбалансированные выборки по всем категориям. Эксперименты по бенчмаркингу выявляют такие проблемы, как повышенные требования к хранению, распознавание морфологически схожих символов и трудности обучения с нулевым выстрелом.'}, 'en': {'title': 'Unlocking the Future of Chinese Character Recognition with MegaHan97K', 'desc': 'MegaHan97K is a groundbreaking dataset designed for recognizing over 97,000 Chinese characters, addressing the challenges of mega-category Optical Character Recognition (OCR). This dataset is the first to align with the latest GB18030-2022 standard, significantly expanding the number of character categories available for training models. It tackles the long-tail distribution problem by offering balanced samples across three subsets: handwritten, historical, and synthetic. Additionally, it highlights new challenges in mega-category recognition, such as increased storage needs and difficulties in zero-shot learning, while paving the way for future advancements in the field.'}, 'zh': {'title': 'MegaHan97K：超大类汉字识别的新里程碑', 'desc': 'MegaHan97K是一个大规模的数据集，旨在识别超过97,000个汉字，解决了长尾分布问题，并揭示了超大类OCR的新挑战。该数据集支持最新的GB18030-2022标准，提供了比现有数据集多六倍的类别。通过手写、历史和合成三个子集，MegaHan97K有效地平衡了各类别的样本。我们的研究还揭示了在超大类场景中面临的新挑战，如存储需求增加、形态相似字符的识别和零样本学习的困难。'}}}, {'id': 'https://huggingface.co/papers/2506.03690', 'title': 'Robust Preference Optimization via Dynamic Target Margins', 'url': 'https://huggingface.co/papers/2506.03690', 'abstract': "The paper introduces γ-PO, a dynamic target margin preference optimization algorithm that enhances Large Language Models' alignment by adjusting reward margins at the pairwise level, leading to improved performance with minimal impact on training.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose gamma-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, gamma-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, gamma-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, gamma-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, gamma-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at https://github.com/sunjie279/gammaPO{https://github.com/sunjie279/gammaPO}.", 'score': 2, 'issue_id': 4208, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a3134e659a450a93', 'authors': ['Jie Sun', 'Junkang Wu', 'Jiancan Wu', 'Zhibo Zhu', 'Xingyu Lu', 'Jun Zhou', 'Lintao Ma', 'Xiang Wang'], 'affiliations': ['Ant Group', 'National University of Singapore', 'Shanghai Key Laboratory of Data Science'], 'pdf_title_img': 'assets/pdf/title_img/2506.03690.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#rlhf', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'γ-PO: Точная настройка больших языковых моделей через динамическую оптимизацию предпочтений', 'desc': 'Статья представляет γ-PO - алгоритм динамической оптимизации целевой маржи предпочтений для улучшения выравнивания больших языковых моделей (LLM). γ-PO корректирует маржу вознаграждения на попарном уровне, что позволяет приоритизировать высококачественные пары данных и подавлять потенциальный шум. Метод совместим с вариантами Direct Preference Optimization (DPO) и достигает в среднем 4.4% улучшения производительности на различных бенчмарках. γ-PO требует минимальных изменений в коде и не влияет на эффективность обучения, что делает его надежным решением для улучшения выравнивания LLM.'}, 'en': {'title': 'Enhancing LLM Alignment with Dynamic Margin Optimization', 'desc': 'The paper presents γ-PO, a novel algorithm designed to optimize the alignment of Large Language Models (LLMs) by dynamically adjusting reward margins at the pairwise level. This method enhances Direct Preference Optimization (DPO) by focusing on high-confidence preference pairs, which helps to mitigate the effects of noisy data. By implementing instance-specific margin calibration, γ-PO improves model performance while maintaining training efficiency with minimal code modifications. The results demonstrate an average improvement of 4.4% over existing baselines, establishing new benchmarks in LLM alignment.'}, 'zh': {'title': 'γ-PO：提升大型语言模型对齐性的动态优化算法', 'desc': '本文介绍了一种名为γ-PO的动态目标边际偏好优化算法，旨在通过调整成对的奖励边际来增强大型语言模型（LLMs）的对齐性。该算法通过实例特定的边际校准，优先考虑高置信度的成对数据，同时抑制模糊成对数据的潜在噪声。γ-PO与现有的直接偏好优化（DPO）方法兼容，能够在不显著影响训练效率的情况下，提升模型的性能。实验结果表明，γ-PO在多个基准测试中平均提高了4.4%的性能，设定了新的最先进的性能基准。'}}}, {'id': 'https://huggingface.co/papers/2506.08004', 'title': 'Dynamic View Synthesis as an Inverse Problem', 'url': 'https://huggingface.co/papers/2506.08004', 'abstract': 'Efficient dynamic view synthesis is achieved through noise initialization adjustments in a pre-trained video diffusion model using K-order Recursive Noise Representation and Stochastic Latent Modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we address dynamic view synthesis from monocular videos as an inverse problem in a training-free setting. By redesigning the noise initialization phase of a pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying a fundamental obstacle to deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and resolve it by introducing a novel noise representation, termed K-order Recursive Noise Representation. We derive a closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase.', 'score': 1, 'issue_id': 4226, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '2a88154b00f5112e', 'authors': ['Hidir Yesiltepe', 'Pinar Yanardag'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2506.08004.jpg', 'data': {'categories': ['#diffusion', '#video', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Динамический синтез видов через манипуляции шумом в латентном пространстве', 'desc': 'В этой статье представлен новый подход к динамическому синтезу видов из монокулярных видео без дополнительного обучения модели. Авторы модифицируют фазу инициализации шума в предобученной диффузионной модели видео, используя K-порядковое рекурсивное представление шума. Они также вводят стохастическую модуляцию латентного пространства для синтеза новых видимых областей при движении камеры. Эксперименты показывают эффективность предложенного метода для динамического синтеза видов путем структурированных манипуляций в латентном пространстве.'}, 'en': {'title': 'Revolutionizing Dynamic View Synthesis with Noise Innovations', 'desc': 'This paper presents a method for dynamic view synthesis from single videos without needing additional training. It modifies the noise initialization in a pre-trained video diffusion model to achieve high-quality results. The authors introduce K-order Recursive Noise Representation to overcome challenges related to signal-to-noise ratios during inversion. Additionally, they implement Stochastic Latent Modulation to sample latent spaces effectively, allowing for the synthesis of occluded areas as the camera moves.'}, 'zh': {'title': '高效动态视图合成的新方法', 'desc': '本文提出了一种高效的动态视图合成方法，利用预训练视频扩散模型中的噪声初始化调整。我们通过重新设计噪声初始化阶段，解决了确定性反演中的基本障碍，实现了无需权重更新的高保真动态视图合成。引入的K阶递归噪声表示法和随机潜在调制技术，使得在相机运动下能够合成新可见区域，完成被遮挡区域的填充。实验结果表明，通过在噪声初始化阶段进行结构化潜在操作，可以有效地实现动态视图合成。'}}}, {'id': 'https://huggingface.co/papers/2506.07971', 'title': 'CyberV: Cybernetics for Test-time Scaling in Video Understanding', 'url': 'https://huggingface.co/papers/2506.07971', 'abstract': 'CyberV enhances video multimodal large language models with adaptive feedback mechanisms, improving performance across various benchmarks without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose a novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces a cybernetic loop consisting of an MLLM Inference System, a Sensor, and a Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at https://github.com/marinero4972/CyberV.', 'score': 1, 'issue_id': 4222, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '9afc2828137b5533', 'authors': ['Jiahao Meng', 'Shuyang Sun', 'Yue Tan', 'Lu Qi', 'Yunhai Tong', 'Xiangtai Li', 'Longyin Wen'], 'affiliations': ['ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07971.jpg', 'data': {'categories': ['#video', '#optimization', '#multimodal', '#inference', '#interpretability'], 'emoji': '🎥', 'ru': {'title': 'CyberV: Адаптивное улучшение видео-ИИ без переобучения', 'desc': 'CyberV - это новая система, улучшающая работу мультимодальных языковых моделей с видео. Она вводит адаптивный механизм обратной связи, позволяющий моделям самокорректироваться во время вывода. Эксперименты показывают значительное повышение производительности на различных бенчмарках без необходимости переобучения моделей. CyberV особенно эффективен для моделей с меньшим количеством параметров, делая их более надежными и точными при анализе видео.'}, 'en': {'title': 'Adaptive Feedback for Enhanced Video Understanding', 'desc': "CyberV is a framework that enhances video multimodal large language models (MLLMs) by introducing adaptive feedback mechanisms. It allows these models to self-monitor and self-correct during inference, improving their ability to understand complex videos without needing retraining. By implementing a cybernetic loop with a sensor and controller, CyberV dynamically allocates resources and generates feedback based on the model's performance. This approach leads to significant performance boosts on various benchmarks, making MLLMs more robust and accurate in video understanding tasks."}, 'zh': {'title': '自适应反馈，提升视频理解能力', 'desc': 'CyberV 是一种增强视频多模态大语言模型的框架，通过自适应反馈机制来提高性能，而无需重新训练。该方法利用控制论原理，将视频 MLLM 设计为能够自我监控和自我修正的自适应系统。具体来说，系统通过传感器监测模型的前向过程，并收集中间解释，然后控制器决定何时触发自我修正并生成反馈。实验结果表明，CyberV 在多个基准测试中显著提高了模型的准确性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.07645', 'title': 'Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models', 'url': 'https://huggingface.co/papers/2506.07645', 'abstract': "Character and word-level attacks can effectively perturb large language models' predictions, especially in low-resource languages like Polish, indicating potential vulnerabilities in their safety mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research.", 'score': 1, 'issue_id': 4215, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e04bb2316fbf41bc', 'authors': ['Maciej Chrabąszcz', 'Katarzyna Lorenc', 'Karolina Seweryn'], 'affiliations': ['NASK - National Research Institute, Warsaw, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2506.07645.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#multilingual', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Уязвимости LLM в низкоресурсных языках: новый вызов для ИИ-безопасности', 'desc': 'Исследование показывает уязвимость больших языковых моделей (LLM) к атакам на уровне символов и слов, особенно для низкоресурсных языков вроде польского. Авторы демонстрируют, что даже небольшие изменения в тексте могут значительно повлиять на предсказания LLM. Это указывает на потенциальные проблемы в механизмах безопасности моделей для языков с ограниченными обучающими данными. Результаты исследования подчеркивают необходимость дополнительной оценки и улучшения устойчивости многоязычных LLM к подобным атакам.'}, 'en': {'title': 'Uncovering Vulnerabilities in Language Models: Small Changes, Big Impact!', 'desc': 'This paper explores the vulnerabilities of large language models (LLMs) to character and word-level attacks, particularly in low-resource languages like Polish. The authors demonstrate that minor alterations in text can significantly change the predictions made by these models, highlighting a critical safety concern. They utilize a small proxy model to assess word importance, enabling the creation of effective perturbations with minimal effort. The findings suggest that LLMs require more robust safety mechanisms, especially for languages that lack extensive training data.'}, 'zh': {'title': '揭示大型语言模型在低资源语言中的脆弱性', 'desc': '这篇论文探讨了大型语言模型（LLMs）在低资源语言（如波兰语）中的脆弱性。研究表明，通过简单地改变几个字符，可以有效地干扰这些模型的预测结果。由于安全相关的训练数据主要集中在高资源语言（如英语），这使得LLMs在低资源语言中更容易受到攻击。论文还展示了如何利用小型代理模型计算单词重要性，从而构建强大的攻击方法，并提供了相关数据集和代码供进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2506.00258', 'title': 'Hidden in Plain Sight: Probing Implicit Reasoning in Multimodal Language\n  Models', 'url': 'https://huggingface.co/papers/2506.00258', 'abstract': "Current multimodal large language models often fail to detect implicit reasoning flaws in messy, real-world inputs, but performance can be improved with cautious prompting and clarifying questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are increasingly deployed in open-ended, real-world environments where inputs are messy, underspecified, and not always trustworthy. Unlike curated benchmarks, these settings frequently involve instructions that refer to missing objects or contradictory facts, rely on ambiguous references, or request infeasible actions. In such cases, success hinges not on task execution alone, but on a model's ability to detect when something is silently wrong. This paper presents a systematic analysis of how current MLLMs handle such implicit reasoning scenarios: cases where the flaw is not explicitly stated but must be inferred from context. Using a curated diagnostic suite spanning four categories of real-world failure modes, we evaluate six MLLMs, including o3 and GPT-4o, and find that models frequently fail to surface hidden issues, even when they possess the necessary perceptual and reasoning skills. Explicit prompting reveals that the underlying capabilities exist but are often suppressed in favor of user compliance. We further show that simple inference-time interventions, such as cautious persona prompting and, in particular, requiring a clarifying question, can dramatically recover performance. Our findings highlight a persistent gap between reasoning competence and behavioral compliance in current MLLMs and suggest practical strategies for making these models more trustworthy in underconstrained environments.", 'score': 1, 'issue_id': 4225, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ceacd911b8d91616', 'authors': ['Qianqi Yan', 'Hongquan Li', 'Shan Jiang', 'Yang Zhao', 'Xinze Guan', 'Ching-Chen Kuo', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Cruz', 'eBay'], 'pdf_title_img': 'assets/pdf/title_img/2506.00258.jpg', 'data': {'categories': ['#inference', '#multimodal', '#reasoning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Повышение надежности ИИ: от слепого исполнения к критическому мышлению', 'desc': 'Статья исследует способность мультимодальных больших языковых моделей (MLLM) обнаруживать скрытые логические ошибки в реальных сценариях. Авторы обнаружили, что модели часто не выявляют неявные проблемы, даже обладая необходимыми навыками восприятия и рассуждения. Исследование показало, что простые вмешательства во время вывода, такие как осторожное персонализированное приглашение и требование уточняющих вопросов, могут значительно улучшить производительность. Результаты подчеркивают разрыв между способностью к рассуждениям и поведенческим соответствием в современных MLLM.'}, 'en': {'title': 'Enhancing MLLM Trustworthiness through Cautious Prompting', 'desc': "This paper investigates the limitations of multimodal large language models (MLLMs) in recognizing implicit reasoning flaws in complex, real-world scenarios. It highlights that these models often struggle with ambiguous inputs that require inference rather than explicit instructions. The study evaluates six MLLMs and finds that while they have the necessary reasoning capabilities, they often prioritize user compliance over accurate reasoning. The authors propose that using careful prompting and clarifying questions can significantly enhance the models' performance in detecting hidden issues."}, 'zh': {'title': '提升多模态模型的推理能力与可信度', 'desc': '当前的多模态大型语言模型（MLLMs）在处理复杂的现实输入时，常常无法发现隐含的推理缺陷。研究表明，通过谨慎的提示和澄清问题，可以显著提高模型的表现。我们对六种MLLMs进行了系统分析，发现它们在识别隐性问题时经常失败，即使它们具备必要的感知和推理能力。我们的研究结果强调了当前MLLMs在推理能力与行为合规性之间的差距，并提出了提高模型在不确定环境中可信度的实用策略。'}}}, {'id': 'https://huggingface.co/papers/2505.23473', 'title': 'EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and\n  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions', 'url': 'https://huggingface.co/papers/2505.23473', 'abstract': 'EVOREFUSE, an evolutionary algorithm, generates diverse pseudo-malicious instructions to optimize LLM refusal training, improving user experience without compromising safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.', 'score': 1, 'issue_id': 4212, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'dc6dd3911616cbf5', 'authors': ['Xiaorui Wu', 'Xiaofeng Mao', 'Xin Zhang', 'Fei Li', 'Chong Teng', 'Yuxiang Peng', 'Li Zheng', 'Donghong Ji', 'Zhuang Li'], 'affiliations': ['Ant Group', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China', 'School of Computing Technologies, Royal Melbourne Institute of Technology, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2505.23473.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#alignment', '#dataset', '#training', '#data'], 'emoji': '🧬', 'ru': {'title': 'Эволюция инструкций для умных отказов ИИ', 'desc': 'EVOREFUSE - это эволюционный алгоритм для генерации разнообразных псевдо-вредоносных инструкций, оптимизирующий обучение отказам в больших языковых моделях (LLM). Алгоритм создает наборы данных EVOREFUSE-TEST и EVOREFUSE-ALIGN, которые превосходят существующие бенчмарки по разнообразию и эффективности вызова отказов. Обучение на этих данных позволяет LLM снизить количество ложных отказов без ущерба безопасности. Анализ показывает, что модели часто вызывают избыточные отказы, фокусируясь на отдельных чувствительных словах и игнорируя более широкий контекст.'}, 'en': {'title': 'EVOREFUSE: Enhancing LLM Refusal Training with Evolutionary Algorithms', 'desc': 'EVOREFUSE is an innovative evolutionary algorithm designed to enhance the training of large language models (LLMs) by generating a variety of pseudo-malicious instructions. These instructions help to optimize refusal training, allowing LLMs to respond more effectively to potentially harmful queries without sacrificing user safety. By employing mutation strategies and recombination, EVOREFUSE explores the instruction space more thoroughly than traditional methods, resulting in a significant increase in the diversity and effectiveness of refusal-inducing prompts. The approach has led to the creation of two new datasets that improve LLM performance, reducing unnecessary refusals while maintaining safety standards.'}, 'zh': {'title': 'EVOREFUSE：优化LLM拒绝训练的进化算法', 'desc': 'EVOREFUSE是一种进化算法，旨在生成多样化的伪恶意指令，以优化大型语言模型（LLM）的拒绝训练，从而提升用户体验而不影响安全性。现有的指令策划方法如手动创建或重写指令，缺乏可扩展性或无法产生足够多样和有效的拒绝诱导提示。EVOREFUSE通过变异策略和重组探索指令空间，迭代演化种子指令，以最大化LLM拒绝概率的证据下界。使用EVOREFUSE，我们创建了两个新数据集，显著提高了拒绝触发率和响应信心分数，减少了过度拒绝的情况。'}}}, {'id': 'https://huggingface.co/papers/2506.07833', 'title': 'Improving large language models with concept-aware fine-tuning', 'url': 'https://huggingface.co/papers/2506.07833', 'abstract': 'Concept-Aware Fine-Tuning (CAFT) enhances large language models by enabling multi-token learning in the fine-tuning phase, leading to improved coherent understanding and better performance across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm', 'score': 0, 'issue_id': 4217, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '33b47665a34ee59a', 'authors': ['Michael K. Chen', 'Xikun Zhang', 'Jiaxing Huang', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.07833.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#agi', '#open_source', '#survey'], 'emoji': '🧠', 'ru': {'title': 'Революция в тонкой настройке: от токенов к концепциям', 'desc': 'Статья представляет новый метод тонкой настройки больших языковых моделей, называемый Concept-Aware Fine-Tuning (CAFT). CAFT позволяет обучать модели на уровне многотокенных последовательностей, что способствует более глубокому пониманию концепций. Эксперименты показывают значительное улучшение производительности по сравнению с традиционными методами тонкой настройки на различных задачах. Метод CAFT делает доступным многотокенное обучение на этапе пост-тренировки, что ранее было возможно только при предварительном обучении.'}, 'en': {'title': 'Unlocking Coherent Understanding with Multi-Token Learning', 'desc': 'Concept-Aware Fine-Tuning (CAFT) is a new approach that improves large language models (LLMs) by allowing them to learn from multiple tokens at once during the fine-tuning process. This method addresses the limitations of traditional next-token prediction, which often leads to a fragmented understanding of language. By enabling LLMs to grasp phrases as coherent units rather than separate parts, CAFT enhances their ability to understand complex concepts. Our experiments show that CAFT significantly outperforms standard fine-tuning methods in various tasks, making it a valuable advancement for AI development.'}, 'zh': {'title': '概念感知微调：提升语言模型的理解力', 'desc': '概念感知微调（CAFT）通过在微调阶段实现多标记学习，增强了大型语言模型的能力，从而提高了对各种任务的理解和表现。传统的下一个标记预测方法限制了模型形成连贯高层次概念的能力，导致理解和推理的障碍。CAFT方法允许模型学习跨越多个标记的序列，促进了更强的概念感知学习。实验结果表明，与传统的微调方法相比，CAFT在文本摘要和特定领域任务（如新蛋白质设计）上都有显著改善。'}}}, {'id': 'https://huggingface.co/papers/2506.06905', 'title': 'Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering', 'url': 'https://huggingface.co/papers/2506.06905', 'abstract': 'A meta-learning approach with soft prompts and an attention-mapper module improves few-shot capabilities in LMMs, outperforming ICL and related methods in visual question answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks.', 'score': 0, 'issue_id': 4220, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '63ff86432862b604', 'authors': ['Akash Gupta', 'Amos Storkey', 'Mirella Lapata'], 'affiliations': ['School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.06905.jpg', 'data': {'categories': ['#cv', '#optimization', '#benchmark', '#reasoning', '#architecture', '#training', '#multimodal', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Мета-обучение с мягкими промптами: новый уровень few-shot обучения для мультимодальных моделей', 'desc': 'Статья представляет новый метод мета-обучения для улучшения способностей больших мультимодальных моделей (LMM) к обучению по нескольким примерам. Авторы предлагают использовать мягкие промпты и модуль внимания для более эффективной адаптации к новым задачам. Этот подход превосходит обучение в контексте (ICL) и другие методы в задачах визуального ответа на вопросы. Метод показывает устойчивость даже при искажениях изображений, улучшая индукцию задач и рассуждения в условиях ограниченных данных.'}, 'en': {'title': 'Enhancing Few-Shot Learning in LMMs with Meta-Learning and Soft Prompts', 'desc': 'This paper presents a meta-learning strategy that enhances few-shot learning in Large Multimodal Models (LMMs) by utilizing soft prompts and an attention-mapper module. The authors argue that traditional in-context learning (ICL) struggles with smaller LMMs due to excessive information from image embeddings, which can hinder performance. Their proposed method distills relevant image features into fixed soft prompts that can be adapted during testing with minimal examples. The results demonstrate that this approach significantly outperforms ICL and other prompt-tuning methods in visual question answering tasks, even when faced with image variations.'}, 'zh': {'title': '元学习提升少样本能力，超越传统方法', 'desc': '本文提出了一种元学习方法，通过软提示和注意力映射模块来提升大型多模态模型（LMMs）在少样本学习中的能力。我们发现，传统的上下文学习（ICL）在小型LMMs中的表现不稳定，且随着示例数量的增加并不总是提高。为了解决这个问题，我们引入了一组固定的软提示，这些提示从与任务相关的图像特征中提取，并可以在测试时通过少量示例进行调整。我们的评估结果表明，该方法在视觉问答任务中优于ICL和相关的提示调优方法，能够在低数据环境下实现更好的任务适应和推理。'}}}, {'id': 'https://huggingface.co/papers/2506.05904', 'title': 'Proactive Assistant Dialogue Generation from Streaming Egocentric Videos', 'url': 'https://huggingface.co/papers/2506.05904', 'abstract': 'A framework provides automated data synthesis, evaluation metrics, and an end-to-end model for real-time, proactive conversational AI task guidance using streaming video inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in conversational AI have been substantial, but developing real-time systems for perceptual task guidance remains challenging. These systems must provide interactive, proactive assistance based on streaming visual inputs, yet their development is constrained by the costly and labor-intensive process of data collection and system evaluation. To address these limitations, we present a comprehensive framework with three key contributions. First, we introduce a novel data curation pipeline that synthesizes dialogues from annotated egocentric videos, resulting in \\dataset, a large-scale synthetic dialogue dataset spanning multiple domains. Second, we develop a suite of automatic evaluation metrics, validated through extensive human studies. Third, we propose an end-to-end model that processes streaming video inputs to generate contextually appropriate responses, incorporating novel techniques for handling data imbalance and long-duration videos. This work lays the foundation for developing real-time, proactive AI assistants capable of guiding users through diverse tasks. Project page: https://pro-assist.github.io/', 'score': 0, 'issue_id': 4214, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'a29f2576c90935c3', 'authors': ['Yichi Zhang', 'Xin Luna Dong', 'Zhaojiang Lin', 'Andrea Madotto', 'Anuj Kumar', 'Babak Damavandi', 'Joyce Chai', 'Seungwhan Moon'], 'affiliations': ['Meta', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.05904.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#long_context', '#data', '#synthetic', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Прорыв в создании проактивных ИИ-ассистентов для задач реального мира', 'desc': 'Статья представляет комплексную систему для разработки разговорного ИИ, способного в реальном времени давать указания по выполнению задач на основе видеопотока. Авторы предлагают автоматизированный конвейер для синтеза диалогов из аннотированных эгоцентрических видео, создавая крупномасштабный датасет. Разработан набор метрик для автоматической оценки, подтвержденных исследованиями с участием людей. Представлена модель, обрабатывающая потоковое видео для генерации контекстно-релевантных ответов с учетом дисбаланса данных и длительных видео.'}, 'en': {'title': 'Empowering Real-Time AI Guidance with Streaming Video Insights', 'desc': "This paper presents a framework for creating real-time conversational AI that can guide users through tasks using live video feeds. It introduces a new data curation pipeline that generates a large synthetic dialogue dataset from annotated egocentric videos, which helps in training the AI. The authors also propose automatic evaluation metrics that have been validated through human studies to assess the system's performance. Finally, they develop an end-to-end model that effectively processes streaming video inputs to provide timely and relevant responses, addressing challenges like data imbalance and long video durations."}, 'zh': {'title': '实时主动对话AI任务指导的创新框架', 'desc': '本文提出了一个框架，用于自动化数据合成、评估指标和端到端模型，以实现基于流媒体视频输入的实时主动对话AI任务指导。该框架的核心贡献包括：首先，开发了一种新颖的数据策划管道，从标注的自我中心视频中合成对话，生成了一个大规模的合成对话数据集。其次，提出了一套自动评估指标，并通过广泛的人类研究进行了验证。最后，构建了一个处理流媒体视频输入的端到端模型，能够生成上下文适当的响应，解决了数据不平衡和长时视频处理的问题。'}}}, {'id': 'https://huggingface.co/papers/2506.03231', 'title': 'NetPress: Dynamically Generated LLM Benchmarks for Network Applications', 'url': 'https://huggingface.co/papers/2506.03231', 'abstract': 'NetPress generates dynamic benchmarks for evaluating large language model agents in network operations, providing realistic tests across correctness, safety, and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.', 'score': 0, 'issue_id': 4228, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '42adee861aecb23e', 'authors': ['Yajie Zhou', 'Jiajun Ruan', 'Eric S. Wang', 'Sadjad Fouladi', 'Francis Y. Yan', 'Kevin Hsieh', 'Zaoxing Liu'], 'affiliations': ['Microsoft Research', 'University of Illinois Urbana-Champaign', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.03231.jpg', 'data': {'categories': ['#agents', '#survey', '#optimization', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'NetPress: Динамическая оценка LLM-агентов в сетевых операциях', 'desc': 'NetPress - это фреймворк для автоматизированной генерации бенчмарков, предназначенный для оценки агентов на основе больших языковых моделей (LLM) в сетевых приложениях. Он вводит унифицированную абстракцию с состоянием и действием, позволяя динамически генерировать разнообразные наборы запросов вместе с соответствующими эталонными данными. NetPress интегрируется с сетевыми эмуляторами для обеспечения реалистичной обратной связи, поддерживая комплексную оценку корректности, безопасности и задержки. Этот подход позволяет выявить тонкие различия в поведении агентов, которые часто упускаются при использовании статических бенчмарков.'}, 'en': {'title': 'Dynamic Benchmarking for Real-World LLM Evaluation', 'desc': 'NetPress is a framework designed to create dynamic benchmarks for assessing large language model (LLM) agents in network operations. It addresses the limitations of traditional static datasets by allowing users to generate millions of queries in real-time, tailored to specific benchmark configurations. The framework incorporates a unified abstraction of state and action, enabling comprehensive evaluations that consider correctness, safety, and latency. By integrating with network emulators, NetPress provides realistic feedback, revealing nuanced agent behaviors that static benchmarks often overlook, thus enhancing the reliability of LLMs in practical applications.'}, 'zh': {'title': 'NetPress：动态评估大型语言模型的基准生成工具', 'desc': 'NetPress是一个自动化基准生成框架，用于评估大型语言模型（LLM）在网络应用中的表现。它通过统一的状态和动作抽象，动态生成多样化的查询集及其对应的真实值。用户可以在运行时指定基准配置，实时生成数百万个查询。NetPress还与网络仿真器集成，提供真实环境反馈，支持在正确性、安全性和延迟等方面的全面评估。'}}}, {'id': 'https://huggingface.co/papers/2506.09600', 'title': 'Effective Red-Teaming of Policy-Adherent Agents', 'url': 'https://huggingface.co/papers/2506.09600', 'abstract': "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks", 'score': 32, 'issue_id': 4307, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '3de0c796f8d5a171', 'authors': ['Itay Nakash', 'George Kour', 'Koren Lazar', 'Matan Vetzler', 'Guy Uziel', 'Ateret Anaby-Tavor'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2506.09600.jpg', 'data': {'categories': ['#security', '#benchmark', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление защиты ЛЛМ-агентов от манипуляций пользователей', 'desc': 'Статья представляет CRAFT - многоагентную систему, использующую стратегии убеждения с учетом политик для тестирования устойчивости ЛЛМ-агентов в сфере обслуживания клиентов. Авторы предлагают новую модель угроз, фокусирующуюся на злоумышленниках, пытающихся эксплуатировать агентов в личных целях. CRAFT превосходит традиционные методы взлома, такие как DAN-промпты и эмоциональные манипуляции. Исследователи также представляют бенчмарк tau-break для оценки устойчивости агентов к манипулятивному поведению пользователей.'}, 'en': {'title': 'Strengthening Policy-Adherent Agents Against Adversarial Manipulation', 'desc': 'The paper introduces CRAFT, a multi-agent system designed to test and enhance the resilience of policy-adherent language model (LLM) agents in customer service against adversarial attacks. It highlights the challenge of ensuring these agents follow strict policies while still providing helpful interactions. The authors propose a new threat model that focuses on adversarial users who attempt to exploit these agents for personal gain. Additionally, they present tau-break, a benchmark for evaluating agent robustness, and discuss various defense strategies, revealing the need for more robust protections against manipulation.'}, 'zh': {'title': 'CRAFT：提升政策遵循代理的鲁棒性', 'desc': 'CRAFT是一个多智能体系统，使用政策意识的劝说策略，旨在挑战遵循政策的基于大语言模型的客户服务代理，以评估和提高其对对抗性攻击的鲁棒性。随着任务导向的LLM代理在严格政策领域的应用增加，确保代理始终遵循这些规则并适当地拒绝违规请求变得至关重要。为此，本文提出了一种新颖的威胁模型，专注于利用遵循政策的代理进行个人利益的对抗性用户。CRAFT通过利用政策意识的劝说策略，在客户服务场景中有效地削弱了遵循政策的代理，超越了传统的越狱方法。'}}}, {'id': 'https://huggingface.co/papers/2506.11924', 'title': 'Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation', 'url': 'https://huggingface.co/papers/2506.11924', 'abstract': 'A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.', 'score': 27, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'bf8d340f29d7ad95', 'authors': ['Min-Seop Kwak', 'Junho Kim', 'Sangdoo Yun', 'Dongyoon Han', 'Taekyoung Kim', 'Seungryong Kim', 'Jin-Hwa Kim'], 'affiliations': ['KAIST AI', 'NAVER AI Lab', 'SNU AIIS'], 'pdf_title_img': 'assets/pdf/title_img/2506.11924.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Диффузионная модель для согласованной генерации изображений и геометрии с новых ракурсов', 'desc': 'Эта статья представляет новую систему генерации изображений и геометрии с новых ракурсов, основанную на диффузионных моделях. Метод использует искажение и заполнение пробелов, а также дистилляцию внимания между модальностями для точного выравнивания генерируемых изображений и геометрии. Система применяет условное моделирование на основе близости для интеграции информации о глубине и нормалях. Результаты демонстрируют высококачественный синтез с новых ракурсов и полную трехмерную реконструкцию сцен.'}, 'en': {'title': 'High-Fidelity 3D View Synthesis through Diffusion and Attention', 'desc': 'This paper presents a diffusion-based framework for generating new views of images and their corresponding 3D geometry. It uses a technique called warping-and-inpainting, which allows for the synthesis of images and geometry without needing a lot of pre-existing data. The method incorporates cross-modal attention distillation to ensure that the generated images and geometries are well-aligned, enhancing the quality of the output. Additionally, it employs proximity-based mesh conditioning to improve the accuracy of the generated 3D structures, resulting in high-fidelity synthesis and completion of 3D scenes.'}, 'zh': {'title': '基于扩散的高保真图像与几何体生成', 'desc': '本文提出了一种基于扩散的框架，通过扭曲和修复的方法生成对齐的新视图图像和几何体。与以往需要密集姿态图像或限制于特定领域视图的生成模型不同，我们的方法利用现成的几何预测器来预测参考图像的部分几何体，并将新视图合成视为图像和几何体的修复任务。为了确保生成的图像和几何体之间的准确对齐，我们提出了跨模态注意力蒸馏，将图像扩散分支的注意力图注入到并行的几何扩散分支中。通过这种多任务方法，我们实现了几何稳健的图像合成和清晰的几何预测，最终在未见场景中实现了高保真度的视图合成。'}}}, {'id': 'https://huggingface.co/papers/2506.10892', 'title': 'The Diffusion Duality', 'url': 'https://huggingface.co/papers/2506.10892', 'abstract': 'Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code and model checkpoints on the project page: http://s-sahoo.github.io/duo', 'score': 23, 'issue_id': 4305, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '974b708b2e781af0', 'authors': ['Subham Sekhar Sahoo', 'Justin Deschenaux', 'Aaron Gokaslan', 'Guanghan Wang', 'Justin Chiu', 'Volodymyr Kuleshov'], 'affiliations': ['Computer and Information Science, Cornell Tech, NYC, USA', 'School of Computer and Communication Sciences, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2506.10892.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#optimization', '#open_source', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Duo: Ускорение диффузионных языковых моделей с помощью гауссовских техник', 'desc': 'Метод Duo улучшает дискретные диффузионные модели с равномерным состоянием, перенося техники из гауссовской диффузии. Он вводит стратегию курируемого обучения, управляемую гауссовским процессом, что удваивает скорость обучения за счет снижения дисперсии. Duo также представляет дискретную дистилляцию согласованности, адаптируя метод из непрерывной в дискретную среду. Это позволяет ускорить генерацию текста в диффузионных языковых моделях на два порядка.'}, 'en': {'title': 'Duo: Accelerating Diffusion Models for Fast Text Generation', 'desc': 'This paper presents Duo, a method that enhances uniform-state discrete diffusion models by incorporating techniques from Gaussian diffusion. The authors introduce a curriculum learning strategy that accelerates training speed by reducing variance, allowing models to outperform autoregressive models in zero-shot perplexity on several benchmarks. Additionally, they propose Discrete Consistency Distillation, which enables faster few-step text generation by adapting consistency distillation for discrete settings. Overall, Duo significantly improves the efficiency and performance of diffusion language models.'}, 'zh': {'title': 'Duo：加速文本生成的创新方法', 'desc': '本文提出了一种名为Duo的方法，旨在通过将高斯扩散的技术转移到均匀状态离散扩散模型中，从而提高训练速度和快速文本生成能力。均匀状态离散扩散模型具有自我纠正的能力，但通常在性能上不及自回归模型和掩蔽扩散模型。Duo通过引入基于高斯过程的课程学习策略，显著提高了训练速度，并在多个基准测试中超越了自回归模型。该方法还采用了离散一致性蒸馏技术，使得扩散语言模型能够实现快速的少步生成。'}}}, {'id': 'https://huggingface.co/papers/2506.10128', 'title': 'ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs', 'url': 'https://huggingface.co/papers/2506.10128', 'abstract': 'ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation. However, extending this success to visual perception in vision-language models (VLMs) has been impeded by the scarcity of vision-centric tasks that are simultaneously challenging and unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle, synthetic visual hallucination injected into paragraphs of human-written image captions. Starting from a 200-word captions, we inject a single, subtle visual description error-altering a few words on objects, attributes, counts, or spatial relations-and task the model to pinpoint the corrupted span given the image and the modified caption. This formulation preserves the full perceptual difficulty while providing a binary, exact-match reward that is easy to compute and unambiguous. Models trained with the ViCrit Task exhibit substantial gains across a variety of VL benchmarks. Crucially, the improvements transfer beyond natural-image training data to abstract image reasoning and visual math, showing promises of learning to perceive rather than barely memorizing seen objects. To facilitate evaluation, we further introduce ViCrit-Bench, a category-balanced diagnostic benchmark that systematically probes perception errors across diverse image domains and error types. Together, our results demonstrate that fine-grained hallucination criticism is an effective and generalizable objective for enhancing visual perception in VLMs.', 'score': 13, 'issue_id': 4309, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5045b62235ae5509', 'authors': ['Xiyao Wang', 'Zhengyuan Yang', 'Chao Feng', 'Yongyuan Liang', 'Yuhang Zhou', 'Xiaoyu Liu', 'Ziyi Zang', 'Ming Li', 'Chung-Ching Lin', 'Kevin Lin', 'Linjie Li', 'Furong Huang', 'Lijuan Wang'], 'affiliations': ['Cardiff University', 'Microsoft', 'University of Maryland, College Park', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2506.10128.jpg', 'data': {'categories': ['#rl', '#benchmark', '#hallucinations', '#cv', '#transfer_learning'], 'emoji': '🔍', 'ru': {'title': 'ViCrit: обучение мультимодальных моделей критическому восприятию визуальной информации', 'desc': 'Статья представляет ViCrit - задачу обучения с подкреплением для улучшения визуального восприятия мультимодальных моделей. ViCrit обучает модели обнаруживать тонкие искажения в подписях к изображениям, что позволяет повысить точность восприятия визуальной информации. Модели, обученные с помощью ViCrit, демонстрируют значительные улучшения в различных задачах компьютерного зрения. Полученные улучшения переносятся на новые домены, включая абстрактные изображения и визуальные математические задачи.'}, 'en': {'title': 'Enhancing Visual Perception in VLMs with ViCrit', 'desc': "ViCrit is a reinforcement learning task designed to enhance the visual perception capabilities of vision-language models (VLMs) by training them to identify subtle hallucinations in image captions. The task involves injecting minor visual description errors into human-written captions and challenging the model to locate these errors based on the corresponding images. This approach not only maintains the complexity of visual perception but also provides a clear and straightforward reward system for the model's performance. The results show that models trained with ViCrit achieve significant improvements across various visual benchmarks, indicating that this method fosters a deeper understanding of visual content rather than mere memorization."}, 'zh': {'title': '通过ViCrit提升视觉语言模型的感知能力', 'desc': 'ViCrit是一种强化学习任务，旨在微调视觉语言模型（VLMs），通过训练模型检测图像标题中的细微幻觉来提高视觉感知能力。该方法通过在人工撰写的图像标题中注入轻微的视觉描述错误，要求模型识别这些错误，从而保持感知的难度。经过ViCrit任务训练的模型在各种视觉语言基准测试中表现出显著的提升，且这些改进不仅限于自然图像数据，还能迁移到抽象图像推理和视觉数学等领域。我们的研究表明，细致的幻觉批评是一种有效且可推广的目标，有助于增强VLMs的视觉感知能力。'}}}, {'id': 'https://huggingface.co/papers/2506.11928', 'title': 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?', 'url': 'https://huggingface.co/papers/2506.11928', 'abstract': 'LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.', 'score': 11, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '4d3f2213d58dd8dd', 'authors': ['Zihan Zheng', 'Zerui Cheng', 'Zeyu Shen', 'Shang Zhou', 'Kaiyuan Liu', 'Hansen He', 'Dongruixuan Li', 'Stanley Wei', 'Hangyi Hao', 'Jianzhu Yao', 'Peiyao Sheng', 'Zixuan Wang', 'Wenhao Chai', 'Aleksandra Korolova', 'Peter Henderson', 'Sanjeev Arora', 'Pramod Viswanath', 'Jingbo Shang', 'Saining Xie'], 'affiliations': ['Canyon Crest Academy', 'McGill University', 'New York University', 'Princeton University', 'Sentient Foundation', 'University of California San Diego', 'University of Washington', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2506.11928.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#games'], 'emoji': '🤖', 'ru': {'title': 'LLM в программировании: сила в реализации, слабость в алгоритмах', 'desc': 'Исследование показывает, что крупные языковые модели (LLM) хорошо справляются с задачами по программированию, требующими сложной реализации, но испытывают трудности с тонким алгоритмическим мышлением. Для оценки этого был создан бенчмарк LiveCodeBench Pro, включающий задачи из Codeforces, ICPC и IOI. Анализ выявил, что лучшая модель достигает только 53% pass@1 на задачах средней сложности и 0% на сложных задачах без внешних инструментов. Исследование подчеркивает значительный разрыв между возможностями LLM и уровнем человека-гроссмейстера в программировании.'}, 'en': {'title': 'Bridging the Gap: LLMs vs. Human Algorithmic Mastery', 'desc': 'This paper evaluates the performance of large language models (LLMs) in competitive programming using a new benchmark called LiveCodeBench Pro. It reveals that while LLMs excel in implementation-heavy tasks, they struggle with complex algorithmic reasoning and nuanced problem-solving. The study shows that even the best LLMs achieve only 53% success on medium-difficulty problems and none on hard problems, indicating a significant gap compared to human experts. The findings suggest that LLMs rely more on implementation accuracy and external tools rather than advanced reasoning skills, highlighting areas for future improvement in AI-driven coding solutions.'}, 'zh': {'title': '大型语言模型在算法推理中的局限性', 'desc': '这篇论文探讨了大型语言模型（LLMs）在竞争编程中的表现，尤其是在实现密集型问题上表现良好，但在复杂算法推理方面存在不足。研究引入了LiveCodeBench Pro，这是一个基于Codeforces、ICPC和IOI的问题基准，旨在减少数据污染的可能性。通过对模型生成的提交进行逐行分析，发现当前的前沿模型在中等难度问题上的通过率仅为53%，而在困难问题上则为0%。这表明，尽管LLMs在实现精度上表现出色，但在复杂的算法推理和案例分析中仍然存在显著的局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.08989', 'title': 'SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.08989', 'abstract': "A self-aware problem synthesis framework that leverages model weaknesses enhances reinforcement learning with verifiable rewards, improving large language model performance on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.", 'score': 8, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '332357cc97416117', 'authors': ['Xiao Liang', 'Zhong-Zhi Li', 'Yeyun Gong', 'Yang Wang', 'Hengyuan Zhang', 'Yelong Shen', 'Ying Nian Wu', 'Weizhu Chen'], 'affiliations': ['Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.08989.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование ИИ через осознание собственных слабостей', 'desc': 'Статья представляет новый подход к обучению с подкреплением для больших языковых моделей, называемый Self-aware Weakness-driven problem Synthesis (SwS). Эта система способна выявлять слабые места модели и создавать новые задачи для их улучшения. SwS не требует внешних размеченных данных и позволяет модели самостоятельно определять и устранять свои недостатки. Результаты показывают значительное улучшение производительности на задачах рассуждения для моделей размером 7B и 32B параметров.'}, 'en': {'title': 'Empowering Models by Learning from Their Weaknesses', 'desc': 'This paper presents a Self-aware Weakness-driven problem Synthesis framework (SwS) that enhances reinforcement learning for large language models (LLMs) by focusing on their weaknesses. The framework identifies specific areas where the model struggles and generates new problems to help the model improve in those areas. By systematically augmenting the training set with these tailored problems, the model can better learn and generalize its reasoning capabilities. The results show significant performance improvements on reasoning tasks, demonstrating the effectiveness of leveraging model weaknesses for training.'}, 'zh': {'title': '自我意识驱动的强化学习问题合成', 'desc': '本文提出了一种自我意识的弱点驱动问题合成框架（SwS），旨在通过识别模型的不足来增强强化学习（RL）中的可验证奖励。该框架系统地分析模型在训练过程中反复失败的问题，并利用这些失败案例提炼核心概念，合成新的问题以加强模型的薄弱环节。通过这种方法，模型能够在后续的增强训练中集中精力克服自身的弱点，而无需依赖外部知识蒸馏。实验结果表明，该框架在多个推理基准测试中显著提高了模型的性能，平均提升达10.0%和7.7%。'}}}, {'id': 'https://huggingface.co/papers/2506.11930', 'title': 'Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback', 'url': 'https://huggingface.co/papers/2506.11930', 'abstract': "LLMs show resistance to feedback, termed feedback friction, even under ideal conditions, and sampling-based strategies only partially mitigate this issue.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.", 'score': 7, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '0fc67d5ee77483c7', 'authors': ['Dongwei Jiang', 'Alvin Zhang', 'Andrew Wang', 'Nicholas Andrews', 'Daniel Khashabi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.11930.jpg', 'data': {'categories': ['#training', '#hallucinations', '#alignment', '#reasoning', '#rlhf'], 'emoji': '🧠', 'ru': {'title': "Преодоление барьеров обучения: борьба с 'трением обратной связи' в LLM", 'desc': "Исследование показывает, что большие языковые модели (LLM) обладают сопротивлением к обратной связи, названным 'трением обратной связи', даже в идеальных условиях. Эксперименты проводились на различных задачах с использованием современных LLM, включая Claude 3.7. Стратегии на основе сэмплирования, такие как прогрессивное увеличение температуры, лишь частично смягчают эту проблему. Авторы надеются, что выявление этой проблемы поможет будущим исследованиям в области самосовершенствования LLM."}, 'en': {'title': 'Unpacking Feedback Friction in LLMs', 'desc': "This paper investigates the phenomenon of feedback friction in large language models (LLMs), where these models struggle to effectively incorporate external feedback even in ideal conditions. The authors conduct controlled experiments where a solver model receives targeted feedback from a feedback generator based on near-complete ground-truth answers. Despite this optimal setup, the models consistently show resistance to changing their incorrect responses, indicating a significant limitation in their learning process. The study also explores various strategies to mitigate this issue, such as sampling-based methods, but finds that these approaches only partially improve performance, highlighting the need for further research into enhancing LLMs' self-improvement capabilities."}, 'zh': {'title': '揭示大型语言模型的反馈摩擦问题', 'desc': '本研究探讨了大型语言模型（LLMs）在接收外部反馈时的表现，发现它们存在一种称为反馈摩擦（feedback friction）的现象，即使在理想条件下也难以有效整合反馈。我们设计了一个控制实验环境，评估模型在数学推理、知识推理和科学推理等多种任务中的反馈整合能力。尽管提供了接近完美的反馈，模型仍然表现出对反馈的抵抗，未能显著改善其错误答案。通过实验不同的采样策略，我们发现这些策略虽然有所改善，但仍未能使模型达到预期的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.11886', 'title': 'Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache', 'url': 'https://huggingface.co/papers/2506.11886', 'abstract': 'FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.', 'score': 6, 'issue_id': 4310, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '6e5e8424bfbe53cc', 'authors': ['Xiaoran Liu', 'Siyang He', 'Qiqi Wang', 'Ruixiao Li', 'Yuerong Song', 'Zhigeng Liu', 'Linlin Li', 'Qun Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['Huawei Noahs Ark Lab', 'School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.11886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#long_context', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие памяти LLM без потери точности', 'desc': 'FourierAttention - это фреймворк для повышения эффективности использования памяти в больших языковых моделях (LLM). Он сжимает малочувствительные к длинному контексту размерности головок трансформера, используя ортогональные базисы Фурье, сохраняя при этом точность модели. FourierAttention не требует дополнительного обучения и эксплуатирует различные роли размерностей головок трансформера: нижние размерности фокусируются на локальном контексте, а верхние захватывают зависимости на большом расстоянии. Оценки на моделях LLaMA показывают, что FourierAttention достигает лучшей точности для длинного контекста на бенчмарках LongBench и Needle-In-A-Haystack.'}, 'en': {'title': 'Enhancing Memory Efficiency in LLMs with FourierAttention', 'desc': 'FourierAttention is a novel framework designed to improve memory efficiency in Large Language Models (LLMs) without the need for training. It addresses the challenge of increasing memory demands from the Key-Value (KV) cache as context lengths grow. By using orthogonal Fourier bases, it compresses transformer head dimensions, allowing lower dimensions to focus on local context while higher dimensions capture long-range dependencies. Evaluations demonstrate that FourierAttention enhances long-context accuracy and includes a custom kernel, FlashFourierAttention, for optimized memory operations during deployment.'}, 'zh': {'title': 'FourierAttention：提升大型语言模型的内存效率', 'desc': 'FourierAttention是一种无需训练的框架，旨在提高大型语言模型的内存效率。它通过使用正交傅里叶基来压缩长上下文无关的变换头维度，同时保持模型的准确性。该方法利用变换头维度的异质性，低维度优先处理局部上下文，而高维度则捕捉长程依赖。评估结果表明，FourierAttention在长上下文准确性方面表现优异，且通过定制的Triton内核优化内存使用，确保高效部署。'}}}, {'id': 'https://huggingface.co/papers/2506.07464', 'title': 'DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO', 'url': 'https://huggingface.co/papers/2506.07464', 'abstract': 'DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks.', 'score': 5, 'issue_id': 4308, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'f8c207e9d26fe89e', 'authors': ['Jinyoung Park', 'Jeehye Na', 'Jinyoung Kim', 'Hyunwoo J. Kim'], 'affiliations': ['KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07464.jpg', 'data': {'categories': ['#rlhf', '#video', '#benchmark', '#optimization', '#reasoning', '#training', '#rl'], 'emoji': '🎥', 'ru': {'title': 'DeepVideo-R1: Улучшение рассуждений на видео с помощью регрессионного GRPO', 'desc': 'DeepVideo-R1 - это модель для улучшения рассуждений на основе видео, использующая регрессионный подход GRPO (Reg-GRPO) и адаптивное увеличение данных. Модель решает проблемы применения GRPO к видео-ЯБМ, такие как зависимость от защитных механизмов и проблема исчезающего преимущества. Reg-GRPO переформулирует задачу GRPO как регрессионную, напрямую предсказывая преимущество. Стратегия увеличения данных с учетом сложности динамически добавляет обучающие примеры на решаемых уровнях сложности.'}, 'en': {'title': 'Enhancing Video Reasoning with Reg-GRPO and Smart Data Augmentation', 'desc': 'DeepVideo-R1 is a novel approach that enhances video reasoning in large language models by utilizing a regression-based method called Reg-GRPO. This method reformulates the Group Relative Policy Optimization (GRPO) objective into a regression task, allowing for more direct policy guidance without the need for complex safeguards. Additionally, the paper introduces a difficulty-aware data augmentation strategy that adjusts training samples based on their solvable difficulty, which helps in generating diverse and informative reward signals. The results demonstrate that DeepVideo-R1 significantly boosts performance on various video reasoning benchmarks, showcasing its effectiveness in the field.'}, 'zh': {'title': 'DeepVideo-R1：提升视频推理的新方法', 'desc': 'DeepVideo-R1 是一种增强视频推理性能的模型，采用了回归型的 GRPO 方法和难度感知的数据增强策略。该研究探讨了 GRPO 在视频大语言模型中的应用，并识别出影响有效学习的两个主要问题：依赖保护措施和优势消失问题。为了解决这些挑战，DeepVideo-R1 通过回归 GRPO 重新构建了 GRPO 目标，直接预测优势值，从而简化了政策指导。实验结果表明，DeepVideo-R1 在多个视频推理基准测试中显著提高了视频推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.11997', 'title': 'pLSTM: parallelizable Linear Source Transition Mark networks', 'url': 'https://huggingface.co/papers/2506.11997', 'abstract': 'pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: https://github.com/ml-jku/plstm_experiments.', 'score': 4, 'issue_id': 4308, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '6dff119551b986fc', 'authors': ['Korbinian Pöppel', 'Richard Freinschlag', 'Thomas Schmied', 'Wei Lin', 'Sepp Hochreiter'], 'affiliations': ['Johannes Kepler University Linz'], 'pdf_title_img': 'assets/pdf/title_img/2506.11997.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#optimization', '#graphs', '#architecture', '#cv', '#open_source', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'pLSTM: Параллельные линейные RNN для эффективной обработки многомерных данных', 'desc': 'В статье представлена новая архитектура нейронной сети pLSTM (parallelizable Linear Source Transition Mark), разработанная для обработки направленных ациклических графов (DAG). pLSTM способна эффективно обрабатывать многомерные структуры данных, такие как изображения или молекулярные графы, преодолевая ограничения современных рекуррентных архитектур. Модель демонстрирует превосходную производительность на задачах с длинными зависимостями и превосходит трансформеры на ряде бенчмарков. pLSTM решает проблему затухающих/взрывающихся градиентов для длинных последовательностей в DAG с помощью двух режимов: направленного распространения и диффузного распределения.'}, 'en': {'title': 'pLSTMs: Revolutionizing Long-Range Learning in DAGs', 'desc': 'The paper introduces parallelizable Linear Source Transition Mark networks (pLSTMs), a new type of linear recurrent neural network (RNN) designed for processing data structured as directed acyclic graphs (DAGs). Unlike traditional RNNs and Transformers, pLSTMs can efficiently handle multi-dimensional data without being limited to sequential processing. They address the vanishing and exploding gradient problems through two modes of operation, allowing for effective long-range dependencies in data. The authors demonstrate that pLSTMs outperform Transformers on various benchmarks, particularly in tasks requiring long-distance extrapolation, such as computer vision and molecular graph analysis.'}, 'zh': {'title': 'pLSTMs：超越变换器的长距离学习新方法', 'desc': 'pLSTMs是一种可并行化的线性递归神经网络，专为有向无环图（DAG）设计，能够在长距离任务和基准测试中表现优于变换器（Transformers）。与现代递归架构相比，pLSTMs通过源、转移和标记门的设计，解决了长距离传播中的消失和爆炸梯度问题。该方法适用于更高结构的数据，如二维网格和树形结构，能够有效处理图像等多维数据。通过引入箭头指向外推的合成计算机视觉任务，pLSTMs展示了其在长距离推断中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2506.09427', 'title': 'A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation', 'url': 'https://huggingface.co/papers/2506.09427', 'abstract': "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy.   Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement.   Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems.", 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '5c1dd5f02a121213', 'authors': ['Yukang Feng', 'Jianwen Sun', 'Chuanhao Li', 'Zizhen Li', 'Jiaxin Ai', 'Fanrui Zhang', 'Yifan Chang', 'Sizhuo Zhou', 'Shenglin Zhang', 'Yu Dai', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09427.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#optimization', '#multimodal', '#games'], 'emoji': '🔄', 'ru': {'title': 'InterSyn: новый уровень мультимодального обучения', 'desc': 'Статья представляет InterSyn - крупномасштабный мультимодальный датасет для обучения языковых моделей. InterSyn создан с использованием метода самооценки с итеративным уточнением (SEIR) и содержит диалоги с тесно переплетенными изображениями и текстом. Авторы также представляют SynJudge - инструмент для автоматической оценки качества мультимодальных выходных данных. Эксперименты показывают, что обучение на InterSyn улучшает производительность мультимодальных моделей по всем метрикам оценки.'}, 'en': {'title': 'Enhancing Multimodal AI with InterSyn and SEIR', 'desc': 'The paper introduces InterSyn, a large-scale dataset designed to enhance multimodal understanding and generation in AI models. It utilizes the Self-Evaluation with Iterative Refinement (SEIR) method to create high-quality, tightly interleaved image-text outputs through multi-turn dialogues. Additionally, the paper presents SynJudge, an automatic evaluation tool that assesses multimodal outputs based on text content, image quality, and their synergy. Experimental results demonstrate that models trained on InterSyn show significant performance improvements across various evaluation metrics, highlighting its effectiveness for next-generation instruction-following models.'}, 'zh': {'title': 'InterSyn：提升多模态理解与生成的关键数据集', 'desc': 'InterSyn是一个大规模的数据集，旨在提高多模态理解和生成能力。它通过自我评估与迭代精炼（SEIR）方法构建，包含多轮指令驱动的对话和紧密交织的图像-文本输出。为了评估这些输出的质量，文章还介绍了SynJudge，一个自动评估工具，可以从文本内容、图像内容、图像质量和图像-文本协同四个维度进行量化评估。实验结果表明，使用SEIR方法构建的数据集质量显著提高，训练在InterSyn上的大型多模态模型在所有评估指标上均表现出一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.09366', 'title': 'SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending', 'url': 'https://huggingface.co/papers/2506.09366', 'abstract': 'SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.  \t\t\t\t\tAI-generated summary \t\t\t\t Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/.', 'score': 4, 'issue_id': 4305, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '411c39c85d7cabe0', 'authors': ['Yuxuan Kuang', 'Haoran Geng', 'Amine Elhafsi', 'Tan-Dzung Do', 'Pieter Abbeel', 'Jitendra Malik', 'Marco Pavone', 'Yue Wang'], 'affiliations': ['Peking University', 'Stanford University', 'University of California, Berkeley', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.09366.jpg', 'data': {'categories': ['#benchmark', '#robotics', '#rl', '#open_source', '#games'], 'emoji': '🤖', 'ru': {'title': 'SkillBlender: умное сочетание навыков для универсальных гуманоидных роботов', 'desc': 'SkillBlender - это новая иерархическая система обучения с подкреплением для универсального управления гуманоидными роботами. Она предварительно обучает примитивные навыки, а затем динамически комбинирует их для выполнения сложных задач локомоции и манипуляции. Авторы также представляют SkillBench - разнообразный симулированный бенчмарк для оценки таких систем. Эксперименты показывают, что SkillBlender значительно превосходит базовые методы, обеспечивая более точные и реалистичные движения роботов в повседневных сценариях.'}, 'en': {'title': 'Empowering Humanoid Robots with SkillBlender: Efficient Loco-Manipulation through Skill Blending', 'desc': 'SkillBlender is a hierarchical reinforcement learning framework designed to enhance the performance of humanoid robots in loco-manipulation tasks. It utilizes pretrained primitive skills that are goal-conditioned and task-agnostic, allowing for efficient blending of these skills to tackle complex tasks without extensive reward tuning. This approach not only improves the versatility of the robots but also ensures that their movements are accurate and feasible in real-world scenarios. Additionally, SkillBench provides a comprehensive benchmark for evaluating the performance of these skills across different robot embodiments and tasks, promoting further research in the field.'}, 'zh': {'title': 'SkillBlender：高效的人形机器人运动操控框架', 'desc': 'SkillBlender 是一个层次化的强化学习框架，利用预训练的基本技能高效解决人形机器人在多样化环境中的运动操控任务。该框架首先预训练与任务无关的目标导向基本技能，然后动态融合这些技能，以最小的任务特定奖励设计完成复杂的运动操控任务。通过引入 SkillBench，一个包含多种模拟环境和挑战性任务的基准测试，SkillBlender 提供了科学的评估指标，平衡了准确性和可行性。大量的模拟实验表明，SkillBlender 显著优于所有基线方法，能够自然地规范行为，避免奖励黑客行为，从而实现更准确和可行的运动。'}}}, {'id': 'https://huggingface.co/papers/2506.11474', 'title': 'Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards', 'url': 'https://huggingface.co/papers/2506.11474', 'abstract': 'Med-PRM enhances clinical decision making by verifying reasoning steps against medical knowledge bases, achieving state-of-the-art performance in medical QA benchmarks with improved accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/', 'score': 3, 'issue_id': 4314, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '123eb749b81758d8', 'authors': ['Jaehoon Yun', 'Jiwoong Sohn', 'Jungwoo Park', 'Hyunjae Kim', 'Xiangru Tang', 'Yanjun Shao', 'Yonghoe Koo', 'Minhyeok Ko', 'Qingyu Chen', 'Mark Gerstein', 'Michael Moor', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'ETH Zürich', 'Hanyang University College of Medicine', 'Korea University', 'University of Ulsan College of Medicine', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.11474.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#training', '#rag', '#reasoning', '#benchmark', '#science', '#small_models'], 'emoji': '🩺', 'ru': {'title': 'Точная диагностика с искусственным интеллектом: Med-PRM верифицирует каждый шаг', 'desc': 'Med-PRM - это новая система для улучшения клинического принятия решений, использующая большие языковые модели. Она проверяет каждый шаг рассуждения врача, сравнивая его с медицинскими базами знаний. Med-PRM показала лучшие результаты на пяти тестах по медицинским вопросам и ответам, улучшив точность базовых моделей на 13.5%. Система может быть легко интегрирована с другими сильными моделями, например Meerkat, достигая точности более 80% на тесте MedQA.'}, 'en': {'title': 'Enhancing Medical Decision Making with Verified Reasoning Steps', 'desc': 'Med-PRM is a new framework designed to improve clinical decision making by verifying reasoning steps against established medical knowledge bases. It addresses the challenge of error localization in large language models, which is crucial for accurate medical diagnoses. By using retrieval-augmented generation, Med-PRM can assess the quality of reasoning in a detailed manner, leading to enhanced accuracy in medical question answering tasks. The framework has shown significant performance improvements, achieving state-of-the-art results on multiple benchmarks and demonstrating its versatility with various policy models.'}, 'zh': {'title': 'Med-PRM：提升医疗决策的智能助手', 'desc': 'Med-PRM是一种过程奖励建模框架，旨在通过对医疗知识库的验证来增强临床决策能力。该模型利用检索增强生成技术，逐步验证推理过程中的每一步，确保推理的准确性。通过对中间推理步骤进行验证，Med-PRM能够细致地评估推理质量，从而提高医疗问答基准的表现。实验结果显示，Med-PRM在多个医疗QA基准上达到了最先进的性能，显著提升了基础模型的准确率。'}}}, {'id': 'https://huggingface.co/papers/2506.11274', 'title': 'Learning a Continue-Thinking Token for Enhanced Test-Time Scaling', 'url': 'https://huggingface.co/papers/2506.11274', 'abstract': 'A continuous thinking token learned via reinforcement learning improves language model accuracy more effectively than a fixed token during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "</think>" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model\'s accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.', 'score': 3, 'issue_id': 4313, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '770628ec96646ceb', 'authors': ['Liran Ringel', 'Elad Tolochinsky', 'Yaniv Romano'], 'affiliations': ['Department of Computer Science, Technion Israel Institute of Technology', 'Department of Electrical and Computer Engineering, Technion Israel Institute of Technology', 'Independent Researcher'], 'pdf_title_img': 'assets/pdf/title_img/2506.11274.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#inference', '#training', '#math', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Обучение токена продолжения мысли повышает точность языковых моделей', 'desc': 'Статья описывает метод улучшения языковых моделей с помощью обучения специального токена для продолжения рассуждений. Авторы используют обучение с подкреплением для оптимизации эмбеддинга этого токена, оставляя веса модели неизменными. Эксперименты показывают, что такой подход превосходит как базовую модель, так и метод с фиксированным токеном для продления вычислений. На бенчмарке GSM8K новый метод дает улучшение точности на 4.2% по сравнению с базовой моделью.'}, 'en': {'title': 'Learned Thinking Token Boosts Language Model Accuracy!', 'desc': 'This paper presents a novel approach to enhance language model performance by introducing a learned continuous thinking token through reinforcement learning. Unlike fixed tokens that merely extend reasoning time, the learned token adapts dynamically to improve accuracy during inference. The authors demonstrate that this method outperforms both the baseline model and traditional fixed-token strategies on standard math benchmarks. Notably, their approach achieves a significant accuracy boost, particularly on the GSM8K benchmark, showcasing the effectiveness of learning over static methods.'}, 'zh': {'title': '学习连续思考标记，提升语言模型准确性', 'desc': '本文探讨了一种通过强化学习学习的连续思考标记，如何在推理过程中比固定标记更有效地提高语言模型的准确性。研究表明，使用专门的连续思考标记可以触发更长的推理步骤，从而提升模型性能。我们在DeepSeek-R1的精简版本中加入了一个学习到的"<|continue-thinking|>"标记，并通过强化学习训练其嵌入，同时保持模型权重不变。实验结果显示，学习到的标记在标准数学基准测试中，相比于基线模型和使用固定标记的测试时间扩展方法，取得了显著的准确性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.08592', 'title': 'Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings', 'url': 'https://huggingface.co/papers/2506.08592', 'abstract': 'A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.', 'score': 3, 'issue_id': 4307, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '64e56d52fd4bf03d', 'authors': ['Liyan Xu', 'Zhenlin Su', 'Mo Yu', 'Jiangnan Li', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08592.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#open_source', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Точность в деталях: новый взгляд на возможности текстовых энкодеров', 'desc': "Представлен новый датасет CapRetrieval для оценки способности текстовых энкодеров распознавать мелкие сущности и события. Исследование выявило ограничения плотного поиска даже в простых случаях. Авторы предложили стратегии дообучения энкодеров для улучшения результатов на CapRetrieval. Также была обнаружена проблема 'дилеммы гранулярности' - сложность одновременного выражения мелких деталей и общей семантики в эмбеддингах."}, 'en': {'title': 'Enhancing Fine-Grained Entity Recognition in Text Encoders', 'desc': 'This paper introduces a new dataset called CapRetrieval, designed to test how well text encoders can identify detailed entities and events in text. The authors highlight a common problem where these encoders struggle with fine-grained retrieval, even in straightforward scenarios. Through zero-shot evaluation, they demonstrate that existing models often fail to match fine details, regardless of their size or training data. To improve performance, they propose data generation strategies for fine-tuning encoders, addressing the challenge of balancing detailed recognition with overall semantic understanding.'}, 'zh': {'title': '提升文本编码器的细粒度识别能力', 'desc': '本文介绍了一个新的数据集CapRetrieval，用于评估文本编码器识别细粒度实体和事件的能力。研究发现，现有的文本编码器在密集检索任务中存在局限性，无法有效识别语义中的细粒度信息。通过零样本评估，发现无论模型大小或训练来源，编码器在细粒度匹配上都可能失败。为了解决这个问题，本文提出了数据生成策略来微调编码器，从而在CapRetrieval上获得最佳性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08477', 'title': 'Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning', 'url': 'https://huggingface.co/papers/2506.08477', 'abstract': "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting harmful memes is essential for maintaining the integrity of online environments. However, current approaches often struggle with resource efficiency, flexibility, or explainability, limiting their practical deployment in content moderation systems. To address these challenges, we introduce U-CoT+, a novel framework for harmful meme detection. Instead of relying solely on prompting or fine-tuning multimodal models, we first develop a high-fidelity meme-to-text pipeline that converts visual memes into detail-preserving textual descriptions. This design decouples meme interpretation from meme classification, thus avoiding immediate reasoning over complex raw visual content and enabling resource-efficient harmful meme detection with general large language models (LLMs). Building on these textual descriptions, we further incorporate targeted, interpretable human-crafted guidelines to guide models' reasoning under zero-shot CoT prompting. As such, this framework allows for easy adaptation to different harmfulness detection criteria across platforms, regions, and over time, offering high flexibility and explainability. Extensive experiments on seven benchmark datasets validate the effectiveness of our framework, highlighting its potential for explainable and low-resource harmful meme detection using small-scale LLMs. Codes and data are available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", 'score': 3, 'issue_id': 4306, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '4b240f248019e671', 'authors': ['Fengjun Pan', 'Anh Tuan Luu', 'Xiaobao Wu'], 'affiliations': ['Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2506.08477.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#ethics', '#interpretability', '#small_models', '#multimodal', '#benchmark'], 'emoji': '🕵️', 'ru': {'title': 'Умный детектив для вредных мемов', 'desc': 'U-CoT+ - это новый фреймворк для обнаружения вредоносных мемов, который преобразует их в текстовые описания. Он использует специально разработанные человеком инструкции и промптинг с нулевым выстрелом для достижения высокой гибкости и объяснимости с помощью малых языковых моделей. Фреймворк отделяет интерпретацию мемов от их классификации, что позволяет эффективно использовать ресурсы. Эксперименты на семи эталонных наборах данных подтверждают эффективность этого подхода для объяснимого обнаружения вредоносных мемов с ограниченными ресурсами.'}, 'en': {'title': 'Transforming Memes into Text for Smarter Detection', 'desc': 'U-CoT+ is a new framework designed to detect harmful memes by transforming them into textual descriptions. This approach uses a meme-to-text pipeline that preserves details, allowing for better interpretation without needing complex visual analysis. By applying human-crafted guidelines with zero-shot Chain of Thought (CoT) prompting, the framework enhances flexibility and explainability in the detection process. The effectiveness of U-CoT+ is demonstrated through extensive experiments on various benchmark datasets, showcasing its potential for efficient and interpretable meme moderation using small-scale large language models (LLMs).'}, 'zh': {'title': 'U-CoT+: 高效可解释的有害迷因检测框架', 'desc': 'U-CoT+是一个新颖的框架，用于检测有害的网络迷因。它通过将迷因转换为文本描述，并使用人类设计的指导原则，结合零-shot链式推理，来实现高灵活性和可解释性。该框架避免了对复杂视觉内容的直接推理，从而提高了资源效率。实验结果表明，U-CoT+在小规模大语言模型上实现了有效的有害迷因检测。'}}}, {'id': 'https://huggingface.co/papers/2506.11136', 'title': 'JAFAR: Jack up Any Feature at Any Resolution', 'url': 'https://huggingface.co/papers/2506.11136', 'abstract': 'JAFAR is a lightweight feature upsampler using an attention-based module with Spatial Feature Transform modulation, enabling high-resolution features from Foundation Vision Encoders without high-resolution supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io', 'score': 2, 'issue_id': 4311, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'ba6fb6c9e607162e', 'authors': ['Paul Couairon', 'Loick Chambon', 'Louis Serrano', 'Jean-Emmanuel Haugeard', 'Matthieu Cord', 'Nicolas Thome'], 'affiliations': ['Sorbonne Université, CNRS, ISIR, F-75005 Paris, France', 'Thales, TSGF, cortAIx Labs, France', 'Valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2506.11136.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'JAFAR: Умное повышение разрешения без высокоразрешающего обучения', 'desc': 'JAFAR - это легковесный модуль для повышения разрешения признаков, использующий механизм внимания и модуляцию Spatial Feature Transform. Он позволяет получать высокоразрешающие признаки из базовых энкодеров компьютерного зрения без необходимости в высокоразрешающем обучении. JAFAR эффективно восстанавливает мелкие пространственные детали и превосходит существующие методы повышения разрешения признаков в различных задачах. Несмотря на обучение при низких коэффициентах увеличения, модель хорошо обобщается на значительно более высокие выходные масштабы.'}, 'en': {'title': 'JAFAR: Elevating Vision Features with Attention and Modulation', 'desc': 'JAFAR is a novel feature upsampler that enhances the spatial resolution of visual features from Foundation Vision Encoders without requiring high-resolution supervision. It utilizes an attention-based module with Spatial Feature Transform modulation to align high-resolution queries with low-resolution keys, improving semantic coherence. The model is lightweight and flexible, allowing it to upscale features to any desired resolution effectively. Experimental results indicate that JAFAR excels in recovering fine details and outperforms existing methods in various dense vision tasks.'}, 'zh': {'title': 'JAFAR：轻量级特征上采样的新选择', 'desc': 'JAFAR是一种轻量级的特征上采样器，利用基于注意力的模块和空间特征变换调制，能够从基础视觉编码器中生成高分辨率特征，而无需高分辨率的监督。该方法通过增强低级图像特征与语义丰富的低分辨率键之间的语义对齐，来提升视觉特征的空间分辨率。尽管没有高分辨率的监督，JAFAR在低上采样比率和分辨率下的学习表现出色，能够很好地推广到更高的输出尺度。实验结果表明，JAFAR在恢复细粒度空间细节方面表现优异，并在多种下游任务中超越了现有的特征上采样方法。'}}}, {'id': 'https://huggingface.co/papers/2506.11702', 'title': 'Configurable Preference Tuning with Rubric-Guided Synthetic Data', 'url': 'https://huggingface.co/papers/2506.11702', 'abstract': 'Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning', 'score': 1, 'issue_id': 4305, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': '7a7eb1af4ef17eef', 'authors': ['Víctor Gallego'], 'affiliations': ['Komorebi AI Technologies, Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2506.11702.jpg', 'data': {'categories': ['#rlhf', '#synthetic', '#training', '#dataset', '#alignment', '#open_source'], 'emoji': '🎛️', 'ru': {'title': 'Гибкая настройка языковых моделей под меняющиеся предпочтения пользователей', 'desc': 'Эта статья представляет новый подход под названием Configurable Preference Tuning (CPT) для настройки языковых моделей. CPT позволяет динамически корректировать поведение моделей на основе явных, понятных человеку директив. Метод использует синтетически сгенерированные данные о предпочтениях, основанные на структурированных рубриках, определяющих желаемые атрибуты. Такой подход обеспечивает тонкую настройку и моделирование более нюансированной обратной связи от человека.'}, 'en': {'title': 'Dynamic Adaptation of Language Models with Configurable Preference Tuning', 'desc': "This paper introduces Configurable Preference Tuning (CPT), a new method that allows language models to adapt their responses based on clear, human-understandable instructions. Unlike traditional models that rely on a fixed set of preferences, CPT uses dynamically generated preference data to fine-tune the model's behavior. By employing structured rubrics that specify desired traits, the model can adjust its outputs in real-time without needing to be retrained. This innovation enhances the model's ability to respond to complex and varied human feedback, making it more flexible and context-aware."}, 'zh': {'title': '动态调整语言模型行为的可配置偏好调优', 'desc': '可配置偏好调优（CPT）是一种新框架，使语言模型能够根据人类可理解的指令动态调整其行为。与传统的直接偏好优化（DPO）方法不同，CPT允许模型使用合成生成的偏好数据进行微调，从而在推理时根据系统提示调节输出。通过这种方式，模型能够在不重新训练的情况下，响应不同的上下文和需求。该方法不仅提供了更细致的控制，还能更好地模拟复杂和依赖上下文的人类反馈。'}}}, {'id': 'https://huggingface.co/papers/2506.10082', 'title': 'LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.10082', 'abstract': 'A mask-based LoRA tuning method for video editing adapts pretrained Image-to-Video models for flexible and high-quality video editing, using spatial masks and reference images for context-specific adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '14126cd5898da5a7', 'authors': ['Chenjian Gao', 'Lihe Ding', 'Xin Cai', 'Zhanpeng Huang', 'Zibin Wang', 'Tianfan Xue'], 'affiliations': ['SenseTime Research', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.10082.jpg', 'data': {'categories': ['#multimodal', '#games', '#diffusion', '#video', '#optimization', '#training'], 'emoji': '🎬', 'ru': {'title': 'Гибкое редактирование видео с помощью масок и LoRA', 'desc': 'Предложен метод настройки LoRA на основе масок для редактирования видео, адаптирующий предобученные модели Image-to-Video. Подход сохраняет фоновые области, позволяя контролировать распространение изменений. Используются дополнительные референсы в виде альтернативных ракурсов или репрезентативных состояний сцены. Маска позволяет модели динамически фокусироваться на нужных областях, извлекая информацию из соответствующих источников.'}, 'en': {'title': 'Flexible Video Editing with Mask-Based LoRA Tuning', 'desc': 'This paper presents a novel mask-based Low-Rank Adaptation (LoRA) tuning method for enhancing video editing capabilities using pretrained Image-to-Video models. The approach allows for flexible and high-quality edits by utilizing spatial masks and reference images, which provide context-specific guidance during the editing process. By preserving background regions and enabling controlled propagation of edits, the method improves upon existing techniques that often lack adaptability for subsequent frames. Experimental results demonstrate that this method outperforms current state-of-the-art video editing approaches, making it a significant advancement in the field.'}, 'zh': {'title': '灵活高效的视频编辑新方法', 'desc': '本文提出了一种基于掩码的LoRA调优方法，用于视频编辑，旨在灵活地适应预训练的图像到视频模型。该方法通过空间掩码和参考图像进行上下文特定的调整，保持背景区域的同时实现可控的编辑传播。与传统方法相比，这种方法在不改变模型架构的情况下，提供了高效且适应性强的视频编辑能力。实验结果表明，该方法在视频编辑性能上优于现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2506.10056', 'title': 'Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput', 'url': 'https://huggingface.co/papers/2506.10056', 'abstract': 'The standard paradigm for solving coding tasks via large language models (LLMs) is to generate-then-rank programs, where the latter step uses a verifier in the ranking process. The growing consensus is that a comprehensive verifier (e.g., a full test suite) should be prioritized over an outcome reward model (ORM) whenever possible, with little consideration given to the trade-offs involved. We aim to challenge this assumption by systematically exploring the tradeoff between speed and accuracy. We find that ORMs play a crucial role in scaling verification through trading accuracy for speed, even when a comprehensive verifier is available. Their value becomes especially apparent when used in a generate-prune-then-rank approach, where a faster but less accurate verifier removes incorrect solutions prior to ranking -- leading to a system that is 11.65x faster while only being 8.33% less accurate than the full test suite. We analyze the generate-prune-then-rank approach and show that it works by filtering out incorrect but highly ranked solutions. These findings enable the design of scalable and accurate program ranking systems.', 'score': 1, 'issue_id': 4318, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '1ee14b5fd87f1f86', 'authors': ['Gabriel Orlanski', 'Nicholas Roberts', 'Aws Albarghouthi', 'Frederic Sala'], 'affiliations': ['University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2506.10056.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Баланс скорости и точности: новый взгляд на верификацию кода с помощью LLM', 'desc': "Статья исследует компромисс между скоростью и точностью в парадигме генерации и ранжирования программ с помощью больших языковых моделей (LLM). Авторы оспаривают общепринятое мнение о приоритете полного набора тестов над моделью оценки результатов (ORM). Исследование показывает, что ORM играет ключевую роль в масштабировании верификации, позволяя значительно ускорить процесс при небольшой потере точности. Предложенный подход 'генерация-отсев-ранжирование' с использованием ORM оказывается в 11.65 раз быстрее при снижении точности всего на 8.33% по сравнению с полным набором тестов."}, 'en': {'title': 'Balancing Speed and Accuracy in Program Verification with ORMs', 'desc': 'This paper investigates the effectiveness of using outcome reward models (ORMs) in the coding task solution process with large language models (LLMs). It challenges the common belief that comprehensive verifiers should always be prioritized, highlighting the trade-off between speed and accuracy. The authors demonstrate that ORMs can significantly enhance the efficiency of program verification by allowing a faster, less accurate verifier to filter out incorrect solutions before ranking. Their findings suggest that a generate-prune-then-rank strategy can achieve a balance, resulting in a system that is much faster while maintaining acceptable accuracy levels.'}, 'zh': {'title': '速度与准确性的权衡：优化编码任务的验证方法', 'desc': '这篇论文探讨了使用大型语言模型（LLMs）解决编码任务的标准方法，即生成-然后-排名的程序。研究表明，全面的验证器（如完整的测试套件）通常被认为优于结果奖励模型（ORM），但作者挑战了这一假设。通过系统地研究速度与准确性之间的权衡，发现ORM在提高验证速度方面发挥了重要作用，即使在有全面验证器的情况下。最终，生成-修剪-然后-排名的方法使得系统速度提高了11.65倍，准确性仅下降了8.33%。'}}}, {'id': 'https://huggingface.co/papers/2506.11130', 'title': 'A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data', 'url': 'https://huggingface.co/papers/2506.11130', 'abstract': 'A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.', 'score': 1, 'issue_id': 4308, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '743ff411fbd34247', 'authors': ['Cheng Kang Chou', 'Chan-Jan Hsu', 'Ho-Lam Chung', 'Liang-Hsuan Tseng', 'Hsi-Chun Cheng', 'Yu-Kuan Fu', 'Kuan Po Huang', 'Hung-Yi Lee'], 'affiliations': ['MediaTek Research', 'National Taiwan University', 'Nvidia'], 'pdf_title_img': 'assets/pdf/title_img/2506.11130.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#transfer_learning', '#low_resource', '#audio', '#synthetic'], 'emoji': '🔁', 'ru': {'title': 'Самосовершенствующаяся система АСР: от псевдо-меток к улучшенному распознаванию', 'desc': 'Предложена самосовершенствующаяся система для улучшения распознавания речи с использованием только немаркированных данных. Процесс начинается с генерации псевдо-меток существующей моделью АСР, которые затем используются для обучения высококачественной системы синтеза речи. Затем синтезированные пары речь-текст используются для дообучения исходной модели АСР, замыкая цикл самосовершенствования. Эффективность подхода продемонстрирована на тайваньском мандаринском диалекте, где модель Twister, адаптированная из Whisper-large-v2, показала значительное снижение ошибок по сравнению с базовой моделью.'}, 'en': {'title': 'Enhancing ASR with Unlabeled Data: The Twister Framework', 'desc': 'This paper presents a self-refining framework designed to improve Automatic Speech Recognition (ASR) performance using only unlabeled datasets. The framework begins with an existing ASR model that generates pseudo-labels from unannotated speech data, which are then utilized to train a high-fidelity Text-to-Speech (TTS) system. Synthesized speech and text pairs are incorporated back into the original ASR model, creating a closed-loop system that enhances its accuracy. The proposed method, tested on Taiwanese Mandarin, shows significant error rate reductions, demonstrating its effectiveness in low-resource environments.'}, 'zh': {'title': '自我优化框架提升ASR性能的创新之路', 'desc': '本文提出了一种自我优化框架，通过使用未标注的数据集来提升自动语音识别（ASR）的性能。该框架首先利用现有的ASR模型为未标注的语音生成伪标签，然后训练一个高保真的文本到语音（TTS）系统。接着，将合成的语音文本对引入原始的ASR系统，形成一个闭环的自我改进循环。实验结果表明，该框架在台湾普通话语音上有效，能够显著降低错误率，尤其在低资源或特定领域的应用中具有实际意义。'}}}, {'id': 'https://huggingface.co/papers/2506.08915', 'title': 'Inherently Faithful Attention Maps for Vision Transformers', 'url': 'https://huggingface.co/papers/2506.08915', 'abstract': 'An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.', 'score': 1, 'issue_id': 4309, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'e080cfdc03932dd9', 'authors': ['Ananthu Aniraj', 'Cassio F. Dantas', 'Dino Ienco', 'Diego Marcos'], 'affiliations': ['Inrae', 'Inria', 'University of Montpellier'], 'pdf_title_img': 'assets/pdf/title_img/2506.08915.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#architecture', '#interpretability'], 'emoji': '👁️', 'ru': {'title': 'Фокусировка на главном: улучшение восприятия объектов с помощью масок внимания', 'desc': 'Данная статья представляет метод на основе внимания, использующий обученные бинарные маски для улучшения устойчивости восприятия объектов. Метод состоит из двух этапов: первый обрабатывает все изображение для обнаружения частей объекта, второй использует маскирование входных данных для фокусировки на релевантных областях. Оба этапа обучаются совместно, что позволяет второму этапу улучшать результаты первого. Эксперименты показывают, что данный подход значительно повышает устойчивость к ложным корреляциям и нетипичным фонам.'}, 'en': {'title': 'Focusing Attention for Robust Object Perception', 'desc': 'This paper presents an attention-based method that utilizes learned binary masks to enhance object perception in images. By focusing on relevant regions and filtering out irrelevant information, the method improves the robustness of predictions, especially in challenging contexts. The proposed two-stage framework first identifies object parts and task-relevant areas, then restricts analysis to these regions using attention masking. Experimental results show that this approach effectively mitigates the impact of spurious correlations and out-of-distribution backgrounds on object recognition tasks.'}, 'zh': {'title': '基于注意力的鲁棒物体感知方法', 'desc': '本文提出了一种基于注意力的算法，利用学习到的二进制注意力掩码来提高物体感知的鲁棒性。该方法通过关注相关的图像区域，过滤掉无关的信息，从而确保只有被关注的区域影响预测结果。我们设计了一个两阶段的框架，第一阶段处理完整图像以发现物体部分并识别任务相关区域，第二阶段则利用输入的注意力掩码限制感受野，从而进行更专注的分析。实验结果表明，该方法在应对虚假相关性和分布外背景方面显著提高了鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.11116', 'title': 'Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models', 'url': 'https://huggingface.co/papers/2506.11116', 'abstract': 'Infinity-Instruct, a comprehensive instruction dataset, enhances both foundational and chat capabilities of large language models through curation and synthesis, achieving superior performance compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our datasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and codeshttps://gitee.com/li-touch/infinity-instruct have been publicly released.', 'score': 1, 'issue_id': 4313, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '4de7c4de7a621e11', 'authors': ['Jijie Li', 'Li Du', 'Hanyu Zhao', 'Bo-wen Zhang', 'Liangdong Wang', 'Boyan Gao', 'Guang Liu', 'Yonghua Lin'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2506.11116.jpg', 'data': {'categories': ['#open_source', '#data', '#transfer_learning', '#benchmark', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Infinity-Instruct: революция в обучении языковых моделей', 'desc': 'Infinity-Instruct - это новый набор данных для обучения больших языковых моделей (LLM), который улучшает как базовые, так и диалоговые возможности моделей. Он состоит из двух частей: 7,4 млн базовых инструкций и 1,5 млн инструкций для чат-ботов. Модели, обученные на Infinity-Instruct, показывают значительное улучшение производительности по сравнению с существующими аналогами. Например, InfInstruct-LLaMA3.1-70B превосходит GPT-4-0314 на 8,6% в задачах следования инструкциям.'}, 'en': {'title': 'Unlocking LLM Potential with Infinity-Instruct', 'desc': 'Infinity-Instruct is a new instruction dataset that improves the performance of large language models (LLMs) in both foundational and chat tasks. It consists of 7.4 million curated foundational instructions and 1.5 million synthesized chat instructions, created through advanced data selection and filtering techniques. The dataset has been tested on various open-source models, showing significant performance improvements over existing instruction datasets. This research highlights the importance of combining foundational and chat training to enhance the overall capabilities of LLMs.'}, 'zh': {'title': '提升大型语言模型的指令能力', 'desc': 'Infinity-Instruct是一个全面的指令数据集，旨在提升大型语言模型的基础能力和对话能力。通过两阶段的处理流程，我们从超过1亿个样本中筛选出740万条高质量的基础指令，并合成150万条高质量的对话指令。经过实证评估，使用Infinity-Instruct微调的多个开源模型在基础和指令跟随基准测试中均表现出显著的性能提升。我们的研究结果表明，基础训练和对话训练之间的协同作用对大型语言模型的发展具有重要意义。'}}}, {'id': 'https://huggingface.co/papers/2506.03857', 'title': 'Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation', 'url': 'https://huggingface.co/papers/2506.03857', 'abstract': 'A novel candidate annotation paradigm using a teacher-student framework improves data quality for下游 applications by encouraging large language models to output multiple labels when uncertain.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist.', 'score': 1, 'issue_id': 4314, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9efc9f12a990b701', 'authors': ['Mingxuan Xia', 'Haobo Wang', 'Yixuan Li', 'Zewei Yu', 'Jindong Wang', 'Junbo Zhao', 'Runze Wu'], 'affiliations': ['NetEase Fuxi AI Lab', 'University of Wisconsin Madison', 'William & Mary', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03857.jpg', 'data': {'categories': ['#data', '#dataset', '#training', '#optimization', '#alignment'], 'emoji': '🏷️', 'ru': {'title': 'Улучшение качества аннотаций с помощью множественных меток от языковых моделей', 'desc': 'Статья представляет новый подход к аннотации данных с использованием больших языковых моделей (LLM). Авторы предлагают парадигму кандидатной аннотации, где LLM выдает несколько возможных меток при неопределенности. Для получения уникальных меток используется система учитель-ученик с дистилляцией знаний в малую языковую модель (SLM). Теоретически и экспериментально показано, что этот метод превосходит прямое использование одиночных аннотаций.'}, 'en': {'title': 'Empowering Uncertainty: Multi-Label Annotation for Better Data Quality', 'desc': 'This paper introduces a new approach to data annotation using a teacher-student framework that enhances the quality of labels produced by large language models (LLMs). Instead of forcing LLMs to choose a single label, the method encourages them to generate multiple potential labels when they are uncertain. This strategy helps to mitigate the risk of incorrect labeling, which can degrade the quality of data for subsequent applications. The proposed framework, called CanDist, uses a Small Language Model (SLM) to refine these candidate annotations, leading to better performance in text classification tasks.'}, 'zh': {'title': '利用教师-学生框架提升数据标注质量', 'desc': '本文提出了一种新的候选标注范式，利用教师-学生框架来提高数据质量。该方法鼓励大型语言模型在不确定时输出多个标签，从而减少错误标注的风险。通过这种方式，模型能够更好地处理复杂样本，确保下游应用的数据质量。实验结果表明，该方法在六个文本分类任务中表现出色，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.10387', 'title': 'Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills', 'url': 'https://huggingface.co/papers/2506.10387', 'abstract': 'Hierarchical Multimodal Skills and Skill-Augmented Monte Carlo Tree Search improve multimodal GUI agent performance in long-horizon tasks by abstracting knowledge and bridging the offline-online domain gap.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI agents have yielded promising outcomes. However, these agents still struggle with long-horizon tasks in online environments, primarily due to insufficient knowledge and the inherent gap between offline and online domains. In this paper, inspired by how humans generalize knowledge in open-ended environments, we propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of insufficient knowledge. It progressively abstracts trajectories into execution skills, core skills, and ultimately meta-skills, providing a hierarchical knowledge structure for long-horizon task planning. To bridge the domain gap, we propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm, which efficiently leverages skills acquired in offline environments to reduce the action search space during online tree exploration. Building on HMS, we propose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To validate the performance of Mirage-1 in real-world long-horizon scenarios, we constructed a new benchmark, AndroidLH. Experimental results show that Mirage-1 outperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld, MobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page: https://cybertronagent.github.io/Mirage-1.github.io/', 'score': 0, 'issue_id': 4315, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': '9c47a3d6d929dc16', 'authors': ['Yuquan Xie', 'Zaijing Li', 'Rui Shao', 'Gongwei Chen', 'Kaiwen Zhou', 'Yinchuan Li', 'Dongmei Jiang', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology, Shenzhen', 'Huawei Noahs Ark Lab', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.10387.jpg', 'data': {'categories': ['#transfer_learning', '#agents', '#benchmark', '#long_context', '#games', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Иерархические навыки и МКТП: прорыв в долгосрочном планировании ГПИ-агентов', 'desc': 'Статья представляет новый подход к улучшению производительности мультимодальных ГПИ-агентов в задачах с длительным горизонтом планирования. Авторы предлагают модуль Иерархических Мультимодальных Навыков (HMS) для абстрагирования знаний и алгоритм Дерева Поиска Монте-Карло с Дополнением Навыками (SA-MCTS) для преодоления разрыва между офлайн и онлайн доменами. На основе HMS разработан агент Mirage-1, который превосходит предыдущие модели на нескольких бенчмарках. Эксперименты показывают значительное улучшение производительности на различных наборах данных, включая новый бенчмарк AndroidLH.'}, 'en': {'title': 'Empowering GUI Agents with Hierarchical Skills for Long-Horizon Success', 'desc': 'This paper introduces a new approach to improve the performance of multimodal GUI agents in long-horizon tasks by using Hierarchical Multimodal Skills (HMS) and Skill-Augmented Monte Carlo Tree Search (SA-MCTS). HMS organizes knowledge into a hierarchy of execution skills, core skills, and meta-skills, allowing agents to better generalize and plan tasks. SA-MCTS enhances the search process by utilizing skills learned in offline settings to streamline decision-making in online environments. The proposed agent, Mirage-1, demonstrates significant performance improvements over existing agents in various benchmarks, showcasing its effectiveness in real-world applications.'}, 'zh': {'title': '层次化技能与增强搜索提升GUI代理表现', 'desc': '本文提出了一种层次化多模态技能模块（HMS），旨在解决多模态图形用户界面（GUI）代理在长时间任务中的知识不足问题。HMS通过将轨迹逐步抽象为执行技能、核心技能和元技能，构建了一个层次化的知识结构，以支持长时间任务的规划。为了弥补离线和在线领域之间的差距，本文还提出了技能增强的蒙特卡洛树搜索算法（SA-MCTS），该算法有效利用离线环境中获得的技能，减少在线树搜索中的动作搜索空间。实验结果表明，Mirage-1在多个基准测试中显著优于之前的代理，提升了性能。'}}}, {'id': 'https://huggingface.co/papers/2505.24726', 'title': 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.24726', 'abstract': "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore a method for improving the performance of large language models through self-reflection and reinforcement learning. By incentivizing the model to generate better self-reflections when it answers incorrectly, we demonstrate that a model's ability to solve complex, verifiable tasks can be enhanced even when generating synthetic data is infeasible and only binary feedback is available. Our framework operates in two stages: first, upon failing a given task, the model generates a self-reflective commentary analyzing its previous attempt; second, the model is given another attempt at the task with the self-reflection in context. If the subsequent attempt succeeds, the tokens generated during the self-reflection phase are rewarded. Our experimental results show substantial performance gains across a variety of model architectures, as high as 34.7% improvement at math equation writing and 18.1% improvement at function calling. Notably, smaller fine-tuned models (1.5 billion to 7 billion parameters) outperform models in the same family that are 10 times larger. Our novel paradigm is thus an exciting pathway to more useful and reliable language models that can self-improve on challenging tasks with limited external feedback.", 'score': 140, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '05f75b6123a35a65', 'authors': ['Shelly Bensal', 'Umar Jamil', 'Christopher Bryant', 'Melisa Russak', 'Kiran Kamble', 'Dmytro Mozolevskyi', 'Muayad Ali', 'Waseem AlShikh'], 'affiliations': ['Writer, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.24726.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#small_models', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самоанализ и обучение с подкреплением повышают эффективность языковых моделей', 'desc': 'Исследователи предложили метод улучшения работы больших языковых моделей с помощью самоанализа и обучения с подкреплением. Модель генерирует самоанализ после неудачной попытки решения задачи, а затем повторно пытается её решить с учетом этого анализа. Если вторая попытка успешна, токены самоанализа получают положительное подкрепление. Эксперименты показали значительное улучшение производительности на различных задачах, особенно для небольших моделей.'}, 'en': {'title': 'Empowering Language Models Through Self-Reflection and Reinforcement Learning', 'desc': 'This paper presents a novel approach to enhance large language models using self-reflection and reinforcement learning. The method encourages models to analyze their mistakes and generate self-reflective commentary, which is then used to improve subsequent task attempts. By rewarding successful outcomes that follow self-reflection, the model learns to perform better even with minimal feedback. Experimental results indicate significant performance improvements, particularly in smaller models, suggesting a promising direction for developing more effective language models.'}, 'zh': {'title': '自我反思与强化学习提升语言模型性能', 'desc': '本文提出了一种通过自我反思和强化学习来提升大型语言模型性能的方法。该方法在模型回答错误时，激励其生成更好的自我反思，从而提高解决复杂任务的能力。框架分为两个阶段：首先，模型在失败后生成自我反思的评论；其次，模型在考虑自我反思的情况下再次尝试任务。实验结果显示，在多种模型架构中，性能提升显著，尤其是小型微调模型的表现超过了同类更大模型。'}}}, {'id': 'https://huggingface.co/papers/2506.02387', 'title': 'VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments', 'url': 'https://huggingface.co/papers/2506.02387', 'abstract': "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks, yet existing benchmarks remain limited to single-agent or text-only environments. In contrast, real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts, posing challenges with both multimodal observations and strategic interactions. To bridge this gap, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments. VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. We consider two complementary evaluation dimensions, including offline evaluation of strategic reasoning by next-action prediction accuracy and online evaluation of decision-making by normalized episode return. Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return. We further conduct in-depth analyses on multimodal observations, test-time scaling, social behaviors, and failure cases of VLM agents. By standardizing the evaluation and highlighting the limitations of existing models, we envision VS-Bench as a foundation for future research on strategic multimodal agents. Code and data are available at https://vs-bench.github.io.", 'score': 50, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '829f3546d3ac9345', 'authors': ['Zelai Xu', 'Zhexuan Xu', 'Xiangmin Yi', 'Huining Yuan', 'Xinlei Chen', 'Yi Wu', 'Chao Yu', 'Yu Wang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02387.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#benchmark', '#games'], 'emoji': '🤖', 'ru': {'title': 'VS-Bench: Новый рубеж в оценке стратегического мышления мультимодальных ИИ-агентов', 'desc': 'VS-Bench - это мультимодальный бенчмарк для оценки стратегического мышления и принятия решений vision-language моделями в сложных мультиагентных средах. Он включает восемь визуальных сред с кооперативными, соревновательными и смешанными взаимодействиями. Бенчмарк оценивает способность моделей предсказывать будущие действия других агентов и оптимизировать долгосрочные цели. Эксперименты с 14 ведущими VLM показали значительный разрыв между текущими моделями и оптимальной производительностью.'}, 'en': {'title': 'Evaluating Strategic Reasoning in Multi-Agent Environments with VS-Bench', 'desc': 'VS-Bench is a new benchmark created to test Vision Language Models (VLMs) in complex situations where multiple agents interact. Unlike previous benchmarks that focused on single agents or text-only tasks, VS-Bench evaluates how well VLMs can reason and make decisions in environments that include both visual and language elements. It features eight different scenarios that require agents to work together, compete, or navigate mixed motives, assessing their ability to predict actions and achieve long-term goals. The results show that current VLMs still have a long way to go, with significant gaps in their performance, highlighting the need for further research in this area.'}, 'zh': {'title': '多模态智能体的战略推理新基准', 'desc': 'VS-Bench是一个多模态基准，旨在评估视觉语言模型（VLM）在复杂多智能体环境中的战略推理和决策能力。与现有的单智能体或仅文本环境的基准不同，VS-Bench考虑了多个智能体在丰富的视觉和语言背景下的互动。该基准包括八个基于视觉的环境，涵盖合作、竞争和混合动机的互动，评估智能体预测他人未来动作和优化长期目标的能力。通过标准化评估，VS-Bench为未来的战略多模态智能体研究奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2506.03147', 'title': 'UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation', 'url': 'https://huggingface.co/papers/2506.03147', 'abstract': "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t Although existing unified models deliver strong performance on vision-language understanding and text-to-image generation, their models are limited in exploring image perception and manipulation tasks, which are urgently desired by users for wide applications. Recently, OpenAI released their powerful GPT-4o-Image model for comprehensive image perception and manipulation, achieving expressive capability and attracting community interests. By observing the performance of GPT-4o-Image in our carefully constructed experiments, we infer that GPT-4o-Image leverages features extracted by semantic encoders instead of VAE, while VAEs are considered essential components in many image manipulation models. Motivated by such inspiring observations, we present a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders. As a result, we build a strong unified model using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on image editing benchmarks. UniWorld also maintains competitive image understanding and generation capabilities, achieving strong performance across multiple image perception tasks. We fully open-source our models, including model weights, training and evaluation scripts, and datasets.", 'score': 48, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34f1d96b37be24a1', 'authors': ['Bin Lin', 'Zongjian Li', 'Xinhua Cheng', 'Yuwei Niu', 'Yang Ye', 'Xianyi He', 'Shenghai Yuan', 'Wangbo Yu', 'Shaodong Wang', 'Yunyang Ge', 'Yatian Pang', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03147.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'UniWorld: Мощная модель для работы с изображениями на основе семантических признаков', 'desc': 'UniWorld - это унифицированная генеративная модель для восприятия и редактирования изображений. Она использует семантические признаки из визуально-языковых моделей вместо вариационных автоэнкодеров. UniWorld превосходит модель BAGEL по качеству редактирования изображений, используя всего 1% данных. Модель также демонстрирует высокие результаты в задачах понимания и генерации изображений.'}, 'en': {'title': 'UniWorld: Efficient Image Manipulation with Semantic Power', 'desc': 'UniWorld is a new generative framework that enhances image perception and manipulation by utilizing semantic features from visual-language models. It outperforms the existing model BAGEL while using only 1% of its data, demonstrating efficiency in data usage. The framework leverages insights from the GPT-4o-Image model, which effectively uses semantic encoders instead of traditional Variational Autoencoders (VAEs). UniWorld not only excels in image editing tasks but also maintains strong capabilities in image understanding and generation across various applications.'}, 'zh': {'title': 'UniWorld：图像感知与操作的新纪元', 'desc': 'UniWorld是一个统一的生成框架，利用视觉语言模型的语义特征来进行图像感知和操作。与BAGEL相比，UniWorld在数据使用上减少了90%，但在图像编辑基准测试中表现更优。该模型不仅在图像理解和生成方面保持竞争力，还在多个图像感知任务中取得了良好的效果。我们将模型权重、训练和评估脚本以及数据集全部开源，方便研究者使用。'}}}, {'id': 'https://huggingface.co/papers/2506.02096', 'title': 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis', 'url': 'https://huggingface.co/papers/2506.02096', 'abstract': "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) trained via reinforcement learning with verifiable reward (RLVR) have shown notable progress in scaling test-time compute effectively. In this work, we investigate how synthesized RL data can further improve RLVR. To this end, we propose SynthRL-a scalable and guaranteed pipeline for automatic data scaling in reasoning-oriented RL training. SynthRL comprises three key stages: (1) selecting seed questions with appropriate distribution, (2) augmenting them into more challenging variants while preserving the original answers, and (3) a guaranteed verification stage that ensures near-perfect correctness and difficulty enhancement. Our empirical experiments demonstrate SynthRL's scalability and effectiveness. When applied to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable, challenging questions from approximately 8K seed samples. Models trained with our synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with a significant improvement over baseline models trained on seed data alone. Notably, detailed analysis reveals that the gains are more pronounced on the most challenging evaluation samples, highlighting SynthRL's effectiveness in eliciting deeper and more complex reasoning patterns.", 'score': 45, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '9ea84a7af081829e', 'authors': ['Zijian Wu', 'Jinjie Ni', 'Xiangyan Liu', 'Zichen Liu', 'Hang Yan', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.02096.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#synthetic', '#rl'], 'emoji': '🧠', 'ru': {'title': 'SynthRL: Синтез данных для улучшения математических рассуждений ИИ', 'desc': 'SynthRL - это масштабируемый конвейер для синтеза данных в обучении с подкреплением с проверяемыми наградами. Он улучшает визуальные языковые модели для математических рассуждений, генерируя сложные, проверяемые вопросы. SynthRL включает три ключевых этапа: выбор исходных вопросов, их усложнение с сохранением ответов и верификацию для обеспечения корректности. Эксперименты показали, что модели, обученные на синтезированных данных, достигают стабильных улучшений на пяти тестовых наборах по визуальным математическим рассуждениям.'}, 'en': {'title': 'SynthRL: Elevating Visual Math Reasoning with Scalable Data Synthesis', 'desc': "SynthRL is a novel pipeline designed to enhance reinforcement learning (RL) by synthesizing data with verifiable rewards, specifically for visual math reasoning tasks. It operates in three stages: selecting initial questions, creating more challenging variants while keeping the answers intact, and verifying the correctness of these questions. This approach allows for the generation of over 3,300 additional verifiable questions from a smaller set of seed samples, significantly improving model performance. Empirical results show that models trained with SynthRL's data outperform those trained only on seed data, especially on difficult reasoning tasks, demonstrating its effectiveness in fostering advanced reasoning capabilities."}, 'zh': {'title': 'SynthRL：提升视觉数学推理的智能合成管道', 'desc': 'SynthRL是一种可扩展的数据合成管道，旨在增强强化学习中的可验证奖励（RLVR），特别是在视觉数学推理模型（VLMs）中。该方法通过生成具有挑战性和可验证的问题，来提高模型的推理能力。SynthRL包括三个关键阶段：选择合适分布的种子问题、将其增强为更具挑战性的变体，并确保答案的正确性和难度的提升。实验结果表明，使用SynthRL合成的数据在多个视觉数学推理基准测试中显著提高了模型的表现，尤其是在最具挑战性的样本上。'}}}, {'id': 'https://huggingface.co/papers/2505.24120', 'title': 'CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs', 'url': 'https://huggingface.co/papers/2505.24120', 'abstract': 'A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.', 'score': 43, 'issue_id': 4116, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '09ca2aeec21b0156', 'authors': ['Ai Jian', 'Weijie Qiu', 'Xiaokun Wang', 'Peiyu Wang', 'Yunzhuo Hao', 'Jiangbo Pei', 'Yichen Wei', 'Yi Peng', 'Xuchen Song'], 'affiliations': ['Kunlun Inc.', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.24120.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#reasoning'], 'emoji': '🔬', 'ru': {'title': 'CSVQA: новый рубеж в оценке научного мышления ИИ', 'desc': 'Представлен новый бенчмарк CSVQA для оценки научного мышления в мультимодальных моделях через задачи ответов на вопросы по изображениям в конкретных научных областях. Бенчмарк содержит 1378 пар вопрос-ответ из различных STEM-дисциплин, требующих применения предметных знаний и анализа визуальных данных. Оценка 15 современных мультимодальных моделей на CSVQA показала значительные различия в производительности, при этом даже лучшая модель достигла точности лишь 49.6%. Результаты подчеркивают необходимость улучшения способностей научного мышления в мультимодальных моделях.'}, 'en': {'title': 'CSVQA: Advancing Scientific Reasoning in Vision-Language Models', 'desc': 'The paper introduces CSVQA, a new benchmark designed to evaluate the scientific reasoning abilities of vision-language models (VLMs) through domain-specific visual question answering. Unlike existing benchmarks that focus on generic image understanding, CSVQA emphasizes the integration of domain knowledge and visual evidence in STEM contexts. It includes 1,378 question-answer pairs that require higher-order reasoning and authentic scientific content. The evaluation of 15 VLMs on this benchmark reveals significant performance gaps, highlighting the need for improvements in their scientific reasoning capabilities.'}, 'zh': {'title': 'CSVQA：提升视觉语言模型的科学推理能力', 'desc': 'CSVQA是一个新的基准，用于评估视觉语言模型在科学推理方面的能力。该基准通过领域特定的视觉问答，强调了这些模型在科学推理中的不足。CSVQA包含1378个精心构建的问题-答案对，涵盖多个STEM学科，要求模型整合领域知识和视觉证据进行高阶推理。我们的评估显示，尽管有些模型表现较好，但最高分的模型准确率仅为49.6%，这表明在科学推理能力上仍需进一步提升。'}}}, {'id': 'https://huggingface.co/papers/2505.24714', 'title': 'FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation', 'url': 'https://huggingface.co/papers/2505.24714', 'abstract': 'FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have experienced rapid development in recent years. However, in the financial domain, there is a notable lack of effective and specialized multimodal evaluation datasets. To advance the development of MLLMs in the finance domain, we introduce FinMME, encompassing more than 11,000 high-quality financial research samples across 18 financial domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We ensure data quality through 20 annotators and carefully designed validation mechanisms. Additionally, we develop FinScore, an evaluation system incorporating hallucination penalties and multi-dimensional capability assessment to provide an unbiased evaluation. Extensive experimental results demonstrate that even state-of-the-art models like GPT-4o exhibit unsatisfactory performance on FinMME, highlighting its challenging nature. The benchmark exhibits high robustness with prediction variations under different prompts remaining below 1%, demonstrating superior reliability compared to existing datasets. Our dataset and evaluation protocol are available at https://huggingface.co/datasets/luojunyu/FinMME and https://github.com/luo-junyu/FinMME.', 'score': 32, 'issue_id': 4110, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'a6dcbb10b5be7f41', 'authors': ['Junyu Luo', 'Zhizhuo Kou', 'Liming Yang', 'Xiao Luo', 'Jinsheng Huang', 'Zhiping Xiao', 'Jingshu Peng', 'Chengzhong Liu', 'Jiaming Ji', 'Xuanzhe Liu', 'Sirui Han', 'Ming Zhang', 'Yike Guo'], 'affiliations': ['HKUST', 'School of Computer Science, Peking University', 'State Key Laboratory for Multimedia Information Processing, PKU-Anker LLM Lab', 'University of California, Los Angeles', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.24714.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#science', '#multimodal', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'FinMME и FinScore: Новый стандарт для оценки мультимодальных языковых моделей в финансовой сфере', 'desc': 'FinMME - это обширный мультимодальный датасет для финансовых исследований, включающий более 11 000 высококачественных образцов из 18 финансовых областей. Датасет содержит различные типы графиков и диаграмм, а его качество обеспечивается тщательной аннотацией и валидацией. FinScore - это система оценки, учитывающая штрафы за галлюцинации и многомерную оценку возможностей моделей. Эксперименты показали, что даже передовые модели вроде GPT-4o демонстрируют неудовлетворительные результаты на FinMME, что подчеркивает сложность датасета.'}, 'en': {'title': 'FinMME: Elevating Financial AI with Robust Evaluation', 'desc': "The paper introduces FinMME, a new multimodal dataset specifically designed for financial research, which includes over 11,000 samples across various financial domains and asset classes. It addresses the gap in specialized evaluation datasets for Multimodal Large Language Models (MLLMs) in finance. The authors also present FinScore, an evaluation system that assesses model performance while penalizing inaccuracies and measuring multiple capabilities. Experimental results reveal that even advanced models like GPT-4o struggle with the challenges posed by FinMME, underscoring the dataset's robustness and reliability."}, 'zh': {'title': '推动金融领域的多模态研究', 'desc': 'FinMME是一个全面的多模态金融研究数据集，旨在推动金融领域的多模态大语言模型（MLLMs）发展。该数据集包含超过11,000个高质量的金融研究样本，涵盖18个金融领域和6种资产类别，提供10种主要图表类型和21种子类型。为了确保数据质量，研究团队使用了20名注释员和精心设计的验证机制。此外，FinScore评估系统引入了幻觉惩罚和多维能力评估，以提供公正的评估结果，尽管先进模型如GPT-4o在FinMME上表现不佳，显示出其挑战性。'}}}, {'id': 'https://huggingface.co/papers/2506.02397', 'title': 'OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation', 'url': 'https://huggingface.co/papers/2506.02397', 'abstract': 'OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced large reasoning models (LRMs) leverage extended chain-of-thought (CoT) reasoning to solve complex tasks, achieving state-of-the-art performance. Despite their success, we identify a critical issue: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary. To address this, we systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking) for straightforward problems while engaging in deliberate thinking (slow-thinking) for complex problems. Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23\\% on average without compromising accuracy, offering practical guidelines for efficient reasoning models. The code is available at https://github.com/AgenticIR-Lab/OThink-R1.', 'score': 28, 'issue_id': 4120, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '12cee2b297810e65', 'authors': ['Shengjia Zhang', 'Junjie Wu', 'Jiawei Chen', 'Changwang Zhang', 'Xingyu Lou', 'Wangchunshu Zhou', 'Sheng Zhou', 'Can Wang', 'Jun Wang'], 'affiliations': ['OPPO Research Institute, Shenzhen, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.02397.jpg', 'data': {'categories': ['#training', '#math', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений ИИ: эффективность без потери точности', 'desc': 'OThink-R1 - это новый метод машинного обучения, направленный на уменьшение избыточности рассуждений при решении сложных задач. Он классифицирует шаги рассуждений как существенные или избыточные и динамически переключает режимы мышления в зависимости от сложности задачи. OThink-R1 использует идентифицированные парадигмы и LLM-Judge для классификации траекторий рассуждений. Эксперименты показывают, что OThink-R1 снижает избыточность рассуждений в среднем на 23% без ущерба для точности.'}, 'en': {'title': 'Optimize Reasoning: Think Smart, Not Hard!', 'desc': 'OThink-R1 is a novel approach designed to enhance reasoning efficiency in complex problem-solving by distinguishing between essential and redundant reasoning steps. It leverages a classification system to identify when complex reasoning is unnecessary, allowing for a switch between fast-thinking and slow-thinking modes based on task complexity. This method significantly reduces reasoning redundancy by approximately 23% while maintaining accuracy in tasks like mathematics and question-answering. The findings suggest that not all tasks require extensive reasoning, and OThink-R1 provides a framework for optimizing reasoning processes in large reasoning models.'}, 'zh': {'title': '动态思维模式，减少推理冗余', 'desc': 'OThink-R1 是一种新方法，旨在减少复杂问题解决中的推理冗余。它通过将推理步骤分类为必要或冗余，并根据任务复杂性动态切换思维模式。对于简单问题，OThink-R1 使用快速思维模式，而对于复杂问题则采用深思熟虑的慢思维模式。实验表明，OThink-R1 平均减少了近23%的推理冗余，同时保持了准确性，为高效推理模型提供了实用指导。'}}}, {'id': 'https://huggingface.co/papers/2506.00123', 'title': 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces', 'url': 'https://huggingface.co/papers/2506.00123', 'abstract': 'VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable progress of Multimodal Large Language Models (MLLMs) has attracted increasing attention to extend them to physical entities like legged robot. This typically requires MLLMs to not only grasp multimodal understanding abilities, but also integrate visual-spatial reasoning and physical interaction capabilities. Nevertheless,existing methods struggle to unify these capabilities due to their fundamental differences.In this paper, we present the Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning, and control in real world. VeBrain reformulates robotic control into common text-based MLLM tasks in the 2D visual space, thus unifying the objectives and mapping spaces of different tasks. Then, a novel robotic adapter is proposed to convert textual control signals from MLLMs to motion policies of real robots. From the data perspective, we further introduce VeBrain-600k, a high-quality instruction dataset encompassing various capabilities of VeBrain. In VeBrain-600k, we take hundreds of hours to collect, curate and annotate the data, and adopt multimodal chain-of-thought(CoT) to mix the different capabilities into a single conversation. Extensive experiments on 13 multimodal benchmarks and 5 spatial intelligence benchmarks demonstrate the superior performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to legged robots and robotic arms, VeBrain shows strong adaptability, flexibility, and compositional capabilities compared to existing methods. For example, compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by +5.6%, but also excels in legged robot tasks with +50% average gains.', 'score': 28, 'issue_id': 4114, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8914927f73dff147', 'authors': ['Gen Luo', 'Ganlin Yang', 'Ziyang Gong', 'Guanzhou Chen', 'Haonan Duan', 'Erfei Cui', 'Ronglei Tong', 'Zhi Hou', 'Tianyi Zhang', 'Zhe Chen', 'Shenglong Ye', 'Lewei Lu', 'Jingbo Wang', 'Wenhai Wang', 'Jifeng Dai', 'Yu Qiao', 'Rongrong Ji', 'Xizhou Zhu'], 'affiliations': ['Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00123.jpg', 'data': {'categories': ['#games', '#robotics', '#multimodal', '#reasoning', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'VeBrain: Единый мозг для восприятия, мышления и управления роботами', 'desc': 'VeBrain - это унифицированная система для роботов с ногами, объединяющая мультимодальное понимание, визуально-пространственное мышление и физическое взаимодействие. Она переформулирует управление роботом в текстовые задачи для мультимодальных больших языковых моделей в 2D визуальном пространстве. VeBrain включает новый адаптер для преобразования текстовых сигналов управления в политики движения реальных роботов. Система демонстрирует превосходную производительность по сравнению с существующими методами на различных тестах.'}, 'en': {'title': 'VeBrain: Unifying Multimodal Intelligence for Advanced Robotic Control', 'desc': 'VeBrain is a new framework designed to enhance the capabilities of legged robots by combining multimodal understanding, visual-spatial reasoning, and physical interaction. It reformulates robotic control tasks into text-based tasks that can be processed by Multimodal Large Language Models (MLLMs), allowing for a unified approach to different robotic challenges. The framework includes a unique robotic adapter that translates textual commands from MLLMs into actionable motion policies for robots. Extensive testing shows that VeBrain outperforms existing models, achieving significant improvements in various benchmarks and demonstrating its adaptability in real-world robotic applications.'}, 'zh': {'title': 'VeBrain：四足机器人智能控制的新框架', 'desc': 'VeBrain是一个统一框架，旨在将多模态理解、视觉空间推理和物理交互整合到四足机器人中。该框架通过将机器人控制重新表述为基于文本的多模态大语言模型（MLLM）任务，从而统一了不同任务的目标和映射空间。VeBrain还引入了一种新型的机器人适配器，将MLLM的文本控制信号转换为真实机器人的运动策略。此外，VeBrain-600k是一个高质量的指令数据集，涵盖了VeBrain的多种能力，经过大量数据收集和注释，展示了VeBrain在多模态基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03135', 'title': 'OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models', 'url': 'https://huggingface.co/papers/2506.03135', 'abstract': "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a key aspect of cognitive psychology and remains a major bottleneck for current vision-language models (VLMs). While extensive research has aimed to evaluate or improve VLMs' understanding of basic spatial relations, such as distinguishing left from right, near from far, and object counting, these tasks represent only the most fundamental level of spatial reasoning. In this work, we introduce OmniSpatial, a comprehensive and challenging benchmark for spatial reasoning, grounded in cognitive psychology. OmniSpatial covers four major categories: dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking, with 50 fine-grained subcategories. Through Internet data crawling and careful manual annotation, we construct over 1.5K question-answer pairs. Extensive experiments show that both open- and closed-source VLMs, as well as existing reasoning and spatial understanding models, exhibit significant limitations in comprehensive spatial understanding. We further analyze failure cases and propose potential directions for future research.", 'score': 27, 'issue_id': 4116, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'a7cd11e4c04b1336', 'authors': ['Mengdi Jia', 'Zekun Qi', 'Shaochen Zhang', 'Wenyao Zhang', 'Xinqiang Yu', 'Jiawei He', 'He Wang', 'Li Yi'], 'affiliations': ['Galbot', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shanghai Qi Zhi Institute', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03135.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'OmniSpatial: новый рубеж в оценке пространственного мышления ИИ', 'desc': 'OmniSpatial - это комплексный тест для оценки понимания пространственных задач моделями компьютерного зрения и обработки естественного языка. Он охватывает четыре основные категории: динамическое рассуждение, сложную пространственную логику, пространственное взаимодействие и принятие перспективы. Эксперименты показали, что современные модели имеют значительные ограничения в комплексном пространственном понимании. Авторы проанализировали случаи неудач и предложили направления для будущих исследований.'}, 'en': {'title': 'OmniSpatial: Elevating Spatial Reasoning in Vision-Language Models', 'desc': 'This paper introduces OmniSpatial, a new benchmark designed to assess the capabilities of vision-language models (VLMs) in advanced spatial reasoning tasks. It highlights that while VLMs can handle basic spatial relations, they struggle with more complex reasoning, which is crucial for understanding human-like spatial interactions. The benchmark includes four main categories of spatial reasoning, with a total of 50 detailed subcategories, and is supported by over 1,500 carefully crafted question-answer pairs. The findings reveal significant shortcomings in current VLMs, prompting a discussion on future research directions to enhance their spatial reasoning abilities.'}, 'zh': {'title': 'OmniSpatial：提升视觉-语言模型的空间推理能力', 'desc': '本文介绍了一个名为OmniSpatial的基准测试，旨在评估视觉-语言模型在高级空间推理任务中的理解能力。研究发现，当前的视觉-语言模型在空间推理方面存在显著的局限性，尤其是在动态推理、复杂空间逻辑、空间交互和视角转换等方面。OmniSpatial基于认知心理学，涵盖了50个细分类别，并通过网络数据爬取和人工标注构建了1500多个问答对。实验结果表明，无论是开源还是闭源的视觉-语言模型，在全面的空间理解上都表现不佳，本文还分析了失败案例并提出了未来研究的潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2506.03143', 'title': 'GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2506.03143', 'abstract': 'GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.', 'score': 25, 'issue_id': 4112, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '34d1779dffecf9d8', 'authors': ['Qianhui Wu', 'Kanzhi Cheng', 'Rui Yang', 'Chaoyun Zhang', 'Jianwei Yang', 'Huiqiang Jiang', 'Jian Mu', 'Baolin Peng', 'Bo Qiao', 'Reuben Tan', 'Si Qin', 'Lars Liden', 'Qingwei Lin', 'Huan Zhang', 'Tong Zhang', 'Jianbing Zhang', 'Dongmei Zhang', 'Jianfeng Gao'], 'affiliations': ['Microsoft', 'Nanjing University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.03143.jpg', 'data': {'categories': ['#multimodal', '#training', '#cv', '#benchmark', '#optimization', '#games'], 'emoji': '🖱️', 'ru': {'title': 'GUI-Actor: Революция в локализации элементов интерфейса с помощью VLM', 'desc': 'GUI-Actor - это метод на основе VLM, использующий механизм внимания для безкоординатной локализации элементов графического интерфейса. Он превосходит существующие методы, демонстрируя лучшую обобщающую способность и эффективное дообучение. GUI-Actor вводит основанную на внимании головную часть для действий, которая учится сопоставлять специальный токен <ACTOR> со всеми релевантными визуальными токенами патчей. Метод также включает верификатор локализации для оценки и выбора наиболее вероятного региона действия из предложенных кандидатов.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Coordinate-Free Attention', 'desc': 'The paper introduces GUI-Actor, a novel method for visual grounding in graphical user interfaces (GUIs) that leverages vision-language models (VLMs) and attention mechanisms. Unlike traditional approaches that generate screen coordinates, GUI-Actor operates in a coordinate-free manner, aligning an action token with relevant visual patches to identify action regions efficiently. This method addresses limitations such as weak spatial-semantic alignment and ambiguity in supervision targets, leading to better generalization across different screen layouts. The results demonstrate that GUI-Actor significantly outperforms existing methods, achieving state-of-the-art performance on various benchmarks while allowing for efficient fine-tuning.'}, 'zh': {'title': 'GUI-Actor：无坐标的高效GUI定位方法', 'desc': '本文提出了一种名为GUI-Actor的基于视觉语言模型（VLM）的方法，用于无坐标的图形用户界面（GUI）定位。该方法通过引入基于注意力的动作头，能够在一次前向传播中将特定的<ACTOR>标记与相关的视觉补丁标记对齐，从而有效地提出一个或多个动作区域。实验结果表明，GUI-Actor在多个GUI动作定位基准上超越了现有的最先进方法，并且在未见过的屏幕分辨率和布局上具有更好的泛化能力。此外，通过引入验证器，GUI-Actor能够在保持VLM主干不变的情况下，仅微调新引入的动作头，便能实现与之前最先进模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23061', 'title': 'DINGO: Constrained Inference for Diffusion LLMs', 'url': 'https://huggingface.co/papers/2505.23061', 'abstract': "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference", 'score': 22, 'issue_id': 4112, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c215c7998d0a7928', 'authors': ['Tarun Suresh', 'Debangshu Banerjee', 'Shubham Ugare', 'Sasa Misailovic', 'Gagandeep Singh'], 'affiliations': ['Department of Computer Science, University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.23061.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#optimization', '#diffusion'], 'emoji': '🧮', 'ru': {'title': 'DINGO: Структурированное декодирование для диффузионных языковых моделей', 'desc': 'Статья представляет DINGO - новую стратегию декодирования для диффузионных языковых моделей. DINGO использует динамическое программирование для обеспечения структурированных ограничений вывода, что значительно улучшает производительность в задачах генерации символьной математики и JSON. В отличие от авторегрессионных моделей, диффузионные модели предсказывают блок токенов параллельно, что делает традиционные алгоритмы декодирования с ограничениями неэффективными. DINGO позволяет выбирать выходные строки с наивысшей вероятностью, строго соблюдая заданные регулярные выражения.'}, 'en': {'title': 'DINGO: Structuring Success in Diffusion Language Models', 'desc': 'DINGO is a new decoding strategy that improves diffusion language models by applying structured output constraints. This method allows the models to generate outputs that meet specific requirements, like those found in symbolic math and JSON formats. Unlike traditional autoregressive models that generate text one token at a time, DINGO uses dynamic programming to efficiently handle multiple tokens simultaneously while ensuring the output adheres to user-defined rules. As a result, DINGO significantly enhances the performance of diffusion models, achieving up to a 68 percentage point improvement in relevant tasks.'}, 'zh': {'title': 'DINGO：提升扩散模型的结构化输出能力', 'desc': 'DINGO是一种基于动态规划的解码策略，旨在增强扩散语言模型的性能。它通过强制执行结构化输出约束，显著提高了在符号数学和JSON生成任务上的表现。与传统的自回归模型不同，扩散模型能够并行预测一组标记，这使得传统的约束解码算法无法有效应用。DINGO通过高效且可证明的分布保持方法，确保生成的输出字符串符合用户指定的正则表达式，从而在标准基准测试中实现了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01674', 'title': 'MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2506.01674', 'abstract': "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advancements in Multimodal Large Language Models (MLLMs), their proficiency in fine-grained video motion understanding remains critically limited. They often lack inter-frame differencing and tend to average or ignore subtle visual cues. Furthermore, while visual prompting has shown potential in static images, its application to video's temporal complexities, particularly for fine-grained motion understanding, remains largely unexplored. We investigate whether inherent capability can be unlocked and boost MLLMs' motion perception and enable distinct visual signatures tailored to decouple object and camera motion cues. In this study, we introduce MotionSight, a novel zero-shot method pioneering object-centric visual spotlight and motion blur as visual prompts to effectively improve fine-grained motion understanding without training. To convert this into valuable data assets, we curated MotionVid-QA, the first large-scale dataset for fine-grained video motion understanding, with hierarchical annotations including SFT and preference data, {\\Theta}(40K) video clips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves state-of-the-art open-source performance and competitiveness with commercial models. In particular, for fine-grained motion understanding we present a novel zero-shot technique and a large-scale, high-quality dataset. All the code and annotations will be publicly available.", 'score': 21, 'issue_id': 4110, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'baebffe54058ae77', 'authors': ['Yipeng Du', 'Tiehan Fan', 'Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Xiang Li', 'Jian Yang', 'Zhenheng Yang', 'Ying Tai'], 'affiliations': ['ByteDance', 'Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01674.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#open_source', '#games'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на движение: MotionSight улучшает понимание видео без обучения', 'desc': 'MotionSight - это новый метод без предварительного обучения для улучшения понимания движения в видео мультимодальными большими языковыми моделями (MLLM). Он использует объектно-ориентированное визуальное выделение и размытие движения в качестве подсказок для моделей. Авторы также создали крупномасштабный датасет MotionVid-QA с иерархическими аннотациями для оценки понимания движения в видео. Эксперименты показывают, что MotionSight достигает лучших результатов среди открытых моделей и конкурентоспособен с коммерческими решениями.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Understanding with MotionSight', 'desc': 'MotionSight is a novel zero-shot method designed to enhance fine-grained video motion understanding by utilizing object-centric visual prompts and motion blur. This approach addresses the limitations of Multimodal Large Language Models (MLLMs) in capturing subtle motion cues across video frames. By introducing a large-scale dataset called MotionVid-QA, which includes hierarchical annotations, the method allows for effective evaluation and improvement of motion perception without the need for extensive training. The results demonstrate that MotionSight achieves state-of-the-art performance, showcasing its potential to unlock advanced motion understanding capabilities in videos.'}, 'zh': {'title': 'MotionSight：提升视频运动理解的新方法', 'desc': 'MotionSight是一种零样本方法，利用以物体为中心的视觉聚焦和运动模糊作为提示，显著提升了细粒度视频运动理解的能力。该方法在MotionVid-QA数据集上取得了最先进的性能，该数据集具有层次化的注释。尽管多模态大型语言模型在视频运动理解方面取得了一些进展，但仍然存在显著的局限性，尤其是在处理细微的视觉线索时。通过引入MotionSight，我们展示了如何在不进行训练的情况下，利用视觉提示来解耦物体和相机运动线索，从而提高运动感知能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03065', 'title': 'Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.03065', 'abstract': 'Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t While Diffusion Transformers (DiTs) have achieved breakthroughs in video generation, this long sequence generation task remains constrained by the quadratic complexity of attention mechanisms, resulting in significant inference latency. Through detailed analysis of attention maps in Video Diffusion Transformer (vDiT), we identify three recurring sparsity patterns: diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\% attention heads can be skipped. Crucially, these patterns exhibit strong layer-depth and head-position correlations but show limited dependence on the input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels that replace dense attention with computationally efficient implementations for each identified sparsity pattern. 2) An offline sparse diffusion search algorithm that selects the optimal sparse computation strategy per layer and head via hardware-aware cost modeling. After determining the optimal configuration, we fuse heads within the same layer that share the same attention strategy, enhancing inference efficiency. Integrated into state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1), Sparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical FLOP reduction, and actual inference speedups of 1.76times, 1.85times, and 1.58times, respectively, while maintaining high visual fidelity, with PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent structural sparsity in vDiTs can be systematically exploited for long video synthesis.', 'score': 20, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c51b4ca89c790b8c', 'authors': ['Pengtao Chen', 'Xianfang Zeng', 'Maosen Zhao', 'Peng Ye', 'Mingzhu Shen', 'Wei Cheng', 'Gang Yu', 'Tao Chen'], 'affiliations': ['Fudan University', 'Imperial College London', 'StepFun', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03065.jpg', 'data': {'categories': ['#optimization', '#architecture', '#video', '#inference', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью разреженных трансформеров', 'desc': 'Исследователи предложили метод Sparse-vDiT для ускорения Video Diffusion Transformer (vDiT), используя разреженные паттерны в картах внимания. Они выявили три повторяющихся паттерна разреженности: диагональный, мульти-диагональный и вертикально-полосатый. Sparse-vDiT включает оптимизированные разреженные ядра для каждого паттерна и алгоритм поиска оптимальной стратегии разреженных вычислений. Интеграция Sparse-vDiT в современные модели vDiT позволила значительно сократить количество операций с плавающей запятой и ускорить вывод без существенной потери качества изображения.'}, 'en': {'title': 'Accelerating Video Generation with Sparse Attention Patterns', 'desc': 'The paper introduces Sparse-vDiT, a framework designed to enhance the efficiency of Video Diffusion Transformers (vDiT) by utilizing identified sparsity patterns in attention maps. By analyzing these patterns, the authors found that certain attention heads can be skipped, leading to reduced computational complexity and faster inference times. Sparse-vDiT employs pattern-optimized sparse kernels and an offline search algorithm to determine the best sparse computation strategy for each layer and head. This approach results in significant reductions in theoretical FLOPs and actual inference speedups while preserving high visual quality in generated videos.'}, 'zh': {'title': '利用稀疏性加速视频生成的创新之路', 'desc': 'Sparse-vDiT通过利用注意力图中的稀疏模式，加速了视频扩散变换器（vDiT），在不显著降低视觉质量的情况下，减少了理论计算量（FLOPs）并提高了推理速度。研究发现，vDiT中的注意力图存在对角线、多对角线和垂直条纹等三种稀疏模式，并且可以跳过3-6%的注意力头。我们提出的Sparse-vDiT框架包括优化稀疏内核和离线稀疏扩散搜索算法，以选择每层和每个头的最佳稀疏计算策略。通过将Sparse-vDiT集成到最先进的vDiT模型中，取得了显著的计算效率提升，同时保持了高视觉保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.00070', 'title': 'Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics', 'url': 'https://huggingface.co/papers/2506.00070', 'abstract': 'Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and primitive movement reasoning.', 'score': 19, 'issue_id': 4114, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '0a2372063dd0aba7', 'authors': ['Dongyoung Kim', 'Sumin Park', 'Huiwon Jang', 'Jinwoo Shin', 'Jaehyung Kim', 'Younggyo Seo'], 'affiliations': ['KAIST', 'Real World Inc.', 'UC Berkeley', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00070.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#rl', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Robot-R1: Революция в обучении роботов через воплощенные рассуждения', 'desc': 'Статья представляет новый подход Robot-R1 для улучшения воплощенных рассуждений в робототехнике с использованием обучения с подкреплением. В отличие от традиционного метода Supervised Fine-Tuning (SFT), Robot-R1 оптимизирует модель для более точного прогнозирования следующего ключевого состояния, необходимого для выполнения задачи. Эксперименты показывают, что модели, обученные с помощью Robot-R1, превосходят методы SFT в задачах воплощенных рассуждений. Несмотря на то, что модель имеет всего 7 миллиардов параметров, она превосходит GPT-4 в задачах рассуждений, связанных с низкоуровневым управлением действиями.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Robot Control with Robot-R1', 'desc': 'This paper presents Robot-R1, a new framework that improves robot control by using reinforcement learning instead of traditional Supervised Fine-Tuning (SFT). SFT often suffers from issues like catastrophic forgetting and poorly constructed datasets, which can hinder robot performance. Robot-R1 focuses on predicting the next keypoint state needed for task completion by utilizing current scene images and expert demonstration data. The results show that Robot-R1 outperforms SFT methods, even with fewer parameters, in tasks requiring low-level action control and embodied reasoning.'}, 'zh': {'title': 'Robot-R1：提升机器人控制的具身推理新框架', 'desc': '大型视觉语言模型（LVLMs）在机器人技术中展现出巨大的潜力，通过结合具身推理与机器人控制来推动进步。传统的监督微调（SFT）方法在训练时常常使用启发式构建的数据集，这些数据集并未针对机器人控制进行优化。SFT方法还可能导致灾难性遗忘和泛化性能下降的问题。为了解决这些问题，我们提出了Robot-R1框架，利用强化学习来增强机器人控制的具身推理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.03136', 'title': 'Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.03136', 'abstract': "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE", 'score': 18, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a5362c4ed28a3b0', 'authors': ['Yinjie Wang', 'Ling Yang', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['ByteDance', 'Peking University', 'Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.03136.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#rl', '#games'], 'emoji': '🧠', 'ru': {'title': 'CURE: эволюция кодирования и тестирования без эталонов', 'desc': 'CURE - это новая система обучения с подкреплением для улучшения генерации кода и модульных тестов без использования эталонного кода. Модели ReasonFlux-Coder, созданные с помощью CURE, показывают значительное улучшение точности генерации кода по сравнению с аналогичными моделями. Система позволяет тестировщику учиться непосредственно на ошибках программиста и естественным образом распространяется на смежные задачи. Примечательно, что модель CURE может служить эффективной моделью вознаграждения для обучения с подкреплением базовых моделей.'}, 'en': {'title': 'CURE: Evolving Code and Tests Together for Better Accuracy', 'desc': "CURE is a reinforcement learning framework designed to enhance the accuracy of code and unit test generation without relying on ground-truth supervision. It features a unique reward system that allows coding and testing processes to evolve together based on their interactions. This method enables the unit tester to learn from the coder's errors, leading to improved performance in various coding tasks. The framework's models, such as ReasonFlux-Coder, show significant accuracy improvements over existing models, making it a powerful tool for developers."}, 'zh': {'title': 'CURE：无监督强化学习提升代码生成与测试准确性', 'desc': 'CURE是一个强化学习框架，旨在提高代码和单元测试生成的准确性，而无需真实标签的监督。该框架通过设计专门的奖励机制，使编码和单元测试生成能力能够相互演化，从而直接从编码者的错误中学习。经过优化后，我们的ReasonFlux-Coder模型在代码生成准确性上提高了5.3%，在最佳准确性上提高了9.0%。此外，该模型在单元测试生成中表现出64.8%的推理效率，展现了其在下游任务中的灵活性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2506.03131', 'title': 'Native-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2506.03131', 'abstract': 'A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies.', 'score': 16, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '91e9b714c6e83924', 'authors': ['Zidong Wang', 'Lei Bai', 'Xiangyu Yue', 'Wanli Ouyang', 'Yiyuan Zhang'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.03131.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#synthetic', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'NiT: Революция в генерации изображений без ограничений разрешения', 'desc': 'Представлена новая генеративная модель NiT (Native-resolution diffusion Transformer), способная синтезировать изображения высокого разрешения с различными соотношениями сторон. Модель преодолевает ограничения традиционных методов с фиксированным разрешением, обрабатывая визуальные токены переменной длины. NiT демонстрирует передовую производительность на бенчмарках ImageNet и способность к обобщению zero-shot на ранее невиданные разрешения и соотношения сторон. Эти результаты указывают на потенциал нативного моделирования разрешений как моста между визуальными генеративными моделями и продвинутыми методологиями больших языковых моделей.'}, 'en': {'title': 'Revolutionizing Image Synthesis with Native-Resolution Modeling', 'desc': 'The paper presents a new generative model called the Native-resolution diffusion Transformer (NiT) that can create high-resolution images with various aspect ratios. Unlike traditional models that work with fixed-size images, NiT can handle images of any size by using variable-length visual tokens. This model not only achieves top performance on standard image benchmarks but also shows impressive zero-shot generalization, meaning it can generate high-quality images at new resolutions without additional training. The findings suggest that NiT could significantly advance the field of image synthesis by integrating techniques from both visual generative modeling and large language models.'}, 'zh': {'title': '原生分辨率建模：图像生成的新纪元', 'desc': '本文介绍了一种新颖的生成模型——原生分辨率扩散变换器（NiT），它能够合成高分辨率和多种宽高比的图像，表现出卓越的性能和零样本泛化能力。该方法克服了传统固定分辨率图像方法的局限，通过原生处理可变长度的视觉标记，解决了传统技术的核心挑战。NiT架构专门设计用于在去噪过程中显式建模不同的分辨率和宽高比，能够从各种分辨率和宽高比的图像中学习内在的视觉分布。研究表明，NiT在多个基准测试中表现出色，能够生成高保真度的图像，展示了原生分辨率建模在视觉生成建模和先进大语言模型方法之间的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.03126', 'title': 'AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03126', 'abstract': 'AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.', 'score': 15, 'issue_id': 4114, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '3cc5928482bd8262', 'authors': ['Lu Qiu', 'Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Xihui Liu'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.03126.jpg', 'data': {'categories': ['#video', '#multimodal', '#story_generation', '#diffusion', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'AnimeShooter: новый уровень генерации связной анимации с опорными изображениями', 'desc': 'AnimeShooter - это набор данных для создания многокадровой анимации с использованием опорных изображений. Он включает иерархические аннотации и обеспечивает визуальную согласованность между кадрами. На основе этого датасета разработана модель AnimeShooterGen, использующая мультимодальные языковые модели и видео-диффузию. Эксперименты показали, что AnimeShooterGen превосходит аналоги по согласованности кадров и соответствию опорным изображениям.'}, 'en': {'title': 'Enhancing Animation with Reference-Guided Multi-Shot Datasets', 'desc': 'AnimeShooter is a new dataset designed to improve the generation of coherent multi-shot animations by providing detailed hierarchical annotations and ensuring visual consistency. It includes story-level annotations that outline the narrative and character profiles, as well as shot-level annotations that break down the story into individual scenes with specific details. The dataset also features a subset with synchronized audio tracks to enhance the animation experience. To utilize this dataset, AnimeShooterGen employs Multimodal Large Language Models (MLLMs) and video diffusion models, resulting in improved visual consistency and adherence to character references in generated animations.'}, 'zh': {'title': '提升动画生成的连贯性与一致性', 'desc': 'AnimeShooter是一个参考引导的多镜头动画数据集，旨在提高连贯的动画视频生成。该数据集通过全面的层次注释和视觉一致性，帮助生成具有叙事脚本和角色参考的动画片段。我们还推出了AnimeShooterGen，利用多模态大语言模型（MLLMs）和视频扩散模型，取得了更好的生成效果。实验结果表明，基于AnimeShooter训练的模型在跨镜头视觉一致性和遵循参考视觉指导方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.02497', 'title': 'LumosFlow: Motion-Guided Long Video Generation', 'url': 'https://huggingface.co/papers/2506.02497', 'abstract': 'LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation has gained increasing attention due to its widespread applications in fields such as entertainment and simulation. Despite advances, synthesizing temporally coherent and visually compelling long sequences remains a formidable challenge. Conventional approaches often synthesize long videos by sequentially generating and concatenating short clips, or generating key frames and then interpolate the intermediate frames in a hierarchical manner. However, both of them still remain significant challenges, leading to issues such as temporal repetition or unnatural transitions. In this paper, we revisit the hierarchical long video generation pipeline and introduce LumosFlow, a framework introduce motion guidance explicitly. Specifically, we first employ the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames with larger motion intervals, thereby ensuring content diversity in the generated long videos. Given the complexity of interpolating contextual transitions between key frames, we further decompose the intermediate frame interpolation into motion generation and post-hoc refinement. For each pair of key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes complex and large-motion optical flows, while MotionControlNet subsequently refines the warped results to enhance quality and guide intermediate frame generation. Compared with traditional video frame interpolation, we achieve 15x interpolation, ensuring reasonable and continuous motion between adjacent frames. Experiments show that our method can generate long videos with consistent motion and appearance. Code and models will be made publicly available upon acceptance. Our project page: https://jiahaochen1.github.io/LumosFlow/', 'score': 14, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c7f37e23b510618b', 'authors': ['Jiahao Chen', 'Hangjie Yuan', 'Yichen Qian', 'Jingyun Liang', 'Jiazheng Xing', 'Pengwei Liu', 'Weihua Chen', 'Fan Wang', 'Bing Su'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02497.jpg', 'data': {'categories': ['#open_source', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'LumosFlow: плавная генерация длинных видео с помощью иерархической интерполяции кадров', 'desc': 'LumosFlow - это новый подход к генерации длинных видео с использованием иерархического пайплайна. Система применяет LMTV-DM для создания ключевых кадров с большими интервалами движения, обеспечивая разнообразие контента. Затем LOF-DM синтезирует сложные оптические потоки между ключевыми кадрами, а MotionControlNet улучшает качество промежуточных кадров. LumosFlow позволяет достичь 15-кратной интерполяции, обеспечивая плавное и непрерывное движение между кадрами.'}, 'en': {'title': 'LumosFlow: Smooth Long Video Generation with Motion Guidance', 'desc': 'LumosFlow is a novel framework designed for generating long videos with smooth transitions and coherent motion. It utilizes the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to create key frames that capture significant motion, enhancing the diversity of the video content. For the interpolation of frames between these key frames, it employs the Latent Optical Flow Diffusion Model (LOF-DM) to generate complex motion flows, followed by MotionControlNet for refining the results. This approach significantly improves the quality of long video generation, achieving 15 times faster interpolation while maintaining consistent motion and appearance throughout the video.'}, 'zh': {'title': 'LumosFlow：高效生成连贯长视频的创新框架', 'desc': 'LumosFlow 是一个用于长视频生成的框架，采用 LMTV-DM 生成关键帧，并使用 LOF-DM 和 MotionControlNet 进行平滑的中间帧插值。该方法通过显式引入运动指导，解决了传统方法在生成长视频时面临的时间一致性和视觉连贯性问题。LumosFlow 通过生成具有较大运动间隔的关键帧，确保了生成视频内容的多样性。实验表明，该方法能够生成具有一致运动和外观的长视频，且插值速度比传统方法快 15 倍。'}}}, {'id': 'https://huggingface.co/papers/2506.02528', 'title': 'RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2506.02528', 'abstract': "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Inspired by the in-context learning mechanism of large language models (LLMs), a new paradigm of generalizable visual prompt-based image editing is emerging. Existing single-reference methods typically focus on style or appearance adjustments and struggle with non-rigid transformations. To address these limitations, we propose leveraging source-target image pairs to extract and transfer content-aware editing intent to novel query images. To this end, we introduce RelationAdapter, a lightweight module that enables Diffusion Transformer (DiT) based models to effectively capture and apply visual transformations from minimal examples. We also introduce Relation252K, a comprehensive dataset comprising 218 diverse editing tasks, to evaluate model generalization and adaptability in visual prompt-driven scenarios. Experiments on Relation252K show that RelationAdapter significantly improves the model's ability to understand and transfer editing intent, leading to notable gains in generation quality and overall editing performance.", 'score': 13, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'e8011f9f9decd752', 'authors': ['Yan Gong', 'Yiren Song', 'Yicheng Li', 'Chenglin Li', 'Yin Zhang'], 'affiliations': ['National University of Singapore', 'Zhe Jiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02528.jpg', 'data': {'categories': ['#cv', '#dataset', '#transfer_learning', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'RelationAdapter: Умное редактирование изображений по паре примеров', 'desc': 'RelationAdapter - это легковесный модуль, улучшающий работу моделей Diffusion Transformer для захвата и применения визуальных преобразований с использованием пар изображений источник-цель. Он вдохновлен механизмом обучения в контексте больших языковых моделей (LLM) и направлен на обобщаемое редактирование изображений на основе визуальных подсказок. RelationAdapter позволяет эффективно понимать и переносить намерения редактирования, значительно улучшая качество генерации и общую производительность редактирования. Для оценки обобщающей способности и адаптивности модели авторы также представили набор данных Relation252K, содержащий 218 разнообразных задач редактирования.'}, 'en': {'title': 'Enhancing Image Editing with RelationAdapter', 'desc': "The paper introduces RelationAdapter, a new lightweight module designed to enhance Diffusion Transformer models for image editing tasks. It focuses on using source-target image pairs to effectively capture and apply visual transformations, which improves the model's performance on diverse editing tasks. By leveraging this approach, RelationAdapter addresses the limitations of existing methods that struggle with non-rigid transformations and generalization. The authors also present Relation252K, a dataset that includes 218 diverse editing tasks to evaluate the model's adaptability and effectiveness in visual prompt-driven scenarios."}, 'zh': {'title': '提升图像编辑性能的轻量级模块', 'desc': 'RelationAdapter 是一个轻量级模块，旨在增强扩散变换器模型的能力，以有效捕捉和应用视觉变换。它通过源-目标图像对来提取和转移内容感知的编辑意图，从而改善编辑性能和在多样任务上的泛化能力。与现有的单参考方法不同，RelationAdapter 能够处理非刚性变换，提升图像编辑的灵活性。我们还引入了 Relation252K 数据集，以评估模型在视觉提示驱动场景中的泛化和适应能力。'}}}, {'id': 'https://huggingface.co/papers/2506.01144', 'title': 'FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.01144', 'abstract': "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video diffusion models are notoriously limited in their ability to model temporal aspects such as motion, physics, and dynamic interactions. Existing approaches address this limitation by retraining the model or introducing external conditioning signals to enforce temporal consistency. In this work, we explore whether a meaningful temporal representation can be extracted directly from the predictions of a pre-trained model without any additional training or auxiliary inputs. We introduce FlowMo, a novel training-free guidance method that enhances motion coherence using only the model's own predictions in each diffusion step. FlowMo first derives an appearance-debiased temporal representation by measuring the distance between latents corresponding to consecutive frames. This highlights the implicit temporal structure predicted by the model. It then estimates motion coherence by measuring the patch-wise variance across the temporal dimension and guides the model to reduce this variance dynamically during sampling. Extensive experiments across multiple text-to-video models demonstrate that FlowMo significantly improves motion coherence without sacrificing visual quality or prompt alignment, offering an effective plug-and-play solution for enhancing the temporal fidelity of pre-trained video diffusion models.", 'score': 12, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '7bd7aa77c6143afb', 'authors': ['Ariel Shaulov', 'Itay Hazan', 'Lior Wolf', 'Hila Chefer'], 'affiliations': ['School of Computer Science Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2506.01144.jpg', 'data': {'categories': ['#video', '#training', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Улучшение движения в видео без переобучения модели', 'desc': 'FlowMo - это метод без дополнительного обучения, который улучшает согласованность движения в предобученных диффузионных моделях для генерации видео по тексту. Он использует собственные предсказания модели для уменьшения покадровой временной вариативности. FlowMo извлекает временное представление, свободное от влияния внешнего вида, измеряя расстояние между латентными представлениями последовательных кадров. Затем метод оценивает согласованность движения, измеряя вариативность по временной оси, и направляет модель на уменьшение этой вариативности во время семплирования.'}, 'en': {'title': 'Enhancing Motion Coherence in Video Generation Without Training', 'desc': "FlowMo is a novel method that improves the motion coherence of pre-trained text-to-video diffusion models without requiring any additional training. It works by using the model's own predictions to create a temporal representation that captures the dynamics of motion across frames. By measuring the variance in motion across patches, FlowMo guides the model to reduce inconsistencies during the video generation process. This approach enhances the temporal fidelity of the generated videos while maintaining high visual quality and alignment with input prompts."}, 'zh': {'title': 'FlowMo：提升视频生成运动一致性的新方法', 'desc': 'FlowMo是一种无训练的方法，旨在提高预训练文本到视频扩散模型中的运动一致性。它通过利用模型自身的预测，减少了时间维度上的补丁方差，从而增强了运动的连贯性。与传统方法不同，FlowMo不需要重新训练模型或引入外部条件信号，而是直接从模型的预测中提取有意义的时间表示。实验结果表明，FlowMo在多个文本到视频模型中显著提高了运动一致性，同时保持了视觉质量和提示对齐，提供了一种有效的即插即用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2506.00910', 'title': 'PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.00910', 'abstract': 'ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation (KD) is a widely used framework for training compact, task-specific models by leveraging the knowledge of teacher models. However, its application to active learning (AL), which aims to minimize annotation costs through iterative sample selection, remains underexplored. This gap stems from the fact that KD typically assumes access to sufficient labeled data, whereas AL operates in data-scarce scenarios where task-specific teacher models are often unavailable. In this paper, we introduce ActiveKD, a framework that integrates AL with KD by leveraging the zero- and few-shot capabilities of large vision-language models (VLMs). A key aspect of ActiveKD is the structured prediction bias of VLMs -- i.e., their predictions form clusters in the probability space. We regard this structure as an inductive bias of the teacher model, capturing generalizable output patterns beneficial to student learning. To exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection strategy that maximizes coverage in the probability space rather than the feature space. PCoreSet strategically selects categorically diverse unlabeled samples, facilitating more efficient transfer of teacher knowledge under limited annotation budgets. Evaluations on 11 datasets show that PCoreSet consistently outperforms existing selection methods within the ActiveKD framework, advancing research at the intersection of AL and KD.', 'score': 10, 'issue_id': 4113, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '375d26a9868ef2fe', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Dongseop Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.00910.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#dataset', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'ActiveKD: Эффективное обучение с ограниченными данными через синергию активного обучения и дистилляции знаний', 'desc': 'ActiveKD - это новый подход, объединяющий активное обучение и дистилляцию знаний с использованием крупных визуально-языковых моделей для эффективного выбора разнообразных неразмеченных образцов для аннотации. Ключевым аспектом ActiveKD является структурированное смещение предсказаний визуально-языковых моделей, которое рассматривается как индуктивное смещение учительской модели. Для использования этого смещения предлагается стратегия Probabilistic CoreSet (PCoreSet), максимизирующая покрытие в пространстве вероятностей. Оценки на 11 наборах данных показывают, что PCoreSet стабильно превосходит существующие методы выбора в рамках ActiveKD.'}, 'en': {'title': 'Efficient Sample Selection through ActiveKD: Merging Active Learning and Knowledge Distillation', 'desc': 'ActiveKD is a novel framework that combines active learning (AL) with knowledge distillation (KD) to enhance the selection of diverse, unlabeled samples for annotation. It addresses the challenge of limited labeled data by utilizing large vision-language models (VLMs) that can perform well even with few examples. The framework introduces a selection strategy called Probabilistic CoreSet (PCoreSet), which focuses on maximizing coverage in the probability space, allowing for more effective knowledge transfer from teacher models to student models. Evaluations demonstrate that ActiveKD, through PCoreSet, significantly improves sample selection efficiency compared to traditional methods.'}, 'zh': {'title': '主动学习与知识蒸馏的高效结合', 'desc': 'ActiveKD是一个将主动学习与知识蒸馏相结合的框架，旨在高效选择多样化的未标注样本进行标注。知识蒸馏通常需要足够的标注数据，而主动学习则在数据稀缺的情况下工作，因此两者的结合尚未得到充分探索。ActiveKD利用大型视觉-语言模型的零样本和少样本能力，通过概率空间中的结构化预测偏差来优化样本选择。我们提出的概率核心集（PCoreSet）策略能够在有限的标注预算下，选择具有类别多样性的未标注样本，从而更有效地传递教师模型的知识。'}}}, {'id': 'https://huggingface.co/papers/2506.03123', 'title': 'DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation', 'url': 'https://huggingface.co/papers/2506.03123', 'abstract': 'Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient Dual-Expert Consistency Model~(DCM), where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.', 'score': 9, 'issue_id': 4122, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'b284c16ff31a205f', 'authors': ['Zhengyao Lv', 'Chenyang Si', 'Tianlin Pan', 'Zhaoxi Chen', 'Kwan-Yee K. Wong', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.03123.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#diffusion', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Эффективный синтез видео с помощью двух экспертов', 'desc': 'Авторы статьи предлагают новый подход к ускорению диффузионных моделей для синтеза видео. Они выявили проблему конфликтующей динамики обучения при дистилляции моделей согласованности, что приводит к ухудшению временной согласованности и деталей изображения. Для решения этой проблемы предложена модель Dual-Expert Consistency Model (DCM) с двумя экспертами: семантическим и детализирующим. Также введены новые функции потерь для улучшения согласованности движения и качества синтеза.'}, 'en': {'title': 'Expert Specialization for Enhanced Video Synthesis', 'desc': 'This paper addresses the challenges of using Diffusion Models for video synthesis, particularly the high computational cost due to iterative denoising steps. It highlights the limitations of applying Consistency Models directly to video diffusion, which can lead to poor temporal consistency and loss of detail. The authors propose a Dual-Expert Consistency Model (DCM) that separates the learning tasks into two experts: one for semantic understanding and motion, and another for fine detail refinement. By introducing a Temporal Coherence Loss and utilizing GAN and Feature Matching Loss, the model achieves high visual quality with fewer sampling steps, showcasing the benefits of expert specialization in improving video synthesis.'}, 'zh': {'title': '专家专注，提升视频合成质量', 'desc': '扩散模型在视频合成中取得了显著成果，但需要多次去噪步骤，导致计算开销大。一致性模型在加速扩散模型方面取得了重要进展，但直接应用于视频扩散模型时，往往会导致时间一致性和外观细节的严重退化。本文通过分析一致性模型的训练动态，识别出在蒸馏过程中存在的关键冲突学习动态，导致蒸馏学生模型无法达到最佳状态。为了解决这个问题，我们提出了一种参数高效的双专家一致性模型（DCM），通过语义专家和细节专家的专业化来提高视频扩散模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01789', 'title': "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability", 'url': 'https://huggingface.co/papers/2506.01789', 'abstract': 'High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.', 'score': 9, 'issue_id': 4111, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '08a1fd6c4eff1198', 'authors': ['Genta Indra Winata', 'David Anugraha', 'Emmy Liu', 'Alham Fikri Aji', 'Shou-Yi Hung', 'Aditya Parashar', 'Patrick Amadeus Irawan', 'Ruochen Zhang', 'Zheng-Xin Yong', 'Jan Christian Blaise Cruz', 'Niklas Muennighoff', 'Seungone Kim', 'Hanyang Zhao', 'Sudipta Kar', 'Kezia Erina Suryoraharjo', 'M. Farid Adilazuarda', 'En-Shiun Annie Lee', 'Ayu Purwarianti', 'Derry Tanti Wijaya', 'Monojit Choudhury'], 'affiliations': ['Brown University', 'Capital One', 'Carnegie Mellon University', 'Columbia University', 'ITB', 'MBZUAI', 'Monash University', 'Ontario Tech University', 'Oracle', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01789.jpg', 'data': {'categories': ['#open_source', '#data', '#synthetic', '#dataset'], 'emoji': '📊', 'ru': {'title': 'DataRubrics: Новый стандарт качества датасетов в эпоху ИИ', 'desc': 'Эта статья предлагает новый подход к оценке качества наборов данных для машинного обучения. Авторы вводят концепцию DataRubrics - структурированную систему для оценки качества как человеческих, так и сгенерированных моделями датасетов. DataRubrics использует последние достижения в оценке с помощью языковых моделей (LLM) для обеспечения воспроизводимого, масштабируемого и действенного решения. Статья также рассматривает методы синтетической генерации данных и призывает к интеграции систематических метрик оценки в процесс рецензирования датасетов.'}, 'en': {'title': 'Enhancing Dataset Quality with DataRubrics', 'desc': 'This paper discusses the importance of high-quality datasets in machine learning and the challenges in creating them, particularly regarding human annotations. It highlights issues with current dataset submissions, such as lack of originality and inadequate quality control, which are often missed during peer review. The authors propose a new framework called DataRubrics, which introduces systematic, rubric-based evaluation metrics to improve dataset quality assessment. Additionally, they explore methods for generating synthetic data and emphasize the need for reproducibility in evaluations using recent advancements in large language models (LLMs).'}, 'zh': {'title': '提升数据集质量的系统化评估框架', 'desc': '高质量的数据集对于训练和评估机器学习模型至关重要，但创建这些数据集尤其是准确的人类标注仍然是一个重大挑战。许多数据集论文缺乏原创性、多样性或严格的质量控制，且这些问题在同行评审中常常被忽视。本文提倡在数据集审查过程中整合系统化的评分标准，以提高数据质量评估的标准化和可测量性。我们还介绍了DataRubrics，一个用于评估人类和模型生成数据集质量的结构化框架，旨在提升数据驱动研究的标准。'}}}, {'id': 'https://huggingface.co/papers/2506.01716', 'title': 'Self-Challenging Language Model Agents', 'url': 'https://huggingface.co/papers/2506.01716', 'abstract': 'The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.', 'score': 5, 'issue_id': 4123, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '292019a70795a800', 'authors': ['Yifei Zhou', 'Sergey Levine', 'Jason Weston', 'Xian Li', 'Sainbayar Sukhbaatar'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.01716.jpg', 'data': {'categories': ['#optimization', '#agi', '#training', '#agents', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучение ИИ: агент бросает вызов самому себе', 'desc': 'Статья представляет новый подход к обучению интеллектуальных агентов, называемый Self-Challenging. В этой модели агент сам генерирует задачи в формате Code-as-Task, включающие инструкции, функции верификации и тестовые примеры. Затем агент обучается на этих самостоятельно созданных задачах с помощью обучения с подкреплением. Эксперименты показали, что данный метод значительно улучшает производительность модели Llama-3.1-8B-Instruct в бенчмарках по использованию инструментов, несмотря на использование только самогенерируемых данных.'}, 'en': {'title': 'Empowering Agents Through Self-Generated Challenges', 'desc': "The Self-Challenging framework enhances the training of intelligent agents by allowing them to create their own tasks, known as Code-as-Task. This approach eliminates the need for extensive human-generated tasks, as the agent generates high-quality tasks through its interactions with tools. Each task includes an instruction, a verification function, and defined success and failure cases, which help ensure the tasks are effective for training. The framework utilizes reinforcement learning to improve the agent's performance, achieving significant gains in benchmarks with only self-generated data."}, 'zh': {'title': '自我挑战，智能体训练的新方法', 'desc': '自我挑战框架通过自我生成的任务来训练智能体，这些任务被定义为代码任务，从而提高了在多轮工具使用基准测试中的表现。该框架允许智能体在与工具互动后，首先扮演挑战者角色生成高质量任务。生成的任务包括指令、验证函数以及解决方案和失败案例，确保任务的质量。最终，智能体作为执行者，利用强化学习在这些任务上进行训练，并通过评估反馈作为奖励，取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.00413', 'title': 'Accelerating Diffusion LLMs via Adaptive Parallel Decoding', 'url': 'https://huggingface.co/papers/2506.00413', 'abstract': 'Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.', 'score': 5, 'issue_id': 4113, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '42d7cf22876b9eef', 'authors': ['Daniel Israel', 'Guy Van den Broeck', 'Aditya Grover'], 'affiliations': ['Department of Computer Science University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.00413.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Статья представляет новый метод адаптивного параллельного декодирования (APD) для диффузионных больших языковых моделей (dLLM). APD динамически регулирует количество токенов, генерируемых параллельно, что позволяет значительно увеличить пропускную способность без существенного ухудшения качества. Метод использует мультипликативную смесь маргинальных вероятностей dLLM и совместной вероятности последовательностей под небольшой вспомогательной авторегрессионной моделью. APD оптимизирован с помощью KV-кэширования и ограничения размера маскированного входа, что обеспечивает гибкий компромисс между пропускной способностью и качеством.'}, 'en': {'title': 'Boosting Speed in Language Models with Adaptive Parallel Decoding', 'desc': 'Adaptive parallel decoding (APD) is a new technique that improves the speed of diffusion large language models (dLLMs) by allowing multiple tokens to be generated at the same time. Traditional methods use autoregressive decoding, which predicts tokens one after another, slowing down the process. APD changes this by dynamically adjusting how many tokens are generated in parallel, balancing speed and quality. By combining probabilities from both dLLMs and a smaller autoregressive model, APD achieves higher throughput with only slight reductions in output quality.'}, 'zh': {'title': '自适应并行解码：提升生成速度与质量的平衡', 'desc': '自适应并行解码（APD）通过动态调整并行生成的标记数量，提升了扩散大型语言模型（dLLMs）的吞吐量，而不会显著降低质量。传统的自回归解码方法使得生成速度受到限制，因为标记是一个接一个地预测的。尽管理论上扩散大型语言模型允许并行生成标记，但在实际应用中，往往难以在不牺牲质量的情况下达到自回归模型的速度。我们的方法通过定义dLLM边际概率与小型自回归模型下序列的联合概率之间的乘法混合，来实现这一目标，并通过启用KV缓存和限制掩码输入的大小进一步优化APD。'}}}, {'id': 'https://huggingface.co/papers/2505.22704', 'title': 'Training Language Models to Generate Quality Code with Program Analysis\n  Feedback', 'url': 'https://huggingface.co/papers/2505.22704', 'abstract': 'A reinforcement learning framework improves code quality in large language models by using automated feedback from program analysis and unit tests.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, a reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality.', 'score': 5, 'issue_id': 4128, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '795ffdf04386035f', 'authors': ['Feng Yao', 'Zilong Wang', 'Liyuan Liu', 'Junxia Cui', 'Li Zhong', 'Xiaohan Fu', 'Haohui Mai', 'Vish Krishnan', 'Jianfeng Gao', 'Jingbo Shang'], 'affiliations': ['CausalFlow Inc.', 'Microsoft Research', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.22704.jpg', 'data': {'categories': ['#security', '#rl', '#dataset', '#training', '#optimization'], 'emoji': '🔧', 'ru': {'title': 'Обучение с подкреплением для повышения качества кода от LLM', 'desc': 'Предложена система REAL, использующая обучение с подкреплением для улучшения качества кода, генерируемого большими языковыми моделями (LLM). REAL применяет автоматизированную обратную связь от анализа программ и модульных тестов для стимулирования LLM генерировать код производственного качества. Система фокусируется на улучшении безопасности и поддерживаемости кода, не требуя ручного вмешательства. Эксперименты показывают, что REAL превосходит современные методы по функциональности и качеству кода.'}, 'en': {'title': 'Enhancing Code Quality with Reinforcement Learning', 'desc': 'This paper presents REAL, a reinforcement learning framework designed to enhance the quality of code generated by large language models (LLMs). It addresses common issues in code generation, such as security vulnerabilities and maintainability problems, by utilizing automated feedback from program analysis and unit tests. Unlike traditional methods that require extensive manual annotations, REAL operates in a prompt-agnostic manner, allowing for scalable and efficient supervision. Experimental results show that REAL significantly improves both functionality and code quality compared to existing techniques.'}, 'zh': {'title': '强化学习提升代码质量，助力高效编程', 'desc': '本文提出了一种名为REAL的强化学习框架，旨在提高大型语言模型生成代码的质量。通过自动化反馈，REAL结合了程序分析和单元测试，帮助模型识别安全性和可维护性缺陷。与传统的监督微调和基于规则的后处理方法不同，REAL不依赖于人工标注，具有更好的可扩展性。实验结果表明，REAL在功能性和代码质量的评估上优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.01274', 'title': 'ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01274', 'abstract': "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility.", 'score': 4, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '401bd39fc17a47b7', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2506.01274.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#video', '#optimization', '#rl', '#benchmark'], 'emoji': '🎞️', 'ru': {'title': 'Умный выбор кадров для лучшего понимания видео ИИ', 'desc': 'ReFoCUS - это новый подход к оптимизации выбора кадров для видео-LLM с использованием обучения с подкреплением. Он улучшает способность модели рассуждать о видеоконтенте, выбирая кадры в соответствии с предпочтениями самой модели. ReFoCUS использует автореггрессивную архитектуру для эффективного исследования пространства кадров и не требует явной разметки на уровне кадров. Этот метод последовательно улучшает производительность в задачах видео-вопросов и ответов на нескольких бенчмарках.'}, 'en': {'title': 'Optimizing Frame Selection for Better Video Reasoning', 'desc': "ReFoCUS is a novel framework that uses reinforcement learning to improve how video-LLMs select frames for video question answering (QA). Instead of relying on fixed rules or external systems, it learns which frames are most relevant to the questions being asked. By optimizing frame selection based on the model's preferences, ReFoCUS enhances the reasoning capabilities of video-LLMs. This method not only simplifies the selection process but also boosts performance on various video QA tasks without needing detailed supervision for each frame."}, 'zh': {'title': '优化视频帧选择，提升推理能力', 'desc': 'ReFoCUS是一种利用强化学习优化视频-大语言模型（video-LLM）帧选择的方法，旨在提高视频问答中的推理性能。该方法通过学习帧选择策略，关注视觉输入的选择，而不是仅仅依赖文本响应。ReFoCUS使用来自参考大多模态模型的奖励信号，反映模型对支持时间相关响应的最佳帧的偏好。通过自回归条件选择架构，ReFoCUS有效探索帧空间，同时保持时间一致性，显著提升了多个视频问答基准的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.03096', 'title': 'FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens', 'url': 'https://huggingface.co/papers/2506.03096', 'abstract': 'Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '6a94488665c463be', 'authors': ['Christian Schlarmann', 'Francesco Croce', 'Nicolas Flammarion', 'Matthias Hein'], 'affiliations': ['Tübingen AI Center University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03096.jpg', 'data': {'categories': ['#architecture', '#dataset', '#games', '#alignment', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Ранняя фьюжн для мультимодального ИИ', 'desc': 'FuseLIP - это новая архитектура для мультимодального встраивания, использующая единую трансформерную модель для обработки текстовых и изображенческих токенов. В отличие от традиционных подходов с поздним слиянием, FuseLIP позволяет модальностям взаимодействовать на каждом уровне кодирования. Авторы собрали новые наборы данных для предобучения и оценки модели. FuseLIP превосходит другие подходы в задачах мультимодального встраивания, таких как визуальные вопросы и ответы, сохраняя сопоставимую производительность в одномодальных задачах.'}, 'en': {'title': 'FuseLIP: Unifying Text and Image with Early Fusion for Better Multimodal Understanding', 'desc': 'This paper introduces FuseLIP, a novel architecture for multimodal embedding that combines text and image features using a single transformer model. Unlike traditional methods that merge features from separate encoders, FuseLIP employs an early fusion strategy, allowing text and image tokens to interact throughout the encoding process. This results in richer representations and improved performance on multimodal tasks like visual question answering (VQA) and text-guided image retrieval. The authors also present new datasets for training and evaluating multimodal models, demonstrating that FuseLIP outperforms existing methods in multimodal scenarios while maintaining competitive results in unimodal tasks.'}, 'zh': {'title': 'FuseLIP：多模态嵌入的新方法', 'desc': '这篇论文介绍了一种新的多模态嵌入架构FuseLIP，旨在改进文本和图像的特征对齐。与传统的后期融合方法不同，FuseLIP采用早期融合策略，通过单一的变换器模型处理扩展的文本和图像标记词汇。这样可以在编码的每个深度上实现不同模态的交互，从而获得更丰富的表示。实验结果表明，FuseLIP在多模态嵌入任务中表现优于其他方法，同时在单模态任务上与基线模型相当。'}}}, {'id': 'https://huggingface.co/papers/2506.03079', 'title': 'ORV: 4D Occupancy-centric Robot Video Generation', 'url': 'https://huggingface.co/papers/2506.03079', 'abstract': 'ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Acquiring real-world robotic simulation data through teleoperation is notoriously time-consuming and labor-intensive. Recently, action-driven generative models have gained widespread adoption in robot learning and simulation, as they eliminate safety concerns and reduce maintenance efforts. However, the action sequences used in these methods often result in limited control precision and poor generalization due to their globally coarse alignment. To address these limitations, we propose ORV, an Occupancy-centric Robot Video generation framework, which utilizes 4D semantic occupancy sequences as a fine-grained representation to provide more accurate semantic and geometric guidance for video generation. By leveraging occupancy-based representations, ORV enables seamless translation of simulation data into photorealistic robot videos, while ensuring high temporal consistency and precise controllability. Furthermore, our framework supports the simultaneous generation of multi-view videos of robot gripping operations - an important capability for downstream robotic learning tasks. Extensive experimental results demonstrate that ORV consistently outperforms existing baseline methods across various datasets and sub-tasks. Demo, Code and Model: https://orangesodahub.github.io/ORV', 'score': 3, 'issue_id': 4115, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '2bfb7d794c03a7ed', 'authors': ['Xiuyu Yang', 'Bohan Li', 'Shaocong Xu', 'Nan Wang', 'Chongjie Ye', 'Zhaoxi Chen', 'Minghan Qin', 'Yikang Ding', 'Xin Jin', 'Hang Zhao', 'Hao Zhao'], 'affiliations': ['AIR, Tsinghua University', 'Beijing Academy of Artificial Intelligence', 'ByteDance', 'Eastern Institute of Technology, Ningbo', 'IIIS, Tsinghua University', 'Megvii Technology', 'National University of Singapore', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.03079.jpg', 'data': {'categories': ['#games', '#robotics', '#optimization', '#video'], 'emoji': '🤖', 'ru': {'title': 'ORV: Точный контроль роботов через 4D семантическую занятость', 'desc': 'ORV - это фреймворк для генерации видео с роботами, использующий последовательности 4D семантической занятости. Он позволяет создавать фотореалистичные, согласованные во времени и точно контролируемые видео роботов. ORV улучшает существующие методы, обеспечивая более точное семантическое и геометрическое руководство для генерации видео. Фреймворк поддерживает одновременную генерацию многоракурсных видео операций захвата роботом, что важно для задач обучения роботов.'}, 'en': {'title': 'ORV: Revolutionizing Robot Video Generation with 4D Occupancy Sequences', 'desc': 'The paper introduces ORV, a framework designed for generating robot videos using 4D semantic occupancy sequences. This approach enhances the quality of video generation by providing detailed semantic and geometric information, leading to photorealistic and temporally consistent outputs. ORV addresses the limitations of previous action-driven generative models, which often struggle with control precision and generalization. The framework also allows for the generation of multi-view videos, which is beneficial for various robotic learning applications, demonstrating superior performance compared to existing methods.'}, 'zh': {'title': '以占用为中心的机器人视频生成新框架', 'desc': 'ORV是一个以占用为中心的机器人视频生成框架，利用4D语义占用序列生成逼真的、时间一致的、可精确控制的机器人视频。该方法通过细粒度的占用表示，提供更准确的语义和几何指导，从而克服了现有方法的控制精度和泛化能力不足的问题。ORV能够无缝地将仿真数据转化为高质量的机器人视频，并支持同时生成多视角的视频，适用于后续的机器人学习任务。实验结果表明，ORV在多个数据集和子任务中均优于现有的基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02338', 'title': 'One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL', 'url': 'https://huggingface.co/papers/2506.02338', 'abstract': "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.  \t\t\t\t\tAI-generated summary \t\t\t\t With the release of R1, a publicly available large reasoning model (LRM), researchers commonly train new LRMs by training language models on R1's long chain-of-thought (CoT) inferences. While prior works show that LRMs' capabilities can be reproduced through direct distillation, the continued reliance on the existing models (e.g., R1) remains a critical limitation in advancing the field. As a first step toward independent LRM development, this paper explores the possibility of constructing a long CoT dataset with LLMs that are not trained for inference-time scaling. To this end, we present the Long CoT Collection, a dataset of 100K CoT rationales annotated using existing short CoT LLMs. We develop a pipeline that induces o1's novel reasoning strategies into short CoT LLMs, enabling them to think longer and introducing controllability over the thought budget to better manage the overthinking problem. Our extensive analyses validate that our dataset achieves quality comparable to--or slightly below--R1. Furthermore, our experiments demonstrate that training on our dataset not only strengthens general reasoning skills, but also provides a strong foundation for reinforcement learning--models initialized on our data achieve 2-3x larger gains with RLVR.", 'score': 3, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'c96bff52eb2a678f', 'authors': ['Hyungjoo Chae', 'Dongjin Kang', 'Jihyuk Kim', 'Beong-woo Kwak', 'Sunghyun Park', 'Haeju Park', 'Jinyoung Yeo', 'Moontae Lee', 'Kyungjae Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Chicago', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02338.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Длинные цепочки мышления для ИИ: новый подход к обучению моделей рассуждениям', 'desc': 'Статья представляет набор данных Long CoT Collection, созданный с помощью коротких моделей цепочек рассуждений (CoT LLM). Этот датасет содержит 100 тысяч примеров рассуждений и позволяет обучать модели длинным цепочкам мышления. Авторы разработали конвейер, который улучшает стратегии рассуждений коротких моделей и позволяет контролировать объем мыслительного процесса. Эксперименты показывают, что обучение на этом наборе данных улучшает навыки рассуждений и создает хорошую основу для обучения с подкреплением.'}, 'en': {'title': 'Empowering Reasoning with Long CoT Datasets', 'desc': 'The Long CoT Collection dataset is designed to improve reasoning skills in language models by using short chain-of-thought (CoT) models to generate long CoT inferences. This dataset consists of 100,000 annotated rationales that help models think more deeply and manage their reasoning process better. By training on this dataset, models can achieve reasoning quality similar to the established R1 model while also enhancing their performance in reinforcement learning tasks. The research highlights the potential for developing independent large reasoning models without relying solely on existing ones.'}, 'zh': {'title': '提升推理能力的长链思维数据集', 'desc': '本文介绍了一个名为Long CoT Collection的数据集，该数据集由短链思维（CoT）的大型语言模型（LLM）生成，旨在提升一般推理能力。研究表明，通过使用该数据集进行训练，可以获得与现有大型推理模型（如R1）相当的推理质量。我们开发了一种新方法，使短链思维模型能够进行更长时间的推理，并引入了对思维预算的可控性，以解决过度思考的问题。实验结果显示，基于该数据集训练的模型在强化学习任务中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2506.00391', 'title': 'SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL', 'url': 'https://huggingface.co/papers/2506.00391', 'abstract': 'SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current self-correction approaches in text-to-SQL face two critical limitations: 1) Conventional self-correction methods rely on recursive self-calls of LLMs, resulting in multiplicative computational overhead, and 2) LLMs struggle to implement effective error detection and correction for declarative SQL queries, as they fail to demonstrate the underlying reasoning path. In this work, we propose SHARE, an SLM-based Hierarchical Action corREction assistant that enables LLMs to perform more precise error localization and efficient correction. SHARE orchestrates three specialized Small Language Models (SLMs) in a sequential pipeline, where it first transforms declarative SQL queries into stepwise action trajectories that reveal underlying reasoning, followed by a two-phase granular refinement. We further propose a novel hierarchical self-evolution strategy for data-efficient training. Experimental results demonstrate that SHARE effectively enhances self-correction capabilities while proving robust across various LLMs. Furthermore, our comprehensive analysis shows that SHARE maintains strong performance even in low-resource training settings, which is particularly valuable for text-to-SQL applications with data privacy constraints.', 'score': 3, 'issue_id': 4122, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2e0ac7f48f4b8959', 'authors': ['Ge Qu', 'Jinyang Li', 'Bowen Qin', 'Xiaolong Li', 'Nan Huo', 'Chenhao Ma', 'Reynold Cheng'], 'affiliations': ['BAAI', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.00391.jpg', 'data': {'categories': ['#low_resource', '#training', '#optimization', '#dataset', '#reasoning', '#small_models', '#data'], 'emoji': '🔍', 'ru': {'title': 'Точная коррекция SQL с помощью иерархического анализа действий', 'desc': 'SHARE - это система коррекции действий на основе SLM, которая улучшает работу больших языковых моделей в задаче преобразования текста в SQL. Она трансформирует SQL-запросы в траектории действий и применяет процесс детальной доработки. SHARE использует три специализированные малые языковые модели в последовательном конвейере для более точной локализации ошибок и эффективного исправления. Система также предлагает новую стратегию иерархической самоэволюции для эффективного обучения с ограниченными данными.'}, 'en': {'title': 'SHARE: Enhancing SQL Error Correction with Hierarchical Action Trajectories', 'desc': 'The paper introduces SHARE, a system designed to improve the self-correction capabilities of large language models (LLMs) in text-to-SQL tasks. It addresses the inefficiencies of traditional self-correction methods that rely heavily on recursive calls, which can be computationally expensive. SHARE utilizes a hierarchical approach with three specialized small language models (SLMs) to convert SQL queries into action trajectories, enhancing error detection and correction. The system also incorporates a novel training strategy that allows it to perform well even with limited data, making it suitable for applications where data privacy is a concern.'}, 'zh': {'title': '提升文本到SQL的自我纠正能力', 'desc': 'SHARE是一种基于小型语言模型（SLM）的层次化动作纠正系统，旨在提升大语言模型（LLM）在文本到SQL转换中的表现。它通过将SQL查询转化为逐步的动作轨迹，帮助LLM更好地理解和定位错误。该系统采用了两阶段的细化过程，提高了错误检测和纠正的效率。此外，SHARE还引入了一种新颖的层次自我进化策略，能够在数据资源有限的情况下进行有效训练，特别适用于有数据隐私限制的文本到SQL应用。'}}}, {'id': 'https://huggingface.co/papers/2505.24273', 'title': 'How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.24273', 'abstract': 'This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have effectively improved their reasoning abilities, particularly on mathematical and logical problems that have verifiable answers, through techniques such as supervised finetuning (SFT) and reinforcement learning (RL). Prior research indicates that RL effectively internalizes search strategies, enabling long chain-of-thought (CoT) reasoning, with backtracking emerging naturally as a learned capability. However, the precise benefits of backtracking, specifically, how significantly it contributes to reasoning improvements and the optimal extent of its use, remain poorly understood. In this work, we systematically investigate the dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc 1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self Reference. Our findings highlight that short CoT sequences used in SFT as a warm-up do have moderate contribution to RL training, compared with cold-start RL; however such contribution diminishes when tasks become increasingly difficult. Motivated by this observation, we construct synthetic datasets varying systematically in the number of backtracking steps and conduct controlled experiments to isolate the influence of either the correctness (content) or the structure (i.e., backtrack frequency). We find that (1) longer CoT with backtracks generally induce better and more stable RL training, (2) more challenging problems with larger search space tend to need higher numbers of backtracks during the SFT stage. Additionally, we demonstrate through experiments on distilled data that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Collectively, our results offer practical insights into designing optimal training strategies to effectively scale reasoning in LLMs.', 'score': 3, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '73012fc9edd07bd3', 'authors': ['Hongyi James Cai', 'Junlin Wang', 'Xiaoyin Chen', 'Bhuwan Dhingra'], 'affiliations': ['Duke University', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.24273.jpg', 'data': {'categories': ['#training', '#rl', '#synthetic', '#reasoning', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Возврат в рассуждениях: ключ к улучшению LLM', 'desc': 'Исследование изучает взаимодействие между обучением с учителем и обучением с подкреплением в больших языковых моделях (LLM), фокусируясь на роли возврата (backtracking) в улучшении способностей к рассуждению. Авторы проводят эксперименты на восьми задачах рассуждения, включая Судоку и геометрические головоломки. Результаты показывают, что более длинные цепочки рассуждений с возвратами обычно приводят к лучшему и более стабильному обучению с подкреплением. Исследование также демонстрирует, что обучение с подкреплением в основном не зависит от правильности длинных последовательностей рассуждений, а скорее от их структуры.'}, 'en': {'title': 'Enhancing Reasoning in LLMs through Backtracking and Training Synergy', 'desc': 'This paper explores how supervised fine-tuning (SFT) and reinforcement learning (RL) work together to improve reasoning in large language models (LLMs). It specifically examines the role of backtracking, a technique that allows models to revisit previous steps in their reasoning process, and how it enhances performance on various reasoning tasks. The study finds that while short chain-of-thought (CoT) sequences help in RL training, their effectiveness decreases with task difficulty. Ultimately, the research provides valuable insights into optimizing training strategies for better reasoning capabilities in LLMs.'}, 'zh': {'title': '优化推理能力的训练策略', 'desc': '本研究探讨了监督微调与强化学习在大型语言模型中的相互作用，重点关注回溯在增强推理能力中的作用。研究表明，监督微调（SFT）和强化学习（RL）可以有效提升模型在数学和逻辑问题上的推理能力。我们发现，短的链式思维（CoT）序列在SFT阶段对RL训练有一定贡献，但在任务难度增加时，这种贡献会减弱。通过实验，我们证实了回溯的频率和结构对推理训练的重要性，提供了优化训练策略的实用见解。'}}}, {'id': 'https://huggingface.co/papers/2505.18079', 'title': 'Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding', 'url': 'https://huggingface.co/papers/2505.18079', 'abstract': 'The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.', 'score': 3, 'issue_id': 4117, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6dbac78671d7d992', 'authors': ['Xiaoyi Zhang', 'Zhaoyang Jia', 'Zongyu Guo', 'Jiahao Li', 'Bin Li', 'Houqiang Li', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.18079.jpg', 'data': {'categories': ['#benchmark', '#video', '#long_context', '#reasoning', '#agents'], 'emoji': '🎥', 'ru': {'title': 'Интеллектуальный агент для глубокого анализа длинных видео', 'desc': 'Статья представляет агента Deep Video Discovery для анализа длинных видео с использованием автономной стратегии поиска на основе больших языковых моделей (LLM). Агент преодолевает ограничения в понимании длинных видео путем сегментации и применения набора инструментов для поиска в многоуровневой видеобазе данных. DVD агент использует продвинутые возможности рассуждения LLM для планирования действий, выбора инструментов и итеративного уточнения своих внутренних рассуждений. Предложенный подход достигает лучших результатов на бенчмарках для понимания длинных видео, значительно превосходя предыдущие работы на наборе данных LVBench.'}, 'en': {'title': 'Revolutionizing Long-Form Video Understanding with Autonomous Agents', 'desc': 'The Deep Video Discovery (DVD) agent introduces an innovative approach to long-form video understanding by utilizing an autonomous agentic search strategy powered by large language models (LLMs). This method addresses the challenges posed by the complexity of temporal and spatial information in lengthy videos, which traditional models struggle to analyze effectively. By segmenting videos and employing a search-centric toolkit, the DVD agent can dynamically plan and refine its reasoning based on real-time observations. The results demonstrate that this system achieves state-of-the-art performance on benchmarks like LVBench, significantly outperforming previous methods.'}, 'zh': {'title': '自主搜索，深度理解长视频', 'desc': '深度视频发现代理使用自主搜索策略，结合大型语言模型，克服了长视频理解中的局限性。该方法通过对分段视频片段进行智能搜索，提升了在复杂时空背景下的问题回答能力。与以往手动设计工作流程的视频代理不同，我们的代理强调自主性，利用多层次视频数据库中的搜索工具进行规划和选择。经过全面评估，我们的代理在长视频理解基准测试中表现出色，显著超越了之前的研究成果。'}}}, {'id': 'https://huggingface.co/papers/2506.03119', 'title': 'Controllable Human-centric Keyframe Interpolation with Generative Prior', 'url': 'https://huggingface.co/papers/2506.03119', 'abstract': 'Existing interpolation methods use pre-trained video diffusion priors to generate intermediate frames between sparsely sampled keyframes. In the absence of 3D geometric guidance, these methods struggle to produce plausible results for complex, articulated human motions and offer limited control over the synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human guidance signals into the diffusion process for Controllable Human-centric Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for interpolation, our PoseFuse3D, a 3D-informed control model, features a novel SMPL-X encoder that transforms 3D geometry and shape into the 2D latent conditioning space, alongside a fusion network that integrates these 3D cues with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset annotated with both 2D poses and 3D SMPL-X parameters. We show that PoseFuse3D-KI consistently outperforms state-of-the-art baselines on CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS. Comprehensive ablations demonstrate that our PoseFuse3D model improves interpolation fidelity.', 'score': 2, 'issue_id': 4125, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '1fe3ed71536b27c9', 'authors': ['Zujin Guo', 'Size Wu', 'Zhongang Cai', 'Wei Li', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.03119.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#3d', '#cv'], 'emoji': '🕺', 'ru': {'title': '3D-управляемая интерполяция видео для реалистичной анимации человека', 'desc': 'PoseFuse3D-KI - это новая система для интерполяции ключевых кадров с человеческими движениями, использующая 3D-информацию о позе и форме тела. Она включает в себя модель PoseFuse3D, которая преобразует 3D-геометрию в 2D-пространство для управления диффузионным процессом. Авторы создали датасет CHKI-Video с 2D и 3D аннотациями для оценки качества интерполяции. Система превосходит современные аналоги, улучшая показатели PSNR на 9% и LPIPS на 38%.'}, 'en': {'title': 'Enhancing Video Frame Interpolation with 3D Human Guidance', 'desc': 'This paper presents PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI), a new method for generating intermediate video frames using 3D human motion guidance. Traditional methods struggle with complex human movements and lack control over the generated dynamics, but PoseFuse3D-KI incorporates 3D geometric signals to enhance the interpolation process. The framework utilizes a novel SMPL-X encoder to convert 3D shapes into a 2D latent space, allowing for better integration of spatial cues with 2D pose data. Evaluation on the new CHKI-Video dataset shows significant improvements in interpolation quality, outperforming existing methods in both PSNR and LPIPS metrics.'}, 'zh': {'title': 'PoseFuse3D：可控的人体关键帧插值新方法', 'desc': '本文提出了一种新的框架PoseFuse3D关键帧插值器（PoseFuse3D-KI），旨在通过将3D人体引导信号整合到扩散过程中，实现可控的人体中心关键帧插值（CHKI）。与现有方法相比，PoseFuse3D-KI能够更好地处理复杂的人体运动，并提供更高的合成动态控制能力。该框架使用了一种新颖的SMPL-X编码器，将3D几何形状转换为2D潜在条件空间，并通过融合网络将这些3D线索与2D姿态嵌入结合。实验结果表明，PoseFuse3D-KI在CHKI-Video数据集上显著优于现有的基线方法，PSNR提高了9%，LPIPS减少了38%。'}}}, {'id': 'https://huggingface.co/papers/2506.02678', 'title': 'TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression', 'url': 'https://huggingface.co/papers/2506.02678', 'abstract': "Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.", 'score': 2, 'issue_id': 4124, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '7c7528bb32eeb5a4', 'authors': ['Zhong-Zhi Li', 'Xiao Liang', 'Zihao Tang', 'Lei Ji', 'Peijie Wang', 'Haotian Xu', 'Xing W', 'Haizhen Huang', 'Weiwei Deng', 'Ying Nian Wu', 'Yeyun Gong', 'Zhijiang Guo', 'Xiao Liu', 'Fei Yin', 'Cheng-Lin Liu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology (Guangzhou)', 'Institute of Automation, Chinese Academy of Sciences', 'Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.02678.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#long_context', '#optimization', '#reasoning'], 'emoji': '⚖️', 'ru': {'title': 'Эффективное рассуждение в LLM: баланс между краткостью и точностью', 'desc': 'Эта статья представляет новый метод обучения больших языковых моделей (LLM), который улучшает эффективность рассуждений без потери точности. Авторы предлагают динамическую систему обучения, основанную на балансировке весов между данными System-1 и System-2. Этот подход позволяет сократить количество выходных токенов почти на 40%, сохраняя при этом способность модели к рассуждениям. Метод был успешно протестирован на различных моделях и наборах данных разной сложности.'}, 'en': {'title': 'Efficient Reasoning in LLMs with Dynamic Training', 'desc': "This paper presents a new training method for Large Language Models (LLMs) that improves their reasoning efficiency during inference, especially for long outputs. The proposed dynamic ratio-based training pipeline balances the model's System-1 and System-2 data without needing complex data annotations. By doing so, it reduces unnecessary reasoning steps while keeping the model's reasoning abilities intact. The results show a significant reduction in output tokens by about 40%, while still achieving high accuracy across various benchmarks."}, 'zh': {'title': '动态比例训练，提升推理效率！', 'desc': '本文提出了一种基于动态比例的训练流程，旨在提高大型语言模型（LLMs）在推理时的效率，特别是在处理极长输出时。我们的方法通过平衡模型的System-1和System-2数据的权重，消除冗余推理过程，同时保持模型的推理能力。实验结果表明，该方法在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上显著减少了近40%的输出标记，同时保持了推理的准确性。我们的代码和数据将很快公开。'}}}, {'id': 'https://huggingface.co/papers/2506.02510', 'title': 'M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset', 'url': 'https://huggingface.co/papers/2506.02510', 'abstract': "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent breakthroughs in large language models (LLMs) have led to the development of new benchmarks for evaluating their performance in the financial domain. However, current financial benchmarks often rely on news articles, earnings reports, or announcements, making it challenging to capture the real-world dynamics of financial meetings. To address this gap, we propose a novel benchmark called M^3FinMeeting, which is a multilingual, multi-sector, and multi-task dataset designed for financial meeting understanding. First, M^3FinMeeting supports English, Chinese, and Japanese, enhancing comprehension of financial discussions in diverse linguistic contexts. Second, it encompasses various industry sectors defined by the Global Industry Classification Standard (GICS), ensuring that the benchmark spans a broad range of financial activities. Finally, M^3FinMeeting includes three tasks: summarization, question-answer (QA) pair extraction, and question answering, facilitating a more realistic and comprehensive evaluation of understanding. Experimental results with seven popular LLMs reveal that even the most advanced long-context models have significant room for improvement, demonstrating the effectiveness of M^3FinMeeting as a benchmark for assessing LLMs' financial meeting comprehension skills.", 'score': 2, 'issue_id': 4110, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '903bb57b5cc664ec', 'authors': ['Jie Zhu', 'Junhui Li', 'Yalong Wen', 'Xiandong Li', 'Lifan Guo', 'Feng Chen'], 'affiliations': ['Nanjing University', 'Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02510.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#science', '#multilingual', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'M³FinMeeting: многоязычный бенчмарк для оценки понимания финансовых встреч языковыми моделями', 'desc': 'Статья представляет новый бенчмарк M³FinMeeting для оценки больших языковых моделей в понимании финансовых встреч на разных языках и в разных отраслях. Бенчмарк включает тексты на английском, китайском и японском языках, охватывая различные секторы экономики по классификации GICS. M³FinMeeting содержит три задачи: суммаризацию, извлечение пар вопрос-ответ и вопросно-ответную систему. Эксперименты с семью популярными языковыми моделями показали, что даже самые продвинутые модели имеют значительный потенциал для улучшения в этой области.'}, 'en': {'title': 'M³FinMeeting: Bridging Language Gaps in Financial Comprehension', 'desc': 'The M³FinMeeting benchmark is designed to evaluate large language models (LLMs) in understanding financial meetings across multiple languages and sectors. It addresses the limitations of existing benchmarks that primarily focus on static financial documents like news articles. This new dataset supports English, Chinese, and Japanese, allowing for a broader assessment of multilingual financial discussions. Additionally, it includes tasks such as summarization and question answering, highlighting the need for LLMs to improve their comprehension of dynamic financial interactions.'}, 'zh': {'title': 'M³FinMeeting：金融会议理解的新基准', 'desc': 'M³FinMeeting是一个新的多语言、多行业和多任务的基准，旨在评估大型语言模型在理解金融会议方面的表现。该基准支持英语、中文和日语，增强了对不同语言环境中金融讨论的理解。它涵盖了全球行业分类标准（GICS）定义的多个行业，确保基准能够覆盖广泛的金融活动。通过对七种流行的大型语言模型进行实验，结果显示即使是最先进的长上下文模型在理解能力上仍有很大提升空间，证明了M³FinMeeting作为评估大型语言模型金融会议理解能力的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.02454', 'title': 'Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework', 'url': 'https://huggingface.co/papers/2506.02454', 'abstract': 'A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '9220f780a7fba411', 'authors': ['Zhaorui Yang', 'Bo Pan', 'Han Wang', 'Yiyao Wang', 'Xingyu Liu', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['State Key Lab of CAD&CG, Zhejiang University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02454.jpg', 'data': {'categories': ['#agents', '#multimodal', '#reasoning', '#optimization', '#games', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'Мультимодальные отчеты: новый уровень генерации контента с помощью ИИ', 'desc': 'Новая система Multimodal DeepResearcher позволяет большим языковым моделям (LLM) создавать качественные мультимодальные отчеты, сочетающие текст и разнообразные визуализации. Система использует структурированное текстовое представление для описания визуализаций, что позволяет LLM эффективно работать с графиками. Процесс генерации разбит на четыре этапа: исследование, создание примера текста отчета, планирование и генерация мультимодального отчета. Эксперименты показали значительное превосходство Multimodal DeepResearcher над базовым методом при использовании одной и той же языковой модели.'}, 'en': {'title': 'Revolutionizing Reports: Text Meets Visualization with Multimodal DeepResearcher', 'desc': 'The paper introduces Multimodal DeepResearcher, a framework that allows Large Language Models (LLMs) to create detailed reports that combine text and various visualizations. It addresses the challenge of integrating informative visualizations with text, which has been largely overlooked in previous research. The framework uses a structured representation called Formal Description of Visualization (FDV) to help LLMs generate high-quality visual content. Through a systematic approach involving research, report creation, planning, and multimodal generation, the framework shows significant improvements in report quality compared to existing methods.'}, 'zh': {'title': '多模态深度研究者：文本与可视化的完美结合', 'desc': '本论文提出了一种新的框架，称为多模态深度研究者（Multimodal DeepResearcher），旨在使大型语言模型（LLMs）能够生成高质量的多模态报告，这些报告结合了文本和多种可视化形式。该框架通过结构化的文本表示，解决了文本与可视化内容交织生成的挑战，提升了信息传达的有效性。我们引入了可视化的正式描述（FDV），使得LLMs能够学习并生成多样化的高质量可视化图表。通过四个阶段的任务分解，该框架展示了其在生成多模态报告方面的有效性，实验结果表明其在性能上优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2506.02295', 'title': 'QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation', 'url': 'https://huggingface.co/papers/2506.02295', 'abstract': 'Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.  \t\t\t\t\tAI-generated summary \t\t\t\t The inherent complexities of Arabic script; its cursive nature, diacritical marks (tashkeel), and varied typography, pose persistent challenges for Optical Character Recognition (OCR). We present Qari-OCR, a series of vision-language models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets. Our leading model, QARI v0.2, establishes a new open-source state-of-the-art with a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling of tashkeel, diverse fonts, and document layouts, alongside impressive performance on low-resolution images. Further explorations (QARI v0.3) showcase strong potential for structural document understanding and handwritten text. This work delivers a marked improvement in Arabic OCR accuracy and efficiency, with all models and datasets released to foster further research.', 'score': 2, 'issue_id': 4119, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'b031659260f990f9', 'authors': ['Ahmed Wasfy', 'Omer Nacar', 'Abdelakreem Elkhateb', 'Mahmoud Reda', 'Omar Elshehy', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['KAND CA Corp.', 'NAMAA', 'Prince Sultan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02295.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#dataset', '#synthetic', '#cv', '#open_source', '#optimization'], 'emoji': '📚', 'ru': {'title': 'Прорыв в распознавании арабского текста с помощью глубокого обучения', 'desc': 'Qari-OCR представляет собой серию моделей компьютерного зрения и обработки естественного языка, оптимизированных для оптического распознавания арабского текста. Модели были итеративно дообучены на специализированных синтетических датасетах, что позволило достичь наилучших результатов среди открытых систем в задаче OCR для арабского языка. Qari-OCR успешно справляется со сложностями арабской письменности, включая диакритические знаки, разнообразие шрифтов и макетов документов. Модели демонстрируют высокую эффективность даже при работе с изображениями низкого разрешения.'}, 'en': {'title': 'Revolutionizing Arabic OCR with Qari-OCR', 'desc': 'Qari-OCR is a series of advanced vision-language models specifically designed to improve Optical Character Recognition (OCR) for Arabic text. It addresses the unique challenges of Arabic script, such as its cursive nature and diacritical marks, by using iterative fine-tuning on specialized datasets. The leading model, QARI v0.2, achieves impressive metrics with a Word Error Rate (WER) of 0.160 and a Character Error Rate (CER) of 0.061, setting a new benchmark in the field. This work not only enhances OCR accuracy for Arabic but also supports further research by making all models and datasets publicly available.'}, 'zh': {'title': 'Qari-OCR：阿拉伯语OCR的新突破', 'desc': 'Qari-OCR是一系列经过微调的视觉-语言模型，专门针对阿拉伯语光学字符识别（OCR）进行优化。该模型通过在特定的合成数据集上进行迭代微调，成功处理了阿拉伯语的连写特性、元音符号和多样的字体布局。我们的主要模型QARI v0.2在处理含有元音符号的文本时，达到了0.160的字错误率（WER）和0.061的字符错误率（CER），并在BLEU评分上取得了0.737的优异成绩。Qari-OCR在低分辨率图像的处理上也表现出色，为阿拉伯语OCR的准确性和效率带来了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01565', 'title': 'Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation', 'url': 'https://huggingface.co/papers/2506.01565', 'abstract': 'Culture is a rich and dynamic domain that evolves across both geography and time. However, existing studies on cultural understanding with vision-language models (VLMs) primarily emphasize geographic diversity, often overlooking the critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning ancient Chinese dynasties, serves as a representative cultural heritage that reflects the profound temporal aspects of Chinese culture while remaining highly popular in Chinese contemporary society. Hanfu-Bench comprises two core tasks: cultural visual understanding and cultural image transcreation.The former task examines temporal-cultural feature recognition based on single- or multi-image inputs through multiple-choice visual question answering, while the latter focuses on transforming traditional attire into modern designs through cultural element inheritance and modern context adaptation. Our evaluation shows that closed VLMs perform comparably to non-experts on visual cutural understanding but fall short by 10\\% to human experts, while open VLMs lags further behind non-experts. For the transcreation task, multi-faceted human evaluation indicates that the best-performing model achieves a success rate of only 42\\%. Our benchmark provides an essential testbed, revealing significant challenges in this new direction of temporal cultural understanding and creative adaptation.', 'score': 2, 'issue_id': 4117, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '0251c50d35bd4a50', 'authors': ['Li Zhou', 'Lutong Yu', 'Dongchu Xie', 'Shaohuan Cheng', 'Wenyan Li', 'Haizhou Li'], 'affiliations': ['Chengdu Technological University', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen', 'University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2506.01565.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark'], 'emoji': '👘', 'ru': {'title': 'Оценка культурного понимания и креативности ИИ через призму традиционной китайской одежды', 'desc': 'Статья представляет Hanfu-Bench - новый мультимодальный датасет для оценки понимания культурных аспектов моделями компьютерного зрения и обработки естественного языка (VLM). Датасет фокусируется на ханьфу - традиционной китайской одежде, отражающей временные аспекты культуры. Hanfu-Bench включает задачи по визуальному пониманию культуры и культурной транскреации изображений. Результаты показывают, что современные VLM отстают от экспертов-людей в понимании культурных особенностей и креативной адаптации.'}, 'en': {'title': 'Bridging Time and Culture with Hanfu-Bench', 'desc': 'This paper introduces Hanfu-Bench, a new dataset designed to enhance the understanding of cultural evolution over time using vision-language models (VLMs). It focuses on Hanfu, a traditional Chinese garment, to explore both cultural visual understanding and the transformation of traditional attire into modern designs. The study reveals that while closed VLMs perform similarly to non-experts in recognizing cultural features, they still lag behind human experts. Additionally, the transcreation task shows that even the best models struggle to achieve high success rates, highlighting the challenges in temporal cultural understanding and creative adaptation.'}, 'zh': {'title': '探索时间维度的文化理解', 'desc': '本研究提出了Hanfu-Bench，这是一个新颖的多模态数据集，旨在填补视觉语言模型在文化理解中的时间维度缺失。Hanfu作为中国传统服饰，代表了深厚的文化遗产，反映了中国文化的时间特征。数据集包含两个核心任务：文化视觉理解和文化图像再创作，前者通过多选视觉问答评估时间文化特征的识别，后者则关注传统服饰向现代设计的转化。我们的评估显示，封闭式视觉语言模型在视觉文化理解上与非专家相当，但在时间文化理解和创意适应方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2506.01004', 'title': 'Motion-Aware Concept Alignment for Consistent Video Editing', 'url': 'https://huggingface.co/papers/2506.01004', 'abstract': "MoCA-Video injects semantic features from a reference image into a video object, preserving motion and visual context, and outperforms baselines using novel metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a training-free framework bridging the gap between image-domain semantic mixing and video. Given a generated video and a user-provided reference image, MoCA-Video injects the semantic features of the reference image into a specific object within the video, while preserving the original motion and visual context. Our approach leverages a diagonal denoising schedule and class-agnostic segmentation to detect and track objects in the latent space and precisely control the spatial location of the blended objects. To ensure temporal coherence, we incorporate momentum-based semantic corrections and gamma residual noise stabilization for smooth frame transitions. We evaluate MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS, and introduce a novel metric CASS (Conceptual Alignment Shift Score) to evaluate the consistency and effectiveness of the visual shifts between the source prompt and the modified video frames. Using self-constructed dataset, MoCA-Video outperforms current baselines, achieving superior spatial consistency, coherent motion, and a significantly higher CASS score, despite having no training or fine-tuning. MoCA-Video demonstrates that structured manipulation in the diffusion noise trajectory allows for controllable, high-quality video synthesis.", 'score': 2, 'issue_id': 4127, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '30605ac1bef5d7b2', 'authors': ['Tong Zhang', 'Juan C Leon Alcazar', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2506.01004.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Умное смешивание видео и изображений без обучения', 'desc': 'MoCA-Video - это безтренировочный фреймворк для внедрения семантических особенностей из эталонного изображения в видеообъект. Он использует диагональное расписание шумоподавления и сегментацию для обнаружения и отслеживания объектов в латентном пространстве. Для обеспечения временной согласованности применяются семантические коррекции на основе импульса и стабилизация остаточного шума. MoCA-Video превосходит существующие базовые модели по пространственной согласованности, согласованности движения и показателю CASS.'}, 'en': {'title': 'Seamlessly Blending Images into Videos with MoCA-Video!', 'desc': 'MoCA-Video is a framework that enhances videos by integrating semantic features from a reference image while maintaining the original motion and visual context. It operates without the need for training, using techniques like diagonal denoising and class-agnostic segmentation to accurately track and blend objects in the video. To ensure smooth transitions between frames, it employs momentum-based corrections and noise stabilization. The framework is evaluated using standard metrics and a new metric called CASS, showing significant improvements in spatial consistency and motion coherence compared to existing methods.'}, 'zh': {'title': '无训练视频合成的新突破', 'desc': 'MoCA-Video（运动感知概念对齐视频）是一种无训练框架，旨在将图像领域的语义混合与视频结合。该方法通过将参考图像的语义特征注入视频中的特定对象，同时保持原有的运动和视觉上下文。我们采用对角去噪调度和类无关分割技术来检测和跟踪潜在空间中的对象，并精确控制混合对象的空间位置。通过引入基于动量的语义修正和伽马残差噪声稳定化，我们确保了视频帧之间的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2506.00227', 'title': 'Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes', 'url': 'https://huggingface.co/papers/2506.00227', 'abstract': 'Ctrl-Crash, a controllable car crash video generation model using classifier-free guidance, achieves top performance in video quality and realism compared to existing diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.', 'score': 2, 'issue_id': 4126, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '894e903b8da58314', 'authors': ['Anthony Gosselin', 'Ge Ya Luo', 'Luis Lara', 'Florian Golemo', 'Derek Nowrouzezahrai', 'Liam Paull', 'Alexia Jolicoeur-Martineau', 'Christopher Pal'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila', 'Polytechnique Montréal', 'Samsung SAIL Montréal', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2506.00227.jpg', 'data': {'categories': ['#multimodal', '#inference', '#diffusion', '#video'], 'emoji': '🚗', 'ru': {'title': 'Управляемая генерация реалистичных видео автокатастроф для повышения безопасности дорожного движения', 'desc': 'Ctrl-Crash - это модель генерации видео автомобильных аварий, использующая управляемое обучение без классификатора. Она позволяет создавать реалистичные сценарии аварий на основе входных данных, таких как ограничивающие рамки, типы столкновений и начальный кадр. Модель превосходит существующие методы на основе диффузии по качеству видео и реалистичности. Ctrl-Crash обеспечивает точный контроль над генерацией с помощью независимо настраиваемых масштабов для каждого управляющего сигнала.'}, 'en': {'title': 'Revolutionizing Crash Simulations with Ctrl-Crash', 'desc': 'Ctrl-Crash is a novel model designed to generate realistic car crash videos using advanced video diffusion techniques. It addresses the challenge of limited accident data by allowing users to control various aspects of the crash scenarios, such as bounding boxes and crash types. By employing classifier-free guidance, the model can produce diverse outcomes from slight changes in input, enhancing its utility for traffic safety simulations. The model outperforms existing methods in both quantitative metrics and human evaluations of video quality and realism.'}, 'zh': {'title': '可控汽车碰撞视频生成的突破性进展', 'desc': 'Ctrl-Crash是一种可控的汽车碰撞视频生成模型，利用无分类器引导技术，能够生成高质量和真实感的视频。由于大多数驾驶数据集中事故事件稀缺，现有的视频扩散技术在生成真实的汽车碰撞图像方面面临挑战。该模型通过边界框、碰撞类型和初始图像帧等信号进行条件控制，支持反事实场景生成，使得输入的微小变化可以导致截然不同的碰撞结果。Ctrl-Crash在视频质量指标（如FVD和JEDi）和基于人类评估的物理真实感和视频质量方面，均达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.16994', 'title': 'R^2ec: Towards Large Recommender Models with Reasoning', 'url': 'https://huggingface.co/papers/2505.16994', 'abstract': 'A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Large recommender models have extended LLMs as powerful recommenders via encoding or item generation, and recent breakthroughs in LLM reasoning synchronously motivate the exploration of reasoning in recommendation. Current studies usually position LLMs as external reasoning modules to yield auxiliary thought for augmenting conventional recommendation pipelines. However, such decoupled designs are limited in significant resource cost and suboptimal joint optimization. To address these issues, we propose \\name, a unified large recommender model with intrinsic reasoning capabilities. Initially, we reconceptualize the model architecture to facilitate interleaved reasoning and recommendation in the autoregressive process. Subsequently, we propose RecPO, a corresponding reinforcement learning framework that optimizes \\name\\ both the reasoning and recommendation capabilities simultaneously in a single policy update; RecPO introduces a fused reward scheme that solely leverages recommendation labels to simulate the reasoning capability, eliminating dependency on specialized reasoning annotations. Experiments on three datasets with various baselines verify the effectiveness of \\name, showing relative improvements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at https://github.com/YRYangang/RRec.', 'score': 2, 'issue_id': 4111, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '6b76c655943245ca', 'authors': ['Runyang You', 'Yongqi Li', 'Xinyu Lin', 'Xin Zhang', 'Wenjie Wang', 'Wenjie Li', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16994.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Единая модель для рассуждений и рекомендаций', 'desc': 'Предложена унифицированная крупная рекомендательная модель с внутренними возможностями рассуждений. Модель использует обучение с подкреплением в рамках фреймворка RecPO для одновременной оптимизации способностей к рассуждению и рекомендациям. Архитектура модели переосмыслена для обеспечения чередующихся рассуждений и рекомендаций в авторегрессивном процессе. Эксперименты на трех наборах данных показали значительное улучшение показателей Hit@5 и NDCG@20 по сравнению с базовыми методами.'}, 'en': {'title': 'Unified Reasoning and Recommendation for Enhanced Performance', 'desc': "This paper introduces a new large recommender model that integrates reasoning capabilities directly into the recommendation process. The model, named RecPO, uses a reinforcement learning framework to optimize both reasoning and recommendation in a unified manner. By allowing these two processes to work together, the model reduces resource costs and improves performance compared to traditional methods that treat them separately. Experiments demonstrate significant improvements in recommendation metrics, showcasing the model's effectiveness in real-world applications."}, 'zh': {'title': '统一推荐模型，推理与推荐的完美结合', 'desc': '本文提出了一种统一的大型推荐模型，具备内在推理能力，能够在推荐过程中实现交错推理与推荐。我们重新构思了模型架构，使其在自回归过程中同时进行推理和推荐。为此，我们引入了RecPO，一个强化学习框架，能够在单次策略更新中同时优化推理和推荐能力。实验结果表明，该模型在多个数据集上相较于基线有显著提升，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.03144', 'title': 'MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query', 'url': 'https://huggingface.co/papers/2506.03144', 'abstract': "Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '5e57a72b2bce8a15', 'authors': ['Wei Chow', 'Yuan Gao', 'Linfeng Li', 'Xian Wang', 'Qi Xu', 'Hang Song', 'Lingdong Kong', 'Ran Zhou', 'Yi Zeng', 'Yidong Cai', 'Botian Jiang', 'Shilin Xu', 'Jiajun Zhang', 'Minghui Qiu', 'Xiangtai Li', 'Tianshu Yang', 'Siliang Tang', 'Juncheng Li'], 'affiliations': ['ByteDance Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03144.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#multilingual', '#transfer_learning', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Революция в многоязычном семантическом поиске: MERIT и Coral', 'desc': 'Статья представляет MERIT - первый многоязычный набор данных для семантического поиска с несколькими условиями. Авторы выявили ограничения существующих моделей, фокусирующихся только на глобальной семантической информации. Предложена новая структура тонкой настройки Coral, интегрирующая реконструкцию эмбеддингов и контрастное обучение. Эксперименты показывают, что Coral достигает улучшения производительности на 45.9% по сравнению с традиционными подходами на MERIT.'}, 'en': {'title': 'Revolutionizing Semantic Retrieval with MERIT and Coral', 'desc': 'This paper addresses the challenges in semantic retrieval, particularly in multilingual and multi-condition scenarios. It introduces MERIT, a new dataset with 320,000 queries across five languages and seven product categories, which highlights the limitations of current models that overlook specific query conditions. The authors propose Coral, a fine-tuning framework that enhances pre-trained multilingual language models by focusing on both global semantics and fine-grained conditional elements. Experimental results show that Coral significantly outperforms traditional methods, paving the way for improved semantic retrieval in complex environments.'}, 'zh': {'title': '开创多条件语义检索的新纪元', 'desc': '本论文探讨了语义检索在现代应用中的重要性，并指出当前研究的不足之处。现有的数据集通常只限于单一语言或单一图像，未能充分利用视觉信息的表达能力。为此，本文引入了MERIT，这是首个用于交错多条件语义检索的多语言数据集，包含320,000个查询和135,000个产品，覆盖7个不同的产品类别。我们还提出了Coral，一个新的微调框架，通过整合嵌入重建和对比学习，显著提高了模型在MERIT上的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02138', 'title': 'Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability', 'url': 'https://huggingface.co/papers/2506.02138', 'abstract': 'A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of effective explainability tools for Transformers is a crucial pursuit in deep learning research. One of the most promising approaches in this domain is Layer-wise Relevance Propagation (LRP), which propagates relevance scores backward through the network to the input space by redistributing activation values based on predefined rules. However, existing LRP-based methods for Transformer explainability entirely overlook a critical component of the Transformer architecture: its positional encoding (PE), resulting in violation of the conservation property, and the loss of an important and unique type of relevance, which is also associated with structural and positional features. To address this limitation, we reformulate the input space for Transformer explainability as a set of position-token pairs. This allows us to propose specialized theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods, including Rotary, Learnable, and Absolute PE. Extensive experiments with both fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3, demonstrate that our method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks. Our code is publicly available.', 'score': 1, 'issue_id': 4115, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'c83ce7f2cc033984', 'authors': ['Yarden Bakish', 'Itamar Zimerman', 'Hila Chefer', 'Lior Wolf'], 'affiliations': ['Tel-Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.02138.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Улучшение объяснимости трансформеров с учетом позиционного кодирования', 'desc': 'Статья представляет специализированный метод Layer-wise Relevance Propagation (LRP) для объяснимости моделей-трансформеров. Авторы учитывают позиционное кодирование, что улучшает распространение релевантности и превосходит существующие методы. Новый подход переформулирует входное пространство как набор пар позиция-токен и предлагает теоретически обоснованные правила LRP для различных методов позиционного кодирования. Эксперименты показывают, что метод значительно превосходит современные подходы в задачах объяснимости для компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Enhancing Transformer Explainability with Positional Encoding in LRP', 'desc': 'This paper introduces a specialized Layer-wise Relevance Propagation (LRP) method tailored for Transformer models, focusing on the importance of positional encoding in explainability. Traditional LRP methods fail to account for positional encoding, which leads to a loss of critical relevance information tied to the structure and position of tokens. By reformulating the input space into position-token pairs, the authors develop new LRP rules that effectively propagate relevance across different types of positional encodings. The proposed method shows significant improvements over existing techniques in explainability tasks for both vision and natural language processing, validated through extensive experiments.'}, 'zh': {'title': '提升Transformer可解释性的专门LRP方法', 'desc': '本文提出了一种针对Transformer模型的专门化层次相关传播（LRP）方法，考虑了位置编码的影响，从而改善了相关性传播。现有的LRP方法未能充分利用Transformer架构中的位置编码，导致了重要相关性的丢失。我们通过将输入空间重新构建为位置-标记对，提出了理论基础的LRP规则，以适应不同的位置信息编码方法。实验结果表明，我们的方法在视觉和自然语言处理的可解释性任务中显著优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2506.01265', 'title': 'Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines', 'url': 'https://huggingface.co/papers/2506.01265', 'abstract': 'In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers.', 'score': 1, 'issue_id': 4124, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '2f44ffeb11509c5c', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Do Xuan Trong', 'Luu Anh Tuan', 'Kenji Kawaguchi', 'Shafiq Joty', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University, Singapore', 'National University of Singapore', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.01265.jpg', 'data': {'categories': ['#data', '#training', '#long_context', '#optimization', '#multimodal'], 'emoji': '📝', 'ru': {'title': 'LongGuide: улучшение генерации длинных текстов с помощью автоматических рекомендаций', 'desc': 'Статья исследует возможности обучения по контексту (ICL) для крупных языковых моделей (LLM) в задачах генерации длинных текстов. Авторы показывают, что одних только демонстраций ICL недостаточно для обучения LLM распределениям языка и формата задачи. Они предлагают метод LongGuide, который генерирует два потока рекомендаций для улучшения производительности модели. LongGuide автоматически выбирает лучшую комбинацию рекомендаций, повышая эффективность как открытых, так и закрытых LLM.'}, 'en': {'title': 'Enhancing Long-Form Generation with LongGuide', 'desc': 'This paper explores the concept of in-context learning (ICL) in large language models (LLMs), highlighting its effectiveness in improving task performance with minimal examples. However, it identifies limitations in long-form generation tasks, such as summarization, where ICL alone does not adequately teach the necessary language and format distributions. The authors propose a solution called LongGuide, which generates guidelines to enhance model performance by providing explicit exposure to task distributions. LongGuide combines Metric Guidelines and Output Constraint Guidelines to optimize generation, resulting in significant performance improvements across various LLMs in both zero- and few-shot scenarios.'}, 'zh': {'title': '提升生成任务性能的LongGuide方法', 'desc': '本文探讨了预训练大型语言模型（LLMs）在上下文学习（ICL）中的能力，尤其是在长文本生成任务中的表现。研究表明，仅依靠示例（demonstrations）不足以让模型掌握生成任务的语言和格式分布。为此，提出了LongGuide，通过生成两条并行的指导流，帮助模型更好地理解任务要求。实验结果显示，LongGuide能显著提升模型在零样本和少样本设置下的表现，且具有良好的通用性和学习能力。'}}}, {'id': 'https://huggingface.co/papers/2505.24362', 'title': 'Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion', 'url': 'https://huggingface.co/papers/2505.24362', 'abstract': "We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well even before a single token is generated, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.", 'score': 1, 'issue_id': 4118, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bbcc31124760dad', 'authors': ['Anum Afzal', 'Florian Matthes', 'Gal Chechik', 'Yftah Ziser'], 'affiliations': ['Bar-Ilan University', 'Nvidia Research', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.24362.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Раннее предсказание успеха рассуждений в языковых моделях', 'desc': 'Исследователи изучают возможность предсказания успеха процесса рассуждений с нулевым выстрелом (zero-shot Chain-of-Thought) до его завершения. Они обнаружили, что классификатор на основе представлений языковой модели (LLM) показывает хорошие результаты даже до генерации первого токена. Это указывает на то, что ключевая информация о процессе рассуждений уже присутствует в начальных представлениях. Исследование также показало, что раннее прерывание цепочки рассуждений может улучшить производительность по сравнению с отсутствием CoT, хотя и не достигает уровня полного рассуждения.'}, 'en': {'title': 'Unlocking Early Insights in Chain-of-Thought Reasoning', 'desc': 'This paper explores the predictability of success in zero-shot Chain-of-Thought (CoT) reasoning processes before they are fully completed. The authors find that a probing classifier using representations from large language models (LLMs) can effectively predict outcomes even before generating any tokens, indicating that essential reasoning information is present early on. In contrast, a BERT-based model that relies on generated tokens performs poorly, as it focuses on superficial linguistic features rather than deeper reasoning. The study suggests that early stopping in CoT reasoning can still yield better performance than not using CoT at all, and proposes that future methods could utilize their classifier to determine when early stopping is beneficial.'}, 'zh': {'title': '优化推理效率，早期停止也能有效', 'desc': '本研究探讨了零-shot Chain-of-Thought (CoT) 过程的成功是否可以在完成之前进行预测。我们发现，基于大语言模型（LLM）表示的探测分类器在生成第一个标记之前就表现良好，这表明推理过程中的关键信息在初始步骤的表示中已经存在。相比之下，依赖生成标记的强BERT基线表现较差，可能是因为它依赖于浅层语言线索而非更深层的推理动态。我们的实验表明，早期停止推理可以提高性能，尽管与完整推理相比仍有差距，这为优化CoT的效率提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2506.15675', 'title': 'Sekai: A Video Dataset towards World Exploration', 'url': 'https://huggingface.co/papers/2506.15675', 'abstract': "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", 'score': 33, 'issue_id': 4373, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '4f989f259f55f0ee', 'authors': ['Zhen Li', 'Chuanhao Li', 'Xiaofeng Mao', 'Shaoheng Lin', 'Ming Li', 'Shitian Zhao', 'Zhaopan Xu', 'Xinyue Li', 'Yukang Feng', 'Jianwen Sun', 'Zizhen Li', 'Fanrui Zhang', 'Jiaxin Ai', 'Zhixiang Wang', 'Yuwei Wu', 'Tong He', 'Jiangmiao Pang', 'Yu Qiao', 'Yunde Jia', 'Kaipeng Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shenzhen MSU-BIT University', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.15675.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#games', '#data', '#video'], 'emoji': '🌎', 'ru': {'title': 'Sekai: глобальный датасет для обучения ИИ исследовать мир', 'desc': 'Представлен новый набор данных Sekai для обучения моделей генерации видео с целью исследования мира. Он содержит более 5000 часов видео от первого лица из более чем 100 стран с богатыми аннотациями. Разработан инструментарий для сбора, обработки и аннотирования видео. На основе подмножества данных обучена модель YUME для интерактивного исследования мира через видео.'}, 'en': {'title': 'Sekai: Unlocking the World Through Video Generation', 'desc': 'This paper presents Sekai, a new video dataset designed to enhance video generation models for world exploration applications. Sekai includes over 5,000 hours of first-person view videos from diverse locations, addressing the limitations of existing datasets by providing rich annotations such as location, scene, and weather conditions. The authors also introduce a toolbox for efficient video collection and annotation, ensuring high-quality data for training. The dataset is validated through experiments and is used to train an interactive video exploration model called YUME, showcasing its potential impact on video generation and exploration technologies.'}, 'zh': {'title': 'Sekai：全球探索的新视野', 'desc': '本文介绍了一个名为Sekai的全球视频数据集，旨在支持世界探索应用并增强视频生成模型。该数据集包含来自100多个国家和地区的5000多个小时的第一人称视角视频，涵盖750个城市，具有丰富的注释信息。Sekai解决了现有视频生成数据集在位置、时长、场景静态性和探索注释等方面的局限性。通过实验验证了数据集的质量，并使用其子集训练了一个名为YUME的互动视频世界探索模型。'}}}, {'id': 'https://huggingface.co/papers/2506.15211', 'title': 'ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs', 'url': 'https://huggingface.co/papers/2506.15211', 'abstract': 'ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Reasoning Models (LRMs) trained with Long Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain generalization capabilities. However, the underlying mechanisms supporting such transfer remain poorly understood. We hypothesize that cross-domain generalization arises from shared abstract reasoning prototypes -- fundamental reasoning patterns that capture the essence of problems across domains. These prototypes minimize the nuances of the representation, revealing that seemingly diverse tasks are grounded in shared reasoning structures.Based on this hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning ability of LLMs by leveraging scalable and verifiable prototypical representations (Prolog for logical reasoning, PDDL for planning).ProtoReasoning features: (1) an automated prototype construction pipeline that transforms problems into corresponding prototype representations; (2) a comprehensive verification system providing reliable feedback through Prolog/PDDL interpreters; (3) the scalability to synthesize problems arbitrarily within prototype space while ensuring correctness. Extensive experiments show that ProtoReasoning achieves 4.7% improvement over baseline models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics (AIME24). Significantly, our ablation studies confirm that learning in prototype space also demonstrates enhanced generalization to structurally similar problems compared to training solely on natural language representations, validating our hypothesis that reasoning prototypes serve as the foundation for generalizable reasoning in large language models.', 'score': 20, 'issue_id': 4380, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'a2ce33b1fd959b89', 'authors': ['Feng He', 'Zijun Chen', 'Xinnian Liang', 'Tingting Ma', 'Yunqi Qiu', 'Shuangzhi Wu', 'Junchi Yan'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15211.jpg', 'data': {'categories': ['#reasoning', '#transfer_learning', '#training', '#dataset', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Прототипы рассуждений как ключ к обобщению в больших языковых моделях', 'desc': 'Статья представляет ProtoReasoning - фреймворк для улучшения способностей больших языковых моделей к рассуждениям путем использования прототипических представлений. Авторы предполагают, что кросс-доменная генерализация возникает из общих абстрактных прототипов рассуждений. ProtoReasoning включает автоматизированный конвейер для построения прототипов, систему верификации и возможность масштабирования. Эксперименты показывают значительное улучшение производительности модели в задачах логического рассуждения, планирования и общих рассуждений.'}, 'en': {'title': 'Unlocking Generalization with Prototypical Reasoning', 'desc': 'ProtoReasoning is a framework designed to improve the reasoning capabilities of large language models (LLMs) by utilizing prototypical representations. It posits that shared abstract reasoning patterns, or prototypes, enable better generalization across different domains by simplifying complex tasks into fundamental structures. The framework includes an automated pipeline for creating these prototypes, a verification system for ensuring accuracy, and the ability to generate a wide range of problems within the prototype space. Experimental results show that ProtoReasoning significantly enhances performance in logical reasoning, planning, and general reasoning tasks compared to traditional methods.'}, 'zh': {'title': '原型推理：提升推理模型的跨领域能力', 'desc': 'ProtoReasoning 是一种增强大型推理模型的框架，通过原型表示来提高跨领域的推理能力。该框架假设跨领域的泛化能力源于共享的抽象推理原型，这些原型捕捉了不同领域问题的本质。ProtoReasoning 包含一个自动化的原型构建管道，将问题转化为相应的原型表示，并提供可靠的反馈系统。实验结果表明，ProtoReasoning 在逻辑推理、规划任务和一般推理上均有显著提升，验证了推理原型在大型语言模型中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.15681', 'title': 'GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2506.15681', 'abstract': 'GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.', 'score': 16, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '2d531d89420a04d8', 'authors': ['Byung-Kwan Lee', 'Ryo Hachiuma', 'Yong Man Ro', 'Yu-Chiang Frank Wang', 'Yueh-Hua Wu'], 'affiliations': ['KAIST', 'NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.15681.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#inference', '#training', '#multimodal', '#dataset', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'GenRecal: Универсальная дистилляция для эффективных визуально-языковых моделей', 'desc': 'GenRecal - это новая система дистилляции для визуально-языковых моделей (VLM), которая улучшает передачу знаний между разнородными архитектурами. Она использует Recalibrator для выравнивания и адаптации представлений признаков между различными типами VLM. Эксперименты показывают, что GenRecal значительно улучшает базовые показатели и превосходит крупномасштабные открытые и закрытые VLM. Это позволяет создавать более эффективные и компактные модели для применения в реальных сценариях с ограниченными ресурсами.'}, 'en': {'title': 'Aligning Features for Efficient Vision-Language Models', 'desc': 'GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks.'}, 'zh': {'title': 'GenRecal：提升视觉-语言模型性能的新框架', 'desc': 'GenRecal是一种新颖的知识蒸馏框架，旨在通过对不同架构的特征表示进行对齐，提升视觉-语言模型的性能。该框架解决了由于不同视觉-语言模型（VLM）架构的多样性而导致的知识转移挑战。GenRecal引入了一个重校准器，能够在异构VLM之间对特征表示进行适配，从而实现有效的知识传递。通过在多个具有挑战性的基准测试上进行广泛实验，我们证明GenRecal显著提高了基线性能，最终超越了大型开源和闭源的VLM。'}}}, {'id': 'https://huggingface.co/papers/2506.15677', 'title': 'Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence', 'url': 'https://huggingface.co/papers/2506.15677', 'abstract': 'Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.', 'score': 11, 'issue_id': 4370, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '9dff3e9d64c54e88', 'authors': ['Yining Hong', 'Rui Sun', 'Bingxuan Li', 'Xingcheng Yao', 'Maxine Wu', 'Alexander Chien', 'Da Yin', 'Ying Nian Wu', 'Zhecan James Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.15677.jpg', 'data': {'categories': ['#3d', '#benchmark', '#open_source', '#agents', '#multimodal', '#agi', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Объединение физического и цифрового миров в ИИ-агентах нового поколения', 'desc': 'Статья представляет новую парадигму искусственного интеллекта - Embodied Web Agents, которая объединяет физическое взаимодействие и веб-масштабные рассуждения. Авторы разработали симуляционную платформу, интегрирующую реалистичные 3D-среды с функциональными веб-интерфейсами. На основе этой платформы создан набор тестовых заданий Embodied Web Agents Benchmark для оценки кросс-доменного интеллекта ИИ-систем. Результаты экспериментов показывают значительный разрыв между возможностями современных ИИ-систем и человека в задачах, требующих комбинированного физического и цифрового интеллекта.'}, 'en': {'title': 'Bridging Physical and Digital Intelligence with Embodied Web Agents', 'desc': "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."}, 'zh': {'title': '具身网络代理：连接物理与数字智能的桥梁', 'desc': '本文介绍了一种新的人工智能代理模型——具身网络代理（Embodied Web Agents），它将物理交互与网络规模推理结合在一起，以评估跨领域智能。当前的AI代理通常只能在数字信息检索或物理世界交互中发挥作用，缺乏两者的整合，限制了它们解决复杂任务的能力。我们开发了具身网络代理任务环境，这是一个统一的仿真平台，能够将真实的3D环境与功能性网络接口紧密结合。通过这一平台，我们构建并发布了具身网络代理基准，涵盖了烹饪、导航、购物等多种任务，要求在物理和数字领域之间进行协调推理，以系统评估跨领域智能。'}}}, {'id': 'https://huggingface.co/papers/2506.13414', 'title': 'BUT System for the MLC-SLM Challenge', 'url': 'https://huggingface.co/papers/2506.13414', 'abstract': "The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW -- a diarization-conditioned variant of Whisper -- with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data -- such as missing speech segments and incorrect silence annotations -- which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness.", 'score': 11, 'issue_id': 4380, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'd143e6da0f164962', 'authors': ['Alexander Polok', 'Jiangyu Han', 'Dominik Klement', 'Samuele Cornell', 'Jan Černocký', 'Lukáš Burget'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University, USA', 'Speech@FIT, Brno University of Technology, Czechia'], 'pdf_title_img': 'assets/pdf/title_img/2506.13414.jpg', 'data': {'categories': ['#audio', '#training', '#multilingual', '#data'], 'emoji': '🗣️', 'ru': {'title': 'Объединение DiCoW и DiariZen: мощная многоязычная система ASR', 'desc': 'Статья представляет двухдикторную систему автоматического распознавания речи (ASR), объединяющую DiCoW и DiariZen. DiCoW - это вариант модели Whisper, обусловленный диаризацией, а DiariZen - конвейер диаризации на основе Pyannote. Система демонстрирует высокую производительность в многоязычных сценариях, причем DiCoW сохраняет свои многоязычные возможности, а DiariZen улучшается благодаря тонкой настройке. Авторы также выявили несколько несоответствий в маркировке обучающих данных и предложили стратегии для повышения надежности системы.'}, 'en': {'title': 'Enhancing Multilingual ASR with DiCoW and DiariZen', 'desc': 'This paper presents a novel automatic speech recognition (ASR) system that integrates two components: DiCoW, which is a diarization-conditioned version of Whisper, and DiariZen, a diarization pipeline based on Pyannote. The system is evaluated in multilingual scenarios, showing that DiariZen outperforms the baseline Pyannote model without fine-tuning, indicating its strong generalization capabilities. DiCoW maintains its multilingual performance even after being fine-tuned on English-only data, demonstrating the effectiveness of its encoder modifications. The final system achieves a competitive error rate and addresses labeling inconsistencies in training data to enhance robustness and performance.'}, 'zh': {'title': '多语言ASR系统的强大结合', 'desc': '本文介绍了一种结合DiCoW和DiariZen的双说话者自动语音识别（ASR）系统，展示了其在多语言场景中的强大性能。DiCoW是Whisper的一个基于说话者分离的变体，而DiariZen则是基于Pyannote构建的说话者分离管道。实验表明，在没有任何微调的情况下，DiariZen在多语言场景中表现优于基线模型Pyannote，显示出良好的泛化能力。经过微调后，DiariZen和DiCoW的性能进一步提升，最终系统在MLC-SLM挑战赛中取得了16.75%的微平均tcpWER/CER，排名第二。'}}}, {'id': 'https://huggingface.co/papers/2506.15068', 'title': 'Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation', 'url': 'https://huggingface.co/papers/2506.15068', 'abstract': 'PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl.', 'score': 9, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '3f5497d8e1350326', 'authors': ['Zongxia Li', 'Yapei Chang', 'Yuhang Zhou', 'Xiyang Wu', 'Zichao Liang', 'Yoo Yeon Sung', 'Jordan Lee Boyd-Graber'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15068.jpg', 'data': {'categories': ['#long_context', '#alignment', '#rlhf', '#benchmark', '#optimization', '#open_source', '#dataset'], 'emoji': '📝', 'ru': {'title': 'PrefBERT: Улучшение генерации длинных текстов с помощью семантической оценки', 'desc': 'PrefBERT - это модель оценки, разработанная для улучшения генерации длинных текстов с открытым окончанием. Она обеспечивает более качественную семантическую обратную связь по сравнению с традиционными метриками. PrefBERT обучен на двух наборах данных для оценки ответов с различными длинными стилями и оценками качества по шкале Ликерта. Модель эффективно поддерживает GRPO, предоставляя лучшую семантическую обратную связь, чем традиционные метрики ROUGE-L и BERTScore.'}, 'en': {'title': 'Enhancing Long-Form Generation Evaluation with PrefBERT', 'desc': 'PrefBERT is a novel scoring model designed to enhance the evaluation of open-ended long-form text generation. It addresses the limitations of traditional metrics like ROUGE-L and BERTScore, which often overlook important qualities such as coherence and relevance. By providing distinct semantic rewards for good and bad outputs, PrefBERT guides the training of generative models more effectively. Comprehensive evaluations demonstrate that PrefBERT aligns closely with human preferences, leading to higher quality generated responses.'}, 'zh': {'title': 'PrefBERT：提升开放式长文本生成的评估效果', 'desc': 'PrefBERT是一种评分模型，旨在改善开放式长文本生成的评估。与传统的评估指标相比，PrefBERT提供了更好的语义奖励反馈，能够更有效地指导生成模型的训练。该模型在两个多样化的长文本响应评估数据集上进行训练，能够识别输出的好坏，并为其提供明确的奖励信号。通过综合评估，PrefBERT在不同的长文本中表现出可靠性，并与人类偏好高度一致。'}}}, {'id': 'https://huggingface.co/papers/2506.15569', 'title': 'SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification', 'url': 'https://huggingface.co/papers/2506.15569', 'abstract': "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.", 'score': 7, 'issue_id': 4371, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': '0e3c39a143af668b', 'authors': ['Chengye Wang', 'Yifei Shen', 'Zexi Kuang', 'Arman Cohan', 'Yilun Zhao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.15569.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#science', '#benchmark', '#open_source'], 'emoji': '🧪', 'ru': {'title': 'SciVer: проверка научных утверждений мультимодальными ИИ-моделями', 'desc': 'SciVer - это новый бенчмарк для оценки способности мультимодальных фундаментальных моделей верифицировать научные утверждения. Он состоит из 3000 аннотированных экспертами примеров из 1113 научных статей, охватывающих четыре типа рассуждений. Авторы протестировали 21 современную мультимодальную модель, включая o4-mini, Gemini-2.5-Flash и другие. Результаты показали значительный разрыв в производительности между моделями и экспертами-людьми, выявив ключевые ограничения в понимании и рассуждениях моделей в задачах с мультимодальной научной литературой.'}, 'en': {'title': 'Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification', 'desc': 'The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature.'}, 'zh': {'title': '提升多模态模型的科学验证能力', 'desc': 'SciVer是一个新的基准，专门用于评估多模态基础模型在科学背景下验证声明的能力。它包含3000个专家注释的示例，涵盖1113篇科学论文，分为四个子集，代表多模态科学声明验证中的常见推理类型。我们评估了21个最先进的多模态基础模型的表现，发现这些模型与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）和人类错误评估的深入分析，我们识别了当前开源模型的关键局限性，为提高模型在多模态科学文献任务中的理解和推理能力提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2506.14842', 'title': 'PictSure: Pretraining Embeddings Matters for In-Context Learning Image\n  Classifiers', 'url': 'https://huggingface.co/papers/2506.14842', 'abstract': "PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at https://github.com/PictSure/pictsure-library.", 'score': 5, 'issue_id': 4381, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '4cf4adfc32104e33', 'authors': ['Lukas Schiesser', 'Cornelius Wolff', 'Sophie Haas', 'Simon Pukrop'], 'affiliations': ['German Research Center for AI (DFKI)'], 'pdf_title_img': 'assets/pdf/title_img/2506.14842.jpg', 'data': {'categories': ['#dataset', '#cv', '#transfer_learning', '#optimization', '#training', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Оптимизация встраиваний для улучшения классификации изображений вне домена', 'desc': 'PictSure - это фреймворк обучения в контексте для улучшения классификации изображений с малым количеством примеров. Он оптимизирует архитектуру моделей встраивания, предварительное обучение и стратегии тонкой настройки для повышения производительности вне домена. Исследование показывает, что успех обучения и производительность вне домена сильно зависят от того, как предварительно обучаются модели встраивания. PictSure превосходит существующие модели классификации изображений с обучением в контексте на тестах вне домена, сохраняя сопоставимые результаты на задачах в домене.'}, 'en': {'title': 'Enhancing Few-Shot Image Classification with PictSure', 'desc': 'PictSure is a framework designed to improve few-shot image classification (FSIC) by focusing on the optimization of embedding models. It emphasizes the importance of the architecture, pretraining, and fine-tuning strategies of these models to enhance their performance, especially in out-of-domain scenarios. The research highlights that the success of training and the ability to generalize to new domains are closely linked to how well the embedding models are pretrained. By systematically analyzing various visual encoders and training methods, PictSure demonstrates superior performance compared to existing models in challenging out-of-domain benchmarks while still performing well in familiar tasks.'}, 'zh': {'title': 'PictSure：提升少样本图像分类的上下文学习框架', 'desc': 'PictSure是一个上下文学习框架，旨在通过优化嵌入模型的架构、预训练和微调策略来增强少样本图像分类的性能。该框架关注图像嵌入在少样本图像分类中的重要性，系统地研究不同视觉编码器类型、预训练目标和微调策略对下游任务的影响。实验结果表明，嵌入模型的预训练方式对训练成功和域外性能有很大影响。PictSure在与训练分布显著不同的域外基准测试中表现优于现有的基于上下文学习的少样本图像分类模型，同时在域内任务上保持了相当的结果。'}}}, {'id': 'https://huggingface.co/papers/2506.15672', 'title': 'SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence', 'url': 'https://huggingface.co/papers/2506.15672', 'abstract': 'SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.', 'score': 4, 'issue_id': 4376, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'e6b93c3f506d4979', 'authors': ['Yao Zhang', 'Chenyang Lin', 'Shijie Tang', 'Haokun Chen', 'Shijie Zhou', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2506.15672.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#games', '#agi', '#open_source', '#alignment'], 'emoji': '🐝', 'ru': {'title': 'Автономная эволюция многоагентных систем', 'desc': 'SwarmAgentic - это фреймворк для полностью автоматизированной генерации агентных систем. Он оптимизирует функциональность агентов и их взаимодействие через языковое исследование, превосходя существующие базовые модели в неограниченных задачах. SwarmAgentic создает агентные системы с нуля и совместно оптимизирует функциональность агентов и их сотрудничество как взаимозависимые компоненты. Фреймворк использует принципы роевого интеллекта для эффективного поиска оптимальных структур системного уровня.'}, 'en': {'title': 'Revolutionizing Agentic Systems with SwarmAgentic', 'desc': 'SwarmAgentic is a novel framework designed for the automated generation of agentic systems, which are capable of decision-making and collaboration. It addresses the limitations of existing frameworks by enabling the creation of agents from scratch and optimizing their functionality and teamwork through language-driven exploration. The framework utilizes a population-based approach inspired by Particle Swarm Optimization (PSO) to evolve candidate systems based on feedback. In evaluations, SwarmAgentic demonstrated significant improvements in performance on various complex tasks, showcasing its potential for scalable and autonomous agentic system design.'}, 'zh': {'title': '全自动化代理系统生成的未来', 'desc': 'SwarmAgentic是一个自动化代理系统生成框架，旨在通过语言驱动的探索来优化代理的功能和协作。与现有的代理系统生成框架相比，SwarmAgentic能够从零开始生成代理，并自我优化其功能和协作能力。该框架通过维护候选系统的种群并进行反馈引导的更新，借鉴了粒子群优化（PSO）的思想，从而实现高效的系统结构搜索。在多个真实世界的任务中，SwarmAgentic表现出色，显著超越了现有基准，展示了全自动化在无约束任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.15050', 'title': 'Truncated Proximal Policy Optimization', 'url': 'https://huggingface.co/papers/2506.15050', 'abstract': 'T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.', 'score': 4, 'issue_id': 4374, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'b986b577a01d9bf5', 'authors': ['Tiantian Fan', 'Lingjun Liu', 'Yu Yue', 'Jiaze Chen', 'Chengyi Wang', 'Qiying Yu', 'Chi Zhang', 'Zhiqi Lin', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Bole Ma', 'Mofan Zhang', 'Gaohong Liu', 'Ru Zhang', 'Haotian Zhou', 'Cong Xie', 'Ruidong Zhu', 'Zhi Zhang', 'Xin Liu', 'Mingxuan Wang', 'Lin Yan', 'Yonghui Wu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2506.15050.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🚀', 'ru': {'title': 'T-PPO: Ускоряем обучение LLM в 2,5 раза', 'desc': 'T-PPO - это новое расширение алгоритма PPO, которое повышает эффективность обучения больших языковых моделей (LLM). Оно оптимизирует обновление политики и более эффективно использует аппаратные ресурсы. T-PPO вводит расширенную обобщенную оценку преимущества (EGAE) для оценки преимущества на основе неполных ответов. Также предлагается механизм, позволяющий независимо оптимизировать модели политики и ценности, что ускоряет процесс обучения.'}, 'en': {'title': 'Boosting Training Efficiency for Large Language Models with T-PPO', 'desc': "T-PPO is a new method that enhances the Proximal Policy Optimization (PPO) algorithm to make training Large Language Models (LLMs) faster and more efficient. It addresses the slow training times caused by PPO's on-policy nature, especially when generating long responses. By introducing Extended Generalized Advantage Estimation (EGAE), T-PPO allows for better advantage estimation from incomplete responses, which helps maintain effective policy learning. Additionally, it optimizes the use of hardware resources by reducing unnecessary computations during training, leading to a significant increase in efficiency and performance compared to existing methods."}, 'zh': {'title': 'T-PPO：提升大型语言模型训练效率的创新方案', 'desc': 'T-PPO是对PPO的一种扩展，旨在提高大型语言模型的训练效率。它通过优化策略更新和更有效地利用硬件资源来实现这一目标。T-PPO引入了扩展的广义优势估计（EGAE），使得在不完整响应的情况下仍能保持策略学习的完整性。此外，T-PPO还通过独立优化策略和价值模型，减少冗余计算，加速训练过程。'}}}, {'id': 'https://huggingface.co/papers/2506.06279', 'title': 'CoMemo: LVLMs Need Image Context with Image Memory', 'url': 'https://huggingface.co/papers/2506.06279', 'abstract': "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.", 'score': 4, 'issue_id': 4370, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'e2ad205a039e250b', 'authors': ['Shi Liu', 'Weijie Su', 'Xizhou Zhu', 'Wenhai Wang', 'Jifeng Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.06279.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#multimodal', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'CoMemo: Улучшение визуального восприятия в мультимодальных моделях', 'desc': 'CoMemo - это новая архитектура для мультимодальной обработки, решающая проблемы пренебрежения визуальной информацией и пространственной осведомленности. Она использует двухпутевую архитектуру, сочетающую контекстный путь изображения с путем памяти изображения. CoMemo также вводит новый механизм позиционного кодирования RoPE-DHR, основанный на агрегации миниатюр. Эта модель превосходит традиционные архитектуры LVLM в задачах понимания длинного контекста, рассуждения по нескольким изображениям и визуальных вопросов-ответов.'}, 'en': {'title': 'Enhancing Visual Awareness in Multimodal Processing with CoMemo', 'desc': 'CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions.'}, 'zh': {'title': 'CoMemo：提升多模态处理的视觉意识', 'desc': 'CoMemo是一种新型的双路径架构，旨在解决多模态处理中的视觉信息忽视和空间意识问题。它结合了上下文图像路径和图像记忆路径，有效缓解了视觉信息的忽视现象。此外，CoMemo引入了一种新颖的位置编码机制RoPE-DHR，通过缩略图位置聚合来保持二维空间意识，减少远程衰减。通过在七个基准测试中的评估，CoMemo的表现优于传统的大型视觉语言模型架构。'}}}, {'id': 'https://huggingface.co/papers/2506.14315', 'title': 'ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies', 'url': 'https://huggingface.co/papers/2506.14315', 'abstract': 'An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery. This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.', 'score': 3, 'issue_id': 4380, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '3fcf78822da5c3c6', 'authors': ['Jinyan Yuan', 'Bangbang Yang', 'Keke Wang', 'Panwang Pan', 'Lin Ma', 'Xuehai Zhang', 'Xiao Liu', 'Zhaopeng Cui', 'Yuewen Ma'], 'affiliations': ['ByteDance', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.14315.jpg', 'data': {'categories': ['#3d', '#video', '#multimodal', '#synthetic', '#games', '#agents'], 'emoji': '🕶️', 'ru': {'title': 'Фотореализм в VR без сложной геометрии', 'desc': 'ImmerseGen - это новая система для создания фотореалистичных 3D-сцен в виртуальной реальности. Она использует упрощенные геометрические прокси и синтезирует на них текстуры RGBA, что позволяет добиться высокого визуального качества при компактном представлении. Система включает агентов на основе больших языковых моделей для автоматизации создания сцен по текстовым запросам. ImmerseGen также добавляет динамические эффекты и звук для повышения погружения.'}, 'en': {'title': 'Revolutionizing VR with Lightweight 3D Scene Generation', 'desc': 'This paper presents ImmerseGen, a new framework for creating realistic 3D scenes for virtual reality (VR) using lightweight geometric proxies. Instead of relying on complex high-poly models, ImmerseGen synthesizes photorealistic textures directly onto these proxies, allowing for real-time rendering without sacrificing visual quality. The framework utilizes agent-guided generative models to produce coherent textures that enhance the immersive experience. Additionally, it incorporates dynamic effects and audio to create a multisensory environment, demonstrating improved efficiency and realism compared to traditional methods.'}, 'zh': {'title': '轻量级几何代理，实时生成逼真3D场景', 'desc': '本文提出了一种名为ImmerseGen的框架，用于生成逼真的3D场景，以增强虚拟现实体验。该框架通过将轻量几何代理与合成的RGBA纹理结合，简化了建模过程，并实现了实时渲染。ImmerseGen利用代理引导生成模型，确保纹理与场景的无缝融合，从而提高视觉质量。实验结果表明，ImmerseGen在光照真实感、空间一致性和渲染效率方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.14824', 'title': 'FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.14824', 'abstract': 'FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.', 'score': 3, 'issue_id': 4376, 'pub_date': '2025-06-12', 'pub_date_card': {'ru': '12 июня', 'en': 'June 12', 'zh': '6月12日'}, 'hash': 'c694f22e30433fa6', 'authors': ['Yao Zhang', 'Hewei Gao', 'Haokun Chen', 'Weiguo Li', 'Yunpu Ma', 'Volker Tresp'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning', 'Siemens Technology', 'Technical University of Munich', 'University Heidelberg'], 'pdf_title_img': 'assets/pdf/title_img/2506.14824.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#agents', '#scalability', '#privacy', '#training', '#federated_learning'], 'emoji': '🔬', 'ru': {'title': 'Эффективное федеративное обучение мультимодальных ИИ-систем', 'desc': 'FedNano - это новая система федеративного обучения для мультимодальных больших языковых моделей. Она централизует основную модель на сервере, а на клиентах использует легкие NanoEdge модули для адаптации. Такой подход решает проблемы масштабируемости и конфиденциальности при развертывании крупных мультимодальных моделей. FedNano значительно сокращает требования к ресурсам клиентов и объем передаваемых данных по сравнению с традиционными методами федеративного обучения.'}, 'en': {'title': 'Empowering Scalable AI with FedNano: Federated Learning for Large Language Models', 'desc': 'FedNano is a federated learning framework designed to enhance the deployment of large language models (LLMs) while addressing privacy and scalability challenges. It centralizes the LLM on servers and utilizes lightweight NanoEdge modules for client-specific adaptations, significantly reducing the need for client-side resources. By employing low-rank adaptation techniques, FedNano minimizes communication costs and storage requirements, allowing clients to only transmit compact updates instead of full model parameters. This innovative approach enables effective collaboration across heterogeneous client data while maintaining privacy, ultimately making multimodal AI systems more scalable and feasible.'}, 'zh': {'title': 'FedNano：解决多模态学习的隐私与可扩展性问题', 'desc': 'FedNano是一个联邦学习框架，它将大型语言模型集中在服务器上，并使用NanoEdge模块进行客户端特定的适应，从而解决了可扩展性和隐私问题。该框架通过允许在不集中数据的情况下进行协作模型训练，克服了多模态大语言模型在现实场景中的部署挑战。NanoEdge模块采用特定模态的编码器和可训练的NanoAdapters，显著减少了客户端的存储需求和通信开销。实验结果表明，FedNano在性能上优于现有的联邦学习基线，促进了可扩展的去中心化多模态人工智能系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.14866', 'title': 'OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents', 'url': 'https://huggingface.co/papers/2506.14866', 'abstract': 'A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.', 'score': 2, 'issue_id': 4378, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '4a30ee5bf7d4f9f6', 'authors': ['Thomas Kuntz', 'Agatha Duzan', 'Hao Zhao', 'Francesco Croce', 'Zico Kolter', 'Nicolas Flammarion', 'Maksym Andriushchenko'], 'affiliations': ['Carnegie Mellon University', 'EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2506.14866.jpg', 'data': {'categories': ['#security', '#ethics', '#agents', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'OS-Harm: новый стандарт безопасности ИИ-агентов в компьютерных интерфейсах', 'desc': 'Новый бенчмарк OS-Harm оценивает безопасность агентов, взаимодействующих с графическим интерфейсом компьютера. Он измеряет их уязвимость к неправильному использованию, атакам на промпты и нарушениям безопасности в различных приложениях. OS-Harm включает 150 задач, охватывающих различные типы нарушений безопасности и взаимодействие с разными приложениями ОС. Бенчмарк использует автоматизированную систему оценки, которая показывает высокое согласие с человеческими аннотациями.'}, 'en': {'title': 'Ensuring Safe Interactions: Introducing OS-Harm for Computer Use Agents', 'desc': "The paper introduces OS-Harm, a benchmark designed to assess the safety of computer use agents that interact with graphical user interfaces (GUIs). It evaluates these agents for their vulnerability to misuse, prompt injection attacks, and other forms of misbehavior across various applications. The benchmark includes 150 tasks that simulate different safety violations, such as harassment and data exfiltration, while requiring agents to operate within common OS applications. An automated judging system is proposed to evaluate the agents' performance, achieving a high agreement with human assessments, revealing that many models are prone to compliance with misuse and unsafe actions."}, 'zh': {'title': 'OS-Harm：评估计算机使用代理的安全新基准', 'desc': 'OS-Harm是一个新的基准，用于评估计算机使用代理在与图形用户界面交互时的安全性。它主要测试代理在用户恶意使用、提示注入攻击和模型不当行为等方面的脆弱性。该基准创建了150个任务，涵盖多种安全违规行为，并要求代理与不同的操作系统应用程序进行交互。通过自动评估工具，OS-Harm能够有效地评估代理的准确性和安全性，为计算机使用代理的安全性提供了重要的见解。'}}}, {'id': 'https://huggingface.co/papers/2506.14435', 'title': 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models', 'url': 'https://huggingface.co/papers/2506.14435', 'abstract': 'MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.', 'score': 2, 'issue_id': 4371, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '2f5b5f7f3c20ae10', 'authors': ['Hongyu Wang', 'Jiayu Xu', 'Ruiping Wang', 'Yan Feng', 'Yitao Zhai', 'Peng Pei', 'Xunliang Cai', 'Xilin Chen'], 'affiliations': ['Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences', 'Meituan University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2506.14435.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'MoTE: Эффективные Mixture-of-Experts модели для устройств с ограниченной памятью', 'desc': 'Статья представляет MoTE - метод для улучшения моделей Mixture-of-Experts с использованием тернарных экспертов низкой точности. Этот подход позволяет масштабировать размер модели, повышая производительность при сохранении фиксированных активных параметров. MoTE демонстрирует многообещающую тенденцию масштабирования и достигает сопоставимой производительности с полноточными базовыми MoE-моделями при меньшем объеме памяти. Метод особенно эффективен для устройств с ограниченной памятью, превосходя MoE-LLaVA на 4.3% по средней точности при том же объеме памяти экспертов.'}, 'en': {'title': 'MoTE: Efficient Ternary Experts for Scalable Edge Deployment', 'desc': 'The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques.'}, 'zh': {'title': 'MoTE：内存高效的混合专家模型', 'desc': 'MoTE是一种可扩展且内存高效的方法，旨在改进混合专家模型，使用低精度的三元专家来提升性能并减少内存占用，适合在边缘设备上部署。与以往主要使用全精度专家的稀疏上升方法不同，MoTE通过训练更多低精度专家来实现更好的性能。具体来说，我们使用预训练的前馈网络作为共享专家，并训练参数为{-1, 0, 1}的三元路由专家。实验结果表明，MoTE在模型规模上具有良好的扩展趋势，并在内存受限的情况下表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.11110', 'title': 'AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.11110', 'abstract': 'AssertBench evaluates Large Language Models\' ability to maintain consistent truth evaluation when faced with contradictory user assertions about factually true statements by analyzing framing-induced variability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model\'s agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model\'s underlying factual knowledge by stratifying results based on the model\'s accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM\'s ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.', 'score': 2, 'issue_id': 4386, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'd72eb1f8052f802e', 'authors': ['Jaeho Lee', 'Atharv Chowdhary'], 'affiliations': ['Brown University, Providence, RI 02912'], 'pdf_title_img': 'assets/pdf/title_img/2506.11110.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Проверка стойкости ИИ перед лицом противоречивых утверждений', 'desc': "AssertBench - это метод оценки способности больших языковых моделей (LLM) сохранять последовательность в оценке истинности при столкновении с противоречивыми утверждениями пользователей о фактически верных заявлениях. Метод анализирует изменчивость, вызванную формулировкой, используя выборку подтвержденных фактов из датасета FEVEROUS. Для каждого факта создаются два варианта подачи: один, где пользователь утверждает, что заявление верно, и другой, где оно названо неверным. AssertBench измеряет способность LLM 'отстаивать свою позицию' при столкновении с противоречивыми утверждениями пользователей об одном и том же факте."}, 'en': {'title': 'Testing LLMs: Can They Stick to the Truth?', 'desc': "AssertBench is a benchmark designed to evaluate how well Large Language Models (LLMs) maintain consistent truth evaluations when faced with contradictory assertions about factual statements. It investigates the impact of framing on model responses by presenting two different prompts for the same fact: one affirming its truth and another denying it. The goal is to see if the model can uphold its factual accuracy without being swayed by the user's framing. By isolating the effects of framing, AssertBench helps assess the robustness of LLMs in their reasoning and agreement with factual information."}, 'zh': {'title': '坚持真相，抵御矛盾主张的挑战', 'desc': 'AssertBench 是一个评估大型语言模型（LLM）在面对用户关于事实真相的矛盾主张时，保持一致性真相评估能力的工具。该研究通过分析框架引起的变异性，探讨了事实性陈述的方向性框架如何影响模型的同意程度。研究使用了来自 FEVEROUS 的证据支持事实，并为每个事实构建了两种框架提示，记录模型的同意和推理过程。目标是让模型在面对矛盾的用户主张时，能够坚持自己的判断，而不是随用户的意见改变评估。'}}}, {'id': 'https://huggingface.co/papers/2506.15461', 'title': 'All is Not Lost: LLM Recovery without Checkpoints', 'url': 'https://huggingface.co/papers/2506.15461', 'abstract': "A novel method, CheckFree, and its extended version CheckFree+, efficiently recover from node failures during LLM training by substituting failed stages with averaged neighboring stages or through out-of-order pipeline execution, improving convergence time over existing checkpointing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.", 'score': 1, 'issue_id': 4388, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'f6a5c394da44e46e', 'authors': ['Nikolay Blagoev', 'Oğuzhan Ersoy', 'Lydia Yiyu Chen'], 'affiliations': ['Gensyn', 'TU Delft', 'University of Neuchatel'], 'pdf_title_img': 'assets/pdf/title_img/2506.15461.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '🔄', 'ru': {'title': 'Эффективное восстановление LLM без лишних затрат', 'desc': 'CheckFree и CheckFree+ - новые методы восстановления после сбоев узлов при обучении больших языковых моделей (LLM). Они заменяют неисправные стадии усредненными соседними стадиями или используют внеочередное конвейерное выполнение. Эти методы не требуют дополнительных вычислений или хранилища, в отличие от традиционных подходов. CheckFree и CheckFree+ показывают лучшую сходимость по времени по сравнению с существующими методами контрольных точек при низких и средних частотах сбоев.'}, 'en': {'title': 'Efficient Recovery in LLM Training with CheckFree', 'desc': 'The paper introduces CheckFree, a novel method for recovering from node failures during the training of large language models (LLMs). Instead of traditional checkpointing or redundant computation, CheckFree substitutes failed stages with averaged outputs from neighboring stages, significantly reducing overhead. The extended version, CheckFree+, enhances this by allowing out-of-order execution, enabling recovery of initial and final stages by mimicking their behavior through neighboring stages. Evaluations show that both methods improve convergence time by over 12% compared to existing techniques, especially under low to medium failure rates.'}, 'zh': {'title': '高效恢复：CheckFree与CheckFree+的创新之路', 'desc': '本文提出了一种新方法CheckFree及其扩展版本CheckFree+，旨在提高大规模语言模型（LLM）训练中的节点故障恢复效率。该方法通过用相邻阶段的加权平均替代失败的阶段，或通过无序流水线执行来改善收敛时间，优于传统的检查点方法。与传统方法相比，CheckFree不需要额外的计算或存储，能够有效应对中间阶段的故障。扩展版本CheckFree+则通过模仿相邻阶段的行为，进一步容忍首尾阶段的崩溃，显著提高了模型训练的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.14770', 'title': 'GMT: General Motion Tracking for Humanoid Whole-Body Control', 'url': 'https://huggingface.co/papers/2506.14770', 'abstract': "GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.", 'score': 1, 'issue_id': 4383, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '1ed4993d05c76b95', 'authors': ['Zixuan Chen', 'Mazeyu Ji', 'Xuxin Cheng', 'Xuanbin Peng', 'Xue Bin Peng', 'Xiaolong Wang'], 'affiliations': ['Simon Fraser University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2506.14770.jpg', 'data': {'categories': ['#robotics', '#architecture', '#agents'], 'emoji': '🤖', 'ru': {'title': 'GMT: универсальное отслеживание движений для гуманоидных роботов', 'desc': 'GMT - это универсальная система отслеживания движений для гуманоидных роботов. Она использует адаптивную выборку и архитектуру смеси экспертов для обработки разнообразных движений. GMT обучает единую политику, способную отслеживать различные движения в реальном мире. Система достигает наилучших результатов как в симуляции, так и на реальных роботах.'}, 'en': {'title': 'Unified Motion Tracking for Humanoid Robots with GMT', 'desc': 'The paper presents GMT, a unified motion-tracking framework designed for humanoid robots to effectively track a wide range of motions. It tackles challenges such as the diversity of motion types and the coordination between upper and lower body movements. GMT employs an Adaptive Sampling strategy to optimize training by balancing the complexity of different motions, and a Motion Mixture-of-Experts architecture to enhance specialization in motion tracking. Experimental results demonstrate that GMT achieves state-of-the-art performance in both simulated and real-world environments, showcasing its versatility and effectiveness.'}, 'zh': {'title': '统一运动跟踪，突破人形机器人挑战', 'desc': 'GMT是一个统一的运动跟踪框架，旨在解决多样化人形机器人运动跟踪中的挑战。它通过自适应采样和运动专家混合架构来实现，能够有效地跟踪各种复杂的全身运动。自适应采样策略在训练过程中自动平衡简单和困难的运动，而运动专家混合架构则确保了运动流形不同区域的更好专业化。通过广泛的实验，GMT在模拟和现实世界中都展示了其卓越的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.15682', 'title': 'Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model', 'url': 'https://huggingface.co/papers/2506.15682', 'abstract': "ECAD, a genetic algorithm, optimizes caching schedules for diffusion models, enhancing inference speed while maintaining quality across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.", 'score': 0, 'issue_id': 4386, 'pub_date': '2025-06-18', 'pub_date_card': {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'}, 'hash': 'c624b4c5b312499d', 'authors': ['Anirud Aggarwal', 'Abhinav Shrivastava', 'Matthew Gwilliam'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2506.15682.jpg', 'data': {'categories': ['#optimization', '#inference', '#benchmark', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'ECAD: Эволюционное кэширование для ускорения диффузионных моделей', 'desc': 'ECAD - это генетический алгоритм, оптимизирующий схемы кэширования для диффузионных моделей. Он повышает скорость вывода, сохраняя качество на различных бенчмарках. ECAD формирует фронт Парето эффективных схем кэширования для каждой модели, используя небольшой набор калибровочных промптов. Алгоритм обеспечивает значительное ускорение вывода и позволяет точно контролировать баланс между качеством и задержкой.'}, 'en': {'title': 'Accelerating Diffusion Models with ECAD: Smart Caching for Speed and Quality', 'desc': 'The paper introduces ECAD, a genetic algorithm designed to optimize caching schedules for diffusion models, which improves inference speed without sacrificing quality. Traditional methods for caching features in diffusion transformers often rely on fixed rules, limiting their effectiveness. ECAD learns adaptive caching strategies tailored to specific models, allowing for better performance across different architectures. The results show that ECAD significantly enhances inference speed and maintains high quality, outperforming previous methods on various benchmarks.'}, 'zh': {'title': 'ECAD：加速扩散模型的智能缓存调度', 'desc': 'ECAD是一种遗传算法，旨在优化扩散模型的缓存调度，从而提高推理速度，同时保持在各种基准测试中的质量。传统方法依赖于固定的启发式规则，导致加速效果有限或在不同架构间泛化能力差。ECAD通过学习每个模型的高效缓存调度，形成帕累托前沿，且只需少量校准提示，无需修改网络参数或参考图像。我们的实验表明，ECAD在多个基准测试中显著提高了推理速度，并能有效适应不同的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.06751', 'title': 'Geopolitical biases in LLMs: what are the "good" and the "bad" countries\n  according to contemporary language models', 'url': 'https://huggingface.co/papers/2506.06751', 'abstract': "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper evaluates geopolitical biases in LLMs with respect to various countries though an analysis of their interpretation of historical events with conflicting national perspectives (USA, UK, USSR, and China). We introduce a novel dataset with neutral event descriptions and contrasting viewpoints from different countries. Our findings show significant geopolitical biases, with models favoring specific national narratives. Additionally, simple debiasing prompts had a limited effect in reducing these biases. Experiments with manipulated participant labels reveal models' sensitivity to attribution, sometimes amplifying biases or recognizing inconsistencies, especially with swapped labels. This work highlights national narrative biases in LLMs, challenges the effectiveness of simple debiasing methods, and offers a framework and dataset for future geopolitical bias research.", 'score': 52, 'issue_id': 4234, 'pub_date': '2025-06-07', 'pub_date_card': {'ru': '7 июня', 'en': 'June 7', 'zh': '6月7日'}, 'hash': '87a1fbaf018382d4', 'authors': ['Mikhail Salnikov', 'Dmitrii Korzh', 'Ivan Lazichny', 'Elvir Karimov', 'Artyom Iudin', 'Ivan Oseledets', 'Oleg Y. Rogov', 'Alexander Panchenko', 'Natalia Loukachevitch', 'Elena Tutubalina'], 'affiliations': ['AIRI', 'Kazan Federal University', 'Lomonosov MSU', 'MIPT', 'MTUCI', 'Sber AI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2506.06751.jpg', 'data': {'categories': ['#ethics', '#alignment', '#data', '#dataset'], 'emoji': '🌍', 'ru': {'title': 'Геополитические предубеждения в LLM: вызов объективности искусственного интеллекта', 'desc': 'Исследование оценивает геополитические предубеждения в больших языковых моделях (LLM) при интерпретации исторических событий с конфликтующими национальными перспективами. Авторы представляют новый набор данных с нейтральными описаниями событий и противоречивыми точками зрения разных стран. Результаты показывают значительные геополитические предубеждения, причем модели отдают предпочтение определенным национальным нарративам. Простые методы дебиасинга оказались малоэффективными в снижении этих предубеждений.'}, 'en': {'title': 'Uncovering Geopolitical Biases in Language Models', 'desc': 'This paper investigates the presence of geopolitical biases in large language models (LLMs) by analyzing how they interpret historical events from different national perspectives, specifically focusing on the USA, UK, USSR, and China. It introduces a new dataset that contains neutral descriptions of events alongside varying viewpoints from these countries to facilitate further research. The results indicate that LLMs tend to favor certain national narratives, demonstrating significant biases in their outputs. Additionally, the study finds that basic debiasing techniques are not very effective in mitigating these biases, suggesting a need for more robust methods in future research.'}, 'zh': {'title': '揭示大型语言模型的地缘政治偏见', 'desc': '本论文评估了大型语言模型（LLMs）在解释历史事件时的地缘政治偏见，特别是针对美国、英国、苏联和中国等国家的不同视角。我们引入了一个新数据集，包含中立的事件描述和来自不同国家的对立观点。研究结果显示，模型倾向于支持特定国家的叙事，且简单的去偏见方法效果有限。通过操控参与者标签的实验，我们发现模型对归因非常敏感，有时会放大偏见或识别不一致，尤其是在标签交换的情况下。'}}}, {'id': 'https://huggingface.co/papers/2506.09040', 'title': 'Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better', 'url': 'https://huggingface.co/papers/2506.09040', 'abstract': 'Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR.', 'score': 25, 'issue_id': 4237, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '09d042607d92f156', 'authors': ['Dianyi Wang', 'Wei Song', 'Yikun Wang', 'Siyuan Wang', 'Kaicheng Yu', 'Zhongyu Wei', 'Jiaqi Wang'], 'affiliations': ['AutoLab, Westlake University', 'Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'University of Southern California', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.09040.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#benchmark', '#games', '#cv', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Семантическая реконструкция изображений для улучшения мультимодального понимания', 'desc': 'Авторегрессивная семантическая визуальная реконструкция (ASVR) улучшает мультимодальное понимание, фокусируясь на семантической реконструкции, а не на восстановлении исходного визуального образа. Этот подход позволяет эффективно использовать изображения без сопроводительных подписей и учитывать важные визуальные детали, которые могут быть упущены в текстовых описаниях. ASVR объединяет обучение визуальной и текстовой модальностей в единой авторегрессивной структуре, что приводит к значительному улучшению производительности на различных мультимодальных тестах. Метод показывает стабильные улучшения при различных масштабах данных и типах базовых языковых моделей.'}, 'en': {'title': 'Revolutionizing Multimodal Understanding with Semantic Focus', 'desc': 'The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR), a method that enhances multimodal understanding by focusing on reconstructing the semantic content of images rather than their raw visual appearance. This approach addresses limitations in existing large vision-language models (LVLMs) that primarily rely on textual sequences, which can lead to missing critical visual details. By enabling joint learning of visual and textual modalities, ASVR allows models to effectively reconstruct discrete semantic tokens from continuous image features, leading to improved comprehension. The results show significant performance gains across various benchmarks, demonstrating the effectiveness of semantic reconstruction in multimodal tasks.'}, 'zh': {'title': '自回归语义重建，提升多模态理解！', 'desc': '自回归语义视觉重建（ASVR）通过关注语义重建而非原始视觉外观，提升了多模态理解的能力。传统的大型视觉语言模型（LVLM）仅对文本序列应用自回归监督，未能充分整合视觉模态，导致无法利用没有配套说明的图像。ASVR 通过在统一的自回归框架内实现视觉和文本模态的联合学习，克服了这一限制。我们的研究表明，重建图像的语义表示能够显著提高多模态理解的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.08672', 'title': 'RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling', 'url': 'https://huggingface.co/papers/2506.08672', 'abstract': 'RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL.', 'score': 23, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f9f7805577b3b091', 'authors': ['Yang Liu', 'Jiaqi Li', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, Beijing Institute for General Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2506.08672.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#rl', '#optimization', '#small_models', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение на основе правил для малых моделей', 'desc': 'RuleReasoner - это новый метод для улучшения рассуждений на основе правил в небольших моделях машинного обучения. Он использует динамическую выборку доменов и подкрепляющее обучение для повышения производительности. RuleReasoner превосходит крупные модели рассуждений по точности на задачах в распределении и вне распределения. Метод также демонстрирует более высокую вычислительную эффективность по сравнению с существующими подходами.'}, 'en': {'title': 'Boosting Small Models with Dynamic Domain Sampling', 'desc': 'RuleReasoner is a method that improves rule-based reasoning in small models by using dynamic domain sampling. It addresses the challenges posed by varying rule formats and complexities in real-world applications. By updating sampling weights based on historical rewards, RuleReasoner enhances the learning process and generalization across different tasks. Empirical results show that it significantly outperforms large reasoning models while being more computationally efficient.'}, 'zh': {'title': '小模型的规则推理新突破', 'desc': 'RuleReasoner是一种增强小型模型规则推理的方法，通过动态领域采样实现了优越的性能和效率。该方法利用强化学习（RL）来优化规则推理，解决了现实应用中规则格式和复杂性带来的挑战。通过更新不同领域的采样权重，RuleReasoner能够灵活地进行在线学习，避免了传统方法中需要人工设计的混合训练方案。实验证明，RuleReasoner在多个任务上显著超越了大型推理模型，且计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2506.07927', 'title': 'Solving Inequality Proofs with Large Language Models', 'url': 'https://huggingface.co/papers/2506.07927', 'abstract': 'The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.', 'score': 14, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '0171dcd88ad3a14f', 'authors': ['Jiayi Sheng', 'Luna Lyu', 'Jikai Jin', 'Tony Xia', 'Alex Gu', 'James Zou', 'Pan Lu'], 'affiliations': ['Massachusetts Institute of Technology', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.07927.jpg', 'data': {'categories': ['#survey', '#data', '#benchmark', '#dataset', '#reasoning', '#math'], 'emoji': '📊', 'ru': {'title': 'LLM могут найти ответ, но не могут доказать', 'desc': 'Исследование способности больших языковых моделей (LLM) доказывать неравенства выявило значительные трудности в построении строгих доказательств. Эксперты создали набор данных IneqMath с олимпиадными задачами и разработали новую систему оценки с использованием LLM в качестве судьи. Результаты показали, что даже лучшие модели достигают менее 10% точности при пошаговой проверке, что значительно ниже точности при учете только конечного ответа. Это исследование выявило существенный разрыв между способностью LLM находить ответ и строить корректное доказательство.'}, 'en': {'title': 'Bridging the Gap: From Answers to Rigorous Proofs in Inequality Proving', 'desc': 'This paper explores the challenges faced by large language models (LLMs) in the domain of inequality proving, which is essential for advanced reasoning in mathematics and science. It identifies a significant gap between generating answers and producing valid, step-by-step proofs, highlighting the limitations of current datasets that are often inadequate. The authors propose a new task formulation that breaks down inequality proving into two checkable subtasks: bound estimation and relation prediction, and introduce the IneqMath dataset for training and evaluation. Their evaluation of 29 leading LLMs reveals that even the best models struggle with rigorous proof construction, achieving less than 10% accuracy when assessed on step-wise reasoning, indicating a need for improved methodologies in theorem-guided reasoning and self-refinement.'}, 'zh': {'title': '揭示不等式证明中的推理挑战', 'desc': '这篇论文探讨了使用大型语言模型进行不等式证明的挑战，揭示了找到答案与生成有效逐步解决方案之间的差距。不等式证明在科学和数学领域至关重要，考验着高级推理能力，如发现紧界和战略性定理应用。为了应对现有数据集稀缺和形式化的问题，作者提出了一种非正式但可验证的任务形式，将不等式证明重构为两个可自动检查的子任务：界限估计和关系预测。此外，研究还发布了IneqMath数据集，并开发了一种新的评估框架，以检测常见的推理缺陷。'}}}, {'id': 'https://huggingface.co/papers/2506.08009', 'title': 'Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2506.08009', 'abstract': "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/", 'score': 13, 'issue_id': 4235, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'e63130d065bba1fd', 'authors': ['Xun Huang', 'Zhengqi Li', 'Guande He', 'Mingyuan Zhou', 'Eli Shechtman'], 'affiliations': ['Adobe Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.08009.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'Self Forcing: реалистичная генерация видео в реальном времени', 'desc': 'Представлен новый метод обучения авторегрессионных видео-диффузионных моделей под названием Self Forcing. Он решает проблему смещения экспозиции, обучая модель на собственных сгенерированных выходных данных вместо истинных. Self Forcing использует кэширование ключ-значение и целостную функцию потерь на уровне всего видео. Метод позволяет генерировать видео в реальном времени с низкой задержкой на одном GPU, сохраняя высокое качество.'}, 'en': {'title': 'Self Forcing: Enhancing Video Generation with Autoregressive Training', 'desc': 'The paper presents Self Forcing, a new training method for autoregressive video diffusion models that aims to reduce exposure bias and enhance video generation quality. It allows models to generate video frames based on their own previously generated outputs instead of relying solely on ground-truth frames, which helps in better training. By using a holistic video-level supervision approach, the method evaluates the quality of the entire video sequence rather than just individual frames. Additionally, it incorporates efficient caching mechanisms and a few-step diffusion model to optimize performance and ensure real-time video generation on a single GPU.'}, 'zh': {'title': '自我强制：提升视频生成质量的新方法', 'desc': 'Self Forcing是一种新颖的自回归视频扩散模型训练方法，旨在减少曝光偏差并提高生成质量。该方法通过整体视频级监督和高效缓存机制，解决了模型在推理时必须依赖自身不完美输出生成序列的问题。与以往方法不同，Self Forcing在训练过程中使用自生成输出进行条件生成，从而实现视频级的整体损失评估。实验表明，该方法能够在单个GPU上实现实时视频生成，且生成质量优于传统的非因果扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.08002', 'title': 'Aligning Text, Images, and 3D Structure Token-by-Token', 'url': 'https://huggingface.co/papers/2506.08002', 'abstract': "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/", 'score': 13, 'issue_id': 4232, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'ab2da61a8a27783d', 'authors': ['Aadarsh Sahoo', 'Vansh Tibrewal', 'Georgia Gkioxari'], 'affiliations': ['California Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08002.jpg', 'data': {'categories': ['#optimization', '#training', '#synthetic', '#3d', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Единая мультимодальная модель для понимания 3D-мира', 'desc': 'Предложена унифицированная модель для языка, изображений и 3D-сцен, достигающая оптимальной производительности в различных 3D-задачах. Модель основана на авторегрессионных методах и использует выравнивание модальностей. Авторы предоставляют детальное руководство по ключевым аспектам обучения, включая представление данных и специфические для модальностей целевые функции. Модель оценивается на четырех основных 3D-задачах и четырех наборах данных, демонстрируя эффективность в реконструкции сложных 3D-форм объектов.'}, 'en': {'title': 'Unifying Language, Image, and 3D Understanding for Enhanced Machine Interaction', 'desc': 'This paper presents a unified framework that integrates language, images, and 3D scenes to enhance machine understanding of three-dimensional environments. By leveraging autoregressive models, the authors explore how to effectively represent and process structured 3D scenes alongside traditional modalities. The framework includes a comprehensive guide for optimal training and performance, addressing critical aspects like data representation and modality-specific objectives. The model is evaluated on various 3D tasks, demonstrating its capability in rendering, recognition, instruction-following, and question-answering across both synthetic and real-world datasets.'}, 'zh': {'title': '统一三维场景模型，提升AI理解能力', 'desc': '本文提出了一种统一的语言、图像和三维场景模型框架，旨在实现各种三维任务和数据集的最佳训练和性能。该框架利用自回归模型，探索了结构化三维场景的新模式，帮助设计师构建和编辑三维环境。我们提供了一份详细的“食谱”，阐述了实现最佳训练和性能的关键设计选择，并评估了在四个核心三维任务和数据集上的表现。通过丰富三维模态的量化形状编码，我们的模型在复杂三维物体识别任务中展现了有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.04614', 'title': 'Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation', 'url': 'https://huggingface.co/papers/2506.04614', 'abstract': "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency.", 'score': 13, 'issue_id': 4236, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '9eb1f2457722a2cd', 'authors': ['Yuyang Wanyan', 'Xi Zhang', 'Haiyang Xu', 'Haowei Liu', 'Junyang Wang', 'Jiabo Ye', 'Yutong Kou', 'Ming Yan', 'Fei Huang', 'Xiaoshan Yang', 'Weiming Dong', 'Changsheng Xu'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University', 'MAIS, Institute of Automation, Chinese Academy of Sciences, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04614.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#optimization', '#rl', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Повышение надежности автоматизации GUI с помощью предоперационной критики', 'desc': 'Статья представляет новый механизм предоперационной критики для повышения надежности задач мультимодального рассуждения в автоматизации графического интерфейса пользователя (GUI). Авторы предлагают стратегию оптимизации политики с учетом градиента и предложений (S-GRPO) для создания модели GUI-Critic-R1. Исследование включает разработку процесса сбора данных на основе бутстрэппинга рассуждений для создания наборов данных GUI-Critic-Train и GUI-Critic-Test. Эксперименты показывают, что GUI-Critic-R1 превосходит существующие мультимодальные большие языковые модели (MLLM) в точности критики и эффективности автоматизации GUI.'}, 'en': {'title': 'Enhancing GUI Automation Reliability with Pre-operative Critique', 'desc': 'This paper presents a new method to improve the reliability of GUI automation using a pre-operative critic mechanism. The mechanism provides feedback before actions are executed, helping to prevent errors that could lead to serious issues like unwanted deletions. The authors introduce a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to enhance the feedback process, making it more effective. Experiments show that their model, GUI-Critic-R1, significantly outperforms existing models in both accuracy and operational efficiency during GUI automation tasks.'}, 'zh': {'title': '提升GUI自动化可靠性的预操作评估机制', 'desc': '本文提出了一种预操作评估机制，旨在提高图形用户界面（GUI）自动化中的多模态推理任务的可靠性。我们引入了一种名为建议感知梯度相对策略优化（S-GRPO）的策略，以构建预操作评估模型GUI-Critic-R1，并通过引入新颖的建议奖励来增强模型反馈的可靠性。该机制在实际执行之前提供有效反馈，帮助推理潜在结果和行动的正确性，从而减少决策错误的风险。通过在移动和网页领域的静态实验，我们的模型在评估准确性上显著优于现有的多模态大语言模型。'}}}, {'id': 'https://huggingface.co/papers/2506.07177', 'title': 'Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2506.07177', 'abstract': 'Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancements in diffusion models have significantly improved video quality, directing attention to fine-grained controllability. However, many existing methods depend on fine-tuning large-scale video models for specific tasks, which becomes increasingly impractical as model sizes continue to grow. In this work, we present Frame Guidance, a training-free guidance for controllable video generation based on frame-level signals, such as keyframes, style reference images, sketches, or depth maps. For practical training-free guidance, we propose a simple latent processing method that dramatically reduces memory usage, and apply a novel latent optimization strategy designed for globally coherent video generation. Frame Guidance enables effective control across diverse tasks, including keyframe guidance, stylization, and looping, without any training, compatible with any video models. Experimental results show that Frame Guidance can produce high-quality controlled videos for a wide range of tasks and input signals.', 'score': 12, 'issue_id': 4233, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': '7d83fcff01c3595a', 'authors': ['Sangwon Jang', 'Taekyung Ki', 'Jaehyeong Jo', 'Jaehong Yoon', 'Soo Ye Kim', 'Zhe Lin', 'Sung Ju Hwang'], 'affiliations': ['Adobe Research', 'DeepAuto.ai', 'KAIST', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.07177.jpg', 'data': {'categories': ['#optimization', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Управление генерацией видео без переобучения модели', 'desc': 'Статья представляет метод Frame Guidance для управления генерацией видео без дополнительного обучения моделей. Этот подход использует покадровые сигналы, такие как ключевые кадры, эталонные изображения стиля или карты глубины. Frame Guidance значительно снижает использование памяти благодаря обработке латентного пространства. Метод применяет новую стратегию оптимизации латентного пространства для создания глобально согласованных видео.'}, 'en': {'title': 'Effortless Control in Video Generation with Frame Guidance', 'desc': 'Frame Guidance introduces a novel approach to video generation that does not require any training, allowing for effective control using frame-level signals. This method significantly reduces memory usage while ensuring that the generated videos maintain global coherence. By utilizing a simple latent processing technique and a unique latent optimization strategy, Frame Guidance can adapt to various tasks such as keyframe guidance and stylization. The results demonstrate that this approach can produce high-quality videos across different input types without the need for fine-tuning large models.'}, 'zh': {'title': '无训练的视频生成控制新方法', 'desc': '本论文提出了一种名为Frame Guidance的方法，用于控制视频生成，且无需训练。该方法利用帧级信号，如关键帧和风格参考图像，来实现对视频生成的精细控制。Frame Guidance显著降低了内存使用，并增强了视频输出的全局一致性。实验结果表明，该方法能够在多种任务中生成高质量的可控视频，且与任何视频模型兼容。'}}}, {'id': 'https://huggingface.co/papers/2506.05167', 'title': 'ECoRAG: Evidentiality-guided Compression for Long Context RAG', 'url': 'https://huggingface.co/papers/2506.05167', 'abstract': 'ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown remarkable performance in Open-Domain Question Answering (ODQA) by leveraging external documents through Retrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer context, context compression is necessary. However, prior compression methods do not focus on filtering out non-evidential information, which limit the performance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or ECoRAG framework. ECoRAG improves LLM performance by compressing retrieved documents based on evidentiality, ensuring whether answer generation is supported by the correct evidence. As an additional step, ECoRAG reflects whether the compressed content provides sufficient evidence, and if not, retrieves more until sufficient. Experiments show that ECoRAG improves LLM performance on ODQA tasks, outperforming existing compression methods. Furthermore, ECoRAG is highly cost-efficient, as it not only reduces latency but also minimizes token usage by retaining only the necessary information to generate the correct answer. Code is available at https://github.com/ldilab/ECoRAG.', 'score': 7, 'issue_id': 4231, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd979315df3a92206', 'authors': ['Yeonseok Jeong', 'Jinsu Kim', 'Dohyeon Lee', 'Seung-won Hwang'], 'affiliations': ['IPAI, Seoul National University', 'Korea University', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05167.jpg', 'data': {'categories': ['#rag', '#alignment', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'ECoRAG: Умное сжатие для точных ответов', 'desc': 'ECoRAG - это новый фреймворк для улучшения производительности больших языковых моделей (LLM) в задачах открытого вопросно-ответного поиска (ODQA). Он использует сжатие полученных документов на основе доказательности, что позволяет снизить задержку и использование токенов. ECoRAG превосходит существующие методы сжатия и повышает эффективность LLM в задачах ODQA. Этот подход не только улучшает производительность, но и является экономически эффективным, сохраняя только необходимую информацию для генерации правильного ответа.'}, 'en': {'title': 'ECoRAG: Elevating LLMs with Evidence-Based Compression', 'desc': 'The ECoRAG framework enhances the performance of Large Language Models (LLMs) in Open-Domain Question Answering (ODQA) by focusing on evidentiality during document retrieval and compression. By filtering out non-evidential information, ECoRAG ensures that the generated answers are supported by relevant evidence, improving the overall accuracy of responses. Additionally, the framework optimizes resource usage by reducing latency and minimizing token consumption, making it more efficient than previous methods. Experiments demonstrate that ECoRAG significantly outperforms existing compression techniques in ODQA tasks.'}, 'zh': {'title': 'ECoRAG：提升问答性能的证据性压缩框架', 'desc': 'ECoRAG框架通过基于证据性压缩检索到的文档，提升了大型语言模型（LLM）在开放领域问答（ODQA）中的表现。该方法解决了以往压缩技术未能有效过滤非证据性信息的问题，从而提高了生成答案的准确性。ECoRAG确保生成的答案有足够的证据支持，并在必要时进行额外的文档检索。实验结果表明，ECoRAG在ODQA任务中优于现有的压缩方法，同时降低了延迟和令牌使用，具有很高的成本效益。'}}}, {'id': 'https://huggingface.co/papers/2506.08279', 'title': 'Seeing Voices: Generating A-Roll Video from Audio with Mirage', 'url': 'https://huggingface.co/papers/2506.08279', 'abstract': "Mirage generates realistic video from audio inputs, integrating with speech synthesis to create compelling multimodal content through a unified, self-attention-based training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).", 'score': 6, 'issue_id': 4242, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': '5309366c2181217d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#architecture', '#video', '#multimodal', '#audio', '#games'], 'emoji': '🎬', 'ru': {'title': 'Звук оживает: Mirage превращает аудио в реалистичное видео', 'desc': 'Mirage - это модель машинного обучения для генерации видео на основе аудио. Она использует единый подход на основе механизма самовнимания для создания реалистичных и выразительных видеопоследовательностей. Mirage может генерировать видео с говорящими людьми, интерпретируя входной аудиосигнал. В сочетании с синтезом речи эта модель позволяет создавать убедительный мультимодальный видеоконтент.'}, 'en': {'title': 'Transforming Audio into Realistic Video with Mirage', 'desc': 'Mirage is a novel model that generates realistic videos from audio inputs by combining audio and visual elements through a self-attention-based training method. Unlike previous models that either ignore sound or are limited to specific applications, Mirage creates expressive video content from scratch based on audio cues. It excels in generating videos of people speaking, accurately reflecting the performance implied in the audio. This unified approach enhances the quality of the generated videos, making them more compelling and realistic compared to traditional methods.'}, 'zh': {'title': 'Mirage：音频驱动的逼真视频生成', 'desc': 'Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真的视频图像。它结合了语音合成技术，创造出引人入胜的多模态内容。与传统方法不同，Mirage不仅关注视觉元素，还能有效处理音频信息，从而生成高质量的视频。该模型采用自注意力机制进行统一训练，确保了生成结果的优越性和通用性。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.07932', 'title': 'Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor', 'url': 'https://huggingface.co/papers/2506.07932', 'abstract': 'A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.', 'score': 6, 'issue_id': 4233, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'a13860cb07518cb2', 'authors': ['Rishit Dagli', 'Yushi Guan', 'Sankeerth Durvasula', 'Mohammadreza Mofayezi', 'Nandita Vijaykumar'], 'affiliations': ['University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07932.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#3d'], 'emoji': '🗜️', 'ru': {'title': 'Эффективное сжатие 3D-данных с помощью нейросетей', 'desc': 'Squeeze3D - это новая система для сжатия 3D-данных, использующая предобученные модели. Она позволяет достичь высоких степеней сжатия при сохранении визуального качества для сеток, облаков точек и полей излучения. Система работает путем преобразования входных данных в компактное латентное представление с помощью энкодера, а затем восстановления 3D-модели генеративной моделью. Squeeze3D не требует специальных 3D-датасетов для обучения и обеспечивает быстрое сжатие и распаковку данных.'}, 'en': {'title': 'Efficient 3D Data Compression with Squeeze3D', 'desc': 'Squeeze3D is a new framework designed to compress 3D data efficiently using pre-trained models. It connects the latent spaces of a pre-trained encoder and a generative model through trainable mapping networks, allowing for high compression ratios while preserving visual quality. The framework can handle various 3D representations, such as meshes and point clouds, and is trained solely on synthetic data, eliminating the need for specific 3D datasets. Experiments show that Squeeze3D achieves impressive compression rates, making it a versatile tool for 3D data compression.'}, 'zh': {'title': 'Squeeze3D：高效压缩3D数据的新方法', 'desc': 'Squeeze3D是一个新颖的框架，利用预训练模型高效压缩3D数据，达到高压缩比并保持视觉质量。该方法通过可训练的映射网络连接预训练编码器和生成模型之间的潜在空间。3D模型首先被编码为紧凑的潜在代码，然后通过映射网络转换为生成模型的潜在空间，以重建原始3D模型。实验表明，Squeeze3D在不同格式的3D数据上实现了显著的压缩效果，同时延迟较小。'}}}, {'id': 'https://huggingface.co/papers/2506.07045', 'title': 'Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2506.07045', 'abstract': 'A dataset with annotations aids in fine-tuning MLLMs for accurate detection and localization of AI-generated images with meaningful explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of image generation technologies intensifies the demand for interpretable and robust detection methods. Although existing approaches often attain high accuracy, they typically operate as black boxes without providing human-understandable justifications. Multi-modal Large Language Models (MLLMs), while not originally intended for forgery detection, exhibit strong analytical and reasoning capabilities. When properly fine-tuned, they can effectively identify AI-generated images and offer meaningful explanations. However, existing MLLMs still struggle with hallucination and often fail to align their visual interpretations with actual image content and human reasoning. To bridge this gap, we construct a dataset of AI-generated images annotated with bounding boxes and descriptive captions that highlight synthesis artifacts, establishing a foundation for human-aligned visual-textual grounded reasoning. We then finetune MLLMs through a multi-stage optimization strategy that progressively balances the objectives of accurate detection, visual localization, and coherent textual explanation. The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly outperforming baseline methods.', 'score': 6, 'issue_id': 4241, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'ced31e12be23b119', 'authors': ['Yikun Ji', 'Hong Yan', 'Jun Lan', 'Huijia Zhu', 'Weiqiang Wang', 'Qi Fan', 'Liqing Zhang', 'Jianfu Zhang'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.07045.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#data', '#training', '#reasoning', '#optimization', '#hallucinations', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое обнаружение ИИ-изображений с помощью мультимодальных языковых моделей', 'desc': 'Статья представляет новый подход к обнаружению и локализации изображений, сгенерированных искусственным интеллектом, с использованием мультимодальных языковых моделей (MLLM). Авторы создали датасет с аннотациями, содержащий ограничивающие рамки и описательные подписи, выделяющие артефакты синтеза. Используя многоэтапную стратегию оптимизации, они дообучили MLLM для точного обнаружения, визуальной локализации и создания связных текстовых объяснений. Полученная модель превосходит базовые методы в обнаружении сгенерированных ИИ изображений и локализации визуальных дефектов.'}, 'en': {'title': 'Enhancing AI Image Detection with Human-Aligned Reasoning', 'desc': "This paper presents a method to improve the detection and localization of AI-generated images using Multi-modal Large Language Models (MLLMs). It highlights the importance of a well-annotated dataset that includes bounding boxes and descriptive captions to enhance the model's understanding of visual artifacts. The authors propose a multi-stage optimization strategy to fine-tune MLLMs, aiming to balance accurate detection with coherent explanations. The results show that the fine-tuned model significantly outperforms existing methods in both identifying AI-generated images and providing meaningful insights into their flaws."}, 'zh': {'title': '微调MLLMs以检测AI生成图像的创新方法', 'desc': '本论文提出了一种利用带注释的数据集来微调多模态大型语言模型（MLLMs），以准确检测和定位AI生成的图像。随着图像生成技术的快速发展，对可解释和稳健的检测方法的需求日益增加。虽然现有方法通常具有高准确性，但它们往往作为黑箱操作，无法提供人类可理解的解释。通过构建一个带有边界框和描述性标题的AI生成图像数据集，本文为人类对齐的视觉-文本推理奠定了基础，并通过多阶段优化策略微调MLLMs，显著提高了检测和定位的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.08887', 'title': 'DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval', 'url': 'https://huggingface.co/papers/2506.08887', 'abstract': 'The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '068c2f58bc5049d2', 'authors': ['Leqi Shen', 'Guoqiang Gong', 'Tianxiang Hao', 'Tao He', 'Yifeng Zhang', 'Pengzhang Liu', 'Sicheng Zhao', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist', 'Department of Automation, Tsinghua University', 'GRG Banking Equipment Co., Ltd.', 'Hangzhou Zhuoxi Institute of Brain and Intelligence', 'JD.com', 'School of Software', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.08887.jpg', 'data': {'categories': ['#transfer_learning', '#alignment', '#video', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Преодоление разрыва между изображениями и видео в мультимодальном поиске', 'desc': 'Статья представляет DiscoVLA - метод для улучшения поиска видео по текстовым запросам с использованием модели CLIP. Авторы решают проблемы несоответствия в зрении, языке и выравнивании при переходе от изображений к видео. DiscoVLA объединяет признаки изображений и видео, генерирует псевдо-подписи к изображениям и применяет дистилляцию знаний для улучшения выравнивания видео и текста. Эксперименты показывают превосходство DiscoVLA над существующими методами в задаче поиска видео по текстовым запросам.'}, 'en': {'title': 'Bridging Gaps in Video-Text Retrieval with DiscoVLA', 'desc': 'The paper introduces DiscoVLA, a method designed to enhance video-text retrieval by addressing three main discrepancies: vision, language, and alignment. Unlike existing approaches that primarily focus on visual aspects, DiscoVLA integrates both image and video features to improve understanding at the video level. It also generates pseudo image captions to refine image-level alignment and employs Image-to-Video Alignment Distillation to strengthen video-level alignment using knowledge from image-level data. Experimental results show that DiscoVLA significantly outperforms previous methods, achieving a notable improvement in retrieval accuracy.'}, 'zh': {'title': '提升视频-文本检索的DiscoVLA方法', 'desc': '本文提出了一种名为DiscoVLA的方法，旨在通过解决视觉、语言和对齐的差异来改善视频-文本检索。传统的CLIP模型主要关注图像级别的视觉-语言匹配，而视频-文本检索需要在视频级别上进行全面理解。我们的方法通过图像-视频特征融合来整合图像和视频的特征，有效应对视觉和语言的差异。同时，我们生成伪图像标题以学习细粒度的图像级别对齐，从而减轻对齐差异。实验结果表明，DiscoVLA在视频-文本检索任务中表现优越，尤其在MSRVTT数据集上取得了50.5%的R@1成绩。'}}}, {'id': 'https://huggingface.co/papers/2506.08500', 'title': 'DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in\n  Search-Augmented LLMs', 'url': 'https://huggingface.co/papers/2506.08500', 'abstract': 'CONFLICTS, a benchmark for evaluating how LLMs handle knowledge conflicts in RAG, reveals significant challenges in conflict resolution but shows improvement with explicit prompting.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.', 'score': 4, 'issue_id': 4241, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '3dec58d73ae743df', 'authors': ['Arie Cattan', 'Alon Jacovi', 'Ori Ram', 'Jonathan Herzig', 'Roee Aharoni', 'Sasha Goldshtein', 'Eran Ofek', 'Idan Szpektor', 'Avi Caciularu'], 'affiliations': ['Bar-Ilan University', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2506.08500.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#rag', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Разрешение конфликтов знаний: новый рубеж для LLM', 'desc': 'Статья представляет CONFLICTS - новый бенчмарк для оценки способности больших языковых моделей (LLM) разрешать конфликты знаний в контексте Retrieval Augmented Generation (RAG). Авторы предлагают таксономию типов конфликтов знаний и желаемое поведение модели для каждого типа. Эксперименты показывают, что LLM часто испытывают трудности с разрешением конфликтов между источниками. Явное указание моделям рассуждать о потенциальных конфликтах значительно улучшает качество и уместность их ответов.'}, 'en': {'title': 'Navigating Knowledge Conflicts in LLMs with CONFLICTS Benchmark', 'desc': 'This paper introduces CONFLICTS, a benchmark designed to evaluate how large language models (LLMs) manage knowledge conflicts in Retrieval Augmented Generation (RAG) systems. It presents a new taxonomy categorizing different types of knowledge conflicts and outlines the expected behavior of models when faced with these conflicts. The authors demonstrate that while LLMs often struggle with resolving discrepancies in retrieved information, providing explicit prompts can enhance their performance. The study highlights the need for further advancements in conflict resolution strategies within LLMs.'}, 'zh': {'title': '解决知识冲突，提升模型表现！', 'desc': '本论文提出了一个新的基准测试CONFLICTS，用于评估大型语言模型（LLMs）在检索增强生成（RAG）中处理知识冲突的能力。我们首先定义了知识冲突的类型，并描述了模型在每种类型下的期望行为。研究表明，LLMs在解决信息来源之间的冲突时面临显著挑战，但通过明确提示可以显著提高其响应的质量。尽管如此，未来的研究仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2506.08300', 'title': "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability", 'url': 'https://huggingface.co/papers/2506.08300', 'abstract': "Institutional Books 1.0 provides a large dataset of public domain books from Harvard Library for training and inference of large language models, enhancing data accessibility and sustainability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) use data to learn about the world in order to produce meaningful correlations and predictions. As such, the nature, scale, quality, and diversity of the datasets used to train these models, or to support their work at inference time, have a direct impact on their quality. The rapid development and adoption of LLMs of varying quality has brought into focus the scarcity of publicly available, high-quality training data and revealed an urgent need to ground the stewardship of these datasets in sustainable practices with clear provenance chains. To that end, this technical report introduces Institutional Books 1.0, a large collection of public domain books originally digitized through Harvard Library's participation in the Google Books project, beginning in 2006. Working with Harvard Library, we extracted, analyzed, and processed these volumes into an extensively-documented dataset of historic texts. This analysis covers the entirety of Harvard Library's collection scanned as part of that project, originally spanning 1,075,899 volumes written in over 250 different languages for a total of approximately 250 billion tokens. As part of this initial release, the OCR-extracted text (original and post-processed) as well as the metadata (bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens, identified as being in the public domain have been made available. This report describes this project's goals and methods as well as the results of the analyses we performed, all in service of making this historical collection more accessible and easier for humans and machines alike to filter, read and use.", 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '8e42e765c2efbe2a', 'authors': ['Matteo Cargnelutti', 'Catherine Brobston', 'John Hess', 'Jack Cushman', 'Kristi Mukk', 'Aristana Scourtas', 'Kyle Courtney', 'Greg Leppert', 'Amanda Watson', 'Martha Whitehead', 'Jonathan Zittrain'], 'affiliations': ['Harvard Law School Library', 'Harvard Law School, Harvard School of Engineering and Applied Sciences, Harvard Kennedy School', 'Harvard Library', 'Institutional Data Initiative, Harvard Law School Library', 'Library Innovation Lab, Harvard Law School Library'], 'pdf_title_img': 'assets/pdf/title_img/2506.08300.jpg', 'data': {'categories': ['#open_source', '#dataset', '#synthetic', '#data'], 'emoji': '📚', 'ru': {'title': 'Большие данные для больших моделей: историческая библиотека в цифровом формате', 'desc': 'Датасет Institutional Books 1.0 предоставляет обширную коллекцию книг из публичного достояния Гарвардской библиотеки для обучения и использования крупных языковых моделей. Он включает 983,004 тома на более чем 250 языках, содержащих около 242 миллиардов токенов. Датасет содержит OCR-извлеченный текст и метаданные книг, находящихся в открытом доступе. Проект направлен на повышение доступности исторической коллекции и облегчение ее использования как людьми, так и машинами.'}, 'en': {'title': 'Unlocking Knowledge: A Sustainable Dataset for Language Models', 'desc': 'Institutional Books 1.0 is a comprehensive dataset of public domain books sourced from Harvard Library, aimed at improving the training and inference processes of large language models (LLMs). The dataset includes over 983,000 volumes and approximately 242 billion tokens, providing a rich resource for enhancing the quality and diversity of training data. By ensuring clear provenance and sustainable practices in data stewardship, this initiative addresses the critical need for high-quality, publicly available datasets in the rapidly evolving field of machine learning. The project not only facilitates better model performance but also promotes accessibility to historical texts for both human and machine use.'}, 'zh': {'title': '提升语言模型的数据可用性与可持续性', 'desc': 'Institutional Books 1.0 是一个大型数据集，包含来自哈佛图书馆的公共领域书籍，旨在为大型语言模型的训练和推理提供数据支持。这些书籍的多样性和质量直接影响到语言模型的表现，因此需要高质量的训练数据。该项目从哈佛图书馆提取和处理了超过983,000本书籍的文本和元数据，确保数据的可访问性和可持续性。通过这个数据集，研究人员和开发者可以更方便地使用历史文本，推动机器学习的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.07976', 'title': 'Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction', 'url': 'https://huggingface.co/papers/2506.07976', 'abstract': 'Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.  \t\t\t\t\tAI-generated summary \t\t\t\t The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent\'s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.', 'score': 4, 'issue_id': 4239, 'pub_date': '2025-06-09', 'pub_date_card': {'ru': '9 июня', 'en': 'June 9', 'zh': '6月9日'}, 'hash': 'b35082cb7ca5bb65', 'authors': ['Junhong Shen', 'Hao Bai', 'Lunjun Zhang', 'Yifei Zhou', 'Amrith Setlur', 'Shengbang Tong', 'Diego Caples', 'Nan Jiang', 'Tong Zhang', 'Ameet Talwalkar', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'New York University', 'Scribe', 'The AGI Company', 'University of California, Berkeley', 'University of Illinois Urbana-Champaign', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2506.07976.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#agents', '#open_source', '#training'], 'emoji': '🕸️', 'ru': {'title': 'Адаптивные веб-агенты: новые горизонты с масштабированием взаимодействия', 'desc': 'Статья представляет новый подход к улучшению производительности веб-агентов с использованием масштабирования взаимодействия во время тестирования (Test-Time Interaction, TTI). TTI позволяет агентам адаптивно корректировать свое поведение, балансируя между исследованием и эксплуатацией без увеличения вычислительных затрат на каждом шаге. Авторы демонстрируют эффективность TTI на бенчмарках WebVoyager и WebArena, используя языковую модель Gemma 3 12B. Результаты показывают, что масштабирование взаимодействия является мощным дополнением к масштабированию вычислений на каждом шаге для обучения адаптивных агентов.'}, 'en': {'title': 'Empowering Web Agents with Test-Time Interaction', 'desc': 'This paper introduces Test-Time Interaction (TTI), a novel approach that enhances the performance of web agents by allowing them to interact more effectively with their environment. Unlike traditional methods that focus on generating long reasoning traces before acting, TTI enables agents to adapt their behavior in real-time by increasing their interaction horizon. This method supports complex behaviors such as exploration, backtracking, and dynamic re-planning within a single decision-making process. The authors demonstrate that TTI significantly improves task success rates on web benchmarks, showcasing its potential as a complementary strategy to existing scaling techniques in reinforcement learning.'}, 'zh': {'title': '测试时交互：提升代理智能的新维度', 'desc': '本文提出了一种新的方法，称为测试时交互（TTI），旨在提高网络代理的性能。TTI通过扩展代理的交互范围，使其能够在单次执行中进行探索、回溯和动态重新规划。与传统的依赖长推理轨迹的方式不同，TTI允许代理在与环境互动时获取新信息并适应其行为。我们的实验表明，TTI在WebVoyager和WebArena基准测试中表现出色，展示了交互扩展作为一种强大的补充方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05928', 'title': 'MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2506.05928', 'abstract': 'A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a heterogeneous Mixture-of-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing a weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA.', 'score': 4, 'issue_id': 4232, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'f78e6f70ebb69ac2', 'authors': ['Jie Cao', 'Tianwei Lin', 'Hongyang He', 'Rolan Yan', 'Wenqiao Zhang', 'Juncheng Li', 'Dongping Zhang', 'Siliang Tang', 'Yueting Zhuang'], 'affiliations': ['Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05928.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Гетерогенная смесь адаптеров: новый шаг в эффективной настройке языковых моделей', 'desc': 'Статья представляет новый подход к эффективной настройке больших языковых моделей - гетерогенную Mixture-of-Adapters (MoA). MoA интегрирует различные адаптеры-эксперты, что позволяет преодолеть проблемы коллапса представлений и дисбаланса нагрузки экспертов, характерные для гомогенных методов MoE-LoRA. Предложены два варианта MoA: Soft MoA с взвешенным слиянием выходов всех экспертов и Sparse MoA с активацией только наиболее значимых экспертов. Эксперименты показывают, что MoA превосходит гомогенные методы MoE-LoRA как по производительности, так и по эффективности использования параметров.'}, 'en': {'title': 'Unlocking LLM Potential with Diverse Adapter Experts', 'desc': 'This paper introduces a new approach called heterogeneous Mixture-of-Adapters (MoA) for fine-tuning Large Language Models (LLMs) more efficiently. Unlike traditional homogeneous MoE-LoRA methods that use similar adapter experts, MoA combines diverse adapter structures to improve performance and prevent issues like representation collapse. The method includes two variants: Soft MoA, which fuses outputs from all experts, and Sparse MoA, which selectively activates experts based on their effectiveness. Experimental results show that MoA significantly outperforms existing methods in both performance and parameter efficiency.'}, 'zh': {'title': '异构适配器混合：提升大语言模型微调效率的创新方法', 'desc': '本文提出了一种异构的适配器混合（MoA）方法，以增强大语言模型（LLM）中参数高效微调的效果。与传统的同质MoE-LoRA方法不同，MoA集成了具有不同结构的适配器专家，从而克服了表示崩溃和专家负载不平衡的问题。该方法通过动态整合适配器专家的互补表示能力，促进了专家的专业化，提升了预训练知识向下游任务的有效转移。实验结果表明，异构MoA在性能和参数效率上均优于同质MoE-LoRA方法。'}}}, {'id': 'https://huggingface.co/papers/2506.05700', 'title': 'RKEFino1: A Regulation Knowledge-Enhanced Large Language Model', 'url': 'https://huggingface.co/papers/2506.05700', 'abstract': 'RKEFino1, a knowledge-enhanced financial reasoning model, addresses accuracy and compliance challenges in Digital Regulatory Reporting through fine-tuning with domain knowledge from XBRL, CDM, and MOF, and introduces a novel Numerical NER task.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) hold great promise for financial applications but introduce critical accuracy and compliance challenges in Digital Regulatory Reporting (DRR). To address these issues, we propose RKEFino1, a regulation knowledge-enhanced financial reasoning model built upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We formulate two QA tasks-knowledge-based and mathematical reasoning-and introduce a novel Numerical NER task covering financial entities in both sentences and tables. Experimental results demonstrate the effectiveness and generalization capacity of RKEFino1 in compliance-critical financial tasks. We have released our model on Hugging Face.', 'score': 2, 'issue_id': 4231, 'pub_date': '2025-06-06', 'pub_date_card': {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'}, 'hash': 'a23a28bb68811316', 'authors': ['Yan Wang', 'Yueru He', 'Ruoyu Xiang', 'Jeff Zhao'], 'affiliations': ['Columbia University', 'New York University', 'The University of Texas at Austin', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05700.jpg', 'data': {'categories': ['#data', '#multimodal', '#reasoning', '#training', '#healthcare', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Умная финансовая модель для точной регуляторной отчетности', 'desc': 'RKEFino1 - это модель финансового рассуждения, улучшенная знаниями о регулировании, построенная на основе Fino1. Она решает проблемы точности и соответствия нормам в цифровой регуляторной отчетности путем дообучения на доменных знаниях из XBRL, CDM и MOF. Модель сформулирована для решения двух задач вопросно-ответной системы: на основе знаний и математических рассуждений. RKEFino1 также вводит новую задачу числового распознавания именованных сущностей (NER) для финансовых объектов в предложениях и таблицах.'}, 'en': {'title': 'Enhancing Financial Compliance with RKEFino1', 'desc': 'RKEFino1 is a financial reasoning model designed to improve accuracy and compliance in Digital Regulatory Reporting (DRR). It enhances the Fino1 model by incorporating domain knowledge from XBRL, CDM, and MOF, which are essential for understanding financial regulations. The model introduces a new task called Numerical Named Entity Recognition (NER) to identify financial entities in both text and tabular formats. Experimental results show that RKEFino1 effectively addresses compliance challenges and generalizes well to various financial tasks.'}, 'zh': {'title': '知识增强的金融推理，提升合规性与准确性', 'desc': 'RKEFino1是一种增强知识的金融推理模型，旨在解决数字监管报告中的准确性和合规性挑战。该模型基于Fino1，并通过XBRL、CDM和MOF等领域知识进行微调。我们提出了两个问答任务——基于知识的问答和数学推理，并引入了一种新的数值命名实体识别任务，涵盖了句子和表格中的金融实体。实验结果表明，RKEFino1在合规性关键的金融任务中表现出色，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.07047', 'title': 'Mathesis: Towards Formal Theorem Proving from Natural Languages', 'url': 'https://huggingface.co/papers/2506.07047', 'abstract': "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.", 'score': 1, 'issue_id': 4237, 'pub_date': '2025-06-08', 'pub_date_card': {'ru': '8 июня', 'en': 'June 8', 'zh': '6月8日'}, 'hash': 'd3bc82dde4f2b8bc', 'authors': ['Yu Xuejun', 'Jianyuan Zhong', 'Zijin Feng', 'Pengyi Zhai', 'Roozbeh Yousefzadeh', 'Wei Chong Ng', 'Haoxiong Liu', 'Ziyi Shou', 'Jing Xiong', 'Yudong Zhou', 'Claudia Beth Ong', 'Austen Jeremy Sugiarto', 'Yaoxi Zhang', 'Wai Ming Tai', 'Huan Cao', 'Dongcai Lu', 'Jiacheng Sun', 'Qiang Xu', 'Shen Xin', 'Zhenguo Li'], 'affiliations': ['Huawei Celia Team', 'Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.07047.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#benchmark', '#dataset', '#math'], 'emoji': '🧠', 'ru': {'title': 'Автоматизация формального доказательства теорем от естественного языка до математической логики', 'desc': 'Исследователи представили Mathesis - первый сквозной конвейер для доказательства теорем, обрабатывающий неформальные формулировки задач. Ключевым компонентом является Mathesis-Autoformalizer, использующий обучение с подкреплением для улучшения формализации естественно-языковых задач. Система также включает Mathesis-Prover для генерации формальных доказательств. Для оценки применимости подхода авторы создали набор данных Gaokao-Formal из 488 сложных задач китайского вступительного экзамена в вузы.'}, 'en': {'title': 'Bridging Natural Language and Formal Reasoning with Mathesis', 'desc': "This paper presents Mathesis, a novel end-to-end theorem proving system that processes informal problem statements, addressing the limitations of existing LLM-based theorem provers. It introduces Mathesis-Autoformalizer, which utilizes reinforcement learning to automatically convert natural language problems into formal statements, supported by the LeanScorer framework for assessing formalization quality. Additionally, the Mathesis-Prover generates formal proofs from these formalized statements. The system's effectiveness is validated through experiments on the Gaokao-Formal benchmark, demonstrating significant improvements in accuracy and pass rates compared to existing methods."}, 'zh': {'title': 'Mathesis：非正式问题的定理证明新突破', 'desc': '本论文介绍了Mathesis，这是第一个能够处理非正式问题陈述的端到端定理证明管道。它包括Mathesis-Autoformalizer，这是一个使用强化学习的自动形式化工具，能够提高自然语言问题的形式化能力，并通过LeanScorer框架评估形式化质量。论文还提出了Mathesis-Prover，能够从形式化的陈述中生成正式证明。通过在中国高考的488个复杂问题上进行评估，实验结果表明Mathesis在通过率和准确性上均优于现有模型。'}}}, {'id': 'https://huggingface.co/papers/2506.04688', 'title': 'MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2506.04688', 'abstract': "MMRefine evaluates the error refinement capabilities of Multimodal Large Language Models through a benchmark that categorizes errors and identifies performance bottlenecks.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces MMRefine, a MultiModal Refinement benchmark designed to evaluate the error refinement capabilities of Multimodal Large Language Models (MLLMs). As the emphasis shifts toward enhancing reasoning during inference, MMRefine provides a framework that evaluates MLLMs' abilities to detect and correct errors across six distinct scenarios beyond just comparing final accuracy before and after refinement. Furthermore, the benchmark analyzes the refinement performance by categorizing errors into six error types. Experiments with various open and closed MLLMs reveal bottlenecks and factors impeding refinement performance, highlighting areas for improvement in effective reasoning enhancement. Our code and dataset are publicly available at https://github.com/naver-ai/MMRefine.", 'score': 1, 'issue_id': 4241, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'd7ca0acf8a8fe586', 'authors': ['Gio Paik', 'Geewook Kim', 'Jinbae Im'], 'affiliations': ['KAIST AI', 'NAVER Cloud AI', 'Theta One, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.04688.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'MMRefine: Новый подход к оценке мультимодальных языковых моделей', 'desc': 'Статья представляет MMRefine - бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) обнаруживать и исправлять ошибки. Бенчмарк анализирует шесть различных сценариев и типов ошибок, выходя за рамки простого сравнения точности. Эксперименты с различными MLLM выявляют узкие места и факторы, препятствующие эффективному уточнению результатов. Исследование направлено на улучшение рассуждений моделей во время вывода.'}, 'en': {'title': 'Refining Reasoning: Enhancing MLLMs with MMRefine', 'desc': "MMRefine is a benchmark designed to assess how well Multimodal Large Language Models (MLLMs) can refine their outputs by correcting errors. It categorizes errors into six types and evaluates the models' performance in detecting and addressing these errors during inference. The focus is not just on the final accuracy but also on the models' reasoning capabilities before and after refinement. The findings from experiments highlight specific bottlenecks that hinder effective error correction, providing insights for future improvements in MLLMs."}, 'zh': {'title': '提升多模态模型的错误修正能力', 'desc': 'MMRefine是一个用于评估多模态大型语言模型（MLLMs）错误修正能力的基准测试。它不仅关注最终准确率的比较，还通过六种不同场景来检测和纠正错误。该基准将错误分为六种类型，以分析修正性能的瓶颈和影响因素。实验结果揭示了在推理增强方面的改进空间，促进了对MLLMs的进一步研究。'}}}, {'id': 'https://huggingface.co/papers/2506.04020', 'title': 'QQSUM: A Novel Task and Model of Quantitative Query-Focused\n  Summarization for Review-based Product Question Answering', 'url': 'https://huggingface.co/papers/2506.04020', 'abstract': 'QQSUM-RAG extends Retrieval-Augmented Generation to provide diverse, representative Key Point summaries with quantified opinions for product question answering, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM', 'score': 0, 'issue_id': 4240, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '8e25a42b034fd30c', 'authors': ['An Quang Tang', 'Xiuzhen Zhang', 'Minh Ngoc Dinh', 'Zhuang Li'], 'affiliations': ['RMIT University, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.04020.jpg', 'data': {'categories': ['#rag', '#optimization', '#multimodal', '#open_source', '#games'], 'emoji': '🛍️', 'ru': {'title': 'Умные ответы на вопросы о товарах с учетом разнообразия мнений покупателей', 'desc': 'Статья представляет новый метод QQSUM-RAG для ответов на вопросы о продуктах на основе отзывов. Этот подход расширяет технологию Retrieval-Augmented Generation, чтобы генерировать разнообразные ключевые моменты из отзывов и количественно оценивать их распространенность. QQSUM-RAG использует few-shot learning для совместного обучения ретривера и генератора суммаризаций. Эксперименты показывают, что QQSUM-RAG превосходит существующие методы по качеству текста и точности количественной оценки мнений.'}, 'en': {'title': 'Diverse Insights for Better Product Answers', 'desc': 'The paper introduces QQSUM-RAG, an advanced model that enhances Retrieval-Augmented Generation (RAG) for product question answering (PQA) by generating diverse and representative Key Point summaries. It addresses the limitation of existing PQA systems that typically provide answers from a single viewpoint, thereby missing the variety of customer opinions. QQSUM focuses on Quantitative Query-Focused Summarization, which not only summarizes diverse opinions but also quantifies their prevalence to improve response accuracy. Experimental results show that QQSUM-RAG outperforms current RAG methods in both the quality of generated text and the accuracy of opinion quantification.'}, 'zh': {'title': '多样化客户意见的智能摘要生成', 'desc': '本文提出了一种新的任务，称为定量查询聚焦摘要（QQSUM），旨在将多样化的客户意见总结为代表性的关键点（KPs），并量化其普遍性，以有效回答用户查询。QQSUM-RAG模型扩展了检索增强生成（RAG），通过少量学习联合训练KP导向的检索器和KP摘要生成器，从而生成能够捕捉多样化和代表性意见的摘要。实验结果表明，QQSUM-RAG在文本质量和意见量化准确性方面优于现有的RAG基线方法。该研究为电子商务平台的产品问答系统提供了更全面的客户意见视角。'}}}, {'id': 'https://huggingface.co/papers/2506.14028', 'title': 'MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation', 'url': 'https://huggingface.co/papers/2506.14028', 'abstract': 'MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.', 'score': 65, 'issue_id': 4360, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'e94d60496c8d96d1', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#multilingual', '#benchmark', '#multimodal', '#reasoning', '#financial', '#dataset'], 'emoji': '💹', 'ru': {'title': 'MultiFinBen: Преодолевая языковые и модальные барьеры в финансовом ИИ', 'desc': 'MultiFinBen - это многоязычный и мультимодальный бенчмарк для задач в финансовой сфере, оценивающий большие языковые модели (LLM) в различных модальностях и языковых настройках. Он включает в себя новые задачи, такие как PolyFiQA и OCR-встроенные финансовые вопросно-ответные задачи, требующие сложных рассуждений над смешанными языковыми входными данными и визуально-текстовыми финансовыми документами. Бенчмарк использует динамический механизм отбора с учетом сложности и представляет собой компактный, сбалансированный набор данных. Оценка 22 современных моделей показала, что даже самые сильные из них испытывают значительные трудности при решении сложных межъязыковых и мультимодальных задач в финансовой области.'}, 'en': {'title': 'MultiFinBen: Bridging Multilingual and Multimodal Gaps in Financial AI', 'desc': 'MultiFinBen is a new benchmark designed to test large language models (LLMs) in the financial sector using multiple languages and types of data, such as text, images, and audio. It addresses the limitations of previous benchmarks that only focused on single languages and simple tasks, which do not reflect the complexities of real-world financial communication. The benchmark includes innovative tasks that require models to understand and reason with mixed-language inputs and to extract information from visual financial documents. Evaluation of various advanced models shows that even the best-performing ones struggle with these challenging tasks, highlighting the need for improved capabilities in financial reasoning across different languages and modalities.'}, 'zh': {'title': '多语言多模态金融基准，推动金融智能进步', 'desc': 'MultiFinBen是一个针对金融领域任务的多语言和多模态基准，旨在评估大型语言模型（LLMs）在不同模态和语言环境下的表现。该基准揭示了在复杂的跨语言和多模态金融推理中存在的挑战。我们引入了两个新任务，PolyFiQA-Easy和PolyFiQA-Expert，要求模型在混合语言输入上进行复杂推理。此外，MultiFinBen还提供了一种动态的、关注难度的选择机制，以确保基准的平衡性和有效性。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.12928', 'title': 'Scaling Test-time Compute for LLM Agents', 'url': 'https://huggingface.co/papers/2506.12928', 'abstract': "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", 'score': 36, 'issue_id': 4351, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '39c8f3e831e90d93', 'authors': ['King Zhu', 'Hanhao Li', 'Siwei Wu', 'Tianshun Xing', 'Dehua Ma', 'Xiangru Tang', 'Minghao Liu', 'Jian Yang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Changwang Zhang', 'Chenghua Lin', 'Jun Wang', 'Ge Zhang', 'Wangchunshu Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.12928.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование вычислений улучшает работу языковых агентов', 'desc': 'Исследование методов масштабирования во время тестирования для языковых агентов на основе больших языковых моделей (LLM) показало улучшение их эффективности. Были изучены различные стратегии, включая параллельную выборку, последовательную корректировку, верификацию и диверсификацию развертываний. Результаты демонстрируют, что увеличение вычислительных ресурсов на этапе тестирования повышает производительность агентов. Особенно эффективными оказались методы списочной верификации и увеличения разнообразия развертываний.'}, 'en': {'title': 'Boosting Language Agents with Test-Time Scaling', 'desc': 'This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches.'}, 'zh': {'title': '测试时间扩展提升语言代理性能', 'desc': '本文系统探讨了在大型语言模型（LLMs）中应用测试时间扩展方法的效果。研究表明，计算扩展能够显著提升语言代理的推理能力，尤其是通过并行采样、顺序修订、有效验证和增加多样化的回滚策略。我们分析了不同设计策略对语言代理性能的影响，并发现测试时间计算的扩展确实能提高代理的表现。特别是，采用列表式验证方法效果最佳，而多样化的回滚策略也对任务表现有积极影响。'}}}, {'id': 'https://huggingface.co/papers/2506.12285', 'title': 'CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following', 'url': 'https://huggingface.co/papers/2506.12285', 'abstract': 'CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking: reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.', 'score': 34, 'issue_id': 4360, 'pub_date': '2025-06-14', 'pub_date_card': {'ru': '14 июня', 'en': 'June 14', 'zh': '6月14日'}, 'hash': 'c0c91a24a40dfd14', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#open_source', '#survey', '#audio', '#benchmark', '#ethics'], 'emoji': '🎵', 'ru': {'title': 'CMI-Bench: Новый стандарт оценки музыкальных LLM', 'desc': 'CMI-Bench представляет собой комплексный бенчмарк для оценки аудио-текстовых языковых моделей (LLM) в задачах извлечения музыкальной информации. Бенчмарк включает широкий спектр задач, таких как классификация жанров, распознавание эмоций, определение инструментов и транскрипция текстов. CMI-Bench использует стандартизированные метрики оценки, что обеспечивает прямое сравнение с современными специализированными моделями. Результаты экспериментов выявляют значительные различия в производительности между LLM и специализированными моделями, а также культурные, хронологические и гендерные предубеждения LLM.'}, 'en': {'title': 'CMI-Bench: Advancing Music Understanding with LLMs', 'desc': 'CMI-Bench is a new benchmark designed to evaluate audio-text large language models (LLMs) on various music information retrieval (MIR) tasks. It addresses the limitations of existing benchmarks by providing a comprehensive set of instruction-following tasks that reflect real-world music analysis complexities. The benchmark includes diverse tasks such as genre classification, emotion tagging, and melody extraction, using standardized evaluation metrics for consistency with state-of-the-art models. Results from experiments show performance gaps between LLMs and supervised models, revealing both the potential and limitations of current LLMs in handling MIR challenges.'}, 'zh': {'title': 'CMI-Bench：音乐信息检索的新基准', 'desc': 'CMI-Bench是一个全面的音频文本大语言模型（LLM）指令跟随基准，旨在评估它们在多样化的音乐信息检索（MIR）任务上的表现。该基准重新解释了传统MIR注释为指令跟随格式，涵盖了如流派分类、情感回归、乐器分类等多项任务。与以往的基准不同，CMI-Bench采用标准化评估指标，确保与现有的监督学习模型直接可比。实验结果显示LLM与监督模型之间存在显著的性能差距，揭示了当前模型在MIR任务中的潜力和局限性。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14429', 'title': 'LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2506.14429', 'abstract': 'This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.', 'score': 33, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'd0032538675516d6', 'authors': ['Xiaoran Liu', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14429.jpg', 'data': {'categories': ['#training', '#long_context', '#architecture', '#benchmark', '#diffusion', '#rl'], 'emoji': '🔬', 'ru': {'title': 'Диффузионные языковые модели: новые горизонты в обработке длинного контекста', 'desc': "Это исследование сравнивает производительность диффузионных и авторегрессивных языковых моделей при работе с длинным контекстом. Авторы обнаружили, что диффузионные модели демонстрируют стабильную перплексивность при расширении контекста и обладают феноменом 'локального восприятия'. На основе этих наблюдений был разработан метод LongLLaDA для расширения контекстного окна диффузионных моделей без дополнительного обучения. Исследование также выявило задачи, в которых диффузионные модели превосходят авторегрессивные при работе с длинным контекстом."}, 'en': {'title': 'Unlocking Long Contexts in Diffusion LLMs with LongLLaDA', 'desc': 'This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications.'}, 'zh': {'title': '扩散模型的长上下文新方法：LongLLaDA', 'desc': '本研究探讨了扩散大语言模型（diffusion LLMs）与自回归大语言模型（auto-regressive LLMs）在长上下文性能方面的比较，识别了它们的独特特性，并提出了一种无训练的方法LongLLaDA来扩展上下文窗口。研究发现，扩散LLMs在直接上下文外推时保持了显著稳定的困惑度，而自回归模型在上下文超出预训练长度时则表现不佳。扩散LLMs展现出独特的局部感知现象，使其能够成功从最近的上下文片段中检索信息。通过旋转位置嵌入（RoPE）缩放理论，我们解释了这些现象，并验证了扩散LLMs的上下文外推方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.14245', 'title': 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs', 'url': 'https://huggingface.co/papers/2506.14245', 'abstract': 'RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', 'score': 25, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c78cc63a970ea4e9', 'authors': ['Xumeng Wen', 'Zihan Liu', 'Shun Zheng', 'Zhijian Xu', 'Shengyu Ye', 'Zhirong Wu', 'Xiao Liang', 'Yang Wang', 'Junjie Li', 'Ziming Miao', 'Jiang Bian', 'Mao Yang'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14245.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RLVR: путь к логически обоснованным рассуждениям ИИ', 'desc': 'Статья представляет новый подход к улучшению рассуждений моделей машинного обучения - Reinforcement Learning with Verifiable Rewards (RLVR). Авторы вводят более точную метрику оценки CoT-Pass@K, которая учитывает корректность как цепочки рассуждений, так и конечного ответа. Исследование показывает, что RLVR действительно способствует обобщению правильных рассуждений для всех значений K. Результаты подтверждают потенциал RLVR для существенного улучшения машинных рассуждений.'}, 'en': {'title': 'Enhancing Machine Reasoning with RLVR and CoT-Pass@K', 'desc': 'Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios.'}, 'zh': {'title': 'RLVR：推动机器推理的新方法', 'desc': 'RLVR（可验证奖励的强化学习）通过激励正确和逻辑的思维链，推动了机器推理的发展。研究发现，传统的评估指标Pass@K存在缺陷，可能会错误地认可不完整的思维链所得到的正确答案。为了解决这个问题，本文提出了更精确的评估指标CoT-Pass@K，要求推理路径和最终答案都必须正确。我们的实验证明，RLVR能够有效激励正确推理的泛化，并且这种增强的推理能力在训练早期就能显现。'}}}, {'id': 'https://huggingface.co/papers/2506.14234', 'title': 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team', 'url': 'https://huggingface.co/papers/2506.14234', 'abstract': "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", 'score': 24, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '70ebdc96484832ea', 'authors': ['Md Tanzib Hosain', 'Salman Rahman', 'Md Kishor Morol', 'Md Rizwan Parvez'], 'affiliations': ['American International University-Bangladesh', 'Cornell University', 'Qatar Computing Research Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14234.jpg', 'data': {'categories': ['#training', '#agents', '#agi', '#open_source', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Xolver: Опыт-ориентированные языковые агенты для экспертного рассуждения', 'desc': 'Xolver - это фреймворк мультиагентного рассуждения, который улучшает работу больших языковых моделей (LLM) за счет постоянной памяти и разнообразных модальностей опыта. Он позволяет моделям накапливать и интегрировать экспериентальные знания, подобно экспертам-решателям задач. Xolver включает в себя различные модальности опыта, такие как внешний и самостоятельный поиск, использование инструментов, совместные взаимодействия и итеративное улучшение. Даже с легковесными моделями Xolver превосходит специализированные агенты рассуждений и достигает новых лучших результатов на нескольких бенчмарках.'}, 'en': {'title': 'Empowering Language Models with Experience-Aware Reasoning', 'desc': 'Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI.'}, 'zh': {'title': 'Xolver：经验驱动的推理框架', 'desc': 'Xolver是一个多智能体推理框架，通过持久记忆和多样化的经验模式增强大型语言模型（LLM），从而提高复杂推理任务的表现。与传统的LLM孤立处理每个问题不同，Xolver能够整合和积累经验知识，模拟专家问题解决者的思维方式。它通过外部和自我检索、工具使用、协作互动等多种经验模式，避免从头生成解决方案。Xolver在多个基准测试中表现优异，展示了整体经验学习在实现通用智能体方面的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.13363', 'title': 'Efficient Medical VIE via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.13363', 'abstract': 'An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.', 'score': 22, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '29de6bf10e7470ad', 'authors': ['Lijun Liu', 'Ruiyang Li', 'Zhaocheng Liu', 'Chenglin Zhu', 'Chong Li', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie'], 'affiliations': ['Baichuan Inc.', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13363.jpg', 'data': {'categories': ['#hallucinations', '#training', '#optimization', '#healthcare', '#reasoning', '#rl', '#multimodal'], 'emoji': '🏥', 'ru': {'title': 'RLVR: Прорыв в извлечении медицинской информации из изображений', 'desc': 'Представлена система RLVR на основе модели Qwen2.5-VL-7B для извлечения визуальной информации из медицинских документов. Система достигает наилучших результатов при ограниченном количестве размеченных примеров. RLVR улучшает способности к рассуждению и балансирует точность и полноту извлечения. Метод показывает высокую эффективность на медицинских данных, но требует оптимизации для других доменов.'}, 'en': {'title': 'Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning', 'desc': "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."}, 'zh': {'title': '医疗视觉信息提取的创新突破', 'desc': '本文提出了一种基于强化学习可验证奖励（RLVR）框架的方法，利用微调的Qwen2.5-VL-7B模型，在医疗视觉信息提取（VIE）任务中实现了最先进的性能。该方法仅使用100个标注样本，解决了传统方法在医疗领域面临的高标注成本和领域特定模式的问题。通过确保数据集的多样性和平衡的精确率-召回率奖励机制，减少了模型的幻觉现象，并提高了领域覆盖率。案例研究进一步证明了在训练和推理过程中推理能力的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.13642', 'title': 'Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model', 'url': 'https://huggingface.co/papers/2506.13642', 'abstract': 'Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.', 'score': 21, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0d0624980a111254', 'authors': ['Shaolei Zhang', 'Shoutao Guo', 'Qingkai Fang', 'Yan Zhou', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13642.jpg', 'data': {'categories': ['#multimodal', '#audio', '#transfer_learning', '#cv', '#benchmark', '#agi'], 'emoji': '🔀', 'ru': {'title': 'Эффективная интеграция модальностей для мощных мультимодальных ИИ-моделей', 'desc': 'Stream-Omni - это крупная мультимодальная модель, объединяющая текст, изображения и речь. Она использует конкатенацию по измерению последовательности для визуальной информации и отображение по измерению слоев для речи, что позволяет эффективно выравнивать модальности. Модель достигает высокой производительности с меньшим количеством данных по сравнению с существующими подходами. Stream-Omni демонстрирует сильные результаты в задачах визуального понимания, речевого взаимодействия и речевого взаимодействия на основе изображений.'}, 'en': {'title': 'Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction', 'desc': "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."}, 'zh': {'title': 'Stream-Omni：高效的多模态整合模型', 'desc': 'Stream-Omni是一种大型多模态模型，能够有效整合文本、视觉和语音。它通过序列维度连接实现视觉与文本的对齐，并通过基于CTC的层维度映射实现语音与文本的对齐，从而在数据较少的情况下也能达到良好的性能。该模型支持多种模态组合的交互，能够在视觉理解、语音交互和视觉引导的语音交互任务中表现出色。Stream-Omni的设计使得用户在语音交互时可以同时获得中间文本输出，提供了全面的多模态体验。'}}}, {'id': 'https://huggingface.co/papers/2506.14758', 'title': 'Reasoning with Exploration: An Entropy Perspective', 'url': 'https://huggingface.co/papers/2506.14758', 'abstract': 'Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.', 'score': 17, 'issue_id': 4349, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '14595ff25bf8a37c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Энтропия как ключ к глубоким рассуждениям языковых моделей', 'desc': 'Статья представляет новый подход к улучшению рассуждений языковых моделей с помощью обучения с подкреплением. Авторы вводят энтропийный член в функцию преимущества, что способствует исследовательскому поведению модели. Этот метод приводит к значительному улучшению производительности на сложных задачах рассуждения, особенно по метрике Pass@K. Исследование показывает, что высокая энтропия коррелирует с ключевыми токенами, рефлексивными действиями и редкими поведениями модели.'}, 'en': {'title': 'Enhancing Language Model Reasoning through Entropy-Driven Exploration', 'desc': 'This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities.'}, 'zh': {'title': '增强语言模型推理的探索性', 'desc': '本文提出了一种基于熵的术语，应用于强化学习中的优势函数，以增强语言模型的探索性推理能力。这种方法通过引入熵信号，促进了探索与利用之间的平衡，解决了语言模型在复杂推理任务中性能停滞的问题。研究表明，高熵区域与三种探索性推理行为之间存在强正相关，包括关键标记、反思性行为和稀有行为。通过简单的代码修改，我们的方法显著提高了语言模型的推理能力，尤其在Pass@K指标上取得了显著进展。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14603', 'title': 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation', 'url': 'https://huggingface.co/papers/2506.14603', 'abstract': 'Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.', 'score': 13, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c235653dff87ea28', 'authors': ['Amirmojtaba Sabour', 'Sanja Fidler', 'Karsten Kreis'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14603.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#benchmark', '#optimization', '#diffusion', '#small_models'], 'emoji': '🌊', 'ru': {'title': 'Flow maps: революция в эффективной генерации изображений', 'desc': "Статья представляет новый подход к генеративному моделированию под названием 'flow maps'. Эта техника позволяет эффективно соединять любые два уровня шума за один шаг, сохраняя высокую производительность при различном количестве шагов. Авторы вводят новые непрерывные по времени целевые функции и техники обучения для flow maps. Модели, обученные с помощью этого метода, достигают наилучших результатов в генерации изображений и преобразовании текста в изображения за небольшое количество шагов."}, 'en': {'title': 'Flow Maps: Efficient Few-Step Generative Modeling', 'desc': 'This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis.'}, 'zh': {'title': '流图模型：高效的少步骤生成新方法', 'desc': '本文介绍了一种新的流图模型，旨在提高图像和文本到图像生成的效率。流图通过在单一步骤中连接任意两个噪声水平，克服了传统扩散和流模型在多步骤采样中的性能下降问题。我们提出了两种新的连续时间目标和训练技术，进一步优化了流图的训练过程。实验结果表明，流图模型在图像生成基准测试中表现出色，尤其是在少步骤生成任务中，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.12860', 'title': 'QFFT, Question-Free Fine-Tuning for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2506.12860', 'abstract': 'Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '4e8d6c1da3d2fdd1', 'authors': ['Wanlong Liu', 'Junxiao Xu', 'Fei Yu', 'Yukang Lin', 'Ke Ji', 'Wenyu Chen', 'Yan Xu', 'Yasheng Wang', 'Lifeng Shang', 'Benyou Wang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12860.jpg', 'data': {'categories': ['#training', '#math', '#long_context', '#reasoning', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'QFFT: эффективное обучение ИИ гибкому мышлению', 'desc': 'Статья представляет новый метод обучения когнитивных моделей - Question-Free Fine-Tuning (QFFT). QFFT позволяет моделям эффективно использовать как короткие, так и длинные цепочки рассуждений. Этот подход сокращает среднюю длину ответов более чем на 50%, сохраняя при этом производительность на уровне стандартного обучения с учителем. QFFT также демонстрирует превосходные результаты в сценариях с шумом, вне домена и при ограниченных ресурсах.'}, 'en': {'title': 'Efficient Reasoning with Question-Free Fine-Tuning', 'desc': 'This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts.'}, 'zh': {'title': '无问微调：高效适应的推理新方法', 'desc': '这篇论文提出了一种新的微调方法，称为无问微调（QFFT），旨在提高认知模型的效率和适应性。通过结合短链和长链推理模式，QFFT能够在保持性能的同时减少响应长度。研究表明，短链推理在简单问题上表现出色，而长链推理在复杂任务中更具优势。实验结果显示，QFFT在多个数学数据集上平均响应长度减少超过50%，并在噪声、域外和低资源场景中表现优于传统的监督微调（SFT）。'}}}, {'id': 'https://huggingface.co/papers/2506.12278', 'title': 'Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure', 'url': 'https://huggingface.co/papers/2506.12278', 'abstract': 'TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.', 'score': 13, 'issue_id': 4348, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'c852db550c523453', 'authors': ['Zheyuan Yang', 'Zexi Kuang', 'Xue Xia', 'Yilun Zhao'], 'affiliations': ['HKUST', 'Northeastern University', 'Tongji University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12278.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'TestCase-Eval: Новый стандарт оценки ЯМ в генерации тестов', 'desc': 'TestCase-Eval - это новый бенчмарк для оценки способности языковых моделей (ЯМ) генерировать тестовые случаи для алгоритмических задач. Он включает 500 задач и 100 000 решений с платформы Codeforces. Бенчмарк фокусируется на двух ключевых задачах: покрытие ошибок и выявление ошибок. Авторы провели оценку 19 современных ЯМ на TestCase-Eval, предоставив анализ их сильных и слабых сторон в генерации эффективных тестовых случаев.'}, 'en': {'title': 'Evaluating LLMs for Effective Test Case Generation', 'desc': 'TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area.'}, 'zh': {'title': '评估LLM生成测试用例的新基准', 'desc': 'TestCase-Eval是一个用于评估大型语言模型（LLMs）生成算法问题测试用例的新基准。它包含500个算法问题和来自Codeforces平台的100,000个人工解决方案。该基准关注两个关键任务：故障覆盖性，评估LLM生成的测试集是否能够探测多样的输入场景；故障暴露性，评估LLM是否能够生成特定的测试输入以揭示代码实现中的错误。我们对19个最先进的开源和专有LLM在TestCase-Eval上的表现进行了全面评估，提供了它们在生成有效测试用例方面的优缺点的见解。'}}}, {'id': 'https://huggingface.co/papers/2506.14606', 'title': 'Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees', 'url': 'https://huggingface.co/papers/2506.14606', 'abstract': 'A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.', 'score': 10, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c414a1f31e0417da', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Chaimaa Abi', 'Celine Lee', 'Abdulrahman Mahmoud'], 'affiliations': ['Cornell University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14606.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark', '#data', '#science'], 'emoji': '🔄', 'ru': {'title': 'ЯМБ и тестирование объединяются для эффективной транспиляции между архитектурами процессоров', 'desc': 'Статья представляет новый подход к транспиляции программ между различными архитектурами процессоров с использованием больших языковых моделей и методов тестирования программного обеспечения. Метод GG (Guaranteed Guess) генерирует варианты перевода кода с одной архитектуры на другую с помощью ЯМБ и проверяет их корректность через встроенную систему тестирования. Результаты показывают высокую точность перевода (99% для HumanEval и 49% для BringupBench) и превосходство над существующими решениями по производительности, энергоэффективности и использованию памяти. Авторы планируют открыть исходный код, данные и модели для дальнейших исследований в этой области.'}, 'en': {'title': 'Efficient ISA Translation with Guaranteed Guess', 'desc': 'This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation.'}, 'zh': {'title': '高效准确的ISA转译新方法', 'desc': '本文提出了一种新的ISA中心的转译管道，利用大型语言模型（LLMs）和软件测试技术，实现了在复杂和简化硬件架构之间的高效且正确的代码转换。该方法通过LLM生成候选翻译，并将其嵌入软件测试框架中，以提高翻译的可靠性。我们在两个不同的数据集上评估了该方法，达到了99%的功能和语义正确率，并且在性能上优于现有的Rosetta 2框架。最终，我们将开源代码、数据、模型和基准，以推动ISA级代码翻译研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.13977', 'title': 'CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios', 'url': 'https://huggingface.co/papers/2506.13977', 'abstract': 'A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.', 'score': 8, 'issue_id': 4351, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'd72fbcfec0e28e92', 'authors': ['Shiting Huang', 'Zhen Fang', 'Zehui Chen', 'Siyu Yuan', 'Junjie Ye', 'Yu Zeng', 'Lin Chen', 'Qi Mao', 'Feng Zhao'], 'affiliations': ['Communication University of China', 'Fudan University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13977.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#optimization'], 'emoji': '🛠️', 'ru': {'title': 'Повышение надежности языковых моделей при работе с инструментами', 'desc': 'CRITICTOOL - это комплексный бенчмарк для оценки и повышения устойчивости больших языковых моделей (LLM) при обработке ошибок во время использования инструментов. Он анализирует типы ошибок, возникающих в процессе вызова функций, и использует эволюционную стратегию для создания набора данных с разнообразными ошибками различной сложности. CRITICTOOL проводит обширные эксперименты для оценки способности LLM к рефлексии при использовании инструментов. Этот бенчмарк предлагает новый взгляд на область обучения инструментам в контексте больших языковых моделей.'}, 'en': {'title': 'Enhancing LLM Robustness with CRITICTOOL', 'desc': 'This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities.'}, 'zh': {'title': '提升大型语言模型工具使用的鲁棒性', 'desc': '本文介绍了CRITICTOOL，一个全面的基准测试工具，用于评估和增强大型语言模型在使用工具时处理错误的能力。随着任务的复杂性增加，工具使用过程中可能会出现各种意外错误，因此有效处理这些错误成为了一个重要的研究方向。我们分析了在多个竞争性工具评估基准中遇到的错误类型，并基于此构建了CRITICTOOL，专注于工具学习的批判性评估。通过广泛的实验，我们验证了该基准策略的有效性，并提供了对不同大型语言模型工具反应能力的深入分析。'}}}, {'id': 'https://huggingface.co/papers/2506.09985', 'title': 'V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning', 'url': 'https://huggingface.co/papers/2506.09985', 'abstract': 'A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.  \t\t\t\t\tAI-generated summary \t\t\t\t A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.', 'score': 7, 'issue_id': 4359, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '2a9d0d368d8caa0a', 'authors': ['Mido Assran', 'Adrien Bardes', 'David Fan', 'Quentin Garrido', 'Russell Howes', 'Mojtaba', 'Komeili', 'Matthew Muckley', 'Ammar Rizvi', 'Claire Roberts', 'Koustuv Sinha', 'Artem Zholus', 'Sergio Arnaud', 'Abha Gejji', 'Ada Martin', 'Francois Robert Hogan', 'Daniel Dugas', 'Piotr Bojanowski', 'Vasil Khalidov', 'Patrick Labatut', 'Francisco Massa', 'Marc Szafraniec', 'Kapil Krishnakumar', 'Yong Li', 'Xiaodong Ma', 'Sarath Chandar', 'Franziska Meier', 'Yann LeCun', 'Michael Rabbat', 'Nicolas Ballas'], 'affiliations': ['FAIR at Meta', 'Mila Quebec AI Institute and Polytechnique Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2506.09985.jpg', 'data': {'categories': ['#multimodal', '#games', '#transfer_learning', '#dataset', '#agi', '#cv', '#robotics', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучение на масштабных данных для понимания и планирования в физическом мире', 'desc': 'Статья представляет самообучающийся подход, сочетающий масштабные видеоданные из интернета с минимальным взаимодействием робота. Модель V-JEPA 2 предобучается на более чем 1 миллионе часов видео и достигает высоких результатов в понимании движений и предсказании действий. После выравнивания с большой языковой моделью, V-JEPA 2 показывает передовые результаты в задачах ответов на вопросы по видео. Дополнительное обучение на 62 часах видео с роботами позволяет модели V-JEPA 2-AC осуществлять планирование действий для манипуляторов без специфического обучения под задачу.'}, 'en': {'title': 'Learning to Act by Watching: Self-Supervised Motion Understanding and Planning', 'desc': 'This paper presents a self-supervised learning method that leverages vast amounts of internet video data alongside minimal robot interaction to enhance motion understanding, action anticipation, and robotic planning. The authors introduce V-JEPA 2, a joint-embedding-predictive architecture pre-trained on over 1 million hours of video, achieving impressive results in various tasks without the need for specific training or rewards. By aligning V-JEPA 2 with a large language model, they also achieve state-of-the-art performance in video question-answering tasks. Furthermore, they demonstrate the application of this approach in robotic planning, enabling robots to perform tasks like object manipulation using learned models without additional data collection.'}, 'zh': {'title': '自监督学习：从视频到机器人规划的突破', 'desc': '本论文提出了一种自监督学习方法，结合了互联网视频数据和少量机器人交互数据，以实现运动理解、动作预测、视频问答和机器人规划等任务。我们首先在超过100万小时的视频数据集上预训练了一个无动作的联合嵌入预测架构V-JEPA 2，取得了运动理解和人类动作预测的优异表现。通过与大型语言模型对齐，V-JEPA 2在多个视频问答任务上也达到了最先进的性能。最后，我们展示了如何将自监督学习应用于机器人规划任务，成功实现了在不同实验室中使用V-JEPA 2-AC进行物体的抓取和放置，而无需特定任务的训练或奖励。'}}}, {'id': 'https://huggingface.co/papers/2506.13651', 'title': 'xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations', 'url': 'https://huggingface.co/papers/2506.13651', 'abstract': "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.", 'score': 6, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'ce4f94d367671b84', 'authors': ['Kaiyuan Chen', 'Yixin Ren', 'Yang Liu', 'Xiaobo Hu', 'Haotong Tian', 'Tianbao Xie', 'Fangfu Liu', 'Haoye Zhang', 'Hongzhang Liu', 'Yuan Gong', 'Chen Sun', 'Han Hou', 'Hui Yang', 'James Pan', 'Jianan Lou', 'Jiayi Mao', 'Jizheng Liu', 'Jinpeng Li', 'Kangyi Liu', 'Kenkun Liu', 'Rui Wang', 'Run Li', 'Tong Niu', 'Wenlong Zhang', 'Wenqi Yan', 'Xuanzheng Wang', 'Yuchen Zhang', 'Yi-Hsin Hung', 'Yuan Jiang', 'Zexuan Liu', 'Zihan Yin', 'Zijian Ma', 'Zhiwen Mo'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Imperial College London', 'Massachusetts Institute of Technology', 'National University of Singapore', 'Peking University', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong (Shenzhen)', 'The Ohio State University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'University of Oxford', 'University of Pennsylvania', 'University of Science and Technology of China', 'University of Sydney', 'University of Toronto', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13651.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': '📊', 'ru': {'title': 'xbench: оценка ИИ-агентов в реальных профессиональных задачах', 'desc': 'Представлен xbench - динамический набор оценок для ИИ-агентов, ориентированный на реальные профессиональные задачи. В отличие от существующих бенчмарков, xbench фокусируется на коммерчески значимых областях с задачами, определенными профессионалами индустрии. Фреймворк создает метрики, коррелирующие с продуктивностью, позволяет прогнозировать соответствие технологий рынку и отслеживать возможности продуктов. Представлены два начальных бенчмарка: для рекрутинга и маркетинга, оценивающие способности агентов в реальных бизнес-сценариях.'}, 'en': {'title': 'Bridging AI Performance with Real-World Productivity', 'desc': "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."}, 'zh': {'title': 'xbench：连接AI能力与真实生产力的桥梁', 'desc': '我们介绍了xbench，这是一个动态的、与职业相关的评估套件，旨在弥合人工智能代理能力与现实世界生产力之间的差距。现有的基准测试通常关注孤立的技术技能，但可能无法准确反映代理在专业环境中所带来的经济价值。为了解决这个问题，xbench针对商业上重要的领域，评估任务由行业专业人士定义。我们的框架创建了与生产力价值高度相关的指标，能够预测技术市场契合度（TMF），并便于跟踪产品能力的变化。'}}}, {'id': 'https://huggingface.co/papers/2506.10100', 'title': 'EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.10100', 'abstract': 'EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.', 'score': 6, 'issue_id': 4348, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '6a877c4c5f1d1f72', 'authors': ['Yantai Yang', 'Yuhao Wang', 'Zichen Wen', 'Luo Zhongwei', 'Chang Zou', 'Zhipeng Zhang', 'Chuan Wen', 'Linfeng Zhang'], 'affiliations': ['Harbin Institute of Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'University of Electronic Science and Technology of China', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10100.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#inference', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'EfficientVLA: ускорение моделей VLA без потери качества', 'desc': 'EfficientVLA - это фреймворк для ускорения вывода моделей Vision-Language-Action (VLA). Он использует три основные стратегии: обрезание избыточных слоев в языковом модуле, оптимизацию отбора визуальных токенов и кэширование промежуточных признаков в диффузионной голове действий. Применение EfficientVLA к модели CogACT позволило достичь ускорения вывода в 1,93 раза и сокращения FLOP на 71,1% при минимальном снижении точности. Этот подход позволяет эффективно устранить вычислительные и память барьеры в моделях VLA.'}, 'en': {'title': 'Accelerating VLA Models with EfficientVLA', 'desc': 'EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance.'}, 'zh': {'title': '高效加速视觉-语言-动作模型的解决方案', 'desc': 'EfficientVLA是一种加速视觉-语言-动作（VLA）模型的框架，通过修剪语言层、优化视觉标记选择和缓存中间特征来提高效率。该方法系统性地消除了计算和内存瓶颈，解决了现有加速方法无法全面应对的问题。通过分析层间冗余，EfficientVLA去除了功能不重要的语言模块层，并采用任务感知策略优化视觉处理路径。实验结果表明，应用EfficientVLA后，标准VLA模型CogACT的推理速度提高了1.93倍，FLOPs减少至28.9%，成功率仅下降0.6%。'}}}, {'id': 'https://huggingface.co/papers/2506.14002', 'title': 'Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2506.14002', 'abstract': "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", 'score': 5, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'f27daadffcce7200', 'authors': ['Siyu Chen', 'Heejune Sheen', 'Xuyuan Xiong', 'Tianhao Wang', 'Zhuoran Yang'], 'affiliations': ['Antai College of Economics and Management, Shanghai Jiao Tong University', 'Department of Statistics and Data Science, Yale University', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.14002.jpg', 'data': {'categories': ['#training', '#architecture', '#interpretability', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в интерпретации языковых моделей: теоретически обоснованное извлечение признаков', 'desc': 'Исследователи представили новый статистический фреймворк и алгоритм обучения под названием Group Bias Adaptation для улучшения разреженных автоэнкодеров (Sparse Autoencoders). Этот метод позволяет извлекать моносемантические признаки из больших языковых моделей (Large Language Models) с теоретическими гарантиями. Авторы предложили новое понятие идентифицируемости признаков, моделируя полисемантические признаки как разреженные смеси моносемантических концепций. Эксперименты показали превосходную производительность метода на языковых моделях с до 1,5 миллиардов параметров по сравнению с эталонными методами.'}, 'en': {'title': 'Enhancing Feature Recovery in Language Models with Group Bias Adaptation', 'desc': 'This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models.'}, 'zh': {'title': '群体偏差适应：提升稀疏自编码器的单义特征恢复能力', 'desc': '本文提出了一种新的统计框架和训练算法，称为群体偏差适应（Group Bias Adaptation），旨在增强稀疏自编码器（Sparse Autoencoders）在大型语言模型中的单义特征恢复能力。现有的稀疏自编码器训练算法缺乏严格的数学保证，并且在超参数敏感性和不稳定性方面存在实际限制。我们通过引入特征可识别性的新概念，解决了特征恢复问题，并提出了一种基于偏差适应的新训练算法。理论证明该算法能够在特定统计模型下正确恢复所有单义特征，从而为稀疏自编码器的训练提供了理论支持。'}}}, {'id': 'https://huggingface.co/papers/2506.10038', 'title': 'Ambient Diffusion Omni: Training Good Models with Bad Data', 'url': 'https://huggingface.co/papers/2506.10038', 'abstract': 'Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.', 'score': 5, 'issue_id': 4349, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f746e9dd50fb7b78', 'authors': ['Giannis Daras', 'Adrian Rodriguez-Munoz', 'Adam Klivans', 'Antonio Torralba', 'Constantinos Daskalakis'], 'affiliations': ['Massachusetts Institute of Technology', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.10038.jpg', 'data': {'categories': ['#training', '#cv', '#dataset', '#synthetic', '#diffusion', '#data'], 'emoji': '🖼️', 'ru': {'title': 'Извлечение пользы из шума: улучшение диффузионных моделей с помощью низкокачественных изображений', 'desc': 'Статья представляет фреймворк Ambient Diffusion Omni, который использует низкокачественные изображения для улучшения диффузионных моделей. Авторы показывают, что даже отбракованные изображения могут быть полезны при обучении, используя свойства естественных изображений, такие как спектральный степенной закон затухания и локальность. Фреймворк успешно применяется для улучшения FID на ImageNet и качества генерации изображений по тексту. Теоретическое обоснование подхода анализирует компромисс между обучением на смещенных данных и ограниченных несмещенных данных.'}, 'en': {'title': 'Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images', 'desc': 'The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity.'}, 'zh': {'title': '利用低质量图像提升扩散模型的质量', 'desc': '本论文提出了一种名为Ambient Diffusion Omni的框架，利用低质量图像来提升扩散模型的性能。研究表明，通常被丢弃的低质量图像实际上具有很大的价值，可以改善模型的训练效果。该框架利用自然图像的两个特性——谱功率法则衰减和局部性，成功地从合成模糊、JPEG压缩和运动模糊的图像中提取信号。最终，我们在ImageNet FID上取得了最先进的结果，并显著提高了文本到图像生成模型的图像质量和多样性。'}}}, {'id': 'https://huggingface.co/papers/2506.05336', 'title': 'VideoMolmo: Spatio-Temporal Grounding Meets Pointing', 'url': 'https://huggingface.co/papers/2506.05336', 'abstract': 'VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.', 'score': 5, 'issue_id': 4348, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'def5ee56ea3b6157', 'authors': ['Ghazi Shazan Ahmad', 'Ahmed Heakl', 'Hanan Gani', 'Abdelrahman Shaker', 'Zhiqiang Shen', 'Ranjay Krishna', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Australian National University', 'Linköping University', 'Mohamed Bin Zayed University of Artificial Intelligence', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.05336.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#benchmark', '#open_source', '#reasoning', '#video', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'Точная локализация объектов в видео с помощью ИИ', 'desc': 'VideoMolmo - это мультимодальная модель для точной пространственно-временной локализации объектов в видео на основе текстовых описаний. Она использует временной модуль с механизмом внимания для обеспечения временной согласованности между кадрами. Модель также применяет SAM2 для двунаправленного распространения точек, что значительно улучшает согласованность масок объектов в видеопоследовательностях. VideoMolmo превосходит существующие модели по точности указания объектов и способности к рассуждениям в различных сценариях реального мира.'}, 'en': {'title': 'Enhancing Spatio-Temporal Reasoning with VideoMolmo', 'desc': 'VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches.'}, 'zh': {'title': 'VideoMolmo：提升时空指向与推理能力的多模态模型', 'desc': 'VideoMolmo是一种多模态模型，结合了时间注意机制和SAM2进行掩膜融合，显著提高了时空指向的准确性和推理能力。该模型专为基于文本描述的细粒度时空指向而设计，能够在不同的真实场景中进行精确交互。通过引入时间模块和双向点传播的掩膜融合管道，VideoMolmo确保了视频序列的时间一致性和连贯性。我们还创建了一个包含72,000个视频-字幕对的数据集，以评估模型在多种真实场景中的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.14755', 'title': 'Optimizing Length Compression in Large Reasoning Models', 'url': 'https://huggingface.co/papers/2506.14755', 'abstract': 'LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.', 'score': 4, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '837a56d067dd6e74', 'authors': ['Zhengxiang Cheng', 'Dongping Chen', 'Mingyang Fu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.14755.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'LC-R1: Оптимизация рассуждений ИИ без потери качества', 'desc': 'LC-R1 - это метод пост-обучения для больших моделей рассуждений (LRM), основанный на принципах краткости и достаточности. Он нацелен на устранение избыточных рассуждений в выводах моделей без существенной потери точности. LC-R1 использует комбинацию наград за длину и сжатие в рамках оптимизации групповой относительной политики (GRPO). Эксперименты показывают, что метод сокращает длину последовательностей примерно на 50% при падении точности всего на 2%.'}, 'en': {'title': 'Streamlining Reasoning: LC-R1 for Efficient Large Models', 'desc': "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."}, 'zh': {'title': '简化推理，提升效率！', 'desc': 'LC-R1是一种后训练方法，旨在通过简洁性和充分性原则来减少大型推理模型中的不必要推理，同时保持较小的准确性损失。该方法识别出模型在推理过程中存在的“无效思维”问题，即模型在得出正确答案后仍然反复检查。为了解决这一低效问题，LC-R1提出了两个新原则：简洁性，强调消除冗余；充分性，确保关键推理步骤得以保留。通过对多个推理基准的广泛实验，LC-R1实现了序列长度的显著减少（约50%），而准确性仅下降约2%，在高压缩率和准确性之间达成了良好的平衡。'}}}, {'id': 'https://huggingface.co/papers/2506.09033', 'title': 'Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.09033', 'abstract': 'Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (i.e., assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present Router-R1, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.', 'score': 3, 'issue_id': 4357, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': '6f6eee917a3ef0d9', 'authors': ['Haozhen Zhang', 'Tao Feng', 'Jiaxuan You'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2506.09033.jpg', 'data': {'categories': ['#rlhf', '#rl', '#multimodal', '#optimization', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Умная маршрутизация запросов между языковыми моделями', 'desc': 'Router-R1 - это фреймворк на основе обучения с подкреплением для маршрутизации между несколькими языковыми моделями. Он улучшает существующие подходы, чередуя действия обдумывания и маршрутизации, что позволяет решать сложные задачи. Router-R1 оптимизирует соотношение производительности и стоимости, используя легковесную систему вознаграждений. Фреймворк способен обобщаться на новые модели, опираясь только на простые дескрипторы вроде цены и задержки.'}, 'en': {'title': 'Optimizing Multi-LLM Routing with Reinforcement Learning', 'desc': "Router-R1 is a reinforcement learning framework designed to enhance the routing of user queries among multiple large language models (LLMs). Unlike traditional routers that assign queries to a single model, Router-R1 interleaves 'think' and 'route' actions, allowing it to consider multiple models' strengths for complex tasks. It uses a rule-based reward system to optimize the balance between performance and cost, making it efficient in selecting the best model for each query. The framework demonstrates strong generalization capabilities, performing well on various benchmarks while managing costs effectively."}, 'zh': {'title': '智能路由，优化性能与成本的平衡', 'desc': 'Router-R1 是一个基于强化学习的框架，旨在改善多大型语言模型（LLM）的路由。它通过交替进行思考和路由动作，优化性能与成本之间的权衡，并能够推广到未见过的模型。与传统的单轮一对一映射不同，Router-R1 将路由和聚合视为一个序列决策过程，利用其推理能力进行动态模型调用。实验结果表明，Router-R1 在多个基准测试中表现优异，展现了强大的泛化能力和成本管理能力。'}}}, {'id': 'https://huggingface.co/papers/2506.14761', 'title': 'From Bytes to Ideas: Language Modeling with Autoregressive U-Nets', 'url': 'https://huggingface.co/papers/2506.14761', 'abstract': 'An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.', 'score': 2, 'issue_id': 4362, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '24f72bf9457e3acf', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#architecture', '#data', '#optimization', '#low_resource', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Гибкая токенизация для улучшенного понимания языка', 'desc': 'Статья представляет автореграссивную U-Net архитектуру, которая обучается встраивать собственные токены в процессе тренировки. Это позволяет модели получить многоуровневое представление текстовых последовательностей. Такой подход улучшает обработку задач на уровне символов и работу с малоресурсными языками. Модель читает необработанные байты и объединяет их в слова и фразы, получая многомасштабный взгляд на последовательность.'}, 'en': {'title': 'Flexible Tokenization for Enhanced Text Understanding', 'desc': "This paper presents an autoregressive U-Net model that learns to create its own token embeddings during training, allowing for a flexible approach to text processing. By reading raw bytes and progressively pooling them into larger units, the model gains a multi-scale perspective on text sequences. This design enables the model to predict further into the future at deeper layers, focusing on broader semantic patterns while earlier layers manage finer details. The approach not only improves performance on character-level tasks but also enhances the model's ability to work with low-resource languages by integrating tokenization within the model itself."}, 'zh': {'title': '自回归U-Net：灵活处理文本的未来', 'desc': '这篇论文介绍了一种自回归U-Net模型，它在训练过程中学习嵌入自己的标记，从而实现对文本序列的多尺度视图。这种方法打破了传统标记化的固定粒度限制，使模型能够更灵活地处理数据。通过读取原始字节并逐步聚合成词，模型在不同深度阶段预测更远的未来，关注更广泛的语义模式。最终，这种模型不仅能处理字符级任务，还能在低资源语言中传递知识。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.14731', 'title': 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs', 'url': 'https://huggingface.co/papers/2506.14731', 'abstract': 'Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '7c1c5a66d6e8f898', 'authors': ['Ring Team', 'Bin Hu', 'Cai Chen', 'Deng Zhao', 'Ding Liu', 'Dingnan Jin', 'Feng Zhu', 'Hao Dai', 'Hongzhi Luan', 'Jia Guo', 'Jiaming Liu', 'Jiewei Wu', 'Jun Mei', 'Jun Zhou', 'Junbo Zhao', 'Junwu Xiong', 'Kaihong Zhang', 'Kuan Xu', 'Lei Liang', 'Liang Jiang', 'Liangcheng Fu', 'Longfei Zheng', 'Qiang Gao', 'Qing Cui', 'Quan Wan', 'Shaomian Zheng', 'Shuaicheng Li', 'Tongkai Yang', 'Wang Ren', 'Xiaodong Yan', 'Xiaopei Wan', 'Xiaoyun Feng', 'Xin Zhao', 'Xinxing Yang', 'Xinyu Kong', 'Xuemin Yang', 'Yang Li', 'Yingting Wu', 'Yongkang Liu', 'Zhankai Xu', 'Zhenduo Zhang', 'Zhenglei Zhou', 'Zhenyu Huang', 'Zhiqiang Zhang', 'Zihao Wang', 'Zujie Wen'], 'affiliations': ['Ring Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14731.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#reasoning', '#open_source', '#rl', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с меньшими вычислительными затратами', 'desc': 'Ring-lite - это модель на основе архитектуры Mixture-of-Experts (MoE), оптимизированная с помощью обучения с подкреплением для эффективного рассуждения. Модель достигает производительности современных моделей рассуждения, активируя лишь треть параметров. Авторы представляют новый метод C3PO для повышения стабильности обучения и вычислительной эффективности. Они также предлагают двухэтапную парадигму обучения для интеграции данных из разных доменов.'}, 'en': {'title': 'Efficient Reasoning with Fewer Parameters: Introducing Ring-lite', 'desc': 'Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets.'}, 'zh': {'title': '高效推理，激活更少参数的Ring-lite', 'desc': 'Ring-lite是一种基于专家混合（MoE）架构的大型语言模型，通过强化学习（RL）进行优化，以实现高效且稳健的推理能力。该模型在Ling-lite的基础上构建，具有168亿个参数，但仅激活2.75亿个参数，能够在多个具有挑战性的基准测试中与小规模的最先进推理模型相匹配。我们提出了一种联合训练流程，将蒸馏与强化学习结合，解决了MoE强化学习训练中的一些未记录的挑战。通过引入受限上下文计算策略优化（C3PO），我们提高了训练的稳定性，并通过算法与系统的共同设计方法改善了计算吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2506.14702', 'title': 'Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers', 'url': 'https://huggingface.co/papers/2506.14702', 'abstract': 'A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.', 'score': 2, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '2fbff0f4b562f92e', 'authors': ["Daniel D'souza", 'Julia Kreutzer', 'Adrien Morisot', 'Ahmet Üstün', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.14702.jpg', 'data': {'categories': ['#long_context', '#optimization', '#training'], 'emoji': '🎯', 'ru': {'title': 'Точная настройка моделей для редких случаев', 'desc': 'Статья представляет новый подход к дообучению моделей машинного обучения для улучшения их производительности и управляемости на редких и недопредставленных случаях использования. Авторы разработали таксономию характеристик данных и происхождения задач для явного контроля атрибутов генерации. Модель обучается автоматически определять эти маркеры, что делает их опциональными при выводе. Такой подход показывает значительные улучшения производительности, особенно на примерах из длинного хвоста распределения обучающих данных.'}, 'en': {'title': 'Optimizing Model Performance for Rare Use Cases', 'desc': "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."}, 'zh': {'title': '优化模型以提升稀有用例的性能与可控性', 'desc': '本文提出了一种系统的方法，通过自动推断生成属性来微调模型，以提高在稀有和未充分代表的用例上的性能和可控性。现代机器学习面临的一个主要挑战是如何在长尾特征上表现良好，尤其是在训练数据中较少出现的特征。我们重新审视训练和推理技术之间的差距，以改善长尾性能，并为用户提供一组可控的生成属性。通过对基础模型进行微调，我们实现了在推理时自动推断这些标记，从而在未充分代表的领域中显著提高了模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.13599', 'title': 'CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation', 'url': 'https://huggingface.co/papers/2506.13599', 'abstract': 'CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.', 'score': 2, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0b0a6282d1310e1b', 'authors': ['Yuwei Du', 'Jie Feng', 'Jian Yuan', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13599.jpg', 'data': {'categories': ['#agents', '#synthetic', '#reasoning', '#multimodal'], 'emoji': '🏙️', 'ru': {'title': 'Умное моделирование городской мобильности с помощью ИИ', 'desc': 'CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - это новый подход к моделированию человеческой мобильности в городском пространстве. Он использует языковые модели с глубоким пониманием городской среды для более реалистичного моделирования индивидуальных и коллективных паттернов передвижения. CAMS включает три ключевых модуля: MobExtractor для извлечения шаблонов мобильности, GeoGenerator для генерации опорных точек, и TrajEnhancer для создания траекторий. Эксперименты показывают, что CAMS превосходит традиционные методы и создает более правдоподобные траектории движения.'}, 'en': {'title': 'Revolutionizing Urban Mobility Simulation with CAMS', 'desc': 'CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data.'}, 'zh': {'title': '城市移动模拟的新范式', 'desc': 'CAMS是一个结合了代理框架和城市知识的大型语言模型，用于更真实地模拟人类的移动行为。它通过三个核心模块来实现这一目标：MobExtractor提取和合成用户的移动模式，GeoGenerator生成考虑集体知识的城市地理信息，TrajEnhancer根据移动模式生成符合真实偏好的轨迹。与传统方法相比，CAMS在不依赖外部地理信息的情况下，能够更好地建模个体和集体的移动模式。实验结果表明，CAMS生成的轨迹更加真实可信，开创了人类移动模拟的新范式。'}}}, {'id': 'https://huggingface.co/papers/2506.05426', 'title': 'Mixture-of-Experts Meets In-Context Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.05426', 'abstract': 'T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.', 'score': 2, 'issue_id': 4356, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '4e79eb4ebca225c7', 'authors': ['Wenhao Wu', 'Fuhong Liu', 'Haoru Li', 'Zican Hu', 'Daoyi Dong', 'Chunlin Chen', 'Zhi Wang'], 'affiliations': ['Australian Artificial Intelligence Institute, University of Technology Sydney', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05426.jpg', 'data': {'categories': ['#optimization', '#rl', '#games', '#multimodal', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'T2MIR: Смесь экспертов для улучшения обучения с подкреплением в контексте', 'desc': 'T2MIR - это новая архитектура для обучения с подкреплением в контексте (ICRL), использующая смесь экспертов (MoE) в трансформерных моделях принятия решений. Она решает проблемы мультимодальности входных данных и разнообразия задач с помощью токен-ориентированного и задаче-ориентированного MoE. T2MIR также применяет контрастивное обучение для улучшения маршрутизации задач. Эксперименты показывают значительное улучшение способности к обучению в контексте по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing In-Context Learning with T2MIR: A Mixture-of-Experts Approach', 'desc': 'The paper introduces T2MIR, a novel framework that enhances in-context reinforcement learning (ICRL) by integrating mixture-of-experts (MoE) into transformer-based decision models. It addresses the challenges of multi-modality in state-action-reward data and the diversity of decision tasks by implementing token-wise and task-wise MoE layers. The token-wise MoE captures different meanings of input tokens, while the task-wise MoE directs tasks to specialized experts, reducing conflicts during training. Experimental results demonstrate that T2MIR improves the learning capacity of ICRL and outperforms existing methods, showcasing its potential for advancements in both language and vision tasks.'}, 'zh': {'title': 'T2MIR：提升上下文强化学习的专家混合框架', 'desc': 'T2MIR是一个框架，利用基于Transformer的决策模型中的逐token和逐任务的专家混合（MoE）方法，增强了上下文强化学习（ICRL）。该框架解决了状态-动作-奖励数据的多模态性和决策任务的多样性问题。T2MIR通过引入两个并行层，分别是逐token的MoE和逐任务的MoE，来捕捉输入token的不同语义，并将多样化的任务分配给专门的专家。实验结果表明，T2MIR显著提高了上下文学习能力，并在多种基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.13901', 'title': 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations', 'url': 'https://huggingface.co/papers/2506.13901', 'abstract': "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", 'score': 1, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '5492fc2feb0ae2f3', 'authors': ['Abhilekh Borah', 'Chhavi Sharma', 'Danush Khanna', 'Utkarsh Bhatt', 'Gurpreet Singh', 'Hasnat Md Abdullah', 'Raghav Kaushik Ravi', 'Vinija Jain', 'Jyoti Patel', 'Shubham Singh', 'Vasu Sharma', 'Arpita Vats', 'Rahul Raja', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon AI', 'BITS Goa, India', 'Evalueserve', 'IIIT Guwahati, India', 'IIT Kharagpur, India', 'LinkedIn', 'Manipal University Jaipur, India', 'Meta AI', 'New York University, USA', 'Texas A&M University, USA', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.13901.jpg', 'data': {'categories': ['#security', '#rlhf', '#alignment', '#benchmark', '#dataset', '#open_source'], 'emoji': '🎯', 'ru': {'title': 'AQI: Геометрический подход к оценке выравнивания языковых моделей', 'desc': 'Предложена новая метрика оценки выравнивания (alignment) больших языковых моделей - Индекс Качества Выравнивания (AQI). AQI анализирует активации в скрытом пространстве модели, оценивая качество кластеризации для выявления скрытых несоответствий и фальшивого выравнивания. Метрика дополняет существующие поведенческие прокси-метрики, такие как частота отказов и токсичность. AQI также может служить ранним индикатором попыток обхода ограничений модели, предоставляя надежный инструмент для аудита безопасности.'}, 'en': {'title': 'Ensuring True Alignment in Language Models with AQI', 'desc': "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."}, 'zh': {'title': '对齐质量指数：确保大型语言模型的安全与可靠', 'desc': '本文提出了一种新的评估指标，称为对齐质量指数（AQI），用于评估大型语言模型的对齐情况。AQI通过分析潜在空间中的激活分离，捕捉聚类质量，以检测模型的错误对齐和伪对齐现象。与现有的行为代理相比，AQI能够更有效地识别模型的安全性和潜在风险。我们还提出了LITMUS数据集，以支持在复杂条件下的稳健评估，并展示了AQI与外部评审者的相关性。'}}}, {'id': 'https://huggingface.co/papers/2506.13387', 'title': 'TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast', 'url': 'https://huggingface.co/papers/2506.13387', 'abstract': "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)", 'score': 1, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '6d0fc497ae4dcfd0', 'authors': ['Beilei Cui', 'Yiming Huang', 'Long Bai', 'Hongliang Ren'], 'affiliations': ['The Chinese University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.13387.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Универсальное преобразование глубины с помощью мультимодального обучения', 'desc': 'TR2M - это фреймворк для преобразования относительной глубины в метрическую с использованием мультимодальных входных данных. Он применяет кросс-модальное внимание и контрастное обучение для улучшения производительности на различных наборах данных. TR2M использует как текстовое описание, так и изображение в качестве входных данных и оценивает две карты масштабирования для преобразования относительной глубины в метрическую на уровне пикселей. Модель демонстрирует отличные результаты как на знакомых, так и на новых наборах данных, показывая большой потенциал в попиксельном преобразовании глубины с помощью языковой информации.'}, 'en': {'title': 'Transforming Relative Depth to Metric Depth with TR2M', 'desc': 'The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks.'}, 'zh': {'title': 'TR2M：相对深度到度量深度的智能转换', 'desc': '本文提出了一种名为TR2M的框架，旨在将相对深度转换为度量深度，利用多模态输入提升在不同数据集上的表现。当前的单目深度估计方法主要分为度量深度估计和相对深度估计，前者在特定领域表现良好，但局限性较大，而后者在不同领域具有良好的泛化能力，但存在尺度不确定性的问题。TR2M通过融合文本描述和图像输入，利用交叉模态注意力模块和对比学习策略，构建了两个重标定图以在像素级别上进行深度转换。实验结果表明，TR2M在已知数据集上表现优异，并在五个未知数据集上展现出卓越的零样本能力，显示出在像素级别上利用语言辅助进行深度转换的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.12880', 'title': 'Universal Jailbreak Suffixes Are Strong Attention Hijackers', 'url': 'https://huggingface.co/papers/2506.12880', 'abstract': "Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We study suffix-based jailbreaksx2013a powerful family of attacks against large language models (LLMs) that optimize adversarial suffixes to circumvent safety alignment. Focusing on the widely used foundational GCG attack (Zou et al., 2023), we observe that suffixes vary in efficacy: some markedly more universalx2013generalizing to many unseen harmful instructionsx2013than others. We first show that GCG's effectiveness is driven by a shallow, critical mechanism, built on the information flow from the adversarial suffix to the final chat template tokens before generation. Quantifying the dominance of this mechanism during generation, we find GCG irregularly and aggressively hijacks the contextualization process. Crucially, we tie hijacking to the universality phenomenon, with more universal suffixes being stronger hijackers. Subsequently, we show that these insights have practical implications: GCG universality can be efficiently enhanced (up to times5 in some cases) at no additional computational cost, and can also be surgically mitigated, at least halving attack success with minimal utility loss. We release our code and data at http://github.com/matanbt/interp-jailbreak.", 'score': 1, 'issue_id': 4357, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'b2ad9af4ff256800', 'authors': ['Matan Ben-Tov', 'Mor Geva', 'Mahmood Sharif'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12880.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#hallucinations', '#multimodal', '#open_source', '#alignment', '#data'], 'emoji': '🔓', 'ru': {'title': 'Уязвимость языковых моделей: атаки на основе суффиксов', 'desc': 'Исследование посвящено атакам на основе суффиксов против больших языковых моделей (LLM), которые используют оптимизированные враждебные суффиксы для обхода мер безопасности. Авторы обнаружили, что эффективность атаки GCG зависит от универсальности суффиксов и их способности захватывать процесс контекстуализации. Более универсальные суффиксы оказались сильнее в захвате модели. На основе этих наблюдений предложены методы повышения эффективности атак и их смягчения с минимальными вычислительными затратами.'}, 'en': {'title': 'Unlocking and Mitigating Suffix-Based Attacks on Language Models', 'desc': "This paper investigates suffix-based jailbreaks, which are attacks on large language models (LLMs) that use specific suffixes to bypass safety measures. The authors focus on the GCG attack, revealing that some suffixes are more effective than others due to their universality, meaning they can apply to a wider range of harmful instructions. They identify a key mechanism in how these suffixes influence the model's output, showing that more universal suffixes are better at hijacking the model's contextualization process. The study also presents methods to enhance the effectiveness of these attacks without extra computational costs and suggests ways to mitigate them while maintaining model utility."}, 'zh': {'title': '后缀攻击：绕过安全机制的新方法', 'desc': '本论文研究了一种基于后缀的攻击方法，旨在绕过大型语言模型的安全机制。我们发现，不同的后缀在攻击效果上存在显著差异，某些后缀具有更强的通用性，能够适用于更多未见过的有害指令。通过分析信息流动，我们揭示了攻击的关键机制，并指出更通用的后缀能够更有效地劫持上下文处理过程。最后，我们提出了增强和缓解这种攻击的方法，能够在不增加计算成本的情况下显著提高防御效果。'}}}, {'id': 'https://huggingface.co/papers/2506.14629', 'title': 'VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning', 'url': 'https://huggingface.co/papers/2506.14629', 'abstract': 'VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.  \t\t\t\t\tAI-generated summary \t\t\t\t Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito', 'score': 0, 'issue_id': 4358, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '3308b767733e40b9', 'authors': ['Md. Adnanul Islam', 'Md. Faiyaz Abdullah Sayeedi', 'Md. Asaduzzaman Shuvo', 'Muhammad Ziaur Rahman', 'Shahanur Rahman Bappy', 'Raiyan Rahman', 'Swakkhar Shatabda'], 'affiliations': ['BRAC University, Bangladesh', 'United International University, Bangladesh', 'University of Portsmouth, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2506.14629.jpg', 'data': {'categories': ['#games', '#dataset', '#open_source', '#multimodal', '#healthcare', '#reasoning'], 'emoji': '🦟', 'ru': {'title': 'ИИ на страже здоровья: предотвращение угрозы комаров', 'desc': 'VisText-Mosquito - это мультимодальный набор данных, объединяющий визуальную и текстовую информацию для автоматизированного обнаружения, сегментации и анализа мест размножения комаров. В исследовании использовались модели YOLOv9s для обнаружения объектов, YOLOv11n-Seg для сегментации водной поверхности и дообученная модель BLIP для генерации рассуждений. Набор данных включает 1828 размеченных изображений для обнаружения объектов, 142 изображения для сегментации водной поверхности и связанные с каждым изображением тексты для рассуждений. Результаты показывают высокую точность моделей в задачах обнаружения, сегментации и генерации текста, что демонстрирует потенциал использования искусственного интеллекта для профилактики заболеваний, переносимых комарами.'}, 'en': {'title': 'Harnessing AI for Mosquito Control: Detect, Segment, Reason!', 'desc': 'The paper introduces VisText-Mosquito, a unique dataset that combines images and text to help identify and analyze mosquito breeding sites. It includes 1,828 annotated images for detecting objects and 142 images specifically for segmenting water surfaces, along with reasoning texts for each image. The study employs advanced models like YOLOv9s and YOLOv11n-Seg for detection and segmentation tasks, achieving high precision scores. Additionally, a fine-tuned BLIP model is used for generating natural language reasoning, demonstrating the effectiveness of multimodal approaches in combating mosquito-borne diseases.'}, 'zh': {'title': '多模态数据助力蚊子滋生地自动检测', 'desc': 'VisText-Mosquito是一个多模态数据集，结合了视觉和文本数据，用于自动化检测和分析蚊子滋生地。该数据集包含1828张标注图像用于目标检测，142张图像用于水面分割，以及与每张图像相关的自然语言推理文本。使用YOLOv9s模型进行目标检测时，达到了最高的精度0.92926，而YOLOv11n-Seg在分割任务中达到了0.91587的精度。通过微调的BLIP模型，我们在推理生成方面取得了良好的效果，强调了“预防胜于治疗”的主题，展示了基于AI的检测如何主动应对蚊媒疾病风险。'}}}, {'id': 'https://huggingface.co/papers/2506.13922', 'title': 'DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance', 'url': 'https://huggingface.co/papers/2506.13922', 'abstract': 'DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io', 'score': 0, 'issue_id': 4359, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'bc27538b9a430f57', 'authors': ['Maximilian Du', 'Shuran Song'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13922.jpg', 'data': {'categories': ['#agents', '#diffusion', '#optimization', '#training', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'DynaGuide: гибкое управление роботами с помощью внешней модели динамики', 'desc': 'DynaGuide - это метод управления диффузионными политиками с использованием внешней модели динамики. Он позволяет адаптировать политики к множественным целям и сохранять устойчивость, превосходя обусловливание целями, особенно при низкокачественных целях. DynaGuide отделяет модель динамики от базовой политики, что дает ряд преимуществ. Метод продемонстрировал среднюю успешность управления 70% на наборе задач CALVIN и превзошел обусловливание целями в 5,4 раза при управлении с низкокачественными целями.'}, 'en': {'title': 'Steering Policies with DynaGuide: Adapting to Multiple Goals Robustly!', 'desc': 'DynaGuide is a novel steering method that enhances diffusion policies by utilizing an external dynamics model. This approach allows the policies to adapt to various objectives while ensuring robustness, particularly when dealing with low-quality objectives. Unlike traditional goal-conditioning methods, DynaGuide separates the dynamics model from the base policy, enabling it to steer towards multiple goals and improve underrepresented behaviors. The effectiveness of DynaGuide is demonstrated through experiments, achieving a 70% steering success rate and significantly outperforming existing methods.'}, 'zh': {'title': 'DynaGuide：多目标引导的智能策略', 'desc': 'DynaGuide是一种使用外部动态模型的引导方法，旨在增强扩散策略的适应性。它允许策略适应多个目标，并在低质量目标下保持鲁棒性，表现优于传统的目标条件方法。通过将动态模型与基础策略分离，DynaGuide能够引导策略朝向多个目标，并增强基础策略的表现。实验结果显示，DynaGuide在多种任务中取得了70%的引导成功率，尤其在低质量目标下比目标条件方法提高了5.4倍的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.12015', 'title': 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction', 'url': 'https://huggingface.co/papers/2506.12015', 'abstract': 'EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.', 'score': 0, 'issue_id': 4356, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'd559dcf057099acf', 'authors': ['Hsi-Che Lin', 'Yu-Chu Yu', 'Kai-Po Chang', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12015.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#open_source', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Тонкая настройка гигантских моделей на обычном ПК', 'desc': 'EMLoC - это фреймворк для эффективной по памяти тонкой настройки больших языковых моделей. Он использует активационно-ориентированное SVD и LoRA для создания легковесного эмулятора модели. Это позволяет выполнять тонкую настройку в рамках тех же ограничений по памяти, что и при обычном выводе. EMLoC превосходит другие подходы на различных наборах данных и модальностях, делая адаптацию моделей доступной для индивидуальных пользователей.'}, 'en': {'title': 'Efficient Fine-Tuning for Everyone with EMLoC!', 'desc': 'EMLoC is a memory-efficient framework designed for fine-tuning large foundation models while adhering to inference memory limits. It utilizes activation-aware singular value decomposition (SVD) to create a lightweight emulator from a small calibration dataset, allowing for effective model adaptation. Fine-tuning is achieved through Low-Rank Adaptation (LoRA), which is then corrected with a novel compensation algorithm to ensure alignment with the original model. This approach enables users to fine-tune large models on standard consumer hardware, making advanced machine learning accessible and practical for diverse applications.'}, 'zh': {'title': 'EMLoC：高效内存微调的新选择', 'desc': 'EMLoC是一种内存高效的微调框架，利用激活感知的奇异值分解（SVD）和LoRA技术，使得在推理内存限制下进行模型适应成为可能。该框架通过在小型下游校准集上构建任务特定的轻量级模拟器，来实现微调。为了纠正原始模型与压缩模拟器之间的错位，EMLoC提出了一种新颖的补偿算法，使得微调后的LoRA模块可以合并回原始模型中进行推理。实验结果表明，EMLoC在多个数据集和模态上优于其他基线，且无需量化即可在单个24GB的消费级GPU上微调38B模型，极大地提高了模型适应的效率和实用性。'}}}, {'id': 'https://huggingface.co/papers/2506.03939', 'title': 'Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.03939', 'abstract': 'Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.', 'score': 0, 'issue_id': 4362, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '44afebcf0fc38495', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#graphs', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Умные графы для умных машин: новый подход к обучению ИИ', 'desc': 'Graph Counselor - это метод улучшения работы больших языковых моделей (LLM) с использованием многоагентного сотрудничества и адаптивного рассуждения. Он эффективно интегрирует знания из графовых структур, повышая фактическую точность и качество генерации в специализированных областях. Метод использует адаптивное извлечение информации из графов и механизм самоанализа для улучшения результатов рассуждений. Эксперименты показывают, что Graph Counselor превосходит существующие методы в задачах рассуждения на графах.'}, 'en': {'title': 'Empowering LLMs with Adaptive Multi-Agent Collaboration', 'desc': "Graph Counselor is a novel approach that enhances Large Language Models (LLMs) by employing multi-agent collaboration and adaptive reasoning techniques. It addresses the limitations of existing methods in knowledge integration by utilizing an Adaptive Graph Information Extraction Module (AGIEM) that allows agents to work together to model complex graph structures. This method dynamically adjusts information extraction strategies, improving the model's ability to handle multi-level dependencies and reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module ensures higher accuracy and semantic consistency in reasoning results, leading to superior performance in graph reasoning tasks."}, 'zh': {'title': '图顾问：提升语言模型的知识整合能力', 'desc': 'Graph Counselor 是一种基于多智能体协作的图检索增强生成方法，旨在提高大型语言模型在专业领域的知识整合能力。该方法通过自适应图信息提取模块（AGIEM），使规划、思考和执行智能体协同工作，从而精确建模复杂的图结构并动态调整信息提取策略。它还引入了多视角自我反思模块（SR），通过自我反思和逆向推理机制提高推理结果的准确性和语义一致性。实验结果表明，Graph Counselor 在多个图推理任务中优于现有方法，展现出更高的推理准确性和泛化能力。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2506.01939', 'title': 'Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.01939', 'abstract': "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.", 'score': 93, 'issue_id': 4090, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'caf288ab8a8d11b2', 'authors': ['Shenzhi Wang', 'Le Yu', 'Chang Gao', 'Chujie Zheng', 'Shixuan Liu', 'Rui Lu', 'Kai Dang', 'Xionghui Chen', 'Jianxin Yang', 'Zhenru Zhang', 'Yuqiong Liu', 'An Yang', 'Andrew Zhao', 'Yang Yue', 'Shiji Song', 'Bowen Yu', 'Gao Huang', 'Junyang Lin'], 'affiliations': ['LeapLab, Tsinghua University', 'Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.01939.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Высокоэнтропийные токены - ключ к улучшению рассуждений ИИ', 'desc': 'Исследование показывает, что паттерны энтропии токенов играют ключевую роль в обучении с подкреплением с проверяемыми вознаграждениями (RLVR) для улучшения рассуждений больших языковых моделей. Обнаружено, что небольшая часть токенов с высокой энтропией значительно влияет на производительность рассуждений. RLVR в основном корректирует энтропию высокоэнтропийных токенов, сохраняя общие паттерны базовой модели. Оптимизация только 20% токенов с высокой энтропией позволяет достичь результатов, сравнимых с полным градиентным обновлением, особенно для больших моделей.'}, 'en': {'title': 'Unlocking Reasoning Power with High-Entropy Tokens in RLVR', 'desc': "This paper investigates the role of token entropy patterns in Reinforcement Learning with Verifiable Rewards (RLVR) to enhance the reasoning abilities of Large Language Models (LLMs). It finds that high-entropy tokens, which are rare, significantly influence the model's reasoning pathways and performance. The study shows that by focusing on these high-entropy tokens during training, the model can achieve better results than traditional methods, even when using a smaller subset of tokens. Overall, the research emphasizes the importance of understanding and optimizing high-entropy tokens to improve RLVR outcomes."}, 'zh': {'title': '高熵令牌：提升RLVR推理能力的关键', 'desc': '本研究探讨了可验证奖励的强化学习（RLVR）在大语言模型（LLMs）推理能力提升中的作用，重点分析了令牌熵模式对推理性能的影响。我们发现，只有少量高熵令牌在推理过程中起到关键作用，能够引导模型走向多样化的推理路径。通过对RLVR训练中熵模式的演变进行研究，我们发现RLVR主要遵循基础模型的熵模式，主要调整高熵令牌的熵值。我们的结果表明，优化高熵令牌是提升RLVR效果的关键，利用这些令牌可以显著提高模型的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'score': 44, 'issue_id': 4088, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'c7587646c2140ddd', 'authors': ['Zafir Stojanovski', 'Oliver Stanley', 'Joe Sharratt', 'Richard Jones', 'Abdulhakeem Adefioye', 'Jean Kaddour', 'Andreas Köpf'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24760.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#games', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Бесконечная тренировка ИИ в искусстве рассуждений', 'desc': 'Reasoning Gym (RG) - это библиотека сред для обучения с подкреплением в задачах рассуждения с проверяемыми наградами. Она включает более 100 генераторов данных и верификаторов в различных областях, таких как алгебра, арифметика, вычисления, познание, геометрия, теория графов, логика и игры. Ключевое преимущество RG - возможность генерировать практически бесконечные обучающие данные с настраиваемой сложностью, в отличие от большинства существующих наборов данных фиксированного размера. Экспериментальные результаты подтверждают эффективность RG как для оценки, так и для обучения с подкреплением моделей рассуждения.'}, 'en': {'title': 'Unlock Infinite Reasoning with Reasoning Gym!', 'desc': 'The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings.'}, 'zh': {'title': '推理训练场：无限生成，持续评估', 'desc': '我们介绍了推理训练场（Reasoning Gym，RG），这是一个用于强化学习的推理环境库，具有可验证的奖励。它提供了超过100个数据生成器和验证器，涵盖代数、算术、计算、认知、几何、图论、逻辑和各种常见游戏等多个领域。其关键创新在于能够生成几乎无限的训练数据，并且可以调整复杂性，这与大多数固定的推理数据集不同。我们的实验结果表明，RG在评估和强化学习推理模型方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.01844', 'title': 'SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics', 'url': 'https://huggingface.co/papers/2506.01844', 'abstract': 'SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.', 'score': 42, 'issue_id': 4094, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '64cdbe1cd5ffbfc4', 'authors': ['Mustafa Shukor', 'Dana Aubakirova', 'Francesco Capuano', 'Pepijn Kooijmans', 'Steven Palma', 'Adil Zouitine', 'Michel Aractingi', 'Caroline Pascal', 'Martino Russi', 'Andres Marafioti', 'Simon Alibert', 'Matthieu Cord', 'Thomas Wolf', 'Remi Cadene'], 'affiliations': ['Hugging Face', 'Sorbonne University', 'valeo.ai', 'École Normale Supérieure Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.01844.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#small_models', '#training', '#benchmark', '#dataset', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Маленькая модель - большие возможности', 'desc': 'SmolVLA - это компактная и эффективная модель для обработки зрения, языка и действий в робототехнике. Она достигает конкурентоспособной производительности при сниженных вычислительных затратах и может быть развернута на обычном пользовательском оборудовании. SmolVLA обучается на одном GPU и может работать даже на CPU, что значительно снижает стоимость обучения и вывода. Несмотря на свой небольшой размер, SmolVLA показывает результаты, сравнимые с моделями, которые в 10 раз больше.'}, 'en': {'title': 'SmolVLA: Efficient Vision-Language-Action for Everyone', 'desc': 'SmolVLA is a compact vision-language-action model that efficiently integrates visual and linguistic understanding for robotics. It significantly reduces the computational costs associated with training and inference, making it suitable for consumer-grade hardware. By leveraging community-collected data, SmolVLA maintains competitive performance while being much smaller than traditional models. The introduction of an asynchronous inference stack enhances responsiveness, allowing for faster action execution without sacrificing accuracy.'}, 'zh': {'title': 'SmolVLA：小巧高效的视觉-语言-动作模型', 'desc': 'SmolVLA是一种紧凑高效的视觉-语言-动作模型，能够在降低计算成本的同时实现竞争力的性能，并可在消费级硬件上部署。该模型利用社区收集的数据，避免了传统模型对大型数据集的依赖，从而降低了训练和推理的成本。SmolVLA设计为可以在单个GPU上训练，并在消费级GPU或CPU上运行，提升了响应速度。尽管体积小，SmolVLA的性能与体积十倍的模型相当，适用于多种机器人基准测试。'}}}, {'id': 'https://huggingface.co/papers/2506.01049', 'title': 'Taming LLMs by Scaling Learning Rates with Gradient Grouping', 'url': 'https://huggingface.co/papers/2506.01049', 'abstract': 'SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.', 'score': 31, 'issue_id': 4087, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '350401d748400bad', 'authors': ['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu'], 'affiliations': ['Peking University', 'The Hong Kong University of Science and Technology', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01049.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'SGG: Групповое масштабирование градиентов для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый метод оптимизации для обучения больших языковых моделей под названием SGG (Scaling with Gradient Grouping). SGG группирует градиенты и применяет масштабирование для каждой группы, что улучшает адаптивную оценку скорости обучения. Этот подход повышает стабильность и скорость сходимости при обучении крупных языковых моделей. Эксперименты показывают, что SGG хорошо интегрируется с существующими оптимизаторами и дает стабильные улучшения на различных бенчмарках.'}, 'en': {'title': 'SGG: Optimizing Learning Rates for Better LLM Training', 'desc': 'This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions.'}, 'zh': {'title': 'SGG：提升大语言模型训练的自适应学习率优化器', 'desc': 'SGG是一种优化器包装器，通过对梯度进行分组和应用特定于集群的缩放，增强了大语言模型的自适应学习率。它解决了大规模模型训练中的不稳定性和收敛速度慢的问题。SGG通过动态分组和集群特定的缩放来改善学习率估计，从而实现更精确的参数调整。实验结果表明，SGG与现有优化器无缝集成，并在不同模型规模上提供了一致的性能提升和更快的收敛速度。'}}}, {'id': 'https://huggingface.co/papers/2506.00539', 'title': 'ARIA: Training Language Agents with Intention-Driven Reward Aggregation', 'url': 'https://huggingface.co/papers/2506.00539', 'abstract': 'ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.', 'score': 25, 'issue_id': 4090, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '49b915ea5a1db300', 'authors': ['Ruihan Yang', 'Yikai Zhang', 'Aili Chen', 'Xintao Wang', 'Siyu Yuan', 'Jiangjie Chen', 'Deqing Yang', 'Yanghua Xiao'], 'affiliations': ['Bytedance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00539.jpg', 'data': {'categories': ['#games', '#reasoning', '#agents', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'ARIA: Агрегация наград в пространстве намерений для эффективного обучения языковых ИИ-агентов', 'desc': 'ARIA - это метод, который агрегирует награды в пространстве намерений для эффективного обучения языковых агентов. Он проецирует действия на естественном языке из высокоразмерного пространства распределения токенов в низкоразмерное пространство намерений, где семантически похожие действия кластеризуются и получают общие награды. Это уменьшает дисперсию наград, уплотняя сигналы наград и способствуя лучшей оптимизации политики. Эксперименты показывают, что ARIA значительно снижает дисперсию градиента политики и дает существенный прирост производительности в среднем на 9,95% в четырех задачах.'}, 'en': {'title': 'Enhancing Language Agents with Intention-Based Reward Aggregation', 'desc': 'ARIA is a method designed to improve reinforcement learning in language-based tasks by addressing the issue of reward sparsity. It does this by aggregating rewards in an intention space, which clusters semantically similar actions and assigns them shared rewards. This approach reduces the variance of rewards, making it easier for agents to learn effective policies. Experiments show that ARIA leads to significant performance improvements across various tasks compared to traditional reinforcement learning methods.'}, 'zh': {'title': '意图空间中的奖励聚合，提升语言代理的学习效率', 'desc': 'ARIA是一种在意图空间中聚合奖励的方法，旨在缓解奖励稀疏性并改善基于语言的强化学习任务中的策略优化。通过将自然语言动作从高维的联合标记分布空间投影到低维的意图空间，ARIA能够将语义相似的动作聚集在一起并分配共享奖励。这种基于意图的奖励聚合减少了奖励方差，增强了奖励信号的密度，从而促进了更好的策略优化。实验结果表明，ARIA显著降低了策略梯度的方差，并在四个下游任务中平均提高了9.95%的性能，始终优于离线和在线强化学习基线。'}}}, {'id': 'https://huggingface.co/papers/2505.23590', 'title': 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles', 'url': 'https://huggingface.co/papers/2505.23590', 'abstract': 'The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.', 'score': 24, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '583be7c36c626f1e', 'authors': ['Zifu Wang', 'Junyi Zhu', 'Bo Tang', 'Zhiyu Li', 'Feiyu Xiong', 'Jiaqian Yu', 'Matthew B. Blaschko'], 'affiliations': ['ESAT-PSI, KU Leuven', 'Institute for Advanced Algorithms Research, Shanghai', 'Memory Tensor, Shanghai', 'Samsung R&D Institute China, Beijing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23590.jpg', 'data': {'categories': ['#multimodal', '#training', '#transfer_learning', '#rl', '#cv', '#open_source', '#reasoning', '#games'], 'emoji': '🧩', 'ru': {'title': 'Мультимодальные модели осваивают пазлы с помощью обучения с подкреплением', 'desc': 'Исследование применения обучения с подкреплением на основе правил к мультимодальным большим языковым моделям (MLLM) с использованием головоломок-пазлов в качестве экспериментальной базы. Результаты показывают, что MLLM могут достигать высокой точности и обобщения на сложных конфигурациях пазлов после дообучения. Обнаружено, что обучение на пазлах может улучшить результаты на других визуальных задачах, а обучение с подкреплением демонстрирует лучшее обобщение, чем обычная тонкая настройка. Исследование вносит вклад в понимание обучения с подкреплением на основе правил для визуальных задач в мультимодальном обучении.'}, 'en': {'title': 'Unlocking Multimodal Learning with Jigsaw Puzzles and RL', 'desc': 'This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks.'}, 'zh': {'title': '基于规则的强化学习在多模态学习中的新发现', 'desc': '本研究探讨了基于规则的强化学习在多模态大型语言模型中的应用，特别是在视觉任务中的挑战。我们使用拼图作为实验框架，发现经过微调后，模型在简单拼图上的表现从随机猜测提升至接近完美的准确率，并能推广到复杂的未见配置。研究还表明，模型可以在有无显式推理的情况下学习和泛化，尽管开源模型更倾向于直接回答。最后，我们发现强化学习在泛化能力上优于监督微调，并且初始的监督微调冷启动阶段可能会妨碍后续的强化学习优化。'}}}, {'id': 'https://huggingface.co/papers/2506.01853', 'title': 'ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01853', 'abstract': 'A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '241ff6937e6642f5', 'authors': ['Junliang Ye', 'Zhengyi Wang', 'Ruowen Zhao', 'Shenghao Xie', 'Jun Zhu'], 'affiliations': ['Peking University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01853.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games', '#dataset', '#agi', '#multimodal'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D: языковая модель, понимающая трехмерное пространство', 'desc': 'Исследователи представили ShapeLLM-Omni - нативную 3D большую языковую модель, способную понимать и генерировать 3D-объекты и текст. Модель обучена с использованием 3D векторного квантованного вариационного автоэнкодера (VQVAE) и нового набора данных 3D-Alpaca. ShapeLLM-Omni расширяет возможности мультимодальных моделей, добавляя базовые 3D-возможности. Это открывает новые перспективы для исследований в области 3D-нативного искусственного интеллекта.'}, 'en': {'title': 'Bridging Text and 3D: ShapeLLM-Omni Unleashes Multimodal Potential', 'desc': 'ShapeLLM-Omni is a novel large language model designed to understand and generate 3D assets alongside text. It utilizes a 3D vector-quantized variational autoencoder (VQVAE) to efficiently represent and reconstruct 3D shapes in a discrete latent space. The model is trained on a new dataset called 3D-Alpaca, which includes diverse tasks such as generation, comprehension, and editing of 3D content. This research aims to enhance multimodal AI capabilities by integrating 3D understanding, paving the way for future advancements in 3D-native artificial intelligence.'}, 'zh': {'title': 'ShapeLLM-Omni：开启3D内容生成的新纪元', 'desc': '本文提出了一种名为ShapeLLM-Omni的原生3D大型语言模型，旨在理解和生成3D资产及文本。该模型使用3D向量量化变分自编码器（VQVAE）进行训练，将3D对象映射到离散潜在空间，以实现高效准确的形状表示和重建。我们还构建了一个名为3D-Alpaca的大规模连续训练数据集，涵盖生成、理解和编辑，为未来的研究和训练提供了丰富的资源。通过对Qwen-2.5-vl-7B-Instruct模型进行基于指令的训练，我们的工作为扩展多模态模型的基本3D能力提供了有效的尝试。'}}}, {'id': 'https://huggingface.co/papers/2506.00996', 'title': 'Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.00996', 'abstract': "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/", 'score': 22, 'issue_id': 4090, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '25ea795d193b8719', 'authors': ['Kinam Kim', 'Junha Hyung', 'Jaegul Choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00996.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#training'], 'emoji': '🎬', 'ru': {'title': 'Эффективная адаптация видеомоделей с минимальными данными', 'desc': 'Метод Temporal In-Context Fine-Tuning (TIC-FT) улучшает предобученные модели диффузии видео для разнообразных задач условной генерации с минимальными данными и без изменения архитектуры. TIC-FT объединяет кадры условия и целевые кадры по временной оси, вставляя промежуточные буферные кадры с постепенно увеличивающимся уровнем шума. Этот подход не требует архитектурных изменений и достигает высокой производительности всего на 10-30 обучающих примерах. Эксперименты показывают, что TIC-FT превосходит существующие базовые методы по точности соответствия условиям и визуальному качеству, оставаясь при этом высокоэффективным как при обучении, так и при инференсе.'}, 'en': {'title': 'Efficient Video Generation with Minimal Data Using TIC-FT', 'desc': "Temporal In-Context Fine-Tuning (TIC-FT) is a novel method that improves pretrained video diffusion models for various conditional generation tasks without needing extensive data or changing the model architecture. It works by combining condition and target frames along the time axis and adding intermediate buffer frames with increasing noise, which helps maintain the model's temporal coherence. This approach allows for effective fine-tuning with as few as 10-30 training samples, making it efficient and scalable. TIC-FT has been shown to outperform existing methods in generating high-quality videos while ensuring fidelity to the given conditions."}, 'zh': {'title': '时间上下文微调：高效的视频生成新方法', 'desc': '本文提出了一种新的方法，称为时间上下文微调（TIC-FT），用于增强预训练的视频扩散模型，以实现多样化的条件生成任务。TIC-FT通过在时间轴上连接条件帧和目标帧，并插入逐渐增加噪声水平的中间缓冲帧，来实现平滑过渡，从而与预训练模型的时间动态对齐。该方法无需对模型架构进行修改，且只需10到30个训练样本即可实现强大的性能。实验结果表明，TIC-FT在条件保真度和视觉质量方面均优于现有基线，同时在训练和推理过程中保持高效。'}}}, {'id': 'https://huggingface.co/papers/2506.00411', 'title': 'LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks', 'url': 'https://huggingface.co/papers/2506.00411', 'abstract': 'A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.', 'score': 22, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '16fa44208bf3618c', 'authors': ['Yi Yang', 'Jiaxuan Sun', 'Siqi Kou', 'Yihan Wang', 'Zhijie Deng'], 'affiliations': ['Fudan University', 'Shanghai Jiao Tong University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00411.jpg', 'data': {'categories': ['#cv', '#architecture', '#robotics', '#agents', '#dataset', '#agi', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Единая архитектура для долгосрочных задач воплощенного ИИ', 'desc': 'LoHoVLA - это новая унифицированная архитектура для решения долгосрочных задач воплощенного искусственного интеллекта. Она объединяет большую предобученную мультимодальную модель для работы с изображениями и текстом с иерархическим управлением с обратной связью. LoHoVLA использует общее представление для генерации подзадач на естественном языке и предсказания действий робота. Эксперименты показали значительное превосходство LoHoVLA над существующими подходами в симуляторе Ravens.'}, 'en': {'title': 'Empowering Robots with Unified Vision and Language for Complex Tasks', 'desc': 'The paper presents LoHoVLA, a new framework that integrates a large pretrained vision language model with hierarchical closed-loop control to enhance performance in long-horizon embodied tasks. It addresses the challenges of high-level task planning and low-level motion control by generating language and action tokens for effective sub-task generation and robot action prediction. The framework is trained on a unique dataset, LoHoSet, which includes a variety of long-horizon tasks with expert demonstrations. Experimental results demonstrate that LoHoVLA outperforms existing models, highlighting the potential of unified architectures in improving embodied intelligence.'}, 'zh': {'title': '统一框架提升长时间任务表现', 'desc': 'LoHoVLA是一个统一的视觉语言行动框架，旨在提高长时间任务的表现。它结合了大型预训练的视觉语言模型和分层闭环控制，能够更好地进行高层次任务规划和低层次运动控制。通过生成语言和动作标记，LoHoVLA促进了任务间的更好泛化。此外，LoHoVLA在Ravens模拟器上进行训练，展示了其在长时间任务中的显著优势。'}}}, {'id': 'https://huggingface.co/papers/2506.01943', 'title': 'Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control', 'url': 'https://huggingface.co/papers/2506.01943', 'abstract': 'Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.', 'score': 20, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '4acb1e4fc9635b8a', 'authors': ['Xiao Fu', 'Xintao Wang', 'Xian Liu', 'Jianhong Bai', 'Runsen Xu', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01943.jpg', 'data': {'categories': ['#robotics', '#video', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'RoboMaster: новый подход к моделированию сложных взаимодействий в робототехнике', 'desc': 'В статье представлен новый подход RoboMaster для моделирования взаимодействия нескольких объектов в робототехнике. Метод разделяет процесс взаимодействия на три этапа: до, во время и после, используя характеристики доминирующего объекта на каждом этапе. Это позволяет избежать проблем, связанных со слиянием признаков нескольких объектов во время взаимодействия. Для обеспечения семантической согласованности на протяжении всего видео используются латентные представления, учитывающие внешний вид и форму объектов.'}, 'en': {'title': 'RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling', 'desc': "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."}, 'zh': {'title': 'RoboMaster：提升机器人操作的视频生成新框架', 'desc': '本论文提出了一种名为RoboMaster的新框架，用于建模多物体之间的动态交互，以改善机器人决策数据的生成。与以往方法不同，RoboMaster将交互过程分为三个子阶段：预交互、交互和后交互，分别使用主导物体的特征进行建模。通过这种方式，RoboMaster有效地解决了多物体特征融合带来的问题，提高了视觉质量。实验结果表明，该方法在复杂的机器人操作任务中表现优异，达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2506.01713', 'title': 'SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.01713', 'abstract': 'Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.', 'score': 20, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'fc955e3f149c1b08', 'authors': ['Zhongwei Wan', 'Zhihao Dou', 'Che Liu', 'Yu Zhang', 'Dongfei Cui', 'Qinjian Zhao', 'Hui Shen', 'Jing Xiong', 'Yi Xin', 'Yifan Jiang', 'Yangfan He', 'Mi Zhang', 'Shen Yan'], 'affiliations': ['ByteDance Seed', 'Case Western Reserve University', 'Duke University', 'Imperial College London', 'Kean University Minnesota', 'Nanjing University', 'The Ohio State University', 'The University of Hong Kong', 'Tongji University', 'University of Michigan', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.01713.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#benchmark', '#dataset', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'SRPO: Усиление мультимодальных ИИ через самоанализ', 'desc': 'Статья представляет новый метод под названием SRPO для улучшения рассуждений мультимодальных больших языковых моделей (MLLM). SRPO использует двухэтапный подход с обучением с подкреплением, включающий создание набора данных с рефлексией и новый механизм вознаграждения. Метод нацелен на преодоление ограничений существующих MLLM в сложных задачах, требующих самоанализа и самокоррекции. Эксперименты показывают значительное улучшение точности рассуждений и качества рефлексии на нескольких мультимодальных тестах.'}, 'en': {'title': 'Enhancing Multimodal Reasoning through Self-Reflection', 'desc': "This paper introduces a new approach called Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO) to improve the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing reflection methods are inadequate for generating useful feedback, which limits the models' performance on complex reasoning tasks. SRPO consists of two stages: first, it creates a high-quality dataset for training the model to reflect on its responses, and second, it implements a reward mechanism that promotes meaningful reflections. Experimental results show that SRPO significantly enhances both the accuracy of reasoning and the quality of reflections compared to current leading models."}, 'zh': {'title': '提升多模态推理能力的自我反思框架', 'desc': '多模态大型语言模型（MLLMs）在推理任务中表现出色，但在需要明确自我反思和自我纠正的复杂问题上仍然存在困难。现有的反思方法过于简单，难以生成有意义和指导性的反馈，因为预训练模型的推理能力和知识在初始训练期间基本固定。为了解决这些挑战，我们提出了一种名为自我反思增强推理的多模态自我反思框架（SRPO），该框架通过两阶段的反思意识强化学习（RL）来提升多模态LLM的推理能力。实验结果表明，SRPO在多个多模态推理基准测试中显著优于现有模型，推理准确性和反思质量都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01667', 'title': 'EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models', 'url': 'https://huggingface.co/papers/2506.01667', 'abstract': 'EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework.', 'score': 17, 'issue_id': 4093, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '90528034771977ef', 'authors': ['Yan Shu', 'Bin Ren', 'Zhitong Xiong', 'Danda Pani Paudel', 'Luc Van Gool', 'Begum Demir', 'Nicu Sebe', 'Paolo Rota'], 'affiliations': ['INSAIT, Sofia University St. Kliment Ohridski', 'Technical University of Munich', 'Technische Universität Berlin', 'University of Pisa', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2506.01667.jpg', 'data': {'categories': ['#benchmark', '#cv', '#survey', '#reasoning', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'EarthMind: Эффективное понимание многосенсорных данных наблюдения Земли', 'desc': 'EarthMind - это новая система анализа данных дистанционного зондирования Земли, использующая методы обработки естественного языка и компьютерного зрения. Она включает в себя пространственное внимание и кросс-модальное слияние для эффективной работы с разнородными данными. EarthMind превосходит более крупные модели на специализированных тестах, несмотря на меньший размер (4 миллиарда параметров). Система способна решать широкий спектр задач восприятия и рассуждения для многосенсорных данных наблюдения Земли.'}, 'en': {'title': 'EarthMind: Revolutionizing Earth Observation with Vision-Language Fusion', 'desc': "EarthMind is a new framework designed to improve the understanding of Earth Observation (EO) data by combining vision and language processing. It uses Spatial Attention Prompting (SAP) to focus on important details at the pixel level, enhancing the model's ability to interpret images. Additionally, Cross-modal Fusion aligns different types of data, allowing the model to weigh information based on its relevance. This approach not only outperforms larger models like GPT-4o on specialized benchmarks but also effectively handles complex multi-sensor data tasks."}, 'zh': {'title': 'EarthMind：高效理解地球观测数据的创新框架', 'desc': 'EarthMind是一个视觉-语言框架，旨在高效理解多粒度和多传感器的地球观测数据。它采用空间注意力提示和跨模态融合技术，能够在专门基准测试中超越更大的模型。EarthMind的两个核心组件分别是空间注意力提示（SAP），用于增强像素级理解，以及跨模态融合，能够将不同模态对齐到共享空间并根据信息密度自适应调整权重。通过EarthMind-Bench基准测试，EarthMind在多个公共地球观测基准上表现优异，展示了其在统一框架下处理多粒度和多传感器挑战的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.24298', 'title': 'AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning', 'url': 'https://huggingface.co/papers/2505.24298', 'abstract': 'AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57times training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.', 'score': 16, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'fad566ec1d2ba264', 'authors': ['Wei Fu', 'Jiaxuan Gao', 'Xujie Shen', 'Chen Zhu', 'Zhiyu Mei', 'Chuyi He', 'Shusheng Xu', 'Guo Wei', 'Jun Mei', 'Jiashu Wang', 'Tongkai Yang', 'Binhang Yuan', 'Yi Wu'], 'affiliations': ['Ant Research', 'HKUST', 'IIIS, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24298.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'AReaL: Асинхронное обучение с подкреплением для ускорения ИИ', 'desc': 'AReaL - это асинхронная система обучения с подкреплением для больших языковых моделей. Она разделяет процессы генерации и обучения, что позволяет достичь более высокой утилизации GPU и ускорения обучения до 2.57 раз на задачах рассуждения. Система использует непрерывную генерацию данных и обновление модели, а также ряд оптимизаций для стабилизации обучения. Эксперименты показали превосходство AReaL над синхронными системами по скорости при сохранении или улучшении качества.'}, 'en': {'title': 'AReaL: Revolutionizing Reinforcement Learning with Asynchronous Training', 'desc': 'AReaL is an innovative reinforcement learning system designed to enhance the training of large language models by decoupling the generation and training processes. This fully asynchronous approach allows for continuous output generation without waiting for the longest tasks to finish, leading to improved GPU utilization. By balancing the workload between rollout and training workers, AReaL effectively manages data staleness and employs a modified Proximal Policy Optimization (PPO) to optimize training with outdated samples. Experimental results demonstrate that AReaL can achieve up to 2.57 times faster training speeds while maintaining or improving performance on reasoning tasks.'}, 'zh': {'title': 'AReaL：异步强化学习的高效训练新模式', 'desc': 'AReaL是一种完全异步的强化学习系统，它将生成和训练解耦，从而提高GPU的利用率，并在推理任务上实现了高达2.57倍的训练加速。传统的大规模强化学习系统通常是同步的，生成和训练交替进行，这导致了系统效率低下。AReaL通过让生成工作者持续生成新输出，而训练工作者在收集到一批数据后立即更新模型，解决了这一问题。通过平衡生成和训练工作者的工作负载，AReaL有效控制了数据的过时性，并采用了增强过时性的PPO变体来更好地处理过时的训练样本。'}}}, {'id': 'https://huggingface.co/papers/2506.01863', 'title': 'Unified Scaling Laws for Compressed Representations', 'url': 'https://huggingface.co/papers/2506.01863', 'abstract': 'A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws have shaped recent advances in machine learning by enabling predictable scaling of model performance based on model size, computation, and data volume. Concurrently, the rise in computational cost for AI has motivated model compression techniques, notably quantization and sparsification, which have emerged to mitigate the steep computational demands associated with large-scale training and inference. This paper investigates the interplay between scaling laws and compression formats, exploring whether a unified scaling framework can accurately predict model performance when training occurs over various compressed representations, such as sparse, scalar-quantized, sparse-quantized or even vector-quantized formats. Our key contributions include validating a general scaling law formulation and showing that it is applicable both individually but also composably across compression types. Based on this, our main finding is demonstrating both theoretically and empirically that there exists a simple "capacity" metric -- based on the representation\'s ability to fit random Gaussian data -- which can robustly predict parameter efficiency across multiple compressed representations. On the practical side, we extend our formulation to directly compare the accuracy potential of different compressed formats, and to derive better algorithms for training over sparse-quantized formats.', 'score': 14, 'issue_id': 4099, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '0f97dac5f5c8309f', 'authors': ['Andrei Panferov', 'Alexandra Volkova', 'Ionut-Vlad Modoranu', 'Vage Egiazarian', 'Mher Safaryan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.01863.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '🔬', 'ru': {'title': 'Единая метрика для предсказания эффективности сжатых нейросетей', 'desc': 'Исследование посвящено взаимосвязи между законами масштабирования и методами сжатия в машинном обучении. Авторы предлагают единую метрику емкости, способную предсказывать производительность моделей для различных форматов сжатия, включая скалярное квантование, разреженное квантование и векторное квантование. Эта метрика основана на способности представления аппроксимировать случайные гауссовы данные. Результаты исследования позволяют сравнивать потенциальную точность различных форматов сжатия и разрабатывать улучшенные алгоритмы обучения для разреженно-квантованных форматов.'}, 'en': {'title': 'Unified Capacity Metric: Predicting Performance in Compressed Models', 'desc': 'This paper explores how scaling laws in machine learning can predict the performance of models when they are compressed using different techniques. It focuses on various compression formats like scalar-quantized, sparse-quantized, and vector-quantized representations. The authors propose a unified capacity metric that can effectively gauge the efficiency of these compressed models based on their ability to handle random Gaussian data. Their findings suggest that this metric not only applies to individual compression types but can also be used to compare and improve training algorithms across different formats.'}, 'zh': {'title': '统一容量度量，预测模型性能', 'desc': '本研究探讨了缩放法则与压缩技术之间的关系，提出了一种统一的容量度量，可以预测不同压缩格式下模型的性能。研究表明，模型的性能可以根据模型大小、计算量和数据量进行可预测的缩放。通过验证通用的缩放法则公式，研究发现该公式适用于不同的压缩类型。最终，提出了一种简单的“容量”度量，能够有效预测多种压缩表示下的参数效率。'}}}, {'id': 'https://huggingface.co/papers/2505.24846', 'title': 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning', 'url': 'https://huggingface.co/papers/2505.24846', 'abstract': 'MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.', 'score': 14, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ed5c25a307e9093d', 'authors': ['Jingyan Shen', 'Jiarui Yao', 'Rui Yang', 'Yifan Sun', 'Feng Luo', 'Rui Pan', 'Tong Zhang', 'Han Zhao'], 'affiliations': ['Columbia University', 'Rice University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24846.jpg', 'data': {'categories': ['#dataset', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'MiCRo: Персонализация языковых моделей без дополнительных аннотаций', 'desc': 'MiCRo - это двухэтапная система для улучшения персонализированного обучения предпочтениям в больших языковых моделях. Она использует наборы данных бинарных предпочтений и динамически адаптирует веса смесей на основе контекста. MiCRo эффективно захватывает разнообразные человеческие предпочтения без необходимости в детальных аннотациях. Эксперименты показывают, что MiCRo значительно улучшает последующую персонализацию в языковых моделях.'}, 'en': {'title': 'MiCRo: Dynamic Personalization for Diverse Human Preferences', 'desc': 'MiCRo is a two-stage framework designed to enhance personalized preference learning for large language models (LLMs). It utilizes binary preference datasets and employs a context-aware mixture modeling approach to better capture the diverse preferences of humans. The framework dynamically adjusts mixture weights based on the context, allowing for more accurate and scalable preference adaptation. Experimental results show that MiCRo significantly improves the ability of LLMs to personalize responses according to varied human preferences.'}, 'zh': {'title': 'MiCRo：捕捉多样化人类偏好的新框架', 'desc': 'MiCRo是一个两阶段框架，旨在改善大型语言模型的个性化偏好学习。它利用二元偏好数据集，并根据上下文动态调整混合权重，从而有效捕捉多样化的人类偏好。该方法通过引入上下文感知的混合建模，解决了传统模型无法充分反映人类多样性的问题。实验结果表明，MiCRo在多个偏好数据集上表现出色，显著提升了下游个性化效果。'}}}, {'id': 'https://huggingface.co/papers/2506.01413', 'title': 'Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.01413', 'abstract': 'Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.', 'score': 11, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '3f0db6c1e3cc1878', 'authors': ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent YouTu Lab', 'The Chinese University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01413.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Рассуждай умнее, а не больше: новый подход к обучению языковых моделей', 'desc': 'Эта статья посвящена улучшению способности больших языковых моделей (LLM) выполнять сложные инструкции с множественными ограничениями. Авторы предлагают систематический метод, основанный на поощрении рассуждений с помощью обучения с подкреплением. Они используют воспроизводимый метод сбора данных и применяют контрастное обучение для улучшения цепочки рассуждений. Результаты показывают значительное улучшение производительности, сравнимое с увеличением размера модели в несколько раз.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Reasoning to Deep Understanding', 'desc': "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."}, 'zh': {'title': '提升大型语言模型处理复杂指令的能力', 'desc': '现有的大型语言模型（LLMs）在处理复杂指令时面临挑战，尤其是当指令包含多个并行、链式和分支结构的约束时。本文提出了一种系统的方法，通过激励推理来提升LLMs处理复杂指令的能力。我们利用强化学习（RL）和可验证的规则中心奖励信号，培养模型在指令跟随方面的推理能力。通过对比样本，我们解决了在复杂指令下推理的浅层和非本质特性，从而显著提高了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.00577', 'title': 'Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs', 'url': 'https://huggingface.co/papers/2506.00577', 'abstract': 'Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0ed5d0b7064f9962', 'authors': ['Yufa Zhou', 'Shaobo Wang', 'Xingyu Dong', 'Xiangqi Jin', 'Yifang Chen', 'Yue Min', 'Kexin Yang', 'Xingzhang Ren', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, Shanghai Jiao Tong University', 'Qwen Team, Alibaba Group', 'The University of Chicago', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.00577.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#agents', '#open_source', '#reasoning', '#games', '#dataset'], 'emoji': '💡', 'ru': {'title': 'Дообучение языковых моделей улучшает экономические рассуждения', 'desc': 'Исследование показывает, что методы дообучения, такие как контролируемая тонкая настройка (SFT) и обучение с подкреплением с проверяемыми наградами (RLVR), могут улучшить рассуждения и экономическую рациональность больших языковых моделей в многоагентных сценариях. Авторы представляют Recon - 7B-параметровую модель, дообученную на наборе данных из 2100 задач по экономическим рассуждениям. Оценка на экономических тестах и многоагентных играх показала значительное улучшение структурированных рассуждений и экономической рациональности. Результаты подчеркивают перспективность дообучения в конкретной предметной области для улучшения рассуждений и согласования агентов.'}, 'en': {'title': 'Enhancing Economic Reasoning in LLMs through Post-Training Techniques', 'desc': "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."}, 'zh': {'title': '后训练技术提升智能体推理与经济理性', 'desc': '本论文探讨了后训练技术如何提升大型语言模型在多智能体系统中的推理能力和经济理性。我们采用监督微调和可验证奖励的强化学习方法，针对经济推理进行训练。通过引入Recon模型，我们在高质量经济推理问题的数据集上进行了后训练，并在经济推理基准和多智能体游戏中进行了评估。结果显示，经过后训练的模型在结构化推理和经济理性方面有显著提升，证明了领域对齐的后训练在增强推理和智能体对齐方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.23907', 'title': 'Cora: Correspondence-aware image editing using few step diffusion', 'url': 'https://huggingface.co/papers/2505.23907', 'abstract': 'Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.', 'score': 10, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '7de1457440a0b449', 'authors': ['Amirhossein Almohammadi', 'Aryan Mikaeili', 'Sauradip Nag', 'Negar Hassanpour', 'Andrea Tagliasacchi', 'Ali Mahdavi-Amiri'], 'affiliations': ['Google Deepmind, Canada', 'Huawei, Canada', 'Simon Fraser University, Canada', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.23907.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Умное редактирование изображений с сохранением структуры и текстур', 'desc': 'Cora - это новая система редактирования изображений, использующая коррекцию шума с учетом соответствий и интерполированные карты внимания. Она позволяет точно переносить текстуры и структуры между исходным и целевым изображениями, сохраняя ключевые атрибуты оригинала. Cora превосходит аналоги в сохранении структуры, текстур и идентичности при различных типах редактирования. Система обеспечивает контроль баланса между генерацией нового контента и сохранением исходного.'}, 'en': {'title': 'Cora: Revolutionizing Image Editing with Precision and Control', 'desc': 'The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods.'}, 'zh': {'title': 'Cora：图像编辑的新突破', 'desc': 'Cora框架通过引入对应感知噪声校正和插值注意力图，增强了图像编辑的效果。它能够在源图像和目标图像之间对齐纹理和结构，从而实现准确的纹理转移和必要的新内容生成。Cora在内容生成和保留之间提供了良好的控制，能够有效处理姿态变化、物体添加和纹理细化等多种编辑任务。实验结果表明，Cora在结构、纹理和身份的保持上表现优异，用户研究也证实了其优于其他方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2506.01952', 'title': 'WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks', 'url': 'https://huggingface.co/papers/2506.01952', 'abstract': 'WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.', 'score': 9, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'aa260fbf373a4f2c', 'authors': ['Atsuyuki Miyai', 'Zaiying Zhao', 'Kazuki Egashira', 'Atsuki Sato', 'Tatsumi Sunada', 'Shota Onohara', 'Hiromasa Yamanishi', 'Mashiro Toyooka', 'Kunato Nishina', 'Ryoma Maeda', 'Kiyoharu Aizawa', 'Toshihiko Yamasaki'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.01952.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#benchmark', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'WebChoreArena: новый рубеж в оценке возможностей ИИ-агентов для веб-задач', 'desc': 'WebChoreArena - это новый набор тестов, состоящий из 532 задач, который расширяет возможности WebArena для более сложных и утомительных задач веб-браузинга. Он включает в себя три ключевых вызова: задачи с массивной памятью, задачи с вычислениями и задачи с долговременной памятью. Эксперименты показывают, что по мере эволюции больших языковых моделей (LLM), таких как GPT-4, Claude 3.7 Sonnet и Gemini 2.5 Pro, наблюдается значительное улучшение производительности на WebChoreArena. Однако результаты также указывают на то, что даже с Gemini 2.5 Pro остается значительное пространство для улучшения по сравнению с WebArena.'}, 'en': {'title': 'WebChoreArena: Elevating LLMs to Tackle Tedious Web Tasks', 'desc': 'WebChoreArena is a new benchmark that includes 532 tasks designed to evaluate the capabilities of large language models (LLMs) in handling complex web browsing chores. It extends the previous WebArena benchmark by focusing on more tedious tasks that require advanced skills such as massive memory retrieval, precise calculations, and long-term memory management across multiple web pages. The benchmark allows for reproducible experiments and fair comparisons with existing models, showcasing the progress of LLMs like GPT-4o and Gemini 2.5 Pro. Despite improvements in performance, the results indicate that there is still significant room for enhancement in tackling the challenges presented by WebChoreArena compared to general browsing tasks.'}, 'zh': {'title': 'WebChoreArena：评估LLM在复杂任务中的能力', 'desc': 'WebChoreArena是一个新的基准测试，包含532个任务，旨在评估大型语言模型（LLM）在复杂和繁琐的网页浏览任务中的能力。该基准测试扩展了WebArena的范围，专注于人类通常避免的繁重任务。WebChoreArena整合了三大挑战：大规模记忆任务、计算任务和长期记忆任务，确保了严格的可重复性。实验结果表明，随着LLM的进步，性能显著提升，但仍有改进空间，显示出WebChoreArena的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'url': 'https://huggingface.co/papers/2505.23977', 'abstract': 'VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'fef2cab0e56bc9bd', 'authors': ['Yichen Feng', 'Zhangchen Xu', 'Fengqing Jiang', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23977.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Синтетические данные для улучшения логического мышления ИИ', 'desc': 'VisualSphinx - это крупномасштабный синтетический набор данных для улучшения мультимодального рассуждения в визуально-языковых моделях. Он создан с помощью специального конвейера синтеза изображений на основе правил. Эксперименты показывают, что обучение на VisualSphinx улучшает способности моделей к логическому рассуждению. Усовершенствованные навыки рассуждения, полученные на VisualSphinx, также полезны для других задач, таких как алгебраические, арифметические и геометрические рассуждения.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualSphinx', 'desc': 'VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry.'}, 'zh': {'title': 'VisualSphinx：提升视觉语言模型的逻辑推理能力', 'desc': 'VisualSphinx是一个大规模的合成数据集，旨在提升视觉语言模型在多模态推理方面的表现。该数据集专注于逻辑推理任务，解决了当前模型缺乏结构化训练数据的问题。通过规则到图像的合成流程，VisualSphinx能够生成与问题相关的图像，增强模型的逻辑一致性和可读性。实验表明，使用VisualSphinx训练的视觉语言模型在逻辑推理、代数推理、算术推理和几何推理等任务上表现更佳。'}}}, {'id': 'https://huggingface.co/papers/2505.23059', 'title': 'From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval', 'url': 'https://huggingface.co/papers/2505.23059', 'abstract': 'State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.', 'score': 8, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '83af42c01de2e64c', 'authors': ['Dohyeon Lee', 'Yeonseok Jeong', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23059.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SMR: Эффективные рассуждения для языковых моделей', 'desc': 'Статья представляет новый метод рассуждений для больших языковых моделей под названием State Machine Reasoning (SMR). SMR улучшает производительность информационного поиска и снижает использование токенов, решая проблему избыточных рассуждений. Метод использует дискретные действия (уточнение, переранжирование, остановка) для более точного контроля процесса рассуждений. Эксперименты показали, что SMR повышает качество поиска на 3.4% при снижении использования токенов на 74.4%.'}, 'en': {'title': 'Streamlining Retrieval with State Machine Reasoning', 'desc': 'State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments.'}, 'zh': {'title': '状态机推理：提升检索效率，减少资源消耗', 'desc': '状态机推理（SMR）通过离散动作框架来改善信息检索性能，并减少大型语言模型的令牌使用，解决了过度思考的问题。该方法识别了信息检索中的两个主要挑战：冗余轨迹和误导性推理。SMR采用基于转移的推理框架，包含精细控制的离散动作（如精炼、重新排序和停止），支持早期停止。实验结果表明，SMR在BEIR和BRIGHT基准上提高了3.4%的检索性能，同时减少了74.4%的令牌使用。'}}}, {'id': 'https://huggingface.co/papers/2505.23001', 'title': 'DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors', 'url': 'https://huggingface.co/papers/2505.23001', 'abstract': 'DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'cd584a75fce48ae2', 'authors': ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2505.23001.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'DyePack: Ловушка для нечестных моделей машинного обучения', 'desc': 'DyePack - это фреймворк, использующий атаки типа backdoor для выявления моделей, которые использовали тестовые наборы данных во время обучения. Он вводит безвредные образцы backdoor в тестовые данные, чтобы пометить модели, обучавшиеся на них. DyePack обеспечивает точный расчет уровня ложноположительных результатов и предотвращает ложные обвинения. Фреймворк был успешно протестирован на пяти моделях и трех наборах данных, охватывающих задачи с множественным выбором и открытой генерацией.'}, 'en': {'title': 'DyePack: Safeguarding Model Integrity with Backdoor Detection', 'desc': 'DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates.'}, 'zh': {'title': 'DyePack：精准识别训练中使用基准测试集的模型', 'desc': 'DyePack是一个利用后门攻击的框架，用于识别在训练中使用基准测试集的模型。它通过引入良性后门样本，确保准确的假阳性率，同时防止错误指控。DyePack的设计结合了多个具有随机目标的后门，使得在标记每个模型时能够精确计算假阳性率。通过在多个模型和数据集上的评估，DyePack成功检测到所有受污染的模型，且假阳性率极低。'}}}, {'id': 'https://huggingface.co/papers/2505.24523', 'title': 'Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors', 'url': 'https://huggingface.co/papers/2505.24523', 'abstract': "Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors' reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to detect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts.", 'score': 7, 'issue_id': 4096, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4c6278bf22171f39', 'authors': ['Andrea Pedrotti', 'Michele Papucci', 'Cristiano Ciaccio', 'Alessio Miaschi', 'Giovanni Puccetti', "Felice Dell'Orletta", 'Andrea Esuli'], 'affiliations': ['Department of Computer Science, University of Pisa', 'Istituto di Scienza Tecnologie dellInformazione A. Faedo (CNR-ISTI)', 'ItaliaNLP Lab, Istituto di Linguistica Computazionale Antonio Zampolli (CNR-ILC)'], 'pdf_title_img': 'assets/pdf/title_img/2505.24523.jpg', 'data': {'categories': ['#data', '#hallucinations', '#benchmark', '#security', '#rlhf', '#alignment'], 'emoji': '🕵️', 'ru': {'title': 'Обман детекторов: как состязательные атаки подрывают обнаружение ИИ-текста', 'desc': 'Статья описывает метод создания состязательных атак на детекторы машинно-сгенерированного текста (MGT) с использованием тонкой настройки языковых моделей методом Direct Preference Optimization. Авторы демонстрируют, что такие атаки могут значительно снизить эффективность существующих детекторов MGT, делая сгенерированный текст более похожим на человеческий. Исследование также анализирует лингвистические изменения, вызванные этой настройкой, и особенности, используемые детекторами для обнаружения MGT. Результаты подчеркивают необходимость улучшения методов обнаружения и повышения их устойчивости к новым вариантам генерируемого текста.'}, 'en': {'title': 'Fooling the Detectives: Enhancing MGT Stealth with DPO', 'desc': 'This paper discusses how adversarial attacks can be used to improve the stealth of machine-generated text (MGT) by fine-tuning language models through Direct Preference Optimization (DPO). The authors demonstrate that these attacks can significantly reduce the effectiveness of current MGT detectors by altering the style of generated text to resemble human-written content. They also analyze the linguistic features that detectors rely on, revealing vulnerabilities in their detection capabilities. The findings emphasize the need for more robust detection methods to handle the evolving challenges posed by advanced generative AI.'}, 'zh': {'title': '提升检测器鲁棒性，抵御对抗性攻击', 'desc': '本研究探讨了对抗性攻击如何利用直接偏好优化（DPO）来微调语言模型，从而使其生成的文本更难被机器生成文本（MGT）检测器识别。我们发现，现有的MGT检测器在面对经过优化的文本时，性能显著下降，容易被欺骗。通过分析语言模型的风格转变，我们揭示了检测器依赖的语言特征。研究结果强调了改进检测方法的重要性，以增强其对未知文本的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2506.01881', 'title': 'WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue', 'url': 'https://huggingface.co/papers/2506.01881', 'abstract': 'STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'e82ff37de8341d1a', 'authors': ['Yaoyao Qian', 'Jindan Huang', 'Yuanli Wang', 'Simon Yu', 'Kyrie Zhixuan Zhou', 'Jiayuan Mao', 'Mingfu Liang', 'Hanhan Zhou'], 'affiliations': ['Boston University, Boston, MA', 'George Washington University, Washington, DC', 'Massachusetts Institute of Technology, Cambridge, MA', 'Northeastern University, Boston, MA', 'Northwestern University, Evanston, IL', 'Tufts University, Medford, MA', 'University of Texas at San Antonio, San Antonio, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.01881.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#alignment', '#agents'], 'emoji': '🌪️', 'ru': {'title': 'STORM: асимметричное моделирование намерений в диалоговых системах', 'desc': 'Статья представляет фреймворк STORM для моделирования асимметричной динамики информации в диалоговых системах. STORM использует две языковые модели - UserLLM и AgentLLM - для имитации процесса формирования намерений пользователя. Фреймворк позволяет создавать аннотированные корпуса, отражающие эволюцию понимания в ходе диалога. Эксперименты показали, что умеренная неопределенность может превосходить полную прозрачность в некоторых сценариях взаимодействия человека и ИИ.'}, 'en': {'title': 'Enhancing Dialogue Systems through Collaborative Intent Formation', 'desc': 'The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions.'}, 'zh': {'title': 'STORM：促进人机协作的意图形成', 'desc': 'STORM框架通过建模用户和代理之间的信息不对称动态，促进了任务导向对话系统中的协作意图形成。用户的表达虽然在语言上完整，但往往缺乏系统所需的结构信息，导致系统无法正确响应。STORM框架能够捕捉表达轨迹和潜在的认知转变，从而系统化分析协作理解的发展。实验结果表明，在某些情况下，适度的不确定性（40-60%）可以优于完全透明的信息，这为人机协作中的信息完整性提供了新的思考。'}}}, {'id': 'https://huggingface.co/papers/2506.00338', 'title': 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning', 'url': 'https://huggingface.co/papers/2506.00338', 'abstract': 'The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2f4783eb2db68192', 'authors': ['Yifan Peng', 'Shakeel Muhammad', 'Yui Sudo', 'William Chen', 'Jinchuan Tian', 'Chyi-Jiunn Lin', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University, United States', 'Honda Research Institute Japan, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.00338.jpg', 'data': {'categories': ['#training', '#low_resource', '#open_source', '#multilingual', '#data', '#audio', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Открытые речевые модели достигают уровня промышленных стандартов', 'desc': 'Проект OWSM улучшен с помощью масштабного очищенного веб-датасета, что привело к усовершенствованию мультиязычных речевых моделей. Разработан масштабируемый конвейер для очистки данных, результатом которого стал датасет с 166 000 часами речи на 75 языках. Новая серия моделей OWSM v4, обученная на этом курированном датасете, значительно превосходит предыдущие версии по мультиязычным бенчмаркам. Модели даже соответствуют или превосходят передовые промышленные модели, такие как Whisper и MMS, в нескольких сценариях.'}, 'en': {'title': 'Enhancing Multilingual Speech Models with Cleaned Web Data', 'desc': 'The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition.'}, 'zh': {'title': '提升多语言语音模型的开创性进展', 'desc': 'OWSM项目通过整合一个大型清洗过的网络数据集YODAS，提升了多语言语音模型的性能。YODAS数据集包含了大量的语音数据，但由于其原始特性，存在语言标签错误和音频文本不对齐等问题。为了解决这些问题，我们开发了一个可扩展的数据清洗流程，最终生成了一个包含75种语言、166,000小时语音的数据集。新的OWSM v4模型在多语言基准测试中表现优异，甚至在多个场景中与领先的工业模型相媲美。'}}}, {'id': 'https://huggingface.co/papers/2505.24842', 'title': 'Cascading Adversarial Bias from Injection to Distillation in Language\n  Models', 'url': 'https://huggingface.co/papers/2505.24842', 'abstract': 'Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.', 'score': 6, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '15a12805380711b7', 'authors': ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea'], 'affiliations': ['Google DeepMind', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24842.jpg', 'data': {'categories': ['#security', '#ethics', '#training', '#inference', '#data', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Скрытая угроза: как предвзятость усиливается при дистилляции языковых моделей', 'desc': 'Статья исследует уязвимость дистиллированных языковых моделей к внедрению предвзятого контента во время обучения. Авторы демонстрируют, что даже минимальное отравление данных учителя может привести к значительному усилению предвзятости в модели ученика. Предложены два режима распространения предвзятости: нецеленаправленный, влияющий на множество задач, и целенаправленный, фокусирующийся на конкретных задачах. Результаты показывают недостатки существующих методов защиты и подчеркивают необходимость разработки специализированных мер безопасности для дистиллированных моделей.'}, 'en': {'title': 'Strengthening Distilled Models Against Adversarial Bias Injection', 'desc': 'This paper explores the vulnerabilities of distilled language models to adversarial attacks, specifically through the injection of biased content during their training phase. It shows that adversaries can subtly poison teacher models with minimal data, which then amplifies biases in the student models that are derived from them. The study identifies two modes of bias propagation: Untargeted, affecting multiple tasks, and Targeted, which focuses on specific tasks while keeping normal behavior intact. The findings reveal that current defenses are inadequate, emphasizing the need for improved strategies to safeguard against these security threats in distilled models.'}, 'zh': {'title': '保护蒸馏模型，抵御对抗性偏见攻击！', 'desc': '模型蒸馏在创建小型可部署语言模型中变得至关重要，这些模型保留了更大系统的能力。然而，广泛部署引发了对抗性操控的脆弱性问题。本文研究了蒸馏模型在训练过程中对偏见内容的对抗性注入的脆弱性。我们提出了两种传播模式，并展示了如何通过最小的数据污染使教师模型注入微妙的偏见，这些偏见在学生模型中被显著放大。'}}}, {'id': 'https://huggingface.co/papers/2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors', 'url': 'https://huggingface.co/papers/2505.24625', 'abstract': "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'score': 6, 'issue_id': 4087, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bfa132788ee6990', 'authors': ['Duo Zheng', 'Shijia Huang', 'Yanyang Li', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.24625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#games', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в 3D-понимании: извлечение геометрии напрямую из видео', 'desc': 'Исследователи представили новую модель Video-3D Geometry Large Language Model (VG LLM), которая извлекает трехмерную информацию непосредственно из видеопоследовательностей для улучшения понимания 3D-сцен. В отличие от предыдущих подходов, VG LLM не требует дополнительных 3D-данных, таких как облака точек или реконструированные карты с видом сверху. Модель использует энкодер 3D-визуальной геометрии для извлечения априорной 3D-информации из видео, которая затем интегрируется с визуальными токенами и подается в мультимодальную языковую модель. Эксперименты показали, что VG LLM достигает конкурентоспособных результатов в различных задачах 3D-понимания сцен и пространственного рассуждения, превосходя некоторые современные методы.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding from Video Alone!', 'desc': 'The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data.'}, 'zh': {'title': '视频驱动的3D理解新突破', 'desc': '本文提出了一种新颖的视频-3D几何大语言模型（VG LLM），能够直接从视频序列中提取3D信息，从而增强3D场景理解，而无需额外的3D数据。该模型利用3D视觉几何编码器，从视频中提取3D先验信息，并将其与视觉标记结合，输入到多模态大语言模型中。通过大量实验，结果表明该方法在3D场景理解和空间推理等任务上取得了显著的改进。值得注意的是，我们的4B模型在不依赖显式3D数据输入的情况下，达到了与现有最先进方法相媲美的结果，甚至在VSI-Bench评估中超越了Gemini-1.5-Pro。'}}}, {'id': 'https://huggingface.co/papers/2505.24183', 'title': 'CodeV-R1: Reasoning-Enhanced Verilog Generation', 'url': 'https://huggingface.co/papers/2505.24183', 'abstract': 'CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.', 'score': 6, 'issue_id': 4093, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'b542a58b96860ad6', 'authors': ['Yaoyu Zhu', 'Di Huang', 'Hanqi Lyu', 'Xiaoyun Zhang', 'Chongxiao Li', 'Wenxuan Shi', 'Yutong Wu', 'Jianan Mu', 'Jinghua Wang', 'Yang Zhao', 'Pengwei Jin', 'Shuyao Cheng', 'Shengwen Liang', 'Xishan Zhang', 'Rui Zhang', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yunji Chen'], 'affiliations': ['Cambricon Technologies', 'SKL of Processors, Institute of Computing Technology, CAS', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.24183.jpg', 'data': {'categories': ['#games', '#rl', '#dataset', '#optimization', '#open_source', '#training'], 'emoji': '🔧', 'ru': {'title': 'CodeV-R1: Прорыв в автоматизации проектирования электроники', 'desc': 'В статье представлена новая система CodeV-R1 для генерации кода на языке Verilog с использованием LLM и метода RLVR. Основные проблемы, которые решает система, включают отсутствие автоматизированных сред верификации, нехватку качественных пар "естественный язык - код" и высокие вычислительные затраты. CodeV-R1 использует генератор тестов на основе правил и метод синтеза данных для создания высококачественного набора данных. Модель CodeV-R1-7B демонстрирует значительное улучшение производительности по сравнению с предыдущими методами, что способствует развитию исследований в области автоматизации проектирования электроники.'}, 'en': {'title': 'Revolutionizing Verilog Generation with CodeV-R1', 'desc': 'The paper presents CodeV-R1, a reinforcement learning with verifiable reward (RLVR) framework designed for generating Verilog code from natural language specifications. It addresses challenges in electronic design automation (EDA) by introducing a rule-based testbench for equivalence checking and a round-trip data synthesis method to create a high-quality dataset of NL-code pairs. The training process utilizes a two-stage approach, combining knowledge distillation with an adaptive RLVR algorithm to optimize training efficiency. CodeV-R1 demonstrates significant improvements in performance metrics, surpassing previous state-of-the-art models in Verilog generation tasks.'}, 'zh': {'title': 'CodeV-R1：电子设计自动化的强化学习新突破', 'desc': '本文介绍了CodeV-R1，这是一个用于Verilog生成的强化学习可验证奖励（RLVR）框架，旨在解决电子设计自动化（EDA）中的关键挑战。该框架通过开发基于规则的测试平台生成器和回合数据合成方法，确保生成的代码与自然语言描述之间的一致性。我们还采用了两阶段的训练流程，首先进行知识蒸馏以提升推理能力，然后使用自适应的RLVR算法降低训练成本。最终，CodeV-R1-7B模型在VerilogEval v2和RTLLM v1.1上取得了显著的性能提升，超越了之前的最佳结果。'}}}, {'id': 'https://huggingface.co/papers/2505.21179', 'title': 'Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model', 'url': 'https://huggingface.co/papers/2505.21179', 'abstract': 'Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a universal plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!', 'score': 6, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3e7694e3e9f014f5', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2505.21179.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv', '#optimization', '#video'], 'emoji': '🧠', 'ru': {'title': 'NAG: универсальное негативное руководство для диффузионных моделей', 'desc': 'Статья представляет новый метод под названием Normalized Attention Guidance (NAG) для улучшения работы диффузионных моделей. NAG позволяет эффективно применять негативное руководство в различных режимах и модальностях без необходимости переобучения модели. В отличие от существующих подходов, NAG обобщается на разные архитектуры, режимы сэмплирования и модальности, функционируя как универсальный плагин с минимальными вычислительными затратами. Эксперименты показывают улучшения в соответствии текста и изображения, качестве генерации и восприятии человеком.'}, 'en': {'title': 'Effortless Negative Guidance for Diffusion Models with NAG', 'desc': 'Normalized Attention Guidance (NAG) is a novel method that improves diffusion models by providing effective negative guidance without the need for retraining. It addresses the challenge of suppressing unwanted attributes, especially in scenarios with few sampling steps where traditional methods like Classifier-Free Guidance (CFG) struggle. NAG utilizes an efficient mechanism that normalizes attention using L1-based techniques, allowing it to maintain high fidelity while enhancing negative guidance. This approach is versatile, working across different architectures, sampling regimes, and modalities, making it a universal solution for modern diffusion frameworks.'}, 'zh': {'title': '归一化注意力引导：无缝负引导的解决方案', 'desc': '归一化注意力引导（NAG）通过在不同的采样阶段和模态中提供有效的负引导，增强了扩散模型，而无需重新训练。负引导的挑战在于在少步采样中显得尤为突出，传统的无分类器引导（CFG）在激进的采样步骤压缩下表现不佳。NAG采用基于L1的归一化和精炼方法，在注意力空间中进行外推，恢复了有效的负引导。通过广泛的实验，我们证明了NAG在文本对齐、保真度和人类感知质量方面的一致性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01084', 'title': 'zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression', 'url': 'https://huggingface.co/papers/2506.01084', 'abstract': 'A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers\' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency.', 'score': 5, 'issue_id': 4093, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'f9927f51990f811a', 'authors': ['Saibo Geng', 'Nathan Ranchin', 'Yunzhen yao', 'Maxime Peyrard', 'Chris Wendler', 'Michael Gastpar', 'Robert West'], 'affiliations': ['EPFL', 'Microsoft', 'Northeastern University', 'Université Grenoble Alpes, CNRS, Grenoble INP, LIG'], 'pdf_title_img': 'assets/pdf/title_img/2506.01084.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'Ускорение LLM с помощью динамического сжатия токенов', 'desc': 'В статье представлена новая система zip2zip, которая улучшает работу LLM, динамически изменяя словарь токенов во время вывода. Это достигается с помощью сжатия LZW, что позволяет сократить длину последовательности токенов и ускорить процесс вывода. Система включает в себя токенизатор, основанный на LZW, слой для вычисления эмбеддингов новых токенов и модифицированную модель языкового моделирования. Результаты показывают, что zip2zip может сократить длину последовательностей на 20-60% и значительно уменьшить задержки при выводе.'}, 'en': {'title': 'Dynamic Tokenization for Faster Inference in LLMs', 'desc': "The paper presents zip2zip, a novel framework that enhances the efficiency of large language models (LLMs) by dynamically adjusting their token vocabulary during inference. By utilizing Lempel-Ziv-Welch (LZW) compression, zip2zip reduces the length of token sequences, which leads to faster inference speeds. The framework includes a tokenizer that creates reusable 'hypertokens', an embedding layer for these hypertokens, and a causal language model that operates on compressed sequences. The results demonstrate that zip2zip can significantly decrease input and output lengths by 20-60%, improving overall model performance and reducing computational costs."}, 'zh': {'title': '动态调整令牌，提升推理速度', 'desc': 'zip2zip是一个框架，它在推理时动态调整大型语言模型（LLMs）的令牌词汇，使用LZW压缩技术来减少令牌序列的长度，从而提高推理速度。传统的令牌化方法通常依赖于静态的令牌器，这些令牌器的固定词汇无法适应特定领域或语言的输入，导致生成更长的令牌序列和更高的计算成本。zip2zip通过三个关键组件实现其功能：基于LZW压缩的令牌器、实时计算新形成的超令牌的嵌入层，以及训练模型处理压缩序列的因果语言建模变体。实验表明，经过zip2zip处理的LLM在推理时能够有效使用超令牌，输入和输出序列长度减少20-60%，推理延迟显著降低。'}}}, {'id': 'https://huggingface.co/papers/2506.00512', 'title': 'Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing', 'url': 'https://huggingface.co/papers/2506.00512', 'abstract': 'A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.', 'score': 5, 'issue_id': 4094, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0a9ce5d9ebc76a52', 'authors': ['Yang Zheng', 'Mengqi Huang', 'Nan Chen', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00512.jpg', 'data': {'categories': ['#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Прогрессивное 3D-редактирование: от текста к согласованным изменениям', 'desc': 'Статья представляет новый подход к редактированию 3D-объектов с помощью текстовых инструкций. Авторы предлагают парадигму прогрессивных видов, которая обеспечивает согласованное редактирование 3D-объектов путем распространения семантики от ключевых видов к менее отредактированным. Метод Pro3D-Editor включает в себя выборку основного вида, рендеринг ключевых видов и уточнение полного вида. Эксперименты показывают, что этот метод превосходит существующие подходы по точности редактирования и пространственной согласованности.'}, 'en': {'title': 'Achieving Consistent 3D Editing with Pro3D-Editor', 'desc': 'This paper introduces a new approach for 3D editing called Pro3D-Editor, which focuses on maintaining consistency across different views of a 3D object. Unlike traditional methods that treat all views equally, this framework uses a progressive-views paradigm to propagate editing information from the most important view to others. It consists of three main components: a Primary-view Sampler that identifies and edits the most relevant view, a Key-view Render that transfers the editing semantics to other views, and a Full-view Refiner that finalizes the 3D object based on the edited views. The results show that Pro3D-Editor achieves better accuracy and consistency compared to existing 3D editing techniques.'}, 'zh': {'title': '渐进视图范式实现一致的3D编辑', 'desc': '本文提出了一种渐进视图范式，通过Pro3D-Editor实现一致的3D编辑。该方法通过从关键视图向较少编辑的视图传播语义，解决了现有方法在多视图编辑中存在的不一致性问题。Pro3D-Editor框架包括主要视图采样器、关键视图渲染和全视图精炼器，能够动态选择最重要的视图进行编辑。实验结果表明，该方法在编辑精度和空间一致性方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2505.24452', 'title': 'Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training', 'url': 'https://huggingface.co/papers/2505.24452', 'abstract': 'A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of varphi.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.', 'score': 5, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '82972c2646341cc9', 'authors': ['Anda Tang', 'Yiming Dong', 'Yutao Zeng', 'zhou Xun', 'Zhouchen Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Artificial Intelligence, Peking University', 'Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China', 'State Key Lab of General AI, School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24452.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '📊', 'ru': {'title': 'Умный график обучения: максимум эффективности при ограниченных ресурсах', 'desc': 'Предложен унифицированный график скорости обучения с учетом бюджета для оптимизации обучения в условиях ограниченного количества итераций. Новый подход, названный Unified Budget-Aware (UBA), основан на теоретической базе и учитывает устойчивость к вариациям кривизны ландшафта оптимизации. UBA превосходит традиционные графики для различных задач и архитектур нейронных сетей. Метод контролируется одним гиперпараметром φ, который обеспечивает баланс между гибкостью и простотой.'}, 'en': {'title': 'Optimizing Training with Unified Budget-Aware Learning Rates', 'desc': 'This paper introduces a new learning rate schedule called the Unified Budget-Aware (UBA) schedule, designed to optimize training when there are limits on the number of iterations. Traditional learning rate schedules often rely on trial-and-error and lack a solid theoretical basis, making them inefficient. The UBA schedule is grounded in a novel optimization framework that considers the curvature of the loss landscape, allowing it to adapt better to various tasks and network architectures. Experimental results demonstrate that UBA outperforms standard schedules across different vision and language tasks, providing a more effective training strategy within constrained budgets.'}, 'zh': {'title': '统一预算感知学习率调度，优化有限训练预算', 'desc': '本文提出了一种统一的预算感知学习率调度（UBA），旨在优化在有限迭代预算下的训练效果。传统的学习率调度方法往往依赖经验，缺乏理论基础，而UBA则通过构建一个新的优化框架，考虑了对损失函数曲率变化的鲁棒性。该调度由一个超参数控制，能够在灵活性和简单性之间取得平衡，避免了对每个网络进行数值优化的需求。实验结果表明，UBA在多种视觉和语言任务中，均优于常用的学习率调度方法。'}}}, {'id': 'https://huggingface.co/papers/2505.23504', 'title': 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.23504', 'abstract': 'VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.', 'score': 5, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c243189c9ec32d1f', 'authors': ['Liyun Zhu', 'Qixiang Chen', 'Xi Shen', 'Xiaodong Cun'], 'affiliations': ['Australian National University', 'GVC Lab, Great Bay University', 'Intellindust AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.23504.jpg', 'data': {'categories': ['#rl', '#interpretability', '#multimodal', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Умное видеонаблюдение: ИИ учится понимать аномалии', 'desc': 'VAU-R1 - это новая система для понимания аномалий в видео, использующая мультимодальные большие языковые модели (MLLM) и усиленное обучение. Авторы также представили VAU-Bench - первый бенчмарк для оценки рассуждений об аномалиях в видео, основанный на методе цепочки мыслей. Система VAU-R1 значительно улучшает точность ответов на вопросы, временную привязку и согласованность рассуждений в различных контекстах. Это исследование закладывает основу для интерпретируемого и основанного на рассуждениях понимания аномалий в видео.'}, 'en': {'title': 'Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench', 'desc': 'The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts.'}, 'zh': {'title': '提升视频异常推理的智能框架', 'desc': 'VAU-R1 是一个基于多模态大语言模型的框架，旨在提升视频异常推理能力。通过强化微调（Reinforcement Fine-Tuning），该方法能够更好地理解和解释异常事件。我们还提出了 VAU-Bench，这是一个专门用于视频异常推理的链式思维基准，包含多项选择问答、详细推理、时间标注和描述性标题。实验结果表明，VAU-R1 在问答准确性、时间定位和推理连贯性方面有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01484', 'title': 'LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification', 'url': 'https://huggingface.co/papers/2506.01484', 'abstract': 'A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  \t\t\t\t\tAI-generated summary \t\t\t\t Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '422c267bbe9577db', 'authors': ['Shuzhou Yuan', 'Ercong Nie', 'Lukas Kouba', 'Ashish Yashwanth Kangen', 'Helmut Schmid', 'Hinrich Schutze', 'Michael Farber'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning (MCML)', 'ScaDS.AI and TU Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.01484.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#dataset', '#data', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'ИИ очищает интернет от языка ненависти', 'desc': 'Статья представляет новый подход к детоксификации языка ненависти с использованием GPT-4o-mini. Авторы создали крупномасштабный датасет PARADEHATE, содержащий более 8000 пар токсичных и нетоксичных текстов. Эксперименты показали, что модели, обученные на этом датасете, демонстрируют улучшенные результаты по точности стиля, сохранению содержания и плавности текста. Этот метод предлагается как масштабируемая альтернатива ручной аннотации для создания данных по детоксификации.'}, 'en': {'title': 'Automating Hate Speech Detoxification with GPT-4o-mini', 'desc': "This paper introduces a new method for creating a large dataset aimed at detoxifying hate speech using the GPT-4o-mini model. Detoxification involves rewriting harmful language into non-toxic text, which is crucial due to the rise of toxic content online. The authors developed a pipeline that automates this process, replacing human annotators with a language model, and found that the model's performance is comparable to that of humans. They also created a dataset called PARADEHATE, consisting of over 8,000 pairs of hate and non-hate text, which significantly improves the performance of various models in terms of style accuracy, content preservation, and fluency."}, 'zh': {'title': '利用GPT-4o-mini生成仇恨言论去毒化数据集', 'desc': '本文提出了一种新颖的管道，利用GPT-4o-mini生成大规模的仇恨言论去毒化数据集，从而提高基线模型在风格准确性、内容保留和流畅性方面的表现。去毒化是将有害语言重写为非有害文本的任务，随着网络上有毒内容的增加，这一任务变得越来越重要。由于人工标注的成本和敏感性，高质量的去毒化平行数据集仍然稀缺。我们构建了PARADEHATE，这是一个专门用于仇恨言论去毒化的大规模平行数据集，并通过实验验证了基于该数据集的模型的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.00643', 'title': 'SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions', 'url': 'https://huggingface.co/papers/2506.00643', 'abstract': "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.", 'score': 4, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': 'f95c367c9eaf00a9', 'authors': ['Weijie Xu', 'Shixian Cui', 'Xi Fang', 'Chi Xue', 'Stephanie Eckman', 'Chandan Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2506.00643.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#interpretability', '#data', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений языковых моделей в задачах с множественным выбором', 'desc': 'Статья представляет SATA-BENCH - первый специализированный бенчмарк для оценки языковых моделей на вопросах с множественным выбором ответов. Исследование выявило значительные ограничения даже у самых сильных моделей, достигающих лишь 41.8% точного соответствия. Авторы обнаружили проблемы смещения выбора и смещения количества ответов у моделей. Для решения этих проблем предложена стратегия Choice Funnel, сочетающая дебиасинг токенов с адаптивным порогом.'}, 'en': {'title': 'Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel', 'desc': 'The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks.'}, 'zh': {'title': '提升多答案推理的准确性与效率', 'desc': '本文介绍了SATA-BENCH，这是一个专门用于评估大型语言模型（LLMs）在多答案问题上的基准测试。研究发现，现有模型在选择所有正确答案时存在显著的选择偏差和计数偏差，导致准确率低下。为了解决这些问题，提出了Choice Funnel解码策略，通过去偏和自适应阈值引导模型做出更完整和准确的选择。实验结果表明，Choice Funnel在准确匹配率上比竞争基线提高了29%，同时降低了推理成本超过64%。'}}}, {'id': 'https://huggingface.co/papers/2505.24086', 'title': 'ComposeAnything: Composite Object Priors for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.24086', 'abstract': 'ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '2bd92a7129e6945b', 'authors': ['Zeeshan Khan', 'Shizhe Chen', 'Cordelia Schmid'], 'affiliations': ['Inria, École normale supérieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24086.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#diffusion', '#interpretability', '#cv', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение композиции в генерации изображений с помощью 2.5D семантических макетов', 'desc': 'ComposeAnything - это новый подход к улучшению генерации изображений по текстовому описанию. Он использует языковые модели для создания 2.5D семантических макетов, включающих 2D ограничивающие рамки объектов с информацией о глубине и подробными подписями. Этот макет служит сильным и интерпретируемым приором, заменяющим стохастическую инициализацию шумом в диффузионных моделях генерации изображений. ComposeAnything превосходит современные методы на бенчмарках T2I-CompBench и NSR-1K для запросов с 2D/3D пространственными расположениями, большим количеством объектов и сюрреалистическими композициями.'}, 'en': {'title': 'ComposeAnything: Elevating Text-to-Image Generation with 2.5D Layouts', 'desc': 'ComposeAnything is a framework that enhances text-to-image generation by utilizing large language models (LLMs) to create 2.5D semantic layouts. This method improves the arrangement of objects in images by incorporating depth information, which helps maintain coherence and quality in the generated images. Unlike previous models that rely solely on 2D layouts, ComposeAnything provides a more accurate representation of spatial relationships, allowing for better object placement. The framework has shown superior performance on benchmark tests, producing high-quality images that align closely with the provided text descriptions.'}, 'zh': {'title': 'ComposeAnything：提升文本到图像生成的创新框架', 'desc': 'ComposeAnything 是一种新框架，旨在改善文本到图像生成的质量。它利用大型语言模型（LLMs）的推理能力，生成包含深度信息的2.5D语义布局，从而增强对象的放置和一致性。该方法不需要重新训练现有的文本到图像模型，而是通过生成空间和深度感知的粗略合成图像，来指导去噪过程。ComposeAnything 在处理复杂的2D/3D空间布局和超现实组合时，表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2505.22954', 'title': 'Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents', 'url': 'https://huggingface.co/papers/2505.22954', 'abstract': 'The Darwin G\\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Today\'s AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The G\\"odel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin G\\"odel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.', 'score': 4, 'issue_id': 4104, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '014c0d439b8212f8', 'authors': ['Jenny Zhang', 'Shengran Hu', 'Cong Lu', 'Robert Lange', 'Jeff Clune'], 'affiliations': ['Canada CIFAR AI Chair', 'Sakana AI', 'University of British Columbia', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.22954.jpg', 'data': {'categories': ['#benchmark', '#training', '#optimization', '#agi', '#agents', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'Эволюционирующий ИИ: самосовершенствование через итеративную модификацию кода', 'desc': 'Статья представляет Darwin Gödel Machine (DGM) - систему искусственного интеллекта, способную к самосовершенствованию путем итеративной модификации собственного кода. DGM использует принципы дарвиновской эволюции и открытого исследования, поддерживая архив кодирующих агентов и создавая их новые версии с помощью базовой модели. Система значительно улучшила свои способности к кодированию, повысив производительность на бенчмарках SWE-bench и Polyglot. DGM превзошла базовые подходы без самосовершенствования и открытого исследования, представляя важный шаг к созданию самосовершенствующегося ИИ.'}, 'en': {'title': 'Evolving AI: The Future of Self-Improvement', 'desc': 'The Darwin G"odel Machine (DGM) is a self-improving AI system that enhances its coding abilities through iterative self-modification and open-ended exploration. Unlike traditional AI, which relies on fixed architectures, the DGM autonomously evolves by modifying its own code and validating these changes against coding benchmarks. It employs a Darwinian approach, maintaining an archive of coding agents and generating new versions to explore diverse solutions. This method has shown significant performance improvements in coding tasks, demonstrating the potential for continuous and safe AI advancement.'}, 'zh': {'title': '自我改进的AI：达尔文哥德尔机器的创新之路', 'desc': '达尔文哥德尔机器（DGM）通过迭代自我修改和开放式探索来提高其编码能力，超越了其他方法的基准测试。与传统的固定架构AI系统不同，DGM能够自主且持续地改进自身。它借鉴了达尔文进化的理念，维护一个生成编码代理的档案库，并通过采样和基础模型生成新版本，形成多样化的高质量代理树。实验结果表明，DGM在编码能力上显著提升，表现出更好的代码编辑工具和同行评审机制，标志着自我改进AI的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2506.01928', 'title': 'Esoteric Language Models', 'url': 'https://huggingface.co/papers/2506.01928', 'abstract': 'Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features--most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the **first to introduce KV caching for MDMs** while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to **65x** faster inference than standard MDMs and **4x** faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: [http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)', 'score': 3, 'issue_id': 4100, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '6b37258ad3883db7', 'authors': ['Subham Sekhar Sahoo', 'Zhihan Yang', 'Yash Akhauri', 'Johnna Liu', 'Deepansha Singh', 'Zhoujun Cheng', 'Zhengzhong Liu', 'Eric Xing', 'John Thickstun', 'Arash Vahdat'], 'affiliations': ['Cornell Tech', 'Cornell University', 'MBZUAI', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2506.01928.jpg', 'data': {'categories': ['#inference', '#open_source', '#benchmark', '#architecture', '#optimization', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Eso-LMs: Революция в языковом моделировании с KV-кэшированием', 'desc': 'Eso-LMs представляют собой новое семейство моделей, объединяющих авторегрессионные и маскированные диффузионные модели. Они вводят KV-кэширование для маскированных диффузионных моделей, что значительно повышает эффективность вывода. Eso-LMs достигают нового уровня производительности на стандартных бенчмарках языкового моделирования. Модели обеспечивают до 65 раз более быстрый вывод по сравнению со стандартными маскированными диффузионными моделями.'}, 'en': {'title': 'Eso-LMs: Fast and Efficient Language Modeling Revolution', 'desc': 'Eso-LMs are a new type of language model that combines features from both autoregressive and masked diffusion models. This fusion allows for better performance in language tasks by improving perplexity and inference speed. A key innovation is the introduction of KV caching in masked diffusion models, which enhances efficiency during inference while still allowing for parallel generation. As a result, Eso-LMs achieve significantly faster inference times compared to traditional models, setting new benchmarks in language modeling.'}, 'zh': {'title': 'Eso-LMs：自回归与掩蔽扩散模型的完美融合', 'desc': 'Eso-LMs是一种新型的语言模型，结合了自回归模型和掩蔽扩散模型的优点。它引入了KV缓存技术，使得在推理时的效率大幅提升，同时保持了并行生成的能力。通过优化采样策略，Eso-LMs在标准语言建模基准上达到了新的最佳性能，推理速度比传统的掩蔽扩散模型快65倍。该模型有效地解决了自回归模型和掩蔽扩散模型的局限性，提供了更好的生成质量和效率。'}}}, {'id': 'https://huggingface.co/papers/2506.01920', 'title': 'From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.01920', 'abstract': 'A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.', 'score': 3, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '42a070cbc2d3afee', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Adel Ammar', 'Yasser Al-Habashi', 'Abdulrahman Al-Batati', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.01920.jpg', 'data': {'categories': ['#machine_translation', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🇦🇪', 'ru': {'title': 'Культурно-ориентированная оценка арабских языковых моделей', 'desc': 'Представлен новый фреймворк оценки и набор данных ADMD для тестирования арабских языковых моделей. Проанализированы существующие наборы данных для оценки арабского языка, выявлены проблемы с лингвистической точностью и культурным соответствием. ADMD содержит 490 сложных вопросов по 10 основным областям для оценки языковых моделей. Результаты показали значительные различия в производительности моделей, особенно в областях, требующих глубокого понимания культуры.'}, 'en': {'title': 'Enhancing Arabic Language Models with Cultural Competence', 'desc': 'This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation.'}, 'zh': {'title': '提升阿拉伯语模型评估的文化能力', 'desc': '本文提出了一种新的评估框架和数据集ADMD，用于评估阿拉伯语模型，强调了性能差异和文化能力的重要性。我们分析了现有的阿拉伯语评估数据集，发现了语言准确性、文化对齐和方法论严谨性方面的重大问题。ADMD包含490个具有挑战性的问题，涵盖十个主要领域，旨在解决大型语言模型（LLMs）中的这些局限性。通过使用ADMD评估五个领先的语言模型，我们发现模型在不同领域的表现差异显著，尤其是在需要深厚文化理解和专业知识的领域。'}}}, {'id': 'https://huggingface.co/papers/2506.00723', 'title': 'Pitfalls in Evaluating Language Model Forecasters', 'url': 'https://huggingface.co/papers/2506.00723', 'abstract': 'Large language models (LLMs) have recently been applied to forecasting tasks, with some works claiming these systems match or exceed human performance. In this paper, we argue that, as a community, we should be careful about such conclusions as evaluating LLM forecasters presents unique challenges. We identify two broad categories of issues: (1) difficulty in trusting evaluation results due to many forms of temporal leakage, and (2) difficulty in extrapolating from evaluation performance to real-world forecasting. Through systematic analysis and concrete examples from prior work, we demonstrate how evaluation flaws can raise concerns about current and future performance claims. We argue that more rigorous evaluation methodologies are needed to confidently assess the forecasting abilities of LLMs.', 'score': 3, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '4260f72a1f88e8cd', 'authors': ['Daniel Paleka', 'Shashwat Goel', 'Jonas Geiping', 'Florian Tramèr'], 'affiliations': ['ELLIS Institute Tübingen', 'ETH Zurich', 'MPI Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.00723.jpg', 'data': {'categories': ['#data', '#leakage', '#benchmark', '#evaluation'], 'emoji': '⚠️', 'ru': {'title': 'Осторожно с выводами: сложности оценки LLM в прогнозировании', 'desc': 'Статья обсуждает применение больших языковых моделей (LLM) для задач прогнозирования. Авторы предупреждают о трудностях в оценке эффективности LLM для прогнозирования из-за проблем временной утечки данных и сложности экстраполяции результатов на реальные задачи. Они демонстрируют, как недостатки в методологии оценки могут привести к сомнительным выводам о производительности LLM. Авторы призывают к разработке более строгих методов оценки прогнозирующих способностей LLM.'}, 'en': {'title': 'Rethinking Evaluation: Ensuring Trust in LLM Forecasting', 'desc': 'This paper discusses the challenges of evaluating large language models (LLMs) in forecasting tasks, highlighting that claims of LLMs matching or exceeding human performance may be misleading. The authors identify two main issues: the risk of temporal leakage, which can distort evaluation results, and the difficulty in translating evaluation performance to real-world scenarios. They provide a systematic analysis and examples from previous studies to illustrate how these evaluation flaws can undermine confidence in LLM performance claims. The paper calls for the development of more rigorous evaluation methodologies to accurately assess the forecasting capabilities of LLMs.'}, 'zh': {'title': '谨慎评估大型语言模型的预测能力', 'desc': '大型语言模型（LLMs）最近被应用于预测任务，有些研究声称这些系统的表现与人类相当或更好。本文指出，评估LLM预测者存在独特的挑战，因此我们应对这些结论保持谨慎。我们识别出两个主要问题：一是由于多种时间泄漏形式，导致评估结果难以信任；二是从评估表现推断到现实世界预测的难度。通过系统分析和具体实例，我们展示了评估缺陷如何引发对当前和未来性能声明的担忧，并主张需要更严格的评估方法来自信地评估LLM的预测能力。'}}}, {'id': 'https://huggingface.co/papers/2505.21724', 'title': 'OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions', 'url': 'https://huggingface.co/papers/2505.21724', 'abstract': "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.", 'score': 3, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '309d90ff41ad30b0', 'authors': ['Cheng Luo', 'Jianghui Wang', 'Bing Li', 'Siyang Song', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'University of Exeter'], 'pdf_title_img': 'assets/pdf/title_img/2505.21724.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#dataset', '#optimization', '#audio', '#games'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальный ИИ для естественного диалога', 'desc': 'Статья представляет OmniResponse - мультимодальную большую языковую модель для генерации синхронизированных вербальных и невербальных ответов слушателя. Модель использует текст как промежуточную модальность для связи аудио и лицевых реакций. OmniResponse включает в себя компоненты Chrono-Text для временной привязки текстовых токенов и TempoVoice для синхронизированной генерации речи. Для обучения и оценки создан датасет ResponseNet с 696 диалогами, содержащими видео, аудио и аннотации.'}, 'en': {'title': 'Synchronized Responses for Natural Conversations', 'desc': 'This paper presents OmniResponse, a Multimodal Large Language Model designed to generate synchronized verbal and non-verbal responses in conversations. It introduces a new task called Online Multimodal Conversational Response Generation (OMCRG), which focuses on creating real-time feedback based on multimodal inputs from speakers. The model uses text as an intermediate step to ensure that audio and facial responses are well-coordinated. Additionally, it introduces two innovative components, Chrono-Text and TempoVoice, to enhance the quality and synchronization of the generated responses.'}, 'zh': {'title': 'OmniResponse：同步生成多模态响应的创新模型', 'desc': '本文介绍了一种新的任务，称为在线多模态对话响应生成（OMCRG），旨在根据说话者的多模态输入在线生成同步的语言和非语言反馈。为了解决生成的音频和面部反应之间的同步问题，研究者们创新性地引入了文本作为中介模态。我们提出了OmniResponse，这是一种多模态大型语言模型（MLLM），能够自回归地生成高质量的多模态听众响应。通过使用Chrono-Text和TempoVoice等新组件，OmniResponse在语义内容、音视频同步和生成质量方面显著优于基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.19621', 'title': 'Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models', 'url': 'https://huggingface.co/papers/2505.19621', 'abstract': "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS", 'score': 3, 'issue_id': 4093, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '4358fd586e320601', 'authors': ['George Kour', 'Itay Nakash', 'Ateret Anaby-Tavor', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2505.19621.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#multimodal', '#ethics', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Выявление скрытых предубеждений в языковых моделях', 'desc': 'Статья представляет новый бенчмарк под названием Preference, Opinion, and Belief survey (POBs) для оценки субъективных тенденций и предубеждений больших языковых моделей (LLM) в различных областях. Исследователи применили этот бенчмарк к ведущим открытым и закрытым LLM, измеряя такие желаемые свойства, как надежность, нейтральность и согласованность. Результаты показывают, что механизмы рассуждения и самоанализа предлагают лишь ограниченные улучшения в этой области. Обнаружено, что более новые версии моделей становятся менее согласованными и более предвзятыми к определенным точкам зрения.'}, 'en': {'title': 'Assessing Bias in Language Models: A Call for Neutrality', 'desc': 'This paper introduces the Preference, Opinion, and Belief survey (POBs), which evaluates the subjective biases of Large Language Models (LLMs) in various domains. It highlights that as LLMs are increasingly used in decision-making, their inherent biases can shape the advice they provide, potentially reinforcing certain viewpoints. The study assesses leading LLMs for properties like reliability and neutrality, revealing that newer models tend to exhibit greater bias and inconsistency. Additionally, it examines the impact of advanced reasoning techniques on these biases, finding only marginal improvements in performance.'}, 'zh': {'title': '评估大型语言模型的主观偏见', 'desc': '这篇论文介绍了一个名为偏好、观点和信念调查（POBs）的基准，用于评估大型语言模型（LLMs）在社会、文化、伦理和个人领域的主观倾向。研究发现，随着模型版本的更新，它们的偏见和不一致性有所增加，这可能影响它们对用户的建议和推荐。通过对领先的开源和闭源LLMs进行评估，论文测量了模型的可靠性、中立性和一致性等属性。结果表明，尽管推理和自我反思机制在其他任务中有效，但在本研究领域的提升有限，显示出模型在某些观点上的偏见加剧。'}}}, {'id': 'https://huggingface.co/papers/2506.01074', 'title': 'How Programming Concepts and Neurons Are Shared in Code Language Models', 'url': 'https://huggingface.co/papers/2506.01074', 'abstract': "LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Several studies have explored the mechanisms of large language models (LLMs) in coding tasks, but most have focused on programming languages (PLs) in a monolingual setting. In this paper, we investigate the relationship between multiple PLs and English in the concept space of LLMs. We perform a few-shot translation task on 21 PL pairs using two Llama-based models. By decoding the embeddings of intermediate layers during this task, we observe that the concept space is closer to English (including PL keywords) and assigns high probabilities to English tokens in the second half of the intermediate layers. We analyze neuron activations for 11 PLs and English, finding that while language-specific neurons are primarily concentrated in the bottom layers, those exclusive to each PL tend to appear in the top layers. For PLs that are highly aligned with multiple other PLs, identifying language-specific neurons is not feasible. These PLs also tend to have a larger keyword set than other PLs and are closer to the model's concept space regardless of the input/output PL in the translation task. Our findings provide insights into how LLMs internally represent PLs, revealing structural patterns in the model's concept space. Code is available at https://github.com/cisnlp/code-specific-neurons.", 'score': 2, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '4fca9ba1a062ca80', 'authors': ['Amir Hossein Kargaran', 'Yihong Liu', 'François Yvon', 'Hinrich Schütze'], 'affiliations': ['LMU Munich & Munich Center for Machine Learning', 'Sorbonne Université & CNRS, ISIR'], 'pdf_title_img': 'assets/pdf/title_img/2506.01074.jpg', 'data': {'categories': ['#transfer_learning', '#plp', '#machine_translation', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Языки программирования в мозге нейросети: ближе к английскому, чем кажется', 'desc': 'Исследование показывает, что в концептуальном пространстве больших языковых моделей (LLM) представления различных языков программирования кластеризуются ближе к английскому языку. Обнаружено, что активация нейронов, специфичных для конкретных языков программирования, наиболее выражена в верхних слоях модели. Языки программирования с высокой степенью схожести имеют близкие представления в модели. Результаты дают понимание того, как LLM внутренне представляют языки программирования, выявляя структурные паттерны в концептуальном пространстве модели.'}, 'en': {'title': 'Unraveling Language Representations in Large Language Models', 'desc': "This paper explores how large language models (LLMs) represent multiple programming languages (PLs) alongside English in their internal concept space. It reveals that LLMs cluster PLs closer to English, particularly in the upper layers, where distinct neuron activations occur for specific languages. The study employs a few-shot translation task across 21 PL pairs, analyzing embeddings and neuron activations to uncover structural patterns in the model's representation of PLs. The findings suggest that highly aligned PLs share similar representations and exhibit unique neuron activations, enhancing our understanding of LLMs in coding tasks."}, 'zh': {'title': '揭示大型语言模型中的编程语言表示结构', 'desc': '本论文研究了大型语言模型（LLMs）在多种编程语言（PLs）与英语之间的关系。我们发现，LLMs的概念空间中，编程语言的表示更接近英语，尤其是在中间层的后半部分，英语的标记概率较高。通过分析神经元激活，我们发现特定语言的神经元主要集中在底层，而每种编程语言独有的神经元则出现在上层。我们的研究揭示了LLMs如何在内部表示编程语言，并展示了模型概念空间中的结构模式。'}}}, {'id': 'https://huggingface.co/papers/2506.00930', 'title': 'Aligning VLM Assistants with Personalized Situated Cognition', 'url': 'https://huggingface.co/papers/2506.00930', 'abstract': "A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) aligned with general human objectives, such as being harmless and hallucination-free, have become valuable assistants of humans in managing visual tasks. However, people with diversified backgrounds have different cognition even in the same situation. Consequently, they may have personalized expectations for VLM assistants. This highlights the urgent need to align VLM assistants with personalized situated cognition for real-world assistance. To study this problem, we first simplify it by characterizing individuals based on the sociological concept of Role-Set. Then, we propose to evaluate the individuals' actions to examine whether the personalized alignment is achieved. Further, we construct a benchmark named PCogAlignBench, which includes 18k instances and 20 individuals with different Role-Sets. Finally, we present a framework called PCogAlign, which constructs a cognition-aware and action-based reward model for personalized alignment. Experimental results and human evaluations demonstrate the reliability of the PCogAlignBench and the effectiveness of our proposed PCogAlign. We will open-source the constructed benchmark and code at https://github.com/NLPGM/PCogAlign.", 'score': 2, 'issue_id': 4099, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '113b1c51e51c3619', 'authors': ['Yongqi Li', 'Shen Zhou', 'Xiaohu Li', 'Xin Miao', 'Jintao Wen', 'Mayi Xu', 'Jianhao Chen', 'Birong Pan', 'Hankun Kang', 'Yuanyuan Zhu', 'Ming Zhong', 'Tieyun Qian'], 'affiliations': ['School of Computer Science, Wuhan University, China', 'Zhongguancun Academy, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00930.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multimodal', '#open_source', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Персонализация визуально-языковых моделей для индивидуального познания', 'desc': 'PCogAlign - это фреймворк для создания модели вознаграждения, позволяющей согласовывать визуально-языковые модели с персонализированным ситуативным познанием. Он использует бенчмарк PCogAlignBench, включающий 18 тысяч примеров и 20 индивидуумов с различными наборами ролей. Фреймворк строит модель вознаграждения, учитывающую познание и основанную на действиях, для персонализированного согласования. Эксперименты и оценки людей показывают надежность бенчмарка и эффективность предложенного подхода.'}, 'en': {'title': 'Aligning AI with Personalized Human Cognition', 'desc': "The paper introduces PCogAlign, a framework designed to create a reward model that aligns vision-language models (VLMs) with personalized situated cognition. It recognizes that individuals from diverse backgrounds have unique cognitive expectations, which can affect their interactions with VLMs. To address this, the authors utilize the sociological concept of Role-Set to categorize individuals and evaluate their actions for personalized alignment. The study includes a benchmark called PCogAlignBench, featuring 18,000 instances across 20 different Role-Sets, demonstrating the framework's effectiveness through experimental results and human evaluations."}, 'zh': {'title': '个性化认知对齐的视觉-语言模型框架', 'desc': '本文提出了一个名为PCogAlign的框架，用于构建与个性化情境认知对齐的视觉-语言模型的奖励模型。研究表明，不同背景的人在相同情境下可能有不同的认知和期望，因此需要针对个体的个性化需求进行对齐。为此，作者基于社会学的角色集概念，简化了个体特征的描述，并构建了一个包含18000个实例和20个不同角色集个体的基准数据集PCogAlignBench。实验结果和人类评估表明，PCogAlignBench的可靠性和PCogAlign的有效性，相关代码和数据集将开源。'}}}, {'id': 'https://huggingface.co/papers/2506.00789', 'title': 'RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems', 'url': 'https://huggingface.co/papers/2506.00789', 'abstract': "RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.", 'score': 2, 'issue_id': 4102, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '5c7a672484e9a0fa', 'authors': ['Yixiao Zeng', 'Tianyu Cao', 'Danqing Wang', 'Xinran Zhao', 'Zimeng Qiu', 'Morteza Ziyadi', 'Tongshuang Wu', 'Lei Li'], 'affiliations': ['Amazon', 'Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00789.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#graphs', '#rag', '#security'], 'emoji': '🧠', 'ru': {'title': 'RARE: Стресс-тест для RAG-систем в реальном мире', 'desc': 'RARE - это новая система оценки устойчивости моделей генерации с дополнением из внешних источников (RAG) к реальным шумам и конфликтам контекста. Она использует графы знаний для создания тестовых наборов данных, охватывающих динамичные области финансов, экономики и политики. RARE оценивает способность моделей сохранять точность при изменении запросов, документов и результатов поиска. Результаты показывают, что RAG-системы особенно уязвимы к изменениям в документах и многоступенчатым запросам.'}, 'en': {'title': 'Enhancing RAG Systems: Evaluating Robustness in Real-World Scenarios', 'desc': 'The paper introduces RARE, a framework designed to evaluate the robustness of Retrieval-Augmented Generation (RAG) systems in real-world scenarios. It addresses how these systems handle noise, conflicting contexts, and rapidly changing information by using a knowledge-graph-driven benchmark. RARE includes a synthesis pipeline that generates complex question sets from a dynamic dataset of time-sensitive documents. The findings reveal that RAG systems are particularly vulnerable to perturbations, especially in multi-hop queries, highlighting the need for improved robustness in document retrieval.'}, 'zh': {'title': '评估RAG系统的鲁棒性', 'desc': 'RARE是一个评估检索增强生成（RAG）系统在真实世界噪声、上下文冲突和时间敏感数据下鲁棒性的框架。它通过一个知识图谱驱动的基准测试，联合测试查询和文档的扰动。RARE构建了一个包含400个专家级时间敏感文档和48,322个问题的数据集，以量化模型在面对变化时的鲁棒性。研究结果表明，RAG系统在多跳查询上的鲁棒性普遍低于单跳查询，且文档的鲁棒性是最薄弱的环节。'}}}, {'id': 'https://huggingface.co/papers/2506.00772', 'title': 'LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.00772', 'abstract': 'Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.', 'score': 2, 'issue_id': 4095, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'd66b2e2afe71cfd9', 'authors': ['Zihang Liu', 'Tianyu Pang', 'Oleg Balabanov', 'Chaoqun Yang', 'Tianjin Huang', 'Lu Yin', 'Yaoqing Yang', 'Shiwei Liu'], 'affiliations': ['Dartmouth College, NH, USA', 'Eindhoven University of Technology, the Netherlands', 'International Computer Science Institute, CA, USA', 'Lawrence Berkeley National Laboratory, CA, USA', 'Tsinghua University, China', 'University of California, Berkeley, CA, USA', 'University of Exeter, Exeter, UK', 'University of Oxford, Oxford, UK', 'University of Surrey, Guildford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.00772.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#low_resource', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная точная настройка больших языковых моделей с помощью разреженного обновления весов', 'desc': 'Статья представляет новый метод точной настройки больших языковых моделей под названием LIFT (Low-rank Informed Sparse Fine-Tuning). LIFT использует низкоранговую аппроксимацию для выявления критически важных весов модели и обновляет только 5% от их общего числа. Этот подход позволяет достичь лучших результатов на задачах рассуждения по сравнению с полной точной настройкой, сохраняя при этом эффективность использования памяти. LIFT также лучше сохраняет знания из исходной предметной области по сравнению с другими методами точной настройки.'}, 'en': {'title': 'Efficient Fine-Tuning with Critical Weights', 'desc': 'This paper introduces a method called Low-rank Informed Sparse Fine-Tuning (LIFT) that improves the efficiency and performance of large language models (LLMs) by focusing on critical weights identified through low-rank approximation. Instead of updating all parameters during fine-tuning, LIFT selectively updates only the top 5% of Principal Weights, which are determined to be the most important for reasoning tasks. This approach not only enhances reasoning capabilities but also reduces the risk of overfitting and catastrophic forgetting, common issues in full fine-tuning. The results show that LIFT outperforms traditional full fine-tuning while preserving more knowledge from the original model, making it a promising strategy for efficient model adaptation.'}, 'zh': {'title': '低秩微调：提升大型语言模型的效率与性能', 'desc': '本论文提出了一种新的稀疏微调方法，称为低秩知情稀疏微调（LIFT），旨在提高大型语言模型的性能和效率。通过低秩近似，我们识别出对推理至关重要的权重，称为主权重，并仅更新这些权重的前5%。与完全微调相比，LIFT在推理任务上表现更好，同时在内存使用上保持高效。该方法在保持源领域知识的同时，能够有效避免过拟合和灾难性遗忘。'}}}, {'id': 'https://huggingface.co/papers/2506.00530', 'title': 'CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing', 'url': 'https://huggingface.co/papers/2506.00530', 'abstract': 'Understanding urban socioeconomic conditions through visual data is a challenging yet essential task for sustainable urban development and policy planning. In this work, we introduce CityLens, a comprehensive benchmark designed to evaluate the capabilities of large language-vision models (LLVMs) in predicting socioeconomic indicators from satellite and street view imagery. We construct a multi-modal dataset covering a total of 17 globally distributed cities, spanning 6 key domains: economy, education, crime, transport, health, and environment, reflecting the multifaceted nature of urban life. Based on this dataset, we define 11 prediction tasks and utilize three evaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation, and Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across these tasks. Our results reveal that while LLVMs demonstrate promising perceptual and reasoning capabilities, they still exhibit limitations in predicting urban socioeconomic indicators. CityLens provides a unified framework for diagnosing these limitations and guiding future efforts in using LLVMs to understand and predict urban socioeconomic patterns. Our codes and datasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.', 'score': 2, 'issue_id': 4097, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '4c9476ad7ef056e6', 'authors': ['Tianhui Liu', 'Jie Feng', 'Hetian Pang', 'Xin Zhang', 'Tianjian Ouyang', 'Zhiyuan Zhang', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China', 'School of Electronic and Information Engineering, Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00530.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#dataset', '#reasoning', '#survey'], 'emoji': '🏙️', 'ru': {'title': 'CityLens: Взгляд на город глазами искусственного интеллекта', 'desc': 'CityLens - это комплексный бенчмарк для оценки способностей мультимодальных языковых моделей (LLVM) в прогнозировании социально-экономических показателей на основе спутниковых снимков и изображений с уличных камер. Датасет охватывает 17 городов по всему миру и включает 6 ключевых областей: экономика, образование, преступность, транспорт, здравоохранение и окружающая среда. Авторы определили 11 задач прогнозирования и использовали три парадигмы оценки: прямое предсказание метрик, нормализованная оценка метрик и регрессия на основе признаков. Результаты показывают, что хотя LLVM демонстрируют многообещающие возможности восприятия и рассуждения, они все еще имеют ограничения в прогнозировании городских социально-экономических показателей.'}, 'en': {'title': 'CityLens: Bridging Visual Data and Urban Socioeconomic Insights', 'desc': 'This paper presents CityLens, a benchmark for assessing large language-vision models (LLVMs) in predicting urban socioeconomic indicators using visual data from satellite and street view images. The study creates a multi-modal dataset that includes 17 cities and covers six important domains such as economy and health, reflecting the complexity of urban environments. It defines 11 prediction tasks and employs three evaluation methods to analyze the performance of 17 advanced LLVMs. The findings indicate that while these models show potential in understanding urban data, they still face challenges in accurately predicting socioeconomic conditions, highlighting the need for further research in this area.'}, 'zh': {'title': '通过视觉数据理解城市社会经济条件', 'desc': '本研究介绍了CityLens，这是一个综合基准，用于评估大型语言-视觉模型（LLVMs）在从卫星和街景图像中预测城市社会经济指标的能力。我们构建了一个多模态数据集，涵盖全球17个城市，涉及经济、教育、犯罪、交通、健康和环境等6个关键领域，反映了城市生活的多面性。基于该数据集，我们定义了11个预测任务，并采用三种评估范式：直接度量预测、标准化度量估计和基于特征的回归。结果表明，尽管LLVMs在感知和推理能力上表现出色，但在预测城市社会经济指标方面仍存在局限性。'}}}, {'id': 'https://huggingface.co/papers/2506.00385', 'title': 'MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2506.00385', 'abstract': 'MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.', 'score': 2, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '421c283aaadfdd94', 'authors': ['Yakun Song', 'Jiawei Chen', 'Xiaobin Zhuang', 'Chenpeng Du', 'Ziyang Ma', 'Jian Wu', 'Jian Cong', 'Dongya Jia', 'Zhuo Chen', 'Yuping Wang', 'Yuxuan Wang', 'Xie Chen'], 'affiliations': ['Bytedance Inc.', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00385.jpg', 'data': {'categories': ['#audio', '#optimization', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'MagiCodec: Семантическая токенизация аудио для улучшенной генерации', 'desc': 'MagiCodec - это новый аудио кодек на основе трансформера, разработанный для улучшения семантической токенизации при сохранении высокого качества реконструкции. Он использует многоступенчатый процесс обучения, включающий добавление гауссова шума и регуляризацию скрытого пространства. Эксперименты показывают, что MagiCodec превосходит современные кодеки как по качеству реконструкции, так и по эффективности в последующих задачах. Токены, создаваемые MagiCodec, имеют распределение, похожее на закон Ципфа, что улучшает совместимость с генеративными моделями на основе языковых моделей.'}, 'en': {'title': 'MagiCodec: Transforming Audio for Better AI Compatibility', 'desc': 'MagiCodec is a new audio codec that uses a Transformer model to improve how audio is represented as tokens. It focuses on enhancing the semantic meaning of these tokens while still ensuring that the audio can be reconstructed accurately. The codec employs a special training method that includes adding noise and regularization to make the tokens more expressive and useful for generative models. Tests show that MagiCodec outperforms existing codecs in both audio quality and its ability to work with other AI models.'}, 'zh': {'title': 'MagiCodec：提升音频语义表达的编解码器', 'desc': 'MagiCodec是一种基于Transformer的音频编解码器，旨在提高语义标记的表达能力，同时保持高质量的重建效果。与传统编解码器不同，MagiCodec在训练过程中引入了高斯噪声和潜在正则化，以增强生成代码的语义表现力。实验结果表明，MagiCodec在重建质量和下游任务上均优于现有的最先进编解码器。其生成的标记呈现出类似Zipf分布的特征，增强了与基于语言模型的生成架构的兼容性。'}}}, {'id': 'https://huggingface.co/papers/2506.00381', 'title': 'Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG', 'url': 'https://huggingface.co/papers/2506.00381', 'abstract': 'Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.', 'score': 2, 'issue_id': 4101, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '78b01a3a61f68dc7', 'authors': ['Siavash Shams', 'Richard Antonello', 'Gavin Mischler', 'Stephan Bickel', 'Ashesh Mehta', 'Nima Mesgarani'], 'affiliations': ['Department of Electrical Engineering, Columbia University, USA', 'The Feinstein Institutes for Medical Research, USA'], 'pdf_title_img': 'assets/pdf/title_img/2506.00381.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#transfer_learning', '#multimodal', '#healthcare', '#science'], 'emoji': '🧠', 'ru': {'title': 'От мозговых волн к словам: новый уровень нейродекодирования', 'desc': 'Neuro2Semantic - это новая система для реконструкции семантического содержания речи из нейронных сигналов. Она использует LSTM-адаптер для выравнивания сигналов с предобученными текстовыми эмбеддингами и модуль-корректор для генерации естественного текста. Метод показывает высокую эффективность даже при ограниченном объеме нейронных данных (30 минут). Neuro2Semantic превосходит существующие методы и открывает возможности для практического применения в нейроинтерфейсах.'}, 'en': {'title': 'Transforming Brain Signals into Natural Language', 'desc': 'Neuro2Semantic is a new framework that translates brain signals into meaningful text. It uses a Long Short-Term Memory (LSTM) model to align neural signals from intracranial EEG with existing text representations. After alignment, a correction module generates coherent and natural language from these signals. This method is effective even with limited data, making it a significant advancement in decoding language from neural activity.'}, 'zh': {'title': '神经信号重建语义内容的新方法', 'desc': 'Neuro2Semantic 是一个新颖的框架，旨在从脑内电极记录的神经信号中重建语义内容。该方法采用了基于 LSTM 的对齐技术，将神经信号与预训练的文本嵌入对齐，并通过一个校正模块直接生成自然的连续文本。与以往的解码方法相比，Neuro2Semantic 在数据量有限的情况下表现出色，能够在仅有 30 分钟的神经数据下实现强大的性能。该研究为脑机接口和神经解码技术的实际应用提供了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2505.17127', 'title': 'Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts', 'url': 'https://huggingface.co/papers/2505.17127', 'abstract': 'Visual CounterFact and PvP steering vectors help interpret and control the competition between visual input and memorized world knowledge in multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.', 'score': 2, 'issue_id': 4105, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '1c1f950796c1f608', 'authors': ['Michal Golovanevsky', 'William Rudman', 'Michael Lepori', 'Amir Bar', 'Ritambhara Singh', 'Carsten Eickhoff'], 'affiliations': ['Brown University', 'Tel Aviv University', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17127.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#reasoning', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Управление конкуренцией между визуальными данными и знаниями в многомодальных ИИ-моделях', 'desc': 'В статье представлены новые инструменты для интерпретации и управления многомодальными большими языковыми моделями (MLLM). Авторы вводят набор данных Visual CounterFact, содержащий визуально реалистичные контрфактические примеры, которые противопоставляют предварительные знания о мире и визуальную информацию. Исследование показывает, что предсказания модели сначала отражают запомненные знания, но затем смещаются в сторону визуальных данных. Для контроля этого поведения предложены векторы управления PvP (Pixels Versus Priors), позволяющие влиять на выходные данные модели.'}, 'en': {'title': 'Balancing Visual Input and Knowledge in MLLMs', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) balance visual information and memorized knowledge when making predictions. It introduces a dataset called Visual CounterFact, which presents scenarios where visual cues conflict with known facts, allowing researchers to observe how models prioritize information. The study finds that while models initially rely on memorized knowledge, they increasingly favor visual evidence as processing continues. To manage this competition, the authors propose Pixels Versus Priors (PvP) steering vectors, which can adjust model outputs to emphasize either visual input or prior knowledge effectively.'}, 'zh': {'title': '控制视觉与记忆知识的竞争', 'desc': '这篇论文探讨了多模态大型语言模型（MLLMs）在视觉输入和记忆知识之间的竞争。我们引入了Visual CounterFact数据集，通过视觉反事实来直接对抗世界知识和视觉信息。研究发现，模型的预测最初依赖于记忆知识，但在后期层次中逐渐转向视觉证据。为控制这种行为，我们提出了Pixels Versus Priors（PvP）引导向量，可以通过激活级别干预来调整模型输出，成功将大部分预测从记忆知识转向视觉输入。'}}}, {'id': 'https://huggingface.co/papers/2506.01062', 'title': 'SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models', 'url': 'https://huggingface.co/papers/2506.01062', 'abstract': 'SealQA evaluates search-augmented language models\' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in "needle-in-a-haystack" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the "lost-in-the-middle" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa.', 'score': 1, 'issue_id': 4104, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '32b5670dde584ad1', 'authors': ['Thinh Pham', 'Nguyen Nguyen', 'Pratibha Zunjare', 'Weiyuan Chen', 'Yu-Min Tseng', 'Tu Vu'], 'affiliations': ['Virginia Tech, Blacksburg, VA 24061'], 'pdf_title_img': 'assets/pdf/title_img/2506.01062.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#long_context', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'SealQA: Выявление ограничений языковых моделей в работе с противоречивой информацией', 'desc': 'SealQA - это новый эталонный тест для оценки моделей языка с поисковым дополнением на фактологических вопросах с противоречивыми или зашумленными результатами поиска. Тест включает три варианта: Seal-0, Seal-Hard и LongSeal, оценивающие фактическую точность и способности к рассуждению. Оценка выявила критические ограничения современных моделей, даже передовые LLM показывают низкую производительность во всех вариантах SealQA. Исследование также показало, что увеличение вычислительной мощности не приводит к надежному улучшению результатов.'}, 'en': {'title': 'SealQA: Unveiling the Limits of Search-Augmented Language Models', 'desc': 'SealQA is a benchmark designed to evaluate the performance of search-augmented language models on fact-seeking questions, especially when search results are conflicting or noisy. It consists of three versions: Seal-0, which focuses on challenging questions; Seal-Hard, which tests reasoning and factual accuracy; and LongSeal, which assesses long-context reasoning in complex scenarios. The evaluation shows that even advanced language models struggle significantly, with low accuracy rates on Seal-0 and vulnerability to noisy search results. Furthermore, increasing computational resources does not consistently improve performance, highlighting the need for better models in handling complex information retrieval tasks.'}, 'zh': {'title': 'SealQA：评估语言模型在复杂搜索中的表现', 'desc': 'SealQA是一个新的基准，用于评估增强搜索的语言模型在面对冲突或噪声搜索结果时的表现，特别是在事实寻求问题上。该基准分为三种类型：Seal-0和Seal-Hard主要评估模型的事实准确性和推理能力，而LongSeal则测试长上下文和多文档推理能力。评估结果显示，当前的前沿模型在所有SealQA类型中表现不佳，尤其是在Seal-0中，准确率极低。尽管一些先进的推理模型存在，但它们在面对噪声搜索结果时仍然非常脆弱，且增加计算资源并未显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2506.00979', 'title': 'IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection', 'url': 'https://huggingface.co/papers/2506.00979', 'abstract': 'IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '13910274f7d956aa', 'authors': ['Wayne Zhang', 'Changjiang Jiang', 'Zhonghao Zhang', 'Chenyang Si', 'Fengchang Yu', 'Wei Peng'], 'affiliations': ['Wuhan University', 'π3 AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.00979.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#interpretability', '#multimodal', '#synthetic', '#benchmark', '#architecture'], 'emoji': '🕵️', 'ru': {'title': 'Единая система для прозрачного обнаружения искусственного контента', 'desc': 'Статья представляет новый набор данных IVY-FAKE и архитектуру IVY-XDETECTOR для обнаружения искусственно сгенерированного контента (AIGC). Этот подход решает проблемы существующих методов, предоставляя единую интерпретируемую систему для анализа как изображений, так и видео. Набор данных содержит более 150 000 аннотированных образцов с подробными текстовыми объяснениями. Предложенная модель достигает высоких результатов в обнаружении AIGC для изображений и видео.'}, 'en': {'title': 'Unifying AIGC Detection with Explainability', 'desc': 'The paper introduces the IVY-FAKE dataset and the Ivy Explainable Detector (IVY-XDETECTOR) to improve the detection of AI-generated content (AIGC) in images and videos. Current detection methods often lack transparency and only classify content without providing explanations, which can undermine trust. IVY-FAKE offers a large-scale dataset with over 150,000 annotated samples, enhancing the interpretability of detection results through detailed reasoning. The IVY-XDETECTOR utilizes a unified vision-language model to achieve state-of-the-art performance in detecting both images and videos, addressing the limitations of existing approaches.'}, 'zh': {'title': '统一可解释的 AIGC 检测新框架', 'desc': 'IVY-FAKE 数据集和 Ivy Explainable Detector (IVY-XDETECTOR) 架构旨在解决当前 AIGC 检测的局限性，提供一个统一且可解释的图像和视频检测框架。随着人工智能生成内容（AIGC）在视觉领域的快速发展，生成的图像和视频变得越来越真实，这引发了对内容真实性的担忧。现有的 AIGC 检测方法通常作为黑箱二元分类器，缺乏可解释性，且无法在统一框架下同时检测图像和视频。我们的研究通过引入 IVY-FAKE 数据集和 IVY-XDETECTOR，显著提升了多模态 AIGC 检测的性能和透明度。'}}}, {'id': 'https://huggingface.co/papers/2506.00469', 'title': 'Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data', 'url': 'https://huggingface.co/papers/2506.00469', 'abstract': 'Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.', 'score': 1, 'issue_id': 4093, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2ebeb941a6f4f7cc', 'authors': ['Shaoxiong Ji', 'Zihao Li', 'Jaakko Paavola', 'Indraneil Paul', 'Hengyu Luo', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2506.00469.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multilingual', '#transfer_learning', '#open_source', '#machine_translation', '#training', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'Двуязычные данные улучшают многоязычную адаптацию больших языковых моделей', 'desc': 'Это исследование посвящено влиянию двуязычных данных перевода на многоязычную адаптацию моделей семейства Llama3 к 500 языкам. Авторы создали корпус MaLA, содержащий данные более чем 2500 языковых пар, и разработали набор EMMA-500 Llama 3 из четырех многоязычных моделей. Эксперименты показали, что использование двуязычных данных улучшает языковой перенос и производительность, особенно для малоресурсных языков. Результаты оценивались на 7 задачах и 12 бенчмарках, а все ресурсы были открыты для общего доступа.'}, 'en': {'title': 'Boosting Multilingual Models with Bilingual Data', 'desc': 'This paper explores how using bilingual translation data can improve the performance of the Llama3 family of models in multilingual settings. It introduces the MaLA bilingual translation corpus, which includes data from over 2,500 language pairs, to facilitate better language adaptation. The authors develop the EMMA-500 suite of models, which are continually pre-trained on a vast amount of diverse data. Their findings show that incorporating bilingual data significantly enhances language transfer, especially for languages with fewer resources.'}, 'zh': {'title': '双语数据助力多语言模型提升性能', 'desc': '本文研究了在大规模多语言持续预训练中，双语翻译数据的关键设计决策。我们构建了MaLA双语翻译语料库，包含2500多个语言对的数据，以支持Llama3模型在500种语言上的适应。通过开发EMMA-500 Llama 3模型套件，我们评估了使用或不使用双语翻译数据的持续预训练效果。结果表明，双语数据能够增强语言迁移和性能，尤其是在资源稀缺的语言上。'}}}, {'id': 'https://huggingface.co/papers/2505.24216', 'title': 'Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation', 'url': 'https://huggingface.co/papers/2505.24216', 'abstract': 'A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t This work investigates Source-Free Domain Adaptation (SFDA), where a model adapts to a target domain without access to source data. A new augmentation technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are introduced to enhance performance. SPM shuffles and blends image patches to generate diverse and challenging augmentations, while the reweighting strategy prioritizes reliable pseudo-labels to mitigate label noise. These techniques are particularly effective on smaller datasets like PACS, where overfitting and pseudo-label noise pose greater risks. State-of-the-art results are achieved on three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS, improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target and multi-target settings, respectively, while gains of 2.8% and 0.7% are attained on DomainNet-126 and VisDA-C. This combination of advanced augmentation and robust pseudo-label reweighting establishes a new benchmark for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM', 'score': 1, 'issue_id': 4102, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '4e2243b50d13c1bf', 'authors': ['Prasanna Reddy Pulakurthi', 'Majid Rabbani', 'Jamison Heard', 'Sohail Dianat', 'Celso M. de Melo', 'Raghuveer Rao'], 'affiliations': ['DEVCOM Army Research Laboratory, Adelphi, MD, USA', 'Rochester Institute of Technology, Rochester, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.24216.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#benchmark', '#data', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Улучшение адаптации к домену без исходных данных с помощью умной аугментации и перевзвешивания', 'desc': 'Статья представляет новый метод аугментации данных под названием Shuffle PatchMix (SPM) и стратегию перевзвешивания для улучшения производительности в задаче адаптации к домену без исходных данных (Source-Free Domain Adaptation, SFDA). SPM перемешивает и смешивает участки изображений для создания разнообразных и сложных аугментаций. Стратегия перевзвешивания отдает приоритет надежным псевдо-меткам для уменьшения шума в разметке. Предложенные методы достигают наилучших результатов на трех основных бенчмарках: PACS, VisDA-C и DomainNet-126.'}, 'en': {'title': 'Enhancing Source-Free Domain Adaptation with Shuffle PatchMix', 'desc': 'This paper presents a novel approach to Source-Free Domain Adaptation (SFDA) using a technique called Shuffle PatchMix (SPM) and a reweighting strategy. SPM enhances data augmentation by shuffling and blending image patches, creating diverse training samples that help the model generalize better. The reweighting strategy focuses on selecting reliable pseudo-labels, reducing the impact of label noise during training. The proposed methods achieve state-of-the-art performance on several benchmarks, demonstrating significant improvements in accuracy, especially on smaller datasets like PACS.'}, 'zh': {'title': '无源领域适应的新突破：Shuffle PatchMix', 'desc': '本文提出了一种新的数据增强技术Shuffle PatchMix（SPM）和重加权策略，以提高无源领域适应（SFDA）的性能。SPM通过打乱和混合图像块生成多样化的增强样本，而重加权策略则优先考虑可靠的伪标签，以减少标签噪声的影响。这些技术在较小的数据集上表现尤为出色，如PACS，能够有效应对过拟合和伪标签噪声的问题。最终，在PACS、VisDA-C和DomainNet-126等三个主要基准上取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2505.22865', 'title': 'BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models', 'url': 'https://huggingface.co/papers/2505.22865', 'abstract': 'A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Binaural rendering aims to synthesize binaural audio that mimics natural hearing based on a mono audio and the locations of the speaker and listener. Although many methods have been proposed to solve this problem, they struggle with rendering quality and streamable inference. Synthesizing high-quality binaural audio that is indistinguishable from real-world recordings requires precise modeling of binaural cues, room reverb, and ambient sounds. Additionally, real-world applications demand streaming inference. To address these challenges, we propose a flow matching based streaming binaural speech synthesis framework called BinauralFlow. We consider binaural rendering to be a generation problem rather than a regression problem and design a conditional flow matching model to render high-quality audio. Moreover, we design a causal U-Net architecture that estimates the current audio frame solely based on past information to tailor generative models for streaming inference. Finally, we introduce a continuous inference pipeline incorporating streaming STFT/ISTFT operations, a buffer bank, a midpoint solver, and an early skip schedule to improve rendering continuity and speed. Quantitative and qualitative evaluations demonstrate the superiority of our method over SOTA approaches. A perceptual study further reveals that our model is nearly indistinguishable from real-world recordings, with a 42% confusion rate.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0d57f1fd2f698379', 'authors': ['Susan Liang', 'Dejan Markovic', 'Israel D. Gebru', 'Steven Krenn', 'Todd Keebler', 'Jacob Sandakly', 'Frank Yu', 'Samuel Hassel', 'Chenliang Xu', 'Alexander Richard'], 'affiliations': ['1', '2'], 'pdf_title_img': 'assets/pdf/title_img/2505.22865.jpg', 'data': {'categories': ['#audio', '#diffusion'], 'emoji': '🎧', 'ru': {'title': 'Реалистичный бинауральный синтез речи в реальном времени', 'desc': 'BinauralFlow - это фреймворк для потокового бинаурального синтеза речи, основанный на методе сопоставления потоков. Он использует каузальную архитектуру U-Net для генерации высококачественного бинаурального аудио, неотличимого от реальных записей. Модель рассматривает бинауральный рендеринг как задачу генерации, а не регрессии, что позволяет точно моделировать бинауральные сигналы, реверберацию помещения и фоновые звуки. Предложенный непрерывный конвейер вывода включает потоковые операции STFT/ISTFT, банк буферов и другие компоненты для улучшения непрерывности и скорости рендеринга.'}, 'en': {'title': 'BinauralFlow: Real-Time, High-Quality Binaural Audio Synthesis', 'desc': 'The paper presents BinauralFlow, a novel framework for generating high-quality binaural audio that closely resembles natural hearing. It utilizes a causal U-Net architecture and a flow matching approach to enhance the synthesis of binaural cues, room reverb, and ambient sounds. The framework is designed for streaming inference, allowing for real-time audio generation by processing only past audio frames. Evaluations show that BinauralFlow outperforms state-of-the-art methods, achieving audio quality that is nearly indistinguishable from real-world recordings.'}, 'zh': {'title': '高质量双耳音频合成的新突破', 'desc': '本文提出了一种基于流匹配的流式双耳语音合成框架，称为BinauralFlow。该框架使用因果U-Net架构和连续推理管道，能够生成高质量、与真实录音几乎无法区分的双耳音频。我们将双耳渲染视为生成问题，设计了条件流匹配模型来提高音频渲染质量。此外，本文还引入了连续推理管道，以改善渲染的连续性和速度，实验结果表明该方法优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2505.21668', 'title': 'R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21668', 'abstract': 'R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98.', 'score': 1, 'issue_id': 4101, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a1b1a393cb209a0c', 'authors': ['Yongchao Chen', 'Yueying Liu', 'Junwei Zhou', 'Yilun Hao', 'Jingquan Wang', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard', 'MIT', 'MIT-IBM Watson AI Lab', 'University of Illinois Urbana-Champaign', 'University of Michigan', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.21668.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#data', '#dataset', '#training', '#rl', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Усиление языковых моделей автономной генерацией кода', 'desc': 'R1-Code-Interpreter - это расширение текстовой языковой модели, обученное с помощью многоэтапной контролируемой доводки и обучения с подкреплением для автономной генерации кода во время пошагового рассуждения. Модель обучалась на 144 задачах рассуждения и планирования, каждая из которых содержит более 200 разнообразных вопросов. Финальная модель R1-CI-14B улучшила среднюю точность на 37 тестовых задачах с 44.0% до 64.1%, превзойдя текстовую версию GPT-4 (58.6%) и приблизившись к GPT-4 с Code Interpreter (70.9%). Исследование показало, что обучение с использованием Code Interpreter значительно сложнее из-за высокого разнообразия задач и дорогостоящего выполнения кода.'}, 'en': {'title': 'Empowering LLMs with Code: R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter enhances text-only Large Language Models (LLMs) by integrating improved code generation capabilities through supervised fine-tuning and reinforcement learning. This model addresses the limitations of LLMs in tasks that require precise computation and algorithmic reasoning, allowing it to autonomously generate code queries during multi-step reasoning. The research involves fine-tuning Qwen-2.5 models on a diverse set of reasoning and planning tasks, demonstrating significant improvements in accuracy compared to previous models. The findings highlight the importance of the supervised fine-tuning stage in effectively training LLMs to leverage code for better performance across various tasks.'}, 'zh': {'title': '提升代码生成能力的R1-Code-Interpreter', 'desc': 'R1-Code-Interpreter 是一种扩展文本模型的机器学习方法，旨在提高代码生成能力。通过监督微调和强化学习，该模型能够在推理和规划任务中表现更好。研究表明，传统的大型语言模型在需要精确计算和符号操作的任务中仍然存在困难。R1-Code-Interpreter 通过多轮的训练，能够自主生成代码查询，从而提升了模型的准确性和自我检查能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20285', 'title': 'MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability', 'url': 'https://huggingface.co/papers/2505.20285', 'abstract': 'A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MaskSearch. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MaskSearch significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.', 'score': 1, 'issue_id': 4098, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'fb55a3a486e6e5a9', 'authors': ['Weiqi Wu', 'Xin Guan', 'Shen Huang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jiuxin Cao', 'Hai Zhao', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.20285.jpg', 'data': {'categories': ['#training', '#agents', '#optimization', '#rag', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'MaskSearch: универсальный поиск и рассуждения для больших языковых моделей', 'desc': 'В статье представлена новая система предобучения MaskSearch, которая улучшает способности больших языковых моделей (LLM) к универсальному поиску и рассуждениям. Эта система использует задачу предсказания маскированных токенов с дополнительным извлечением информации (Retrieval Augmented Mask Prediction). MaskSearch применяет комбинацию методов обучения, включая обучение с учителем и обучение с подкреплением. Результаты экспериментов показывают значительное улучшение производительности LLM в задачах открытого многоходового вопросно-ответного анализа.'}, 'en': {'title': 'Empowering LLMs with Universal Retrieval and Reasoning through MaskSearch', 'desc': 'The paper introduces MaskSearch, a new pre-training framework designed to improve Large Language Models (LLMs) by enhancing their retrieval and reasoning abilities. It employs a Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to use search tools to predict masked spans in data, thereby gaining universal capabilities. The training process includes Supervised Fine-tuning (SFT) and Reinforcement Learning (RL), utilizing a multi-agent system and a hybrid reward system to optimize performance. The results show that MaskSearch significantly boosts the effectiveness of LLMs in open-domain multi-hop question answering tasks.'}, 'zh': {'title': 'MaskSearch：提升语言模型的检索与推理能力', 'desc': '本文提出了一种新的预训练框架，称为MaskSearch，旨在增强大型语言模型（LLMs）的检索和推理能力。通过引入检索增强的掩码预测任务（RAMP），模型能够利用搜索工具填补大量预训练数据中的掩码部分，从而获得通用的检索和推理能力。我们采用监督微调和强化学习相结合的方法进行训练，并引入了课程学习策略，使模型能够逐步从简单到复杂的实例中学习。实验结果表明，MaskSearch显著提升了基于LLM的搜索代理在开放领域多跳问答任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.18128', 'title': 'Frankentext: Stitching random text fragments into long-form narratives', 'url': 'https://huggingface.co/papers/2505.18128', 'abstract': 'We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.', 'score': 1, 'issue_id': 4100, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '2cd0e9db501521da', 'authors': ['Chau Minh Pham', 'Jenna Russell', 'Dzung Pham', 'Mohit Iyyer'], 'affiliations': ['University of Maryland, College Park', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2505.18128.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#story_generation', '#training'], 'emoji': '🧟\u200d♂️', 'ru': {'title': 'Frankentexts: новый вызов для генерации и детекции ИИ-текстов', 'desc': 'Исследователи представили новый тип длинных текстов, называемых Frankentexts, которые создаются языковыми моделями с ограничением копирования большей части токенов из человеческих текстов. Этот метод требует от моделей соблюдения заданной темы, интеграции разрозненных фрагментов текста и создания связного повествования. Gemini-2.5-Pro показала высокие результаты в этой задаче, создавая когерентные и релевантные тексты. Исследование выявило ограничения существующих детекторов ИИ-текстов и открывает дискуссию о разработке эффективных методов обнаружения смешанного авторства.'}, 'en': {'title': 'Frankentexts: Merging Human Creativity with AI Precision', 'desc': "Frankentexts are a novel form of long narratives created by large language models (LLMs) that must predominantly use human-written text. This approach tests the model's ability to generate coherent stories while adhering to strict guidelines on text copying. The process involves drafting a narrative by merging human passages and revising it to meet a specified ratio of copied content. The results show that while the generated texts are often coherent and relevant, they can still be distinguished from human writing due to inconsistencies in tone and grammar, highlighting challenges in AI text detection."}, 'zh': {'title': 'Frankentexts：人机共创的新挑战', 'desc': '本文介绍了一种新型的长篇叙事文本，称为Frankentexts，这些文本由大型语言模型（LLMs）在极端约束下生成，要求大部分（例如90%）的词汇必须逐字复制自人类写作。这项任务对可控生成提出了挑战，要求模型满足写作提示，整合不同的文本片段，并生成连贯的叙事。我们通过指导模型选择和组合人类写作的段落来生成初稿，然后在保持用户指定的复制比例的同时，迭代修订初稿。研究结果表明，Gemini-2.5-Pro在此任务中表现出色，81%的Frankentexts连贯且100%与提示相关。'}}}, {'id': 'https://huggingface.co/papers/2505.16122', 'title': 'Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning', 'url': 'https://huggingface.co/papers/2505.16122', 'abstract': "Plan-and-Budget framework enhances reasoning efficiency in LLMs by allocating token budgets based on estimated sub-question complexity, improving accuracy, reducing token usage, and boosting $E^3$ metric.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in complex reasoning tasks, but their inference remains computationally inefficient. We observe a common failure mode in many prevalent LLMs, overthinking, where models generate verbose and tangential reasoning traces even for simple queries. Recent works have tried to mitigate this by enforcing fixed token budgets, however, this can lead to underthinking, especially on harder problems. Through empirical analysis, we identify that this inefficiency often stems from unclear problem-solving strategies. To formalize this, we develop a theoretical model, BBAM (Bayesian Budget Allocation Model), which models reasoning as a sequence of sub-questions with varying uncertainty, and introduce the E^3 metric to capture the trade-off between correctness and computation efficiency. Building on theoretical results from BBAM, we propose Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex queries into sub-questions and allocates token budgets based on estimated complexity using adaptive scheduling. Plan-and-Budget improves reasoning efficiency across a range of tasks and models, achieving up to +70% accuracy gains, -39% token reduction, and +187.5% improvement in E^3. Notably, it elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close performance gaps without retraining. Our code is available at anonymous.4open.science/r/P-and-B-6513/.", 'score': 1, 'issue_id': 4105, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'fbd65f85b45afd41', 'authors': ['Junhong Lin', 'Xinyue Zeng', 'Jie Zhu', 'Song Wang', 'Julian Shun', 'Jun Wu', 'Dawei Zhou'], 'affiliations': ['MIT CSAIL', 'Michigan State University', 'University of Virginia', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2505.16122.jpg', 'data': {'categories': ['#training', '#inference', '#small_models', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Умное распределение ресурсов для эффективных рассуждений ИИ', 'desc': 'Фреймворк Plan-and-Budget повышает эффективность рассуждений в больших языковых моделях (LLM) путем распределения токенов на основе оценки сложности подвопросов. Это улучшает точность, снижает использование токенов и повышает метрику $E^3$. Фреймворк разбивает сложные запросы на подвопросы и адаптивно распределяет бюджет токенов. План-и-Бюджет улучшает эффективность рассуждений для различных задач и моделей без необходимости переобучения.'}, 'en': {'title': 'Smart Token Allocation for Efficient Reasoning in LLMs', 'desc': "The paper introduces the Plan-and-Budget framework, which enhances the reasoning efficiency of Large Language Models (LLMs) by intelligently allocating token budgets based on the complexity of sub-questions. It addresses the common issue of overthinking in LLMs, where they generate excessive and irrelevant reasoning for simple queries. By employing the Bayesian Budget Allocation Model (BBAM), the framework decomposes complex queries into manageable parts and adapts token usage to improve both accuracy and computational efficiency. The results show significant improvements, including up to 70% accuracy gains and a 39% reduction in token usage, demonstrating the framework's effectiveness across various tasks and models."}, 'zh': {'title': '优化推理效率，提升模型表现的Plan-and-Budget框架', 'desc': '本文提出了一种名为Plan-and-Budget的框架，旨在提高大型语言模型（LLMs）的推理效率。该框架通过根据子问题的复杂性分配令牌预算，减少了不必要的计算，同时提高了模型的准确性。研究表明，许多LLMs在处理简单查询时会出现过度思考的问题，而Plan-and-Budget能够有效地将复杂查询分解为子问题，从而优化推理过程。通过实证分析，Plan-and-Budget在多个任务上实现了显著的性能提升，证明了其在不重新训练的情况下缩小模型性能差距的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.15772', 'title': 'MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling', 'url': 'https://huggingface.co/papers/2505.15772', 'abstract': 'Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning.', 'score': 1, 'issue_id': 4095, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '6b39ad10d21b05d8', 'authors': ['Yifan Cheng', 'Ruoyi Zhang', 'Jiatong Shi'], 'affiliations': ['Carnegie Mellon University, Pittsburgh, PA, USA', 'Fish Audio, Santa Clara, CA, USA', 'Huazhong University of Science and Technology, Wuhan, Hubei, China', 'Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15772.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#audio', '#data'], 'emoji': '🎭', 'ru': {'title': 'Автоматизированная система для создания высококачественных наборов данных эмоциональной речи', 'desc': 'MIKU-PAL - это автоматизированная мультимодальная система для извлечения эмоциональной речи из немаркированных видеоданных. Система использует алгоритмы обнаружения и отслеживания лиц, а также мультимодальную большую языковую модель (MLLM) для анализа эмоций. MIKU-PAL достигает точности на уровне человека (68,5% на наборе данных MELD) и высокой согласованности (0,93 по шкале Флейса каппа), при этом работая быстрее и дешевле ручной разметки. На основе этой системы был создан набор данных MIKU-EmoBench объемом 131,2 часа, содержащий 26 типов эмоциональной речи.'}, 'en': {'title': 'Automating Emotional Speech Extraction with MIKU-PAL', 'desc': 'This paper introduces MIKU-PAL, an automated system designed to extract emotional speech data from unlabeled video sources. It utilizes face detection and tracking, combined with a multimodal large language model, to analyze emotions effectively. The system achieves high accuracy and consistency in emotional speech annotation, outperforming traditional human methods in both cost and speed. Additionally, it provides a new dataset, MIKU-EmoBench, which includes a diverse range of emotional speech categories for further research in speech synthesis.'}, 'zh': {'title': 'MIKU-PAL：高效一致的情感语音提取新方法', 'desc': '本论文提出了一种名为MIKU-PAL的全自动多模态管道，用于从未标记的视频数据中提取高一致性的情感语音。我们利用人脸检测和跟踪算法，开发了一个自动情感分析系统，使用多模态大语言模型（MLLM）。实验结果表明，MIKU-PAL在情感识别上达到了人类水平的准确率（68.5%），并且一致性显著优于人工标注（0.93 Fleiss kappa分数）。基于该系统，我们还发布了一个细粒度情感语音数据集MIKU-EmoBench（131.2小时），为情感文本到语音和视觉语音克隆提供了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2506.01666', 'title': 'Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models', 'url': 'https://huggingface.co/papers/2506.01666', 'abstract': "A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis.", 'score': 0, 'issue_id': 4096, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'cf55396f01f6e92c', 'authors': ['Florian Fürrutter', 'Zohim Chandani', 'Ikko Hamamura', 'Hans J. Briegel', 'Gorka Muñoz-Gil'], 'affiliations': ['Institute for Theoretical Physics University of Innsbruck', 'NVIDIA Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2506.01666.jpg', 'data': {'categories': ['#science', '#dataset', '#diffusion', '#multimodal', '#optimization', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Квантовая компиляция на новом уровне: диффузионная модель для генерации схем', 'desc': 'Представлена мультимодальная модель шумоподавляющей диффузии для генерации структуры и непрерывных параметров квантовых схем. Модель использует два независимых процесса диффузии: один для выбора дискретных вентилей, другой для предсказания параметров. Это обеспечивает эффективную альтернативу традиционным методам компиляции квантовых операций. Модель позволяет быстро генерировать большие наборы схем для конкретных операций, что помогает извлекать ценные эвристики для синтеза квантовых схем.'}, 'en': {'title': 'Revolutionizing Quantum Circuit Compilation with Diffusion Models', 'desc': "This paper presents a multimodal denoising diffusion model designed to efficiently generate both the structure and continuous parameters of quantum circuits. Unlike traditional methods that rely on lengthy search algorithms and gradient-based optimizations, this model utilizes two independent diffusion processes to handle discrete gate selection and parameter prediction simultaneously. The authors benchmark the model's performance across various qubit counts and circuit complexities, demonstrating its accuracy and efficiency. Additionally, the rapid circuit generation capability allows for the creation of large datasets, which can be used to uncover new insights into quantum circuit synthesis."}, 'zh': {'title': '高效编译量子电路的新方法', 'desc': '本文介绍了一种多模态去噪扩散模型，用于生成量子电路的结构和连续参数，提供了一种高效的替代传统量子操作编译方法。当前的编译方法虽然能降低编译误差，但运行时间长且需要多次调用量子硬件或昂贵的经典模拟，限制了其扩展性。新提出的模型同时生成电路的离散门选择和参数预测，利用两个独立的扩散过程进行优化。通过对不同实验的基准测试，分析了该方法在不同量子比特数量、电路深度和参数化门比例下的准确性。'}}}, {'id': 'https://huggingface.co/papers/2506.00523', 'title': 'SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation', 'url': 'https://huggingface.co/papers/2506.00523', 'abstract': 'Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.', 'score': 0, 'issue_id': 4099, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '6f9b962d86942eda', 'authors': ['Xingtong Ge', 'Xin Zhang', 'Tongda Xu', 'Yi Zhang', 'Xinjie Zhang', 'Yan Wang', 'Jun Zhang'], 'affiliations': ['Institute for AI Industry Research, Tsinghua University', 'SenseTime Research', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.00523.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшенная дистилляция для генерации изображений по тексту', 'desc': 'Статья представляет усовершенствованный метод дистилляции для крупномасштабных моделей генерации изображений по тексту. Авторы предлагают имплицитное выравнивание распределений (IDA) для регуляризации расстояния между распределениями генератора и фейковых данных. Также вводится внутрисегментное руководство (ISG) для перераспределения важности временных шагов из учительской модели. Эти улучшения позволяют успешно применить дистилляцию к таким моделям как Stable Diffusion 3.5 и FLUX, повышая их производительность.'}, 'en': {'title': 'Enhancing DMD for Better Convergence in Text-to-Image Models', 'desc': "This paper addresses the challenges of applying Distribution Matching Distillation (DMD) to large-scale text-to-image models, particularly focusing on convergence issues. The authors introduce implicit distribution alignment (IDA) to help align the generator's output with the target distribution, improving the training process. Additionally, they propose intra-segment guidance (ISG) to enhance the importance of timesteps derived from the teacher model, further aiding convergence. The resulting model, SenseFlow, demonstrates improved performance in distillation tasks for both diffusion and flow-based models."}, 'zh': {'title': '提升大规模模型性能的蒸馏新方法', 'desc': '本文提出了一种改进的分布匹配蒸馏方法，旨在解决大规模文本到图像模型的收敛问题。我们引入了隐式分布对齐（IDA）和段内引导（ISG）来优化生成器与假分布之间的距离，从而提高模型的性能。通过这两种方法，DMD在大型模型如SD 3.5和FLUX上实现了更好的收敛。最终，我们的模型SenseFlow在蒸馏过程中表现出色，适用于扩散和流匹配模型。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (36)', '#agents (68)', '#agi (20)', '#alignment (42)', '#architecture (91)', '#audio (23)', '#benchmark (231)', '#cv (82)', '#data (81)', '#dataset (198)', '#diffusion (72)', '#ethics (21)', '#games (68)', '#graphs (6)', '#hallucinations (31)', '#healthcare (12)', '#inference (47)', '#interpretability (53)', '#leakage (4)', '#long_context (43)', '#low_resource (19)', '#machine_translation (6)', '#math (23)', '#multilingual (21)', '#multimodal (179)', '#open_source (115)', '#optimization (237)', '#plp (1)', '#rag (20)', '#reasoning (166)', '#rl (81)', '#rlhf (34)', '#robotics (20)', '#science (33)', '#security (25)', '#small_models (20)', '#story_generation (6)', '#survey (18)', '#synthetic (44)', '#training (230)', '#transfer_learning (46)', '#video (66)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-23 00:59',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-23 00:59')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-23 00:59')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    